[32m[20221213 14:52:55 @logger.py:105][0m Log file set to ./tmp/walker/stand/20221213_145255/log/walker_stand-20221213_145255.log
[32m[20221213 14:52:55 @agent_ppo2.py:121][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 14:52:55 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 14:52:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |           0.0003 |           0.1902 |           0.2229 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |           0.0024 |           0.0849 |           0.2228 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0079 |           0.0722 |           0.2229 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0034 |           0.0663 |           0.2228 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0016 |           0.0624 |           0.2229 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0029 |           0.0593 |           0.2229 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0171 |           0.0585 |           0.2228 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0036 |           0.0544 |           0.2228 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0056 |           0.0522 |           0.2227 |
[32m[20221213 14:52:56 @agent_ppo2.py:185][0m |          -0.0026 |           0.0507 |           0.2228 |
[32m[20221213 14:52:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:52:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 26.44
[32m[20221213 14:52:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 28.78
[32m[20221213 14:52:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.99
[32m[20221213 14:52:57 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 9.99
[32m[20221213 14:52:57 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 9.99
[32m[20221213 14:52:57 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221213 14:52:57 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 14:52:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 14:52:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:52:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:52:57 @agent_ppo2.py:185][0m |          -0.0008 |           0.0511 |           0.2221 |
[32m[20221213 14:52:57 @agent_ppo2.py:185][0m |           0.0025 |           0.0330 |           0.2220 |
[32m[20221213 14:52:57 @agent_ppo2.py:185][0m |          -0.0069 |           0.0314 |           0.2218 |
[32m[20221213 14:52:57 @agent_ppo2.py:185][0m |          -0.0058 |           0.0298 |           0.2218 |
[32m[20221213 14:52:57 @agent_ppo2.py:185][0m |          -0.0142 |           0.0288 |           0.2216 |
[32m[20221213 14:52:57 @agent_ppo2.py:185][0m |          -0.0085 |           0.0277 |           0.2215 |
[32m[20221213 14:52:58 @agent_ppo2.py:185][0m |          -0.0107 |           0.0269 |           0.2215 |
[32m[20221213 14:52:58 @agent_ppo2.py:185][0m |          -0.0132 |           0.0260 |           0.2214 |
[32m[20221213 14:52:58 @agent_ppo2.py:185][0m |          -0.0133 |           0.0253 |           0.2214 |
[32m[20221213 14:52:58 @agent_ppo2.py:185][0m |          -0.0106 |           0.0247 |           0.2213 |
[32m[20221213 14:52:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:52:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 22.88
[32m[20221213 14:52:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 24.22
[32m[20221213 14:52:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.38
[32m[20221213 14:52:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 22.38
[32m[20221213 14:52:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 22.38
[32m[20221213 14:52:58 @agent_ppo2.py:143][0m Total time:       0.05 min
[32m[20221213 14:52:58 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 14:52:58 @agent_ppo2.py:121][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 14:52:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:52:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:52:58 @agent_ppo2.py:185][0m |           0.0014 |           0.0788 |           0.2219 |
[32m[20221213 14:52:58 @agent_ppo2.py:185][0m |           0.0022 |           0.0621 |           0.2218 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0002 |           0.0594 |           0.2218 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0052 |           0.0579 |           0.2217 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0085 |           0.0563 |           0.2217 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0057 |           0.0547 |           0.2215 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0086 |           0.0527 |           0.2215 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0035 |           0.0515 |           0.2216 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0121 |           0.0494 |           0.2215 |
[32m[20221213 14:52:59 @agent_ppo2.py:185][0m |          -0.0074 |           0.0473 |           0.2215 |
[32m[20221213 14:52:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:52:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 27.51
[32m[20221213 14:52:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 34.56
[32m[20221213 14:52:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.87
[32m[20221213 14:52:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 22.87
[32m[20221213 14:52:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 22.87
[32m[20221213 14:52:59 @agent_ppo2.py:143][0m Total time:       0.07 min
[32m[20221213 14:52:59 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 14:52:59 @agent_ppo2.py:121][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 14:53:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |           0.0044 |           0.0708 |           0.2217 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0022 |           0.0607 |           0.2214 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0124 |           0.0601 |           0.2212 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0042 |           0.0590 |           0.2209 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0086 |           0.0582 |           0.2208 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |           0.0036 |           0.0595 |           0.2206 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0117 |           0.0576 |           0.2205 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0131 |           0.0559 |           0.2205 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0119 |           0.0554 |           0.2204 |
[32m[20221213 14:53:00 @agent_ppo2.py:185][0m |          -0.0131 |           0.0542 |           0.2204 |
[32m[20221213 14:53:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 24.57
[32m[20221213 14:53:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 26.28
[32m[20221213 14:53:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 36.34
[32m[20221213 14:53:01 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 36.34
[32m[20221213 14:53:01 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 36.34
[32m[20221213 14:53:01 @agent_ppo2.py:143][0m Total time:       0.09 min
[32m[20221213 14:53:01 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 14:53:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 14:53:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:01 @agent_ppo2.py:185][0m |           0.0025 |           0.0878 |           0.2192 |
[32m[20221213 14:53:01 @agent_ppo2.py:185][0m |          -0.0022 |           0.0762 |           0.2191 |
[32m[20221213 14:53:01 @agent_ppo2.py:185][0m |          -0.0090 |           0.0742 |           0.2188 |
[32m[20221213 14:53:01 @agent_ppo2.py:185][0m |          -0.0043 |           0.0725 |           0.2187 |
[32m[20221213 14:53:01 @agent_ppo2.py:185][0m |          -0.0125 |           0.0737 |           0.2185 |
[32m[20221213 14:53:01 @agent_ppo2.py:185][0m |          -0.0154 |           0.0720 |           0.2184 |
[32m[20221213 14:53:02 @agent_ppo2.py:185][0m |          -0.0081 |           0.0696 |           0.2182 |
[32m[20221213 14:53:02 @agent_ppo2.py:185][0m |          -0.0071 |           0.0704 |           0.2182 |
[32m[20221213 14:53:02 @agent_ppo2.py:185][0m |          -0.0124 |           0.0684 |           0.2180 |
[32m[20221213 14:53:02 @agent_ppo2.py:185][0m |          -0.0097 |           0.0677 |           0.2180 |
[32m[20221213 14:53:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:53:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 28.13
[32m[20221213 14:53:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 32.62
[32m[20221213 14:53:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.56
[32m[20221213 14:53:02 @agent_ppo2.py:143][0m Total time:       0.11 min
[32m[20221213 14:53:02 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 14:53:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 14:53:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:02 @agent_ppo2.py:185][0m |           0.0040 |           0.1280 |           0.2240 |
[32m[20221213 14:53:02 @agent_ppo2.py:185][0m |           0.0122 |           0.1317 |           0.2240 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0042 |           0.1184 |           0.2238 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0088 |           0.1120 |           0.2237 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0101 |           0.1105 |           0.2236 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0088 |           0.1101 |           0.2236 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0054 |           0.1099 |           0.2236 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0103 |           0.1082 |           0.2237 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0141 |           0.1090 |           0.2238 |
[32m[20221213 14:53:03 @agent_ppo2.py:185][0m |          -0.0138 |           0.1073 |           0.2239 |
[32m[20221213 14:53:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 28.13
[32m[20221213 14:53:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 37.22
[32m[20221213 14:53:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.62
[32m[20221213 14:53:03 @agent_ppo2.py:143][0m Total time:       0.14 min
[32m[20221213 14:53:03 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 14:53:03 @agent_ppo2.py:121][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 14:53:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0056 |           0.1381 |           0.2227 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0010 |           0.1212 |           0.2226 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0025 |           0.1175 |           0.2225 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0071 |           0.1153 |           0.2223 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0093 |           0.1181 |           0.2222 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0084 |           0.1123 |           0.2222 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0034 |           0.1106 |           0.2222 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0091 |           0.1114 |           0.2223 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0094 |           0.1085 |           0.2223 |
[32m[20221213 14:53:04 @agent_ppo2.py:185][0m |          -0.0103 |           0.1065 |           0.2223 |
[32m[20221213 14:53:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 31.04
[32m[20221213 14:53:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 34.29
[32m[20221213 14:53:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.80
[32m[20221213 14:53:05 @agent_ppo2.py:143][0m Total time:       0.16 min
[32m[20221213 14:53:05 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 14:53:05 @agent_ppo2.py:121][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 14:53:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:05 @agent_ppo2.py:185][0m |           0.0044 |           0.2663 |           0.2243 |
[32m[20221213 14:53:05 @agent_ppo2.py:185][0m |          -0.0063 |           0.2429 |           0.2243 |
[32m[20221213 14:53:05 @agent_ppo2.py:185][0m |          -0.0005 |           0.2359 |           0.2242 |
[32m[20221213 14:53:05 @agent_ppo2.py:185][0m |          -0.0057 |           0.2323 |           0.2241 |
[32m[20221213 14:53:05 @agent_ppo2.py:185][0m |          -0.0144 |           0.2290 |           0.2242 |
[32m[20221213 14:53:05 @agent_ppo2.py:185][0m |          -0.0090 |           0.2242 |           0.2242 |
[32m[20221213 14:53:06 @agent_ppo2.py:185][0m |          -0.0112 |           0.2282 |           0.2241 |
[32m[20221213 14:53:06 @agent_ppo2.py:185][0m |          -0.0116 |           0.2183 |           0.2241 |
[32m[20221213 14:53:06 @agent_ppo2.py:185][0m |          -0.0122 |           0.2171 |           0.2241 |
[32m[20221213 14:53:06 @agent_ppo2.py:185][0m |          -0.0067 |           0.2192 |           0.2241 |
[32m[20221213 14:53:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:53:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 35.72
[32m[20221213 14:53:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.95
[32m[20221213 14:53:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 32.21
[32m[20221213 14:53:06 @agent_ppo2.py:143][0m Total time:       0.18 min
[32m[20221213 14:53:06 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 14:53:06 @agent_ppo2.py:121][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 14:53:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:06 @agent_ppo2.py:185][0m |           0.0010 |           0.2796 |           0.2279 |
[32m[20221213 14:53:06 @agent_ppo2.py:185][0m |          -0.0061 |           0.2666 |           0.2278 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0130 |           0.2635 |           0.2276 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0107 |           0.2614 |           0.2274 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0086 |           0.2574 |           0.2273 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0124 |           0.2560 |           0.2272 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0166 |           0.2552 |           0.2272 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0158 |           0.2525 |           0.2273 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0189 |           0.2540 |           0.2272 |
[32m[20221213 14:53:07 @agent_ppo2.py:185][0m |          -0.0112 |           0.2502 |           0.2272 |
[32m[20221213 14:53:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 35.62
[32m[20221213 14:53:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 44.37
[32m[20221213 14:53:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 28.59
[32m[20221213 14:53:07 @agent_ppo2.py:143][0m Total time:       0.20 min
[32m[20221213 14:53:07 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 14:53:07 @agent_ppo2.py:121][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 14:53:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0012 |           0.1788 |           0.2208 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0052 |           0.1601 |           0.2203 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |           0.0026 |           0.1641 |           0.2198 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0128 |           0.1567 |           0.2199 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0106 |           0.1502 |           0.2198 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0134 |           0.1496 |           0.2198 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0135 |           0.1466 |           0.2198 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0191 |           0.1457 |           0.2198 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0156 |           0.1463 |           0.2199 |
[32m[20221213 14:53:08 @agent_ppo2.py:185][0m |          -0.0217 |           0.1451 |           0.2199 |
[32m[20221213 14:53:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:53:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 31.27
[32m[20221213 14:53:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 37.51
[32m[20221213 14:53:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.52
[32m[20221213 14:53:09 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 43.52
[32m[20221213 14:53:09 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 43.52
[32m[20221213 14:53:09 @agent_ppo2.py:143][0m Total time:       0.22 min
[32m[20221213 14:53:09 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 14:53:09 @agent_ppo2.py:121][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 14:53:09 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:09 @agent_ppo2.py:185][0m |           0.0016 |           0.2178 |           0.2234 |
[32m[20221213 14:53:09 @agent_ppo2.py:185][0m |           0.0007 |           0.2002 |           0.2232 |
[32m[20221213 14:53:09 @agent_ppo2.py:185][0m |          -0.0045 |           0.1972 |           0.2231 |
[32m[20221213 14:53:09 @agent_ppo2.py:185][0m |           0.0008 |           0.1940 |           0.2231 |
[32m[20221213 14:53:09 @agent_ppo2.py:185][0m |          -0.0022 |           0.1993 |           0.2232 |
[32m[20221213 14:53:09 @agent_ppo2.py:185][0m |          -0.0060 |           0.1909 |           0.2232 |
[32m[20221213 14:53:10 @agent_ppo2.py:185][0m |          -0.0104 |           0.1846 |           0.2232 |
[32m[20221213 14:53:10 @agent_ppo2.py:185][0m |          -0.0057 |           0.1908 |           0.2233 |
[32m[20221213 14:53:10 @agent_ppo2.py:185][0m |          -0.0109 |           0.1838 |           0.2234 |
[32m[20221213 14:53:10 @agent_ppo2.py:185][0m |          -0.0132 |           0.1796 |           0.2235 |
[32m[20221213 14:53:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:53:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 36.93
[32m[20221213 14:53:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 43.68
[32m[20221213 14:53:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 30.66
[32m[20221213 14:53:10 @agent_ppo2.py:143][0m Total time:       0.25 min
[32m[20221213 14:53:10 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 14:53:10 @agent_ppo2.py:121][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 14:53:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:10 @agent_ppo2.py:185][0m |          -0.0005 |           0.2357 |           0.2282 |
[32m[20221213 14:53:10 @agent_ppo2.py:185][0m |          -0.0010 |           0.2168 |           0.2279 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0082 |           0.2064 |           0.2275 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0043 |           0.2057 |           0.2274 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0106 |           0.2020 |           0.2274 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0110 |           0.1998 |           0.2273 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0108 |           0.1973 |           0.2273 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0113 |           0.1963 |           0.2273 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0083 |           0.1945 |           0.2272 |
[32m[20221213 14:53:11 @agent_ppo2.py:185][0m |          -0.0143 |           0.1939 |           0.2273 |
[32m[20221213 14:53:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:53:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.96
[32m[20221213 14:53:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.49
[32m[20221213 14:53:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.95
[32m[20221213 14:53:11 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 43.95
[32m[20221213 14:53:11 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 43.95
[32m[20221213 14:53:11 @agent_ppo2.py:143][0m Total time:       0.27 min
[32m[20221213 14:53:11 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 14:53:11 @agent_ppo2.py:121][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 14:53:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |           0.0012 |           0.2764 |           0.2216 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0058 |           0.2661 |           0.2213 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0021 |           0.2665 |           0.2211 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0073 |           0.2628 |           0.2208 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0039 |           0.2623 |           0.2206 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0101 |           0.2567 |           0.2207 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0071 |           0.2575 |           0.2207 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0104 |           0.2547 |           0.2206 |
[32m[20221213 14:53:12 @agent_ppo2.py:185][0m |          -0.0104 |           0.2539 |           0.2207 |
[32m[20221213 14:53:13 @agent_ppo2.py:185][0m |          -0.0122 |           0.2539 |           0.2207 |
[32m[20221213 14:53:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:53:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.32
[32m[20221213 14:53:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.43
[32m[20221213 14:53:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.67
[32m[20221213 14:53:13 @agent_ppo2.py:143][0m Total time:       0.29 min
[32m[20221213 14:53:13 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 14:53:13 @agent_ppo2.py:121][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 14:53:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:13 @agent_ppo2.py:185][0m |          -0.0034 |           0.2360 |           0.2286 |
[32m[20221213 14:53:13 @agent_ppo2.py:185][0m |           0.0088 |           0.2345 |           0.2284 |
[32m[20221213 14:53:13 @agent_ppo2.py:185][0m |          -0.0001 |           0.2275 |           0.2284 |
[32m[20221213 14:53:13 @agent_ppo2.py:185][0m |          -0.0117 |           0.2172 |           0.2286 |
[32m[20221213 14:53:13 @agent_ppo2.py:185][0m |          -0.0104 |           0.2157 |           0.2287 |
[32m[20221213 14:53:14 @agent_ppo2.py:185][0m |          -0.0089 |           0.2135 |           0.2288 |
[32m[20221213 14:53:14 @agent_ppo2.py:185][0m |          -0.0127 |           0.2136 |           0.2289 |
[32m[20221213 14:53:14 @agent_ppo2.py:185][0m |          -0.0110 |           0.2115 |           0.2290 |
[32m[20221213 14:53:14 @agent_ppo2.py:185][0m |          -0.0158 |           0.2103 |           0.2290 |
[32m[20221213 14:53:14 @agent_ppo2.py:185][0m |          -0.0131 |           0.2102 |           0.2293 |
[32m[20221213 14:53:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:53:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 42.85
[32m[20221213 14:53:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 44.84
[32m[20221213 14:53:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.17
[32m[20221213 14:53:14 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 44.17
[32m[20221213 14:53:14 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 44.17
[32m[20221213 14:53:14 @agent_ppo2.py:143][0m Total time:       0.32 min
[32m[20221213 14:53:14 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 14:53:14 @agent_ppo2.py:121][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 14:53:14 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 14:53:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |           0.0177 |           0.2207 |           0.2303 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0033 |           0.1869 |           0.2300 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0057 |           0.1826 |           0.2297 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0030 |           0.1832 |           0.2298 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0092 |           0.1797 |           0.2297 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0016 |           0.1818 |           0.2297 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0052 |           0.1797 |           0.2298 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0122 |           0.1769 |           0.2296 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0080 |           0.1764 |           0.2297 |
[32m[20221213 14:53:15 @agent_ppo2.py:185][0m |          -0.0102 |           0.1754 |           0.2297 |
[32m[20221213 14:53:15 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 14:53:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 38.92
[32m[20221213 14:53:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 42.99
[32m[20221213 14:53:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.40
[32m[20221213 14:53:16 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 44.40
[32m[20221213 14:53:16 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 44.40
[32m[20221213 14:53:16 @agent_ppo2.py:143][0m Total time:       0.34 min
[32m[20221213 14:53:16 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 14:53:16 @agent_ppo2.py:121][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 14:53:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:16 @agent_ppo2.py:185][0m |           0.0040 |           0.3062 |           0.2307 |
[32m[20221213 14:53:16 @agent_ppo2.py:185][0m |          -0.0029 |           0.2826 |           0.2308 |
[32m[20221213 14:53:16 @agent_ppo2.py:185][0m |          -0.0040 |           0.2776 |           0.2308 |
[32m[20221213 14:53:16 @agent_ppo2.py:185][0m |           0.0026 |           0.2814 |           0.2308 |
[32m[20221213 14:53:16 @agent_ppo2.py:185][0m |          -0.0078 |           0.2765 |           0.2308 |
[32m[20221213 14:53:16 @agent_ppo2.py:185][0m |           0.0045 |           0.2900 |           0.2309 |
[32m[20221213 14:53:17 @agent_ppo2.py:185][0m |          -0.0051 |           0.2734 |           0.2310 |
[32m[20221213 14:53:17 @agent_ppo2.py:185][0m |          -0.0092 |           0.2747 |           0.2311 |
[32m[20221213 14:53:17 @agent_ppo2.py:185][0m |          -0.0099 |           0.2710 |           0.2311 |
[32m[20221213 14:53:17 @agent_ppo2.py:185][0m |          -0.0057 |           0.2706 |           0.2312 |
[32m[20221213 14:53:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 14:53:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 44.33
[32m[20221213 14:53:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.31
[32m[20221213 14:53:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.17
[32m[20221213 14:53:17 @agent_ppo2.py:143][0m Total time:       0.36 min
[32m[20221213 14:53:17 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 14:53:17 @agent_ppo2.py:121][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 14:53:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:17 @agent_ppo2.py:185][0m |          -0.0033 |           0.2934 |           0.2324 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0058 |           0.2788 |           0.2323 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0096 |           0.2762 |           0.2322 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0099 |           0.2731 |           0.2321 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0142 |           0.2731 |           0.2321 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0067 |           0.2957 |           0.2321 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0176 |           0.2738 |           0.2320 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0109 |           0.2686 |           0.2321 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0128 |           0.2694 |           0.2322 |
[32m[20221213 14:53:18 @agent_ppo2.py:185][0m |          -0.0119 |           0.2639 |           0.2321 |
[32m[20221213 14:53:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:53:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.52
[32m[20221213 14:53:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 43.08
[32m[20221213 14:53:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.41
[32m[20221213 14:53:18 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 44.41
[32m[20221213 14:53:18 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 44.41
[32m[20221213 14:53:18 @agent_ppo2.py:143][0m Total time:       0.39 min
[32m[20221213 14:53:18 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 14:53:18 @agent_ppo2.py:121][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 14:53:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0001 |           0.2728 |           0.2330 |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0066 |           0.2615 |           0.2322 |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0095 |           0.2601 |           0.2321 |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0115 |           0.2588 |           0.2323 |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0057 |           0.2652 |           0.2325 |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0135 |           0.2591 |           0.2325 |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0145 |           0.2554 |           0.2328 |
[32m[20221213 14:53:19 @agent_ppo2.py:185][0m |          -0.0146 |           0.2553 |           0.2329 |
[32m[20221213 14:53:20 @agent_ppo2.py:185][0m |          -0.0134 |           0.2542 |           0.2332 |
[32m[20221213 14:53:20 @agent_ppo2.py:185][0m |          -0.0132 |           0.2547 |           0.2333 |
[32m[20221213 14:53:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 40.82
[32m[20221213 14:53:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 43.15
[32m[20221213 14:53:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.94
[32m[20221213 14:53:20 @agent_ppo2.py:143][0m Total time:       0.41 min
[32m[20221213 14:53:20 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 14:53:20 @agent_ppo2.py:121][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 14:53:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:20 @agent_ppo2.py:185][0m |           0.0063 |           0.4373 |           0.2434 |
[32m[20221213 14:53:20 @agent_ppo2.py:185][0m |          -0.0074 |           0.3970 |           0.2431 |
[32m[20221213 14:53:20 @agent_ppo2.py:185][0m |          -0.0083 |           0.3891 |           0.2431 |
[32m[20221213 14:53:20 @agent_ppo2.py:185][0m |          -0.0092 |           0.3860 |           0.2431 |
[32m[20221213 14:53:20 @agent_ppo2.py:185][0m |          -0.0119 |           0.3819 |           0.2431 |
[32m[20221213 14:53:21 @agent_ppo2.py:185][0m |          -0.0106 |           0.3801 |           0.2431 |
[32m[20221213 14:53:21 @agent_ppo2.py:185][0m |          -0.0118 |           0.3793 |           0.2430 |
[32m[20221213 14:53:21 @agent_ppo2.py:185][0m |          -0.0068 |           0.3786 |           0.2431 |
[32m[20221213 14:53:21 @agent_ppo2.py:185][0m |          -0.0148 |           0.3771 |           0.2430 |
[32m[20221213 14:53:21 @agent_ppo2.py:185][0m |          -0.0122 |           0.3873 |           0.2431 |
[32m[20221213 14:53:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:53:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.47
[32m[20221213 14:53:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.43
[32m[20221213 14:53:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.87
[32m[20221213 14:53:21 @agent_ppo2.py:143][0m Total time:       0.43 min
[32m[20221213 14:53:21 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 14:53:21 @agent_ppo2.py:121][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 14:53:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:21 @agent_ppo2.py:185][0m |          -0.0036 |           0.2947 |           0.2423 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0072 |           0.2831 |           0.2424 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0018 |           0.2816 |           0.2425 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0071 |           0.2867 |           0.2428 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0120 |           0.2767 |           0.2429 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0128 |           0.2737 |           0.2430 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0132 |           0.2740 |           0.2432 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0135 |           0.2719 |           0.2435 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0142 |           0.2719 |           0.2435 |
[32m[20221213 14:53:22 @agent_ppo2.py:185][0m |          -0.0110 |           0.2715 |           0.2436 |
[32m[20221213 14:53:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 43.80
[32m[20221213 14:53:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.06
[32m[20221213 14:53:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 24.50
[32m[20221213 14:53:22 @agent_ppo2.py:143][0m Total time:       0.45 min
[32m[20221213 14:53:22 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 14:53:22 @agent_ppo2.py:121][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 14:53:23 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:53:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |           0.0012 |           0.3151 |           0.2508 |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |           0.0078 |           0.3338 |           0.2508 |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |           0.0112 |           0.3393 |           0.2505 |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |          -0.0042 |           0.3028 |           0.2501 |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |          -0.0056 |           0.3016 |           0.2501 |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |          -0.0074 |           0.3021 |           0.2500 |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |          -0.0078 |           0.2998 |           0.2501 |
[32m[20221213 14:53:23 @agent_ppo2.py:185][0m |          -0.0090 |           0.2997 |           0.2500 |
[32m[20221213 14:53:24 @agent_ppo2.py:185][0m |           0.0038 |           0.3339 |           0.2499 |
[32m[20221213 14:53:24 @agent_ppo2.py:185][0m |          -0.0095 |           0.3019 |           0.2499 |
[32m[20221213 14:53:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.03
[32m[20221213 14:53:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.51
[32m[20221213 14:53:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 38.00
[32m[20221213 14:53:24 @agent_ppo2.py:143][0m Total time:       0.48 min
[32m[20221213 14:53:24 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 14:53:24 @agent_ppo2.py:121][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 14:53:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:24 @agent_ppo2.py:185][0m |          -0.0050 |           0.3890 |           0.2431 |
[32m[20221213 14:53:24 @agent_ppo2.py:185][0m |          -0.0041 |           0.3727 |           0.2430 |
[32m[20221213 14:53:24 @agent_ppo2.py:185][0m |          -0.0058 |           0.3696 |           0.2425 |
[32m[20221213 14:53:24 @agent_ppo2.py:185][0m |          -0.0078 |           0.3631 |           0.2421 |
[32m[20221213 14:53:25 @agent_ppo2.py:185][0m |          -0.0078 |           0.3618 |           0.2419 |
[32m[20221213 14:53:25 @agent_ppo2.py:185][0m |          -0.0100 |           0.3565 |           0.2417 |
[32m[20221213 14:53:25 @agent_ppo2.py:185][0m |          -0.0093 |           0.3579 |           0.2416 |
[32m[20221213 14:53:25 @agent_ppo2.py:185][0m |          -0.0110 |           0.3546 |           0.2414 |
[32m[20221213 14:53:25 @agent_ppo2.py:185][0m |          -0.0098 |           0.3518 |           0.2412 |
[32m[20221213 14:53:25 @agent_ppo2.py:185][0m |          -0.0116 |           0.3515 |           0.2412 |
[32m[20221213 14:53:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.51
[32m[20221213 14:53:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.86
[32m[20221213 14:53:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 25.07
[32m[20221213 14:53:25 @agent_ppo2.py:143][0m Total time:       0.50 min
[32m[20221213 14:53:25 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 14:53:25 @agent_ppo2.py:121][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 14:53:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0049 |           0.4558 |           0.2461 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0046 |           0.4336 |           0.2459 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0060 |           0.4282 |           0.2455 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0043 |           0.4244 |           0.2452 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0084 |           0.4223 |           0.2451 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0036 |           0.4224 |           0.2448 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0105 |           0.4223 |           0.2447 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0079 |           0.4166 |           0.2448 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0106 |           0.4164 |           0.2447 |
[32m[20221213 14:53:26 @agent_ppo2.py:185][0m |          -0.0107 |           0.4168 |           0.2446 |
[32m[20221213 14:53:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:53:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.14
[32m[20221213 14:53:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.51
[32m[20221213 14:53:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.49
[32m[20221213 14:53:26 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 44.49
[32m[20221213 14:53:26 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 44.49
[32m[20221213 14:53:26 @agent_ppo2.py:143][0m Total time:       0.52 min
[32m[20221213 14:53:26 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 14:53:26 @agent_ppo2.py:121][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 14:53:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |          -0.0000 |           0.3754 |           0.2356 |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |          -0.0017 |           0.3679 |           0.2351 |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |           0.0043 |           0.3903 |           0.2349 |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |          -0.0072 |           0.3651 |           0.2344 |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |          -0.0071 |           0.3608 |           0.2345 |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |          -0.0077 |           0.3608 |           0.2345 |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |          -0.0082 |           0.3589 |           0.2342 |
[32m[20221213 14:53:27 @agent_ppo2.py:185][0m |          -0.0097 |           0.3575 |           0.2341 |
[32m[20221213 14:53:28 @agent_ppo2.py:185][0m |          -0.0069 |           0.3584 |           0.2341 |
[32m[20221213 14:53:28 @agent_ppo2.py:185][0m |          -0.0095 |           0.3568 |           0.2339 |
[32m[20221213 14:53:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:53:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.01
[32m[20221213 14:53:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 45.67
[32m[20221213 14:53:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.97
[32m[20221213 14:53:28 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 44.97
[32m[20221213 14:53:28 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 44.97
[32m[20221213 14:53:28 @agent_ppo2.py:143][0m Total time:       0.54 min
[32m[20221213 14:53:28 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 14:53:28 @agent_ppo2.py:121][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 14:53:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:28 @agent_ppo2.py:185][0m |          -0.0009 |           0.3764 |           0.2381 |
[32m[20221213 14:53:28 @agent_ppo2.py:185][0m |          -0.0032 |           0.3650 |           0.2380 |
[32m[20221213 14:53:28 @agent_ppo2.py:185][0m |          -0.0046 |           0.3630 |           0.2379 |
[32m[20221213 14:53:28 @agent_ppo2.py:185][0m |          -0.0028 |           0.3620 |           0.2377 |
[32m[20221213 14:53:29 @agent_ppo2.py:185][0m |          -0.0023 |           0.3648 |           0.2375 |
[32m[20221213 14:53:29 @agent_ppo2.py:185][0m |          -0.0091 |           0.3597 |           0.2374 |
[32m[20221213 14:53:29 @agent_ppo2.py:185][0m |          -0.0096 |           0.3607 |           0.2374 |
[32m[20221213 14:53:29 @agent_ppo2.py:185][0m |          -0.0080 |           0.3583 |           0.2373 |
[32m[20221213 14:53:29 @agent_ppo2.py:185][0m |          -0.0069 |           0.3577 |           0.2372 |
[32m[20221213 14:53:29 @agent_ppo2.py:185][0m |          -0.0057 |           0.3616 |           0.2373 |
[32m[20221213 14:53:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:53:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 45.17
[32m[20221213 14:53:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 46.55
[32m[20221213 14:53:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.31
[32m[20221213 14:53:29 @agent_ppo2.py:143][0m Total time:       0.57 min
[32m[20221213 14:53:29 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 14:53:29 @agent_ppo2.py:121][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 14:53:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0012 |           0.4266 |           0.2450 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0042 |           0.4186 |           0.2449 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0078 |           0.4156 |           0.2448 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0092 |           0.4133 |           0.2447 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0084 |           0.4127 |           0.2448 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0090 |           0.4120 |           0.2450 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0003 |           0.4392 |           0.2449 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0112 |           0.4144 |           0.2450 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0103 |           0.4107 |           0.2451 |
[32m[20221213 14:53:30 @agent_ppo2.py:185][0m |          -0.0029 |           0.4235 |           0.2453 |
[32m[20221213 14:53:30 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221213 14:53:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.17
[32m[20221213 14:53:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.25
[32m[20221213 14:53:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 42.61
[32m[20221213 14:53:31 @agent_ppo2.py:143][0m Total time:       0.59 min
[32m[20221213 14:53:31 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 14:53:31 @agent_ppo2.py:121][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 14:53:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |          -0.0017 |           0.4833 |           0.2404 |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |          -0.0059 |           0.4726 |           0.2405 |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |          -0.0055 |           0.4653 |           0.2406 |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |          -0.0104 |           0.4669 |           0.2409 |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |          -0.0094 |           0.4639 |           0.2411 |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |           0.0048 |           0.5213 |           0.2411 |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |          -0.0105 |           0.4669 |           0.2410 |
[32m[20221213 14:53:31 @agent_ppo2.py:185][0m |          -0.0105 |           0.4593 |           0.2412 |
[32m[20221213 14:53:32 @agent_ppo2.py:185][0m |          -0.0104 |           0.4604 |           0.2413 |
[32m[20221213 14:53:32 @agent_ppo2.py:185][0m |          -0.0109 |           0.4599 |           0.2414 |
[32m[20221213 14:53:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:53:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.75
[32m[20221213 14:53:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.66
[32m[20221213 14:53:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.19
[32m[20221213 14:53:32 @agent_ppo2.py:143][0m Total time:       0.61 min
[32m[20221213 14:53:32 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 14:53:32 @agent_ppo2.py:121][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 14:53:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:32 @agent_ppo2.py:185][0m |           0.0006 |           0.5069 |           0.2415 |
[32m[20221213 14:53:32 @agent_ppo2.py:185][0m |           0.0009 |           0.5029 |           0.2414 |
[32m[20221213 14:53:32 @agent_ppo2.py:185][0m |          -0.0050 |           0.4955 |           0.2413 |
[32m[20221213 14:53:33 @agent_ppo2.py:185][0m |          -0.0006 |           0.5128 |           0.2412 |
[32m[20221213 14:53:33 @agent_ppo2.py:185][0m |          -0.0104 |           0.4939 |           0.2411 |
[32m[20221213 14:53:33 @agent_ppo2.py:185][0m |           0.0002 |           0.5307 |           0.2410 |
[32m[20221213 14:53:33 @agent_ppo2.py:185][0m |          -0.0091 |           0.4903 |           0.2409 |
[32m[20221213 14:53:33 @agent_ppo2.py:185][0m |          -0.0141 |           0.4922 |           0.2408 |
[32m[20221213 14:53:33 @agent_ppo2.py:185][0m |          -0.0102 |           0.4842 |           0.2409 |
[32m[20221213 14:53:33 @agent_ppo2.py:185][0m |          -0.0098 |           0.4846 |           0.2408 |
[32m[20221213 14:53:33 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 14:53:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.78
[32m[20221213 14:53:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.61
[32m[20221213 14:53:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 30.18
[32m[20221213 14:53:33 @agent_ppo2.py:143][0m Total time:       0.64 min
[32m[20221213 14:53:33 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 14:53:33 @agent_ppo2.py:121][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 14:53:34 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:53:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:34 @agent_ppo2.py:185][0m |          -0.0025 |           0.4248 |           0.2404 |
[32m[20221213 14:53:34 @agent_ppo2.py:185][0m |           0.0005 |           0.4281 |           0.2404 |
[32m[20221213 14:53:34 @agent_ppo2.py:185][0m |          -0.0030 |           0.4221 |           0.2401 |
[32m[20221213 14:53:34 @agent_ppo2.py:185][0m |          -0.0052 |           0.4207 |           0.2401 |
[32m[20221213 14:53:34 @agent_ppo2.py:185][0m |          -0.0088 |           0.4164 |           0.2399 |
[32m[20221213 14:53:35 @agent_ppo2.py:185][0m |          -0.0087 |           0.4154 |           0.2399 |
[32m[20221213 14:53:35 @agent_ppo2.py:185][0m |          -0.0006 |           0.4275 |           0.2399 |
[32m[20221213 14:53:35 @agent_ppo2.py:185][0m |          -0.0096 |           0.4177 |           0.2397 |
[32m[20221213 14:53:35 @agent_ppo2.py:185][0m |          -0.0091 |           0.4140 |           0.2397 |
[32m[20221213 14:53:35 @agent_ppo2.py:185][0m |          -0.0088 |           0.4152 |           0.2398 |
[32m[20221213 14:53:35 @agent_ppo2.py:130][0m Policy update time: 1.35 s
[32m[20221213 14:53:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.49
[32m[20221213 14:53:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.94
[32m[20221213 14:53:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.36
[32m[20221213 14:53:35 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 45.36
[32m[20221213 14:53:35 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 45.36
[32m[20221213 14:53:35 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221213 14:53:35 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 14:53:35 @agent_ppo2.py:121][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 14:53:35 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |          -0.0005 |           0.4625 |           0.2433 |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |           0.0091 |           0.5036 |           0.2431 |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |          -0.0041 |           0.4451 |           0.2428 |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |          -0.0065 |           0.4423 |           0.2429 |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |          -0.0019 |           0.4550 |           0.2428 |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |          -0.0086 |           0.4410 |           0.2428 |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |          -0.0073 |           0.4389 |           0.2427 |
[32m[20221213 14:53:36 @agent_ppo2.py:185][0m |          -0.0095 |           0.4423 |           0.2428 |
[32m[20221213 14:53:37 @agent_ppo2.py:185][0m |          -0.0085 |           0.4390 |           0.2428 |
[32m[20221213 14:53:37 @agent_ppo2.py:185][0m |          -0.0085 |           0.4383 |           0.2428 |
[32m[20221213 14:53:37 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 14:53:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.37
[32m[20221213 14:53:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.24
[32m[20221213 14:53:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 29.68
[32m[20221213 14:53:37 @agent_ppo2.py:143][0m Total time:       0.70 min
[32m[20221213 14:53:37 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 14:53:37 @agent_ppo2.py:121][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 14:53:37 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 14:53:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:37 @agent_ppo2.py:185][0m |          -0.0018 |           0.5070 |           0.2443 |
[32m[20221213 14:53:37 @agent_ppo2.py:185][0m |          -0.0047 |           0.4926 |           0.2442 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0075 |           0.4845 |           0.2441 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0113 |           0.4813 |           0.2440 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0090 |           0.4803 |           0.2438 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0079 |           0.4783 |           0.2439 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0144 |           0.4756 |           0.2439 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0142 |           0.4742 |           0.2440 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0134 |           0.4733 |           0.2439 |
[32m[20221213 14:53:38 @agent_ppo2.py:185][0m |          -0.0138 |           0.4741 |           0.2439 |
[32m[20221213 14:53:38 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 14:53:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.97
[32m[20221213 14:53:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.11
[32m[20221213 14:53:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.96
[32m[20221213 14:53:39 @agent_ppo2.py:143][0m Total time:       0.72 min
[32m[20221213 14:53:39 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 14:53:39 @agent_ppo2.py:121][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 14:53:39 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:39 @agent_ppo2.py:185][0m |          -0.0021 |           0.4515 |           0.2474 |
[32m[20221213 14:53:39 @agent_ppo2.py:185][0m |           0.0048 |           0.4841 |           0.2472 |
[32m[20221213 14:53:39 @agent_ppo2.py:185][0m |          -0.0083 |           0.4457 |           0.2467 |
[32m[20221213 14:53:39 @agent_ppo2.py:185][0m |          -0.0035 |           0.4476 |           0.2465 |
[32m[20221213 14:53:39 @agent_ppo2.py:185][0m |           0.0045 |           0.4951 |           0.2463 |
[32m[20221213 14:53:40 @agent_ppo2.py:185][0m |          -0.0085 |           0.4417 |           0.2463 |
[32m[20221213 14:53:40 @agent_ppo2.py:185][0m |          -0.0107 |           0.4382 |           0.2461 |
[32m[20221213 14:53:40 @agent_ppo2.py:185][0m |          -0.0101 |           0.4395 |           0.2459 |
[32m[20221213 14:53:40 @agent_ppo2.py:185][0m |          -0.0112 |           0.4406 |           0.2459 |
[32m[20221213 14:53:40 @agent_ppo2.py:185][0m |           0.0014 |           0.4706 |           0.2458 |
[32m[20221213 14:53:40 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 14:53:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.38
[32m[20221213 14:53:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 48.69
[32m[20221213 14:53:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.57
[32m[20221213 14:53:40 @agent_ppo2.py:143][0m Total time:       0.75 min
[32m[20221213 14:53:40 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 14:53:40 @agent_ppo2.py:121][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 14:53:40 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0028 |           0.4723 |           0.2360 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0049 |           0.4594 |           0.2362 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0070 |           0.4577 |           0.2360 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0055 |           0.4583 |           0.2358 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0088 |           0.4549 |           0.2358 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0089 |           0.4548 |           0.2360 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0107 |           0.4550 |           0.2359 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0092 |           0.4532 |           0.2359 |
[32m[20221213 14:53:41 @agent_ppo2.py:185][0m |          -0.0103 |           0.4529 |           0.2358 |
[32m[20221213 14:53:42 @agent_ppo2.py:185][0m |          -0.0090 |           0.4549 |           0.2360 |
[32m[20221213 14:53:42 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 14:53:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.44
[32m[20221213 14:53:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.80
[32m[20221213 14:53:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.31
[32m[20221213 14:53:42 @agent_ppo2.py:143][0m Total time:       0.78 min
[32m[20221213 14:53:42 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 14:53:42 @agent_ppo2.py:121][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 14:53:42 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:42 @agent_ppo2.py:185][0m |          -0.0015 |           0.4688 |           0.2476 |
[32m[20221213 14:53:42 @agent_ppo2.py:185][0m |          -0.0040 |           0.4664 |           0.2476 |
[32m[20221213 14:53:42 @agent_ppo2.py:185][0m |          -0.0040 |           0.4670 |           0.2472 |
[32m[20221213 14:53:43 @agent_ppo2.py:185][0m |          -0.0034 |           0.4686 |           0.2469 |
[32m[20221213 14:53:43 @agent_ppo2.py:185][0m |          -0.0058 |           0.4658 |           0.2466 |
[32m[20221213 14:53:43 @agent_ppo2.py:185][0m |          -0.0101 |           0.4653 |           0.2462 |
[32m[20221213 14:53:43 @agent_ppo2.py:185][0m |          -0.0019 |           0.4693 |           0.2462 |
[32m[20221213 14:53:43 @agent_ppo2.py:185][0m |          -0.0017 |           0.4897 |           0.2459 |
[32m[20221213 14:53:43 @agent_ppo2.py:185][0m |          -0.0114 |           0.4615 |           0.2458 |
[32m[20221213 14:53:43 @agent_ppo2.py:185][0m |          -0.0008 |           0.4865 |           0.2456 |
[32m[20221213 14:53:43 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 14:53:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.77
[32m[20221213 14:53:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.35
[32m[20221213 14:53:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.40
[32m[20221213 14:53:43 @agent_ppo2.py:143][0m Total time:       0.80 min
[32m[20221213 14:53:43 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 14:53:43 @agent_ppo2.py:121][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 14:53:44 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:44 @agent_ppo2.py:185][0m |          -0.0006 |           0.5332 |           0.2436 |
[32m[20221213 14:53:44 @agent_ppo2.py:185][0m |          -0.0090 |           0.5234 |           0.2435 |
[32m[20221213 14:53:44 @agent_ppo2.py:185][0m |          -0.0068 |           0.5189 |           0.2435 |
[32m[20221213 14:53:44 @agent_ppo2.py:185][0m |          -0.0080 |           0.5159 |           0.2433 |
[32m[20221213 14:53:44 @agent_ppo2.py:185][0m |          -0.0022 |           0.5264 |           0.2434 |
[32m[20221213 14:53:44 @agent_ppo2.py:185][0m |          -0.0092 |           0.5095 |           0.2432 |
[32m[20221213 14:53:44 @agent_ppo2.py:185][0m |          -0.0106 |           0.5143 |           0.2432 |
[32m[20221213 14:53:45 @agent_ppo2.py:185][0m |          -0.0120 |           0.5082 |           0.2431 |
[32m[20221213 14:53:45 @agent_ppo2.py:185][0m |          -0.0126 |           0.5082 |           0.2431 |
[32m[20221213 14:53:45 @agent_ppo2.py:185][0m |          -0.0138 |           0.5086 |           0.2431 |
[32m[20221213 14:53:45 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 14:53:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.42
[32m[20221213 14:53:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.98
[32m[20221213 14:53:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 31.09
[32m[20221213 14:53:45 @agent_ppo2.py:143][0m Total time:       0.83 min
[32m[20221213 14:53:45 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 14:53:45 @agent_ppo2.py:121][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 14:53:45 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:45 @agent_ppo2.py:185][0m |          -0.0029 |           0.4933 |           0.2369 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0012 |           0.4913 |           0.2367 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0059 |           0.4873 |           0.2367 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0086 |           0.4882 |           0.2365 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0069 |           0.4886 |           0.2363 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0086 |           0.4882 |           0.2362 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0090 |           0.4858 |           0.2360 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0043 |           0.4991 |           0.2360 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0095 |           0.4846 |           0.2358 |
[32m[20221213 14:53:46 @agent_ppo2.py:185][0m |          -0.0114 |           0.4841 |           0.2357 |
[32m[20221213 14:53:46 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 14:53:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.80
[32m[20221213 14:53:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.74
[32m[20221213 14:53:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 28.13
[32m[20221213 14:53:47 @agent_ppo2.py:143][0m Total time:       0.86 min
[32m[20221213 14:53:47 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 14:53:47 @agent_ppo2.py:121][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 14:53:47 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:47 @agent_ppo2.py:185][0m |           0.0086 |           0.5221 |           0.2373 |
[32m[20221213 14:53:47 @agent_ppo2.py:185][0m |          -0.0060 |           0.5009 |           0.2371 |
[32m[20221213 14:53:47 @agent_ppo2.py:185][0m |          -0.0064 |           0.4922 |           0.2371 |
[32m[20221213 14:53:47 @agent_ppo2.py:185][0m |          -0.0082 |           0.4914 |           0.2370 |
[32m[20221213 14:53:47 @agent_ppo2.py:185][0m |          -0.0096 |           0.4884 |           0.2370 |
[32m[20221213 14:53:48 @agent_ppo2.py:185][0m |          -0.0043 |           0.4944 |           0.2369 |
[32m[20221213 14:53:48 @agent_ppo2.py:185][0m |          -0.0113 |           0.4885 |           0.2368 |
[32m[20221213 14:53:48 @agent_ppo2.py:185][0m |          -0.0118 |           0.4895 |           0.2368 |
[32m[20221213 14:53:48 @agent_ppo2.py:185][0m |          -0.0123 |           0.4869 |           0.2367 |
[32m[20221213 14:53:48 @agent_ppo2.py:185][0m |          -0.0051 |           0.4965 |           0.2366 |
[32m[20221213 14:53:48 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 14:53:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 46.91
[32m[20221213 14:53:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.87
[32m[20221213 14:53:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.30
[32m[20221213 14:53:48 @agent_ppo2.py:143][0m Total time:       0.88 min
[32m[20221213 14:53:48 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 14:53:48 @agent_ppo2.py:121][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 14:53:48 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:53:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0026 |           0.5258 |           0.2347 |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0048 |           0.5131 |           0.2346 |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0053 |           0.5050 |           0.2346 |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0060 |           0.5047 |           0.2344 |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0060 |           0.5033 |           0.2343 |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0080 |           0.4998 |           0.2344 |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0091 |           0.5027 |           0.2345 |
[32m[20221213 14:53:49 @agent_ppo2.py:185][0m |          -0.0061 |           0.5193 |           0.2344 |
[32m[20221213 14:53:50 @agent_ppo2.py:185][0m |          -0.0115 |           0.5016 |           0.2344 |
[32m[20221213 14:53:50 @agent_ppo2.py:185][0m |          -0.0115 |           0.4980 |           0.2347 |
[32m[20221213 14:53:50 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 14:53:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.56
[32m[20221213 14:53:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.93
[32m[20221213 14:53:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 27.57
[32m[20221213 14:53:50 @agent_ppo2.py:143][0m Total time:       0.91 min
[32m[20221213 14:53:50 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 14:53:50 @agent_ppo2.py:121][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 14:53:50 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:50 @agent_ppo2.py:185][0m |          -0.0008 |           0.5308 |           0.2404 |
[32m[20221213 14:53:50 @agent_ppo2.py:185][0m |          -0.0056 |           0.5230 |           0.2402 |
[32m[20221213 14:53:50 @agent_ppo2.py:185][0m |          -0.0075 |           0.5230 |           0.2399 |
[32m[20221213 14:53:51 @agent_ppo2.py:185][0m |          -0.0081 |           0.5202 |           0.2397 |
[32m[20221213 14:53:51 @agent_ppo2.py:185][0m |          -0.0124 |           0.5204 |           0.2394 |
[32m[20221213 14:53:51 @agent_ppo2.py:185][0m |          -0.0105 |           0.5253 |           0.2392 |
[32m[20221213 14:53:51 @agent_ppo2.py:185][0m |          -0.0099 |           0.5189 |           0.2391 |
[32m[20221213 14:53:51 @agent_ppo2.py:185][0m |          -0.0059 |           0.5253 |           0.2390 |
[32m[20221213 14:53:51 @agent_ppo2.py:185][0m |          -0.0148 |           0.5221 |           0.2388 |
[32m[20221213 14:53:51 @agent_ppo2.py:185][0m |          -0.0104 |           0.5175 |           0.2388 |
[32m[20221213 14:53:51 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 14:53:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.48
[32m[20221213 14:53:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.77
[32m[20221213 14:53:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.06
[32m[20221213 14:53:51 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 54.06
[32m[20221213 14:53:51 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 54.06
[32m[20221213 14:53:51 @agent_ppo2.py:143][0m Total time:       0.94 min
[32m[20221213 14:53:51 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 14:53:51 @agent_ppo2.py:121][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 14:53:52 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:53:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:52 @agent_ppo2.py:185][0m |          -0.0001 |           0.5310 |           0.2348 |
[32m[20221213 14:53:52 @agent_ppo2.py:185][0m |           0.0022 |           0.5309 |           0.2346 |
[32m[20221213 14:53:52 @agent_ppo2.py:185][0m |          -0.0058 |           0.5241 |           0.2345 |
[32m[20221213 14:53:52 @agent_ppo2.py:185][0m |          -0.0038 |           0.5222 |           0.2344 |
[32m[20221213 14:53:52 @agent_ppo2.py:185][0m |          -0.0067 |           0.5231 |           0.2343 |
[32m[20221213 14:53:52 @agent_ppo2.py:185][0m |          -0.0090 |           0.5217 |           0.2343 |
[32m[20221213 14:53:53 @agent_ppo2.py:185][0m |          -0.0069 |           0.5230 |           0.2342 |
[32m[20221213 14:53:53 @agent_ppo2.py:185][0m |          -0.0099 |           0.5192 |           0.2341 |
[32m[20221213 14:53:53 @agent_ppo2.py:185][0m |          -0.0067 |           0.5203 |           0.2340 |
[32m[20221213 14:53:53 @agent_ppo2.py:185][0m |          -0.0074 |           0.5164 |           0.2339 |
[32m[20221213 14:53:53 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:53:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.41
[32m[20221213 14:53:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.34
[32m[20221213 14:53:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 27.18
[32m[20221213 14:53:53 @agent_ppo2.py:143][0m Total time:       0.96 min
[32m[20221213 14:53:53 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 14:53:53 @agent_ppo2.py:121][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 14:53:53 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 14:53:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:53 @agent_ppo2.py:185][0m |          -0.0012 |           0.5334 |           0.2403 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0036 |           0.5345 |           0.2401 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0044 |           0.5256 |           0.2400 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0044 |           0.5258 |           0.2399 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0070 |           0.5248 |           0.2397 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |           0.0039 |           0.5553 |           0.2395 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0068 |           0.5237 |           0.2395 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0082 |           0.5238 |           0.2393 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0027 |           0.5276 |           0.2394 |
[32m[20221213 14:53:54 @agent_ppo2.py:185][0m |          -0.0100 |           0.5217 |           0.2391 |
[32m[20221213 14:53:54 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:53:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.16
[32m[20221213 14:53:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.22
[32m[20221213 14:53:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.48
[32m[20221213 14:53:54 @agent_ppo2.py:143][0m Total time:       0.99 min
[32m[20221213 14:53:54 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 14:53:54 @agent_ppo2.py:121][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 14:53:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |           0.0003 |           0.5432 |           0.2386 |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |          -0.0041 |           0.5390 |           0.2383 |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |           0.0022 |           0.5401 |           0.2382 |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |          -0.0088 |           0.5330 |           0.2378 |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |          -0.0079 |           0.5311 |           0.2376 |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |          -0.0078 |           0.5330 |           0.2377 |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |          -0.0086 |           0.5281 |           0.2376 |
[32m[20221213 14:53:55 @agent_ppo2.py:185][0m |          -0.0103 |           0.5278 |           0.2374 |
[32m[20221213 14:53:56 @agent_ppo2.py:185][0m |          -0.0112 |           0.5277 |           0.2375 |
[32m[20221213 14:53:56 @agent_ppo2.py:185][0m |          -0.0112 |           0.5324 |           0.2374 |
[32m[20221213 14:53:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:53:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.08
[32m[20221213 14:53:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.79
[32m[20221213 14:53:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.43
[32m[20221213 14:53:56 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 54.43
[32m[20221213 14:53:56 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 54.43
[32m[20221213 14:53:56 @agent_ppo2.py:143][0m Total time:       1.01 min
[32m[20221213 14:53:56 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 14:53:56 @agent_ppo2.py:121][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 14:53:56 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:53:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:56 @agent_ppo2.py:185][0m |          -0.0028 |           0.5307 |           0.2399 |
[32m[20221213 14:53:56 @agent_ppo2.py:185][0m |          -0.0041 |           0.5176 |           0.2399 |
[32m[20221213 14:53:56 @agent_ppo2.py:185][0m |          -0.0022 |           0.5281 |           0.2400 |
[32m[20221213 14:53:56 @agent_ppo2.py:185][0m |           0.0020 |           0.5537 |           0.2400 |
[32m[20221213 14:53:57 @agent_ppo2.py:185][0m |          -0.0110 |           0.5096 |           0.2401 |
[32m[20221213 14:53:57 @agent_ppo2.py:185][0m |          -0.0109 |           0.5026 |           0.2401 |
[32m[20221213 14:53:57 @agent_ppo2.py:185][0m |          -0.0132 |           0.4977 |           0.2401 |
[32m[20221213 14:53:57 @agent_ppo2.py:185][0m |          -0.0122 |           0.4968 |           0.2401 |
[32m[20221213 14:53:57 @agent_ppo2.py:185][0m |          -0.0119 |           0.4932 |           0.2402 |
[32m[20221213 14:53:57 @agent_ppo2.py:185][0m |          -0.0137 |           0.4907 |           0.2403 |
[32m[20221213 14:53:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:53:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.20
[32m[20221213 14:53:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.12
[32m[20221213 14:53:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.91
[32m[20221213 14:53:57 @agent_ppo2.py:143][0m Total time:       1.03 min
[32m[20221213 14:53:57 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 14:53:57 @agent_ppo2.py:121][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 14:53:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0020 |           0.5779 |           0.2390 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0022 |           0.5692 |           0.2390 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0058 |           0.5600 |           0.2390 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0056 |           0.5631 |           0.2388 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0069 |           0.5578 |           0.2388 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0134 |           0.5551 |           0.2388 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0068 |           0.5562 |           0.2387 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0093 |           0.5522 |           0.2387 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0118 |           0.5525 |           0.2386 |
[32m[20221213 14:53:58 @agent_ppo2.py:185][0m |          -0.0118 |           0.5520 |           0.2386 |
[32m[20221213 14:53:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:53:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.31
[32m[20221213 14:53:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.11
[32m[20221213 14:53:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.69
[32m[20221213 14:53:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 55.69
[32m[20221213 14:53:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 55.69
[32m[20221213 14:53:59 @agent_ppo2.py:143][0m Total time:       1.06 min
[32m[20221213 14:53:59 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 14:53:59 @agent_ppo2.py:121][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 14:53:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:53:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:53:59 @agent_ppo2.py:185][0m |          -0.0006 |           0.5593 |           0.2398 |
[32m[20221213 14:53:59 @agent_ppo2.py:185][0m |          -0.0047 |           0.5584 |           0.2394 |
[32m[20221213 14:53:59 @agent_ppo2.py:185][0m |          -0.0055 |           0.5521 |           0.2390 |
[32m[20221213 14:53:59 @agent_ppo2.py:185][0m |          -0.0055 |           0.5497 |           0.2389 |
[32m[20221213 14:53:59 @agent_ppo2.py:185][0m |          -0.0051 |           0.5522 |           0.2385 |
[32m[20221213 14:53:59 @agent_ppo2.py:185][0m |          -0.0078 |           0.5491 |           0.2384 |
[32m[20221213 14:53:59 @agent_ppo2.py:185][0m |          -0.0036 |           0.5576 |           0.2383 |
[32m[20221213 14:54:00 @agent_ppo2.py:185][0m |          -0.0103 |           0.5488 |           0.2379 |
[32m[20221213 14:54:00 @agent_ppo2.py:185][0m |          -0.0043 |           0.5539 |           0.2379 |
[32m[20221213 14:54:00 @agent_ppo2.py:185][0m |           0.0042 |           0.5984 |           0.2377 |
[32m[20221213 14:54:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.10
[32m[20221213 14:54:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 52.12
[32m[20221213 14:54:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 54.94
[32m[20221213 14:54:00 @agent_ppo2.py:143][0m Total time:       1.08 min
[32m[20221213 14:54:00 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 14:54:00 @agent_ppo2.py:121][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 14:54:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:00 @agent_ppo2.py:185][0m |          -0.0019 |           0.5460 |           0.2398 |
[32m[20221213 14:54:00 @agent_ppo2.py:185][0m |          -0.0012 |           0.5402 |           0.2394 |
[32m[20221213 14:54:00 @agent_ppo2.py:185][0m |          -0.0077 |           0.5329 |           0.2390 |
[32m[20221213 14:54:01 @agent_ppo2.py:185][0m |          -0.0080 |           0.5314 |           0.2391 |
[32m[20221213 14:54:01 @agent_ppo2.py:185][0m |          -0.0100 |           0.5322 |           0.2390 |
[32m[20221213 14:54:01 @agent_ppo2.py:185][0m |          -0.0083 |           0.5338 |           0.2388 |
[32m[20221213 14:54:01 @agent_ppo2.py:185][0m |          -0.0097 |           0.5321 |           0.2388 |
[32m[20221213 14:54:01 @agent_ppo2.py:185][0m |          -0.0114 |           0.5319 |           0.2388 |
[32m[20221213 14:54:01 @agent_ppo2.py:185][0m |          -0.0126 |           0.5294 |           0.2387 |
[32m[20221213 14:54:01 @agent_ppo2.py:185][0m |          -0.0117 |           0.5282 |           0.2386 |
[32m[20221213 14:54:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.79
[32m[20221213 14:54:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.45
[32m[20221213 14:54:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 53.75
[32m[20221213 14:54:01 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 14:54:01 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 14:54:01 @agent_ppo2.py:121][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 14:54:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0009 |           0.5487 |           0.2309 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0074 |           0.5401 |           0.2309 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0011 |           0.5667 |           0.2307 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0088 |           0.5412 |           0.2307 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0081 |           0.5411 |           0.2306 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0022 |           0.5572 |           0.2306 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0099 |           0.5391 |           0.2307 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0112 |           0.5379 |           0.2308 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0145 |           0.5391 |           0.2307 |
[32m[20221213 14:54:02 @agent_ppo2.py:185][0m |          -0.0020 |           0.5677 |           0.2307 |
[32m[20221213 14:54:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:54:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.13
[32m[20221213 14:54:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.28
[32m[20221213 14:54:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.68
[32m[20221213 14:54:03 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 75.68
[32m[20221213 14:54:03 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 75.68
[32m[20221213 14:54:03 @agent_ppo2.py:143][0m Total time:       1.12 min
[32m[20221213 14:54:03 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 14:54:03 @agent_ppo2.py:121][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 14:54:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:03 @agent_ppo2.py:185][0m |          -0.0019 |           0.5484 |           0.2373 |
[32m[20221213 14:54:03 @agent_ppo2.py:185][0m |          -0.0058 |           0.5459 |           0.2372 |
[32m[20221213 14:54:03 @agent_ppo2.py:185][0m |          -0.0059 |           0.5409 |           0.2370 |
[32m[20221213 14:54:03 @agent_ppo2.py:185][0m |          -0.0069 |           0.5434 |           0.2368 |
[32m[20221213 14:54:03 @agent_ppo2.py:185][0m |          -0.0078 |           0.5409 |           0.2368 |
[32m[20221213 14:54:03 @agent_ppo2.py:185][0m |          -0.0079 |           0.5409 |           0.2367 |
[32m[20221213 14:54:04 @agent_ppo2.py:185][0m |          -0.0067 |           0.5452 |           0.2366 |
[32m[20221213 14:54:04 @agent_ppo2.py:185][0m |          -0.0079 |           0.5407 |           0.2366 |
[32m[20221213 14:54:04 @agent_ppo2.py:185][0m |          -0.0090 |           0.5398 |           0.2366 |
[32m[20221213 14:54:04 @agent_ppo2.py:185][0m |          -0.0100 |           0.5394 |           0.2366 |
[32m[20221213 14:54:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.19
[32m[20221213 14:54:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.54
[32m[20221213 14:54:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.09
[32m[20221213 14:54:04 @agent_ppo2.py:143][0m Total time:       1.15 min
[32m[20221213 14:54:04 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 14:54:04 @agent_ppo2.py:121][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 14:54:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:04 @agent_ppo2.py:185][0m |          -0.0002 |           0.6061 |           0.2420 |
[32m[20221213 14:54:04 @agent_ppo2.py:185][0m |          -0.0066 |           0.5943 |           0.2417 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0103 |           0.5915 |           0.2414 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0092 |           0.5849 |           0.2414 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0088 |           0.5818 |           0.2412 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0134 |           0.5875 |           0.2412 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0124 |           0.5806 |           0.2411 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0131 |           0.5755 |           0.2409 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0138 |           0.5775 |           0.2408 |
[32m[20221213 14:54:05 @agent_ppo2.py:185][0m |          -0.0131 |           0.5767 |           0.2408 |
[32m[20221213 14:54:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.30
[32m[20221213 14:54:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.66
[32m[20221213 14:54:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 36.17
[32m[20221213 14:54:05 @agent_ppo2.py:143][0m Total time:       1.17 min
[32m[20221213 14:54:05 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 14:54:05 @agent_ppo2.py:121][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 14:54:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0018 |           0.6316 |           0.2277 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0035 |           0.6240 |           0.2276 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0069 |           0.6222 |           0.2274 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0077 |           0.6179 |           0.2274 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0115 |           0.6189 |           0.2272 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0010 |           0.6392 |           0.2272 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0096 |           0.6206 |           0.2271 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0110 |           0.6165 |           0.2270 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0098 |           0.6140 |           0.2269 |
[32m[20221213 14:54:06 @agent_ppo2.py:185][0m |          -0.0098 |           0.6176 |           0.2268 |
[32m[20221213 14:54:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:54:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.15
[32m[20221213 14:54:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.66
[32m[20221213 14:54:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.18
[32m[20221213 14:54:07 @agent_ppo2.py:143][0m Total time:       1.19 min
[32m[20221213 14:54:07 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 14:54:07 @agent_ppo2.py:121][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 14:54:07 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:54:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:07 @agent_ppo2.py:185][0m |           0.0047 |           0.6701 |           0.2351 |
[32m[20221213 14:54:07 @agent_ppo2.py:185][0m |          -0.0066 |           0.6418 |           0.2353 |
[32m[20221213 14:54:07 @agent_ppo2.py:185][0m |          -0.0072 |           0.6355 |           0.2352 |
[32m[20221213 14:54:07 @agent_ppo2.py:185][0m |          -0.0091 |           0.6338 |           0.2353 |
[32m[20221213 14:54:07 @agent_ppo2.py:185][0m |          -0.0140 |           0.6305 |           0.2354 |
[32m[20221213 14:54:07 @agent_ppo2.py:185][0m |          -0.0139 |           0.6288 |           0.2354 |
[32m[20221213 14:54:08 @agent_ppo2.py:185][0m |          -0.0114 |           0.6254 |           0.2356 |
[32m[20221213 14:54:08 @agent_ppo2.py:185][0m |          -0.0117 |           0.6256 |           0.2357 |
[32m[20221213 14:54:08 @agent_ppo2.py:185][0m |          -0.0142 |           0.6234 |           0.2357 |
[32m[20221213 14:54:08 @agent_ppo2.py:185][0m |          -0.0116 |           0.6251 |           0.2357 |
[32m[20221213 14:54:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.38
[32m[20221213 14:54:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.05
[32m[20221213 14:54:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.93
[32m[20221213 14:54:08 @agent_ppo2.py:143][0m Total time:       1.21 min
[32m[20221213 14:54:08 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 14:54:08 @agent_ppo2.py:121][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 14:54:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:08 @agent_ppo2.py:185][0m |          -0.0015 |           0.5614 |           0.2350 |
[32m[20221213 14:54:08 @agent_ppo2.py:185][0m |          -0.0040 |           0.5575 |           0.2348 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |          -0.0068 |           0.5560 |           0.2347 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |          -0.0088 |           0.5546 |           0.2346 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |           0.0007 |           0.5907 |           0.2346 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |          -0.0101 |           0.5561 |           0.2344 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |          -0.0110 |           0.5516 |           0.2346 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |          -0.0019 |           0.5854 |           0.2346 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |          -0.0117 |           0.5543 |           0.2345 |
[32m[20221213 14:54:09 @agent_ppo2.py:185][0m |          -0.0117 |           0.5533 |           0.2346 |
[32m[20221213 14:54:09 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:54:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.30
[32m[20221213 14:54:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 47.92
[32m[20221213 14:54:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 30.34
[32m[20221213 14:54:09 @agent_ppo2.py:143][0m Total time:       1.24 min
[32m[20221213 14:54:09 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 14:54:09 @agent_ppo2.py:121][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 14:54:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0012 |           0.6368 |           0.2369 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0029 |           0.6285 |           0.2369 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0072 |           0.6253 |           0.2371 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0081 |           0.6215 |           0.2371 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0136 |           0.6248 |           0.2372 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |           0.0001 |           0.6788 |           0.2374 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0091 |           0.6204 |           0.2374 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |           0.0102 |           0.7329 |           0.2375 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0075 |           0.6435 |           0.2378 |
[32m[20221213 14:54:10 @agent_ppo2.py:185][0m |          -0.0113 |           0.6099 |           0.2379 |
[32m[20221213 14:54:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.77
[32m[20221213 14:54:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.20
[32m[20221213 14:54:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 38.05
[32m[20221213 14:54:11 @agent_ppo2.py:143][0m Total time:       1.26 min
[32m[20221213 14:54:11 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 14:54:11 @agent_ppo2.py:121][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 14:54:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:11 @agent_ppo2.py:185][0m |           0.0036 |           0.6019 |           0.2456 |
[32m[20221213 14:54:11 @agent_ppo2.py:185][0m |          -0.0040 |           0.5905 |           0.2456 |
[32m[20221213 14:54:11 @agent_ppo2.py:185][0m |          -0.0005 |           0.6028 |           0.2454 |
[32m[20221213 14:54:11 @agent_ppo2.py:185][0m |          -0.0071 |           0.5853 |           0.2451 |
[32m[20221213 14:54:11 @agent_ppo2.py:185][0m |          -0.0085 |           0.5816 |           0.2452 |
[32m[20221213 14:54:11 @agent_ppo2.py:185][0m |          -0.0067 |           0.5802 |           0.2454 |
[32m[20221213 14:54:12 @agent_ppo2.py:185][0m |          -0.0102 |           0.5837 |           0.2453 |
[32m[20221213 14:54:12 @agent_ppo2.py:185][0m |          -0.0114 |           0.5797 |           0.2454 |
[32m[20221213 14:54:12 @agent_ppo2.py:185][0m |          -0.0117 |           0.5814 |           0.2455 |
[32m[20221213 14:54:12 @agent_ppo2.py:185][0m |          -0.0122 |           0.5779 |           0.2456 |
[32m[20221213 14:54:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.97
[32m[20221213 14:54:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.69
[32m[20221213 14:54:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.00
[32m[20221213 14:54:12 @agent_ppo2.py:143][0m Total time:       1.28 min
[32m[20221213 14:54:12 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 14:54:12 @agent_ppo2.py:121][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 14:54:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:12 @agent_ppo2.py:185][0m |           0.0013 |           0.6884 |           0.2454 |
[32m[20221213 14:54:12 @agent_ppo2.py:185][0m |           0.0011 |           0.6782 |           0.2452 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0076 |           0.6682 |           0.2448 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0084 |           0.6643 |           0.2447 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0106 |           0.6613 |           0.2444 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0106 |           0.6568 |           0.2440 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0112 |           0.6542 |           0.2440 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0145 |           0.6588 |           0.2438 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0081 |           0.6616 |           0.2438 |
[32m[20221213 14:54:13 @agent_ppo2.py:185][0m |          -0.0130 |           0.6486 |           0.2436 |
[32m[20221213 14:54:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:54:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.12
[32m[20221213 14:54:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.09
[32m[20221213 14:54:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.57
[32m[20221213 14:54:13 @agent_ppo2.py:143][0m Total time:       1.30 min
[32m[20221213 14:54:13 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 14:54:13 @agent_ppo2.py:121][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 14:54:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |           0.0106 |           0.6783 |           0.2472 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0072 |           0.5804 |           0.2468 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0088 |           0.5734 |           0.2464 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0093 |           0.5703 |           0.2461 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0104 |           0.5680 |           0.2458 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0096 |           0.5685 |           0.2456 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0119 |           0.5652 |           0.2455 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0131 |           0.5637 |           0.2454 |
[32m[20221213 14:54:14 @agent_ppo2.py:185][0m |          -0.0138 |           0.5646 |           0.2453 |
[32m[20221213 14:54:15 @agent_ppo2.py:185][0m |          -0.0122 |           0.5631 |           0.2451 |
[32m[20221213 14:54:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.67
[32m[20221213 14:54:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.48
[32m[20221213 14:54:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.66
[32m[20221213 14:54:15 @agent_ppo2.py:143][0m Total time:       1.33 min
[32m[20221213 14:54:15 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 14:54:15 @agent_ppo2.py:121][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 14:54:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:15 @agent_ppo2.py:185][0m |          -0.0009 |           0.6242 |           0.2452 |
[32m[20221213 14:54:15 @agent_ppo2.py:185][0m |          -0.0049 |           0.5974 |           0.2447 |
[32m[20221213 14:54:15 @agent_ppo2.py:185][0m |          -0.0091 |           0.5943 |           0.2441 |
[32m[20221213 14:54:15 @agent_ppo2.py:185][0m |          -0.0097 |           0.5832 |           0.2437 |
[32m[20221213 14:54:15 @agent_ppo2.py:185][0m |          -0.0149 |           0.5852 |           0.2435 |
[32m[20221213 14:54:16 @agent_ppo2.py:185][0m |          -0.0109 |           0.5775 |           0.2433 |
[32m[20221213 14:54:16 @agent_ppo2.py:185][0m |          -0.0123 |           0.5720 |           0.2431 |
[32m[20221213 14:54:16 @agent_ppo2.py:185][0m |          -0.0099 |           0.5694 |           0.2429 |
[32m[20221213 14:54:16 @agent_ppo2.py:185][0m |          -0.0139 |           0.5698 |           0.2429 |
[32m[20221213 14:54:16 @agent_ppo2.py:185][0m |          -0.0133 |           0.5660 |           0.2428 |
[32m[20221213 14:54:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.39
[32m[20221213 14:54:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.66
[32m[20221213 14:54:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.53
[32m[20221213 14:54:16 @agent_ppo2.py:143][0m Total time:       1.35 min
[32m[20221213 14:54:16 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 14:54:16 @agent_ppo2.py:121][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 14:54:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:16 @agent_ppo2.py:185][0m |          -0.0030 |           0.5647 |           0.2449 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0058 |           0.5634 |           0.2443 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0073 |           0.5529 |           0.2439 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0099 |           0.5539 |           0.2436 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0091 |           0.5502 |           0.2435 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0071 |           0.5530 |           0.2433 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0097 |           0.5489 |           0.2432 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0111 |           0.5479 |           0.2429 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0108 |           0.5461 |           0.2429 |
[32m[20221213 14:54:17 @agent_ppo2.py:185][0m |          -0.0111 |           0.5446 |           0.2427 |
[32m[20221213 14:54:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 47.65
[32m[20221213 14:54:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 48.13
[32m[20221213 14:54:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.90
[32m[20221213 14:54:17 @agent_ppo2.py:143][0m Total time:       1.37 min
[32m[20221213 14:54:17 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 14:54:17 @agent_ppo2.py:121][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 14:54:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0032 |           0.5902 |           0.2307 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0058 |           0.5790 |           0.2304 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0056 |           0.5785 |           0.2301 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0100 |           0.5744 |           0.2299 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0045 |           0.5800 |           0.2296 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0103 |           0.5739 |           0.2294 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0108 |           0.5706 |           0.2292 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0120 |           0.5700 |           0.2291 |
[32m[20221213 14:54:18 @agent_ppo2.py:185][0m |          -0.0131 |           0.5711 |           0.2290 |
[32m[20221213 14:54:19 @agent_ppo2.py:185][0m |          -0.0133 |           0.5679 |           0.2288 |
[32m[20221213 14:54:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.81
[32m[20221213 14:54:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 60.45
[32m[20221213 14:54:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 47.55
[32m[20221213 14:54:19 @agent_ppo2.py:143][0m Total time:       1.39 min
[32m[20221213 14:54:19 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 14:54:19 @agent_ppo2.py:121][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 14:54:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:19 @agent_ppo2.py:185][0m |          -0.0021 |           0.5736 |           0.2393 |
[32m[20221213 14:54:19 @agent_ppo2.py:185][0m |          -0.0067 |           0.5668 |           0.2391 |
[32m[20221213 14:54:19 @agent_ppo2.py:185][0m |          -0.0115 |           0.5658 |           0.2388 |
[32m[20221213 14:54:19 @agent_ppo2.py:185][0m |          -0.0107 |           0.5667 |           0.2386 |
[32m[20221213 14:54:19 @agent_ppo2.py:185][0m |          -0.0113 |           0.5621 |           0.2384 |
[32m[20221213 14:54:20 @agent_ppo2.py:185][0m |          -0.0130 |           0.5610 |           0.2382 |
[32m[20221213 14:54:20 @agent_ppo2.py:185][0m |          -0.0135 |           0.5627 |           0.2379 |
[32m[20221213 14:54:20 @agent_ppo2.py:185][0m |          -0.0141 |           0.5614 |           0.2378 |
[32m[20221213 14:54:20 @agent_ppo2.py:185][0m |          -0.0134 |           0.5621 |           0.2376 |
[32m[20221213 14:54:20 @agent_ppo2.py:185][0m |          -0.0157 |           0.5624 |           0.2375 |
[32m[20221213 14:54:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.03
[32m[20221213 14:54:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 49.24
[32m[20221213 14:54:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.85
[32m[20221213 14:54:20 @agent_ppo2.py:143][0m Total time:       1.41 min
[32m[20221213 14:54:20 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 14:54:20 @agent_ppo2.py:121][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 14:54:20 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:54:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:20 @agent_ppo2.py:185][0m |          -0.0028 |           0.6136 |           0.2366 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0048 |           0.6038 |           0.2365 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0092 |           0.5958 |           0.2365 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0094 |           0.5977 |           0.2360 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0096 |           0.5928 |           0.2359 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0111 |           0.5964 |           0.2357 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0121 |           0.5905 |           0.2356 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0131 |           0.5895 |           0.2355 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0132 |           0.5884 |           0.2355 |
[32m[20221213 14:54:21 @agent_ppo2.py:185][0m |          -0.0132 |           0.5874 |           0.2352 |
[32m[20221213 14:54:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 49.62
[32m[20221213 14:54:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.83
[32m[20221213 14:54:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.79
[32m[20221213 14:54:21 @agent_ppo2.py:143][0m Total time:       1.44 min
[32m[20221213 14:54:21 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 14:54:21 @agent_ppo2.py:121][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 14:54:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |          -0.0008 |           0.6748 |           0.2311 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |          -0.0010 |           0.6606 |           0.2310 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |          -0.0080 |           0.6584 |           0.2310 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |          -0.0065 |           0.6532 |           0.2310 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |           0.0107 |           0.7180 |           0.2308 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |           0.0004 |           0.6992 |           0.2307 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |          -0.0059 |           0.6510 |           0.2306 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |          -0.0089 |           0.6465 |           0.2305 |
[32m[20221213 14:54:22 @agent_ppo2.py:185][0m |          -0.0013 |           0.6711 |           0.2304 |
[32m[20221213 14:54:23 @agent_ppo2.py:185][0m |          -0.0117 |           0.6424 |           0.2303 |
[32m[20221213 14:54:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.19
[32m[20221213 14:54:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 72.59
[32m[20221213 14:54:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.13
[32m[20221213 14:54:23 @agent_ppo2.py:143][0m Total time:       1.46 min
[32m[20221213 14:54:23 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 14:54:23 @agent_ppo2.py:121][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 14:54:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:23 @agent_ppo2.py:185][0m |          -0.0011 |           0.6175 |           0.2276 |
[32m[20221213 14:54:23 @agent_ppo2.py:185][0m |           0.0050 |           0.6617 |           0.2276 |
[32m[20221213 14:54:23 @agent_ppo2.py:185][0m |          -0.0078 |           0.6158 |           0.2275 |
[32m[20221213 14:54:23 @agent_ppo2.py:185][0m |          -0.0077 |           0.6056 |           0.2274 |
[32m[20221213 14:54:23 @agent_ppo2.py:185][0m |           0.0004 |           0.6572 |           0.2274 |
[32m[20221213 14:54:24 @agent_ppo2.py:185][0m |          -0.0102 |           0.6042 |           0.2273 |
[32m[20221213 14:54:24 @agent_ppo2.py:185][0m |          -0.0132 |           0.6024 |           0.2272 |
[32m[20221213 14:54:24 @agent_ppo2.py:185][0m |          -0.0117 |           0.6058 |           0.2272 |
[32m[20221213 14:54:24 @agent_ppo2.py:185][0m |          -0.0139 |           0.6035 |           0.2272 |
[32m[20221213 14:54:24 @agent_ppo2.py:185][0m |          -0.0140 |           0.6019 |           0.2272 |
[32m[20221213 14:54:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.40
[32m[20221213 14:54:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 51.81
[32m[20221213 14:54:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.68
[32m[20221213 14:54:24 @agent_ppo2.py:143][0m Total time:       1.48 min
[32m[20221213 14:54:24 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 14:54:24 @agent_ppo2.py:121][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 14:54:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:24 @agent_ppo2.py:185][0m |          -0.0030 |           0.6278 |           0.2345 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0054 |           0.6238 |           0.2344 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0100 |           0.6193 |           0.2342 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0127 |           0.6159 |           0.2340 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0124 |           0.6141 |           0.2339 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0125 |           0.6126 |           0.2339 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0173 |           0.6158 |           0.2338 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0129 |           0.6094 |           0.2338 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0156 |           0.6137 |           0.2338 |
[32m[20221213 14:54:25 @agent_ppo2.py:185][0m |          -0.0218 |           0.6109 |           0.2338 |
[32m[20221213 14:54:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.03
[32m[20221213 14:54:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.50
[32m[20221213 14:54:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.44
[32m[20221213 14:54:25 @agent_ppo2.py:143][0m Total time:       1.50 min
[32m[20221213 14:54:25 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 14:54:25 @agent_ppo2.py:121][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 14:54:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0012 |           0.6303 |           0.2403 |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0035 |           0.6213 |           0.2401 |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0057 |           0.6354 |           0.2400 |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0118 |           0.6123 |           0.2399 |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0137 |           0.6101 |           0.2396 |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0122 |           0.6060 |           0.2395 |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0091 |           0.6054 |           0.2395 |
[32m[20221213 14:54:26 @agent_ppo2.py:185][0m |          -0.0133 |           0.6002 |           0.2394 |
[32m[20221213 14:54:27 @agent_ppo2.py:185][0m |          -0.0138 |           0.6007 |           0.2393 |
[32m[20221213 14:54:27 @agent_ppo2.py:185][0m |          -0.0153 |           0.5989 |           0.2392 |
[32m[20221213 14:54:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.18
[32m[20221213 14:54:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 54.23
[32m[20221213 14:54:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.30
[32m[20221213 14:54:27 @agent_ppo2.py:143][0m Total time:       1.53 min
[32m[20221213 14:54:27 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 14:54:27 @agent_ppo2.py:121][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 14:54:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:27 @agent_ppo2.py:185][0m |          -0.0004 |           0.6335 |           0.2320 |
[32m[20221213 14:54:27 @agent_ppo2.py:185][0m |          -0.0014 |           0.6282 |           0.2315 |
[32m[20221213 14:54:27 @agent_ppo2.py:185][0m |          -0.0075 |           0.6178 |           0.2313 |
[32m[20221213 14:54:27 @agent_ppo2.py:185][0m |          -0.0114 |           0.6161 |           0.2311 |
[32m[20221213 14:54:27 @agent_ppo2.py:185][0m |          -0.0077 |           0.6185 |           0.2311 |
[32m[20221213 14:54:28 @agent_ppo2.py:185][0m |          -0.0140 |           0.6144 |           0.2312 |
[32m[20221213 14:54:28 @agent_ppo2.py:185][0m |          -0.0160 |           0.6136 |           0.2312 |
[32m[20221213 14:54:28 @agent_ppo2.py:185][0m |          -0.0025 |           0.6712 |           0.2312 |
[32m[20221213 14:54:28 @agent_ppo2.py:185][0m |          -0.0105 |           0.6138 |           0.2312 |
[32m[20221213 14:54:28 @agent_ppo2.py:185][0m |          -0.0150 |           0.6070 |           0.2313 |
[32m[20221213 14:54:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:54:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 53.27
[32m[20221213 14:54:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.96
[32m[20221213 14:54:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 55.01
[32m[20221213 14:54:28 @agent_ppo2.py:143][0m Total time:       1.55 min
[32m[20221213 14:54:28 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 14:54:28 @agent_ppo2.py:121][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 14:54:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:28 @agent_ppo2.py:185][0m |           0.0019 |           0.6255 |           0.2336 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |          -0.0045 |           0.6195 |           0.2334 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |           0.0030 |           0.6580 |           0.2333 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |          -0.0105 |           0.6116 |           0.2336 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |          -0.0071 |           0.6100 |           0.2335 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |           0.0066 |           0.6752 |           0.2336 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |          -0.0110 |           0.6142 |           0.2337 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |          -0.0124 |           0.6050 |           0.2338 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |          -0.0133 |           0.6050 |           0.2339 |
[32m[20221213 14:54:29 @agent_ppo2.py:185][0m |          -0.0106 |           0.6046 |           0.2340 |
[32m[20221213 14:54:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:54:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.87
[32m[20221213 14:54:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.46
[32m[20221213 14:54:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.33
[32m[20221213 14:54:29 @agent_ppo2.py:143][0m Total time:       1.57 min
[32m[20221213 14:54:29 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 14:54:29 @agent_ppo2.py:121][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 14:54:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0034 |           0.6128 |           0.2351 |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0072 |           0.5958 |           0.2352 |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0105 |           0.5874 |           0.2351 |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0075 |           0.5864 |           0.2349 |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0106 |           0.5861 |           0.2347 |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0067 |           0.5907 |           0.2345 |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0041 |           0.6204 |           0.2343 |
[32m[20221213 14:54:30 @agent_ppo2.py:185][0m |          -0.0159 |           0.5777 |           0.2340 |
[32m[20221213 14:54:31 @agent_ppo2.py:185][0m |          -0.0107 |           0.5761 |           0.2340 |
[32m[20221213 14:54:31 @agent_ppo2.py:185][0m |          -0.0154 |           0.5703 |           0.2338 |
[32m[20221213 14:54:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.84
[32m[20221213 14:54:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 50.60
[32m[20221213 14:54:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.00
[32m[20221213 14:54:31 @agent_ppo2.py:143][0m Total time:       1.59 min
[32m[20221213 14:54:31 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 14:54:31 @agent_ppo2.py:121][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 14:54:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:31 @agent_ppo2.py:185][0m |          -0.0013 |           0.7334 |           0.2334 |
[32m[20221213 14:54:31 @agent_ppo2.py:185][0m |          -0.0059 |           0.7158 |           0.2333 |
[32m[20221213 14:54:31 @agent_ppo2.py:185][0m |          -0.0097 |           0.7101 |           0.2334 |
[32m[20221213 14:54:31 @agent_ppo2.py:185][0m |          -0.0086 |           0.7025 |           0.2334 |
[32m[20221213 14:54:32 @agent_ppo2.py:185][0m |          -0.0084 |           0.7039 |           0.2335 |
[32m[20221213 14:54:32 @agent_ppo2.py:185][0m |          -0.0151 |           0.6920 |           0.2336 |
[32m[20221213 14:54:32 @agent_ppo2.py:185][0m |          -0.0125 |           0.6897 |           0.2337 |
[32m[20221213 14:54:32 @agent_ppo2.py:185][0m |          -0.0133 |           0.6849 |           0.2338 |
[32m[20221213 14:54:32 @agent_ppo2.py:185][0m |          -0.0070 |           0.7205 |           0.2339 |
[32m[20221213 14:54:32 @agent_ppo2.py:185][0m |          -0.0142 |           0.6898 |           0.2341 |
[32m[20221213 14:54:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.64
[32m[20221213 14:54:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.44
[32m[20221213 14:54:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 74.84
[32m[20221213 14:54:32 @agent_ppo2.py:143][0m Total time:       1.62 min
[32m[20221213 14:54:32 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 14:54:32 @agent_ppo2.py:121][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 14:54:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |           0.0066 |           0.7329 |           0.2376 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0026 |           0.6939 |           0.2376 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0010 |           0.7212 |           0.2376 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0080 |           0.6869 |           0.2375 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |           0.0024 |           0.7403 |           0.2374 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0039 |           0.7089 |           0.2376 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0094 |           0.6770 |           0.2376 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0094 |           0.6756 |           0.2376 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0124 |           0.6713 |           0.2376 |
[32m[20221213 14:54:33 @agent_ppo2.py:185][0m |          -0.0121 |           0.6732 |           0.2377 |
[32m[20221213 14:54:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 52.04
[32m[20221213 14:54:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 55.98
[32m[20221213 14:54:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.29
[32m[20221213 14:54:33 @agent_ppo2.py:143][0m Total time:       1.64 min
[32m[20221213 14:54:33 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 14:54:33 @agent_ppo2.py:121][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 14:54:34 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:54:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |           0.0003 |           0.7176 |           0.2392 |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |          -0.0035 |           0.6988 |           0.2392 |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |          -0.0092 |           0.6877 |           0.2391 |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |          -0.0055 |           0.6862 |           0.2390 |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |          -0.0110 |           0.6808 |           0.2389 |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |          -0.0129 |           0.6758 |           0.2389 |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |          -0.0100 |           0.6713 |           0.2389 |
[32m[20221213 14:54:34 @agent_ppo2.py:185][0m |          -0.0170 |           0.6715 |           0.2388 |
[32m[20221213 14:54:35 @agent_ppo2.py:185][0m |          -0.0092 |           0.6749 |           0.2388 |
[32m[20221213 14:54:35 @agent_ppo2.py:185][0m |          -0.0114 |           0.6674 |           0.2388 |
[32m[20221213 14:54:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.01
[32m[20221213 14:54:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.28
[32m[20221213 14:54:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 27.64
[32m[20221213 14:54:35 @agent_ppo2.py:143][0m Total time:       1.66 min
[32m[20221213 14:54:35 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 14:54:35 @agent_ppo2.py:121][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 14:54:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:35 @agent_ppo2.py:185][0m |          -0.0007 |           0.7661 |           0.2472 |
[32m[20221213 14:54:35 @agent_ppo2.py:185][0m |          -0.0079 |           0.7507 |           0.2471 |
[32m[20221213 14:54:35 @agent_ppo2.py:185][0m |          -0.0036 |           0.7475 |           0.2469 |
[32m[20221213 14:54:35 @agent_ppo2.py:185][0m |          -0.0088 |           0.7360 |           0.2467 |
[32m[20221213 14:54:36 @agent_ppo2.py:185][0m |          -0.0083 |           0.7362 |           0.2466 |
[32m[20221213 14:54:36 @agent_ppo2.py:185][0m |          -0.0061 |           0.7328 |           0.2465 |
[32m[20221213 14:54:36 @agent_ppo2.py:185][0m |          -0.0061 |           0.7520 |           0.2463 |
[32m[20221213 14:54:36 @agent_ppo2.py:185][0m |          -0.0112 |           0.7264 |           0.2462 |
[32m[20221213 14:54:36 @agent_ppo2.py:185][0m |          -0.0118 |           0.7249 |           0.2461 |
[32m[20221213 14:54:36 @agent_ppo2.py:185][0m |          -0.0114 |           0.7226 |           0.2461 |
[32m[20221213 14:54:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.11
[32m[20221213 14:54:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.10
[32m[20221213 14:54:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.24
[32m[20221213 14:54:36 @agent_ppo2.py:143][0m Total time:       1.68 min
[32m[20221213 14:54:36 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 14:54:36 @agent_ppo2.py:121][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 14:54:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |           0.0035 |           0.7403 |           0.2363 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0071 |           0.7211 |           0.2362 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |           0.0004 |           0.7796 |           0.2360 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0123 |           0.7167 |           0.2356 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0126 |           0.7040 |           0.2355 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0110 |           0.7027 |           0.2353 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0127 |           0.6981 |           0.2353 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0123 |           0.6949 |           0.2351 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0157 |           0.6949 |           0.2351 |
[32m[20221213 14:54:37 @agent_ppo2.py:185][0m |          -0.0146 |           0.6930 |           0.2349 |
[32m[20221213 14:54:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 50.52
[32m[20221213 14:54:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.75
[32m[20221213 14:54:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.16
[32m[20221213 14:54:37 @agent_ppo2.py:143][0m Total time:       1.70 min
[32m[20221213 14:54:37 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 14:54:37 @agent_ppo2.py:121][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 14:54:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0006 |           0.8564 |           0.2379 |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0039 |           0.8194 |           0.2379 |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0002 |           0.8715 |           0.2379 |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0079 |           0.8039 |           0.2379 |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0105 |           0.7890 |           0.2381 |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0126 |           0.7856 |           0.2382 |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0073 |           0.8221 |           0.2384 |
[32m[20221213 14:54:38 @agent_ppo2.py:185][0m |          -0.0072 |           0.7940 |           0.2381 |
[32m[20221213 14:54:39 @agent_ppo2.py:185][0m |          -0.0136 |           0.7730 |           0.2384 |
[32m[20221213 14:54:39 @agent_ppo2.py:185][0m |          -0.0171 |           0.7734 |           0.2385 |
[32m[20221213 14:54:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.61
[32m[20221213 14:54:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.83
[32m[20221213 14:54:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.87
[32m[20221213 14:54:39 @agent_ppo2.py:143][0m Total time:       1.73 min
[32m[20221213 14:54:39 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 14:54:39 @agent_ppo2.py:121][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 14:54:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:39 @agent_ppo2.py:185][0m |          -0.0012 |           0.7734 |           0.2397 |
[32m[20221213 14:54:39 @agent_ppo2.py:185][0m |          -0.0058 |           0.7494 |           0.2397 |
[32m[20221213 14:54:39 @agent_ppo2.py:185][0m |          -0.0091 |           0.7346 |           0.2395 |
[32m[20221213 14:54:39 @agent_ppo2.py:185][0m |           0.0047 |           0.8049 |           0.2394 |
[32m[20221213 14:54:40 @agent_ppo2.py:185][0m |          -0.0090 |           0.7305 |           0.2393 |
[32m[20221213 14:54:40 @agent_ppo2.py:185][0m |          -0.0153 |           0.7182 |           0.2392 |
[32m[20221213 14:54:40 @agent_ppo2.py:185][0m |          -0.0114 |           0.7196 |           0.2392 |
[32m[20221213 14:54:40 @agent_ppo2.py:185][0m |          -0.0121 |           0.7142 |           0.2392 |
[32m[20221213 14:54:40 @agent_ppo2.py:185][0m |          -0.0143 |           0.7101 |           0.2393 |
[32m[20221213 14:54:40 @agent_ppo2.py:185][0m |          -0.0107 |           0.7077 |           0.2392 |
[32m[20221213 14:54:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.35
[32m[20221213 14:54:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 59.26
[32m[20221213 14:54:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.94
[32m[20221213 14:54:40 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221213 14:54:40 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 14:54:40 @agent_ppo2.py:121][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 14:54:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |           0.0052 |           0.8180 |           0.2406 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0094 |           0.7843 |           0.2405 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |           0.0003 |           0.7966 |           0.2403 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0093 |           0.7582 |           0.2403 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0014 |           0.7685 |           0.2402 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0103 |           0.7452 |           0.2402 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0116 |           0.7420 |           0.2401 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0123 |           0.7431 |           0.2402 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0131 |           0.7370 |           0.2402 |
[32m[20221213 14:54:41 @agent_ppo2.py:185][0m |          -0.0145 |           0.7328 |           0.2402 |
[32m[20221213 14:54:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.82
[32m[20221213 14:54:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.65
[32m[20221213 14:54:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 28.00
[32m[20221213 14:54:41 @agent_ppo2.py:143][0m Total time:       1.77 min
[32m[20221213 14:54:41 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 14:54:41 @agent_ppo2.py:121][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 14:54:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0019 |           0.8759 |           0.2424 |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0041 |           0.8567 |           0.2427 |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0088 |           0.8483 |           0.2427 |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0095 |           0.8384 |           0.2427 |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0037 |           0.8574 |           0.2426 |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0005 |           0.8753 |           0.2426 |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0128 |           0.8293 |           0.2425 |
[32m[20221213 14:54:42 @agent_ppo2.py:185][0m |          -0.0052 |           0.8419 |           0.2425 |
[32m[20221213 14:54:43 @agent_ppo2.py:185][0m |          -0.0126 |           0.8215 |           0.2426 |
[32m[20221213 14:54:43 @agent_ppo2.py:185][0m |          -0.0160 |           0.8277 |           0.2425 |
[32m[20221213 14:54:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:54:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.12
[32m[20221213 14:54:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 75.96
[32m[20221213 14:54:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.26
[32m[20221213 14:54:43 @agent_ppo2.py:143][0m Total time:       1.79 min
[32m[20221213 14:54:43 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 14:54:43 @agent_ppo2.py:121][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 14:54:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:43 @agent_ppo2.py:185][0m |          -0.0029 |           0.7907 |           0.2542 |
[32m[20221213 14:54:43 @agent_ppo2.py:185][0m |          -0.0077 |           0.7762 |           0.2541 |
[32m[20221213 14:54:43 @agent_ppo2.py:185][0m |          -0.0048 |           0.7694 |           0.2539 |
[32m[20221213 14:54:44 @agent_ppo2.py:185][0m |          -0.0042 |           0.8206 |           0.2539 |
[32m[20221213 14:54:44 @agent_ppo2.py:185][0m |          -0.0133 |           0.7717 |           0.2538 |
[32m[20221213 14:54:44 @agent_ppo2.py:185][0m |          -0.0105 |           0.7577 |           0.2537 |
[32m[20221213 14:54:44 @agent_ppo2.py:185][0m |          -0.0104 |           0.7520 |           0.2536 |
[32m[20221213 14:54:44 @agent_ppo2.py:185][0m |          -0.0079 |           0.7720 |           0.2535 |
[32m[20221213 14:54:44 @agent_ppo2.py:185][0m |          -0.0131 |           0.7518 |           0.2534 |
[32m[20221213 14:54:44 @agent_ppo2.py:185][0m |          -0.0102 |           0.7506 |           0.2533 |
[32m[20221213 14:54:44 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 14:54:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 55.09
[32m[20221213 14:54:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 58.68
[32m[20221213 14:54:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.89
[32m[20221213 14:54:44 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 77.89
[32m[20221213 14:54:44 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 77.89
[32m[20221213 14:54:44 @agent_ppo2.py:143][0m Total time:       1.82 min
[32m[20221213 14:54:44 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 14:54:44 @agent_ppo2.py:121][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 14:54:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0003 |           0.8709 |           0.2433 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0035 |           0.8314 |           0.2431 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0059 |           0.8247 |           0.2430 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0086 |           0.8155 |           0.2429 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0111 |           0.8143 |           0.2428 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0101 |           0.8097 |           0.2429 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0093 |           0.8017 |           0.2430 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0091 |           0.8026 |           0.2430 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0104 |           0.7958 |           0.2429 |
[32m[20221213 14:54:45 @agent_ppo2.py:185][0m |          -0.0117 |           0.7934 |           0.2430 |
[32m[20221213 14:54:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:54:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 57.55
[32m[20221213 14:54:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 61.22
[32m[20221213 14:54:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.15
[32m[20221213 14:54:46 @agent_ppo2.py:143][0m Total time:       1.84 min
[32m[20221213 14:54:46 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 14:54:46 @agent_ppo2.py:121][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 14:54:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:46 @agent_ppo2.py:185][0m |          -0.0032 |           0.9076 |           0.2411 |
[32m[20221213 14:54:46 @agent_ppo2.py:185][0m |          -0.0054 |           0.8781 |           0.2407 |
[32m[20221213 14:54:46 @agent_ppo2.py:185][0m |          -0.0078 |           0.8592 |           0.2406 |
[32m[20221213 14:54:46 @agent_ppo2.py:185][0m |          -0.0095 |           0.8501 |           0.2405 |
[32m[20221213 14:54:46 @agent_ppo2.py:185][0m |          -0.0081 |           0.8456 |           0.2405 |
[32m[20221213 14:54:46 @agent_ppo2.py:185][0m |          -0.0095 |           0.8402 |           0.2405 |
[32m[20221213 14:54:47 @agent_ppo2.py:185][0m |          -0.0081 |           0.8365 |           0.2406 |
[32m[20221213 14:54:47 @agent_ppo2.py:185][0m |          -0.0103 |           0.8436 |           0.2405 |
[32m[20221213 14:54:47 @agent_ppo2.py:185][0m |          -0.0105 |           0.8241 |           0.2405 |
[32m[20221213 14:54:47 @agent_ppo2.py:185][0m |          -0.0154 |           0.8207 |           0.2405 |
[32m[20221213 14:54:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:54:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.20
[32m[20221213 14:54:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 77.47
[32m[20221213 14:54:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.52
[32m[20221213 14:54:47 @agent_ppo2.py:143][0m Total time:       1.86 min
[32m[20221213 14:54:47 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 14:54:47 @agent_ppo2.py:121][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 14:54:47 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:54:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:47 @agent_ppo2.py:185][0m |          -0.0005 |           0.8098 |           0.2461 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0059 |           0.7762 |           0.2460 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0042 |           0.7600 |           0.2459 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0044 |           0.7525 |           0.2460 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0074 |           0.7502 |           0.2459 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0066 |           0.7477 |           0.2461 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0120 |           0.7438 |           0.2462 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0115 |           0.7430 |           0.2461 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0136 |           0.7409 |           0.2462 |
[32m[20221213 14:54:48 @agent_ppo2.py:185][0m |          -0.0170 |           0.7387 |           0.2463 |
[32m[20221213 14:54:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:54:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.25
[32m[20221213 14:54:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 65.73
[32m[20221213 14:54:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.28
[32m[20221213 14:54:48 @agent_ppo2.py:143][0m Total time:       1.89 min
[32m[20221213 14:54:48 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 14:54:48 @agent_ppo2.py:121][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 14:54:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0002 |           0.8158 |           0.2450 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0071 |           0.8132 |           0.2450 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0043 |           0.8049 |           0.2450 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0075 |           0.8023 |           0.2449 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0103 |           0.8032 |           0.2450 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0093 |           0.8171 |           0.2451 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0056 |           0.8101 |           0.2452 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0086 |           0.8034 |           0.2452 |
[32m[20221213 14:54:49 @agent_ppo2.py:185][0m |          -0.0142 |           0.7916 |           0.2453 |
[32m[20221213 14:54:50 @agent_ppo2.py:185][0m |          -0.0125 |           0.7891 |           0.2455 |
[32m[20221213 14:54:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:54:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.50
[32m[20221213 14:54:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.23
[32m[20221213 14:54:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.12
[32m[20221213 14:54:50 @agent_ppo2.py:143][0m Total time:       1.91 min
[32m[20221213 14:54:50 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 14:54:50 @agent_ppo2.py:121][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 14:54:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:50 @agent_ppo2.py:185][0m |          -0.0035 |           0.8756 |           0.2443 |
[32m[20221213 14:54:50 @agent_ppo2.py:185][0m |          -0.0076 |           0.8614 |           0.2438 |
[32m[20221213 14:54:50 @agent_ppo2.py:185][0m |          -0.0122 |           0.8556 |           0.2435 |
[32m[20221213 14:54:50 @agent_ppo2.py:185][0m |          -0.0136 |           0.8456 |           0.2432 |
[32m[20221213 14:54:50 @agent_ppo2.py:185][0m |          -0.0125 |           0.8525 |           0.2429 |
[32m[20221213 14:54:51 @agent_ppo2.py:185][0m |          -0.0171 |           0.8413 |           0.2428 |
[32m[20221213 14:54:51 @agent_ppo2.py:185][0m |          -0.0097 |           0.8373 |           0.2427 |
[32m[20221213 14:54:51 @agent_ppo2.py:185][0m |          -0.0123 |           0.8324 |           0.2425 |
[32m[20221213 14:54:51 @agent_ppo2.py:185][0m |          -0.0180 |           0.8315 |           0.2425 |
[32m[20221213 14:54:51 @agent_ppo2.py:185][0m |          -0.0148 |           0.8278 |           0.2424 |
[32m[20221213 14:54:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.29
[32m[20221213 14:54:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.31
[32m[20221213 14:54:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.06
[32m[20221213 14:54:51 @agent_ppo2.py:143][0m Total time:       1.93 min
[32m[20221213 14:54:51 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 14:54:51 @agent_ppo2.py:121][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 14:54:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:51 @agent_ppo2.py:185][0m |           0.0102 |           0.9719 |           0.2415 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0063 |           0.9170 |           0.2416 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0110 |           0.9048 |           0.2418 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0098 |           0.8957 |           0.2417 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0128 |           0.8912 |           0.2418 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0129 |           0.8806 |           0.2419 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0126 |           0.8739 |           0.2421 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0156 |           0.8770 |           0.2422 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0138 |           0.8674 |           0.2424 |
[32m[20221213 14:54:52 @agent_ppo2.py:185][0m |          -0.0111 |           0.8687 |           0.2426 |
[32m[20221213 14:54:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.62
[32m[20221213 14:54:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 71.70
[32m[20221213 14:54:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.70
[32m[20221213 14:54:52 @agent_ppo2.py:143][0m Total time:       1.95 min
[32m[20221213 14:54:52 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 14:54:52 @agent_ppo2.py:121][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 14:54:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0008 |           0.6346 |           0.2473 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0065 |           0.6166 |           0.2470 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0074 |           0.6230 |           0.2467 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0097 |           0.6092 |           0.2467 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0129 |           0.6081 |           0.2465 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0143 |           0.6052 |           0.2465 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0142 |           0.6030 |           0.2465 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0136 |           0.6020 |           0.2464 |
[32m[20221213 14:54:53 @agent_ppo2.py:185][0m |          -0.0147 |           0.6010 |           0.2464 |
[32m[20221213 14:54:54 @agent_ppo2.py:185][0m |          -0.0145 |           0.6016 |           0.2465 |
[32m[20221213 14:54:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.96
[32m[20221213 14:54:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.13
[32m[20221213 14:54:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.94
[32m[20221213 14:54:54 @agent_ppo2.py:143][0m Total time:       1.98 min
[32m[20221213 14:54:54 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 14:54:54 @agent_ppo2.py:121][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 14:54:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:54 @agent_ppo2.py:185][0m |           0.0090 |           0.7853 |           0.2496 |
[32m[20221213 14:54:54 @agent_ppo2.py:185][0m |          -0.0024 |           0.7086 |           0.2494 |
[32m[20221213 14:54:54 @agent_ppo2.py:185][0m |          -0.0094 |           0.6808 |           0.2495 |
[32m[20221213 14:54:54 @agent_ppo2.py:185][0m |          -0.0117 |           0.6719 |           0.2495 |
[32m[20221213 14:54:55 @agent_ppo2.py:185][0m |          -0.0130 |           0.6617 |           0.2498 |
[32m[20221213 14:54:55 @agent_ppo2.py:185][0m |          -0.0095 |           0.6551 |           0.2500 |
[32m[20221213 14:54:55 @agent_ppo2.py:185][0m |          -0.0119 |           0.6478 |           0.2501 |
[32m[20221213 14:54:55 @agent_ppo2.py:185][0m |           0.0003 |           0.6948 |           0.2502 |
[32m[20221213 14:54:55 @agent_ppo2.py:185][0m |          -0.0167 |           0.6588 |           0.2505 |
[32m[20221213 14:54:55 @agent_ppo2.py:185][0m |          -0.0067 |           0.6558 |           0.2506 |
[32m[20221213 14:54:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:54:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.68
[32m[20221213 14:54:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 62.01
[32m[20221213 14:54:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.23
[32m[20221213 14:54:55 @agent_ppo2.py:143][0m Total time:       2.00 min
[32m[20221213 14:54:55 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 14:54:55 @agent_ppo2.py:121][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 14:54:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0013 |           0.9901 |           0.2572 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0047 |           0.9625 |           0.2571 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0060 |           0.9501 |           0.2570 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |           0.0091 |           1.0545 |           0.2568 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0102 |           0.9486 |           0.2567 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0129 |           0.9360 |           0.2566 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0091 |           0.9314 |           0.2563 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0125 |           0.9307 |           0.2563 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0120 |           0.9246 |           0.2562 |
[32m[20221213 14:54:56 @agent_ppo2.py:185][0m |          -0.0144 |           0.9189 |           0.2562 |
[32m[20221213 14:54:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:54:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 62.13
[32m[20221213 14:54:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 76.08
[32m[20221213 14:54:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 26.23
[32m[20221213 14:54:57 @agent_ppo2.py:143][0m Total time:       2.02 min
[32m[20221213 14:54:57 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 14:54:57 @agent_ppo2.py:121][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 14:54:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0034 |           0.8573 |           0.2527 |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0103 |           0.8351 |           0.2525 |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0071 |           0.8257 |           0.2524 |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0054 |           0.8280 |           0.2523 |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0079 |           0.8159 |           0.2522 |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0038 |           0.9001 |           0.2522 |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0155 |           0.8108 |           0.2520 |
[32m[20221213 14:54:57 @agent_ppo2.py:185][0m |          -0.0132 |           0.8068 |           0.2522 |
[32m[20221213 14:54:58 @agent_ppo2.py:185][0m |          -0.0113 |           0.8052 |           0.2522 |
[32m[20221213 14:54:58 @agent_ppo2.py:185][0m |          -0.0169 |           0.8005 |           0.2523 |
[32m[20221213 14:54:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:54:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.13
[32m[20221213 14:54:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 64.97
[32m[20221213 14:54:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.46
[32m[20221213 14:54:58 @agent_ppo2.py:143][0m Total time:       2.04 min
[32m[20221213 14:54:58 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 14:54:58 @agent_ppo2.py:121][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 14:54:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:54:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:54:58 @agent_ppo2.py:185][0m |          -0.0006 |           0.8909 |           0.2519 |
[32m[20221213 14:54:58 @agent_ppo2.py:185][0m |          -0.0053 |           0.8485 |           0.2516 |
[32m[20221213 14:54:58 @agent_ppo2.py:185][0m |          -0.0083 |           0.8382 |           0.2513 |
[32m[20221213 14:54:58 @agent_ppo2.py:185][0m |          -0.0136 |           0.8353 |           0.2511 |
[32m[20221213 14:54:59 @agent_ppo2.py:185][0m |          -0.0070 |           0.8528 |           0.2510 |
[32m[20221213 14:54:59 @agent_ppo2.py:185][0m |          -0.0121 |           0.8010 |           0.2506 |
[32m[20221213 14:54:59 @agent_ppo2.py:185][0m |          -0.0079 |           0.8110 |           0.2506 |
[32m[20221213 14:54:59 @agent_ppo2.py:185][0m |          -0.0154 |           0.7940 |           0.2505 |
[32m[20221213 14:54:59 @agent_ppo2.py:185][0m |          -0.0049 |           0.8274 |           0.2503 |
[32m[20221213 14:54:59 @agent_ppo2.py:185][0m |          -0.0136 |           0.7818 |           0.2502 |
[32m[20221213 14:54:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:54:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 59.99
[32m[20221213 14:54:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.80
[32m[20221213 14:54:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.07
[32m[20221213 14:54:59 @agent_ppo2.py:143][0m Total time:       2.07 min
[32m[20221213 14:54:59 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 14:54:59 @agent_ppo2.py:121][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 14:54:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |           0.0029 |           0.9199 |           0.2516 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0007 |           0.8791 |           0.2514 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0071 |           0.8613 |           0.2512 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0068 |           0.8622 |           0.2512 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0111 |           0.8455 |           0.2512 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0056 |           0.8435 |           0.2511 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0069 |           0.8475 |           0.2511 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0136 |           0.8392 |           0.2509 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0060 |           0.8430 |           0.2510 |
[32m[20221213 14:55:00 @agent_ppo2.py:185][0m |          -0.0122 |           0.8252 |           0.2508 |
[32m[20221213 14:55:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:55:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 54.38
[32m[20221213 14:55:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 57.53
[32m[20221213 14:55:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 27.46
[32m[20221213 14:55:01 @agent_ppo2.py:143][0m Total time:       2.09 min
[32m[20221213 14:55:01 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 14:55:01 @agent_ppo2.py:121][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 14:55:01 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:55:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:01 @agent_ppo2.py:185][0m |          -0.0037 |           0.8977 |           0.2528 |
[32m[20221213 14:55:01 @agent_ppo2.py:185][0m |          -0.0061 |           0.8687 |           0.2526 |
[32m[20221213 14:55:01 @agent_ppo2.py:185][0m |          -0.0087 |           0.8586 |           0.2525 |
[32m[20221213 14:55:01 @agent_ppo2.py:185][0m |          -0.0104 |           0.8509 |           0.2524 |
[32m[20221213 14:55:01 @agent_ppo2.py:185][0m |          -0.0146 |           0.8473 |           0.2524 |
[32m[20221213 14:55:01 @agent_ppo2.py:185][0m |          -0.0135 |           0.8377 |           0.2524 |
[32m[20221213 14:55:02 @agent_ppo2.py:185][0m |          -0.0025 |           0.9162 |           0.2524 |
[32m[20221213 14:55:02 @agent_ppo2.py:185][0m |          -0.0149 |           0.8399 |           0.2523 |
[32m[20221213 14:55:02 @agent_ppo2.py:185][0m |          -0.0145 |           0.8252 |           0.2522 |
[32m[20221213 14:55:02 @agent_ppo2.py:185][0m |          -0.0142 |           0.8212 |           0.2524 |
[32m[20221213 14:55:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:55:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 51.66
[32m[20221213 14:55:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 53.05
[32m[20221213 14:55:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.70
[32m[20221213 14:55:02 @agent_ppo2.py:143][0m Total time:       2.11 min
[32m[20221213 14:55:02 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 14:55:02 @agent_ppo2.py:121][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 14:55:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:02 @agent_ppo2.py:185][0m |           0.0024 |           0.9333 |           0.2467 |
[32m[20221213 14:55:02 @agent_ppo2.py:185][0m |           0.0053 |           0.9617 |           0.2463 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0087 |           0.8963 |           0.2460 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0104 |           0.8863 |           0.2457 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0120 |           0.8875 |           0.2455 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0047 |           0.8818 |           0.2452 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0115 |           0.8802 |           0.2449 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0062 |           0.8749 |           0.2448 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0051 |           0.8973 |           0.2444 |
[32m[20221213 14:55:03 @agent_ppo2.py:185][0m |          -0.0148 |           0.8761 |           0.2440 |
[32m[20221213 14:55:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:55:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.38
[32m[20221213 14:55:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 75.15
[32m[20221213 14:55:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.82
[32m[20221213 14:55:03 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 14:55:03 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 14:55:03 @agent_ppo2.py:121][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 14:55:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0020 |           0.7333 |           0.2504 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0065 |           0.7216 |           0.2500 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0072 |           0.7104 |           0.2496 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0084 |           0.7109 |           0.2497 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0095 |           0.6989 |           0.2495 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0101 |           0.6981 |           0.2496 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |           0.0002 |           0.7757 |           0.2494 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0132 |           0.7056 |           0.2492 |
[32m[20221213 14:55:04 @agent_ppo2.py:185][0m |          -0.0139 |           0.6881 |           0.2494 |
[32m[20221213 14:55:05 @agent_ppo2.py:185][0m |          -0.0128 |           0.6861 |           0.2495 |
[32m[20221213 14:55:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:55:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 48.75
[32m[20221213 14:55:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 56.73
[32m[20221213 14:55:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 33.35
[32m[20221213 14:55:05 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221213 14:55:05 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 14:55:05 @agent_ppo2.py:121][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 14:55:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:05 @agent_ppo2.py:185][0m |          -0.0014 |           1.1740 |           0.2487 |
[32m[20221213 14:55:05 @agent_ppo2.py:185][0m |          -0.0047 |           1.1236 |           0.2486 |
[32m[20221213 14:55:05 @agent_ppo2.py:185][0m |          -0.0080 |           1.1019 |           0.2483 |
[32m[20221213 14:55:05 @agent_ppo2.py:185][0m |          -0.0087 |           1.0784 |           0.2482 |
[32m[20221213 14:55:05 @agent_ppo2.py:185][0m |          -0.0106 |           1.0722 |           0.2480 |
[32m[20221213 14:55:06 @agent_ppo2.py:185][0m |          -0.0068 |           1.0714 |           0.2481 |
[32m[20221213 14:55:06 @agent_ppo2.py:185][0m |          -0.0088 |           1.0481 |           0.2482 |
[32m[20221213 14:55:06 @agent_ppo2.py:185][0m |          -0.0034 |           1.0714 |           0.2482 |
[32m[20221213 14:55:06 @agent_ppo2.py:185][0m |          -0.0113 |           1.0388 |           0.2480 |
[32m[20221213 14:55:06 @agent_ppo2.py:185][0m |          -0.0149 |           1.0283 |           0.2480 |
[32m[20221213 14:55:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:55:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 68.38
[32m[20221213 14:55:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 78.28
[32m[20221213 14:55:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.60
[32m[20221213 14:55:06 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221213 14:55:06 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 14:55:06 @agent_ppo2.py:121][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 14:55:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:06 @agent_ppo2.py:185][0m |           0.0009 |           1.2213 |           0.2521 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |           0.0006 |           1.1971 |           0.2519 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0068 |           1.1814 |           0.2519 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0084 |           1.1765 |           0.2518 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0070 |           1.1814 |           0.2518 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0057 |           1.1642 |           0.2517 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0131 |           1.1565 |           0.2517 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0129 |           1.1542 |           0.2516 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0137 |           1.1484 |           0.2517 |
[32m[20221213 14:55:07 @agent_ppo2.py:185][0m |          -0.0137 |           1.1473 |           0.2515 |
[32m[20221213 14:55:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:55:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.92
[32m[20221213 14:55:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.53
[32m[20221213 14:55:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.96
[32m[20221213 14:55:07 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 79.96
[32m[20221213 14:55:07 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 79.96
[32m[20221213 14:55:07 @agent_ppo2.py:143][0m Total time:       2.21 min
[32m[20221213 14:55:07 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 14:55:07 @agent_ppo2.py:121][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 14:55:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |          -0.0030 |           1.1388 |           0.2500 |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |           0.0036 |           1.1315 |           0.2495 |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |          -0.0079 |           1.0880 |           0.2493 |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |          -0.0119 |           1.0791 |           0.2490 |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |          -0.0103 |           1.0642 |           0.2490 |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |          -0.0096 |           1.0585 |           0.2488 |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |          -0.0124 |           1.0516 |           0.2488 |
[32m[20221213 14:55:08 @agent_ppo2.py:185][0m |          -0.0140 |           1.0442 |           0.2486 |
[32m[20221213 14:55:09 @agent_ppo2.py:185][0m |          -0.0144 |           1.0376 |           0.2484 |
[32m[20221213 14:55:09 @agent_ppo2.py:185][0m |          -0.0148 |           1.0335 |           0.2485 |
[32m[20221213 14:55:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:55:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 56.47
[32m[20221213 14:55:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 70.55
[32m[20221213 14:55:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.70
[32m[20221213 14:55:09 @agent_ppo2.py:143][0m Total time:       2.23 min
[32m[20221213 14:55:09 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 14:55:09 @agent_ppo2.py:121][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 14:55:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:09 @agent_ppo2.py:185][0m |           0.0022 |           1.0856 |           0.2514 |
[32m[20221213 14:55:09 @agent_ppo2.py:185][0m |          -0.0008 |           1.1187 |           0.2514 |
[32m[20221213 14:55:09 @agent_ppo2.py:185][0m |           0.0124 |           1.1954 |           0.2515 |
[32m[20221213 14:55:09 @agent_ppo2.py:185][0m |          -0.0080 |           1.0630 |           0.2513 |
[32m[20221213 14:55:10 @agent_ppo2.py:185][0m |          -0.0104 |           1.0441 |           0.2516 |
[32m[20221213 14:55:10 @agent_ppo2.py:185][0m |          -0.0106 |           1.0420 |           0.2516 |
[32m[20221213 14:55:10 @agent_ppo2.py:185][0m |          -0.0094 |           1.0444 |           0.2516 |
[32m[20221213 14:55:10 @agent_ppo2.py:185][0m |          -0.0110 |           1.0338 |           0.2516 |
[32m[20221213 14:55:10 @agent_ppo2.py:185][0m |          -0.0127 |           1.0374 |           0.2516 |
[32m[20221213 14:55:10 @agent_ppo2.py:185][0m |          -0.0111 |           1.0339 |           0.2517 |
[32m[20221213 14:55:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:55:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 67.12
[32m[20221213 14:55:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 76.58
[32m[20221213 14:55:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 58.03
[32m[20221213 14:55:10 @agent_ppo2.py:143][0m Total time:       2.25 min
[32m[20221213 14:55:10 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 14:55:10 @agent_ppo2.py:121][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 14:55:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0019 |           1.1509 |           0.2538 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0034 |           1.1396 |           0.2538 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0088 |           1.1205 |           0.2535 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0028 |           1.1230 |           0.2535 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0093 |           1.1222 |           0.2534 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0093 |           1.0969 |           0.2535 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0025 |           1.1633 |           0.2536 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0161 |           1.0941 |           0.2536 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0123 |           1.0883 |           0.2537 |
[32m[20221213 14:55:11 @agent_ppo2.py:185][0m |          -0.0162 |           1.0886 |           0.2538 |
[32m[20221213 14:55:11 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:55:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.10
[32m[20221213 14:55:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 78.46
[32m[20221213 14:55:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.00
[32m[20221213 14:55:12 @agent_ppo2.py:143][0m Total time:       2.27 min
[32m[20221213 14:55:12 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 14:55:12 @agent_ppo2.py:121][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 14:55:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:12 @agent_ppo2.py:185][0m |          -0.0032 |           1.2828 |           0.2524 |
[32m[20221213 14:55:12 @agent_ppo2.py:185][0m |          -0.0029 |           1.2565 |           0.2523 |
[32m[20221213 14:55:12 @agent_ppo2.py:185][0m |          -0.0075 |           1.2527 |           0.2522 |
[32m[20221213 14:55:12 @agent_ppo2.py:185][0m |          -0.0049 |           1.2392 |           0.2522 |
[32m[20221213 14:55:12 @agent_ppo2.py:185][0m |          -0.0098 |           1.2225 |           0.2520 |
[32m[20221213 14:55:12 @agent_ppo2.py:185][0m |          -0.0073 |           1.2162 |           0.2519 |
[32m[20221213 14:55:12 @agent_ppo2.py:185][0m |          -0.0115 |           1.2040 |           0.2519 |
[32m[20221213 14:55:13 @agent_ppo2.py:185][0m |          -0.0111 |           1.2041 |           0.2519 |
[32m[20221213 14:55:13 @agent_ppo2.py:185][0m |          -0.0088 |           1.1937 |           0.2519 |
[32m[20221213 14:55:13 @agent_ppo2.py:185][0m |          -0.0153 |           1.1939 |           0.2519 |
[32m[20221213 14:55:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:55:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.24
[32m[20221213 14:55:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 74.07
[32m[20221213 14:55:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.12
[32m[20221213 14:55:13 @agent_ppo2.py:143][0m Total time:       2.30 min
[32m[20221213 14:55:13 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 14:55:13 @agent_ppo2.py:121][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 14:55:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:13 @agent_ppo2.py:185][0m |          -0.0032 |           1.1839 |           0.2423 |
[32m[20221213 14:55:13 @agent_ppo2.py:185][0m |          -0.0088 |           1.1727 |           0.2428 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |          -0.0099 |           1.1555 |           0.2429 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |          -0.0098 |           1.1455 |           0.2431 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |          -0.0106 |           1.1322 |           0.2431 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |          -0.0086 |           1.1261 |           0.2430 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |          -0.0141 |           1.1160 |           0.2432 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |          -0.0146 |           1.1082 |           0.2433 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |          -0.0179 |           1.1043 |           0.2434 |
[32m[20221213 14:55:14 @agent_ppo2.py:185][0m |           0.0016 |           1.1839 |           0.2435 |
[32m[20221213 14:55:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 14:55:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 61.38
[32m[20221213 14:55:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 68.09
[32m[20221213 14:55:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.28
[32m[20221213 14:55:14 @agent_ppo2.py:143][0m Total time:       2.32 min
[32m[20221213 14:55:14 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 14:55:14 @agent_ppo2.py:121][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 14:55:15 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:55:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0013 |           1.3667 |           0.2602 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0035 |           1.2949 |           0.2602 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0005 |           1.3049 |           0.2601 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0083 |           1.2786 |           0.2599 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |           0.0042 |           1.3743 |           0.2600 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0097 |           1.2680 |           0.2600 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0142 |           1.2509 |           0.2599 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0110 |           1.2509 |           0.2598 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0140 |           1.2440 |           0.2598 |
[32m[20221213 14:55:15 @agent_ppo2.py:185][0m |          -0.0141 |           1.2304 |           0.2598 |
[32m[20221213 14:55:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:55:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 64.48
[32m[20221213 14:55:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 69.08
[32m[20221213 14:55:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.74
[32m[20221213 14:55:16 @agent_ppo2.py:143][0m Total time:       2.34 min
[32m[20221213 14:55:16 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 14:55:16 @agent_ppo2.py:121][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 14:55:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:16 @agent_ppo2.py:185][0m |          -0.0017 |           1.2226 |           0.2617 |
[32m[20221213 14:55:16 @agent_ppo2.py:185][0m |          -0.0041 |           1.1884 |           0.2615 |
[32m[20221213 14:55:16 @agent_ppo2.py:185][0m |          -0.0105 |           1.1766 |           0.2609 |
[32m[20221213 14:55:16 @agent_ppo2.py:185][0m |          -0.0033 |           1.2541 |           0.2608 |
[32m[20221213 14:55:16 @agent_ppo2.py:185][0m |          -0.0107 |           1.1556 |           0.2606 |
[32m[20221213 14:55:16 @agent_ppo2.py:185][0m |          -0.0151 |           1.1526 |           0.2604 |
[32m[20221213 14:55:17 @agent_ppo2.py:185][0m |          -0.0179 |           1.1451 |           0.2604 |
[32m[20221213 14:55:17 @agent_ppo2.py:185][0m |          -0.0054 |           1.2107 |           0.2604 |
[32m[20221213 14:55:17 @agent_ppo2.py:185][0m |          -0.0040 |           1.2508 |           0.2602 |
[32m[20221213 14:55:17 @agent_ppo2.py:185][0m |          -0.0089 |           1.1307 |           0.2601 |
[32m[20221213 14:55:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 63.65
[32m[20221213 14:55:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 78.46
[32m[20221213 14:55:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 40.72
[32m[20221213 14:55:17 @agent_ppo2.py:143][0m Total time:       2.36 min
[32m[20221213 14:55:17 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 14:55:17 @agent_ppo2.py:121][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 14:55:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:17 @agent_ppo2.py:185][0m |           0.0007 |           1.5066 |           0.2580 |
[32m[20221213 14:55:17 @agent_ppo2.py:185][0m |          -0.0088 |           1.4512 |           0.2577 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0151 |           1.4345 |           0.2576 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0118 |           1.4159 |           0.2580 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0109 |           1.3949 |           0.2580 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0141 |           1.3929 |           0.2582 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0115 |           1.3762 |           0.2583 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0126 |           1.3742 |           0.2585 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0137 |           1.3689 |           0.2587 |
[32m[20221213 14:55:18 @agent_ppo2.py:185][0m |          -0.0173 |           1.3633 |           0.2589 |
[32m[20221213 14:55:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.95
[32m[20221213 14:55:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 79.82
[32m[20221213 14:55:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 52.07
[32m[20221213 14:55:18 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221213 14:55:18 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 14:55:18 @agent_ppo2.py:121][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 14:55:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0019 |           1.5445 |           0.2629 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0070 |           1.4903 |           0.2625 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0084 |           1.4687 |           0.2624 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0139 |           1.4511 |           0.2625 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0075 |           1.4353 |           0.2624 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0115 |           1.4169 |           0.2625 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0113 |           1.4441 |           0.2624 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0104 |           1.4196 |           0.2626 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0146 |           1.3901 |           0.2624 |
[32m[20221213 14:55:19 @agent_ppo2.py:185][0m |          -0.0080 |           1.3972 |           0.2625 |
[32m[20221213 14:55:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.76
[32m[20221213 14:55:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 79.23
[32m[20221213 14:55:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 69.39
[32m[20221213 14:55:20 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 14:55:20 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 14:55:20 @agent_ppo2.py:121][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 14:55:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:20 @agent_ppo2.py:185][0m |          -0.0048 |           1.4645 |           0.2642 |
[32m[20221213 14:55:20 @agent_ppo2.py:185][0m |           0.0045 |           1.5235 |           0.2640 |
[32m[20221213 14:55:20 @agent_ppo2.py:185][0m |          -0.0097 |           1.4149 |           0.2634 |
[32m[20221213 14:55:20 @agent_ppo2.py:185][0m |          -0.0083 |           1.3919 |           0.2635 |
[32m[20221213 14:55:20 @agent_ppo2.py:185][0m |          -0.0150 |           1.3908 |           0.2635 |
[32m[20221213 14:55:21 @agent_ppo2.py:185][0m |          -0.0132 |           1.3886 |           0.2636 |
[32m[20221213 14:55:21 @agent_ppo2.py:185][0m |          -0.0060 |           1.4573 |           0.2637 |
[32m[20221213 14:55:21 @agent_ppo2.py:185][0m |          -0.0159 |           1.3770 |           0.2637 |
[32m[20221213 14:55:21 @agent_ppo2.py:185][0m |          -0.0173 |           1.3704 |           0.2637 |
[32m[20221213 14:55:21 @agent_ppo2.py:185][0m |          -0.0145 |           1.3554 |           0.2637 |
[32m[20221213 14:55:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.54
[32m[20221213 14:55:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.26
[32m[20221213 14:55:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.84
[32m[20221213 14:55:21 @agent_ppo2.py:143][0m Total time:       2.43 min
[32m[20221213 14:55:21 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 14:55:21 @agent_ppo2.py:121][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 14:55:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:21 @agent_ppo2.py:185][0m |           0.0018 |           1.4246 |           0.2637 |
[32m[20221213 14:55:21 @agent_ppo2.py:185][0m |          -0.0093 |           1.3609 |           0.2635 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0127 |           1.3487 |           0.2632 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0056 |           1.3383 |           0.2632 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0138 |           1.3216 |           0.2633 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0125 |           1.3235 |           0.2632 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0118 |           1.3103 |           0.2632 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0124 |           1.3049 |           0.2633 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0184 |           1.2962 |           0.2634 |
[32m[20221213 14:55:22 @agent_ppo2.py:185][0m |          -0.0057 |           1.3637 |           0.2636 |
[32m[20221213 14:55:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:55:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.47
[32m[20221213 14:55:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 74.79
[32m[20221213 14:55:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.04
[32m[20221213 14:55:22 @agent_ppo2.py:143][0m Total time:       2.45 min
[32m[20221213 14:55:22 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 14:55:22 @agent_ppo2.py:121][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 14:55:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |           0.0011 |           1.4370 |           0.2636 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |          -0.0041 |           1.3909 |           0.2636 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |          -0.0076 |           1.3646 |           0.2635 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |          -0.0084 |           1.3510 |           0.2638 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |           0.0012 |           1.4159 |           0.2637 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |           0.0011 |           1.4130 |           0.2640 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |          -0.0136 |           1.3285 |           0.2638 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |          -0.0007 |           1.4356 |           0.2640 |
[32m[20221213 14:55:23 @agent_ppo2.py:185][0m |          -0.0145 |           1.3068 |           0.2639 |
[32m[20221213 14:55:24 @agent_ppo2.py:185][0m |          -0.0124 |           1.2938 |           0.2640 |
[32m[20221213 14:55:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:55:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 72.67
[32m[20221213 14:55:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.93
[32m[20221213 14:55:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.82
[32m[20221213 14:55:24 @agent_ppo2.py:143][0m Total time:       2.48 min
[32m[20221213 14:55:24 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 14:55:24 @agent_ppo2.py:121][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 14:55:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:24 @agent_ppo2.py:185][0m |           0.0028 |           1.3688 |           0.2737 |
[32m[20221213 14:55:24 @agent_ppo2.py:185][0m |           0.0009 |           1.3730 |           0.2734 |
[32m[20221213 14:55:24 @agent_ppo2.py:185][0m |          -0.0112 |           1.3044 |           0.2730 |
[32m[20221213 14:55:24 @agent_ppo2.py:185][0m |          -0.0138 |           1.2837 |           0.2725 |
[32m[20221213 14:55:24 @agent_ppo2.py:185][0m |          -0.0146 |           1.2811 |           0.2721 |
[32m[20221213 14:55:25 @agent_ppo2.py:185][0m |          -0.0165 |           1.2611 |           0.2718 |
[32m[20221213 14:55:25 @agent_ppo2.py:185][0m |          -0.0134 |           1.2544 |           0.2718 |
[32m[20221213 14:55:25 @agent_ppo2.py:185][0m |          -0.0144 |           1.2566 |           0.2715 |
[32m[20221213 14:55:25 @agent_ppo2.py:185][0m |          -0.0128 |           1.2549 |           0.2714 |
[32m[20221213 14:55:25 @agent_ppo2.py:185][0m |          -0.0153 |           1.2310 |           0.2712 |
[32m[20221213 14:55:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:55:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 65.47
[32m[20221213 14:55:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 72.28
[32m[20221213 14:55:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.52
[32m[20221213 14:55:25 @agent_ppo2.py:143][0m Total time:       2.50 min
[32m[20221213 14:55:25 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221213 14:55:25 @agent_ppo2.py:121][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 14:55:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:25 @agent_ppo2.py:185][0m |           0.0000 |           1.4687 |           0.2652 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0044 |           1.4408 |           0.2650 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0076 |           1.4182 |           0.2650 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0108 |           1.4134 |           0.2649 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0061 |           1.3986 |           0.2649 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0065 |           1.4033 |           0.2648 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0123 |           1.3829 |           0.2651 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0098 |           1.3738 |           0.2651 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0131 |           1.3695 |           0.2652 |
[32m[20221213 14:55:26 @agent_ppo2.py:185][0m |          -0.0135 |           1.3630 |           0.2653 |
[32m[20221213 14:55:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:55:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.77
[32m[20221213 14:55:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 79.40
[32m[20221213 14:55:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.48
[32m[20221213 14:55:26 @agent_ppo2.py:143][0m Total time:       2.52 min
[32m[20221213 14:55:26 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221213 14:55:26 @agent_ppo2.py:121][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 14:55:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |           0.0006 |           1.4138 |           0.2672 |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |          -0.0051 |           1.3836 |           0.2669 |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |          -0.0036 |           1.3759 |           0.2667 |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |          -0.0131 |           1.3595 |           0.2668 |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |          -0.0098 |           1.3590 |           0.2669 |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |          -0.0132 |           1.3528 |           0.2669 |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |          -0.0076 |           1.3629 |           0.2668 |
[32m[20221213 14:55:27 @agent_ppo2.py:185][0m |          -0.0138 |           1.3467 |           0.2669 |
[32m[20221213 14:55:28 @agent_ppo2.py:185][0m |          -0.0126 |           1.3310 |           0.2668 |
[32m[20221213 14:55:28 @agent_ppo2.py:185][0m |          -0.0128 |           1.3309 |           0.2670 |
[32m[20221213 14:55:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:55:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.47
[32m[20221213 14:55:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 80.13
[32m[20221213 14:55:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.93
[32m[20221213 14:55:28 @agent_ppo2.py:143][0m Total time:       2.54 min
[32m[20221213 14:55:28 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221213 14:55:28 @agent_ppo2.py:121][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 14:55:28 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:55:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:28 @agent_ppo2.py:185][0m |          -0.0011 |           1.4756 |           0.2702 |
[32m[20221213 14:55:28 @agent_ppo2.py:185][0m |           0.0004 |           1.4606 |           0.2700 |
[32m[20221213 14:55:28 @agent_ppo2.py:185][0m |          -0.0078 |           1.4377 |           0.2698 |
[32m[20221213 14:55:28 @agent_ppo2.py:185][0m |          -0.0055 |           1.4621 |           0.2695 |
[32m[20221213 14:55:29 @agent_ppo2.py:185][0m |          -0.0135 |           1.4265 |           0.2694 |
[32m[20221213 14:55:29 @agent_ppo2.py:185][0m |          -0.0142 |           1.4231 |           0.2691 |
[32m[20221213 14:55:29 @agent_ppo2.py:185][0m |          -0.0115 |           1.4289 |           0.2692 |
[32m[20221213 14:55:29 @agent_ppo2.py:185][0m |          -0.0006 |           1.5816 |           0.2691 |
[32m[20221213 14:55:29 @agent_ppo2.py:185][0m |          -0.0138 |           1.4196 |           0.2690 |
[32m[20221213 14:55:29 @agent_ppo2.py:185][0m |          -0.0098 |           1.4131 |           0.2690 |
[32m[20221213 14:55:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.70
[32m[20221213 14:55:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.12
[32m[20221213 14:55:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.35
[32m[20221213 14:55:29 @agent_ppo2.py:143][0m Total time:       2.57 min
[32m[20221213 14:55:29 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221213 14:55:29 @agent_ppo2.py:121][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 14:55:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0005 |           1.5369 |           0.2624 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0080 |           1.5155 |           0.2626 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0096 |           1.5060 |           0.2628 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0099 |           1.4908 |           0.2627 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0013 |           1.6248 |           0.2627 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0138 |           1.4937 |           0.2628 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0141 |           1.4802 |           0.2628 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0126 |           1.4760 |           0.2629 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0158 |           1.4741 |           0.2630 |
[32m[20221213 14:55:30 @agent_ppo2.py:185][0m |          -0.0140 |           1.4677 |           0.2630 |
[32m[20221213 14:55:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:55:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 76.60
[32m[20221213 14:55:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 80.92
[32m[20221213 14:55:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.77
[32m[20221213 14:55:31 @agent_ppo2.py:143][0m Total time:       2.59 min
[32m[20221213 14:55:31 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221213 14:55:31 @agent_ppo2.py:121][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 14:55:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |           0.0029 |           1.5302 |           0.2748 |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |          -0.0070 |           1.4980 |           0.2746 |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |          -0.0073 |           1.4945 |           0.2742 |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |          -0.0105 |           1.4865 |           0.2739 |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |          -0.0051 |           1.5276 |           0.2738 |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |          -0.0130 |           1.4815 |           0.2735 |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |          -0.0135 |           1.4806 |           0.2732 |
[32m[20221213 14:55:31 @agent_ppo2.py:185][0m |          -0.0103 |           1.4756 |           0.2732 |
[32m[20221213 14:55:32 @agent_ppo2.py:185][0m |          -0.0136 |           1.4666 |           0.2731 |
[32m[20221213 14:55:32 @agent_ppo2.py:185][0m |          -0.0123 |           1.4782 |           0.2730 |
[32m[20221213 14:55:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 74.39
[32m[20221213 14:55:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.46
[32m[20221213 14:55:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.21
[32m[20221213 14:55:32 @agent_ppo2.py:143][0m Total time:       2.61 min
[32m[20221213 14:55:32 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221213 14:55:32 @agent_ppo2.py:121][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 14:55:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:32 @agent_ppo2.py:185][0m |          -0.0020 |           1.5479 |           0.2660 |
[32m[20221213 14:55:32 @agent_ppo2.py:185][0m |          -0.0118 |           1.5204 |           0.2657 |
[32m[20221213 14:55:32 @agent_ppo2.py:185][0m |          -0.0122 |           1.4993 |           0.2657 |
[32m[20221213 14:55:32 @agent_ppo2.py:185][0m |          -0.0147 |           1.4924 |           0.2659 |
[32m[20221213 14:55:33 @agent_ppo2.py:185][0m |          -0.0122 |           1.4884 |           0.2659 |
[32m[20221213 14:55:33 @agent_ppo2.py:185][0m |          -0.0152 |           1.4749 |           0.2661 |
[32m[20221213 14:55:33 @agent_ppo2.py:185][0m |          -0.0139 |           1.4766 |           0.2660 |
[32m[20221213 14:55:33 @agent_ppo2.py:185][0m |          -0.0150 |           1.4660 |           0.2661 |
[32m[20221213 14:55:33 @agent_ppo2.py:185][0m |          -0.0178 |           1.4603 |           0.2662 |
[32m[20221213 14:55:33 @agent_ppo2.py:185][0m |          -0.0145 |           1.4553 |           0.2663 |
[32m[20221213 14:55:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.14
[32m[20221213 14:55:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.95
[32m[20221213 14:55:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.35
[32m[20221213 14:55:33 @agent_ppo2.py:143][0m Total time:       2.63 min
[32m[20221213 14:55:33 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221213 14:55:33 @agent_ppo2.py:121][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 14:55:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0006 |           1.6491 |           0.2666 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0111 |           1.6209 |           0.2660 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0107 |           1.6068 |           0.2657 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0132 |           1.6038 |           0.2657 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0127 |           1.5956 |           0.2656 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0131 |           1.6036 |           0.2657 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0077 |           1.7017 |           0.2657 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0203 |           1.5917 |           0.2654 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0119 |           1.5803 |           0.2656 |
[32m[20221213 14:55:34 @agent_ppo2.py:185][0m |          -0.0165 |           1.5621 |           0.2655 |
[32m[20221213 14:55:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 76.51
[32m[20221213 14:55:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.70
[32m[20221213 14:55:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.39
[32m[20221213 14:55:35 @agent_ppo2.py:143][0m Total time:       2.66 min
[32m[20221213 14:55:35 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221213 14:55:35 @agent_ppo2.py:121][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 14:55:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0024 |           1.6130 |           0.2668 |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0079 |           1.5427 |           0.2669 |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0098 |           1.5175 |           0.2668 |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0090 |           1.5021 |           0.2670 |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0153 |           1.4901 |           0.2670 |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0120 |           1.4879 |           0.2670 |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0042 |           1.5649 |           0.2670 |
[32m[20221213 14:55:35 @agent_ppo2.py:185][0m |          -0.0151 |           1.4773 |           0.2667 |
[32m[20221213 14:55:36 @agent_ppo2.py:185][0m |          -0.0150 |           1.4652 |           0.2670 |
[32m[20221213 14:55:36 @agent_ppo2.py:185][0m |          -0.0113 |           1.4924 |           0.2667 |
[32m[20221213 14:55:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.40
[32m[20221213 14:55:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.69
[32m[20221213 14:55:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.17
[32m[20221213 14:55:36 @agent_ppo2.py:143][0m Total time:       2.68 min
[32m[20221213 14:55:36 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221213 14:55:36 @agent_ppo2.py:121][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 14:55:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:36 @agent_ppo2.py:185][0m |          -0.0007 |           1.5983 |           0.2709 |
[32m[20221213 14:55:36 @agent_ppo2.py:185][0m |           0.0027 |           1.6465 |           0.2706 |
[32m[20221213 14:55:36 @agent_ppo2.py:185][0m |          -0.0084 |           1.5020 |           0.2707 |
[32m[20221213 14:55:36 @agent_ppo2.py:185][0m |          -0.0008 |           1.6402 |           0.2704 |
[32m[20221213 14:55:37 @agent_ppo2.py:185][0m |          -0.0100 |           1.4796 |           0.2701 |
[32m[20221213 14:55:37 @agent_ppo2.py:185][0m |          -0.0148 |           1.4493 |           0.2702 |
[32m[20221213 14:55:37 @agent_ppo2.py:185][0m |          -0.0039 |           1.6221 |           0.2702 |
[32m[20221213 14:55:37 @agent_ppo2.py:185][0m |          -0.0214 |           1.4572 |           0.2702 |
[32m[20221213 14:55:37 @agent_ppo2.py:185][0m |          -0.0186 |           1.4261 |           0.2704 |
[32m[20221213 14:55:37 @agent_ppo2.py:185][0m |          -0.0138 |           1.4159 |           0.2703 |
[32m[20221213 14:55:37 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:55:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.01
[32m[20221213 14:55:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.48
[32m[20221213 14:55:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.58
[32m[20221213 14:55:37 @agent_ppo2.py:143][0m Total time:       2.70 min
[32m[20221213 14:55:37 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221213 14:55:37 @agent_ppo2.py:121][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 14:55:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0039 |           1.8505 |           0.2729 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0053 |           1.8073 |           0.2728 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0056 |           1.7919 |           0.2728 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0067 |           1.7771 |           0.2727 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0065 |           1.7793 |           0.2729 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0153 |           1.7591 |           0.2728 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0075 |           1.8075 |           0.2729 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0126 |           1.7494 |           0.2728 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0189 |           1.7424 |           0.2729 |
[32m[20221213 14:55:38 @agent_ppo2.py:185][0m |          -0.0150 |           1.7395 |           0.2731 |
[32m[20221213 14:55:38 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 14:55:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.72
[32m[20221213 14:55:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.30
[32m[20221213 14:55:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.61
[32m[20221213 14:55:39 @agent_ppo2.py:143][0m Total time:       2.72 min
[32m[20221213 14:55:39 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221213 14:55:39 @agent_ppo2.py:121][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 14:55:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:39 @agent_ppo2.py:185][0m |          -0.0002 |           1.7676 |           0.2718 |
[32m[20221213 14:55:39 @agent_ppo2.py:185][0m |           0.0012 |           1.7624 |           0.2712 |
[32m[20221213 14:55:39 @agent_ppo2.py:185][0m |          -0.0011 |           1.7375 |           0.2712 |
[32m[20221213 14:55:39 @agent_ppo2.py:185][0m |          -0.0144 |           1.6170 |           0.2707 |
[32m[20221213 14:55:39 @agent_ppo2.py:185][0m |          -0.0138 |           1.5971 |           0.2708 |
[32m[20221213 14:55:39 @agent_ppo2.py:185][0m |          -0.0136 |           1.6069 |           0.2708 |
[32m[20221213 14:55:40 @agent_ppo2.py:185][0m |          -0.0173 |           1.5729 |           0.2708 |
[32m[20221213 14:55:40 @agent_ppo2.py:185][0m |          -0.0184 |           1.5717 |           0.2707 |
[32m[20221213 14:55:40 @agent_ppo2.py:185][0m |          -0.0171 |           1.5609 |           0.2707 |
[32m[20221213 14:55:40 @agent_ppo2.py:185][0m |          -0.0187 |           1.5562 |           0.2707 |
[32m[20221213 14:55:40 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 14:55:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 76.64
[32m[20221213 14:55:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.71
[32m[20221213 14:55:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.83
[32m[20221213 14:55:40 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 84.83
[32m[20221213 14:55:40 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 84.83
[32m[20221213 14:55:40 @agent_ppo2.py:143][0m Total time:       2.75 min
[32m[20221213 14:55:40 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221213 14:55:40 @agent_ppo2.py:121][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 14:55:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:40 @agent_ppo2.py:185][0m |          -0.0022 |           1.5823 |           0.2778 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0020 |           1.5359 |           0.2777 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0078 |           1.5298 |           0.2776 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0103 |           1.5149 |           0.2775 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0140 |           1.5095 |           0.2775 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0142 |           1.5001 |           0.2777 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0059 |           1.5914 |           0.2776 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0148 |           1.5001 |           0.2777 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0156 |           1.4872 |           0.2778 |
[32m[20221213 14:55:41 @agent_ppo2.py:185][0m |          -0.0074 |           1.5785 |           0.2780 |
[32m[20221213 14:55:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:55:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.01
[32m[20221213 14:55:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.05
[32m[20221213 14:55:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.51
[32m[20221213 14:55:41 @agent_ppo2.py:143][0m Total time:       2.77 min
[32m[20221213 14:55:41 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221213 14:55:41 @agent_ppo2.py:121][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 14:55:42 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:55:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |           0.0014 |           1.7761 |           0.2798 |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |          -0.0027 |           1.7218 |           0.2800 |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |          -0.0085 |           1.7149 |           0.2798 |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |          -0.0065 |           1.7004 |           0.2799 |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |          -0.0114 |           1.6931 |           0.2800 |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |          -0.0084 |           1.6840 |           0.2801 |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |          -0.0120 |           1.6838 |           0.2801 |
[32m[20221213 14:55:42 @agent_ppo2.py:185][0m |          -0.0088 |           1.7211 |           0.2802 |
[32m[20221213 14:55:43 @agent_ppo2.py:185][0m |          -0.0133 |           1.6799 |           0.2802 |
[32m[20221213 14:55:43 @agent_ppo2.py:185][0m |          -0.0147 |           1.6678 |           0.2804 |
[32m[20221213 14:55:43 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 14:55:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.25
[32m[20221213 14:55:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.13
[32m[20221213 14:55:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.63
[32m[20221213 14:55:43 @agent_ppo2.py:143][0m Total time:       2.79 min
[32m[20221213 14:55:43 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221213 14:55:43 @agent_ppo2.py:121][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 14:55:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:55:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:43 @agent_ppo2.py:185][0m |          -0.0019 |           1.7635 |           0.2898 |
[32m[20221213 14:55:43 @agent_ppo2.py:185][0m |           0.0018 |           1.7204 |           0.2897 |
[32m[20221213 14:55:43 @agent_ppo2.py:185][0m |          -0.0103 |           1.6001 |           0.2896 |
[32m[20221213 14:55:44 @agent_ppo2.py:185][0m |          -0.0099 |           1.5739 |           0.2898 |
[32m[20221213 14:55:44 @agent_ppo2.py:185][0m |          -0.0147 |           1.5473 |           0.2897 |
[32m[20221213 14:55:44 @agent_ppo2.py:185][0m |          -0.0081 |           1.5596 |           0.2897 |
[32m[20221213 14:55:44 @agent_ppo2.py:185][0m |          -0.0211 |           1.5190 |           0.2898 |
[32m[20221213 14:55:44 @agent_ppo2.py:185][0m |          -0.0184 |           1.5103 |           0.2897 |
[32m[20221213 14:55:44 @agent_ppo2.py:185][0m |          -0.0124 |           1.4894 |           0.2898 |
[32m[20221213 14:55:44 @agent_ppo2.py:185][0m |          -0.0150 |           1.4796 |           0.2898 |
[32m[20221213 14:55:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:55:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.42
[32m[20221213 14:55:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.36
[32m[20221213 14:55:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.62
[32m[20221213 14:55:44 @agent_ppo2.py:143][0m Total time:       2.82 min
[32m[20221213 14:55:44 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221213 14:55:44 @agent_ppo2.py:121][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 14:55:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0011 |           1.9897 |           0.2867 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0062 |           1.9318 |           0.2867 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0054 |           1.9034 |           0.2864 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0087 |           1.8772 |           0.2863 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |           0.0061 |           2.0431 |           0.2864 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0103 |           1.8677 |           0.2863 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0095 |           1.8328 |           0.2866 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0137 |           1.8190 |           0.2865 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0111 |           1.8146 |           0.2865 |
[32m[20221213 14:55:45 @agent_ppo2.py:185][0m |          -0.0122 |           1.8229 |           0.2866 |
[32m[20221213 14:55:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 14:55:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.03
[32m[20221213 14:55:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.90
[32m[20221213 14:55:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.25
[32m[20221213 14:55:46 @agent_ppo2.py:143][0m Total time:       2.84 min
[32m[20221213 14:55:46 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221213 14:55:46 @agent_ppo2.py:121][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 14:55:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:46 @agent_ppo2.py:185][0m |          -0.0030 |           1.8271 |           0.2845 |
[32m[20221213 14:55:46 @agent_ppo2.py:185][0m |           0.0015 |           1.8672 |           0.2841 |
[32m[20221213 14:55:46 @agent_ppo2.py:185][0m |          -0.0062 |           1.8121 |           0.2837 |
[32m[20221213 14:55:46 @agent_ppo2.py:185][0m |          -0.0120 |           1.7647 |           0.2836 |
[32m[20221213 14:55:46 @agent_ppo2.py:185][0m |          -0.0051 |           1.8010 |           0.2834 |
[32m[20221213 14:55:46 @agent_ppo2.py:185][0m |          -0.0153 |           1.7498 |           0.2834 |
[32m[20221213 14:55:47 @agent_ppo2.py:185][0m |          -0.0146 |           1.7404 |           0.2832 |
[32m[20221213 14:55:47 @agent_ppo2.py:185][0m |          -0.0124 |           1.7411 |           0.2833 |
[32m[20221213 14:55:47 @agent_ppo2.py:185][0m |          -0.0093 |           1.7628 |           0.2830 |
[32m[20221213 14:55:47 @agent_ppo2.py:185][0m |          -0.0161 |           1.7267 |           0.2829 |
[32m[20221213 14:55:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.29
[32m[20221213 14:55:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 79.12
[32m[20221213 14:55:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.99
[32m[20221213 14:55:47 @agent_ppo2.py:143][0m Total time:       2.86 min
[32m[20221213 14:55:47 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221213 14:55:47 @agent_ppo2.py:121][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 14:55:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:47 @agent_ppo2.py:185][0m |           0.0088 |           1.7925 |           0.2862 |
[32m[20221213 14:55:47 @agent_ppo2.py:185][0m |          -0.0059 |           1.5983 |           0.2855 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0073 |           1.5815 |           0.2852 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0111 |           1.5820 |           0.2852 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0094 |           1.5624 |           0.2851 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0087 |           1.5659 |           0.2852 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0112 |           1.5494 |           0.2851 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0061 |           1.5611 |           0.2850 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0155 |           1.5395 |           0.2850 |
[32m[20221213 14:55:48 @agent_ppo2.py:185][0m |          -0.0124 |           1.5393 |           0.2848 |
[32m[20221213 14:55:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:55:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.97
[32m[20221213 14:55:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.54
[32m[20221213 14:55:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.12
[32m[20221213 14:55:48 @agent_ppo2.py:143][0m Total time:       2.89 min
[32m[20221213 14:55:48 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221213 14:55:48 @agent_ppo2.py:121][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 14:55:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0008 |           1.8333 |           0.2841 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0051 |           1.7940 |           0.2841 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0064 |           1.8198 |           0.2839 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0114 |           1.7754 |           0.2840 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0144 |           1.7628 |           0.2840 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0139 |           1.7549 |           0.2841 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0099 |           1.7554 |           0.2842 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0137 |           1.7456 |           0.2843 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |           0.0035 |           2.0652 |           0.2843 |
[32m[20221213 14:55:49 @agent_ppo2.py:185][0m |          -0.0195 |           1.7665 |           0.2843 |
[32m[20221213 14:55:49 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:55:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.83
[32m[20221213 14:55:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.25
[32m[20221213 14:55:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.38
[32m[20221213 14:55:50 @agent_ppo2.py:143][0m Total time:       2.91 min
[32m[20221213 14:55:50 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221213 14:55:50 @agent_ppo2.py:121][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 14:55:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:50 @agent_ppo2.py:185][0m |          -0.0016 |           1.7385 |           0.2917 |
[32m[20221213 14:55:50 @agent_ppo2.py:185][0m |          -0.0053 |           1.6935 |           0.2914 |
[32m[20221213 14:55:50 @agent_ppo2.py:185][0m |          -0.0073 |           1.6817 |           0.2910 |
[32m[20221213 14:55:50 @agent_ppo2.py:185][0m |          -0.0071 |           1.6751 |           0.2910 |
[32m[20221213 14:55:50 @agent_ppo2.py:185][0m |          -0.0106 |           1.6620 |           0.2909 |
[32m[20221213 14:55:50 @agent_ppo2.py:185][0m |          -0.0095 |           1.6544 |           0.2909 |
[32m[20221213 14:55:51 @agent_ppo2.py:185][0m |          -0.0087 |           1.6569 |           0.2907 |
[32m[20221213 14:55:51 @agent_ppo2.py:185][0m |          -0.0142 |           1.6546 |           0.2906 |
[32m[20221213 14:55:51 @agent_ppo2.py:185][0m |          -0.0107 |           1.6586 |           0.2906 |
[32m[20221213 14:55:51 @agent_ppo2.py:185][0m |          -0.0131 |           1.6351 |           0.2903 |
[32m[20221213 14:55:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.77
[32m[20221213 14:55:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.48
[32m[20221213 14:55:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.01
[32m[20221213 14:55:51 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 85.01
[32m[20221213 14:55:51 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 85.01
[32m[20221213 14:55:51 @agent_ppo2.py:143][0m Total time:       2.93 min
[32m[20221213 14:55:51 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221213 14:55:51 @agent_ppo2.py:121][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 14:55:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:51 @agent_ppo2.py:185][0m |          -0.0016 |           1.7171 |           0.2788 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0047 |           1.6972 |           0.2785 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0068 |           1.6905 |           0.2784 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0080 |           1.6802 |           0.2783 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0056 |           1.6768 |           0.2779 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0122 |           1.6710 |           0.2780 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0096 |           1.6620 |           0.2780 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0107 |           1.6591 |           0.2778 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0128 |           1.6530 |           0.2778 |
[32m[20221213 14:55:52 @agent_ppo2.py:185][0m |          -0.0107 |           1.6593 |           0.2777 |
[32m[20221213 14:55:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:55:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.25
[32m[20221213 14:55:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.30
[32m[20221213 14:55:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.62
[32m[20221213 14:55:52 @agent_ppo2.py:143][0m Total time:       2.95 min
[32m[20221213 14:55:52 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221213 14:55:52 @agent_ppo2.py:121][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 14:55:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0032 |           1.7177 |           0.2832 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0029 |           1.6880 |           0.2829 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0113 |           1.6540 |           0.2826 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0117 |           1.6495 |           0.2825 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0138 |           1.6309 |           0.2823 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0132 |           1.6209 |           0.2820 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0131 |           1.6106 |           0.2820 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0053 |           1.6665 |           0.2818 |
[32m[20221213 14:55:53 @agent_ppo2.py:185][0m |          -0.0158 |           1.6065 |           0.2816 |
[32m[20221213 14:55:54 @agent_ppo2.py:185][0m |          -0.0168 |           1.5824 |           0.2816 |
[32m[20221213 14:55:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.38
[32m[20221213 14:55:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.83
[32m[20221213 14:55:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.48
[32m[20221213 14:55:54 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 90.48
[32m[20221213 14:55:54 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 90.48
[32m[20221213 14:55:54 @agent_ppo2.py:143][0m Total time:       2.98 min
[32m[20221213 14:55:54 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221213 14:55:54 @agent_ppo2.py:121][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 14:55:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:54 @agent_ppo2.py:185][0m |           0.0026 |           1.7935 |           0.2912 |
[32m[20221213 14:55:54 @agent_ppo2.py:185][0m |          -0.0058 |           1.7446 |           0.2907 |
[32m[20221213 14:55:54 @agent_ppo2.py:185][0m |          -0.0075 |           1.7309 |           0.2902 |
[32m[20221213 14:55:54 @agent_ppo2.py:185][0m |          -0.0079 |           1.7191 |           0.2896 |
[32m[20221213 14:55:54 @agent_ppo2.py:185][0m |          -0.0096 |           1.7140 |           0.2891 |
[32m[20221213 14:55:55 @agent_ppo2.py:185][0m |          -0.0108 |           1.7079 |           0.2890 |
[32m[20221213 14:55:55 @agent_ppo2.py:185][0m |          -0.0020 |           1.7702 |           0.2886 |
[32m[20221213 14:55:55 @agent_ppo2.py:185][0m |          -0.0070 |           1.7236 |           0.2883 |
[32m[20221213 14:55:55 @agent_ppo2.py:185][0m |          -0.0102 |           1.6967 |           0.2880 |
[32m[20221213 14:55:55 @agent_ppo2.py:185][0m |          -0.0098 |           1.6862 |           0.2876 |
[32m[20221213 14:55:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.83
[32m[20221213 14:55:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.02
[32m[20221213 14:55:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.29
[32m[20221213 14:55:55 @agent_ppo2.py:143][0m Total time:       3.00 min
[32m[20221213 14:55:55 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221213 14:55:55 @agent_ppo2.py:121][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 14:55:55 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:55:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:55 @agent_ppo2.py:185][0m |           0.0011 |           1.7907 |           0.2907 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |          -0.0084 |           1.7379 |           0.2906 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |          -0.0083 |           1.7191 |           0.2904 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |          -0.0078 |           1.7121 |           0.2904 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |           0.0016 |           1.9270 |           0.2903 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |          -0.0046 |           1.7787 |           0.2903 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |          -0.0124 |           1.6928 |           0.2902 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |          -0.0098 |           1.7094 |           0.2904 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |           0.0003 |           1.7653 |           0.2904 |
[32m[20221213 14:55:56 @agent_ppo2.py:185][0m |          -0.0143 |           1.6856 |           0.2906 |
[32m[20221213 14:55:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:55:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.82
[32m[20221213 14:55:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.12
[32m[20221213 14:55:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.00
[32m[20221213 14:55:56 @agent_ppo2.py:143][0m Total time:       3.02 min
[32m[20221213 14:55:56 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221213 14:55:56 @agent_ppo2.py:121][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 14:55:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:55:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |           0.0062 |           1.8901 |           0.2734 |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |          -0.0069 |           1.7243 |           0.2731 |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |          -0.0073 |           1.7159 |           0.2731 |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |          -0.0113 |           1.7069 |           0.2729 |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |          -0.0107 |           1.6985 |           0.2726 |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |          -0.0124 |           1.7022 |           0.2725 |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |          -0.0136 |           1.6854 |           0.2723 |
[32m[20221213 14:55:57 @agent_ppo2.py:185][0m |          -0.0146 |           1.6856 |           0.2723 |
[32m[20221213 14:55:58 @agent_ppo2.py:185][0m |          -0.0145 |           1.6818 |           0.2722 |
[32m[20221213 14:55:58 @agent_ppo2.py:185][0m |          -0.0093 |           1.7096 |           0.2722 |
[32m[20221213 14:55:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:55:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.02
[32m[20221213 14:55:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.01
[32m[20221213 14:55:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.76
[32m[20221213 14:55:58 @agent_ppo2.py:143][0m Total time:       3.04 min
[32m[20221213 14:55:58 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221213 14:55:58 @agent_ppo2.py:121][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 14:55:58 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:55:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:55:58 @agent_ppo2.py:185][0m |           0.0018 |           1.7427 |           0.2800 |
[32m[20221213 14:55:58 @agent_ppo2.py:185][0m |          -0.0051 |           1.6812 |           0.2799 |
[32m[20221213 14:55:58 @agent_ppo2.py:185][0m |          -0.0054 |           1.6703 |           0.2798 |
[32m[20221213 14:55:59 @agent_ppo2.py:185][0m |          -0.0108 |           1.6545 |           0.2798 |
[32m[20221213 14:55:59 @agent_ppo2.py:185][0m |          -0.0131 |           1.6529 |           0.2798 |
[32m[20221213 14:55:59 @agent_ppo2.py:185][0m |          -0.0114 |           1.6434 |           0.2797 |
[32m[20221213 14:55:59 @agent_ppo2.py:185][0m |          -0.0139 |           1.6321 |           0.2796 |
[32m[20221213 14:55:59 @agent_ppo2.py:185][0m |          -0.0089 |           1.6580 |           0.2799 |
[32m[20221213 14:55:59 @agent_ppo2.py:185][0m |          -0.0148 |           1.6206 |           0.2798 |
[32m[20221213 14:55:59 @agent_ppo2.py:185][0m |          -0.0145 |           1.6161 |           0.2798 |
[32m[20221213 14:55:59 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 14:55:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 71.43
[32m[20221213 14:55:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.69
[32m[20221213 14:55:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 62.77
[32m[20221213 14:55:59 @agent_ppo2.py:143][0m Total time:       3.07 min
[32m[20221213 14:55:59 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221213 14:55:59 @agent_ppo2.py:121][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 14:56:00 @agent_ppo2.py:127][0m Sampling time: 0.28 s by 5 slaves
[32m[20221213 14:56:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:00 @agent_ppo2.py:185][0m |          -0.0013 |           1.5512 |           0.2804 |
[32m[20221213 14:56:00 @agent_ppo2.py:185][0m |          -0.0037 |           1.4902 |           0.2802 |
[32m[20221213 14:56:00 @agent_ppo2.py:185][0m |          -0.0096 |           1.4715 |           0.2799 |
[32m[20221213 14:56:00 @agent_ppo2.py:185][0m |          -0.0134 |           1.4731 |           0.2798 |
[32m[20221213 14:56:01 @agent_ppo2.py:185][0m |          -0.0127 |           1.4446 |           0.2796 |
[32m[20221213 14:56:01 @agent_ppo2.py:185][0m |          -0.0136 |           1.4331 |           0.2796 |
[32m[20221213 14:56:01 @agent_ppo2.py:185][0m |          -0.0058 |           1.5431 |           0.2795 |
[32m[20221213 14:56:01 @agent_ppo2.py:185][0m |          -0.0149 |           1.4291 |           0.2795 |
[32m[20221213 14:56:01 @agent_ppo2.py:185][0m |          -0.0139 |           1.4092 |           0.2797 |
[32m[20221213 14:56:01 @agent_ppo2.py:185][0m |          -0.0181 |           1.4086 |           0.2796 |
[32m[20221213 14:56:01 @agent_ppo2.py:130][0m Policy update time: 1.37 s
[32m[20221213 14:56:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.60
[32m[20221213 14:56:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.56
[32m[20221213 14:56:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.16
[32m[20221213 14:56:01 @agent_ppo2.py:143][0m Total time:       3.10 min
[32m[20221213 14:56:01 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221213 14:56:01 @agent_ppo2.py:121][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 14:56:02 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0012 |           1.9782 |           0.2843 |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0075 |           1.8942 |           0.2839 |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0117 |           1.8635 |           0.2839 |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0052 |           1.8536 |           0.2837 |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0146 |           1.8255 |           0.2839 |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0154 |           1.8127 |           0.2838 |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0136 |           1.8062 |           0.2840 |
[32m[20221213 14:56:02 @agent_ppo2.py:185][0m |          -0.0160 |           1.7964 |           0.2840 |
[32m[20221213 14:56:03 @agent_ppo2.py:185][0m |          -0.0145 |           1.7846 |           0.2842 |
[32m[20221213 14:56:03 @agent_ppo2.py:185][0m |          -0.0093 |           1.8494 |           0.2841 |
[32m[20221213 14:56:03 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:56:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.34
[32m[20221213 14:56:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.66
[32m[20221213 14:56:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.34
[32m[20221213 14:56:03 @agent_ppo2.py:143][0m Total time:       3.13 min
[32m[20221213 14:56:03 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221213 14:56:03 @agent_ppo2.py:121][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 14:56:03 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:56:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:03 @agent_ppo2.py:185][0m |          -0.0024 |           1.9541 |           0.2830 |
[32m[20221213 14:56:03 @agent_ppo2.py:185][0m |          -0.0048 |           1.9008 |           0.2829 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |          -0.0056 |           1.8843 |           0.2827 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |          -0.0047 |           1.9097 |           0.2824 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |          -0.0030 |           1.9192 |           0.2825 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |           0.0028 |           2.0497 |           0.2825 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |          -0.0133 |           1.8557 |           0.2825 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |          -0.0072 |           1.9147 |           0.2826 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |          -0.0097 |           1.8717 |           0.2824 |
[32m[20221213 14:56:04 @agent_ppo2.py:185][0m |          -0.0125 |           1.8521 |           0.2827 |
[32m[20221213 14:56:04 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:56:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.96
[32m[20221213 14:56:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.38
[32m[20221213 14:56:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.41
[32m[20221213 14:56:04 @agent_ppo2.py:143][0m Total time:       3.15 min
[32m[20221213 14:56:04 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221213 14:56:04 @agent_ppo2.py:121][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 14:56:05 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:05 @agent_ppo2.py:185][0m |          -0.0031 |           1.8778 |           0.2842 |
[32m[20221213 14:56:05 @agent_ppo2.py:185][0m |          -0.0002 |           1.8628 |           0.2840 |
[32m[20221213 14:56:05 @agent_ppo2.py:185][0m |          -0.0069 |           1.8304 |           0.2836 |
[32m[20221213 14:56:05 @agent_ppo2.py:185][0m |          -0.0095 |           1.8306 |           0.2834 |
[32m[20221213 14:56:05 @agent_ppo2.py:185][0m |          -0.0089 |           1.8169 |           0.2833 |
[32m[20221213 14:56:05 @agent_ppo2.py:185][0m |          -0.0108 |           1.8115 |           0.2831 |
[32m[20221213 14:56:06 @agent_ppo2.py:185][0m |          -0.0117 |           1.7997 |           0.2831 |
[32m[20221213 14:56:06 @agent_ppo2.py:185][0m |          -0.0048 |           1.8824 |           0.2829 |
[32m[20221213 14:56:06 @agent_ppo2.py:185][0m |          -0.0124 |           1.8106 |           0.2828 |
[32m[20221213 14:56:06 @agent_ppo2.py:185][0m |          -0.0132 |           1.7951 |           0.2827 |
[32m[20221213 14:56:06 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 14:56:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.98
[32m[20221213 14:56:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.24
[32m[20221213 14:56:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.23
[32m[20221213 14:56:06 @agent_ppo2.py:143][0m Total time:       3.18 min
[32m[20221213 14:56:06 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221213 14:56:06 @agent_ppo2.py:121][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 14:56:06 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:56:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:06 @agent_ppo2.py:185][0m |           0.0027 |           1.9574 |           0.2861 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0028 |           1.8936 |           0.2859 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0072 |           1.8651 |           0.2857 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0081 |           1.8578 |           0.2854 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0109 |           1.8552 |           0.2853 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0140 |           1.8427 |           0.2852 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0152 |           1.8359 |           0.2852 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0141 |           1.8287 |           0.2851 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0146 |           1.8235 |           0.2851 |
[32m[20221213 14:56:07 @agent_ppo2.py:185][0m |          -0.0139 |           1.8349 |           0.2852 |
[32m[20221213 14:56:07 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 14:56:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 74.74
[32m[20221213 14:56:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.60
[32m[20221213 14:56:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 96.56
[32m[20221213 14:56:08 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 96.56
[32m[20221213 14:56:08 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 96.56
[32m[20221213 14:56:08 @agent_ppo2.py:143][0m Total time:       3.21 min
[32m[20221213 14:56:08 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221213 14:56:08 @agent_ppo2.py:121][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 14:56:08 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:56:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:08 @agent_ppo2.py:185][0m |          -0.0026 |           1.7768 |           0.2851 |
[32m[20221213 14:56:08 @agent_ppo2.py:185][0m |          -0.0087 |           1.7360 |           0.2844 |
[32m[20221213 14:56:08 @agent_ppo2.py:185][0m |           0.0001 |           1.7675 |           0.2840 |
[32m[20221213 14:56:08 @agent_ppo2.py:185][0m |          -0.0082 |           1.7040 |           0.2836 |
[32m[20221213 14:56:08 @agent_ppo2.py:185][0m |          -0.0094 |           1.6950 |           0.2836 |
[32m[20221213 14:56:09 @agent_ppo2.py:185][0m |          -0.0119 |           1.6893 |           0.2836 |
[32m[20221213 14:56:09 @agent_ppo2.py:185][0m |          -0.0125 |           1.6762 |           0.2834 |
[32m[20221213 14:56:09 @agent_ppo2.py:185][0m |          -0.0031 |           1.7439 |           0.2832 |
[32m[20221213 14:56:09 @agent_ppo2.py:185][0m |          -0.0085 |           1.7228 |           0.2832 |
[32m[20221213 14:56:09 @agent_ppo2.py:185][0m |          -0.0132 |           1.6693 |           0.2832 |
[32m[20221213 14:56:09 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:56:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.82
[32m[20221213 14:56:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.37
[32m[20221213 14:56:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.92
[32m[20221213 14:56:09 @agent_ppo2.py:143][0m Total time:       3.23 min
[32m[20221213 14:56:09 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221213 14:56:09 @agent_ppo2.py:121][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 14:56:09 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |           0.0001 |           1.6166 |           0.2793 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0041 |           1.5620 |           0.2794 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0066 |           1.5440 |           0.2793 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0075 |           1.5324 |           0.2793 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0046 |           1.5493 |           0.2793 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0142 |           1.5161 |           0.2793 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0074 |           1.5372 |           0.2794 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0046 |           1.5515 |           0.2793 |
[32m[20221213 14:56:10 @agent_ppo2.py:185][0m |          -0.0141 |           1.4970 |           0.2792 |
[32m[20221213 14:56:11 @agent_ppo2.py:185][0m |          -0.0167 |           1.4853 |           0.2792 |
[32m[20221213 14:56:11 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:56:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.16
[32m[20221213 14:56:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 80.06
[32m[20221213 14:56:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.83
[32m[20221213 14:56:11 @agent_ppo2.py:143][0m Total time:       3.26 min
[32m[20221213 14:56:11 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221213 14:56:11 @agent_ppo2.py:121][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 14:56:11 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 14:56:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:11 @agent_ppo2.py:185][0m |           0.0001 |           1.9262 |           0.2850 |
[32m[20221213 14:56:11 @agent_ppo2.py:185][0m |          -0.0084 |           1.8687 |           0.2854 |
[32m[20221213 14:56:11 @agent_ppo2.py:185][0m |          -0.0076 |           1.8517 |           0.2855 |
[32m[20221213 14:56:12 @agent_ppo2.py:185][0m |          -0.0099 |           1.8417 |           0.2854 |
[32m[20221213 14:56:12 @agent_ppo2.py:185][0m |          -0.0096 |           1.8344 |           0.2855 |
[32m[20221213 14:56:12 @agent_ppo2.py:185][0m |          -0.0138 |           1.8195 |           0.2854 |
[32m[20221213 14:56:12 @agent_ppo2.py:185][0m |          -0.0107 |           1.8111 |           0.2857 |
[32m[20221213 14:56:12 @agent_ppo2.py:185][0m |          -0.0158 |           1.8008 |           0.2857 |
[32m[20221213 14:56:12 @agent_ppo2.py:185][0m |          -0.0140 |           1.8016 |           0.2858 |
[32m[20221213 14:56:12 @agent_ppo2.py:185][0m |          -0.0175 |           1.7899 |           0.2860 |
[32m[20221213 14:56:12 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 14:56:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 74.15
[32m[20221213 14:56:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 81.91
[32m[20221213 14:56:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.85
[32m[20221213 14:56:12 @agent_ppo2.py:143][0m Total time:       3.29 min
[32m[20221213 14:56:12 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221213 14:56:12 @agent_ppo2.py:121][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 14:56:13 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:56:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |          -0.0013 |           1.8159 |           0.2919 |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |          -0.0049 |           1.7730 |           0.2920 |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |          -0.0102 |           1.7548 |           0.2920 |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |          -0.0133 |           1.7458 |           0.2919 |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |           0.0013 |           1.8792 |           0.2922 |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |          -0.0152 |           1.7314 |           0.2918 |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |          -0.0153 |           1.7163 |           0.2921 |
[32m[20221213 14:56:13 @agent_ppo2.py:185][0m |          -0.0139 |           1.7029 |           0.2923 |
[32m[20221213 14:56:14 @agent_ppo2.py:185][0m |          -0.0158 |           1.6975 |           0.2925 |
[32m[20221213 14:56:14 @agent_ppo2.py:185][0m |          -0.0126 |           1.6911 |           0.2925 |
[32m[20221213 14:56:14 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 14:56:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 74.14
[32m[20221213 14:56:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.36
[32m[20221213 14:56:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.34
[32m[20221213 14:56:14 @agent_ppo2.py:143][0m Total time:       3.31 min
[32m[20221213 14:56:14 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221213 14:56:14 @agent_ppo2.py:121][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 14:56:14 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:14 @agent_ppo2.py:185][0m |          -0.0019 |           1.8545 |           0.2980 |
[32m[20221213 14:56:14 @agent_ppo2.py:185][0m |          -0.0051 |           1.8261 |           0.2976 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |          -0.0071 |           1.8107 |           0.2975 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |          -0.0077 |           1.8104 |           0.2973 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |          -0.0068 |           1.8200 |           0.2971 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |          -0.0119 |           1.7916 |           0.2971 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |           0.0024 |           1.9732 |           0.2971 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |          -0.0135 |           1.7963 |           0.2970 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |          -0.0135 |           1.7716 |           0.2970 |
[32m[20221213 14:56:15 @agent_ppo2.py:185][0m |          -0.0022 |           1.9717 |           0.2970 |
[32m[20221213 14:56:15 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:56:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.95
[32m[20221213 14:56:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.97
[32m[20221213 14:56:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.44
[32m[20221213 14:56:15 @agent_ppo2.py:143][0m Total time:       3.34 min
[32m[20221213 14:56:15 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221213 14:56:15 @agent_ppo2.py:121][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 14:56:16 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:56:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:16 @agent_ppo2.py:185][0m |           0.0010 |           1.8456 |           0.2955 |
[32m[20221213 14:56:16 @agent_ppo2.py:185][0m |          -0.0061 |           1.8070 |           0.2941 |
[32m[20221213 14:56:16 @agent_ppo2.py:185][0m |          -0.0084 |           1.7891 |           0.2940 |
[32m[20221213 14:56:16 @agent_ppo2.py:185][0m |          -0.0078 |           1.8023 |           0.2936 |
[32m[20221213 14:56:16 @agent_ppo2.py:185][0m |          -0.0050 |           1.8170 |           0.2932 |
[32m[20221213 14:56:16 @agent_ppo2.py:185][0m |          -0.0104 |           1.7857 |           0.2933 |
[32m[20221213 14:56:17 @agent_ppo2.py:185][0m |          -0.0133 |           1.7751 |           0.2929 |
[32m[20221213 14:56:17 @agent_ppo2.py:185][0m |          -0.0137 |           1.7770 |           0.2927 |
[32m[20221213 14:56:17 @agent_ppo2.py:185][0m |          -0.0152 |           1.7746 |           0.2923 |
[32m[20221213 14:56:17 @agent_ppo2.py:185][0m |          -0.0045 |           1.9857 |           0.2923 |
[32m[20221213 14:56:17 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 14:56:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.82
[32m[20221213 14:56:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.39
[32m[20221213 14:56:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.73
[32m[20221213 14:56:17 @agent_ppo2.py:143][0m Total time:       3.36 min
[32m[20221213 14:56:17 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221213 14:56:17 @agent_ppo2.py:121][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 14:56:17 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 14:56:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:17 @agent_ppo2.py:185][0m |           0.0079 |           1.9711 |           0.2908 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0039 |           1.7918 |           0.2903 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0081 |           1.7704 |           0.2902 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0088 |           1.7731 |           0.2902 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0016 |           1.8423 |           0.2904 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0111 |           1.7597 |           0.2903 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0053 |           1.8653 |           0.2905 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0122 |           1.7625 |           0.2906 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0119 |           1.7410 |           0.2907 |
[32m[20221213 14:56:18 @agent_ppo2.py:185][0m |          -0.0112 |           1.7445 |           0.2907 |
[32m[20221213 14:56:18 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:56:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.39
[32m[20221213 14:56:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.08
[32m[20221213 14:56:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.26
[32m[20221213 14:56:19 @agent_ppo2.py:143][0m Total time:       3.39 min
[32m[20221213 14:56:19 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221213 14:56:19 @agent_ppo2.py:121][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 14:56:19 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:19 @agent_ppo2.py:185][0m |           0.0004 |           1.8135 |           0.2959 |
[32m[20221213 14:56:19 @agent_ppo2.py:185][0m |          -0.0080 |           1.7836 |           0.2953 |
[32m[20221213 14:56:19 @agent_ppo2.py:185][0m |          -0.0037 |           1.7701 |           0.2951 |
[32m[20221213 14:56:19 @agent_ppo2.py:185][0m |          -0.0095 |           1.7307 |           0.2951 |
[32m[20221213 14:56:19 @agent_ppo2.py:185][0m |          -0.0107 |           1.7214 |           0.2949 |
[32m[20221213 14:56:20 @agent_ppo2.py:185][0m |          -0.0135 |           1.7081 |           0.2947 |
[32m[20221213 14:56:20 @agent_ppo2.py:185][0m |          -0.0141 |           1.6922 |           0.2949 |
[32m[20221213 14:56:20 @agent_ppo2.py:185][0m |          -0.0134 |           1.6738 |           0.2947 |
[32m[20221213 14:56:20 @agent_ppo2.py:185][0m |          -0.0138 |           1.6644 |           0.2949 |
[32m[20221213 14:56:20 @agent_ppo2.py:185][0m |          -0.0137 |           1.6549 |           0.2947 |
[32m[20221213 14:56:20 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 14:56:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.56
[32m[20221213 14:56:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.53
[32m[20221213 14:56:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.65
[32m[20221213 14:56:20 @agent_ppo2.py:143][0m Total time:       3.42 min
[32m[20221213 14:56:20 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221213 14:56:20 @agent_ppo2.py:121][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 14:56:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0014 |           1.8760 |           0.2972 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0103 |           1.7880 |           0.2969 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0107 |           1.7691 |           0.2969 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0125 |           1.7603 |           0.2969 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0139 |           1.7486 |           0.2967 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0166 |           1.7549 |           0.2967 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0169 |           1.7312 |           0.2967 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0166 |           1.7262 |           0.2964 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0183 |           1.7260 |           0.2967 |
[32m[20221213 14:56:21 @agent_ppo2.py:185][0m |          -0.0187 |           1.7169 |           0.2967 |
[32m[20221213 14:56:21 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 14:56:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.84
[32m[20221213 14:56:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.35
[32m[20221213 14:56:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 56.98
[32m[20221213 14:56:22 @agent_ppo2.py:143][0m Total time:       3.44 min
[32m[20221213 14:56:22 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221213 14:56:22 @agent_ppo2.py:121][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 14:56:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:22 @agent_ppo2.py:185][0m |          -0.0038 |           1.7700 |           0.2970 |
[32m[20221213 14:56:22 @agent_ppo2.py:185][0m |          -0.0078 |           1.7410 |           0.2970 |
[32m[20221213 14:56:22 @agent_ppo2.py:185][0m |          -0.0090 |           1.7350 |           0.2969 |
[32m[20221213 14:56:22 @agent_ppo2.py:185][0m |          -0.0105 |           1.7275 |           0.2968 |
[32m[20221213 14:56:22 @agent_ppo2.py:185][0m |          -0.0090 |           1.7296 |           0.2970 |
[32m[20221213 14:56:23 @agent_ppo2.py:185][0m |          -0.0124 |           1.7206 |           0.2970 |
[32m[20221213 14:56:23 @agent_ppo2.py:185][0m |          -0.0121 |           1.7181 |           0.2972 |
[32m[20221213 14:56:23 @agent_ppo2.py:185][0m |          -0.0133 |           1.7172 |           0.2973 |
[32m[20221213 14:56:23 @agent_ppo2.py:185][0m |          -0.0065 |           1.7926 |           0.2972 |
[32m[20221213 14:56:23 @agent_ppo2.py:185][0m |          -0.0138 |           1.7108 |           0.2975 |
[32m[20221213 14:56:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:56:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.04
[32m[20221213 14:56:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.94
[32m[20221213 14:56:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.59
[32m[20221213 14:56:23 @agent_ppo2.py:143][0m Total time:       3.47 min
[32m[20221213 14:56:23 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221213 14:56:23 @agent_ppo2.py:121][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 14:56:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:23 @agent_ppo2.py:185][0m |          -0.0020 |           1.7496 |           0.3003 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0025 |           1.7439 |           0.2998 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0104 |           1.7157 |           0.2996 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0102 |           1.7069 |           0.2994 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0088 |           1.7018 |           0.2993 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0119 |           1.6934 |           0.2988 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0120 |           1.6898 |           0.2989 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0110 |           1.6845 |           0.2986 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0133 |           1.6828 |           0.2985 |
[32m[20221213 14:56:24 @agent_ppo2.py:185][0m |          -0.0121 |           1.6771 |           0.2983 |
[32m[20221213 14:56:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.47
[32m[20221213 14:56:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.71
[32m[20221213 14:56:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.41
[32m[20221213 14:56:24 @agent_ppo2.py:143][0m Total time:       3.49 min
[32m[20221213 14:56:24 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221213 14:56:24 @agent_ppo2.py:121][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 14:56:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |           0.0005 |           1.7895 |           0.2898 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0079 |           1.7565 |           0.2895 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0054 |           1.7484 |           0.2893 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0102 |           1.7294 |           0.2891 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0102 |           1.7301 |           0.2890 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0052 |           1.7694 |           0.2889 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0132 |           1.7132 |           0.2888 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0143 |           1.7142 |           0.2887 |
[32m[20221213 14:56:25 @agent_ppo2.py:185][0m |          -0.0144 |           1.7074 |           0.2886 |
[32m[20221213 14:56:26 @agent_ppo2.py:185][0m |          -0.0130 |           1.7072 |           0.2887 |
[32m[20221213 14:56:26 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.71
[32m[20221213 14:56:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.13
[32m[20221213 14:56:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.65
[32m[20221213 14:56:26 @agent_ppo2.py:143][0m Total time:       3.51 min
[32m[20221213 14:56:26 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221213 14:56:26 @agent_ppo2.py:121][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 14:56:26 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:26 @agent_ppo2.py:185][0m |          -0.0038 |           1.8481 |           0.2953 |
[32m[20221213 14:56:26 @agent_ppo2.py:185][0m |          -0.0042 |           1.8154 |           0.2947 |
[32m[20221213 14:56:26 @agent_ppo2.py:185][0m |          -0.0062 |           1.8116 |           0.2942 |
[32m[20221213 14:56:26 @agent_ppo2.py:185][0m |          -0.0093 |           1.7974 |           0.2937 |
[32m[20221213 14:56:26 @agent_ppo2.py:185][0m |          -0.0120 |           1.7980 |           0.2934 |
[32m[20221213 14:56:27 @agent_ppo2.py:185][0m |          -0.0111 |           1.8007 |           0.2932 |
[32m[20221213 14:56:27 @agent_ppo2.py:185][0m |          -0.0083 |           1.7830 |           0.2929 |
[32m[20221213 14:56:27 @agent_ppo2.py:185][0m |          -0.0108 |           1.7835 |           0.2926 |
[32m[20221213 14:56:27 @agent_ppo2.py:185][0m |          -0.0109 |           1.7738 |           0.2924 |
[32m[20221213 14:56:27 @agent_ppo2.py:185][0m |          -0.0161 |           1.7728 |           0.2923 |
[32m[20221213 14:56:27 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.26
[32m[20221213 14:56:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 79.31
[32m[20221213 14:56:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.45
[32m[20221213 14:56:27 @agent_ppo2.py:143][0m Total time:       3.53 min
[32m[20221213 14:56:27 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221213 14:56:27 @agent_ppo2.py:121][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 14:56:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:27 @agent_ppo2.py:185][0m |          -0.0023 |           1.7490 |           0.2902 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |           0.0110 |           1.9655 |           0.2897 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0104 |           1.7189 |           0.2893 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0069 |           1.7050 |           0.2892 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0107 |           1.6854 |           0.2894 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0129 |           1.6881 |           0.2896 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0111 |           1.6846 |           0.2895 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0139 |           1.6747 |           0.2894 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0107 |           1.6683 |           0.2895 |
[32m[20221213 14:56:28 @agent_ppo2.py:185][0m |          -0.0141 |           1.6606 |           0.2894 |
[32m[20221213 14:56:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.69
[32m[20221213 14:56:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.07
[32m[20221213 14:56:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.83
[32m[20221213 14:56:28 @agent_ppo2.py:143][0m Total time:       3.55 min
[32m[20221213 14:56:28 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221213 14:56:28 @agent_ppo2.py:121][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 14:56:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |          -0.0020 |           1.7704 |           0.2970 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |          -0.0038 |           1.7427 |           0.2965 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |          -0.0059 |           1.7299 |           0.2959 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |          -0.0070 |           1.7305 |           0.2957 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |          -0.0068 |           1.7322 |           0.2953 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |          -0.0084 |           1.7220 |           0.2950 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |           0.0021 |           1.8854 |           0.2949 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |           0.0008 |           1.9280 |           0.2945 |
[32m[20221213 14:56:29 @agent_ppo2.py:185][0m |          -0.0118 |           1.7199 |           0.2944 |
[32m[20221213 14:56:30 @agent_ppo2.py:185][0m |          -0.0114 |           1.7165 |           0.2944 |
[32m[20221213 14:56:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.76
[32m[20221213 14:56:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.04
[32m[20221213 14:56:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.01
[32m[20221213 14:56:30 @agent_ppo2.py:143][0m Total time:       3.58 min
[32m[20221213 14:56:30 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221213 14:56:30 @agent_ppo2.py:121][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 14:56:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:56:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:30 @agent_ppo2.py:185][0m |          -0.0022 |           1.8281 |           0.2805 |
[32m[20221213 14:56:30 @agent_ppo2.py:185][0m |          -0.0039 |           1.8030 |           0.2805 |
[32m[20221213 14:56:30 @agent_ppo2.py:185][0m |          -0.0073 |           1.7924 |           0.2805 |
[32m[20221213 14:56:30 @agent_ppo2.py:185][0m |          -0.0080 |           1.7880 |           0.2806 |
[32m[20221213 14:56:30 @agent_ppo2.py:185][0m |          -0.0126 |           1.7928 |           0.2803 |
[32m[20221213 14:56:31 @agent_ppo2.py:185][0m |          -0.0067 |           1.7889 |           0.2803 |
[32m[20221213 14:56:31 @agent_ppo2.py:185][0m |          -0.0119 |           1.7803 |           0.2802 |
[32m[20221213 14:56:31 @agent_ppo2.py:185][0m |          -0.0108 |           1.7686 |           0.2802 |
[32m[20221213 14:56:31 @agent_ppo2.py:185][0m |          -0.0138 |           1.7662 |           0.2801 |
[32m[20221213 14:56:31 @agent_ppo2.py:185][0m |          -0.0094 |           1.7773 |           0.2801 |
[32m[20221213 14:56:31 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 76.49
[32m[20221213 14:56:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 80.65
[32m[20221213 14:56:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.01
[32m[20221213 14:56:31 @agent_ppo2.py:143][0m Total time:       3.60 min
[32m[20221213 14:56:31 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221213 14:56:31 @agent_ppo2.py:121][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 14:56:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:31 @agent_ppo2.py:185][0m |           0.0087 |           1.8968 |           0.2955 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0046 |           1.7163 |           0.2953 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0078 |           1.6963 |           0.2955 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0077 |           1.6772 |           0.2953 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0092 |           1.6652 |           0.2951 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0106 |           1.6556 |           0.2950 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0130 |           1.6471 |           0.2950 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0142 |           1.6399 |           0.2951 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0111 |           1.6347 |           0.2951 |
[32m[20221213 14:56:32 @agent_ppo2.py:185][0m |          -0.0130 |           1.6318 |           0.2951 |
[32m[20221213 14:56:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.49
[32m[20221213 14:56:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.92
[32m[20221213 14:56:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.36
[32m[20221213 14:56:32 @agent_ppo2.py:143][0m Total time:       3.62 min
[32m[20221213 14:56:32 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221213 14:56:32 @agent_ppo2.py:121][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 14:56:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |           0.0003 |           2.0185 |           0.2917 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |          -0.0054 |           1.9765 |           0.2913 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |          -0.0035 |           1.9625 |           0.2912 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |           0.0037 |           2.1681 |           0.2908 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |          -0.0107 |           1.9447 |           0.2906 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |          -0.0077 |           1.9372 |           0.2906 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |          -0.0090 |           1.9244 |           0.2905 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |          -0.0082 |           1.9235 |           0.2907 |
[32m[20221213 14:56:33 @agent_ppo2.py:185][0m |          -0.0099 |           1.9177 |           0.2906 |
[32m[20221213 14:56:34 @agent_ppo2.py:185][0m |          -0.0097 |           1.9236 |           0.2904 |
[32m[20221213 14:56:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.84
[32m[20221213 14:56:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.00
[32m[20221213 14:56:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.94
[32m[20221213 14:56:34 @agent_ppo2.py:143][0m Total time:       3.64 min
[32m[20221213 14:56:34 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221213 14:56:34 @agent_ppo2.py:121][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 14:56:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:34 @agent_ppo2.py:185][0m |           0.0018 |           1.8754 |           0.3015 |
[32m[20221213 14:56:34 @agent_ppo2.py:185][0m |          -0.0111 |           1.8081 |           0.3010 |
[32m[20221213 14:56:34 @agent_ppo2.py:185][0m |          -0.0060 |           1.8223 |           0.3007 |
[32m[20221213 14:56:34 @agent_ppo2.py:185][0m |          -0.0151 |           1.7840 |           0.3006 |
[32m[20221213 14:56:34 @agent_ppo2.py:185][0m |          -0.0131 |           1.7806 |           0.3004 |
[32m[20221213 14:56:35 @agent_ppo2.py:185][0m |          -0.0130 |           1.7828 |           0.3004 |
[32m[20221213 14:56:35 @agent_ppo2.py:185][0m |          -0.0164 |           1.7684 |           0.3004 |
[32m[20221213 14:56:35 @agent_ppo2.py:185][0m |          -0.0157 |           1.7683 |           0.3005 |
[32m[20221213 14:56:35 @agent_ppo2.py:185][0m |          -0.0172 |           1.7681 |           0.3004 |
[32m[20221213 14:56:35 @agent_ppo2.py:185][0m |          -0.0116 |           1.7731 |           0.3007 |
[32m[20221213 14:56:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.36
[32m[20221213 14:56:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.47
[32m[20221213 14:56:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.40
[32m[20221213 14:56:35 @agent_ppo2.py:143][0m Total time:       3.67 min
[32m[20221213 14:56:35 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221213 14:56:35 @agent_ppo2.py:121][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 14:56:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:35 @agent_ppo2.py:185][0m |           0.0086 |           2.0445 |           0.2956 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0082 |           1.9191 |           0.2950 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0103 |           1.9102 |           0.2948 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0057 |           1.9105 |           0.2950 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0126 |           1.8865 |           0.2949 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0142 |           1.8781 |           0.2948 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0117 |           1.8744 |           0.2949 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0149 |           1.8761 |           0.2949 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0154 |           1.8732 |           0.2950 |
[32m[20221213 14:56:36 @agent_ppo2.py:185][0m |          -0.0146 |           1.8648 |           0.2948 |
[32m[20221213 14:56:36 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.48
[32m[20221213 14:56:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.34
[32m[20221213 14:56:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.52
[32m[20221213 14:56:36 @agent_ppo2.py:143][0m Total time:       3.69 min
[32m[20221213 14:56:36 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221213 14:56:36 @agent_ppo2.py:121][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 14:56:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0023 |           1.8287 |           0.3018 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0093 |           1.7710 |           0.3014 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0073 |           1.7440 |           0.3013 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0146 |           1.7390 |           0.3012 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0071 |           1.7976 |           0.3012 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0098 |           1.7391 |           0.3012 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0181 |           1.7092 |           0.3011 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0045 |           1.9196 |           0.3012 |
[32m[20221213 14:56:37 @agent_ppo2.py:185][0m |          -0.0148 |           1.7013 |           0.3011 |
[32m[20221213 14:56:38 @agent_ppo2.py:185][0m |          -0.0157 |           1.7123 |           0.3011 |
[32m[20221213 14:56:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 70.56
[32m[20221213 14:56:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 77.56
[32m[20221213 14:56:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.24
[32m[20221213 14:56:38 @agent_ppo2.py:143][0m Total time:       3.71 min
[32m[20221213 14:56:38 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221213 14:56:38 @agent_ppo2.py:121][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 14:56:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:38 @agent_ppo2.py:185][0m |          -0.0008 |           1.8136 |           0.2946 |
[32m[20221213 14:56:38 @agent_ppo2.py:185][0m |          -0.0086 |           1.7563 |           0.2947 |
[32m[20221213 14:56:38 @agent_ppo2.py:185][0m |           0.0099 |           1.9454 |           0.2945 |
[32m[20221213 14:56:38 @agent_ppo2.py:185][0m |          -0.0112 |           1.7385 |           0.2942 |
[32m[20221213 14:56:38 @agent_ppo2.py:185][0m |          -0.0110 |           1.7017 |           0.2944 |
[32m[20221213 14:56:39 @agent_ppo2.py:185][0m |          -0.0131 |           1.7007 |           0.2944 |
[32m[20221213 14:56:39 @agent_ppo2.py:185][0m |          -0.0160 |           1.6921 |           0.2943 |
[32m[20221213 14:56:39 @agent_ppo2.py:185][0m |          -0.0066 |           1.7264 |           0.2942 |
[32m[20221213 14:56:39 @agent_ppo2.py:185][0m |          -0.0149 |           1.6771 |           0.2945 |
[32m[20221213 14:56:39 @agent_ppo2.py:185][0m |          -0.0136 |           1.6651 |           0.2944 |
[32m[20221213 14:56:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.91
[32m[20221213 14:56:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.39
[32m[20221213 14:56:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.66
[32m[20221213 14:56:39 @agent_ppo2.py:143][0m Total time:       3.73 min
[32m[20221213 14:56:39 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221213 14:56:39 @agent_ppo2.py:121][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 14:56:39 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:39 @agent_ppo2.py:185][0m |          -0.0044 |           1.8362 |           0.2932 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0071 |           1.8072 |           0.2930 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0091 |           1.7851 |           0.2929 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0110 |           1.7804 |           0.2927 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0100 |           1.7706 |           0.2923 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0140 |           1.7637 |           0.2922 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0162 |           1.7581 |           0.2921 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0153 |           1.7486 |           0.2920 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0157 |           1.7417 |           0.2920 |
[32m[20221213 14:56:40 @agent_ppo2.py:185][0m |          -0.0128 |           1.7509 |           0.2918 |
[32m[20221213 14:56:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.93
[32m[20221213 14:56:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.75
[32m[20221213 14:56:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.78
[32m[20221213 14:56:40 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221213 14:56:40 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221213 14:56:40 @agent_ppo2.py:121][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 14:56:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0004 |           1.8354 |           0.2955 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0002 |           1.8154 |           0.2954 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0097 |           1.7825 |           0.2953 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0100 |           1.7772 |           0.2951 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0106 |           1.7571 |           0.2954 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0134 |           1.7566 |           0.2953 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0097 |           1.7531 |           0.2952 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0133 |           1.7365 |           0.2955 |
[32m[20221213 14:56:41 @agent_ppo2.py:185][0m |          -0.0111 |           1.7442 |           0.2955 |
[32m[20221213 14:56:42 @agent_ppo2.py:185][0m |          -0.0167 |           1.7217 |           0.2955 |
[32m[20221213 14:56:42 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.32
[32m[20221213 14:56:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.47
[32m[20221213 14:56:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.03
[32m[20221213 14:56:42 @agent_ppo2.py:143][0m Total time:       3.78 min
[32m[20221213 14:56:42 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221213 14:56:42 @agent_ppo2.py:121][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 14:56:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:42 @agent_ppo2.py:185][0m |          -0.0004 |           1.9396 |           0.3022 |
[32m[20221213 14:56:42 @agent_ppo2.py:185][0m |          -0.0076 |           1.9186 |           0.3019 |
[32m[20221213 14:56:42 @agent_ppo2.py:185][0m |          -0.0113 |           1.9097 |           0.3015 |
[32m[20221213 14:56:42 @agent_ppo2.py:185][0m |          -0.0114 |           1.9001 |           0.3013 |
[32m[20221213 14:56:42 @agent_ppo2.py:185][0m |          -0.0140 |           1.8970 |           0.3010 |
[32m[20221213 14:56:43 @agent_ppo2.py:185][0m |          -0.0103 |           1.8945 |           0.3009 |
[32m[20221213 14:56:43 @agent_ppo2.py:185][0m |          -0.0081 |           1.9162 |           0.3007 |
[32m[20221213 14:56:43 @agent_ppo2.py:185][0m |          -0.0144 |           1.8784 |           0.3005 |
[32m[20221213 14:56:43 @agent_ppo2.py:185][0m |          -0.0135 |           1.8748 |           0.3004 |
[32m[20221213 14:56:43 @agent_ppo2.py:185][0m |          -0.0142 |           1.8714 |           0.3003 |
[32m[20221213 14:56:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.02
[32m[20221213 14:56:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.94
[32m[20221213 14:56:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.84
[32m[20221213 14:56:43 @agent_ppo2.py:143][0m Total time:       3.80 min
[32m[20221213 14:56:43 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221213 14:56:43 @agent_ppo2.py:121][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 14:56:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:43 @agent_ppo2.py:185][0m |          -0.0004 |           1.8559 |           0.2999 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0064 |           1.8130 |           0.2995 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0120 |           1.8008 |           0.2990 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0074 |           1.7885 |           0.2987 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0094 |           1.7675 |           0.2983 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0131 |           1.7575 |           0.2981 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0126 |           1.7475 |           0.2980 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0130 |           1.7458 |           0.2980 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |           0.0115 |           2.1942 |           0.2979 |
[32m[20221213 14:56:44 @agent_ppo2.py:185][0m |          -0.0157 |           1.7780 |           0.2976 |
[32m[20221213 14:56:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.56
[32m[20221213 14:56:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.62
[32m[20221213 14:56:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 49.61
[32m[20221213 14:56:44 @agent_ppo2.py:143][0m Total time:       3.82 min
[32m[20221213 14:56:44 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221213 14:56:44 @agent_ppo2.py:121][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 14:56:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |           0.0071 |           1.8935 |           0.2887 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |          -0.0047 |           1.8138 |           0.2883 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |          -0.0055 |           1.8008 |           0.2881 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |          -0.0088 |           1.7941 |           0.2877 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |           0.0016 |           1.9809 |           0.2879 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |          -0.0115 |           1.7978 |           0.2875 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |          -0.0054 |           1.7975 |           0.2875 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |          -0.0099 |           1.7951 |           0.2876 |
[32m[20221213 14:56:45 @agent_ppo2.py:185][0m |          -0.0105 |           1.7832 |           0.2875 |
[32m[20221213 14:56:46 @agent_ppo2.py:185][0m |          -0.0139 |           1.7742 |           0.2876 |
[32m[20221213 14:56:46 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.05
[32m[20221213 14:56:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.14
[32m[20221213 14:56:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.10
[32m[20221213 14:56:46 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221213 14:56:46 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221213 14:56:46 @agent_ppo2.py:121][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 14:56:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:46 @agent_ppo2.py:185][0m |           0.0031 |           1.8568 |           0.2890 |
[32m[20221213 14:56:46 @agent_ppo2.py:185][0m |          -0.0104 |           1.7657 |           0.2881 |
[32m[20221213 14:56:46 @agent_ppo2.py:185][0m |          -0.0144 |           1.7493 |           0.2876 |
[32m[20221213 14:56:46 @agent_ppo2.py:185][0m |          -0.0131 |           1.7523 |           0.2873 |
[32m[20221213 14:56:46 @agent_ppo2.py:185][0m |          -0.0138 |           1.7285 |           0.2873 |
[32m[20221213 14:56:47 @agent_ppo2.py:185][0m |          -0.0146 |           1.7145 |           0.2871 |
[32m[20221213 14:56:47 @agent_ppo2.py:185][0m |          -0.0156 |           1.7171 |           0.2871 |
[32m[20221213 14:56:47 @agent_ppo2.py:185][0m |          -0.0168 |           1.7089 |           0.2871 |
[32m[20221213 14:56:47 @agent_ppo2.py:185][0m |          -0.0174 |           1.7022 |           0.2871 |
[32m[20221213 14:56:47 @agent_ppo2.py:185][0m |          -0.0162 |           1.6984 |           0.2871 |
[32m[20221213 14:56:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.14
[32m[20221213 14:56:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.90
[32m[20221213 14:56:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.48
[32m[20221213 14:56:47 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221213 14:56:47 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221213 14:56:47 @agent_ppo2.py:121][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 14:56:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:47 @agent_ppo2.py:185][0m |           0.0219 |           2.1474 |           0.2877 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0033 |           1.8030 |           0.2871 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0070 |           1.7692 |           0.2873 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0066 |           1.7741 |           0.2872 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0084 |           1.7526 |           0.2874 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0122 |           1.7443 |           0.2873 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0040 |           1.7906 |           0.2875 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0138 |           1.7383 |           0.2873 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0135 |           1.7300 |           0.2875 |
[32m[20221213 14:56:48 @agent_ppo2.py:185][0m |          -0.0128 |           1.7229 |           0.2875 |
[32m[20221213 14:56:48 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.74
[32m[20221213 14:56:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.96
[32m[20221213 14:56:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.66
[32m[20221213 14:56:48 @agent_ppo2.py:143][0m Total time:       3.89 min
[32m[20221213 14:56:48 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221213 14:56:48 @agent_ppo2.py:121][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 14:56:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0020 |           1.8365 |           0.2953 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0073 |           1.8184 |           0.2949 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0072 |           1.8119 |           0.2945 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0080 |           1.8031 |           0.2945 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0108 |           1.8024 |           0.2944 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0117 |           1.8021 |           0.2943 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0129 |           1.7980 |           0.2942 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0125 |           1.7946 |           0.2944 |
[32m[20221213 14:56:49 @agent_ppo2.py:185][0m |          -0.0127 |           1.7909 |           0.2944 |
[32m[20221213 14:56:50 @agent_ppo2.py:185][0m |          -0.0056 |           1.8925 |           0.2943 |
[32m[20221213 14:56:50 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.37
[32m[20221213 14:56:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.54
[32m[20221213 14:56:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.94
[32m[20221213 14:56:50 @agent_ppo2.py:143][0m Total time:       3.91 min
[32m[20221213 14:56:50 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221213 14:56:50 @agent_ppo2.py:121][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 14:56:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:50 @agent_ppo2.py:185][0m |          -0.0029 |           1.8458 |           0.3020 |
[32m[20221213 14:56:50 @agent_ppo2.py:185][0m |          -0.0038 |           1.8291 |           0.3017 |
[32m[20221213 14:56:50 @agent_ppo2.py:185][0m |          -0.0019 |           1.8459 |           0.3014 |
[32m[20221213 14:56:50 @agent_ppo2.py:185][0m |          -0.0053 |           1.8167 |           0.3011 |
[32m[20221213 14:56:50 @agent_ppo2.py:185][0m |          -0.0107 |           1.8126 |           0.3011 |
[32m[20221213 14:56:51 @agent_ppo2.py:185][0m |          -0.0075 |           1.8066 |           0.3008 |
[32m[20221213 14:56:51 @agent_ppo2.py:185][0m |          -0.0123 |           1.8045 |           0.3006 |
[32m[20221213 14:56:51 @agent_ppo2.py:185][0m |          -0.0124 |           1.8009 |           0.3005 |
[32m[20221213 14:56:51 @agent_ppo2.py:185][0m |          -0.0127 |           1.7917 |           0.3005 |
[32m[20221213 14:56:51 @agent_ppo2.py:185][0m |          -0.0113 |           1.7905 |           0.3002 |
[32m[20221213 14:56:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.46
[32m[20221213 14:56:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.88
[32m[20221213 14:56:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.96
[32m[20221213 14:56:51 @agent_ppo2.py:143][0m Total time:       3.93 min
[32m[20221213 14:56:51 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221213 14:56:51 @agent_ppo2.py:121][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 14:56:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:51 @agent_ppo2.py:185][0m |          -0.0006 |           1.8925 |           0.2955 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0050 |           1.8640 |           0.2956 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0061 |           1.8556 |           0.2954 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0090 |           1.8444 |           0.2955 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0102 |           1.8352 |           0.2956 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0065 |           1.8712 |           0.2955 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0113 |           1.8264 |           0.2954 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0144 |           1.8246 |           0.2956 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0119 |           1.8206 |           0.2956 |
[32m[20221213 14:56:52 @agent_ppo2.py:185][0m |          -0.0133 |           1.8125 |           0.2955 |
[32m[20221213 14:56:52 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.00
[32m[20221213 14:56:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.96
[32m[20221213 14:56:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.00
[32m[20221213 14:56:52 @agent_ppo2.py:143][0m Total time:       3.95 min
[32m[20221213 14:56:52 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221213 14:56:52 @agent_ppo2.py:121][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 14:56:53 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:56:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0051 |           1.9226 |           0.2940 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0064 |           1.8926 |           0.2935 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0090 |           1.8941 |           0.2934 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0113 |           1.8778 |           0.2934 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0098 |           1.8820 |           0.2933 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0152 |           1.8654 |           0.2933 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0109 |           1.8817 |           0.2935 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0137 |           1.8535 |           0.2934 |
[32m[20221213 14:56:53 @agent_ppo2.py:185][0m |          -0.0159 |           1.8494 |           0.2935 |
[32m[20221213 14:56:54 @agent_ppo2.py:185][0m |          -0.0153 |           1.8432 |           0.2935 |
[32m[20221213 14:56:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.22
[32m[20221213 14:56:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.76
[32m[20221213 14:56:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.47
[32m[20221213 14:56:54 @agent_ppo2.py:143][0m Total time:       3.98 min
[32m[20221213 14:56:54 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221213 14:56:54 @agent_ppo2.py:121][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 14:56:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:54 @agent_ppo2.py:185][0m |          -0.0024 |           1.8485 |           0.3046 |
[32m[20221213 14:56:54 @agent_ppo2.py:185][0m |          -0.0072 |           1.8380 |           0.3041 |
[32m[20221213 14:56:54 @agent_ppo2.py:185][0m |          -0.0111 |           1.8301 |           0.3037 |
[32m[20221213 14:56:54 @agent_ppo2.py:185][0m |          -0.0045 |           1.8542 |           0.3037 |
[32m[20221213 14:56:54 @agent_ppo2.py:185][0m |          -0.0107 |           1.8230 |           0.3037 |
[32m[20221213 14:56:55 @agent_ppo2.py:185][0m |          -0.0146 |           1.8220 |           0.3035 |
[32m[20221213 14:56:55 @agent_ppo2.py:185][0m |          -0.0121 |           1.8171 |           0.3032 |
[32m[20221213 14:56:55 @agent_ppo2.py:185][0m |          -0.0141 |           1.8148 |           0.3032 |
[32m[20221213 14:56:55 @agent_ppo2.py:185][0m |          -0.0138 |           1.8129 |           0.3031 |
[32m[20221213 14:56:55 @agent_ppo2.py:185][0m |          -0.0137 |           1.8163 |           0.3033 |
[32m[20221213 14:56:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.92
[32m[20221213 14:56:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.89
[32m[20221213 14:56:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.81
[32m[20221213 14:56:55 @agent_ppo2.py:143][0m Total time:       4.00 min
[32m[20221213 14:56:55 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221213 14:56:55 @agent_ppo2.py:121][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 14:56:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:55 @agent_ppo2.py:185][0m |          -0.0038 |           1.8158 |           0.3023 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0083 |           1.7855 |           0.3019 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0058 |           1.7888 |           0.3016 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0092 |           1.7498 |           0.3015 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0118 |           1.7438 |           0.3016 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0135 |           1.7356 |           0.3015 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0165 |           1.7350 |           0.3015 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0164 |           1.7238 |           0.3017 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0162 |           1.7235 |           0.3017 |
[32m[20221213 14:56:56 @agent_ppo2.py:185][0m |          -0.0176 |           1.7183 |           0.3018 |
[32m[20221213 14:56:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.63
[32m[20221213 14:56:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.13
[32m[20221213 14:56:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.98
[32m[20221213 14:56:56 @agent_ppo2.py:143][0m Total time:       4.02 min
[32m[20221213 14:56:56 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221213 14:56:56 @agent_ppo2.py:121][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 14:56:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0026 |           1.9148 |           0.3045 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0067 |           1.8717 |           0.3037 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0088 |           1.8623 |           0.3035 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0110 |           1.8554 |           0.3034 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0086 |           1.8855 |           0.3033 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0123 |           1.8402 |           0.3030 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0147 |           1.8348 |           0.3030 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0145 |           1.8376 |           0.3032 |
[32m[20221213 14:56:57 @agent_ppo2.py:185][0m |          -0.0155 |           1.8285 |           0.3031 |
[32m[20221213 14:56:58 @agent_ppo2.py:185][0m |          -0.0133 |           1.8458 |           0.3030 |
[32m[20221213 14:56:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:56:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.87
[32m[20221213 14:56:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.28
[32m[20221213 14:56:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.25
[32m[20221213 14:56:58 @agent_ppo2.py:143][0m Total time:       4.04 min
[32m[20221213 14:56:58 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221213 14:56:58 @agent_ppo2.py:121][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 14:56:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:58 @agent_ppo2.py:185][0m |           0.0019 |           1.7119 |           0.3030 |
[32m[20221213 14:56:58 @agent_ppo2.py:185][0m |           0.0007 |           1.6857 |           0.3023 |
[32m[20221213 14:56:58 @agent_ppo2.py:185][0m |          -0.0108 |           1.5482 |           0.3014 |
[32m[20221213 14:56:58 @agent_ppo2.py:185][0m |          -0.0098 |           1.5108 |           0.3016 |
[32m[20221213 14:56:58 @agent_ppo2.py:185][0m |          -0.0124 |           1.4931 |           0.3014 |
[32m[20221213 14:56:59 @agent_ppo2.py:185][0m |          -0.0116 |           1.4840 |           0.3014 |
[32m[20221213 14:56:59 @agent_ppo2.py:185][0m |          -0.0114 |           1.4804 |           0.3012 |
[32m[20221213 14:56:59 @agent_ppo2.py:185][0m |          -0.0139 |           1.4669 |           0.3012 |
[32m[20221213 14:56:59 @agent_ppo2.py:185][0m |          -0.0146 |           1.4657 |           0.3008 |
[32m[20221213 14:56:59 @agent_ppo2.py:185][0m |          -0.0156 |           1.4618 |           0.3007 |
[32m[20221213 14:56:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:56:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.07
[32m[20221213 14:56:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.95
[32m[20221213 14:56:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.65
[32m[20221213 14:56:59 @agent_ppo2.py:143][0m Total time:       4.06 min
[32m[20221213 14:56:59 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221213 14:56:59 @agent_ppo2.py:121][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 14:56:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:56:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:56:59 @agent_ppo2.py:185][0m |          -0.0024 |           2.1506 |           0.2985 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |           0.0055 |           2.3387 |           0.2980 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0047 |           2.0857 |           0.2979 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0099 |           2.0591 |           0.2981 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0104 |           2.0520 |           0.2982 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0111 |           2.0555 |           0.2982 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0149 |           2.0320 |           0.2980 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0100 |           2.0334 |           0.2981 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0163 |           2.0288 |           0.2982 |
[32m[20221213 14:57:00 @agent_ppo2.py:185][0m |          -0.0160 |           2.0193 |           0.2982 |
[32m[20221213 14:57:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.14
[32m[20221213 14:57:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.20
[32m[20221213 14:57:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.23
[32m[20221213 14:57:00 @agent_ppo2.py:143][0m Total time:       4.09 min
[32m[20221213 14:57:00 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221213 14:57:00 @agent_ppo2.py:121][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 14:57:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |           0.0003 |           1.9088 |           0.3021 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |          -0.0048 |           1.8433 |           0.3020 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |          -0.0058 |           1.8319 |           0.3019 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |           0.0026 |           1.8538 |           0.3019 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |          -0.0116 |           1.7977 |           0.3018 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |          -0.0092 |           1.7939 |           0.3019 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |          -0.0134 |           1.7656 |           0.3017 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |          -0.0070 |           1.8503 |           0.3016 |
[32m[20221213 14:57:01 @agent_ppo2.py:185][0m |          -0.0136 |           1.7557 |           0.3016 |
[32m[20221213 14:57:02 @agent_ppo2.py:185][0m |          -0.0014 |           1.9878 |           0.3017 |
[32m[20221213 14:57:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:57:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.70
[32m[20221213 14:57:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.93
[32m[20221213 14:57:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.30
[32m[20221213 14:57:02 @agent_ppo2.py:143][0m Total time:       4.11 min
[32m[20221213 14:57:02 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221213 14:57:02 @agent_ppo2.py:121][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 14:57:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:02 @agent_ppo2.py:185][0m |          -0.0019 |           1.9963 |           0.3064 |
[32m[20221213 14:57:02 @agent_ppo2.py:185][0m |          -0.0066 |           1.8692 |           0.3060 |
[32m[20221213 14:57:02 @agent_ppo2.py:185][0m |          -0.0090 |           1.8308 |           0.3058 |
[32m[20221213 14:57:02 @agent_ppo2.py:185][0m |          -0.0056 |           1.8532 |           0.3055 |
[32m[20221213 14:57:02 @agent_ppo2.py:185][0m |          -0.0061 |           1.8817 |           0.3051 |
[32m[20221213 14:57:03 @agent_ppo2.py:185][0m |          -0.0129 |           1.7950 |           0.3051 |
[32m[20221213 14:57:03 @agent_ppo2.py:185][0m |          -0.0125 |           1.7621 |           0.3049 |
[32m[20221213 14:57:03 @agent_ppo2.py:185][0m |          -0.0134 |           1.7525 |           0.3049 |
[32m[20221213 14:57:03 @agent_ppo2.py:185][0m |          -0.0132 |           1.7419 |           0.3047 |
[32m[20221213 14:57:03 @agent_ppo2.py:185][0m |          -0.0156 |           1.7429 |           0.3047 |
[32m[20221213 14:57:03 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.13
[32m[20221213 14:57:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.49
[32m[20221213 14:57:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.42
[32m[20221213 14:57:03 @agent_ppo2.py:143][0m Total time:       4.13 min
[32m[20221213 14:57:03 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221213 14:57:03 @agent_ppo2.py:121][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 14:57:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:03 @agent_ppo2.py:185][0m |          -0.0037 |           2.3784 |           0.2910 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0064 |           2.2592 |           0.2902 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0039 |           2.2342 |           0.2897 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0103 |           2.2018 |           0.2895 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0122 |           2.1793 |           0.2895 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0167 |           2.1778 |           0.2895 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0123 |           2.1651 |           0.2897 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0140 |           2.1507 |           0.2895 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0156 |           2.1326 |           0.2896 |
[32m[20221213 14:57:04 @agent_ppo2.py:185][0m |          -0.0147 |           2.1216 |           0.2896 |
[32m[20221213 14:57:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.07
[32m[20221213 14:57:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.44
[32m[20221213 14:57:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.09
[32m[20221213 14:57:04 @agent_ppo2.py:143][0m Total time:       4.15 min
[32m[20221213 14:57:04 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221213 14:57:04 @agent_ppo2.py:121][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 14:57:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0003 |           2.0044 |           0.3033 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0003 |           1.9874 |           0.3027 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0116 |           1.9501 |           0.3020 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0140 |           1.9390 |           0.3018 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0056 |           1.9632 |           0.3018 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0122 |           1.9232 |           0.3012 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0148 |           1.9024 |           0.3012 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0131 |           1.9013 |           0.3011 |
[32m[20221213 14:57:05 @agent_ppo2.py:185][0m |          -0.0114 |           1.9058 |           0.3009 |
[32m[20221213 14:57:06 @agent_ppo2.py:185][0m |          -0.0175 |           1.8960 |           0.3010 |
[32m[20221213 14:57:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.04
[32m[20221213 14:57:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.67
[32m[20221213 14:57:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 92.75
[32m[20221213 14:57:06 @agent_ppo2.py:143][0m Total time:       4.18 min
[32m[20221213 14:57:06 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221213 14:57:06 @agent_ppo2.py:121][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 14:57:06 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:57:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:06 @agent_ppo2.py:185][0m |           0.0042 |           2.1130 |           0.3044 |
[32m[20221213 14:57:06 @agent_ppo2.py:185][0m |          -0.0065 |           2.0395 |           0.3044 |
[32m[20221213 14:57:06 @agent_ppo2.py:185][0m |          -0.0095 |           2.0292 |           0.3044 |
[32m[20221213 14:57:06 @agent_ppo2.py:185][0m |          -0.0103 |           2.0113 |           0.3047 |
[32m[20221213 14:57:06 @agent_ppo2.py:185][0m |          -0.0125 |           1.9983 |           0.3048 |
[32m[20221213 14:57:07 @agent_ppo2.py:185][0m |          -0.0119 |           1.9936 |           0.3047 |
[32m[20221213 14:57:07 @agent_ppo2.py:185][0m |          -0.0148 |           1.9876 |           0.3048 |
[32m[20221213 14:57:07 @agent_ppo2.py:185][0m |          -0.0146 |           1.9794 |           0.3050 |
[32m[20221213 14:57:07 @agent_ppo2.py:185][0m |          -0.0156 |           1.9732 |           0.3048 |
[32m[20221213 14:57:07 @agent_ppo2.py:185][0m |          -0.0143 |           1.9614 |           0.3051 |
[32m[20221213 14:57:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.12
[32m[20221213 14:57:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.71
[32m[20221213 14:57:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.87
[32m[20221213 14:57:07 @agent_ppo2.py:143][0m Total time:       4.20 min
[32m[20221213 14:57:07 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221213 14:57:07 @agent_ppo2.py:121][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 14:57:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:07 @agent_ppo2.py:185][0m |          -0.0008 |           2.0665 |           0.2986 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0055 |           2.0181 |           0.2981 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0095 |           2.0008 |           0.2975 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0103 |           1.9871 |           0.2972 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0105 |           1.9753 |           0.2970 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0110 |           1.9682 |           0.2969 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0135 |           1.9653 |           0.2965 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0123 |           1.9597 |           0.2964 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0112 |           1.9513 |           0.2963 |
[32m[20221213 14:57:08 @agent_ppo2.py:185][0m |          -0.0178 |           1.9433 |           0.2961 |
[32m[20221213 14:57:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:57:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.89
[32m[20221213 14:57:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.60
[32m[20221213 14:57:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.13
[32m[20221213 14:57:08 @agent_ppo2.py:143][0m Total time:       4.22 min
[32m[20221213 14:57:08 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221213 14:57:08 @agent_ppo2.py:121][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 14:57:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0040 |           2.0353 |           0.2946 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0097 |           1.9901 |           0.2939 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0017 |           2.0271 |           0.2934 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0124 |           1.9384 |           0.2931 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0115 |           1.9182 |           0.2928 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0102 |           1.9105 |           0.2926 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0097 |           1.9167 |           0.2924 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0150 |           1.8902 |           0.2922 |
[32m[20221213 14:57:09 @agent_ppo2.py:185][0m |          -0.0159 |           1.8874 |           0.2921 |
[32m[20221213 14:57:10 @agent_ppo2.py:185][0m |          -0.0155 |           1.8829 |           0.2919 |
[32m[20221213 14:57:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 74.53
[32m[20221213 14:57:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.00
[32m[20221213 14:57:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.06
[32m[20221213 14:57:10 @agent_ppo2.py:143][0m Total time:       4.24 min
[32m[20221213 14:57:10 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221213 14:57:10 @agent_ppo2.py:121][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 14:57:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:10 @agent_ppo2.py:185][0m |          -0.0020 |           1.9696 |           0.3000 |
[32m[20221213 14:57:10 @agent_ppo2.py:185][0m |          -0.0059 |           1.9469 |           0.2995 |
[32m[20221213 14:57:10 @agent_ppo2.py:185][0m |          -0.0083 |           1.9341 |           0.2994 |
[32m[20221213 14:57:10 @agent_ppo2.py:185][0m |          -0.0109 |           1.9244 |           0.2993 |
[32m[20221213 14:57:10 @agent_ppo2.py:185][0m |          -0.0088 |           1.9192 |           0.2993 |
[32m[20221213 14:57:11 @agent_ppo2.py:185][0m |          -0.0049 |           1.9331 |           0.2992 |
[32m[20221213 14:57:11 @agent_ppo2.py:185][0m |          -0.0016 |           2.0799 |           0.2994 |
[32m[20221213 14:57:11 @agent_ppo2.py:185][0m |          -0.0152 |           1.9147 |           0.2991 |
[32m[20221213 14:57:11 @agent_ppo2.py:185][0m |          -0.0106 |           1.9153 |           0.2993 |
[32m[20221213 14:57:11 @agent_ppo2.py:185][0m |          -0.0165 |           1.9044 |           0.2994 |
[32m[20221213 14:57:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.41
[32m[20221213 14:57:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.90
[32m[20221213 14:57:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.48
[32m[20221213 14:57:11 @agent_ppo2.py:143][0m Total time:       4.27 min
[32m[20221213 14:57:11 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221213 14:57:11 @agent_ppo2.py:121][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 14:57:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:11 @agent_ppo2.py:185][0m |           0.0008 |           1.9388 |           0.3030 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0041 |           1.8989 |           0.3026 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0069 |           1.8849 |           0.3023 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0079 |           1.8723 |           0.3018 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0119 |           1.8722 |           0.3016 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0113 |           1.8584 |           0.3013 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0138 |           1.8514 |           0.3011 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0117 |           1.8448 |           0.3010 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0142 |           1.8409 |           0.3006 |
[32m[20221213 14:57:12 @agent_ppo2.py:185][0m |          -0.0030 |           2.0751 |           0.3006 |
[32m[20221213 14:57:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.27
[32m[20221213 14:57:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.72
[32m[20221213 14:57:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.42
[32m[20221213 14:57:12 @agent_ppo2.py:143][0m Total time:       4.29 min
[32m[20221213 14:57:12 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221213 14:57:12 @agent_ppo2.py:121][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 14:57:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0041 |           1.8723 |           0.2949 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |           0.0032 |           1.9171 |           0.2945 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0083 |           1.7982 |           0.2942 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0122 |           1.7862 |           0.2943 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0125 |           1.7851 |           0.2943 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0118 |           1.7704 |           0.2943 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0072 |           1.8244 |           0.2944 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0150 |           1.7588 |           0.2947 |
[32m[20221213 14:57:13 @agent_ppo2.py:185][0m |          -0.0167 |           1.7573 |           0.2945 |
[32m[20221213 14:57:14 @agent_ppo2.py:185][0m |          -0.0169 |           1.7508 |           0.2948 |
[32m[20221213 14:57:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 76.44
[32m[20221213 14:57:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 81.77
[32m[20221213 14:57:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.54
[32m[20221213 14:57:14 @agent_ppo2.py:143][0m Total time:       4.31 min
[32m[20221213 14:57:14 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221213 14:57:14 @agent_ppo2.py:121][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 14:57:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:57:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:14 @agent_ppo2.py:185][0m |           0.0027 |           1.9858 |           0.2965 |
[32m[20221213 14:57:14 @agent_ppo2.py:185][0m |          -0.0053 |           1.9326 |           0.2965 |
[32m[20221213 14:57:14 @agent_ppo2.py:185][0m |          -0.0057 |           1.9175 |           0.2965 |
[32m[20221213 14:57:14 @agent_ppo2.py:185][0m |          -0.0104 |           1.9000 |           0.2966 |
[32m[20221213 14:57:14 @agent_ppo2.py:185][0m |          -0.0022 |           1.9732 |           0.2968 |
[32m[20221213 14:57:15 @agent_ppo2.py:185][0m |          -0.0126 |           1.8834 |           0.2969 |
[32m[20221213 14:57:15 @agent_ppo2.py:185][0m |          -0.0116 |           1.8620 |           0.2967 |
[32m[20221213 14:57:15 @agent_ppo2.py:185][0m |          -0.0127 |           1.8456 |           0.2970 |
[32m[20221213 14:57:15 @agent_ppo2.py:185][0m |          -0.0107 |           1.8419 |           0.2970 |
[32m[20221213 14:57:15 @agent_ppo2.py:185][0m |          -0.0093 |           1.8335 |           0.2970 |
[32m[20221213 14:57:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 75.03
[32m[20221213 14:57:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.99
[32m[20221213 14:57:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.76
[32m[20221213 14:57:15 @agent_ppo2.py:143][0m Total time:       4.33 min
[32m[20221213 14:57:15 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221213 14:57:15 @agent_ppo2.py:121][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 14:57:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:15 @agent_ppo2.py:185][0m |          -0.0007 |           2.0063 |           0.3066 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0065 |           1.9580 |           0.3062 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0057 |           1.9585 |           0.3058 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0080 |           1.9298 |           0.3054 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0062 |           1.9716 |           0.3055 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0062 |           2.0185 |           0.3052 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0023 |           2.0813 |           0.3052 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0119 |           1.9165 |           0.3047 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0150 |           1.9071 |           0.3052 |
[32m[20221213 14:57:16 @agent_ppo2.py:185][0m |          -0.0163 |           1.9051 |           0.3051 |
[32m[20221213 14:57:16 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 74.14
[32m[20221213 14:57:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.30
[32m[20221213 14:57:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.19
[32m[20221213 14:57:16 @agent_ppo2.py:143][0m Total time:       4.35 min
[32m[20221213 14:57:16 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221213 14:57:16 @agent_ppo2.py:121][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 14:57:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:57:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |           0.0026 |           1.9102 |           0.2966 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |           0.0018 |           1.9274 |           0.2964 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |          -0.0064 |           1.8716 |           0.2960 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |          -0.0061 |           1.8641 |           0.2960 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |          -0.0093 |           1.8565 |           0.2959 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |          -0.0002 |           2.0252 |           0.2957 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |          -0.0028 |           1.9427 |           0.2955 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |          -0.0117 |           1.8469 |           0.2953 |
[32m[20221213 14:57:17 @agent_ppo2.py:185][0m |          -0.0129 |           1.8413 |           0.2953 |
[32m[20221213 14:57:18 @agent_ppo2.py:185][0m |          -0.0109 |           1.8427 |           0.2955 |
[32m[20221213 14:57:18 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.50
[32m[20221213 14:57:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.03
[32m[20221213 14:57:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.16
[32m[20221213 14:57:18 @agent_ppo2.py:143][0m Total time:       4.38 min
[32m[20221213 14:57:18 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221213 14:57:18 @agent_ppo2.py:121][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 14:57:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:57:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:18 @agent_ppo2.py:185][0m |          -0.0033 |           1.8708 |           0.2895 |
[32m[20221213 14:57:18 @agent_ppo2.py:185][0m |          -0.0054 |           1.8475 |           0.2886 |
[32m[20221213 14:57:18 @agent_ppo2.py:185][0m |          -0.0047 |           1.8441 |           0.2881 |
[32m[20221213 14:57:18 @agent_ppo2.py:185][0m |          -0.0018 |           1.8973 |           0.2880 |
[32m[20221213 14:57:18 @agent_ppo2.py:185][0m |          -0.0120 |           1.8265 |           0.2878 |
[32m[20221213 14:57:19 @agent_ppo2.py:185][0m |          -0.0115 |           1.8269 |           0.2876 |
[32m[20221213 14:57:19 @agent_ppo2.py:185][0m |          -0.0109 |           1.8199 |           0.2876 |
[32m[20221213 14:57:19 @agent_ppo2.py:185][0m |          -0.0130 |           1.8168 |           0.2875 |
[32m[20221213 14:57:19 @agent_ppo2.py:185][0m |          -0.0124 |           1.8187 |           0.2873 |
[32m[20221213 14:57:19 @agent_ppo2.py:185][0m |          -0.0127 |           1.8169 |           0.2874 |
[32m[20221213 14:57:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.07
[32m[20221213 14:57:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.35
[32m[20221213 14:57:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.82
[32m[20221213 14:57:19 @agent_ppo2.py:143][0m Total time:       4.40 min
[32m[20221213 14:57:19 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221213 14:57:19 @agent_ppo2.py:121][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 14:57:19 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:57:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:19 @agent_ppo2.py:185][0m |          -0.0011 |           1.9349 |           0.2952 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0032 |           1.9146 |           0.2951 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0084 |           1.9056 |           0.2950 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0088 |           1.8980 |           0.2948 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0041 |           1.9514 |           0.2947 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0098 |           1.8986 |           0.2945 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0134 |           1.8886 |           0.2942 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0116 |           1.8816 |           0.2943 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0060 |           1.9355 |           0.2941 |
[32m[20221213 14:57:20 @agent_ppo2.py:185][0m |          -0.0110 |           1.8826 |           0.2939 |
[32m[20221213 14:57:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.24
[32m[20221213 14:57:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.28
[32m[20221213 14:57:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 91.48
[32m[20221213 14:57:20 @agent_ppo2.py:143][0m Total time:       4.42 min
[32m[20221213 14:57:20 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221213 14:57:20 @agent_ppo2.py:121][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 14:57:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0001 |           1.8980 |           0.2944 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0056 |           1.8884 |           0.2941 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0083 |           1.8788 |           0.2938 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0083 |           1.8773 |           0.2939 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0095 |           1.8687 |           0.2937 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0113 |           1.8634 |           0.2933 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0114 |           1.8613 |           0.2933 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0121 |           1.8559 |           0.2931 |
[32m[20221213 14:57:21 @agent_ppo2.py:185][0m |          -0.0140 |           1.8558 |           0.2928 |
[32m[20221213 14:57:22 @agent_ppo2.py:185][0m |           0.0035 |           2.0946 |           0.2929 |
[32m[20221213 14:57:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.22
[32m[20221213 14:57:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.61
[32m[20221213 14:57:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.14
[32m[20221213 14:57:22 @agent_ppo2.py:143][0m Total time:       4.44 min
[32m[20221213 14:57:22 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221213 14:57:22 @agent_ppo2.py:121][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 14:57:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:22 @agent_ppo2.py:185][0m |          -0.0009 |           1.8652 |           0.2977 |
[32m[20221213 14:57:22 @agent_ppo2.py:185][0m |          -0.0080 |           1.8101 |           0.2977 |
[32m[20221213 14:57:22 @agent_ppo2.py:185][0m |          -0.0085 |           1.7976 |           0.2977 |
[32m[20221213 14:57:22 @agent_ppo2.py:185][0m |          -0.0110 |           1.7820 |           0.2979 |
[32m[20221213 14:57:22 @agent_ppo2.py:185][0m |          -0.0134 |           1.7828 |           0.2981 |
[32m[20221213 14:57:23 @agent_ppo2.py:185][0m |          -0.0092 |           1.8318 |           0.2983 |
[32m[20221213 14:57:23 @agent_ppo2.py:185][0m |          -0.0133 |           1.7798 |           0.2986 |
[32m[20221213 14:57:23 @agent_ppo2.py:185][0m |          -0.0147 |           1.7582 |           0.2987 |
[32m[20221213 14:57:23 @agent_ppo2.py:185][0m |          -0.0075 |           1.8917 |           0.2991 |
[32m[20221213 14:57:23 @agent_ppo2.py:185][0m |          -0.0186 |           1.7541 |           0.2991 |
[32m[20221213 14:57:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.79
[32m[20221213 14:57:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.65
[32m[20221213 14:57:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.76
[32m[20221213 14:57:23 @agent_ppo2.py:143][0m Total time:       4.46 min
[32m[20221213 14:57:23 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221213 14:57:23 @agent_ppo2.py:121][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 14:57:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:23 @agent_ppo2.py:185][0m |          -0.0025 |           1.9531 |           0.3109 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0074 |           1.9041 |           0.3102 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0104 |           1.8696 |           0.3098 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0108 |           1.8480 |           0.3095 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0125 |           1.8324 |           0.3093 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0138 |           1.8156 |           0.3093 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0117 |           1.8269 |           0.3089 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0141 |           1.7949 |           0.3090 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0177 |           1.7807 |           0.3086 |
[32m[20221213 14:57:24 @agent_ppo2.py:185][0m |          -0.0187 |           1.7710 |           0.3085 |
[32m[20221213 14:57:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.52
[32m[20221213 14:57:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.23
[32m[20221213 14:57:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.58
[32m[20221213 14:57:24 @agent_ppo2.py:143][0m Total time:       4.49 min
[32m[20221213 14:57:24 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221213 14:57:24 @agent_ppo2.py:121][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 14:57:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:57:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0007 |           1.9447 |           0.3018 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |           0.0035 |           2.1085 |           0.3018 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0011 |           1.9846 |           0.3017 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0105 |           1.8926 |           0.3016 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0111 |           1.8921 |           0.3017 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0094 |           1.8794 |           0.3019 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0108 |           1.8809 |           0.3019 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0142 |           1.8814 |           0.3020 |
[32m[20221213 14:57:25 @agent_ppo2.py:185][0m |          -0.0143 |           1.8743 |           0.3021 |
[32m[20221213 14:57:26 @agent_ppo2.py:185][0m |          -0.0130 |           1.8663 |           0.3021 |
[32m[20221213 14:57:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.11
[32m[20221213 14:57:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.79
[32m[20221213 14:57:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.45
[32m[20221213 14:57:26 @agent_ppo2.py:143][0m Total time:       4.51 min
[32m[20221213 14:57:26 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221213 14:57:26 @agent_ppo2.py:121][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 14:57:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:26 @agent_ppo2.py:185][0m |           0.0079 |           2.1030 |           0.2971 |
[32m[20221213 14:57:26 @agent_ppo2.py:185][0m |          -0.0066 |           1.9958 |           0.2968 |
[32m[20221213 14:57:26 @agent_ppo2.py:185][0m |          -0.0052 |           1.9731 |           0.2964 |
[32m[20221213 14:57:26 @agent_ppo2.py:185][0m |          -0.0075 |           1.9746 |           0.2962 |
[32m[20221213 14:57:26 @agent_ppo2.py:185][0m |          -0.0101 |           1.9563 |           0.2959 |
[32m[20221213 14:57:27 @agent_ppo2.py:185][0m |          -0.0123 |           1.9450 |           0.2958 |
[32m[20221213 14:57:27 @agent_ppo2.py:185][0m |          -0.0103 |           1.9342 |           0.2957 |
[32m[20221213 14:57:27 @agent_ppo2.py:185][0m |          -0.0139 |           1.9233 |           0.2955 |
[32m[20221213 14:57:27 @agent_ppo2.py:185][0m |          -0.0055 |           2.0202 |           0.2953 |
[32m[20221213 14:57:27 @agent_ppo2.py:185][0m |          -0.0107 |           1.9290 |           0.2951 |
[32m[20221213 14:57:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.16
[32m[20221213 14:57:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.69
[32m[20221213 14:57:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.80
[32m[20221213 14:57:27 @agent_ppo2.py:143][0m Total time:       4.53 min
[32m[20221213 14:57:27 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221213 14:57:27 @agent_ppo2.py:121][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 14:57:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:27 @agent_ppo2.py:185][0m |          -0.0025 |           1.8697 |           0.3030 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |           0.0012 |           1.8809 |           0.3030 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0071 |           1.7867 |           0.3031 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0112 |           1.7884 |           0.3029 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0050 |           1.8219 |           0.3027 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0042 |           1.9582 |           0.3027 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0130 |           1.7659 |           0.3026 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0152 |           1.7478 |           0.3025 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0145 |           1.7503 |           0.3024 |
[32m[20221213 14:57:28 @agent_ppo2.py:185][0m |          -0.0119 |           1.7430 |           0.3023 |
[32m[20221213 14:57:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 77.50
[32m[20221213 14:57:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.07
[32m[20221213 14:57:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.14
[32m[20221213 14:57:28 @agent_ppo2.py:143][0m Total time:       4.55 min
[32m[20221213 14:57:28 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221213 14:57:28 @agent_ppo2.py:121][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 14:57:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |           0.0045 |           1.9358 |           0.2932 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0026 |           1.9049 |           0.2929 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0098 |           1.8213 |           0.2927 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0078 |           1.8112 |           0.2927 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0160 |           1.7841 |           0.2928 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0138 |           1.7810 |           0.2928 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0108 |           1.7614 |           0.2929 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0165 |           1.7412 |           0.2929 |
[32m[20221213 14:57:29 @agent_ppo2.py:185][0m |          -0.0160 |           1.7364 |           0.2931 |
[32m[20221213 14:57:30 @agent_ppo2.py:185][0m |          -0.0143 |           1.7340 |           0.2930 |
[32m[20221213 14:57:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.18
[32m[20221213 14:57:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 85.54
[32m[20221213 14:57:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.08
[32m[20221213 14:57:30 @agent_ppo2.py:143][0m Total time:       4.58 min
[32m[20221213 14:57:30 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221213 14:57:30 @agent_ppo2.py:121][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 14:57:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:30 @agent_ppo2.py:185][0m |          -0.0004 |           2.0492 |           0.3026 |
[32m[20221213 14:57:30 @agent_ppo2.py:185][0m |          -0.0041 |           2.0025 |           0.3028 |
[32m[20221213 14:57:30 @agent_ppo2.py:185][0m |          -0.0013 |           2.0724 |           0.3026 |
[32m[20221213 14:57:30 @agent_ppo2.py:185][0m |          -0.0121 |           1.9970 |           0.3023 |
[32m[20221213 14:57:30 @agent_ppo2.py:185][0m |          -0.0075 |           1.9732 |           0.3022 |
[32m[20221213 14:57:31 @agent_ppo2.py:185][0m |          -0.0111 |           1.9725 |           0.3021 |
[32m[20221213 14:57:31 @agent_ppo2.py:185][0m |          -0.0166 |           1.9790 |           0.3023 |
[32m[20221213 14:57:31 @agent_ppo2.py:185][0m |          -0.0137 |           1.9634 |           0.3023 |
[32m[20221213 14:57:31 @agent_ppo2.py:185][0m |          -0.0109 |           1.9812 |           0.3022 |
[32m[20221213 14:57:31 @agent_ppo2.py:185][0m |          -0.0173 |           1.9537 |           0.3022 |
[32m[20221213 14:57:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.76
[32m[20221213 14:57:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.22
[32m[20221213 14:57:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.20
[32m[20221213 14:57:31 @agent_ppo2.py:143][0m Total time:       4.60 min
[32m[20221213 14:57:31 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221213 14:57:31 @agent_ppo2.py:121][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 14:57:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:31 @agent_ppo2.py:185][0m |           0.0019 |           1.7909 |           0.2962 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0038 |           1.7366 |           0.2957 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0081 |           1.7192 |           0.2954 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0061 |           1.7071 |           0.2953 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0048 |           1.7459 |           0.2949 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0142 |           1.6872 |           0.2947 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0114 |           1.6763 |           0.2944 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0083 |           1.6947 |           0.2943 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0142 |           1.6689 |           0.2939 |
[32m[20221213 14:57:32 @agent_ppo2.py:185][0m |          -0.0165 |           1.6667 |           0.2937 |
[32m[20221213 14:57:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:57:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 60.62
[32m[20221213 14:57:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 77.28
[32m[20221213 14:57:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.10
[32m[20221213 14:57:32 @agent_ppo2.py:143][0m Total time:       4.62 min
[32m[20221213 14:57:32 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221213 14:57:32 @agent_ppo2.py:121][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 14:57:33 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:57:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |           0.0137 |           2.2461 |           0.2965 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0062 |           1.9920 |           0.2959 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0014 |           1.9839 |           0.2956 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0105 |           1.9521 |           0.2954 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0124 |           1.9521 |           0.2952 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0138 |           1.9429 |           0.2949 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0119 |           1.9418 |           0.2948 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0167 |           1.9401 |           0.2948 |
[32m[20221213 14:57:33 @agent_ppo2.py:185][0m |          -0.0179 |           1.9426 |           0.2945 |
[32m[20221213 14:57:34 @agent_ppo2.py:185][0m |          -0.0157 |           1.9320 |           0.2946 |
[32m[20221213 14:57:34 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.23
[32m[20221213 14:57:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.59
[32m[20221213 14:57:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.92
[32m[20221213 14:57:34 @agent_ppo2.py:143][0m Total time:       4.64 min
[32m[20221213 14:57:34 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221213 14:57:34 @agent_ppo2.py:121][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 14:57:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:34 @agent_ppo2.py:185][0m |          -0.0019 |           2.0018 |           0.2960 |
[32m[20221213 14:57:34 @agent_ppo2.py:185][0m |          -0.0066 |           1.9778 |           0.2961 |
[32m[20221213 14:57:34 @agent_ppo2.py:185][0m |          -0.0060 |           1.9495 |           0.2961 |
[32m[20221213 14:57:34 @agent_ppo2.py:185][0m |          -0.0096 |           1.9467 |           0.2960 |
[32m[20221213 14:57:34 @agent_ppo2.py:185][0m |          -0.0123 |           1.9244 |           0.2961 |
[32m[20221213 14:57:35 @agent_ppo2.py:185][0m |          -0.0140 |           1.9095 |           0.2962 |
[32m[20221213 14:57:35 @agent_ppo2.py:185][0m |          -0.0101 |           1.8993 |           0.2963 |
[32m[20221213 14:57:35 @agent_ppo2.py:185][0m |          -0.0126 |           1.9082 |           0.2962 |
[32m[20221213 14:57:35 @agent_ppo2.py:185][0m |          -0.0142 |           1.8822 |           0.2962 |
[32m[20221213 14:57:35 @agent_ppo2.py:185][0m |          -0.0156 |           1.8744 |           0.2963 |
[32m[20221213 14:57:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.35
[32m[20221213 14:57:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.67
[32m[20221213 14:57:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.70
[32m[20221213 14:57:35 @agent_ppo2.py:143][0m Total time:       4.66 min
[32m[20221213 14:57:35 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221213 14:57:35 @agent_ppo2.py:121][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 14:57:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:35 @agent_ppo2.py:185][0m |          -0.0020 |           2.0873 |           0.2961 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0110 |           2.0580 |           0.2954 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |           0.0024 |           2.1368 |           0.2953 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0106 |           2.0602 |           0.2953 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0124 |           2.0308 |           0.2952 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0133 |           2.0215 |           0.2950 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0148 |           2.0143 |           0.2950 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0103 |           2.0242 |           0.2946 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0167 |           2.0090 |           0.2947 |
[32m[20221213 14:57:36 @agent_ppo2.py:185][0m |          -0.0189 |           2.0069 |           0.2946 |
[32m[20221213 14:57:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.79
[32m[20221213 14:57:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.11
[32m[20221213 14:57:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.33
[32m[20221213 14:57:36 @agent_ppo2.py:143][0m Total time:       4.69 min
[32m[20221213 14:57:36 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221213 14:57:36 @agent_ppo2.py:121][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 14:57:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0001 |           2.0086 |           0.3000 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0053 |           1.9514 |           0.2996 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0091 |           1.9218 |           0.2991 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0116 |           1.8996 |           0.2987 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0129 |           1.8878 |           0.2985 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0141 |           1.8761 |           0.2983 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0136 |           1.8576 |           0.2982 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0148 |           1.8548 |           0.2980 |
[32m[20221213 14:57:37 @agent_ppo2.py:185][0m |          -0.0124 |           1.8533 |           0.2980 |
[32m[20221213 14:57:38 @agent_ppo2.py:185][0m |          -0.0167 |           1.8306 |           0.2979 |
[32m[20221213 14:57:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.89
[32m[20221213 14:57:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.06
[32m[20221213 14:57:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.11
[32m[20221213 14:57:38 @agent_ppo2.py:143][0m Total time:       4.71 min
[32m[20221213 14:57:38 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221213 14:57:38 @agent_ppo2.py:121][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 14:57:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:38 @agent_ppo2.py:185][0m |          -0.0016 |           2.0290 |           0.2942 |
[32m[20221213 14:57:38 @agent_ppo2.py:185][0m |          -0.0042 |           1.9941 |           0.2936 |
[32m[20221213 14:57:38 @agent_ppo2.py:185][0m |          -0.0074 |           1.9846 |           0.2930 |
[32m[20221213 14:57:38 @agent_ppo2.py:185][0m |          -0.0005 |           2.0579 |           0.2927 |
[32m[20221213 14:57:39 @agent_ppo2.py:185][0m |          -0.0107 |           1.9683 |           0.2926 |
[32m[20221213 14:57:39 @agent_ppo2.py:185][0m |          -0.0114 |           1.9608 |           0.2923 |
[32m[20221213 14:57:39 @agent_ppo2.py:185][0m |          -0.0123 |           1.9570 |           0.2921 |
[32m[20221213 14:57:39 @agent_ppo2.py:185][0m |          -0.0133 |           1.9556 |           0.2920 |
[32m[20221213 14:57:39 @agent_ppo2.py:185][0m |          -0.0041 |           2.0910 |           0.2919 |
[32m[20221213 14:57:39 @agent_ppo2.py:185][0m |          -0.0107 |           1.9535 |           0.2915 |
[32m[20221213 14:57:39 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 14:57:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.40
[32m[20221213 14:57:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.48
[32m[20221213 14:57:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.97
[32m[20221213 14:57:39 @agent_ppo2.py:143][0m Total time:       4.73 min
[32m[20221213 14:57:39 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221213 14:57:39 @agent_ppo2.py:121][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 14:57:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0013 |           1.9870 |           0.2902 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0071 |           1.9251 |           0.2896 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0077 |           1.8964 |           0.2895 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0140 |           1.8719 |           0.2891 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0123 |           1.8543 |           0.2891 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0117 |           1.8350 |           0.2890 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0130 |           1.8268 |           0.2889 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0131 |           1.8042 |           0.2890 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0131 |           1.8055 |           0.2888 |
[32m[20221213 14:57:40 @agent_ppo2.py:185][0m |          -0.0142 |           1.7912 |           0.2887 |
[32m[20221213 14:57:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:57:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.41
[32m[20221213 14:57:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.80
[32m[20221213 14:57:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.50
[32m[20221213 14:57:41 @agent_ppo2.py:143][0m Total time:       4.76 min
[32m[20221213 14:57:41 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221213 14:57:41 @agent_ppo2.py:121][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 14:57:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:41 @agent_ppo2.py:185][0m |           0.0025 |           2.1781 |           0.2913 |
[32m[20221213 14:57:41 @agent_ppo2.py:185][0m |          -0.0065 |           2.1068 |           0.2911 |
[32m[20221213 14:57:41 @agent_ppo2.py:185][0m |          -0.0100 |           2.0765 |           0.2910 |
[32m[20221213 14:57:41 @agent_ppo2.py:185][0m |          -0.0070 |           2.0820 |           0.2908 |
[32m[20221213 14:57:41 @agent_ppo2.py:185][0m |          -0.0168 |           2.0447 |           0.2907 |
[32m[20221213 14:57:41 @agent_ppo2.py:185][0m |          -0.0143 |           2.0188 |           0.2906 |
[32m[20221213 14:57:41 @agent_ppo2.py:185][0m |          -0.0156 |           2.0174 |           0.2906 |
[32m[20221213 14:57:42 @agent_ppo2.py:185][0m |          -0.0158 |           2.0119 |           0.2905 |
[32m[20221213 14:57:42 @agent_ppo2.py:185][0m |          -0.0139 |           2.0479 |           0.2906 |
[32m[20221213 14:57:42 @agent_ppo2.py:185][0m |          -0.0064 |           2.1025 |           0.2907 |
[32m[20221213 14:57:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 14:57:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.31
[32m[20221213 14:57:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.32
[32m[20221213 14:57:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 57.82
[32m[20221213 14:57:42 @agent_ppo2.py:143][0m Total time:       4.78 min
[32m[20221213 14:57:42 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221213 14:57:42 @agent_ppo2.py:121][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 14:57:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:42 @agent_ppo2.py:185][0m |          -0.0014 |           2.0822 |           0.2947 |
[32m[20221213 14:57:42 @agent_ppo2.py:185][0m |          -0.0076 |           2.0389 |           0.2944 |
[32m[20221213 14:57:42 @agent_ppo2.py:185][0m |          -0.0117 |           2.0313 |           0.2939 |
[32m[20221213 14:57:43 @agent_ppo2.py:185][0m |          -0.0061 |           2.0287 |           0.2938 |
[32m[20221213 14:57:43 @agent_ppo2.py:185][0m |          -0.0097 |           2.0385 |           0.2940 |
[32m[20221213 14:57:43 @agent_ppo2.py:185][0m |          -0.0141 |           2.0164 |           0.2940 |
[32m[20221213 14:57:43 @agent_ppo2.py:185][0m |          -0.0130 |           2.0133 |           0.2944 |
[32m[20221213 14:57:43 @agent_ppo2.py:185][0m |          -0.0152 |           2.0057 |           0.2942 |
[32m[20221213 14:57:43 @agent_ppo2.py:185][0m |          -0.0163 |           2.0060 |           0.2944 |
[32m[20221213 14:57:43 @agent_ppo2.py:185][0m |          -0.0071 |           2.2357 |           0.2946 |
[32m[20221213 14:57:43 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:57:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.27
[32m[20221213 14:57:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.99
[32m[20221213 14:57:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.34
[32m[20221213 14:57:43 @agent_ppo2.py:143][0m Total time:       4.80 min
[32m[20221213 14:57:43 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221213 14:57:43 @agent_ppo2.py:121][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 14:57:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0014 |           2.0752 |           0.2962 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0015 |           2.0819 |           0.2960 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0104 |           2.0358 |           0.2957 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0126 |           2.0345 |           0.2956 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0124 |           2.0231 |           0.2957 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0122 |           2.0151 |           0.2958 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0144 |           2.0135 |           0.2959 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0149 |           1.9978 |           0.2960 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0136 |           2.0000 |           0.2962 |
[32m[20221213 14:57:44 @agent_ppo2.py:185][0m |          -0.0141 |           2.0069 |           0.2963 |
[32m[20221213 14:57:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:57:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.24
[32m[20221213 14:57:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.45
[32m[20221213 14:57:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.46
[32m[20221213 14:57:45 @agent_ppo2.py:143][0m Total time:       4.82 min
[32m[20221213 14:57:45 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221213 14:57:45 @agent_ppo2.py:121][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 14:57:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:45 @agent_ppo2.py:185][0m |          -0.0013 |           2.0688 |           0.3015 |
[32m[20221213 14:57:45 @agent_ppo2.py:185][0m |           0.0033 |           2.1332 |           0.3008 |
[32m[20221213 14:57:45 @agent_ppo2.py:185][0m |          -0.0057 |           2.0287 |           0.3007 |
[32m[20221213 14:57:45 @agent_ppo2.py:185][0m |          -0.0107 |           2.0076 |           0.3006 |
[32m[20221213 14:57:45 @agent_ppo2.py:185][0m |           0.0006 |           2.2408 |           0.3005 |
[32m[20221213 14:57:45 @agent_ppo2.py:185][0m |          -0.0108 |           1.9974 |           0.3003 |
[32m[20221213 14:57:46 @agent_ppo2.py:185][0m |          -0.0132 |           1.9861 |           0.3006 |
[32m[20221213 14:57:46 @agent_ppo2.py:185][0m |          -0.0143 |           1.9929 |           0.3007 |
[32m[20221213 14:57:46 @agent_ppo2.py:185][0m |          -0.0139 |           1.9765 |           0.3006 |
[32m[20221213 14:57:46 @agent_ppo2.py:185][0m |          -0.0130 |           1.9673 |           0.3007 |
[32m[20221213 14:57:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.57
[32m[20221213 14:57:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.48
[32m[20221213 14:57:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.39
[32m[20221213 14:57:46 @agent_ppo2.py:143][0m Total time:       4.85 min
[32m[20221213 14:57:46 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221213 14:57:46 @agent_ppo2.py:121][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 14:57:46 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:57:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:46 @agent_ppo2.py:185][0m |          -0.0038 |           2.0730 |           0.3001 |
[32m[20221213 14:57:46 @agent_ppo2.py:185][0m |          -0.0076 |           2.0299 |           0.2994 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |          -0.0056 |           2.0218 |           0.2990 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |          -0.0111 |           2.0013 |           0.2987 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |          -0.0119 |           1.9970 |           0.2984 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |          -0.0109 |           1.9855 |           0.2984 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |          -0.0137 |           1.9674 |           0.2982 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |           0.0040 |           2.2580 |           0.2981 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |          -0.0141 |           1.9667 |           0.2977 |
[32m[20221213 14:57:47 @agent_ppo2.py:185][0m |          -0.0130 |           1.9568 |           0.2975 |
[32m[20221213 14:57:47 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.05
[32m[20221213 14:57:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.20
[32m[20221213 14:57:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.77
[32m[20221213 14:57:47 @agent_ppo2.py:143][0m Total time:       4.87 min
[32m[20221213 14:57:47 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221213 14:57:47 @agent_ppo2.py:121][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 14:57:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |           0.0007 |           2.0327 |           0.2980 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |           0.0039 |           2.1944 |           0.2974 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0024 |           2.0058 |           0.2967 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0093 |           1.9675 |           0.2965 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0132 |           1.9651 |           0.2966 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0110 |           1.9533 |           0.2964 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0121 |           1.9517 |           0.2963 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0096 |           1.9562 |           0.2960 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0133 |           1.9556 |           0.2958 |
[32m[20221213 14:57:48 @agent_ppo2.py:185][0m |          -0.0145 |           1.9383 |           0.2957 |
[32m[20221213 14:57:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.35
[32m[20221213 14:57:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.58
[32m[20221213 14:57:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.30
[32m[20221213 14:57:49 @agent_ppo2.py:143][0m Total time:       4.89 min
[32m[20221213 14:57:49 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221213 14:57:49 @agent_ppo2.py:121][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 14:57:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:49 @agent_ppo2.py:185][0m |           0.0137 |           2.1952 |           0.2954 |
[32m[20221213 14:57:49 @agent_ppo2.py:185][0m |          -0.0048 |           1.9708 |           0.2951 |
[32m[20221213 14:57:49 @agent_ppo2.py:185][0m |          -0.0051 |           1.9419 |           0.2950 |
[32m[20221213 14:57:49 @agent_ppo2.py:185][0m |          -0.0084 |           1.9387 |           0.2951 |
[32m[20221213 14:57:49 @agent_ppo2.py:185][0m |          -0.0097 |           1.9271 |           0.2950 |
[32m[20221213 14:57:49 @agent_ppo2.py:185][0m |          -0.0123 |           1.9236 |           0.2949 |
[32m[20221213 14:57:50 @agent_ppo2.py:185][0m |          -0.0124 |           1.9201 |           0.2949 |
[32m[20221213 14:57:50 @agent_ppo2.py:185][0m |          -0.0036 |           1.9744 |           0.2948 |
[32m[20221213 14:57:50 @agent_ppo2.py:185][0m |          -0.0121 |           1.9087 |           0.2949 |
[32m[20221213 14:57:50 @agent_ppo2.py:185][0m |          -0.0141 |           1.8994 |           0.2948 |
[32m[20221213 14:57:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.47
[32m[20221213 14:57:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 86.39
[32m[20221213 14:57:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.65
[32m[20221213 14:57:50 @agent_ppo2.py:143][0m Total time:       4.91 min
[32m[20221213 14:57:50 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221213 14:57:50 @agent_ppo2.py:121][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 14:57:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:50 @agent_ppo2.py:185][0m |          -0.0041 |           1.9828 |           0.2975 |
[32m[20221213 14:57:50 @agent_ppo2.py:185][0m |          -0.0074 |           1.9589 |           0.2972 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0092 |           1.9486 |           0.2972 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0111 |           1.9461 |           0.2970 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0106 |           1.9355 |           0.2970 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0107 |           1.9415 |           0.2969 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0126 |           1.9359 |           0.2969 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0106 |           1.9450 |           0.2970 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0141 |           1.9309 |           0.2971 |
[32m[20221213 14:57:51 @agent_ppo2.py:185][0m |          -0.0135 |           1.9242 |           0.2970 |
[32m[20221213 14:57:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.46
[32m[20221213 14:57:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.21
[32m[20221213 14:57:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.13
[32m[20221213 14:57:51 @agent_ppo2.py:143][0m Total time:       4.94 min
[32m[20221213 14:57:51 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221213 14:57:51 @agent_ppo2.py:121][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 14:57:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |           0.0004 |           2.1009 |           0.3015 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0066 |           2.0723 |           0.3008 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0081 |           2.0563 |           0.3006 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0128 |           2.0534 |           0.3003 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0108 |           2.0353 |           0.2999 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0152 |           2.0267 |           0.2999 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0156 |           2.0217 |           0.2997 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0130 |           2.0282 |           0.2994 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0165 |           1.9985 |           0.2994 |
[32m[20221213 14:57:52 @agent_ppo2.py:185][0m |          -0.0183 |           1.9971 |           0.2994 |
[32m[20221213 14:57:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.23
[32m[20221213 14:57:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.31
[32m[20221213 14:57:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.90
[32m[20221213 14:57:53 @agent_ppo2.py:143][0m Total time:       4.96 min
[32m[20221213 14:57:53 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221213 14:57:53 @agent_ppo2.py:121][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 14:57:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:53 @agent_ppo2.py:185][0m |           0.0061 |           2.2224 |           0.2944 |
[32m[20221213 14:57:53 @agent_ppo2.py:185][0m |          -0.0022 |           2.0914 |           0.2936 |
[32m[20221213 14:57:53 @agent_ppo2.py:185][0m |          -0.0105 |           2.0756 |           0.2932 |
[32m[20221213 14:57:53 @agent_ppo2.py:185][0m |          -0.0052 |           2.1102 |           0.2929 |
[32m[20221213 14:57:53 @agent_ppo2.py:185][0m |          -0.0095 |           2.0686 |           0.2926 |
[32m[20221213 14:57:53 @agent_ppo2.py:185][0m |          -0.0004 |           2.2726 |           0.2923 |
[32m[20221213 14:57:54 @agent_ppo2.py:185][0m |          -0.0125 |           2.0442 |           0.2922 |
[32m[20221213 14:57:54 @agent_ppo2.py:185][0m |          -0.0158 |           2.0314 |           0.2921 |
[32m[20221213 14:57:54 @agent_ppo2.py:185][0m |          -0.0138 |           2.0262 |           0.2919 |
[32m[20221213 14:57:54 @agent_ppo2.py:185][0m |          -0.0165 |           2.0212 |           0.2917 |
[32m[20221213 14:57:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.96
[32m[20221213 14:57:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.89
[32m[20221213 14:57:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.73
[32m[20221213 14:57:54 @agent_ppo2.py:143][0m Total time:       4.98 min
[32m[20221213 14:57:54 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221213 14:57:54 @agent_ppo2.py:121][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 14:57:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:54 @agent_ppo2.py:185][0m |          -0.0031 |           2.0457 |           0.2941 |
[32m[20221213 14:57:54 @agent_ppo2.py:185][0m |          -0.0082 |           2.0176 |           0.2939 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0097 |           1.9966 |           0.2933 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0118 |           1.9965 |           0.2932 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0133 |           1.9838 |           0.2932 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0121 |           1.9732 |           0.2928 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0138 |           1.9673 |           0.2927 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0157 |           1.9596 |           0.2928 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0058 |           2.0507 |           0.2928 |
[32m[20221213 14:57:55 @agent_ppo2.py:185][0m |          -0.0147 |           1.9578 |           0.2924 |
[32m[20221213 14:57:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.24
[32m[20221213 14:57:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.82
[32m[20221213 14:57:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.55
[32m[20221213 14:57:55 @agent_ppo2.py:143][0m Total time:       5.00 min
[32m[20221213 14:57:55 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221213 14:57:55 @agent_ppo2.py:121][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 14:57:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |           0.0023 |           1.9960 |           0.2931 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |          -0.0058 |           1.9203 |           0.2930 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |           0.0013 |           1.9487 |           0.2929 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |          -0.0070 |           1.9154 |           0.2927 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |          -0.0103 |           1.8635 |           0.2926 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |          -0.0057 |           1.8869 |           0.2927 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |          -0.0135 |           1.8372 |           0.2927 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |          -0.0137 |           1.8318 |           0.2929 |
[32m[20221213 14:57:56 @agent_ppo2.py:185][0m |          -0.0134 |           1.8214 |           0.2928 |
[32m[20221213 14:57:57 @agent_ppo2.py:185][0m |          -0.0145 |           1.8150 |           0.2931 |
[32m[20221213 14:57:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.75
[32m[20221213 14:57:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.48
[32m[20221213 14:57:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.92
[32m[20221213 14:57:57 @agent_ppo2.py:143][0m Total time:       5.03 min
[32m[20221213 14:57:57 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221213 14:57:57 @agent_ppo2.py:121][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 14:57:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:57 @agent_ppo2.py:185][0m |          -0.0018 |           2.0595 |           0.2896 |
[32m[20221213 14:57:57 @agent_ppo2.py:185][0m |          -0.0083 |           1.9635 |           0.2888 |
[32m[20221213 14:57:57 @agent_ppo2.py:185][0m |          -0.0101 |           1.9279 |           0.2884 |
[32m[20221213 14:57:57 @agent_ppo2.py:185][0m |          -0.0080 |           1.9386 |           0.2884 |
[32m[20221213 14:57:57 @agent_ppo2.py:185][0m |          -0.0149 |           1.8768 |           0.2881 |
[32m[20221213 14:57:57 @agent_ppo2.py:185][0m |          -0.0031 |           2.1024 |           0.2882 |
[32m[20221213 14:57:58 @agent_ppo2.py:185][0m |          -0.0159 |           1.8486 |           0.2882 |
[32m[20221213 14:57:58 @agent_ppo2.py:185][0m |          -0.0161 |           1.8329 |           0.2879 |
[32m[20221213 14:57:58 @agent_ppo2.py:185][0m |          -0.0148 |           1.8191 |           0.2880 |
[32m[20221213 14:57:58 @agent_ppo2.py:185][0m |          -0.0146 |           1.8019 |           0.2881 |
[32m[20221213 14:57:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:57:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.26
[32m[20221213 14:57:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.80
[32m[20221213 14:57:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.64
[32m[20221213 14:57:58 @agent_ppo2.py:143][0m Total time:       5.05 min
[32m[20221213 14:57:58 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221213 14:57:58 @agent_ppo2.py:121][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 14:57:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:57:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:57:58 @agent_ppo2.py:185][0m |          -0.0031 |           2.1745 |           0.2863 |
[32m[20221213 14:57:58 @agent_ppo2.py:185][0m |          -0.0091 |           2.0552 |           0.2865 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0071 |           2.0208 |           0.2865 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0084 |           2.0918 |           0.2867 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0123 |           1.9952 |           0.2867 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0148 |           1.9749 |           0.2870 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0134 |           1.9825 |           0.2871 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0184 |           1.9570 |           0.2873 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0183 |           1.9444 |           0.2876 |
[32m[20221213 14:57:59 @agent_ppo2.py:185][0m |          -0.0186 |           1.9499 |           0.2879 |
[32m[20221213 14:57:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:57:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.51
[32m[20221213 14:57:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.16
[32m[20221213 14:57:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.80
[32m[20221213 14:57:59 @agent_ppo2.py:143][0m Total time:       5.07 min
[32m[20221213 14:57:59 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221213 14:57:59 @agent_ppo2.py:121][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 14:58:00 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:58:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |           0.0094 |           2.2261 |           0.2957 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |          -0.0047 |           2.1136 |           0.2952 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |          -0.0042 |           2.1298 |           0.2947 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |          -0.0105 |           2.0968 |           0.2944 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |          -0.0030 |           2.2214 |           0.2944 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |          -0.0111 |           2.0751 |           0.2940 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |           0.0111 |           2.4838 |           0.2942 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |          -0.0111 |           2.0764 |           0.2942 |
[32m[20221213 14:58:00 @agent_ppo2.py:185][0m |          -0.0029 |           2.1689 |           0.2944 |
[32m[20221213 14:58:01 @agent_ppo2.py:185][0m |          -0.0141 |           2.0428 |           0.2944 |
[32m[20221213 14:58:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:58:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.23
[32m[20221213 14:58:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.92
[32m[20221213 14:58:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 69.06
[32m[20221213 14:58:01 @agent_ppo2.py:143][0m Total time:       5.09 min
[32m[20221213 14:58:01 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221213 14:58:01 @agent_ppo2.py:121][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 14:58:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:01 @agent_ppo2.py:185][0m |          -0.0012 |           2.1174 |           0.2946 |
[32m[20221213 14:58:01 @agent_ppo2.py:185][0m |          -0.0046 |           2.0748 |           0.2942 |
[32m[20221213 14:58:01 @agent_ppo2.py:185][0m |          -0.0103 |           2.0605 |           0.2937 |
[32m[20221213 14:58:01 @agent_ppo2.py:185][0m |          -0.0093 |           2.0486 |           0.2934 |
[32m[20221213 14:58:01 @agent_ppo2.py:185][0m |          -0.0079 |           2.0337 |           0.2931 |
[32m[20221213 14:58:02 @agent_ppo2.py:185][0m |          -0.0125 |           2.0266 |           0.2930 |
[32m[20221213 14:58:02 @agent_ppo2.py:185][0m |          -0.0088 |           2.0258 |           0.2927 |
[32m[20221213 14:58:02 @agent_ppo2.py:185][0m |          -0.0141 |           2.0071 |           0.2924 |
[32m[20221213 14:58:02 @agent_ppo2.py:185][0m |          -0.0155 |           2.0036 |           0.2924 |
[32m[20221213 14:58:02 @agent_ppo2.py:185][0m |          -0.0149 |           1.9929 |           0.2921 |
[32m[20221213 14:58:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:58:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.91
[32m[20221213 14:58:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.60
[32m[20221213 14:58:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.81
[32m[20221213 14:58:02 @agent_ppo2.py:143][0m Total time:       5.11 min
[32m[20221213 14:58:02 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221213 14:58:02 @agent_ppo2.py:121][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 14:58:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:02 @agent_ppo2.py:185][0m |           0.0027 |           2.1061 |           0.2929 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0100 |           2.0374 |           0.2926 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0042 |           2.0759 |           0.2926 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0124 |           2.0055 |           0.2927 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0119 |           1.9947 |           0.2926 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0137 |           1.9875 |           0.2925 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0139 |           1.9865 |           0.2925 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0163 |           1.9758 |           0.2924 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0160 |           1.9742 |           0.2923 |
[32m[20221213 14:58:03 @agent_ppo2.py:185][0m |          -0.0142 |           1.9672 |           0.2922 |
[32m[20221213 14:58:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:58:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.81
[32m[20221213 14:58:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 83.36
[32m[20221213 14:58:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.52
[32m[20221213 14:58:03 @agent_ppo2.py:143][0m Total time:       5.14 min
[32m[20221213 14:58:03 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221213 14:58:03 @agent_ppo2.py:121][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 14:58:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0038 |           1.9685 |           0.2927 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0069 |           1.9109 |           0.2923 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0103 |           1.8828 |           0.2919 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0091 |           1.8760 |           0.2914 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0132 |           1.8669 |           0.2914 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0119 |           1.8543 |           0.2912 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0038 |           2.0897 |           0.2912 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0120 |           1.8484 |           0.2908 |
[32m[20221213 14:58:04 @agent_ppo2.py:185][0m |          -0.0116 |           1.8544 |           0.2908 |
[32m[20221213 14:58:05 @agent_ppo2.py:185][0m |          -0.0144 |           1.8293 |           0.2908 |
[32m[20221213 14:58:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.69
[32m[20221213 14:58:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.66
[32m[20221213 14:58:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.70
[32m[20221213 14:58:05 @agent_ppo2.py:143][0m Total time:       5.16 min
[32m[20221213 14:58:05 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221213 14:58:05 @agent_ppo2.py:121][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 14:58:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:05 @agent_ppo2.py:185][0m |          -0.0015 |           1.9498 |           0.2903 |
[32m[20221213 14:58:05 @agent_ppo2.py:185][0m |          -0.0050 |           1.8786 |           0.2897 |
[32m[20221213 14:58:05 @agent_ppo2.py:185][0m |          -0.0093 |           1.8351 |           0.2890 |
[32m[20221213 14:58:05 @agent_ppo2.py:185][0m |          -0.0111 |           1.8178 |           0.2884 |
[32m[20221213 14:58:05 @agent_ppo2.py:185][0m |          -0.0078 |           1.7941 |           0.2883 |
[32m[20221213 14:58:06 @agent_ppo2.py:185][0m |          -0.0003 |           1.9411 |           0.2879 |
[32m[20221213 14:58:06 @agent_ppo2.py:185][0m |          -0.0068 |           1.7727 |           0.2876 |
[32m[20221213 14:58:06 @agent_ppo2.py:185][0m |          -0.0147 |           1.7255 |           0.2874 |
[32m[20221213 14:58:06 @agent_ppo2.py:185][0m |          -0.0090 |           1.7333 |           0.2871 |
[32m[20221213 14:58:06 @agent_ppo2.py:185][0m |          -0.0148 |           1.6964 |           0.2871 |
[32m[20221213 14:58:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:58:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.85
[32m[20221213 14:58:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.98
[32m[20221213 14:58:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.27
[32m[20221213 14:58:06 @agent_ppo2.py:143][0m Total time:       5.18 min
[32m[20221213 14:58:06 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221213 14:58:06 @agent_ppo2.py:121][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 14:58:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:06 @agent_ppo2.py:185][0m |          -0.0002 |           2.1939 |           0.2912 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0091 |           2.1281 |           0.2908 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0101 |           2.1112 |           0.2904 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0124 |           2.0934 |           0.2904 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0054 |           2.1232 |           0.2903 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0137 |           2.0516 |           0.2902 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0149 |           2.0526 |           0.2901 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0142 |           2.0258 |           0.2900 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0143 |           2.0173 |           0.2898 |
[32m[20221213 14:58:07 @agent_ppo2.py:185][0m |          -0.0113 |           2.0011 |           0.2899 |
[32m[20221213 14:58:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.82
[32m[20221213 14:58:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.05
[32m[20221213 14:58:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 60.94
[32m[20221213 14:58:07 @agent_ppo2.py:143][0m Total time:       5.20 min
[32m[20221213 14:58:07 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221213 14:58:07 @agent_ppo2.py:121][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 14:58:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0029 |           2.0659 |           0.2988 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0090 |           2.0066 |           0.2984 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0064 |           1.9912 |           0.2976 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0118 |           1.9630 |           0.2975 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0134 |           1.9637 |           0.2974 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0055 |           2.0704 |           0.2973 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0165 |           1.9403 |           0.2971 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0141 |           1.9346 |           0.2971 |
[32m[20221213 14:58:08 @agent_ppo2.py:185][0m |          -0.0057 |           2.0805 |           0.2971 |
[32m[20221213 14:58:09 @agent_ppo2.py:185][0m |          -0.0134 |           1.9323 |           0.2972 |
[32m[20221213 14:58:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.43
[32m[20221213 14:58:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.69
[32m[20221213 14:58:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.04
[32m[20221213 14:58:09 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221213 14:58:09 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221213 14:58:09 @agent_ppo2.py:121][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 14:58:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:09 @agent_ppo2.py:185][0m |          -0.0017 |           1.9252 |           0.2817 |
[32m[20221213 14:58:09 @agent_ppo2.py:185][0m |          -0.0052 |           1.8670 |           0.2814 |
[32m[20221213 14:58:09 @agent_ppo2.py:185][0m |           0.0016 |           2.0045 |           0.2814 |
[32m[20221213 14:58:09 @agent_ppo2.py:185][0m |          -0.0114 |           1.8376 |           0.2811 |
[32m[20221213 14:58:09 @agent_ppo2.py:185][0m |          -0.0120 |           1.8346 |           0.2809 |
[32m[20221213 14:58:10 @agent_ppo2.py:185][0m |          -0.0117 |           1.8029 |           0.2810 |
[32m[20221213 14:58:10 @agent_ppo2.py:185][0m |          -0.0124 |           1.8092 |           0.2809 |
[32m[20221213 14:58:10 @agent_ppo2.py:185][0m |          -0.0127 |           1.7956 |           0.2810 |
[32m[20221213 14:58:10 @agent_ppo2.py:185][0m |          -0.0151 |           1.8017 |           0.2810 |
[32m[20221213 14:58:10 @agent_ppo2.py:185][0m |          -0.0149 |           1.7882 |           0.2810 |
[32m[20221213 14:58:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.76
[32m[20221213 14:58:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.50
[32m[20221213 14:58:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 59.84
[32m[20221213 14:58:10 @agent_ppo2.py:143][0m Total time:       5.25 min
[32m[20221213 14:58:10 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221213 14:58:10 @agent_ppo2.py:121][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 14:58:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:10 @agent_ppo2.py:185][0m |           0.0000 |           2.0548 |           0.2903 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0031 |           2.0041 |           0.2904 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0062 |           1.9687 |           0.2900 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0086 |           1.9490 |           0.2898 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0093 |           1.9295 |           0.2898 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0078 |           1.9230 |           0.2897 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0087 |           1.9135 |           0.2896 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0125 |           1.8993 |           0.2893 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0114 |           1.8844 |           0.2896 |
[32m[20221213 14:58:11 @agent_ppo2.py:185][0m |          -0.0130 |           1.8808 |           0.2894 |
[32m[20221213 14:58:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.24
[32m[20221213 14:58:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.57
[32m[20221213 14:58:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.89
[32m[20221213 14:58:11 @agent_ppo2.py:143][0m Total time:       5.27 min
[32m[20221213 14:58:11 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221213 14:58:11 @agent_ppo2.py:121][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 14:58:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0035 |           2.1843 |           0.2875 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0018 |           2.0904 |           0.2876 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0077 |           2.0668 |           0.2874 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0084 |           2.0281 |           0.2874 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0094 |           2.0169 |           0.2875 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0073 |           2.0071 |           0.2876 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0079 |           2.0656 |           0.2873 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0107 |           1.9831 |           0.2875 |
[32m[20221213 14:58:12 @agent_ppo2.py:185][0m |          -0.0106 |           1.9810 |           0.2875 |
[32m[20221213 14:58:13 @agent_ppo2.py:185][0m |          -0.0082 |           1.9733 |           0.2876 |
[32m[20221213 14:58:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.21
[32m[20221213 14:58:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.72
[32m[20221213 14:58:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.38
[32m[20221213 14:58:13 @agent_ppo2.py:143][0m Total time:       5.29 min
[32m[20221213 14:58:13 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221213 14:58:13 @agent_ppo2.py:121][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 14:58:13 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:58:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:13 @agent_ppo2.py:185][0m |          -0.0000 |           2.0118 |           0.2912 |
[32m[20221213 14:58:13 @agent_ppo2.py:185][0m |          -0.0040 |           1.9664 |           0.2905 |
[32m[20221213 14:58:13 @agent_ppo2.py:185][0m |          -0.0074 |           1.9601 |           0.2901 |
[32m[20221213 14:58:13 @agent_ppo2.py:185][0m |           0.0011 |           2.0747 |           0.2900 |
[32m[20221213 14:58:13 @agent_ppo2.py:185][0m |          -0.0144 |           1.9400 |           0.2897 |
[32m[20221213 14:58:14 @agent_ppo2.py:185][0m |          -0.0123 |           1.9226 |           0.2896 |
[32m[20221213 14:58:14 @agent_ppo2.py:185][0m |          -0.0116 |           1.9129 |           0.2897 |
[32m[20221213 14:58:14 @agent_ppo2.py:185][0m |          -0.0129 |           1.9245 |           0.2896 |
[32m[20221213 14:58:14 @agent_ppo2.py:185][0m |          -0.0142 |           1.9143 |           0.2897 |
[32m[20221213 14:58:14 @agent_ppo2.py:185][0m |          -0.0118 |           1.9094 |           0.2897 |
[32m[20221213 14:58:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 79.77
[32m[20221213 14:58:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.51
[32m[20221213 14:58:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 64.06
[32m[20221213 14:58:14 @agent_ppo2.py:143][0m Total time:       5.31 min
[32m[20221213 14:58:14 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221213 14:58:14 @agent_ppo2.py:121][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 14:58:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:14 @agent_ppo2.py:185][0m |           0.0014 |           1.9115 |           0.2843 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0056 |           1.8813 |           0.2842 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0080 |           1.8562 |           0.2840 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0065 |           1.8513 |           0.2841 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0093 |           1.8361 |           0.2838 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0117 |           1.8278 |           0.2841 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0132 |           1.8198 |           0.2841 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0114 |           1.8200 |           0.2841 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0056 |           1.8726 |           0.2841 |
[32m[20221213 14:58:15 @agent_ppo2.py:185][0m |          -0.0126 |           1.8061 |           0.2844 |
[32m[20221213 14:58:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.44
[32m[20221213 14:58:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.53
[32m[20221213 14:58:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.21
[32m[20221213 14:58:15 @agent_ppo2.py:143][0m Total time:       5.34 min
[32m[20221213 14:58:15 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221213 14:58:15 @agent_ppo2.py:121][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 14:58:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0023 |           2.0528 |           0.2926 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0066 |           2.0332 |           0.2921 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0092 |           2.0290 |           0.2919 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0061 |           2.0264 |           0.2916 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0132 |           2.0071 |           0.2914 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0169 |           2.0103 |           0.2913 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0160 |           2.0044 |           0.2909 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0153 |           2.0030 |           0.2908 |
[32m[20221213 14:58:16 @agent_ppo2.py:185][0m |          -0.0132 |           2.0050 |           0.2906 |
[32m[20221213 14:58:17 @agent_ppo2.py:185][0m |          -0.0156 |           1.9986 |           0.2904 |
[32m[20221213 14:58:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.13
[32m[20221213 14:58:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.02
[32m[20221213 14:58:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.41
[32m[20221213 14:58:17 @agent_ppo2.py:143][0m Total time:       5.36 min
[32m[20221213 14:58:17 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221213 14:58:17 @agent_ppo2.py:121][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 14:58:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:17 @agent_ppo2.py:185][0m |          -0.0033 |           2.1129 |           0.2838 |
[32m[20221213 14:58:17 @agent_ppo2.py:185][0m |          -0.0035 |           2.0660 |           0.2833 |
[32m[20221213 14:58:17 @agent_ppo2.py:185][0m |          -0.0098 |           2.0397 |           0.2830 |
[32m[20221213 14:58:17 @agent_ppo2.py:185][0m |          -0.0119 |           2.0346 |           0.2827 |
[32m[20221213 14:58:17 @agent_ppo2.py:185][0m |          -0.0125 |           2.0217 |           0.2826 |
[32m[20221213 14:58:18 @agent_ppo2.py:185][0m |          -0.0157 |           2.0137 |           0.2825 |
[32m[20221213 14:58:18 @agent_ppo2.py:185][0m |          -0.0120 |           2.0115 |           0.2822 |
[32m[20221213 14:58:18 @agent_ppo2.py:185][0m |          -0.0130 |           2.0100 |           0.2821 |
[32m[20221213 14:58:18 @agent_ppo2.py:185][0m |          -0.0160 |           1.9950 |           0.2820 |
[32m[20221213 14:58:18 @agent_ppo2.py:185][0m |          -0.0124 |           1.9956 |           0.2818 |
[32m[20221213 14:58:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.68
[32m[20221213 14:58:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.21
[32m[20221213 14:58:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.45
[32m[20221213 14:58:18 @agent_ppo2.py:143][0m Total time:       5.38 min
[32m[20221213 14:58:18 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221213 14:58:18 @agent_ppo2.py:121][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 14:58:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:18 @agent_ppo2.py:185][0m |           0.0002 |           2.1499 |           0.2868 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0103 |           2.0625 |           0.2860 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0094 |           2.0533 |           0.2857 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0162 |           2.0178 |           0.2853 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0160 |           2.0076 |           0.2852 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0117 |           1.9997 |           0.2850 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0114 |           2.0674 |           0.2847 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0223 |           1.9792 |           0.2846 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0191 |           1.9634 |           0.2844 |
[32m[20221213 14:58:19 @agent_ppo2.py:185][0m |          -0.0166 |           1.9637 |           0.2843 |
[32m[20221213 14:58:19 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:58:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.26
[32m[20221213 14:58:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 88.03
[32m[20221213 14:58:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.94
[32m[20221213 14:58:19 @agent_ppo2.py:143][0m Total time:       5.40 min
[32m[20221213 14:58:19 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221213 14:58:19 @agent_ppo2.py:121][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 14:58:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |           0.0012 |           2.1920 |           0.2890 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0068 |           2.1552 |           0.2887 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0006 |           2.2389 |           0.2886 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0060 |           2.1475 |           0.2887 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0077 |           2.1181 |           0.2891 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0131 |           2.1115 |           0.2891 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0115 |           2.0995 |           0.2891 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0113 |           2.0937 |           0.2893 |
[32m[20221213 14:58:20 @agent_ppo2.py:185][0m |          -0.0119 |           2.0864 |           0.2893 |
[32m[20221213 14:58:21 @agent_ppo2.py:185][0m |          -0.0143 |           2.0790 |           0.2894 |
[32m[20221213 14:58:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.03
[32m[20221213 14:58:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.32
[32m[20221213 14:58:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.74
[32m[20221213 14:58:21 @agent_ppo2.py:143][0m Total time:       5.43 min
[32m[20221213 14:58:21 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221213 14:58:21 @agent_ppo2.py:121][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 14:58:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:21 @agent_ppo2.py:185][0m |          -0.0010 |           2.0653 |           0.2997 |
[32m[20221213 14:58:21 @agent_ppo2.py:185][0m |           0.0015 |           2.0866 |           0.2995 |
[32m[20221213 14:58:21 @agent_ppo2.py:185][0m |           0.0011 |           2.2082 |           0.2990 |
[32m[20221213 14:58:21 @agent_ppo2.py:185][0m |          -0.0111 |           1.9588 |           0.2986 |
[32m[20221213 14:58:21 @agent_ppo2.py:185][0m |          -0.0138 |           1.9209 |           0.2982 |
[32m[20221213 14:58:22 @agent_ppo2.py:185][0m |          -0.0154 |           1.9011 |           0.2981 |
[32m[20221213 14:58:22 @agent_ppo2.py:185][0m |          -0.0174 |           1.8886 |           0.2978 |
[32m[20221213 14:58:22 @agent_ppo2.py:185][0m |          -0.0175 |           1.8765 |           0.2978 |
[32m[20221213 14:58:22 @agent_ppo2.py:185][0m |          -0.0184 |           1.8593 |           0.2977 |
[32m[20221213 14:58:22 @agent_ppo2.py:185][0m |          -0.0153 |           1.8501 |           0.2976 |
[32m[20221213 14:58:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.28
[32m[20221213 14:58:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.20
[32m[20221213 14:58:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.74
[32m[20221213 14:58:22 @agent_ppo2.py:143][0m Total time:       5.45 min
[32m[20221213 14:58:22 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221213 14:58:22 @agent_ppo2.py:121][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 14:58:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:22 @agent_ppo2.py:185][0m |          -0.0026 |           2.2306 |           0.2910 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0090 |           2.1588 |           0.2906 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |           0.0037 |           2.2850 |           0.2903 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0118 |           2.1048 |           0.2897 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0114 |           2.0767 |           0.2896 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0161 |           2.0670 |           0.2895 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0133 |           2.0859 |           0.2893 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0152 |           2.0474 |           0.2892 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0157 |           2.0309 |           0.2891 |
[32m[20221213 14:58:23 @agent_ppo2.py:185][0m |          -0.0183 |           2.0182 |           0.2889 |
[32m[20221213 14:58:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.43
[32m[20221213 14:58:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.20
[32m[20221213 14:58:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.13
[32m[20221213 14:58:23 @agent_ppo2.py:143][0m Total time:       5.47 min
[32m[20221213 14:58:23 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221213 14:58:23 @agent_ppo2.py:121][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 14:58:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0040 |           2.3093 |           0.2865 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0026 |           2.2568 |           0.2865 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0094 |           2.2228 |           0.2864 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0141 |           2.2107 |           0.2864 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0039 |           2.3679 |           0.2865 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0140 |           2.1974 |           0.2863 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0090 |           2.2127 |           0.2867 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0115 |           2.1610 |           0.2870 |
[32m[20221213 14:58:24 @agent_ppo2.py:185][0m |          -0.0156 |           2.1506 |           0.2870 |
[32m[20221213 14:58:25 @agent_ppo2.py:185][0m |          -0.0194 |           2.1438 |           0.2874 |
[32m[20221213 14:58:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:58:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.19
[32m[20221213 14:58:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.76
[32m[20221213 14:58:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.48
[32m[20221213 14:58:25 @agent_ppo2.py:143][0m Total time:       5.49 min
[32m[20221213 14:58:25 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221213 14:58:25 @agent_ppo2.py:121][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 14:58:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:25 @agent_ppo2.py:185][0m |           0.0007 |           2.2861 |           0.2909 |
[32m[20221213 14:58:25 @agent_ppo2.py:185][0m |          -0.0008 |           2.2363 |           0.2908 |
[32m[20221213 14:58:25 @agent_ppo2.py:185][0m |          -0.0064 |           2.1976 |           0.2906 |
[32m[20221213 14:58:25 @agent_ppo2.py:185][0m |          -0.0088 |           2.1707 |           0.2904 |
[32m[20221213 14:58:25 @agent_ppo2.py:185][0m |          -0.0062 |           2.1946 |           0.2903 |
[32m[20221213 14:58:26 @agent_ppo2.py:185][0m |          -0.0128 |           2.1439 |           0.2902 |
[32m[20221213 14:58:26 @agent_ppo2.py:185][0m |          -0.0106 |           2.1407 |           0.2900 |
[32m[20221213 14:58:26 @agent_ppo2.py:185][0m |          -0.0132 |           2.1178 |           0.2900 |
[32m[20221213 14:58:26 @agent_ppo2.py:185][0m |          -0.0166 |           2.1111 |           0.2899 |
[32m[20221213 14:58:26 @agent_ppo2.py:185][0m |          -0.0146 |           2.1131 |           0.2899 |
[32m[20221213 14:58:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.13
[32m[20221213 14:58:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.62
[32m[20221213 14:58:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.54
[32m[20221213 14:58:26 @agent_ppo2.py:143][0m Total time:       5.51 min
[32m[20221213 14:58:26 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221213 14:58:26 @agent_ppo2.py:121][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 14:58:26 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:58:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:26 @agent_ppo2.py:185][0m |          -0.0026 |           2.4518 |           0.2884 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |           0.0015 |           2.4446 |           0.2881 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0084 |           2.3457 |           0.2878 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0066 |           2.3194 |           0.2876 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0118 |           2.3019 |           0.2874 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0038 |           2.3117 |           0.2872 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0106 |           2.2720 |           0.2870 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0125 |           2.2680 |           0.2868 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0145 |           2.2506 |           0.2867 |
[32m[20221213 14:58:27 @agent_ppo2.py:185][0m |          -0.0141 |           2.2343 |           0.2867 |
[32m[20221213 14:58:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.10
[32m[20221213 14:58:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.98
[32m[20221213 14:58:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.93
[32m[20221213 14:58:27 @agent_ppo2.py:143][0m Total time:       5.54 min
[32m[20221213 14:58:27 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221213 14:58:27 @agent_ppo2.py:121][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 14:58:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |          -0.0015 |           2.4583 |           0.2908 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |          -0.0087 |           2.4255 |           0.2900 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |          -0.0160 |           2.4089 |           0.2897 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |          -0.0090 |           2.4483 |           0.2895 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |           0.0078 |           2.7564 |           0.2893 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |           0.0034 |           2.6985 |           0.2891 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |          -0.0189 |           2.3899 |           0.2889 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |          -0.0153 |           2.3661 |           0.2889 |
[32m[20221213 14:58:28 @agent_ppo2.py:185][0m |          -0.0105 |           2.3959 |           0.2888 |
[32m[20221213 14:58:29 @agent_ppo2.py:185][0m |          -0.0131 |           2.3573 |           0.2888 |
[32m[20221213 14:58:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.82
[32m[20221213 14:58:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 107.93
[32m[20221213 14:58:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.71
[32m[20221213 14:58:29 @agent_ppo2.py:143][0m Total time:       5.56 min
[32m[20221213 14:58:29 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221213 14:58:29 @agent_ppo2.py:121][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 14:58:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:29 @agent_ppo2.py:185][0m |           0.0006 |           2.2694 |           0.2865 |
[32m[20221213 14:58:29 @agent_ppo2.py:185][0m |          -0.0093 |           2.2504 |           0.2857 |
[32m[20221213 14:58:29 @agent_ppo2.py:185][0m |          -0.0095 |           2.2305 |           0.2853 |
[32m[20221213 14:58:29 @agent_ppo2.py:185][0m |          -0.0120 |           2.2280 |           0.2852 |
[32m[20221213 14:58:29 @agent_ppo2.py:185][0m |          -0.0038 |           2.4072 |           0.2851 |
[32m[20221213 14:58:30 @agent_ppo2.py:185][0m |          -0.0111 |           2.2190 |           0.2845 |
[32m[20221213 14:58:30 @agent_ppo2.py:185][0m |          -0.0119 |           2.2019 |           0.2846 |
[32m[20221213 14:58:30 @agent_ppo2.py:185][0m |          -0.0108 |           2.1971 |           0.2845 |
[32m[20221213 14:58:30 @agent_ppo2.py:185][0m |          -0.0155 |           2.2004 |           0.2847 |
[32m[20221213 14:58:30 @agent_ppo2.py:185][0m |          -0.0105 |           2.2016 |           0.2844 |
[32m[20221213 14:58:30 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:58:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.67
[32m[20221213 14:58:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.57
[32m[20221213 14:58:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.03
[32m[20221213 14:58:30 @agent_ppo2.py:143][0m Total time:       5.58 min
[32m[20221213 14:58:30 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221213 14:58:30 @agent_ppo2.py:121][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 14:58:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:30 @agent_ppo2.py:185][0m |           0.0004 |           2.2303 |           0.2917 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0079 |           2.1770 |           0.2916 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0102 |           2.1610 |           0.2912 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0110 |           2.1470 |           0.2910 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0109 |           2.1201 |           0.2910 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0109 |           2.1183 |           0.2905 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0065 |           2.2075 |           0.2902 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0162 |           2.1011 |           0.2899 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0183 |           2.0927 |           0.2897 |
[32m[20221213 14:58:31 @agent_ppo2.py:185][0m |          -0.0168 |           2.0953 |           0.2896 |
[32m[20221213 14:58:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.62
[32m[20221213 14:58:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.43
[32m[20221213 14:58:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.57
[32m[20221213 14:58:31 @agent_ppo2.py:143][0m Total time:       5.60 min
[32m[20221213 14:58:31 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221213 14:58:31 @agent_ppo2.py:121][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 14:58:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0016 |           2.0473 |           0.2927 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0060 |           1.9855 |           0.2922 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0092 |           1.9688 |           0.2921 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |           0.0005 |           2.0896 |           0.2916 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0092 |           1.9622 |           0.2912 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0112 |           1.9242 |           0.2909 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0137 |           1.9257 |           0.2908 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0158 |           1.9190 |           0.2903 |
[32m[20221213 14:58:32 @agent_ppo2.py:185][0m |          -0.0137 |           1.9056 |           0.2904 |
[32m[20221213 14:58:33 @agent_ppo2.py:185][0m |          -0.0128 |           1.8977 |           0.2900 |
[32m[20221213 14:58:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.98
[32m[20221213 14:58:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.59
[32m[20221213 14:58:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.58
[32m[20221213 14:58:33 @agent_ppo2.py:143][0m Total time:       5.63 min
[32m[20221213 14:58:33 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221213 14:58:33 @agent_ppo2.py:121][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 14:58:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:33 @agent_ppo2.py:185][0m |          -0.0009 |           2.2527 |           0.2774 |
[32m[20221213 14:58:33 @agent_ppo2.py:185][0m |          -0.0041 |           2.2693 |           0.2774 |
[32m[20221213 14:58:33 @agent_ppo2.py:185][0m |          -0.0042 |           2.2301 |           0.2773 |
[32m[20221213 14:58:33 @agent_ppo2.py:185][0m |          -0.0108 |           2.1841 |           0.2772 |
[32m[20221213 14:58:33 @agent_ppo2.py:185][0m |          -0.0148 |           2.1497 |           0.2773 |
[32m[20221213 14:58:34 @agent_ppo2.py:185][0m |          -0.0151 |           2.1376 |           0.2773 |
[32m[20221213 14:58:34 @agent_ppo2.py:185][0m |          -0.0171 |           2.1219 |           0.2772 |
[32m[20221213 14:58:34 @agent_ppo2.py:185][0m |          -0.0168 |           2.1210 |           0.2773 |
[32m[20221213 14:58:34 @agent_ppo2.py:185][0m |          -0.0179 |           2.1007 |           0.2773 |
[32m[20221213 14:58:34 @agent_ppo2.py:185][0m |          -0.0177 |           2.0976 |           0.2772 |
[32m[20221213 14:58:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 80.87
[32m[20221213 14:58:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.84
[32m[20221213 14:58:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.85
[32m[20221213 14:58:34 @agent_ppo2.py:143][0m Total time:       5.65 min
[32m[20221213 14:58:34 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221213 14:58:34 @agent_ppo2.py:121][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 14:58:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:34 @agent_ppo2.py:185][0m |          -0.0003 |           2.1523 |           0.2796 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0055 |           2.0831 |           0.2795 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0120 |           2.0601 |           0.2793 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0130 |           2.0299 |           0.2791 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0055 |           2.0643 |           0.2789 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0167 |           2.0011 |           0.2786 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0167 |           1.9826 |           0.2787 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0173 |           1.9718 |           0.2788 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0164 |           1.9510 |           0.2786 |
[32m[20221213 14:58:35 @agent_ppo2.py:185][0m |          -0.0167 |           1.9435 |           0.2786 |
[32m[20221213 14:58:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.73
[32m[20221213 14:58:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 84.40
[32m[20221213 14:58:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.64
[32m[20221213 14:58:35 @agent_ppo2.py:143][0m Total time:       5.67 min
[32m[20221213 14:58:35 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221213 14:58:35 @agent_ppo2.py:121][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 14:58:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |           0.0027 |           2.5029 |           0.2854 |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |          -0.0064 |           2.3630 |           0.2850 |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |          -0.0132 |           2.3080 |           0.2848 |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |          -0.0142 |           2.2741 |           0.2847 |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |          -0.0095 |           2.2405 |           0.2846 |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |          -0.0125 |           2.2127 |           0.2846 |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |          -0.0131 |           2.1906 |           0.2846 |
[32m[20221213 14:58:36 @agent_ppo2.py:185][0m |          -0.0134 |           2.1785 |           0.2847 |
[32m[20221213 14:58:37 @agent_ppo2.py:185][0m |          -0.0109 |           2.2036 |           0.2846 |
[32m[20221213 14:58:37 @agent_ppo2.py:185][0m |          -0.0150 |           2.1493 |           0.2845 |
[32m[20221213 14:58:37 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:58:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.24
[32m[20221213 14:58:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 87.70
[32m[20221213 14:58:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.72
[32m[20221213 14:58:37 @agent_ppo2.py:143][0m Total time:       5.69 min
[32m[20221213 14:58:37 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221213 14:58:37 @agent_ppo2.py:121][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 14:58:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:37 @agent_ppo2.py:185][0m |           0.0029 |           2.4537 |           0.2843 |
[32m[20221213 14:58:37 @agent_ppo2.py:185][0m |          -0.0082 |           2.3871 |           0.2838 |
[32m[20221213 14:58:37 @agent_ppo2.py:185][0m |          -0.0063 |           2.3625 |           0.2835 |
[32m[20221213 14:58:37 @agent_ppo2.py:185][0m |          -0.0052 |           2.4040 |           0.2832 |
[32m[20221213 14:58:37 @agent_ppo2.py:185][0m |          -0.0125 |           2.3276 |           0.2833 |
[32m[20221213 14:58:38 @agent_ppo2.py:185][0m |          -0.0100 |           2.3389 |           0.2829 |
[32m[20221213 14:58:38 @agent_ppo2.py:185][0m |          -0.0150 |           2.3073 |           0.2829 |
[32m[20221213 14:58:38 @agent_ppo2.py:185][0m |          -0.0160 |           2.3047 |           0.2828 |
[32m[20221213 14:58:38 @agent_ppo2.py:185][0m |          -0.0087 |           2.3408 |           0.2828 |
[32m[20221213 14:58:38 @agent_ppo2.py:185][0m |          -0.0186 |           2.2872 |           0.2826 |
[32m[20221213 14:58:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.14
[32m[20221213 14:58:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.37
[32m[20221213 14:58:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 75.74
[32m[20221213 14:58:38 @agent_ppo2.py:143][0m Total time:       5.72 min
[32m[20221213 14:58:38 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221213 14:58:38 @agent_ppo2.py:121][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 14:58:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:38 @agent_ppo2.py:185][0m |          -0.0012 |           2.1770 |           0.2749 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |           0.0076 |           2.3622 |           0.2745 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0102 |           2.1017 |           0.2740 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0078 |           2.0960 |           0.2738 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0140 |           2.0619 |           0.2736 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0149 |           2.0491 |           0.2735 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0135 |           2.0542 |           0.2735 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0159 |           2.0328 |           0.2735 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0073 |           2.1007 |           0.2732 |
[32m[20221213 14:58:39 @agent_ppo2.py:185][0m |          -0.0079 |           2.0668 |           0.2732 |
[32m[20221213 14:58:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.75
[32m[20221213 14:58:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.01
[32m[20221213 14:58:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.89
[32m[20221213 14:58:39 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221213 14:58:39 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221213 14:58:39 @agent_ppo2.py:121][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 14:58:40 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:58:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0027 |           2.0160 |           0.2789 |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0079 |           1.9082 |           0.2790 |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0061 |           1.8868 |           0.2792 |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0092 |           1.8708 |           0.2794 |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0117 |           1.8589 |           0.2795 |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0115 |           1.8539 |           0.2794 |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0150 |           1.8515 |           0.2795 |
[32m[20221213 14:58:40 @agent_ppo2.py:185][0m |          -0.0146 |           1.8427 |           0.2797 |
[32m[20221213 14:58:41 @agent_ppo2.py:185][0m |          -0.0114 |           1.8270 |           0.2798 |
[32m[20221213 14:58:41 @agent_ppo2.py:185][0m |          -0.0164 |           1.8203 |           0.2797 |
[32m[20221213 14:58:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.32
[32m[20221213 14:58:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.32
[32m[20221213 14:58:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.03
[32m[20221213 14:58:41 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221213 14:58:41 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221213 14:58:41 @agent_ppo2.py:121][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 14:58:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:41 @agent_ppo2.py:185][0m |          -0.0054 |           2.3015 |           0.2799 |
[32m[20221213 14:58:41 @agent_ppo2.py:185][0m |          -0.0070 |           2.2137 |           0.2799 |
[32m[20221213 14:58:41 @agent_ppo2.py:185][0m |          -0.0099 |           2.1803 |           0.2801 |
[32m[20221213 14:58:41 @agent_ppo2.py:185][0m |          -0.0105 |           2.1700 |           0.2800 |
[32m[20221213 14:58:42 @agent_ppo2.py:185][0m |           0.0011 |           2.2243 |           0.2801 |
[32m[20221213 14:58:42 @agent_ppo2.py:185][0m |          -0.0015 |           2.3231 |           0.2801 |
[32m[20221213 14:58:42 @agent_ppo2.py:185][0m |          -0.0116 |           2.1270 |           0.2800 |
[32m[20221213 14:58:42 @agent_ppo2.py:185][0m |          -0.0142 |           2.1178 |           0.2801 |
[32m[20221213 14:58:42 @agent_ppo2.py:185][0m |          -0.0145 |           2.0976 |           0.2803 |
[32m[20221213 14:58:42 @agent_ppo2.py:185][0m |          -0.0192 |           2.0954 |           0.2802 |
[32m[20221213 14:58:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 78.20
[32m[20221213 14:58:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 82.07
[32m[20221213 14:58:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 4.52
[32m[20221213 14:58:42 @agent_ppo2.py:143][0m Total time:       5.78 min
[32m[20221213 14:58:42 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221213 14:58:42 @agent_ppo2.py:121][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 14:58:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:42 @agent_ppo2.py:185][0m |           0.0025 |           2.3582 |           0.2916 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |           0.0024 |           2.3116 |           0.2910 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0094 |           2.2619 |           0.2906 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0126 |           2.2365 |           0.2903 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0112 |           2.2190 |           0.2901 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0115 |           2.2171 |           0.2901 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0125 |           2.1991 |           0.2900 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0154 |           2.1848 |           0.2898 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0154 |           2.1760 |           0.2897 |
[32m[20221213 14:58:43 @agent_ppo2.py:185][0m |          -0.0182 |           2.1675 |           0.2896 |
[32m[20221213 14:58:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.07
[32m[20221213 14:58:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.87
[32m[20221213 14:58:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 72.22
[32m[20221213 14:58:43 @agent_ppo2.py:143][0m Total time:       5.80 min
[32m[20221213 14:58:43 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221213 14:58:43 @agent_ppo2.py:121][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 14:58:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0002 |           2.3630 |           0.2909 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0065 |           2.3262 |           0.2908 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |           0.0026 |           2.4744 |           0.2905 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0108 |           2.2878 |           0.2903 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0129 |           2.2588 |           0.2901 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0153 |           2.2567 |           0.2899 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0134 |           2.2378 |           0.2898 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0160 |           2.2264 |           0.2895 |
[32m[20221213 14:58:44 @agent_ppo2.py:185][0m |          -0.0122 |           2.2136 |           0.2893 |
[32m[20221213 14:58:45 @agent_ppo2.py:185][0m |          -0.0145 |           2.2017 |           0.2891 |
[32m[20221213 14:58:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.41
[32m[20221213 14:58:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 91.25
[32m[20221213 14:58:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.27
[32m[20221213 14:58:45 @agent_ppo2.py:143][0m Total time:       5.83 min
[32m[20221213 14:58:45 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221213 14:58:45 @agent_ppo2.py:121][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 14:58:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:45 @agent_ppo2.py:185][0m |          -0.0012 |           2.2976 |           0.2841 |
[32m[20221213 14:58:45 @agent_ppo2.py:185][0m |          -0.0068 |           2.2498 |           0.2839 |
[32m[20221213 14:58:45 @agent_ppo2.py:185][0m |          -0.0005 |           2.2794 |           0.2840 |
[32m[20221213 14:58:45 @agent_ppo2.py:185][0m |          -0.0099 |           2.1796 |           0.2839 |
[32m[20221213 14:58:46 @agent_ppo2.py:185][0m |          -0.0127 |           2.1537 |           0.2836 |
[32m[20221213 14:58:46 @agent_ppo2.py:185][0m |          -0.0065 |           2.1605 |           0.2834 |
[32m[20221213 14:58:46 @agent_ppo2.py:185][0m |          -0.0129 |           2.1179 |           0.2836 |
[32m[20221213 14:58:46 @agent_ppo2.py:185][0m |          -0.0142 |           2.1006 |           0.2833 |
[32m[20221213 14:58:46 @agent_ppo2.py:185][0m |          -0.0124 |           2.0994 |           0.2832 |
[32m[20221213 14:58:46 @agent_ppo2.py:185][0m |          -0.0155 |           2.0788 |           0.2829 |
[32m[20221213 14:58:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.03
[32m[20221213 14:58:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.48
[32m[20221213 14:58:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.90
[32m[20221213 14:58:46 @agent_ppo2.py:143][0m Total time:       5.85 min
[32m[20221213 14:58:46 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221213 14:58:46 @agent_ppo2.py:121][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 14:58:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0030 |           2.1749 |           0.2821 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0057 |           2.1323 |           0.2816 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0113 |           2.1018 |           0.2812 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0016 |           2.2670 |           0.2809 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0135 |           2.0783 |           0.2806 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0114 |           2.0783 |           0.2805 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0160 |           2.0602 |           0.2806 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0145 |           2.0442 |           0.2804 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0161 |           2.0395 |           0.2803 |
[32m[20221213 14:58:47 @agent_ppo2.py:185][0m |          -0.0169 |           2.0327 |           0.2802 |
[32m[20221213 14:58:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.79
[32m[20221213 14:58:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.32
[32m[20221213 14:58:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.66
[32m[20221213 14:58:47 @agent_ppo2.py:143][0m Total time:       5.87 min
[32m[20221213 14:58:47 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221213 14:58:47 @agent_ppo2.py:121][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 14:58:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |          -0.0041 |           2.3836 |           0.2871 |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |          -0.0085 |           2.3092 |           0.2864 |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |          -0.0093 |           2.2924 |           0.2859 |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |          -0.0114 |           2.2910 |           0.2856 |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |          -0.0136 |           2.2643 |           0.2854 |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |          -0.0050 |           2.3408 |           0.2851 |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |          -0.0127 |           2.2597 |           0.2851 |
[32m[20221213 14:58:48 @agent_ppo2.py:185][0m |           0.0060 |           2.6001 |           0.2851 |
[32m[20221213 14:58:49 @agent_ppo2.py:185][0m |          -0.0158 |           2.2465 |           0.2845 |
[32m[20221213 14:58:49 @agent_ppo2.py:185][0m |          -0.0051 |           2.3830 |           0.2845 |
[32m[20221213 14:58:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 14:58:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.02
[32m[20221213 14:58:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 90.10
[32m[20221213 14:58:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.04
[32m[20221213 14:58:49 @agent_ppo2.py:143][0m Total time:       5.89 min
[32m[20221213 14:58:49 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221213 14:58:49 @agent_ppo2.py:121][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 14:58:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:49 @agent_ppo2.py:185][0m |           0.0004 |           2.3776 |           0.2777 |
[32m[20221213 14:58:49 @agent_ppo2.py:185][0m |          -0.0028 |           2.3200 |           0.2775 |
[32m[20221213 14:58:49 @agent_ppo2.py:185][0m |          -0.0066 |           2.2869 |           0.2774 |
[32m[20221213 14:58:49 @agent_ppo2.py:185][0m |           0.0011 |           2.4270 |           0.2774 |
[32m[20221213 14:58:50 @agent_ppo2.py:185][0m |          -0.0156 |           2.2722 |           0.2772 |
[32m[20221213 14:58:50 @agent_ppo2.py:185][0m |          -0.0118 |           2.2504 |           0.2769 |
[32m[20221213 14:58:50 @agent_ppo2.py:185][0m |          -0.0154 |           2.2412 |           0.2771 |
[32m[20221213 14:58:50 @agent_ppo2.py:185][0m |          -0.0131 |           2.2289 |           0.2774 |
[32m[20221213 14:58:50 @agent_ppo2.py:185][0m |          -0.0174 |           2.2266 |           0.2771 |
[32m[20221213 14:58:50 @agent_ppo2.py:185][0m |          -0.0159 |           2.2215 |           0.2774 |
[32m[20221213 14:58:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.47
[32m[20221213 14:58:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.90
[32m[20221213 14:58:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.47
[32m[20221213 14:58:50 @agent_ppo2.py:143][0m Total time:       5.92 min
[32m[20221213 14:58:50 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221213 14:58:50 @agent_ppo2.py:121][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 14:58:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |           0.0010 |           2.4434 |           0.2856 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0093 |           2.3649 |           0.2851 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0017 |           2.5691 |           0.2850 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0133 |           2.3379 |           0.2846 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0124 |           2.3264 |           0.2846 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0145 |           2.3286 |           0.2845 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0128 |           2.3311 |           0.2845 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0149 |           2.3154 |           0.2843 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0167 |           2.3084 |           0.2842 |
[32m[20221213 14:58:51 @agent_ppo2.py:185][0m |          -0.0086 |           2.4643 |           0.2843 |
[32m[20221213 14:58:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 94.40
[32m[20221213 14:58:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.91
[32m[20221213 14:58:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.51
[32m[20221213 14:58:51 @agent_ppo2.py:143][0m Total time:       5.94 min
[32m[20221213 14:58:51 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221213 14:58:51 @agent_ppo2.py:121][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 14:58:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |          -0.0016 |           2.3566 |           0.2862 |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |           0.0006 |           2.4498 |           0.2862 |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |          -0.0102 |           2.2608 |           0.2859 |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |          -0.0143 |           2.2280 |           0.2863 |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |          -0.0133 |           2.2190 |           0.2864 |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |          -0.0148 |           2.1935 |           0.2864 |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |          -0.0132 |           2.1808 |           0.2865 |
[32m[20221213 14:58:52 @agent_ppo2.py:185][0m |          -0.0099 |           2.2735 |           0.2867 |
[32m[20221213 14:58:53 @agent_ppo2.py:185][0m |          -0.0162 |           2.1645 |           0.2868 |
[32m[20221213 14:58:53 @agent_ppo2.py:185][0m |          -0.0183 |           2.1400 |           0.2871 |
[32m[20221213 14:58:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.16
[32m[20221213 14:58:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.41
[32m[20221213 14:58:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.18
[32m[20221213 14:58:53 @agent_ppo2.py:143][0m Total time:       5.96 min
[32m[20221213 14:58:53 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221213 14:58:53 @agent_ppo2.py:121][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 14:58:53 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:58:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:53 @agent_ppo2.py:185][0m |          -0.0030 |           2.3756 |           0.2795 |
[32m[20221213 14:58:53 @agent_ppo2.py:185][0m |          -0.0045 |           2.3223 |           0.2795 |
[32m[20221213 14:58:53 @agent_ppo2.py:185][0m |          -0.0082 |           2.2759 |           0.2790 |
[32m[20221213 14:58:53 @agent_ppo2.py:185][0m |          -0.0143 |           2.2593 |           0.2788 |
[32m[20221213 14:58:54 @agent_ppo2.py:185][0m |          -0.0106 |           2.2512 |           0.2788 |
[32m[20221213 14:58:54 @agent_ppo2.py:185][0m |          -0.0065 |           2.2999 |           0.2786 |
[32m[20221213 14:58:54 @agent_ppo2.py:185][0m |          -0.0143 |           2.2099 |           0.2787 |
[32m[20221213 14:58:54 @agent_ppo2.py:185][0m |          -0.0133 |           2.2028 |           0.2782 |
[32m[20221213 14:58:54 @agent_ppo2.py:185][0m |          -0.0071 |           2.2167 |           0.2783 |
[32m[20221213 14:58:54 @agent_ppo2.py:185][0m |          -0.0143 |           2.1858 |           0.2784 |
[32m[20221213 14:58:54 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:58:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.20
[32m[20221213 14:58:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.31
[32m[20221213 14:58:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.27
[32m[20221213 14:58:54 @agent_ppo2.py:143][0m Total time:       5.98 min
[32m[20221213 14:58:54 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221213 14:58:54 @agent_ppo2.py:121][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 14:58:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0004 |           2.1363 |           0.2864 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0049 |           2.0285 |           0.2864 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0093 |           2.0028 |           0.2865 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0057 |           1.9793 |           0.2865 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0134 |           1.9658 |           0.2865 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0137 |           1.9558 |           0.2866 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0060 |           1.9661 |           0.2869 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0112 |           1.9394 |           0.2869 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0095 |           1.9272 |           0.2871 |
[32m[20221213 14:58:55 @agent_ppo2.py:185][0m |          -0.0099 |           1.9449 |           0.2871 |
[32m[20221213 14:58:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.16
[32m[20221213 14:58:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.69
[32m[20221213 14:58:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.45
[32m[20221213 14:58:56 @agent_ppo2.py:143][0m Total time:       6.01 min
[32m[20221213 14:58:56 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221213 14:58:56 @agent_ppo2.py:121][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 14:58:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0014 |           2.3510 |           0.2890 |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0070 |           2.2788 |           0.2887 |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0099 |           2.2574 |           0.2887 |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0105 |           2.2346 |           0.2885 |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0112 |           2.2237 |           0.2886 |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0135 |           2.2160 |           0.2886 |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0163 |           2.2084 |           0.2888 |
[32m[20221213 14:58:56 @agent_ppo2.py:185][0m |          -0.0185 |           2.1973 |           0.2887 |
[32m[20221213 14:58:57 @agent_ppo2.py:185][0m |          -0.0182 |           2.1902 |           0.2887 |
[32m[20221213 14:58:57 @agent_ppo2.py:185][0m |          -0.0179 |           2.1791 |           0.2888 |
[32m[20221213 14:58:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.44
[32m[20221213 14:58:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.49
[32m[20221213 14:58:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.80
[32m[20221213 14:58:57 @agent_ppo2.py:143][0m Total time:       6.03 min
[32m[20221213 14:58:57 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221213 14:58:57 @agent_ppo2.py:121][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 14:58:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:57 @agent_ppo2.py:185][0m |          -0.0020 |           2.3500 |           0.2810 |
[32m[20221213 14:58:57 @agent_ppo2.py:185][0m |          -0.0022 |           2.3358 |           0.2804 |
[32m[20221213 14:58:57 @agent_ppo2.py:185][0m |          -0.0033 |           2.3467 |           0.2796 |
[32m[20221213 14:58:57 @agent_ppo2.py:185][0m |          -0.0102 |           2.2869 |           0.2792 |
[32m[20221213 14:58:58 @agent_ppo2.py:185][0m |          -0.0122 |           2.2739 |           0.2792 |
[32m[20221213 14:58:58 @agent_ppo2.py:185][0m |          -0.0139 |           2.2748 |           0.2790 |
[32m[20221213 14:58:58 @agent_ppo2.py:185][0m |          -0.0118 |           2.2605 |           0.2785 |
[32m[20221213 14:58:58 @agent_ppo2.py:185][0m |          -0.0135 |           2.2551 |           0.2787 |
[32m[20221213 14:58:58 @agent_ppo2.py:185][0m |          -0.0157 |           2.2553 |           0.2784 |
[32m[20221213 14:58:58 @agent_ppo2.py:185][0m |          -0.0176 |           2.2609 |           0.2782 |
[32m[20221213 14:58:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:58:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.27
[32m[20221213 14:58:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.11
[32m[20221213 14:58:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.13
[32m[20221213 14:58:58 @agent_ppo2.py:143][0m Total time:       6.05 min
[32m[20221213 14:58:58 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221213 14:58:58 @agent_ppo2.py:121][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 14:58:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:58:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |           0.0022 |           2.2819 |           0.2775 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |           0.0060 |           2.2318 |           0.2768 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0041 |           2.1947 |           0.2765 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0105 |           2.1259 |           0.2760 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0113 |           2.1283 |           0.2757 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0038 |           2.3341 |           0.2755 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0132 |           2.1052 |           0.2754 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0139 |           2.0843 |           0.2752 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0149 |           2.0758 |           0.2752 |
[32m[20221213 14:58:59 @agent_ppo2.py:185][0m |          -0.0060 |           2.2876 |           0.2750 |
[32m[20221213 14:58:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:59:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.23
[32m[20221213 14:59:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.70
[32m[20221213 14:59:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 71.95
[32m[20221213 14:59:00 @agent_ppo2.py:143][0m Total time:       6.07 min
[32m[20221213 14:59:00 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221213 14:59:00 @agent_ppo2.py:121][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 14:59:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |          -0.0010 |           2.2463 |           0.2760 |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |           0.0045 |           2.3325 |           0.2760 |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |          -0.0074 |           2.1762 |           0.2759 |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |          -0.0071 |           2.1681 |           0.2760 |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |          -0.0096 |           2.1498 |           0.2760 |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |          -0.0112 |           2.1436 |           0.2761 |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |          -0.0134 |           2.1344 |           0.2763 |
[32m[20221213 14:59:00 @agent_ppo2.py:185][0m |          -0.0137 |           2.1238 |           0.2765 |
[32m[20221213 14:59:01 @agent_ppo2.py:185][0m |          -0.0127 |           2.1214 |           0.2765 |
[32m[20221213 14:59:01 @agent_ppo2.py:185][0m |          -0.0158 |           2.1213 |           0.2765 |
[32m[20221213 14:59:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.65
[32m[20221213 14:59:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.35
[32m[20221213 14:59:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.73
[32m[20221213 14:59:01 @agent_ppo2.py:143][0m Total time:       6.09 min
[32m[20221213 14:59:01 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221213 14:59:01 @agent_ppo2.py:121][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 14:59:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:01 @agent_ppo2.py:185][0m |           0.0001 |           2.2742 |           0.2824 |
[32m[20221213 14:59:01 @agent_ppo2.py:185][0m |          -0.0060 |           2.2300 |           0.2820 |
[32m[20221213 14:59:01 @agent_ppo2.py:185][0m |          -0.0065 |           2.1989 |           0.2817 |
[32m[20221213 14:59:01 @agent_ppo2.py:185][0m |          -0.0093 |           2.1895 |           0.2813 |
[32m[20221213 14:59:02 @agent_ppo2.py:185][0m |          -0.0090 |           2.1761 |           0.2812 |
[32m[20221213 14:59:02 @agent_ppo2.py:185][0m |           0.0007 |           2.3823 |           0.2810 |
[32m[20221213 14:59:02 @agent_ppo2.py:185][0m |          -0.0128 |           2.1677 |           0.2804 |
[32m[20221213 14:59:02 @agent_ppo2.py:185][0m |          -0.0139 |           2.1436 |           0.2805 |
[32m[20221213 14:59:02 @agent_ppo2.py:185][0m |          -0.0125 |           2.1429 |           0.2803 |
[32m[20221213 14:59:02 @agent_ppo2.py:185][0m |          -0.0149 |           2.1330 |           0.2800 |
[32m[20221213 14:59:02 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:59:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.03
[32m[20221213 14:59:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.37
[32m[20221213 14:59:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.01
[32m[20221213 14:59:02 @agent_ppo2.py:143][0m Total time:       6.12 min
[32m[20221213 14:59:02 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221213 14:59:02 @agent_ppo2.py:121][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 14:59:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |           0.0002 |           2.3408 |           0.2797 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0120 |           2.2642 |           0.2788 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0100 |           2.2372 |           0.2786 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0126 |           2.2241 |           0.2784 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0124 |           2.2152 |           0.2779 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0095 |           2.2169 |           0.2778 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0141 |           2.2008 |           0.2777 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0166 |           2.1984 |           0.2774 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0158 |           2.1855 |           0.2772 |
[32m[20221213 14:59:03 @agent_ppo2.py:185][0m |          -0.0149 |           2.1779 |           0.2771 |
[32m[20221213 14:59:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:59:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.84
[32m[20221213 14:59:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.34
[32m[20221213 14:59:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 12.73
[32m[20221213 14:59:04 @agent_ppo2.py:143][0m Total time:       6.14 min
[32m[20221213 14:59:04 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221213 14:59:04 @agent_ppo2.py:121][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 14:59:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:04 @agent_ppo2.py:185][0m |          -0.0031 |           2.4175 |           0.2779 |
[32m[20221213 14:59:04 @agent_ppo2.py:185][0m |          -0.0058 |           2.3735 |           0.2773 |
[32m[20221213 14:59:04 @agent_ppo2.py:185][0m |           0.0120 |           2.6490 |           0.2768 |
[32m[20221213 14:59:04 @agent_ppo2.py:185][0m |          -0.0067 |           2.3428 |           0.2765 |
[32m[20221213 14:59:04 @agent_ppo2.py:185][0m |          -0.0071 |           2.3313 |           0.2762 |
[32m[20221213 14:59:04 @agent_ppo2.py:185][0m |          -0.0142 |           2.2912 |           0.2761 |
[32m[20221213 14:59:04 @agent_ppo2.py:185][0m |          -0.0136 |           2.2763 |           0.2757 |
[32m[20221213 14:59:05 @agent_ppo2.py:185][0m |          -0.0094 |           2.2736 |           0.2756 |
[32m[20221213 14:59:05 @agent_ppo2.py:185][0m |          -0.0150 |           2.2602 |           0.2756 |
[32m[20221213 14:59:05 @agent_ppo2.py:185][0m |          -0.0052 |           2.3528 |           0.2755 |
[32m[20221213 14:59:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:59:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.87
[32m[20221213 14:59:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.03
[32m[20221213 14:59:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.96
[32m[20221213 14:59:05 @agent_ppo2.py:143][0m Total time:       6.16 min
[32m[20221213 14:59:05 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221213 14:59:05 @agent_ppo2.py:121][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 14:59:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:05 @agent_ppo2.py:185][0m |           0.0004 |           2.3637 |           0.2705 |
[32m[20221213 14:59:05 @agent_ppo2.py:185][0m |          -0.0053 |           2.3100 |           0.2703 |
[32m[20221213 14:59:05 @agent_ppo2.py:185][0m |          -0.0073 |           2.2918 |           0.2701 |
[32m[20221213 14:59:06 @agent_ppo2.py:185][0m |          -0.0098 |           2.2787 |           0.2700 |
[32m[20221213 14:59:06 @agent_ppo2.py:185][0m |          -0.0077 |           2.2598 |           0.2699 |
[32m[20221213 14:59:06 @agent_ppo2.py:185][0m |          -0.0091 |           2.2709 |           0.2696 |
[32m[20221213 14:59:06 @agent_ppo2.py:185][0m |          -0.0113 |           2.2423 |           0.2697 |
[32m[20221213 14:59:06 @agent_ppo2.py:185][0m |          -0.0071 |           2.2943 |           0.2695 |
[32m[20221213 14:59:06 @agent_ppo2.py:185][0m |          -0.0138 |           2.2299 |           0.2695 |
[32m[20221213 14:59:06 @agent_ppo2.py:185][0m |          -0.0100 |           2.2402 |           0.2694 |
[32m[20221213 14:59:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.51
[32m[20221213 14:59:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.73
[32m[20221213 14:59:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.96
[32m[20221213 14:59:06 @agent_ppo2.py:143][0m Total time:       6.18 min
[32m[20221213 14:59:06 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221213 14:59:06 @agent_ppo2.py:121][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 14:59:06 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:59:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |           0.0078 |           2.4971 |           0.2735 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0100 |           2.2322 |           0.2728 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0104 |           2.1967 |           0.2723 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0114 |           2.1691 |           0.2719 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0163 |           2.1638 |           0.2718 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0098 |           2.1430 |           0.2716 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0119 |           2.1321 |           0.2712 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0105 |           2.1338 |           0.2709 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0137 |           2.1076 |           0.2710 |
[32m[20221213 14:59:07 @agent_ppo2.py:185][0m |          -0.0119 |           2.0963 |           0.2707 |
[32m[20221213 14:59:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.10
[32m[20221213 14:59:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.70
[32m[20221213 14:59:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.58
[32m[20221213 14:59:08 @agent_ppo2.py:143][0m Total time:       6.21 min
[32m[20221213 14:59:08 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221213 14:59:08 @agent_ppo2.py:121][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 14:59:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:08 @agent_ppo2.py:185][0m |          -0.0045 |           2.5311 |           0.2706 |
[32m[20221213 14:59:08 @agent_ppo2.py:185][0m |          -0.0091 |           2.4511 |           0.2705 |
[32m[20221213 14:59:08 @agent_ppo2.py:185][0m |          -0.0123 |           2.4285 |           0.2707 |
[32m[20221213 14:59:08 @agent_ppo2.py:185][0m |          -0.0156 |           2.4122 |           0.2706 |
[32m[20221213 14:59:08 @agent_ppo2.py:185][0m |          -0.0050 |           2.7003 |           0.2711 |
[32m[20221213 14:59:08 @agent_ppo2.py:185][0m |          -0.0168 |           2.3937 |           0.2711 |
[32m[20221213 14:59:08 @agent_ppo2.py:185][0m |           0.0079 |           2.7526 |           0.2713 |
[32m[20221213 14:59:09 @agent_ppo2.py:185][0m |          -0.0183 |           2.3925 |           0.2716 |
[32m[20221213 14:59:09 @agent_ppo2.py:185][0m |          -0.0171 |           2.3526 |           0.2719 |
[32m[20221213 14:59:09 @agent_ppo2.py:185][0m |          -0.0172 |           2.3471 |           0.2718 |
[32m[20221213 14:59:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:59:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.50
[32m[20221213 14:59:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 89.31
[32m[20221213 14:59:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.94
[32m[20221213 14:59:09 @agent_ppo2.py:143][0m Total time:       6.23 min
[32m[20221213 14:59:09 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221213 14:59:09 @agent_ppo2.py:121][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 14:59:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:09 @agent_ppo2.py:185][0m |           0.0017 |           2.3353 |           0.2724 |
[32m[20221213 14:59:09 @agent_ppo2.py:185][0m |          -0.0063 |           2.2722 |           0.2721 |
[32m[20221213 14:59:09 @agent_ppo2.py:185][0m |          -0.0092 |           2.2473 |           0.2718 |
[32m[20221213 14:59:10 @agent_ppo2.py:185][0m |          -0.0097 |           2.2236 |           0.2717 |
[32m[20221213 14:59:10 @agent_ppo2.py:185][0m |          -0.0088 |           2.2149 |           0.2716 |
[32m[20221213 14:59:10 @agent_ppo2.py:185][0m |          -0.0129 |           2.1954 |           0.2713 |
[32m[20221213 14:59:10 @agent_ppo2.py:185][0m |          -0.0012 |           2.3191 |           0.2713 |
[32m[20221213 14:59:10 @agent_ppo2.py:185][0m |          -0.0055 |           2.2684 |           0.2713 |
[32m[20221213 14:59:10 @agent_ppo2.py:185][0m |          -0.0067 |           2.2304 |           0.2710 |
[32m[20221213 14:59:10 @agent_ppo2.py:185][0m |          -0.0143 |           2.1623 |           0.2709 |
[32m[20221213 14:59:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.52
[32m[20221213 14:59:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.71
[32m[20221213 14:59:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.96
[32m[20221213 14:59:10 @agent_ppo2.py:143][0m Total time:       6.25 min
[32m[20221213 14:59:10 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221213 14:59:10 @agent_ppo2.py:121][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 14:59:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0003 |           2.2655 |           0.2709 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0061 |           2.1797 |           0.2708 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0078 |           2.1605 |           0.2705 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0117 |           2.1441 |           0.2705 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0110 |           2.1281 |           0.2703 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0130 |           2.1041 |           0.2704 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0122 |           2.0896 |           0.2703 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0098 |           2.0856 |           0.2702 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0160 |           2.0683 |           0.2701 |
[32m[20221213 14:59:11 @agent_ppo2.py:185][0m |          -0.0141 |           2.0483 |           0.2702 |
[32m[20221213 14:59:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.08
[32m[20221213 14:59:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.65
[32m[20221213 14:59:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.05
[32m[20221213 14:59:12 @agent_ppo2.py:143][0m Total time:       6.27 min
[32m[20221213 14:59:12 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221213 14:59:12 @agent_ppo2.py:121][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 14:59:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:12 @agent_ppo2.py:185][0m |           0.0001 |           2.6091 |           0.2637 |
[32m[20221213 14:59:12 @agent_ppo2.py:185][0m |          -0.0062 |           2.5423 |           0.2636 |
[32m[20221213 14:59:12 @agent_ppo2.py:185][0m |          -0.0133 |           2.5207 |           0.2633 |
[32m[20221213 14:59:12 @agent_ppo2.py:185][0m |          -0.0120 |           2.5081 |           0.2631 |
[32m[20221213 14:59:12 @agent_ppo2.py:185][0m |          -0.0128 |           2.4945 |           0.2630 |
[32m[20221213 14:59:12 @agent_ppo2.py:185][0m |          -0.0100 |           2.4803 |           0.2631 |
[32m[20221213 14:59:12 @agent_ppo2.py:185][0m |          -0.0147 |           2.4754 |           0.2630 |
[32m[20221213 14:59:13 @agent_ppo2.py:185][0m |          -0.0136 |           2.4768 |           0.2630 |
[32m[20221213 14:59:13 @agent_ppo2.py:185][0m |          -0.0152 |           2.4612 |           0.2629 |
[32m[20221213 14:59:13 @agent_ppo2.py:185][0m |          -0.0136 |           2.4576 |           0.2628 |
[32m[20221213 14:59:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.85
[32m[20221213 14:59:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.12
[32m[20221213 14:59:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.43
[32m[20221213 14:59:13 @agent_ppo2.py:143][0m Total time:       6.30 min
[32m[20221213 14:59:13 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221213 14:59:13 @agent_ppo2.py:121][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 14:59:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:13 @agent_ppo2.py:185][0m |           0.0026 |           2.5072 |           0.2666 |
[32m[20221213 14:59:13 @agent_ppo2.py:185][0m |          -0.0049 |           2.4224 |           0.2663 |
[32m[20221213 14:59:13 @agent_ppo2.py:185][0m |          -0.0118 |           2.3951 |           0.2663 |
[32m[20221213 14:59:14 @agent_ppo2.py:185][0m |          -0.0103 |           2.3682 |           0.2659 |
[32m[20221213 14:59:14 @agent_ppo2.py:185][0m |          -0.0124 |           2.3548 |           0.2658 |
[32m[20221213 14:59:14 @agent_ppo2.py:185][0m |          -0.0153 |           2.3375 |           0.2656 |
[32m[20221213 14:59:14 @agent_ppo2.py:185][0m |          -0.0142 |           2.3225 |           0.2656 |
[32m[20221213 14:59:14 @agent_ppo2.py:185][0m |          -0.0155 |           2.3143 |           0.2653 |
[32m[20221213 14:59:14 @agent_ppo2.py:185][0m |          -0.0143 |           2.2999 |           0.2653 |
[32m[20221213 14:59:14 @agent_ppo2.py:185][0m |          -0.0185 |           2.3004 |           0.2652 |
[32m[20221213 14:59:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.73
[32m[20221213 14:59:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.11
[32m[20221213 14:59:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.71
[32m[20221213 14:59:14 @agent_ppo2.py:143][0m Total time:       6.32 min
[32m[20221213 14:59:14 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221213 14:59:14 @agent_ppo2.py:121][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 14:59:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |           0.0042 |           2.6902 |           0.2698 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0064 |           2.5618 |           0.2694 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0031 |           2.6525 |           0.2691 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0122 |           2.4905 |           0.2688 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0141 |           2.4553 |           0.2685 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0180 |           2.4427 |           0.2684 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0135 |           2.4703 |           0.2682 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0177 |           2.4016 |           0.2680 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0179 |           2.3942 |           0.2678 |
[32m[20221213 14:59:15 @agent_ppo2.py:185][0m |          -0.0196 |           2.3782 |           0.2677 |
[32m[20221213 14:59:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.23
[32m[20221213 14:59:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.18
[32m[20221213 14:59:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.37
[32m[20221213 14:59:16 @agent_ppo2.py:143][0m Total time:       6.34 min
[32m[20221213 14:59:16 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221213 14:59:16 @agent_ppo2.py:121][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 14:59:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:16 @agent_ppo2.py:185][0m |           0.0000 |           2.3406 |           0.2696 |
[32m[20221213 14:59:16 @agent_ppo2.py:185][0m |          -0.0112 |           2.2072 |           0.2693 |
[32m[20221213 14:59:16 @agent_ppo2.py:185][0m |          -0.0127 |           2.1849 |           0.2690 |
[32m[20221213 14:59:16 @agent_ppo2.py:185][0m |          -0.0109 |           2.1393 |           0.2689 |
[32m[20221213 14:59:16 @agent_ppo2.py:185][0m |          -0.0095 |           2.1432 |           0.2687 |
[32m[20221213 14:59:16 @agent_ppo2.py:185][0m |          -0.0149 |           2.1123 |           0.2688 |
[32m[20221213 14:59:16 @agent_ppo2.py:185][0m |          -0.0147 |           2.1051 |           0.2687 |
[32m[20221213 14:59:17 @agent_ppo2.py:185][0m |          -0.0183 |           2.0925 |           0.2687 |
[32m[20221213 14:59:17 @agent_ppo2.py:185][0m |          -0.0194 |           2.0848 |           0.2687 |
[32m[20221213 14:59:17 @agent_ppo2.py:185][0m |          -0.0138 |           2.0834 |           0.2687 |
[32m[20221213 14:59:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.37
[32m[20221213 14:59:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.67
[32m[20221213 14:59:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.21
[32m[20221213 14:59:17 @agent_ppo2.py:143][0m Total time:       6.36 min
[32m[20221213 14:59:17 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221213 14:59:17 @agent_ppo2.py:121][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 14:59:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:17 @agent_ppo2.py:185][0m |           0.0023 |           2.4290 |           0.2663 |
[32m[20221213 14:59:17 @agent_ppo2.py:185][0m |          -0.0022 |           2.3396 |           0.2662 |
[32m[20221213 14:59:17 @agent_ppo2.py:185][0m |          -0.0097 |           2.2922 |           0.2658 |
[32m[20221213 14:59:18 @agent_ppo2.py:185][0m |          -0.0142 |           2.2767 |           0.2657 |
[32m[20221213 14:59:18 @agent_ppo2.py:185][0m |          -0.0124 |           2.2459 |           0.2656 |
[32m[20221213 14:59:18 @agent_ppo2.py:185][0m |          -0.0130 |           2.2346 |           0.2656 |
[32m[20221213 14:59:18 @agent_ppo2.py:185][0m |          -0.0129 |           2.2239 |           0.2655 |
[32m[20221213 14:59:18 @agent_ppo2.py:185][0m |          -0.0140 |           2.2183 |           0.2654 |
[32m[20221213 14:59:18 @agent_ppo2.py:185][0m |          -0.0150 |           2.2039 |           0.2653 |
[32m[20221213 14:59:18 @agent_ppo2.py:185][0m |          -0.0176 |           2.1970 |           0.2653 |
[32m[20221213 14:59:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 81.03
[32m[20221213 14:59:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.37
[32m[20221213 14:59:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.42
[32m[20221213 14:59:18 @agent_ppo2.py:143][0m Total time:       6.38 min
[32m[20221213 14:59:18 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221213 14:59:18 @agent_ppo2.py:121][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 14:59:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0040 |           2.3996 |           0.2731 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0037 |           2.3403 |           0.2729 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0087 |           2.3045 |           0.2728 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0075 |           2.2612 |           0.2728 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0111 |           2.2350 |           0.2727 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0136 |           2.2063 |           0.2726 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0143 |           2.2126 |           0.2725 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0123 |           2.1841 |           0.2723 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0197 |           2.1684 |           0.2725 |
[32m[20221213 14:59:19 @agent_ppo2.py:185][0m |          -0.0197 |           2.1347 |           0.2725 |
[32m[20221213 14:59:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.73
[32m[20221213 14:59:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.67
[32m[20221213 14:59:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.44
[32m[20221213 14:59:20 @agent_ppo2.py:143][0m Total time:       6.41 min
[32m[20221213 14:59:20 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221213 14:59:20 @agent_ppo2.py:121][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 14:59:20 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 14:59:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:20 @agent_ppo2.py:185][0m |          -0.0050 |           2.5895 |           0.2668 |
[32m[20221213 14:59:20 @agent_ppo2.py:185][0m |          -0.0078 |           2.5176 |           0.2667 |
[32m[20221213 14:59:20 @agent_ppo2.py:185][0m |          -0.0132 |           2.4842 |           0.2668 |
[32m[20221213 14:59:20 @agent_ppo2.py:185][0m |          -0.0132 |           2.4678 |           0.2670 |
[32m[20221213 14:59:20 @agent_ppo2.py:185][0m |          -0.0140 |           2.4466 |           0.2672 |
[32m[20221213 14:59:20 @agent_ppo2.py:185][0m |          -0.0145 |           2.4398 |           0.2673 |
[32m[20221213 14:59:21 @agent_ppo2.py:185][0m |          -0.0155 |           2.4309 |           0.2674 |
[32m[20221213 14:59:21 @agent_ppo2.py:185][0m |          -0.0168 |           2.4112 |           0.2674 |
[32m[20221213 14:59:21 @agent_ppo2.py:185][0m |          -0.0151 |           2.3935 |           0.2676 |
[32m[20221213 14:59:21 @agent_ppo2.py:185][0m |          -0.0183 |           2.3791 |           0.2678 |
[32m[20221213 14:59:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:59:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.34
[32m[20221213 14:59:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.00
[32m[20221213 14:59:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 91.46
[32m[20221213 14:59:21 @agent_ppo2.py:143][0m Total time:       6.43 min
[32m[20221213 14:59:21 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221213 14:59:21 @agent_ppo2.py:121][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 14:59:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:21 @agent_ppo2.py:185][0m |          -0.0024 |           2.4466 |           0.2674 |
[32m[20221213 14:59:21 @agent_ppo2.py:185][0m |          -0.0081 |           2.3496 |           0.2670 |
[32m[20221213 14:59:21 @agent_ppo2.py:185][0m |          -0.0156 |           2.3526 |           0.2668 |
[32m[20221213 14:59:22 @agent_ppo2.py:185][0m |          -0.0091 |           2.2935 |           0.2667 |
[32m[20221213 14:59:22 @agent_ppo2.py:185][0m |          -0.0116 |           2.3059 |           0.2665 |
[32m[20221213 14:59:22 @agent_ppo2.py:185][0m |          -0.0148 |           2.2719 |           0.2666 |
[32m[20221213 14:59:22 @agent_ppo2.py:185][0m |          -0.0086 |           2.2699 |           0.2668 |
[32m[20221213 14:59:22 @agent_ppo2.py:185][0m |          -0.0140 |           2.2506 |           0.2667 |
[32m[20221213 14:59:22 @agent_ppo2.py:185][0m |          -0.0127 |           2.2270 |           0.2669 |
[32m[20221213 14:59:22 @agent_ppo2.py:185][0m |          -0.0139 |           2.2173 |           0.2670 |
[32m[20221213 14:59:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 14:59:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.64
[32m[20221213 14:59:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.54
[32m[20221213 14:59:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.75
[32m[20221213 14:59:22 @agent_ppo2.py:143][0m Total time:       6.45 min
[32m[20221213 14:59:22 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221213 14:59:22 @agent_ppo2.py:121][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 14:59:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |           0.0016 |           2.4842 |           0.2742 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0077 |           2.3906 |           0.2742 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0088 |           2.3554 |           0.2739 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0121 |           2.3386 |           0.2738 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0133 |           2.3101 |           0.2738 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0122 |           2.2911 |           0.2738 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0078 |           2.4215 |           0.2737 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0175 |           2.2746 |           0.2737 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0145 |           2.2586 |           0.2735 |
[32m[20221213 14:59:23 @agent_ppo2.py:185][0m |          -0.0156 |           2.2437 |           0.2736 |
[32m[20221213 14:59:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.30
[32m[20221213 14:59:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.57
[32m[20221213 14:59:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 76.51
[32m[20221213 14:59:24 @agent_ppo2.py:143][0m Total time:       6.47 min
[32m[20221213 14:59:24 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221213 14:59:24 @agent_ppo2.py:121][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 14:59:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:24 @agent_ppo2.py:185][0m |          -0.0038 |           2.4008 |           0.2852 |
[32m[20221213 14:59:24 @agent_ppo2.py:185][0m |          -0.0004 |           2.3757 |           0.2849 |
[32m[20221213 14:59:24 @agent_ppo2.py:185][0m |          -0.0068 |           2.3932 |           0.2846 |
[32m[20221213 14:59:24 @agent_ppo2.py:185][0m |          -0.0093 |           2.3230 |           0.2841 |
[32m[20221213 14:59:24 @agent_ppo2.py:185][0m |          -0.0132 |           2.2995 |           0.2838 |
[32m[20221213 14:59:24 @agent_ppo2.py:185][0m |          -0.0088 |           2.3378 |           0.2837 |
[32m[20221213 14:59:24 @agent_ppo2.py:185][0m |          -0.0051 |           2.5665 |           0.2834 |
[32m[20221213 14:59:25 @agent_ppo2.py:185][0m |          -0.0146 |           2.2785 |           0.2831 |
[32m[20221213 14:59:25 @agent_ppo2.py:185][0m |          -0.0208 |           2.2682 |           0.2830 |
[32m[20221213 14:59:25 @agent_ppo2.py:185][0m |          -0.0158 |           2.2449 |           0.2829 |
[32m[20221213 14:59:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.24
[32m[20221213 14:59:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.65
[32m[20221213 14:59:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.69
[32m[20221213 14:59:25 @agent_ppo2.py:143][0m Total time:       6.50 min
[32m[20221213 14:59:25 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221213 14:59:25 @agent_ppo2.py:121][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 14:59:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:25 @agent_ppo2.py:185][0m |          -0.0055 |           2.4033 |           0.2697 |
[32m[20221213 14:59:25 @agent_ppo2.py:185][0m |          -0.0088 |           2.3718 |           0.2689 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |          -0.0123 |           2.3726 |           0.2687 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |          -0.0118 |           2.3374 |           0.2685 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |          -0.0146 |           2.3253 |           0.2684 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |          -0.0089 |           2.3283 |           0.2684 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |          -0.0096 |           2.3067 |           0.2684 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |          -0.0143 |           2.2973 |           0.2682 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |          -0.0121 |           2.2923 |           0.2681 |
[32m[20221213 14:59:26 @agent_ppo2.py:185][0m |           0.0036 |           2.6587 |           0.2680 |
[32m[20221213 14:59:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.57
[32m[20221213 14:59:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.22
[32m[20221213 14:59:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.05
[32m[20221213 14:59:26 @agent_ppo2.py:143][0m Total time:       6.52 min
[32m[20221213 14:59:26 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221213 14:59:26 @agent_ppo2.py:121][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 14:59:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0008 |           2.3030 |           0.2706 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |           0.0009 |           2.3839 |           0.2704 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0113 |           2.1677 |           0.2702 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0098 |           2.1384 |           0.2701 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0107 |           2.1298 |           0.2699 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0139 |           2.1136 |           0.2701 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0120 |           2.1153 |           0.2699 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0117 |           2.1004 |           0.2699 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0183 |           2.0960 |           0.2697 |
[32m[20221213 14:59:27 @agent_ppo2.py:185][0m |          -0.0154 |           2.0809 |           0.2698 |
[32m[20221213 14:59:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.69
[32m[20221213 14:59:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.68
[32m[20221213 14:59:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 73.33
[32m[20221213 14:59:28 @agent_ppo2.py:143][0m Total time:       6.54 min
[32m[20221213 14:59:28 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221213 14:59:28 @agent_ppo2.py:121][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 14:59:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:28 @agent_ppo2.py:185][0m |          -0.0065 |           2.5382 |           0.2679 |
[32m[20221213 14:59:28 @agent_ppo2.py:185][0m |          -0.0022 |           2.4857 |           0.2682 |
[32m[20221213 14:59:28 @agent_ppo2.py:185][0m |           0.0001 |           2.6477 |           0.2681 |
[32m[20221213 14:59:28 @agent_ppo2.py:185][0m |           0.0015 |           2.6617 |           0.2685 |
[32m[20221213 14:59:28 @agent_ppo2.py:185][0m |          -0.0101 |           2.3994 |           0.2683 |
[32m[20221213 14:59:28 @agent_ppo2.py:185][0m |          -0.0039 |           2.4221 |           0.2685 |
[32m[20221213 14:59:29 @agent_ppo2.py:185][0m |          -0.0128 |           2.3712 |           0.2688 |
[32m[20221213 14:59:29 @agent_ppo2.py:185][0m |          -0.0158 |           2.3651 |           0.2687 |
[32m[20221213 14:59:29 @agent_ppo2.py:185][0m |          -0.0143 |           2.3556 |           0.2690 |
[32m[20221213 14:59:29 @agent_ppo2.py:185][0m |          -0.0202 |           2.3467 |           0.2693 |
[32m[20221213 14:59:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.16
[32m[20221213 14:59:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.60
[32m[20221213 14:59:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 73.08
[32m[20221213 14:59:29 @agent_ppo2.py:143][0m Total time:       6.56 min
[32m[20221213 14:59:29 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221213 14:59:29 @agent_ppo2.py:121][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 14:59:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:29 @agent_ppo2.py:185][0m |          -0.0030 |           2.3107 |           0.2742 |
[32m[20221213 14:59:29 @agent_ppo2.py:185][0m |          -0.0065 |           2.2645 |           0.2739 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0087 |           2.2250 |           0.2733 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0066 |           2.2348 |           0.2733 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0125 |           2.1975 |           0.2727 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0137 |           2.1859 |           0.2724 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0136 |           2.1891 |           0.2724 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0169 |           2.1810 |           0.2721 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0128 |           2.1802 |           0.2720 |
[32m[20221213 14:59:30 @agent_ppo2.py:185][0m |          -0.0131 |           2.1479 |           0.2717 |
[32m[20221213 14:59:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.00
[32m[20221213 14:59:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.61
[32m[20221213 14:59:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.75
[32m[20221213 14:59:30 @agent_ppo2.py:143][0m Total time:       6.59 min
[32m[20221213 14:59:30 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221213 14:59:30 @agent_ppo2.py:121][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 14:59:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |           0.0011 |           2.4979 |           0.2632 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0119 |           2.4410 |           0.2628 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |           0.0167 |           2.8829 |           0.2625 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0087 |           2.4176 |           0.2618 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0132 |           2.3596 |           0.2619 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0117 |           2.3423 |           0.2618 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0156 |           2.3380 |           0.2619 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0166 |           2.3289 |           0.2616 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0083 |           2.3841 |           0.2616 |
[32m[20221213 14:59:31 @agent_ppo2.py:185][0m |          -0.0155 |           2.2954 |           0.2613 |
[32m[20221213 14:59:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.85
[32m[20221213 14:59:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 93.98
[32m[20221213 14:59:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 65.84
[32m[20221213 14:59:32 @agent_ppo2.py:143][0m Total time:       6.61 min
[32m[20221213 14:59:32 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221213 14:59:32 @agent_ppo2.py:121][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 14:59:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:32 @agent_ppo2.py:185][0m |          -0.0034 |           2.5503 |           0.2640 |
[32m[20221213 14:59:32 @agent_ppo2.py:185][0m |          -0.0037 |           2.4883 |           0.2640 |
[32m[20221213 14:59:32 @agent_ppo2.py:185][0m |          -0.0076 |           2.4661 |           0.2638 |
[32m[20221213 14:59:32 @agent_ppo2.py:185][0m |          -0.0097 |           2.4661 |           0.2637 |
[32m[20221213 14:59:32 @agent_ppo2.py:185][0m |          -0.0009 |           2.6101 |           0.2638 |
[32m[20221213 14:59:32 @agent_ppo2.py:185][0m |          -0.0117 |           2.4427 |           0.2634 |
[32m[20221213 14:59:33 @agent_ppo2.py:185][0m |          -0.0146 |           2.4334 |           0.2638 |
[32m[20221213 14:59:33 @agent_ppo2.py:185][0m |          -0.0133 |           2.4065 |           0.2637 |
[32m[20221213 14:59:33 @agent_ppo2.py:185][0m |          -0.0099 |           2.4034 |           0.2637 |
[32m[20221213 14:59:33 @agent_ppo2.py:185][0m |          -0.0125 |           2.3939 |           0.2637 |
[32m[20221213 14:59:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.21
[32m[20221213 14:59:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.94
[32m[20221213 14:59:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.84
[32m[20221213 14:59:33 @agent_ppo2.py:143][0m Total time:       6.63 min
[32m[20221213 14:59:33 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221213 14:59:33 @agent_ppo2.py:121][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 14:59:33 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:59:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:33 @agent_ppo2.py:185][0m |           0.0004 |           2.4706 |           0.2614 |
[32m[20221213 14:59:33 @agent_ppo2.py:185][0m |          -0.0052 |           2.3996 |           0.2611 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0079 |           2.3822 |           0.2608 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0105 |           2.3610 |           0.2607 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0072 |           2.3589 |           0.2609 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0100 |           2.3492 |           0.2607 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0138 |           2.3423 |           0.2606 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0114 |           2.3315 |           0.2606 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0133 |           2.3208 |           0.2607 |
[32m[20221213 14:59:34 @agent_ppo2.py:185][0m |          -0.0150 |           2.3254 |           0.2607 |
[32m[20221213 14:59:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 14:59:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.32
[32m[20221213 14:59:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 102.29
[32m[20221213 14:59:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.05
[32m[20221213 14:59:34 @agent_ppo2.py:143][0m Total time:       6.65 min
[32m[20221213 14:59:34 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221213 14:59:34 @agent_ppo2.py:121][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 14:59:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0003 |           2.4876 |           0.2695 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0064 |           2.4196 |           0.2693 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0099 |           2.4008 |           0.2690 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0117 |           2.3873 |           0.2690 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0123 |           2.3727 |           0.2692 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0146 |           2.3647 |           0.2692 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0140 |           2.3516 |           0.2692 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0131 |           2.3473 |           0.2693 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0151 |           2.3328 |           0.2697 |
[32m[20221213 14:59:35 @agent_ppo2.py:185][0m |          -0.0150 |           2.3306 |           0.2694 |
[32m[20221213 14:59:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.37
[32m[20221213 14:59:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.37
[32m[20221213 14:59:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 67.69
[32m[20221213 14:59:36 @agent_ppo2.py:143][0m Total time:       6.67 min
[32m[20221213 14:59:36 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221213 14:59:36 @agent_ppo2.py:121][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 14:59:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:36 @agent_ppo2.py:185][0m |          -0.0045 |           2.6498 |           0.2724 |
[32m[20221213 14:59:36 @agent_ppo2.py:185][0m |          -0.0055 |           2.5880 |           0.2718 |
[32m[20221213 14:59:36 @agent_ppo2.py:185][0m |          -0.0082 |           2.5699 |           0.2720 |
[32m[20221213 14:59:36 @agent_ppo2.py:185][0m |           0.0011 |           2.6594 |           0.2720 |
[32m[20221213 14:59:36 @agent_ppo2.py:185][0m |          -0.0078 |           2.5451 |           0.2722 |
[32m[20221213 14:59:36 @agent_ppo2.py:185][0m |          -0.0106 |           2.5306 |           0.2721 |
[32m[20221213 14:59:37 @agent_ppo2.py:185][0m |          -0.0107 |           2.5205 |           0.2722 |
[32m[20221213 14:59:37 @agent_ppo2.py:185][0m |          -0.0105 |           2.5627 |           0.2723 |
[32m[20221213 14:59:37 @agent_ppo2.py:185][0m |          -0.0181 |           2.5182 |           0.2722 |
[32m[20221213 14:59:37 @agent_ppo2.py:185][0m |          -0.0126 |           2.5016 |           0.2723 |
[32m[20221213 14:59:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.00
[32m[20221213 14:59:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.94
[32m[20221213 14:59:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.24
[32m[20221213 14:59:37 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221213 14:59:37 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221213 14:59:37 @agent_ppo2.py:121][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 14:59:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:37 @agent_ppo2.py:185][0m |           0.0077 |           2.6270 |           0.2772 |
[32m[20221213 14:59:37 @agent_ppo2.py:185][0m |          -0.0067 |           2.5127 |           0.2763 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0104 |           2.4955 |           0.2766 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0122 |           2.4846 |           0.2769 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0145 |           2.4781 |           0.2770 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0108 |           2.4679 |           0.2769 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0135 |           2.4636 |           0.2769 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0137 |           2.4873 |           0.2769 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0147 |           2.4543 |           0.2769 |
[32m[20221213 14:59:38 @agent_ppo2.py:185][0m |          -0.0146 |           2.4706 |           0.2770 |
[32m[20221213 14:59:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 98.23
[32m[20221213 14:59:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 111.16
[32m[20221213 14:59:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.12
[32m[20221213 14:59:38 @agent_ppo2.py:143][0m Total time:       6.72 min
[32m[20221213 14:59:38 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221213 14:59:38 @agent_ppo2.py:121][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 14:59:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0014 |           2.5787 |           0.2766 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0006 |           2.5980 |           0.2761 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0079 |           2.4820 |           0.2759 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0115 |           2.4548 |           0.2758 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0120 |           2.4383 |           0.2754 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0126 |           2.4264 |           0.2752 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0153 |           2.4209 |           0.2751 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0192 |           2.4085 |           0.2748 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0095 |           2.4804 |           0.2747 |
[32m[20221213 14:59:39 @agent_ppo2.py:185][0m |          -0.0137 |           2.3917 |           0.2745 |
[32m[20221213 14:59:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.86
[32m[20221213 14:59:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.84
[32m[20221213 14:59:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.89
[32m[20221213 14:59:40 @agent_ppo2.py:143][0m Total time:       6.74 min
[32m[20221213 14:59:40 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221213 14:59:40 @agent_ppo2.py:121][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 14:59:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:40 @agent_ppo2.py:185][0m |          -0.0017 |           2.5050 |           0.2743 |
[32m[20221213 14:59:40 @agent_ppo2.py:185][0m |          -0.0046 |           2.4296 |           0.2738 |
[32m[20221213 14:59:40 @agent_ppo2.py:185][0m |          -0.0096 |           2.3982 |           0.2738 |
[32m[20221213 14:59:40 @agent_ppo2.py:185][0m |          -0.0127 |           2.3741 |           0.2738 |
[32m[20221213 14:59:40 @agent_ppo2.py:185][0m |          -0.0085 |           2.3695 |           0.2735 |
[32m[20221213 14:59:41 @agent_ppo2.py:185][0m |          -0.0107 |           2.3360 |           0.2733 |
[32m[20221213 14:59:41 @agent_ppo2.py:185][0m |          -0.0093 |           2.3856 |           0.2733 |
[32m[20221213 14:59:41 @agent_ppo2.py:185][0m |          -0.0109 |           2.3036 |           0.2732 |
[32m[20221213 14:59:41 @agent_ppo2.py:185][0m |          -0.0051 |           2.4469 |           0.2732 |
[32m[20221213 14:59:41 @agent_ppo2.py:185][0m |          -0.0114 |           2.2996 |           0.2730 |
[32m[20221213 14:59:41 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:59:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 85.24
[32m[20221213 14:59:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 99.86
[32m[20221213 14:59:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.76
[32m[20221213 14:59:41 @agent_ppo2.py:143][0m Total time:       6.76 min
[32m[20221213 14:59:41 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221213 14:59:41 @agent_ppo2.py:121][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 14:59:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:41 @agent_ppo2.py:185][0m |          -0.0006 |           2.7211 |           0.2724 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0107 |           2.6566 |           0.2722 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0033 |           2.6934 |           0.2720 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0121 |           2.6169 |           0.2721 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0110 |           2.5946 |           0.2721 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0095 |           2.5801 |           0.2721 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0065 |           2.6049 |           0.2724 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |           0.0008 |           2.7525 |           0.2723 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0156 |           2.5617 |           0.2726 |
[32m[20221213 14:59:42 @agent_ppo2.py:185][0m |          -0.0168 |           2.5540 |           0.2724 |
[32m[20221213 14:59:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:59:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.07
[32m[20221213 14:59:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.34
[32m[20221213 14:59:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.30
[32m[20221213 14:59:42 @agent_ppo2.py:143][0m Total time:       6.79 min
[32m[20221213 14:59:42 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221213 14:59:42 @agent_ppo2.py:121][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 14:59:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:59:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0002 |           2.6303 |           0.2690 |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0086 |           2.5594 |           0.2684 |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0114 |           2.4895 |           0.2679 |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0110 |           2.4607 |           0.2678 |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0041 |           2.5270 |           0.2676 |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0127 |           2.4255 |           0.2674 |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0104 |           2.4371 |           0.2675 |
[32m[20221213 14:59:43 @agent_ppo2.py:185][0m |          -0.0185 |           2.3990 |           0.2676 |
[32m[20221213 14:59:44 @agent_ppo2.py:185][0m |          -0.0171 |           2.3861 |           0.2675 |
[32m[20221213 14:59:44 @agent_ppo2.py:185][0m |          -0.0155 |           2.3705 |           0.2676 |
[32m[20221213 14:59:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 14:59:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.82
[32m[20221213 14:59:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 102.07
[32m[20221213 14:59:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 80.27
[32m[20221213 14:59:44 @agent_ppo2.py:143][0m Total time:       6.81 min
[32m[20221213 14:59:44 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221213 14:59:44 @agent_ppo2.py:121][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 14:59:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:44 @agent_ppo2.py:185][0m |          -0.0029 |           2.8929 |           0.2744 |
[32m[20221213 14:59:44 @agent_ppo2.py:185][0m |          -0.0066 |           2.8363 |           0.2739 |
[32m[20221213 14:59:44 @agent_ppo2.py:185][0m |          -0.0079 |           2.8377 |           0.2734 |
[32m[20221213 14:59:44 @agent_ppo2.py:185][0m |          -0.0117 |           2.7947 |           0.2732 |
[32m[20221213 14:59:45 @agent_ppo2.py:185][0m |          -0.0125 |           2.7798 |           0.2731 |
[32m[20221213 14:59:45 @agent_ppo2.py:185][0m |          -0.0148 |           2.7581 |           0.2728 |
[32m[20221213 14:59:45 @agent_ppo2.py:185][0m |          -0.0177 |           2.7419 |           0.2726 |
[32m[20221213 14:59:45 @agent_ppo2.py:185][0m |          -0.0175 |           2.7321 |           0.2725 |
[32m[20221213 14:59:45 @agent_ppo2.py:185][0m |          -0.0198 |           2.7024 |           0.2723 |
[32m[20221213 14:59:45 @agent_ppo2.py:185][0m |          -0.0196 |           2.7005 |           0.2722 |
[32m[20221213 14:59:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 14:59:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.77
[32m[20221213 14:59:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.47
[32m[20221213 14:59:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.88
[32m[20221213 14:59:45 @agent_ppo2.py:143][0m Total time:       6.83 min
[32m[20221213 14:59:45 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221213 14:59:45 @agent_ppo2.py:121][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 14:59:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0025 |           2.7143 |           0.2627 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0058 |           2.6572 |           0.2624 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0130 |           2.6209 |           0.2621 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0125 |           2.6105 |           0.2622 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0133 |           2.6103 |           0.2621 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0107 |           2.6256 |           0.2622 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0093 |           2.6215 |           0.2620 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0186 |           2.5610 |           0.2622 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0174 |           2.5483 |           0.2622 |
[32m[20221213 14:59:46 @agent_ppo2.py:185][0m |          -0.0197 |           2.5320 |           0.2622 |
[32m[20221213 14:59:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 14:59:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 94.94
[32m[20221213 14:59:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.09
[32m[20221213 14:59:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.85
[32m[20221213 14:59:47 @agent_ppo2.py:143][0m Total time:       6.86 min
[32m[20221213 14:59:47 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221213 14:59:47 @agent_ppo2.py:121][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 14:59:47 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 14:59:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:47 @agent_ppo2.py:185][0m |          -0.0037 |           2.5936 |           0.2709 |
[32m[20221213 14:59:47 @agent_ppo2.py:185][0m |          -0.0077 |           2.5281 |           0.2708 |
[32m[20221213 14:59:47 @agent_ppo2.py:185][0m |          -0.0109 |           2.5006 |           0.2706 |
[32m[20221213 14:59:47 @agent_ppo2.py:185][0m |          -0.0128 |           2.4875 |           0.2708 |
[32m[20221213 14:59:47 @agent_ppo2.py:185][0m |          -0.0113 |           2.4518 |           0.2707 |
[32m[20221213 14:59:47 @agent_ppo2.py:185][0m |          -0.0145 |           2.4403 |           0.2708 |
[32m[20221213 14:59:47 @agent_ppo2.py:185][0m |          -0.0174 |           2.4180 |           0.2708 |
[32m[20221213 14:59:48 @agent_ppo2.py:185][0m |          -0.0159 |           2.4016 |           0.2709 |
[32m[20221213 14:59:48 @agent_ppo2.py:185][0m |          -0.0092 |           2.4381 |           0.2708 |
[32m[20221213 14:59:48 @agent_ppo2.py:185][0m |          -0.0140 |           2.4062 |           0.2708 |
[32m[20221213 14:59:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 98.17
[32m[20221213 14:59:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 105.80
[32m[20221213 14:59:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.51
[32m[20221213 14:59:48 @agent_ppo2.py:143][0m Total time:       6.88 min
[32m[20221213 14:59:48 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221213 14:59:48 @agent_ppo2.py:121][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 14:59:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:48 @agent_ppo2.py:185][0m |          -0.0031 |           2.8061 |           0.2757 |
[32m[20221213 14:59:48 @agent_ppo2.py:185][0m |          -0.0036 |           2.7396 |           0.2753 |
[32m[20221213 14:59:48 @agent_ppo2.py:185][0m |          -0.0110 |           2.6930 |           0.2753 |
[32m[20221213 14:59:49 @agent_ppo2.py:185][0m |          -0.0112 |           2.6708 |           0.2752 |
[32m[20221213 14:59:49 @agent_ppo2.py:185][0m |          -0.0107 |           2.6525 |           0.2749 |
[32m[20221213 14:59:49 @agent_ppo2.py:185][0m |          -0.0141 |           2.6444 |           0.2749 |
[32m[20221213 14:59:49 @agent_ppo2.py:185][0m |          -0.0133 |           2.6327 |           0.2748 |
[32m[20221213 14:59:49 @agent_ppo2.py:185][0m |          -0.0097 |           2.6611 |           0.2747 |
[32m[20221213 14:59:49 @agent_ppo2.py:185][0m |          -0.0149 |           2.6085 |           0.2748 |
[32m[20221213 14:59:49 @agent_ppo2.py:185][0m |          -0.0181 |           2.6060 |           0.2748 |
[32m[20221213 14:59:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.80
[32m[20221213 14:59:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 94.62
[32m[20221213 14:59:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 94.89
[32m[20221213 14:59:49 @agent_ppo2.py:143][0m Total time:       6.90 min
[32m[20221213 14:59:49 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221213 14:59:49 @agent_ppo2.py:121][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 14:59:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0019 |           2.8078 |           0.2793 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0050 |           2.7414 |           0.2790 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0053 |           2.7245 |           0.2787 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0099 |           2.6920 |           0.2785 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0003 |           2.9914 |           0.2784 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0093 |           2.7514 |           0.2784 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0002 |           3.0188 |           0.2783 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0165 |           2.6723 |           0.2781 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0153 |           2.6446 |           0.2780 |
[32m[20221213 14:59:50 @agent_ppo2.py:185][0m |          -0.0147 |           2.6297 |           0.2779 |
[32m[20221213 14:59:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.55
[32m[20221213 14:59:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.89
[32m[20221213 14:59:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.90
[32m[20221213 14:59:51 @agent_ppo2.py:143][0m Total time:       6.92 min
[32m[20221213 14:59:51 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221213 14:59:51 @agent_ppo2.py:121][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 14:59:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:51 @agent_ppo2.py:185][0m |          -0.0021 |           2.8166 |           0.2825 |
[32m[20221213 14:59:51 @agent_ppo2.py:185][0m |          -0.0020 |           2.7772 |           0.2821 |
[32m[20221213 14:59:51 @agent_ppo2.py:185][0m |          -0.0088 |           2.7033 |           0.2820 |
[32m[20221213 14:59:51 @agent_ppo2.py:185][0m |          -0.0113 |           2.6731 |           0.2817 |
[32m[20221213 14:59:51 @agent_ppo2.py:185][0m |          -0.0111 |           2.6561 |           0.2817 |
[32m[20221213 14:59:51 @agent_ppo2.py:185][0m |          -0.0108 |           2.6404 |           0.2817 |
[32m[20221213 14:59:51 @agent_ppo2.py:185][0m |          -0.0125 |           2.6179 |           0.2816 |
[32m[20221213 14:59:52 @agent_ppo2.py:185][0m |          -0.0137 |           2.6179 |           0.2815 |
[32m[20221213 14:59:52 @agent_ppo2.py:185][0m |          -0.0158 |           2.6179 |           0.2813 |
[32m[20221213 14:59:52 @agent_ppo2.py:185][0m |          -0.0164 |           2.5915 |           0.2810 |
[32m[20221213 14:59:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.05
[32m[20221213 14:59:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.78
[32m[20221213 14:59:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.36
[32m[20221213 14:59:52 @agent_ppo2.py:143][0m Total time:       6.95 min
[32m[20221213 14:59:52 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221213 14:59:52 @agent_ppo2.py:121][0m #------------------------ Iteration 304 --------------------------#
[32m[20221213 14:59:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:59:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:52 @agent_ppo2.py:185][0m |          -0.0016 |           2.7250 |           0.2653 |
[32m[20221213 14:59:52 @agent_ppo2.py:185][0m |          -0.0068 |           2.6497 |           0.2651 |
[32m[20221213 14:59:52 @agent_ppo2.py:185][0m |          -0.0065 |           2.6230 |           0.2648 |
[32m[20221213 14:59:53 @agent_ppo2.py:185][0m |           0.0010 |           2.8582 |           0.2646 |
[32m[20221213 14:59:53 @agent_ppo2.py:185][0m |          -0.0094 |           2.6272 |           0.2643 |
[32m[20221213 14:59:53 @agent_ppo2.py:185][0m |          -0.0159 |           2.5852 |           0.2643 |
[32m[20221213 14:59:53 @agent_ppo2.py:185][0m |          -0.0094 |           2.6240 |           0.2643 |
[32m[20221213 14:59:53 @agent_ppo2.py:185][0m |          -0.0115 |           2.5624 |           0.2641 |
[32m[20221213 14:59:53 @agent_ppo2.py:185][0m |          -0.0146 |           2.5548 |           0.2642 |
[32m[20221213 14:59:53 @agent_ppo2.py:185][0m |          -0.0102 |           2.6696 |           0.2640 |
[32m[20221213 14:59:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.60
[32m[20221213 14:59:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.21
[32m[20221213 14:59:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 106.20
[32m[20221213 14:59:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 106.20
[32m[20221213 14:59:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 106.20
[32m[20221213 14:59:53 @agent_ppo2.py:143][0m Total time:       6.97 min
[32m[20221213 14:59:53 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221213 14:59:53 @agent_ppo2.py:121][0m #------------------------ Iteration 305 --------------------------#
[32m[20221213 14:59:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |           0.0069 |           2.7291 |           0.2751 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0098 |           2.5230 |           0.2746 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0109 |           2.4964 |           0.2745 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0093 |           2.5080 |           0.2744 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0138 |           2.4701 |           0.2742 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0014 |           2.5779 |           0.2741 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0153 |           2.4354 |           0.2742 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0176 |           2.4199 |           0.2742 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0122 |           2.4311 |           0.2743 |
[32m[20221213 14:59:54 @agent_ppo2.py:185][0m |          -0.0136 |           2.4970 |           0.2742 |
[32m[20221213 14:59:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.84
[32m[20221213 14:59:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.11
[32m[20221213 14:59:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 99.74
[32m[20221213 14:59:55 @agent_ppo2.py:143][0m Total time:       6.99 min
[32m[20221213 14:59:55 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221213 14:59:55 @agent_ppo2.py:121][0m #------------------------ Iteration 306 --------------------------#
[32m[20221213 14:59:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:55 @agent_ppo2.py:185][0m |          -0.0005 |           2.5039 |           0.2781 |
[32m[20221213 14:59:55 @agent_ppo2.py:185][0m |          -0.0059 |           2.4235 |           0.2774 |
[32m[20221213 14:59:55 @agent_ppo2.py:185][0m |          -0.0054 |           2.4099 |           0.2773 |
[32m[20221213 14:59:55 @agent_ppo2.py:185][0m |          -0.0121 |           2.3855 |           0.2770 |
[32m[20221213 14:59:55 @agent_ppo2.py:185][0m |          -0.0113 |           2.3711 |           0.2765 |
[32m[20221213 14:59:55 @agent_ppo2.py:185][0m |          -0.0098 |           2.3572 |           0.2763 |
[32m[20221213 14:59:55 @agent_ppo2.py:185][0m |          -0.0139 |           2.3444 |           0.2762 |
[32m[20221213 14:59:56 @agent_ppo2.py:185][0m |          -0.0106 |           2.3288 |           0.2761 |
[32m[20221213 14:59:56 @agent_ppo2.py:185][0m |          -0.0119 |           2.3416 |           0.2760 |
[32m[20221213 14:59:56 @agent_ppo2.py:185][0m |          -0.0183 |           2.3169 |           0.2757 |
[32m[20221213 14:59:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.11
[32m[20221213 14:59:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.56
[32m[20221213 14:59:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 93.12
[32m[20221213 14:59:56 @agent_ppo2.py:143][0m Total time:       7.01 min
[32m[20221213 14:59:56 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221213 14:59:56 @agent_ppo2.py:121][0m #------------------------ Iteration 307 --------------------------#
[32m[20221213 14:59:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:56 @agent_ppo2.py:185][0m |           0.0092 |           2.8305 |           0.2685 |
[32m[20221213 14:59:56 @agent_ppo2.py:185][0m |          -0.0064 |           2.6093 |           0.2680 |
[32m[20221213 14:59:56 @agent_ppo2.py:185][0m |          -0.0107 |           2.5766 |           0.2677 |
[32m[20221213 14:59:57 @agent_ppo2.py:185][0m |          -0.0123 |           2.5556 |           0.2676 |
[32m[20221213 14:59:57 @agent_ppo2.py:185][0m |           0.0024 |           2.8433 |           0.2674 |
[32m[20221213 14:59:57 @agent_ppo2.py:185][0m |          -0.0111 |           2.5406 |           0.2670 |
[32m[20221213 14:59:57 @agent_ppo2.py:185][0m |          -0.0124 |           2.5258 |           0.2669 |
[32m[20221213 14:59:57 @agent_ppo2.py:185][0m |          -0.0151 |           2.5082 |           0.2668 |
[32m[20221213 14:59:57 @agent_ppo2.py:185][0m |          -0.0154 |           2.4961 |           0.2669 |
[32m[20221213 14:59:57 @agent_ppo2.py:185][0m |          -0.0075 |           2.7006 |           0.2668 |
[32m[20221213 14:59:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 86.38
[32m[20221213 14:59:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 102.65
[32m[20221213 14:59:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 97.67
[32m[20221213 14:59:57 @agent_ppo2.py:143][0m Total time:       7.03 min
[32m[20221213 14:59:57 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221213 14:59:57 @agent_ppo2.py:121][0m #------------------------ Iteration 308 --------------------------#
[32m[20221213 14:59:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 14:59:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0031 |           2.6679 |           0.2636 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0061 |           2.6040 |           0.2630 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0109 |           2.5864 |           0.2629 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0038 |           2.7178 |           0.2626 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0120 |           2.5562 |           0.2625 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0103 |           2.5640 |           0.2623 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0151 |           2.5303 |           0.2621 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0152 |           2.5295 |           0.2619 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0152 |           2.5167 |           0.2621 |
[32m[20221213 14:59:58 @agent_ppo2.py:185][0m |          -0.0152 |           2.5026 |           0.2619 |
[32m[20221213 14:59:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 14:59:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.48
[32m[20221213 14:59:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 105.42
[32m[20221213 14:59:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.12
[32m[20221213 14:59:59 @agent_ppo2.py:143][0m Total time:       7.06 min
[32m[20221213 14:59:59 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221213 14:59:59 @agent_ppo2.py:121][0m #------------------------ Iteration 309 --------------------------#
[32m[20221213 14:59:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 14:59:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 14:59:59 @agent_ppo2.py:185][0m |          -0.0033 |           2.7061 |           0.2716 |
[32m[20221213 14:59:59 @agent_ppo2.py:185][0m |          -0.0031 |           2.6421 |           0.2709 |
[32m[20221213 14:59:59 @agent_ppo2.py:185][0m |          -0.0077 |           2.6193 |           0.2707 |
[32m[20221213 14:59:59 @agent_ppo2.py:185][0m |          -0.0087 |           2.6038 |           0.2706 |
[32m[20221213 14:59:59 @agent_ppo2.py:185][0m |          -0.0126 |           2.5880 |           0.2706 |
[32m[20221213 14:59:59 @agent_ppo2.py:185][0m |          -0.0133 |           2.5780 |           0.2705 |
[32m[20221213 14:59:59 @agent_ppo2.py:185][0m |          -0.0127 |           2.5765 |           0.2704 |
[32m[20221213 15:00:00 @agent_ppo2.py:185][0m |          -0.0124 |           2.5561 |           0.2705 |
[32m[20221213 15:00:00 @agent_ppo2.py:185][0m |          -0.0156 |           2.5500 |           0.2704 |
[32m[20221213 15:00:00 @agent_ppo2.py:185][0m |          -0.0169 |           2.5472 |           0.2706 |
[32m[20221213 15:00:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.54
[32m[20221213 15:00:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 102.02
[32m[20221213 15:00:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.44
[32m[20221213 15:00:00 @agent_ppo2.py:143][0m Total time:       7.08 min
[32m[20221213 15:00:00 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221213 15:00:00 @agent_ppo2.py:121][0m #------------------------ Iteration 310 --------------------------#
[32m[20221213 15:00:00 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:00:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:00 @agent_ppo2.py:185][0m |          -0.0029 |           2.7109 |           0.2696 |
[32m[20221213 15:00:00 @agent_ppo2.py:185][0m |          -0.0101 |           2.6858 |           0.2690 |
[32m[20221213 15:00:00 @agent_ppo2.py:185][0m |          -0.0093 |           2.6518 |           0.2688 |
[32m[20221213 15:00:01 @agent_ppo2.py:185][0m |          -0.0111 |           2.6406 |           0.2683 |
[32m[20221213 15:00:01 @agent_ppo2.py:185][0m |          -0.0126 |           2.6272 |           0.2682 |
[32m[20221213 15:00:01 @agent_ppo2.py:185][0m |          -0.0124 |           2.6059 |           0.2678 |
[32m[20221213 15:00:01 @agent_ppo2.py:185][0m |          -0.0162 |           2.5929 |           0.2678 |
[32m[20221213 15:00:01 @agent_ppo2.py:185][0m |          -0.0156 |           2.5893 |           0.2677 |
[32m[20221213 15:00:01 @agent_ppo2.py:185][0m |          -0.0171 |           2.5794 |           0.2673 |
[32m[20221213 15:00:01 @agent_ppo2.py:185][0m |          -0.0173 |           2.5683 |           0.2672 |
[32m[20221213 15:00:01 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.53
[32m[20221213 15:00:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.09
[32m[20221213 15:00:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.32
[32m[20221213 15:00:01 @agent_ppo2.py:143][0m Total time:       7.10 min
[32m[20221213 15:00:01 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221213 15:00:01 @agent_ppo2.py:121][0m #------------------------ Iteration 311 --------------------------#
[32m[20221213 15:00:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |           0.0014 |           2.8186 |           0.2681 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0073 |           2.7366 |           0.2678 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |           0.0025 |           2.9799 |           0.2676 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0119 |           2.6859 |           0.2673 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0130 |           2.6600 |           0.2674 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0108 |           2.6586 |           0.2671 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0143 |           2.6244 |           0.2672 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0125 |           2.6217 |           0.2672 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0099 |           2.6512 |           0.2671 |
[32m[20221213 15:00:02 @agent_ppo2.py:185][0m |          -0.0164 |           2.6032 |           0.2671 |
[32m[20221213 15:00:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.14
[32m[20221213 15:00:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.49
[32m[20221213 15:00:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 66.74
[32m[20221213 15:00:03 @agent_ppo2.py:143][0m Total time:       7.12 min
[32m[20221213 15:00:03 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221213 15:00:03 @agent_ppo2.py:121][0m #------------------------ Iteration 312 --------------------------#
[32m[20221213 15:00:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:03 @agent_ppo2.py:185][0m |          -0.0008 |           2.6127 |           0.2610 |
[32m[20221213 15:00:03 @agent_ppo2.py:185][0m |          -0.0077 |           2.5131 |           0.2606 |
[32m[20221213 15:00:03 @agent_ppo2.py:185][0m |          -0.0088 |           2.4836 |           0.2603 |
[32m[20221213 15:00:03 @agent_ppo2.py:185][0m |          -0.0112 |           2.4657 |           0.2600 |
[32m[20221213 15:00:03 @agent_ppo2.py:185][0m |          -0.0133 |           2.4361 |           0.2598 |
[32m[20221213 15:00:03 @agent_ppo2.py:185][0m |          -0.0137 |           2.4316 |           0.2597 |
[32m[20221213 15:00:03 @agent_ppo2.py:185][0m |          -0.0139 |           2.4047 |           0.2593 |
[32m[20221213 15:00:04 @agent_ppo2.py:185][0m |          -0.0168 |           2.4041 |           0.2593 |
[32m[20221213 15:00:04 @agent_ppo2.py:185][0m |          -0.0191 |           2.3923 |           0.2592 |
[32m[20221213 15:00:04 @agent_ppo2.py:185][0m |          -0.0184 |           2.3744 |           0.2591 |
[32m[20221213 15:00:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 98.80
[32m[20221213 15:00:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.69
[32m[20221213 15:00:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 99.80
[32m[20221213 15:00:04 @agent_ppo2.py:143][0m Total time:       7.15 min
[32m[20221213 15:00:04 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221213 15:00:04 @agent_ppo2.py:121][0m #------------------------ Iteration 313 --------------------------#
[32m[20221213 15:00:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:04 @agent_ppo2.py:185][0m |          -0.0009 |           2.6684 |           0.2593 |
[32m[20221213 15:00:04 @agent_ppo2.py:185][0m |          -0.0090 |           2.5744 |           0.2591 |
[32m[20221213 15:00:04 @agent_ppo2.py:185][0m |          -0.0108 |           2.5201 |           0.2589 |
[32m[20221213 15:00:05 @agent_ppo2.py:185][0m |          -0.0134 |           2.5006 |           0.2589 |
[32m[20221213 15:00:05 @agent_ppo2.py:185][0m |          -0.0122 |           2.4673 |           0.2586 |
[32m[20221213 15:00:05 @agent_ppo2.py:185][0m |          -0.0160 |           2.4446 |           0.2585 |
[32m[20221213 15:00:05 @agent_ppo2.py:185][0m |           0.0036 |           2.8107 |           0.2585 |
[32m[20221213 15:00:05 @agent_ppo2.py:185][0m |          -0.0166 |           2.4178 |           0.2582 |
[32m[20221213 15:00:05 @agent_ppo2.py:185][0m |          -0.0205 |           2.3964 |           0.2582 |
[32m[20221213 15:00:05 @agent_ppo2.py:185][0m |          -0.0176 |           2.3740 |           0.2582 |
[32m[20221213 15:00:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:00:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.11
[32m[20221213 15:00:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.77
[32m[20221213 15:00:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.53
[32m[20221213 15:00:05 @agent_ppo2.py:143][0m Total time:       7.17 min
[32m[20221213 15:00:05 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221213 15:00:05 @agent_ppo2.py:121][0m #------------------------ Iteration 314 --------------------------#
[32m[20221213 15:00:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0012 |           2.6448 |           0.2639 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0088 |           2.5786 |           0.2640 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0110 |           2.5500 |           0.2639 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0131 |           2.5242 |           0.2641 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0125 |           2.5056 |           0.2640 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0141 |           2.4769 |           0.2640 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0152 |           2.4608 |           0.2639 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0195 |           2.4600 |           0.2641 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0159 |           2.4445 |           0.2643 |
[32m[20221213 15:00:06 @agent_ppo2.py:185][0m |          -0.0167 |           2.4306 |           0.2645 |
[32m[20221213 15:00:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.80
[32m[20221213 15:00:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.03
[32m[20221213 15:00:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 91.75
[32m[20221213 15:00:07 @agent_ppo2.py:143][0m Total time:       7.19 min
[32m[20221213 15:00:07 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221213 15:00:07 @agent_ppo2.py:121][0m #------------------------ Iteration 315 --------------------------#
[32m[20221213 15:00:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:07 @agent_ppo2.py:185][0m |          -0.0001 |           2.7969 |           0.2683 |
[32m[20221213 15:00:07 @agent_ppo2.py:185][0m |          -0.0030 |           2.7321 |           0.2679 |
[32m[20221213 15:00:07 @agent_ppo2.py:185][0m |          -0.0084 |           2.6859 |           0.2674 |
[32m[20221213 15:00:07 @agent_ppo2.py:185][0m |          -0.0126 |           2.6561 |           0.2673 |
[32m[20221213 15:00:07 @agent_ppo2.py:185][0m |          -0.0129 |           2.6394 |           0.2671 |
[32m[20221213 15:00:07 @agent_ppo2.py:185][0m |          -0.0131 |           2.6089 |           0.2671 |
[32m[20221213 15:00:08 @agent_ppo2.py:185][0m |          -0.0146 |           2.6149 |           0.2669 |
[32m[20221213 15:00:08 @agent_ppo2.py:185][0m |          -0.0158 |           2.5834 |           0.2669 |
[32m[20221213 15:00:08 @agent_ppo2.py:185][0m |          -0.0154 |           2.5703 |           0.2668 |
[32m[20221213 15:00:08 @agent_ppo2.py:185][0m |          -0.0156 |           2.5477 |           0.2668 |
[32m[20221213 15:00:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 100.69
[32m[20221213 15:00:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.10
[32m[20221213 15:00:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.95
[32m[20221213 15:00:08 @agent_ppo2.py:143][0m Total time:       7.21 min
[32m[20221213 15:00:08 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221213 15:00:08 @agent_ppo2.py:121][0m #------------------------ Iteration 316 --------------------------#
[32m[20221213 15:00:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:08 @agent_ppo2.py:185][0m |          -0.0006 |           2.9119 |           0.2692 |
[32m[20221213 15:00:08 @agent_ppo2.py:185][0m |          -0.0100 |           2.8103 |           0.2688 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0115 |           2.7635 |           0.2685 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0015 |           3.0430 |           0.2682 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0126 |           2.7098 |           0.2681 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0150 |           2.6633 |           0.2679 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0194 |           2.6297 |           0.2676 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0128 |           2.6076 |           0.2677 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0190 |           2.5749 |           0.2676 |
[32m[20221213 15:00:09 @agent_ppo2.py:185][0m |          -0.0200 |           2.5589 |           0.2674 |
[32m[20221213 15:00:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 89.04
[32m[20221213 15:00:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.68
[32m[20221213 15:00:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 92.86
[32m[20221213 15:00:09 @agent_ppo2.py:143][0m Total time:       7.24 min
[32m[20221213 15:00:09 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221213 15:00:09 @agent_ppo2.py:121][0m #------------------------ Iteration 317 --------------------------#
[32m[20221213 15:00:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |           0.0051 |           2.7683 |           0.2649 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0054 |           2.6323 |           0.2649 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0105 |           2.5742 |           0.2647 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0130 |           2.5392 |           0.2648 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0033 |           2.7340 |           0.2648 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0134 |           2.5077 |           0.2648 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0124 |           2.5034 |           0.2650 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0124 |           2.4664 |           0.2650 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0203 |           2.4520 |           0.2650 |
[32m[20221213 15:00:10 @agent_ppo2.py:185][0m |          -0.0076 |           2.5379 |           0.2650 |
[32m[20221213 15:00:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.50
[32m[20221213 15:00:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 102.04
[32m[20221213 15:00:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 79.05
[32m[20221213 15:00:11 @agent_ppo2.py:143][0m Total time:       7.26 min
[32m[20221213 15:00:11 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221213 15:00:11 @agent_ppo2.py:121][0m #------------------------ Iteration 318 --------------------------#
[32m[20221213 15:00:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:11 @agent_ppo2.py:185][0m |          -0.0064 |           2.7322 |           0.2633 |
[32m[20221213 15:00:11 @agent_ppo2.py:185][0m |          -0.0063 |           2.6885 |           0.2630 |
[32m[20221213 15:00:11 @agent_ppo2.py:185][0m |          -0.0019 |           2.6963 |           0.2631 |
[32m[20221213 15:00:11 @agent_ppo2.py:185][0m |          -0.0087 |           2.6469 |           0.2629 |
[32m[20221213 15:00:11 @agent_ppo2.py:185][0m |          -0.0007 |           2.7847 |           0.2626 |
[32m[20221213 15:00:11 @agent_ppo2.py:185][0m |          -0.0111 |           2.6146 |           0.2625 |
[32m[20221213 15:00:12 @agent_ppo2.py:185][0m |          -0.0125 |           2.6045 |           0.2623 |
[32m[20221213 15:00:12 @agent_ppo2.py:185][0m |          -0.0161 |           2.5975 |           0.2623 |
[32m[20221213 15:00:12 @agent_ppo2.py:185][0m |          -0.0143 |           2.5891 |           0.2622 |
[32m[20221213 15:00:12 @agent_ppo2.py:185][0m |          -0.0144 |           2.5785 |           0.2621 |
[32m[20221213 15:00:12 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 90.41
[32m[20221213 15:00:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.36
[32m[20221213 15:00:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.05
[32m[20221213 15:00:12 @agent_ppo2.py:143][0m Total time:       7.28 min
[32m[20221213 15:00:12 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221213 15:00:12 @agent_ppo2.py:121][0m #------------------------ Iteration 319 --------------------------#
[32m[20221213 15:00:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:12 @agent_ppo2.py:185][0m |          -0.0011 |           2.8881 |           0.2703 |
[32m[20221213 15:00:12 @agent_ppo2.py:185][0m |          -0.0091 |           2.7952 |           0.2697 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0115 |           2.7415 |           0.2690 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0157 |           2.7040 |           0.2685 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0191 |           2.6776 |           0.2680 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0190 |           2.6520 |           0.2677 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0139 |           2.6310 |           0.2673 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0155 |           2.6094 |           0.2670 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0144 |           2.5930 |           0.2667 |
[32m[20221213 15:00:13 @agent_ppo2.py:185][0m |          -0.0113 |           2.6839 |           0.2665 |
[32m[20221213 15:00:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 94.29
[32m[20221213 15:00:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.27
[32m[20221213 15:00:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.25
[32m[20221213 15:00:13 @agent_ppo2.py:143][0m Total time:       7.30 min
[32m[20221213 15:00:13 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221213 15:00:13 @agent_ppo2.py:121][0m #------------------------ Iteration 320 --------------------------#
[32m[20221213 15:00:14 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:00:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0004 |           3.1121 |           0.2526 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0057 |           3.0057 |           0.2525 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0031 |           2.9802 |           0.2526 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0076 |           2.9200 |           0.2526 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0123 |           2.8975 |           0.2524 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0123 |           2.8690 |           0.2523 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0118 |           2.8556 |           0.2523 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0105 |           2.8420 |           0.2522 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0106 |           2.8836 |           0.2520 |
[32m[20221213 15:00:14 @agent_ppo2.py:185][0m |          -0.0115 |           2.8470 |           0.2520 |
[32m[20221213 15:00:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 100.90
[32m[20221213 15:00:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.04
[32m[20221213 15:00:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 99.88
[32m[20221213 15:00:15 @agent_ppo2.py:143][0m Total time:       7.32 min
[32m[20221213 15:00:15 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221213 15:00:15 @agent_ppo2.py:121][0m #------------------------ Iteration 321 --------------------------#
[32m[20221213 15:00:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:15 @agent_ppo2.py:185][0m |          -0.0038 |           2.7305 |           0.2552 |
[32m[20221213 15:00:15 @agent_ppo2.py:185][0m |          -0.0100 |           2.6277 |           0.2552 |
[32m[20221213 15:00:15 @agent_ppo2.py:185][0m |          -0.0021 |           2.6547 |           0.2552 |
[32m[20221213 15:00:15 @agent_ppo2.py:185][0m |          -0.0112 |           2.5440 |           0.2552 |
[32m[20221213 15:00:15 @agent_ppo2.py:185][0m |          -0.0115 |           2.5210 |           0.2554 |
[32m[20221213 15:00:15 @agent_ppo2.py:185][0m |          -0.0106 |           2.5317 |           0.2554 |
[32m[20221213 15:00:16 @agent_ppo2.py:185][0m |          -0.0172 |           2.4909 |           0.2553 |
[32m[20221213 15:00:16 @agent_ppo2.py:185][0m |          -0.0166 |           2.4776 |           0.2554 |
[32m[20221213 15:00:16 @agent_ppo2.py:185][0m |          -0.0169 |           2.4632 |           0.2555 |
[32m[20221213 15:00:16 @agent_ppo2.py:185][0m |          -0.0176 |           2.4442 |           0.2557 |
[32m[20221213 15:00:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.00
[32m[20221213 15:00:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.96
[32m[20221213 15:00:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.69
[32m[20221213 15:00:16 @agent_ppo2.py:143][0m Total time:       7.35 min
[32m[20221213 15:00:16 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221213 15:00:16 @agent_ppo2.py:121][0m #------------------------ Iteration 322 --------------------------#
[32m[20221213 15:00:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:16 @agent_ppo2.py:185][0m |          -0.0012 |           2.6984 |           0.2557 |
[32m[20221213 15:00:16 @agent_ppo2.py:185][0m |           0.0067 |           2.8658 |           0.2552 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0086 |           2.5480 |           0.2549 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0127 |           2.5254 |           0.2545 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0023 |           2.6281 |           0.2543 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0122 |           2.5017 |           0.2539 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0032 |           2.6629 |           0.2537 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0123 |           2.4756 |           0.2535 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0091 |           2.5668 |           0.2534 |
[32m[20221213 15:00:17 @agent_ppo2.py:185][0m |          -0.0153 |           2.4482 |           0.2530 |
[32m[20221213 15:00:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 82.88
[32m[20221213 15:00:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.34
[32m[20221213 15:00:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 92.86
[32m[20221213 15:00:17 @agent_ppo2.py:143][0m Total time:       7.37 min
[32m[20221213 15:00:17 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221213 15:00:17 @agent_ppo2.py:121][0m #------------------------ Iteration 323 --------------------------#
[32m[20221213 15:00:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0024 |           2.5966 |           0.2604 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0069 |           2.5289 |           0.2598 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0097 |           2.4924 |           0.2594 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0133 |           2.4727 |           0.2591 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0010 |           2.7910 |           0.2586 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0085 |           2.4694 |           0.2583 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0076 |           2.6038 |           0.2582 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0175 |           2.4296 |           0.2580 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0149 |           2.4008 |           0.2580 |
[32m[20221213 15:00:18 @agent_ppo2.py:185][0m |          -0.0086 |           2.5117 |           0.2578 |
[32m[20221213 15:00:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.19
[32m[20221213 15:00:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 108.19
[32m[20221213 15:00:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.48
[32m[20221213 15:00:19 @agent_ppo2.py:143][0m Total time:       7.39 min
[32m[20221213 15:00:19 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221213 15:00:19 @agent_ppo2.py:121][0m #------------------------ Iteration 324 --------------------------#
[32m[20221213 15:00:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:19 @agent_ppo2.py:185][0m |          -0.0035 |           2.8207 |           0.2569 |
[32m[20221213 15:00:19 @agent_ppo2.py:185][0m |          -0.0104 |           2.7566 |           0.2566 |
[32m[20221213 15:00:19 @agent_ppo2.py:185][0m |          -0.0084 |           2.7366 |           0.2562 |
[32m[20221213 15:00:19 @agent_ppo2.py:185][0m |          -0.0106 |           2.7258 |           0.2563 |
[32m[20221213 15:00:19 @agent_ppo2.py:185][0m |          -0.0123 |           2.6979 |           0.2563 |
[32m[20221213 15:00:19 @agent_ppo2.py:185][0m |          -0.0132 |           2.6897 |           0.2563 |
[32m[20221213 15:00:20 @agent_ppo2.py:185][0m |          -0.0118 |           2.6908 |           0.2563 |
[32m[20221213 15:00:20 @agent_ppo2.py:185][0m |          -0.0133 |           2.6753 |           0.2563 |
[32m[20221213 15:00:20 @agent_ppo2.py:185][0m |          -0.0169 |           2.6699 |           0.2562 |
[32m[20221213 15:00:20 @agent_ppo2.py:185][0m |          -0.0154 |           2.6605 |           0.2561 |
[32m[20221213 15:00:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.27
[32m[20221213 15:00:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.72
[32m[20221213 15:00:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.48
[32m[20221213 15:00:20 @agent_ppo2.py:143][0m Total time:       7.41 min
[32m[20221213 15:00:20 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221213 15:00:20 @agent_ppo2.py:121][0m #------------------------ Iteration 325 --------------------------#
[32m[20221213 15:00:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:00:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:20 @agent_ppo2.py:185][0m |          -0.0025 |           2.7700 |           0.2552 |
[32m[20221213 15:00:20 @agent_ppo2.py:185][0m |          -0.0089 |           2.6779 |           0.2547 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0048 |           2.6428 |           0.2547 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0125 |           2.5849 |           0.2546 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0127 |           2.5649 |           0.2546 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0104 |           2.5595 |           0.2548 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0152 |           2.5235 |           0.2548 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0179 |           2.5206 |           0.2551 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0185 |           2.4949 |           0.2552 |
[32m[20221213 15:00:21 @agent_ppo2.py:185][0m |          -0.0170 |           2.4800 |           0.2552 |
[32m[20221213 15:00:21 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 84.77
[32m[20221213 15:00:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 96.96
[32m[20221213 15:00:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.86
[32m[20221213 15:00:21 @agent_ppo2.py:143][0m Total time:       7.44 min
[32m[20221213 15:00:21 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221213 15:00:21 @agent_ppo2.py:121][0m #------------------------ Iteration 326 --------------------------#
[32m[20221213 15:00:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0008 |           2.9062 |           0.2545 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0049 |           2.8216 |           0.2543 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0079 |           2.7977 |           0.2540 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0073 |           2.7922 |           0.2541 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0141 |           2.7755 |           0.2542 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0131 |           2.7599 |           0.2542 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0128 |           2.7811 |           0.2541 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0101 |           2.7401 |           0.2542 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0181 |           2.7387 |           0.2543 |
[32m[20221213 15:00:22 @agent_ppo2.py:185][0m |          -0.0167 |           2.7303 |           0.2543 |
[32m[20221213 15:00:22 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 98.78
[32m[20221213 15:00:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.06
[32m[20221213 15:00:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.98
[32m[20221213 15:00:23 @agent_ppo2.py:143][0m Total time:       7.46 min
[32m[20221213 15:00:23 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221213 15:00:23 @agent_ppo2.py:121][0m #------------------------ Iteration 327 --------------------------#
[32m[20221213 15:00:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:23 @agent_ppo2.py:185][0m |          -0.0039 |           2.8328 |           0.2554 |
[32m[20221213 15:00:23 @agent_ppo2.py:185][0m |          -0.0060 |           2.7669 |           0.2554 |
[32m[20221213 15:00:23 @agent_ppo2.py:185][0m |          -0.0047 |           2.8428 |           0.2553 |
[32m[20221213 15:00:23 @agent_ppo2.py:185][0m |          -0.0123 |           2.7332 |           0.2553 |
[32m[20221213 15:00:23 @agent_ppo2.py:185][0m |          -0.0149 |           2.7175 |           0.2557 |
[32m[20221213 15:00:23 @agent_ppo2.py:185][0m |          -0.0114 |           2.8239 |           0.2559 |
[32m[20221213 15:00:24 @agent_ppo2.py:185][0m |          -0.0204 |           2.7006 |           0.2561 |
[32m[20221213 15:00:24 @agent_ppo2.py:185][0m |          -0.0195 |           2.6880 |           0.2562 |
[32m[20221213 15:00:24 @agent_ppo2.py:185][0m |          -0.0176 |           2.6713 |           0.2563 |
[32m[20221213 15:00:24 @agent_ppo2.py:185][0m |          -0.0178 |           2.6624 |           0.2565 |
[32m[20221213 15:00:24 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.29
[32m[20221213 15:00:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.71
[32m[20221213 15:00:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.27
[32m[20221213 15:00:24 @agent_ppo2.py:143][0m Total time:       7.48 min
[32m[20221213 15:00:24 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221213 15:00:24 @agent_ppo2.py:121][0m #------------------------ Iteration 328 --------------------------#
[32m[20221213 15:00:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:24 @agent_ppo2.py:185][0m |          -0.0019 |           2.7455 |           0.2582 |
[32m[20221213 15:00:24 @agent_ppo2.py:185][0m |          -0.0058 |           2.7179 |           0.2577 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0078 |           2.6867 |           0.2576 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0122 |           2.6778 |           0.2576 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0047 |           2.7712 |           0.2576 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0138 |           2.6454 |           0.2575 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0148 |           2.6417 |           0.2574 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0193 |           2.6266 |           0.2573 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0129 |           2.6095 |           0.2575 |
[32m[20221213 15:00:25 @agent_ppo2.py:185][0m |          -0.0200 |           2.6097 |           0.2575 |
[32m[20221213 15:00:25 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.49
[32m[20221213 15:00:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.69
[32m[20221213 15:00:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.84
[32m[20221213 15:00:25 @agent_ppo2.py:143][0m Total time:       7.50 min
[32m[20221213 15:00:25 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221213 15:00:25 @agent_ppo2.py:121][0m #------------------------ Iteration 329 --------------------------#
[32m[20221213 15:00:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0017 |           2.7828 |           0.2590 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0054 |           2.7539 |           0.2586 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0087 |           2.7437 |           0.2583 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0057 |           2.7427 |           0.2579 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0133 |           2.7057 |           0.2579 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0090 |           2.7256 |           0.2576 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0153 |           2.6856 |           0.2577 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0141 |           2.6777 |           0.2576 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0117 |           2.6989 |           0.2575 |
[32m[20221213 15:00:26 @agent_ppo2.py:185][0m |          -0.0164 |           2.6588 |           0.2573 |
[32m[20221213 15:00:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:00:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.28
[32m[20221213 15:00:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.20
[32m[20221213 15:00:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.14
[32m[20221213 15:00:27 @agent_ppo2.py:143][0m Total time:       7.52 min
[32m[20221213 15:00:27 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221213 15:00:27 @agent_ppo2.py:121][0m #------------------------ Iteration 330 --------------------------#
[32m[20221213 15:00:27 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:00:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:27 @agent_ppo2.py:185][0m |          -0.0008 |           2.6868 |           0.2596 |
[32m[20221213 15:00:27 @agent_ppo2.py:185][0m |          -0.0063 |           2.6167 |           0.2596 |
[32m[20221213 15:00:27 @agent_ppo2.py:185][0m |          -0.0089 |           2.5837 |           0.2593 |
[32m[20221213 15:00:27 @agent_ppo2.py:185][0m |          -0.0090 |           2.5692 |           0.2591 |
[32m[20221213 15:00:27 @agent_ppo2.py:185][0m |          -0.0136 |           2.5564 |           0.2588 |
[32m[20221213 15:00:27 @agent_ppo2.py:185][0m |          -0.0161 |           2.5343 |           0.2586 |
[32m[20221213 15:00:28 @agent_ppo2.py:185][0m |          -0.0134 |           2.5224 |           0.2585 |
[32m[20221213 15:00:28 @agent_ppo2.py:185][0m |          -0.0153 |           2.5143 |           0.2582 |
[32m[20221213 15:00:28 @agent_ppo2.py:185][0m |          -0.0155 |           2.4998 |           0.2581 |
[32m[20221213 15:00:28 @agent_ppo2.py:185][0m |          -0.0047 |           2.8271 |           0.2579 |
[32m[20221213 15:00:28 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.26
[32m[20221213 15:00:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.90
[32m[20221213 15:00:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 77.78
[32m[20221213 15:00:28 @agent_ppo2.py:143][0m Total time:       7.55 min
[32m[20221213 15:00:28 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221213 15:00:28 @agent_ppo2.py:121][0m #------------------------ Iteration 331 --------------------------#
[32m[20221213 15:00:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:28 @agent_ppo2.py:185][0m |           0.0038 |           2.6807 |           0.2630 |
[32m[20221213 15:00:28 @agent_ppo2.py:185][0m |          -0.0075 |           2.5669 |           0.2627 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0075 |           2.5299 |           0.2625 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0111 |           2.5003 |           0.2625 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0072 |           2.5297 |           0.2624 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0119 |           2.4570 |           0.2624 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0117 |           2.4547 |           0.2623 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0136 |           2.4385 |           0.2622 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0137 |           2.4227 |           0.2622 |
[32m[20221213 15:00:29 @agent_ppo2.py:185][0m |          -0.0155 |           2.4071 |           0.2620 |
[32m[20221213 15:00:29 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:00:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 83.64
[32m[20221213 15:00:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 92.60
[32m[20221213 15:00:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.86
[32m[20221213 15:00:29 @agent_ppo2.py:143][0m Total time:       7.57 min
[32m[20221213 15:00:29 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221213 15:00:29 @agent_ppo2.py:121][0m #------------------------ Iteration 332 --------------------------#
[32m[20221213 15:00:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |           0.0012 |           2.5768 |           0.2591 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0068 |           2.4839 |           0.2587 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0125 |           2.4503 |           0.2584 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0086 |           2.4216 |           0.2582 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0079 |           2.4151 |           0.2582 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0152 |           2.3973 |           0.2582 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0088 |           2.4404 |           0.2582 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0163 |           2.3770 |           0.2582 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0117 |           2.3576 |           0.2582 |
[32m[20221213 15:00:30 @agent_ppo2.py:185][0m |          -0.0173 |           2.3467 |           0.2583 |
[32m[20221213 15:00:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 87.39
[32m[20221213 15:00:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.51
[32m[20221213 15:00:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 94.33
[32m[20221213 15:00:31 @agent_ppo2.py:143][0m Total time:       7.59 min
[32m[20221213 15:00:31 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221213 15:00:31 @agent_ppo2.py:121][0m #------------------------ Iteration 333 --------------------------#
[32m[20221213 15:00:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:31 @agent_ppo2.py:185][0m |           0.0028 |           2.9320 |           0.2577 |
[32m[20221213 15:00:31 @agent_ppo2.py:185][0m |          -0.0081 |           2.8790 |           0.2573 |
[32m[20221213 15:00:31 @agent_ppo2.py:185][0m |          -0.0104 |           2.8429 |           0.2570 |
[32m[20221213 15:00:31 @agent_ppo2.py:185][0m |          -0.0104 |           2.8267 |           0.2565 |
[32m[20221213 15:00:31 @agent_ppo2.py:185][0m |          -0.0085 |           2.8259 |           0.2566 |
[32m[20221213 15:00:31 @agent_ppo2.py:185][0m |          -0.0153 |           2.8146 |           0.2565 |
[32m[20221213 15:00:32 @agent_ppo2.py:185][0m |          -0.0133 |           2.8002 |           0.2563 |
[32m[20221213 15:00:32 @agent_ppo2.py:185][0m |          -0.0144 |           2.7806 |           0.2562 |
[32m[20221213 15:00:32 @agent_ppo2.py:185][0m |          -0.0092 |           2.8017 |           0.2560 |
[32m[20221213 15:00:32 @agent_ppo2.py:185][0m |          -0.0126 |           2.7987 |           0.2561 |
[32m[20221213 15:00:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:00:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.91
[32m[20221213 15:00:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 102.18
[32m[20221213 15:00:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.80
[32m[20221213 15:00:32 @agent_ppo2.py:143][0m Total time:       7.61 min
[32m[20221213 15:00:32 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221213 15:00:32 @agent_ppo2.py:121][0m #------------------------ Iteration 334 --------------------------#
[32m[20221213 15:00:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:32 @agent_ppo2.py:185][0m |          -0.0018 |           2.8599 |           0.2475 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0078 |           2.8194 |           0.2466 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0094 |           2.7862 |           0.2470 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0076 |           2.8269 |           0.2468 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0061 |           2.8816 |           0.2467 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0092 |           2.7518 |           0.2465 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0122 |           2.7375 |           0.2465 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0037 |           2.9139 |           0.2465 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0110 |           2.7655 |           0.2462 |
[32m[20221213 15:00:33 @agent_ppo2.py:185][0m |          -0.0158 |           2.6867 |           0.2465 |
[32m[20221213 15:00:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.33
[32m[20221213 15:00:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.80
[32m[20221213 15:00:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 94.33
[32m[20221213 15:00:33 @agent_ppo2.py:143][0m Total time:       7.64 min
[32m[20221213 15:00:33 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221213 15:00:33 @agent_ppo2.py:121][0m #------------------------ Iteration 335 --------------------------#
[32m[20221213 15:00:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |           0.0062 |           2.8939 |           0.2601 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |          -0.0101 |           2.6791 |           0.2601 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |          -0.0111 |           2.6443 |           0.2600 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |          -0.0137 |           2.6253 |           0.2601 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |          -0.0135 |           2.6064 |           0.2601 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |          -0.0153 |           2.5975 |           0.2601 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |           0.0021 |           2.8868 |           0.2602 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |          -0.0120 |           2.5918 |           0.2600 |
[32m[20221213 15:00:34 @agent_ppo2.py:185][0m |          -0.0139 |           2.5480 |           0.2602 |
[32m[20221213 15:00:35 @agent_ppo2.py:185][0m |          -0.0173 |           2.5422 |           0.2602 |
[32m[20221213 15:00:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:00:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.52
[32m[20221213 15:00:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.86
[32m[20221213 15:00:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.06
[32m[20221213 15:00:35 @agent_ppo2.py:143][0m Total time:       7.66 min
[32m[20221213 15:00:35 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221213 15:00:35 @agent_ppo2.py:121][0m #------------------------ Iteration 336 --------------------------#
[32m[20221213 15:00:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:35 @agent_ppo2.py:185][0m |          -0.0003 |           2.8241 |           0.2581 |
[32m[20221213 15:00:35 @agent_ppo2.py:185][0m |          -0.0049 |           2.7532 |           0.2581 |
[32m[20221213 15:00:35 @agent_ppo2.py:185][0m |           0.0062 |           2.8986 |           0.2576 |
[32m[20221213 15:00:35 @agent_ppo2.py:185][0m |          -0.0100 |           2.7454 |           0.2573 |
[32m[20221213 15:00:35 @agent_ppo2.py:185][0m |          -0.0130 |           2.7027 |           0.2573 |
[32m[20221213 15:00:36 @agent_ppo2.py:185][0m |          -0.0068 |           2.7921 |           0.2570 |
[32m[20221213 15:00:36 @agent_ppo2.py:185][0m |          -0.0086 |           2.7450 |           0.2568 |
[32m[20221213 15:00:36 @agent_ppo2.py:185][0m |          -0.0140 |           2.6752 |           0.2569 |
[32m[20221213 15:00:36 @agent_ppo2.py:185][0m |           0.0005 |           2.8669 |           0.2567 |
[32m[20221213 15:00:36 @agent_ppo2.py:185][0m |          -0.0189 |           2.6750 |           0.2565 |
[32m[20221213 15:00:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.95
[32m[20221213 15:00:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.58
[32m[20221213 15:00:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.19
[32m[20221213 15:00:36 @agent_ppo2.py:143][0m Total time:       7.68 min
[32m[20221213 15:00:36 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221213 15:00:36 @agent_ppo2.py:121][0m #------------------------ Iteration 337 --------------------------#
[32m[20221213 15:00:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:36 @agent_ppo2.py:185][0m |           0.0005 |           2.9407 |           0.2554 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0044 |           2.8834 |           0.2552 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0061 |           2.8495 |           0.2550 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0070 |           2.8249 |           0.2549 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0075 |           2.8280 |           0.2548 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0082 |           2.8168 |           0.2549 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0105 |           2.7975 |           0.2550 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0143 |           2.7784 |           0.2552 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0155 |           2.7713 |           0.2551 |
[32m[20221213 15:00:37 @agent_ppo2.py:185][0m |          -0.0146 |           2.7575 |           0.2550 |
[32m[20221213 15:00:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.68
[32m[20221213 15:00:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.55
[32m[20221213 15:00:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.08
[32m[20221213 15:00:37 @agent_ppo2.py:143][0m Total time:       7.70 min
[32m[20221213 15:00:37 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221213 15:00:37 @agent_ppo2.py:121][0m #------------------------ Iteration 338 --------------------------#
[32m[20221213 15:00:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0027 |           2.6237 |           0.2599 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0041 |           2.5112 |           0.2596 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0123 |           2.4336 |           0.2592 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0124 |           2.3936 |           0.2588 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0140 |           2.3703 |           0.2585 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0178 |           2.3431 |           0.2583 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0111 |           2.3703 |           0.2580 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0161 |           2.3103 |           0.2578 |
[32m[20221213 15:00:38 @agent_ppo2.py:185][0m |          -0.0163 |           2.2908 |           0.2578 |
[32m[20221213 15:00:39 @agent_ppo2.py:185][0m |          -0.0181 |           2.2763 |           0.2573 |
[32m[20221213 15:00:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:00:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.11
[32m[20221213 15:00:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 97.76
[32m[20221213 15:00:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.00
[32m[20221213 15:00:39 @agent_ppo2.py:143][0m Total time:       7.73 min
[32m[20221213 15:00:39 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221213 15:00:39 @agent_ppo2.py:121][0m #------------------------ Iteration 339 --------------------------#
[32m[20221213 15:00:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:39 @agent_ppo2.py:185][0m |          -0.0007 |           2.8958 |           0.2484 |
[32m[20221213 15:00:39 @agent_ppo2.py:185][0m |          -0.0048 |           2.8079 |           0.2480 |
[32m[20221213 15:00:39 @agent_ppo2.py:185][0m |          -0.0081 |           2.7868 |           0.2479 |
[32m[20221213 15:00:39 @agent_ppo2.py:185][0m |          -0.0033 |           2.9582 |           0.2480 |
[32m[20221213 15:00:39 @agent_ppo2.py:185][0m |          -0.0112 |           2.7595 |           0.2478 |
[32m[20221213 15:00:40 @agent_ppo2.py:185][0m |          -0.0060 |           2.8700 |           0.2477 |
[32m[20221213 15:00:40 @agent_ppo2.py:185][0m |          -0.0145 |           2.7200 |           0.2477 |
[32m[20221213 15:00:40 @agent_ppo2.py:185][0m |          -0.0103 |           2.7632 |           0.2477 |
[32m[20221213 15:00:40 @agent_ppo2.py:185][0m |          -0.0156 |           2.6966 |           0.2476 |
[32m[20221213 15:00:40 @agent_ppo2.py:185][0m |          -0.0168 |           2.6874 |           0.2475 |
[32m[20221213 15:00:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:00:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.86
[32m[20221213 15:00:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 111.36
[32m[20221213 15:00:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.28
[32m[20221213 15:00:40 @agent_ppo2.py:143][0m Total time:       7.75 min
[32m[20221213 15:00:40 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221213 15:00:40 @agent_ppo2.py:121][0m #------------------------ Iteration 340 --------------------------#
[32m[20221213 15:00:40 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:00:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |           0.0005 |           2.8126 |           0.2489 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |           0.0003 |           2.7600 |           0.2488 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |          -0.0004 |           2.7986 |           0.2484 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |           0.0046 |           2.9288 |           0.2482 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |          -0.0094 |           2.6924 |           0.2478 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |          -0.0113 |           2.6779 |           0.2479 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |          -0.0126 |           2.6591 |           0.2477 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |          -0.0140 |           2.6344 |           0.2476 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |          -0.0062 |           2.7091 |           0.2476 |
[32m[20221213 15:00:41 @agent_ppo2.py:185][0m |          -0.0124 |           2.6307 |           0.2474 |
[32m[20221213 15:00:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:00:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.21
[32m[20221213 15:00:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.03
[32m[20221213 15:00:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 73.95
[32m[20221213 15:00:42 @agent_ppo2.py:143][0m Total time:       7.77 min
[32m[20221213 15:00:42 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221213 15:00:42 @agent_ppo2.py:121][0m #------------------------ Iteration 341 --------------------------#
[32m[20221213 15:00:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:42 @agent_ppo2.py:185][0m |          -0.0005 |           2.8060 |           0.2551 |
[32m[20221213 15:00:42 @agent_ppo2.py:185][0m |          -0.0030 |           2.7519 |           0.2549 |
[32m[20221213 15:00:42 @agent_ppo2.py:185][0m |          -0.0112 |           2.7307 |           0.2549 |
[32m[20221213 15:00:42 @agent_ppo2.py:185][0m |          -0.0148 |           2.7088 |           0.2548 |
[32m[20221213 15:00:42 @agent_ppo2.py:185][0m |          -0.0141 |           2.6968 |           0.2547 |
[32m[20221213 15:00:42 @agent_ppo2.py:185][0m |          -0.0093 |           2.6797 |           0.2547 |
[32m[20221213 15:00:42 @agent_ppo2.py:185][0m |          -0.0155 |           2.6535 |           0.2547 |
[32m[20221213 15:00:43 @agent_ppo2.py:185][0m |          -0.0073 |           2.8018 |           0.2546 |
[32m[20221213 15:00:43 @agent_ppo2.py:185][0m |          -0.0151 |           2.6437 |           0.2546 |
[32m[20221213 15:00:43 @agent_ppo2.py:185][0m |          -0.0180 |           2.6109 |           0.2547 |
[32m[20221213 15:00:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:00:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 88.17
[32m[20221213 15:00:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.87
[32m[20221213 15:00:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.72
[32m[20221213 15:00:43 @agent_ppo2.py:143][0m Total time:       7.79 min
[32m[20221213 15:00:43 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221213 15:00:43 @agent_ppo2.py:121][0m #------------------------ Iteration 342 --------------------------#
[32m[20221213 15:00:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:43 @agent_ppo2.py:185][0m |           0.0037 |           2.8584 |           0.2469 |
[32m[20221213 15:00:43 @agent_ppo2.py:185][0m |          -0.0035 |           2.7570 |           0.2466 |
[32m[20221213 15:00:43 @agent_ppo2.py:185][0m |          -0.0068 |           2.6921 |           0.2465 |
[32m[20221213 15:00:44 @agent_ppo2.py:185][0m |          -0.0104 |           2.6674 |           0.2464 |
[32m[20221213 15:00:44 @agent_ppo2.py:185][0m |          -0.0111 |           2.6620 |           0.2463 |
[32m[20221213 15:00:44 @agent_ppo2.py:185][0m |          -0.0118 |           2.6401 |           0.2460 |
[32m[20221213 15:00:44 @agent_ppo2.py:185][0m |          -0.0079 |           2.7116 |           0.2458 |
[32m[20221213 15:00:44 @agent_ppo2.py:185][0m |          -0.0141 |           2.6267 |           0.2456 |
[32m[20221213 15:00:44 @agent_ppo2.py:185][0m |          -0.0140 |           2.6034 |           0.2455 |
[32m[20221213 15:00:44 @agent_ppo2.py:185][0m |          -0.0095 |           2.6019 |           0.2455 |
[32m[20221213 15:00:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:00:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 98.44
[32m[20221213 15:00:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.08
[32m[20221213 15:00:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.79
[32m[20221213 15:00:44 @agent_ppo2.py:143][0m Total time:       7.82 min
[32m[20221213 15:00:44 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221213 15:00:44 @agent_ppo2.py:121][0m #------------------------ Iteration 343 --------------------------#
[32m[20221213 15:00:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0042 |           2.9029 |           0.2514 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0082 |           2.8403 |           0.2508 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0058 |           2.8777 |           0.2508 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0104 |           2.7928 |           0.2506 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0109 |           2.7610 |           0.2508 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0124 |           2.7291 |           0.2507 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0153 |           2.7181 |           0.2509 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0017 |           2.9811 |           0.2508 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0137 |           2.7072 |           0.2507 |
[32m[20221213 15:00:45 @agent_ppo2.py:185][0m |          -0.0144 |           2.6740 |           0.2507 |
[32m[20221213 15:00:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:00:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.81
[32m[20221213 15:00:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.54
[32m[20221213 15:00:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.85
[32m[20221213 15:00:46 @agent_ppo2.py:143][0m Total time:       7.84 min
[32m[20221213 15:00:46 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221213 15:00:46 @agent_ppo2.py:121][0m #------------------------ Iteration 344 --------------------------#
[32m[20221213 15:00:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:46 @agent_ppo2.py:185][0m |          -0.0040 |           2.9172 |           0.2503 |
[32m[20221213 15:00:46 @agent_ppo2.py:185][0m |           0.0023 |           2.9036 |           0.2502 |
[32m[20221213 15:00:46 @agent_ppo2.py:185][0m |          -0.0078 |           2.7971 |           0.2501 |
[32m[20221213 15:00:46 @agent_ppo2.py:185][0m |          -0.0144 |           2.7696 |           0.2499 |
[32m[20221213 15:00:46 @agent_ppo2.py:185][0m |          -0.0129 |           2.7540 |           0.2498 |
[32m[20221213 15:00:46 @agent_ppo2.py:185][0m |          -0.0128 |           2.7324 |           0.2497 |
[32m[20221213 15:00:47 @agent_ppo2.py:185][0m |          -0.0071 |           2.8177 |           0.2498 |
[32m[20221213 15:00:47 @agent_ppo2.py:185][0m |          -0.0115 |           2.7116 |           0.2497 |
[32m[20221213 15:00:47 @agent_ppo2.py:185][0m |          -0.0090 |           2.7580 |           0.2498 |
[32m[20221213 15:00:47 @agent_ppo2.py:185][0m |          -0.0110 |           2.6729 |           0.2498 |
[32m[20221213 15:00:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:00:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.47
[32m[20221213 15:00:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.66
[32m[20221213 15:00:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.57
[32m[20221213 15:00:47 @agent_ppo2.py:143][0m Total time:       7.86 min
[32m[20221213 15:00:47 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221213 15:00:47 @agent_ppo2.py:121][0m #------------------------ Iteration 345 --------------------------#
[32m[20221213 15:00:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:47 @agent_ppo2.py:185][0m |          -0.0028 |           2.9097 |           0.2506 |
[32m[20221213 15:00:47 @agent_ppo2.py:185][0m |          -0.0042 |           2.8446 |           0.2503 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0078 |           2.8138 |           0.2500 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0101 |           2.7998 |           0.2500 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0084 |           2.7666 |           0.2497 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0141 |           2.7642 |           0.2497 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0027 |           2.8574 |           0.2496 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0165 |           2.7416 |           0.2496 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0153 |           2.7362 |           0.2494 |
[32m[20221213 15:00:48 @agent_ppo2.py:185][0m |          -0.0148 |           2.7280 |           0.2494 |
[32m[20221213 15:00:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:00:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.80
[32m[20221213 15:00:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.50
[32m[20221213 15:00:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 87.90
[32m[20221213 15:00:48 @agent_ppo2.py:143][0m Total time:       7.89 min
[32m[20221213 15:00:48 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221213 15:00:48 @agent_ppo2.py:121][0m #------------------------ Iteration 346 --------------------------#
[32m[20221213 15:00:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0009 |           2.8398 |           0.2505 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0103 |           2.7860 |           0.2497 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0107 |           2.7660 |           0.2494 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0050 |           2.9142 |           0.2490 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0126 |           2.7431 |           0.2490 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0140 |           2.7138 |           0.2491 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0076 |           2.8472 |           0.2490 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0172 |           2.6883 |           0.2488 |
[32m[20221213 15:00:49 @agent_ppo2.py:185][0m |          -0.0194 |           2.6843 |           0.2488 |
[32m[20221213 15:00:50 @agent_ppo2.py:185][0m |          -0.0202 |           2.6779 |           0.2487 |
[32m[20221213 15:00:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:00:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.32
[32m[20221213 15:00:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 107.53
[32m[20221213 15:00:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 114.91
[32m[20221213 15:00:50 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 114.91
[32m[20221213 15:00:50 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 114.91
[32m[20221213 15:00:50 @agent_ppo2.py:143][0m Total time:       7.91 min
[32m[20221213 15:00:50 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221213 15:00:50 @agent_ppo2.py:121][0m #------------------------ Iteration 347 --------------------------#
[32m[20221213 15:00:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:50 @agent_ppo2.py:185][0m |          -0.0002 |           2.7760 |           0.2450 |
[32m[20221213 15:00:50 @agent_ppo2.py:185][0m |          -0.0066 |           2.7132 |           0.2446 |
[32m[20221213 15:00:50 @agent_ppo2.py:185][0m |          -0.0104 |           2.7049 |           0.2443 |
[32m[20221213 15:00:50 @agent_ppo2.py:185][0m |          -0.0122 |           2.6817 |           0.2440 |
[32m[20221213 15:00:50 @agent_ppo2.py:185][0m |          -0.0023 |           2.8524 |           0.2440 |
[32m[20221213 15:00:51 @agent_ppo2.py:185][0m |          -0.0140 |           2.6570 |           0.2437 |
[32m[20221213 15:00:51 @agent_ppo2.py:185][0m |          -0.0135 |           2.6377 |           0.2435 |
[32m[20221213 15:00:51 @agent_ppo2.py:185][0m |          -0.0176 |           2.6399 |           0.2435 |
[32m[20221213 15:00:51 @agent_ppo2.py:185][0m |          -0.0159 |           2.6218 |           0.2434 |
[32m[20221213 15:00:51 @agent_ppo2.py:185][0m |          -0.0161 |           2.6046 |           0.2435 |
[32m[20221213 15:00:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.91
[32m[20221213 15:00:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.50
[32m[20221213 15:00:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.24
[32m[20221213 15:00:51 @agent_ppo2.py:143][0m Total time:       7.93 min
[32m[20221213 15:00:51 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221213 15:00:51 @agent_ppo2.py:121][0m #------------------------ Iteration 348 --------------------------#
[32m[20221213 15:00:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:51 @agent_ppo2.py:185][0m |           0.0022 |           2.9233 |           0.2444 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0068 |           2.8279 |           0.2441 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0060 |           2.7933 |           0.2436 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0126 |           2.7501 |           0.2433 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0112 |           2.7078 |           0.2432 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0125 |           2.6783 |           0.2430 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0160 |           2.6522 |           0.2430 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0161 |           2.6173 |           0.2428 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0181 |           2.5933 |           0.2428 |
[32m[20221213 15:00:52 @agent_ppo2.py:185][0m |          -0.0181 |           2.5640 |           0.2429 |
[32m[20221213 15:00:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.08
[32m[20221213 15:00:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.47
[32m[20221213 15:00:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 92.93
[32m[20221213 15:00:52 @agent_ppo2.py:143][0m Total time:       7.95 min
[32m[20221213 15:00:52 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221213 15:00:52 @agent_ppo2.py:121][0m #------------------------ Iteration 349 --------------------------#
[32m[20221213 15:00:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |           0.0090 |           3.3678 |           0.2441 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0073 |           2.9053 |           0.2437 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0139 |           2.8763 |           0.2437 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0111 |           2.8221 |           0.2436 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0142 |           2.8161 |           0.2437 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0153 |           2.7766 |           0.2437 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0184 |           2.7540 |           0.2437 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0172 |           2.7353 |           0.2436 |
[32m[20221213 15:00:53 @agent_ppo2.py:185][0m |          -0.0190 |           2.7264 |           0.2437 |
[32m[20221213 15:00:54 @agent_ppo2.py:185][0m |          -0.0130 |           2.8151 |           0.2436 |
[32m[20221213 15:00:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:00:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.80
[32m[20221213 15:00:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 108.30
[32m[20221213 15:00:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 93.67
[32m[20221213 15:00:54 @agent_ppo2.py:143][0m Total time:       7.98 min
[32m[20221213 15:00:54 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221213 15:00:54 @agent_ppo2.py:121][0m #------------------------ Iteration 350 --------------------------#
[32m[20221213 15:00:54 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:00:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:54 @agent_ppo2.py:185][0m |          -0.0007 |           3.0908 |           0.2466 |
[32m[20221213 15:00:54 @agent_ppo2.py:185][0m |           0.0003 |           3.0097 |           0.2464 |
[32m[20221213 15:00:54 @agent_ppo2.py:185][0m |          -0.0060 |           2.9913 |           0.2461 |
[32m[20221213 15:00:54 @agent_ppo2.py:185][0m |          -0.0090 |           2.9378 |           0.2459 |
[32m[20221213 15:00:54 @agent_ppo2.py:185][0m |          -0.0117 |           2.9101 |           0.2456 |
[32m[20221213 15:00:55 @agent_ppo2.py:185][0m |          -0.0117 |           2.9050 |           0.2456 |
[32m[20221213 15:00:55 @agent_ppo2.py:185][0m |          -0.0144 |           2.8845 |           0.2456 |
[32m[20221213 15:00:55 @agent_ppo2.py:185][0m |          -0.0154 |           2.8718 |           0.2454 |
[32m[20221213 15:00:55 @agent_ppo2.py:185][0m |          -0.0100 |           2.9775 |           0.2454 |
[32m[20221213 15:00:55 @agent_ppo2.py:185][0m |          -0.0185 |           2.8740 |           0.2452 |
[32m[20221213 15:00:55 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:00:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.25
[32m[20221213 15:00:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.56
[32m[20221213 15:00:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 100.27
[32m[20221213 15:00:55 @agent_ppo2.py:143][0m Total time:       8.00 min
[32m[20221213 15:00:55 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221213 15:00:55 @agent_ppo2.py:121][0m #------------------------ Iteration 351 --------------------------#
[32m[20221213 15:00:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:55 @agent_ppo2.py:185][0m |          -0.0014 |           3.0408 |           0.2453 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0003 |           2.9816 |           0.2451 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0121 |           2.9029 |           0.2448 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0125 |           2.8484 |           0.2447 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0136 |           2.8163 |           0.2446 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0157 |           2.7997 |           0.2446 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0162 |           2.7527 |           0.2446 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0146 |           2.7283 |           0.2446 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0160 |           2.7347 |           0.2445 |
[32m[20221213 15:00:56 @agent_ppo2.py:185][0m |          -0.0188 |           2.6750 |           0.2445 |
[32m[20221213 15:00:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 98.85
[32m[20221213 15:00:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 107.88
[32m[20221213 15:00:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 119.74
[32m[20221213 15:00:56 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 119.74
[32m[20221213 15:00:56 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 119.74
[32m[20221213 15:00:56 @agent_ppo2.py:143][0m Total time:       8.02 min
[32m[20221213 15:00:56 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221213 15:00:56 @agent_ppo2.py:121][0m #------------------------ Iteration 352 --------------------------#
[32m[20221213 15:00:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0033 |           3.1742 |           0.2457 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0087 |           3.0307 |           0.2452 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0111 |           3.0015 |           0.2451 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0083 |           2.9591 |           0.2449 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0132 |           2.9411 |           0.2450 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0132 |           2.9306 |           0.2451 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0136 |           2.9005 |           0.2449 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0148 |           2.8801 |           0.2450 |
[32m[20221213 15:00:57 @agent_ppo2.py:185][0m |          -0.0124 |           3.0194 |           0.2450 |
[32m[20221213 15:00:58 @agent_ppo2.py:185][0m |          -0.0143 |           2.8754 |           0.2450 |
[32m[20221213 15:00:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.28
[32m[20221213 15:00:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.01
[32m[20221213 15:00:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 95.63
[32m[20221213 15:00:58 @agent_ppo2.py:143][0m Total time:       8.04 min
[32m[20221213 15:00:58 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221213 15:00:58 @agent_ppo2.py:121][0m #------------------------ Iteration 353 --------------------------#
[32m[20221213 15:00:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:58 @agent_ppo2.py:185][0m |           0.0004 |           3.3031 |           0.2486 |
[32m[20221213 15:00:58 @agent_ppo2.py:185][0m |          -0.0026 |           3.2100 |           0.2482 |
[32m[20221213 15:00:58 @agent_ppo2.py:185][0m |          -0.0080 |           3.1600 |           0.2480 |
[32m[20221213 15:00:58 @agent_ppo2.py:185][0m |          -0.0091 |           3.1478 |           0.2478 |
[32m[20221213 15:00:58 @agent_ppo2.py:185][0m |          -0.0137 |           3.1226 |           0.2476 |
[32m[20221213 15:00:59 @agent_ppo2.py:185][0m |          -0.0144 |           3.1030 |           0.2475 |
[32m[20221213 15:00:59 @agent_ppo2.py:185][0m |          -0.0116 |           3.0855 |           0.2474 |
[32m[20221213 15:00:59 @agent_ppo2.py:185][0m |          -0.0163 |           3.0679 |           0.2474 |
[32m[20221213 15:00:59 @agent_ppo2.py:185][0m |          -0.0149 |           3.0665 |           0.2474 |
[32m[20221213 15:00:59 @agent_ppo2.py:185][0m |          -0.0155 |           3.0380 |           0.2473 |
[32m[20221213 15:00:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:00:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.92
[32m[20221213 15:00:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 113.32
[32m[20221213 15:00:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 109.83
[32m[20221213 15:00:59 @agent_ppo2.py:143][0m Total time:       8.07 min
[32m[20221213 15:00:59 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221213 15:00:59 @agent_ppo2.py:121][0m #------------------------ Iteration 354 --------------------------#
[32m[20221213 15:00:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:00:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:00:59 @agent_ppo2.py:185][0m |          -0.0030 |           3.2134 |           0.2507 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0010 |           3.2010 |           0.2508 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0067 |           3.0881 |           0.2508 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0108 |           3.0687 |           0.2509 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0109 |           3.0318 |           0.2509 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0035 |           3.3348 |           0.2509 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0152 |           3.0323 |           0.2510 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0158 |           2.9952 |           0.2510 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0142 |           2.9744 |           0.2512 |
[32m[20221213 15:01:00 @agent_ppo2.py:185][0m |          -0.0176 |           2.9403 |           0.2512 |
[32m[20221213 15:01:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:01:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.58
[32m[20221213 15:01:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 100.49
[32m[20221213 15:01:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 106.80
[32m[20221213 15:01:00 @agent_ppo2.py:143][0m Total time:       8.09 min
[32m[20221213 15:01:00 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221213 15:01:00 @agent_ppo2.py:121][0m #------------------------ Iteration 355 --------------------------#
[32m[20221213 15:01:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0023 |           3.1525 |           0.2513 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0068 |           3.0135 |           0.2509 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0101 |           2.9650 |           0.2505 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0148 |           2.9284 |           0.2502 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0101 |           2.9040 |           0.2501 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0155 |           2.8819 |           0.2500 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0184 |           2.8636 |           0.2497 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0148 |           2.8396 |           0.2496 |
[32m[20221213 15:01:01 @agent_ppo2.py:185][0m |          -0.0194 |           2.8141 |           0.2497 |
[32m[20221213 15:01:02 @agent_ppo2.py:185][0m |          -0.0161 |           2.7959 |           0.2494 |
[32m[20221213 15:01:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.13
[32m[20221213 15:01:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.80
[32m[20221213 15:01:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.64
[32m[20221213 15:01:02 @agent_ppo2.py:143][0m Total time:       8.11 min
[32m[20221213 15:01:02 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221213 15:01:02 @agent_ppo2.py:121][0m #------------------------ Iteration 356 --------------------------#
[32m[20221213 15:01:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:02 @agent_ppo2.py:185][0m |          -0.0017 |           3.0600 |           0.2485 |
[32m[20221213 15:01:02 @agent_ppo2.py:185][0m |          -0.0081 |           2.9859 |           0.2485 |
[32m[20221213 15:01:02 @agent_ppo2.py:185][0m |          -0.0095 |           2.9515 |           0.2483 |
[32m[20221213 15:01:02 @agent_ppo2.py:185][0m |          -0.0108 |           2.9332 |           0.2483 |
[32m[20221213 15:01:02 @agent_ppo2.py:185][0m |          -0.0114 |           2.9140 |           0.2481 |
[32m[20221213 15:01:03 @agent_ppo2.py:185][0m |          -0.0125 |           2.9025 |           0.2481 |
[32m[20221213 15:01:03 @agent_ppo2.py:185][0m |          -0.0145 |           2.8926 |           0.2481 |
[32m[20221213 15:01:03 @agent_ppo2.py:185][0m |          -0.0134 |           2.8812 |           0.2480 |
[32m[20221213 15:01:03 @agent_ppo2.py:185][0m |          -0.0160 |           2.8708 |           0.2481 |
[32m[20221213 15:01:03 @agent_ppo2.py:185][0m |          -0.0159 |           2.8750 |           0.2480 |
[32m[20221213 15:01:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.40
[32m[20221213 15:01:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 108.13
[32m[20221213 15:01:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.83
[32m[20221213 15:01:03 @agent_ppo2.py:143][0m Total time:       8.13 min
[32m[20221213 15:01:03 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221213 15:01:03 @agent_ppo2.py:121][0m #------------------------ Iteration 357 --------------------------#
[32m[20221213 15:01:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:01:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:03 @agent_ppo2.py:185][0m |          -0.0027 |           3.2996 |           0.2399 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0075 |           3.1717 |           0.2395 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0047 |           3.1410 |           0.2392 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0137 |           3.0983 |           0.2392 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0118 |           3.0586 |           0.2391 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0157 |           3.0397 |           0.2390 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0159 |           3.0264 |           0.2389 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0114 |           3.0825 |           0.2388 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0190 |           2.9792 |           0.2387 |
[32m[20221213 15:01:04 @agent_ppo2.py:185][0m |          -0.0164 |           3.0758 |           0.2388 |
[32m[20221213 15:01:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.19
[32m[20221213 15:01:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.04
[32m[20221213 15:01:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 92.07
[32m[20221213 15:01:04 @agent_ppo2.py:143][0m Total time:       8.15 min
[32m[20221213 15:01:04 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221213 15:01:04 @agent_ppo2.py:121][0m #------------------------ Iteration 358 --------------------------#
[32m[20221213 15:01:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |           0.0046 |           3.2996 |           0.2414 |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |          -0.0085 |           3.1525 |           0.2412 |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |          -0.0114 |           3.1067 |           0.2408 |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |          -0.0136 |           3.0690 |           0.2407 |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |          -0.0123 |           3.0521 |           0.2407 |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |          -0.0157 |           3.0322 |           0.2405 |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |          -0.0072 |           3.1844 |           0.2405 |
[32m[20221213 15:01:05 @agent_ppo2.py:185][0m |          -0.0137 |           3.0099 |           0.2404 |
[32m[20221213 15:01:06 @agent_ppo2.py:185][0m |          -0.0161 |           2.9894 |           0.2404 |
[32m[20221213 15:01:06 @agent_ppo2.py:185][0m |          -0.0184 |           2.9862 |           0.2403 |
[32m[20221213 15:01:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:01:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.56
[32m[20221213 15:01:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 108.03
[32m[20221213 15:01:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 101.55
[32m[20221213 15:01:06 @agent_ppo2.py:143][0m Total time:       8.18 min
[32m[20221213 15:01:06 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221213 15:01:06 @agent_ppo2.py:121][0m #------------------------ Iteration 359 --------------------------#
[32m[20221213 15:01:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:06 @agent_ppo2.py:185][0m |          -0.0008 |           3.1908 |           0.2427 |
[32m[20221213 15:01:06 @agent_ppo2.py:185][0m |          -0.0029 |           3.1664 |           0.2424 |
[32m[20221213 15:01:06 @agent_ppo2.py:185][0m |           0.0020 |           3.3220 |           0.2419 |
[32m[20221213 15:01:06 @agent_ppo2.py:185][0m |          -0.0127 |           3.1123 |           0.2411 |
[32m[20221213 15:01:07 @agent_ppo2.py:185][0m |          -0.0114 |           3.0714 |           0.2416 |
[32m[20221213 15:01:07 @agent_ppo2.py:185][0m |          -0.0029 |           3.3395 |           0.2415 |
[32m[20221213 15:01:07 @agent_ppo2.py:185][0m |          -0.0180 |           3.0658 |           0.2415 |
[32m[20221213 15:01:07 @agent_ppo2.py:185][0m |          -0.0166 |           3.0362 |           0.2414 |
[32m[20221213 15:01:07 @agent_ppo2.py:185][0m |          -0.0162 |           3.0080 |           0.2412 |
[32m[20221213 15:01:07 @agent_ppo2.py:185][0m |          -0.0136 |           3.0063 |           0.2411 |
[32m[20221213 15:01:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 95.78
[32m[20221213 15:01:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 105.14
[32m[20221213 15:01:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.08
[32m[20221213 15:01:07 @agent_ppo2.py:143][0m Total time:       8.20 min
[32m[20221213 15:01:07 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221213 15:01:07 @agent_ppo2.py:121][0m #------------------------ Iteration 360 --------------------------#
[32m[20221213 15:01:07 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0009 |           3.0111 |           0.2488 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0003 |           2.9809 |           0.2489 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0077 |           2.9210 |           0.2483 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0040 |           2.9166 |           0.2482 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0046 |           2.9163 |           0.2477 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0143 |           2.8441 |           0.2477 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0116 |           2.8353 |           0.2475 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0147 |           2.8212 |           0.2474 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0154 |           2.7989 |           0.2473 |
[32m[20221213 15:01:08 @agent_ppo2.py:185][0m |          -0.0156 |           2.7855 |           0.2472 |
[32m[20221213 15:01:08 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:01:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.83
[32m[20221213 15:01:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 108.76
[32m[20221213 15:01:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 104.46
[32m[20221213 15:01:08 @agent_ppo2.py:143][0m Total time:       8.22 min
[32m[20221213 15:01:08 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221213 15:01:08 @agent_ppo2.py:121][0m #------------------------ Iteration 361 --------------------------#
[32m[20221213 15:01:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |           0.0026 |           3.1467 |           0.2424 |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |          -0.0092 |           2.9543 |           0.2423 |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |          -0.0035 |           2.9705 |           0.2422 |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |          -0.0175 |           2.8953 |           0.2423 |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |          -0.0080 |           3.0303 |           0.2424 |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |          -0.0151 |           2.8444 |           0.2422 |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |          -0.0188 |           2.8205 |           0.2424 |
[32m[20221213 15:01:09 @agent_ppo2.py:185][0m |          -0.0192 |           2.8160 |           0.2423 |
[32m[20221213 15:01:10 @agent_ppo2.py:185][0m |          -0.0167 |           2.7975 |           0.2424 |
[32m[20221213 15:01:10 @agent_ppo2.py:185][0m |          -0.0209 |           2.7857 |           0.2423 |
[32m[20221213 15:01:10 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:01:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.80
[32m[20221213 15:01:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 105.10
[32m[20221213 15:01:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 61.62
[32m[20221213 15:01:10 @agent_ppo2.py:143][0m Total time:       8.24 min
[32m[20221213 15:01:10 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221213 15:01:10 @agent_ppo2.py:121][0m #------------------------ Iteration 362 --------------------------#
[32m[20221213 15:01:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:10 @agent_ppo2.py:185][0m |          -0.0037 |           3.0379 |           0.2384 |
[32m[20221213 15:01:10 @agent_ppo2.py:185][0m |          -0.0095 |           2.9366 |           0.2384 |
[32m[20221213 15:01:10 @agent_ppo2.py:185][0m |          -0.0123 |           2.8986 |           0.2382 |
[32m[20221213 15:01:10 @agent_ppo2.py:185][0m |          -0.0130 |           2.8776 |           0.2381 |
[32m[20221213 15:01:11 @agent_ppo2.py:185][0m |          -0.0175 |           2.8445 |           0.2381 |
[32m[20221213 15:01:11 @agent_ppo2.py:185][0m |          -0.0171 |           2.8191 |           0.2382 |
[32m[20221213 15:01:11 @agent_ppo2.py:185][0m |          -0.0155 |           2.8211 |           0.2382 |
[32m[20221213 15:01:11 @agent_ppo2.py:185][0m |          -0.0166 |           2.7939 |           0.2383 |
[32m[20221213 15:01:11 @agent_ppo2.py:185][0m |          -0.0205 |           2.7870 |           0.2382 |
[32m[20221213 15:01:11 @agent_ppo2.py:185][0m |          -0.0212 |           2.7599 |           0.2383 |
[32m[20221213 15:01:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 91.42
[32m[20221213 15:01:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 98.57
[32m[20221213 15:01:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 83.30
[32m[20221213 15:01:11 @agent_ppo2.py:143][0m Total time:       8.27 min
[32m[20221213 15:01:11 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221213 15:01:11 @agent_ppo2.py:121][0m #------------------------ Iteration 363 --------------------------#
[32m[20221213 15:01:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:11 @agent_ppo2.py:185][0m |          -0.0020 |           2.8404 |           0.2435 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0059 |           2.7515 |           0.2433 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0028 |           2.8391 |           0.2431 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0086 |           2.7186 |           0.2427 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0176 |           2.6882 |           0.2427 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0139 |           2.6635 |           0.2426 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |           0.0032 |           3.0338 |           0.2426 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0124 |           2.6450 |           0.2421 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0201 |           2.6128 |           0.2422 |
[32m[20221213 15:01:12 @agent_ppo2.py:185][0m |          -0.0059 |           2.7304 |           0.2422 |
[32m[20221213 15:01:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.77
[32m[20221213 15:01:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 107.40
[32m[20221213 15:01:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 90.31
[32m[20221213 15:01:12 @agent_ppo2.py:143][0m Total time:       8.29 min
[32m[20221213 15:01:12 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221213 15:01:12 @agent_ppo2.py:121][0m #------------------------ Iteration 364 --------------------------#
[32m[20221213 15:01:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0027 |           3.1375 |           0.2370 |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0084 |           3.0420 |           0.2368 |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0095 |           3.0019 |           0.2366 |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0134 |           2.9814 |           0.2368 |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0140 |           2.9683 |           0.2366 |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0124 |           2.9813 |           0.2367 |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0154 |           2.9418 |           0.2369 |
[32m[20221213 15:01:13 @agent_ppo2.py:185][0m |          -0.0175 |           2.9363 |           0.2370 |
[32m[20221213 15:01:14 @agent_ppo2.py:185][0m |          -0.0186 |           2.9397 |           0.2369 |
[32m[20221213 15:01:14 @agent_ppo2.py:185][0m |          -0.0143 |           2.9244 |           0.2369 |
[32m[20221213 15:01:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:01:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 107.70
[32m[20221213 15:01:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.00
[32m[20221213 15:01:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 94.50
[32m[20221213 15:01:14 @agent_ppo2.py:143][0m Total time:       8.31 min
[32m[20221213 15:01:14 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221213 15:01:14 @agent_ppo2.py:121][0m #------------------------ Iteration 365 --------------------------#
[32m[20221213 15:01:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:14 @agent_ppo2.py:185][0m |          -0.0029 |           2.9376 |           0.2458 |
[32m[20221213 15:01:14 @agent_ppo2.py:185][0m |          -0.0069 |           2.8905 |           0.2457 |
[32m[20221213 15:01:14 @agent_ppo2.py:185][0m |          -0.0125 |           2.8485 |           0.2453 |
[32m[20221213 15:01:14 @agent_ppo2.py:185][0m |          -0.0069 |           2.8618 |           0.2452 |
[32m[20221213 15:01:15 @agent_ppo2.py:185][0m |          -0.0136 |           2.8473 |           0.2452 |
[32m[20221213 15:01:15 @agent_ppo2.py:185][0m |          -0.0181 |           2.8114 |           0.2453 |
[32m[20221213 15:01:15 @agent_ppo2.py:185][0m |          -0.0168 |           2.8108 |           0.2452 |
[32m[20221213 15:01:15 @agent_ppo2.py:185][0m |          -0.0159 |           2.7901 |           0.2454 |
[32m[20221213 15:01:15 @agent_ppo2.py:185][0m |          -0.0106 |           2.8397 |           0.2455 |
[32m[20221213 15:01:15 @agent_ppo2.py:185][0m |          -0.0189 |           2.7829 |           0.2452 |
[32m[20221213 15:01:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.46
[32m[20221213 15:01:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.46
[32m[20221213 15:01:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 100.42
[32m[20221213 15:01:15 @agent_ppo2.py:143][0m Total time:       8.33 min
[32m[20221213 15:01:15 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221213 15:01:15 @agent_ppo2.py:121][0m #------------------------ Iteration 366 --------------------------#
[32m[20221213 15:01:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0026 |           2.9022 |           0.2428 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0008 |           2.8857 |           0.2426 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0107 |           2.8310 |           0.2425 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0100 |           2.8140 |           0.2422 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0128 |           2.7805 |           0.2422 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0175 |           2.7813 |           0.2421 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0074 |           2.9515 |           0.2421 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0170 |           2.7703 |           0.2419 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0153 |           2.7299 |           0.2420 |
[32m[20221213 15:01:16 @agent_ppo2.py:185][0m |          -0.0127 |           2.7851 |           0.2420 |
[32m[20221213 15:01:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.40
[32m[20221213 15:01:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 95.70
[32m[20221213 15:01:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 88.04
[32m[20221213 15:01:17 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221213 15:01:17 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221213 15:01:17 @agent_ppo2.py:121][0m #------------------------ Iteration 367 --------------------------#
[32m[20221213 15:01:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |          -0.0011 |           2.9868 |           0.2433 |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |           0.0000 |           3.0257 |           0.2430 |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |          -0.0120 |           2.8640 |           0.2427 |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |          -0.0114 |           2.8372 |           0.2428 |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |          -0.0159 |           2.8228 |           0.2427 |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |          -0.0156 |           2.8103 |           0.2426 |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |          -0.0158 |           2.7925 |           0.2428 |
[32m[20221213 15:01:17 @agent_ppo2.py:185][0m |          -0.0172 |           2.7779 |           0.2429 |
[32m[20221213 15:01:18 @agent_ppo2.py:185][0m |          -0.0165 |           2.7794 |           0.2430 |
[32m[20221213 15:01:18 @agent_ppo2.py:185][0m |          -0.0183 |           2.7626 |           0.2430 |
[32m[20221213 15:01:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.18
[32m[20221213 15:01:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 116.48
[32m[20221213 15:01:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.30
[32m[20221213 15:01:18 @agent_ppo2.py:143][0m Total time:       8.38 min
[32m[20221213 15:01:18 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221213 15:01:18 @agent_ppo2.py:121][0m #------------------------ Iteration 368 --------------------------#
[32m[20221213 15:01:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:18 @agent_ppo2.py:185][0m |          -0.0009 |           3.0018 |           0.2410 |
[32m[20221213 15:01:18 @agent_ppo2.py:185][0m |          -0.0084 |           2.9054 |           0.2406 |
[32m[20221213 15:01:18 @agent_ppo2.py:185][0m |          -0.0131 |           2.8623 |           0.2403 |
[32m[20221213 15:01:18 @agent_ppo2.py:185][0m |          -0.0160 |           2.8540 |           0.2402 |
[32m[20221213 15:01:19 @agent_ppo2.py:185][0m |          -0.0183 |           2.8311 |           0.2402 |
[32m[20221213 15:01:19 @agent_ppo2.py:185][0m |          -0.0135 |           2.8094 |           0.2401 |
[32m[20221213 15:01:19 @agent_ppo2.py:185][0m |          -0.0055 |           3.0262 |           0.2398 |
[32m[20221213 15:01:19 @agent_ppo2.py:185][0m |          -0.0174 |           2.7757 |           0.2398 |
[32m[20221213 15:01:19 @agent_ppo2.py:185][0m |          -0.0176 |           2.7519 |           0.2398 |
[32m[20221213 15:01:19 @agent_ppo2.py:185][0m |          -0.0083 |           2.9357 |           0.2398 |
[32m[20221213 15:01:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.57
[32m[20221213 15:01:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.98
[32m[20221213 15:01:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.57
[32m[20221213 15:01:19 @agent_ppo2.py:143][0m Total time:       8.40 min
[32m[20221213 15:01:19 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221213 15:01:19 @agent_ppo2.py:121][0m #------------------------ Iteration 369 --------------------------#
[32m[20221213 15:01:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0017 |           3.0695 |           0.2434 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0067 |           2.9790 |           0.2428 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0086 |           2.9458 |           0.2427 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0103 |           2.9170 |           0.2425 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0121 |           2.9093 |           0.2423 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0166 |           2.8961 |           0.2421 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0136 |           2.8788 |           0.2419 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0169 |           2.8683 |           0.2419 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0179 |           2.8526 |           0.2416 |
[32m[20221213 15:01:20 @agent_ppo2.py:185][0m |          -0.0143 |           2.8448 |           0.2417 |
[32m[20221213 15:01:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.78
[32m[20221213 15:01:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.96
[32m[20221213 15:01:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 101.50
[32m[20221213 15:01:20 @agent_ppo2.py:143][0m Total time:       8.42 min
[32m[20221213 15:01:20 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221213 15:01:20 @agent_ppo2.py:121][0m #------------------------ Iteration 370 --------------------------#
[32m[20221213 15:01:21 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0027 |           2.9414 |           0.2423 |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0091 |           2.8523 |           0.2418 |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0105 |           2.8217 |           0.2416 |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0137 |           2.7965 |           0.2414 |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0091 |           2.7895 |           0.2413 |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0165 |           2.7646 |           0.2412 |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0175 |           2.7337 |           0.2410 |
[32m[20221213 15:01:21 @agent_ppo2.py:185][0m |          -0.0145 |           2.7291 |           0.2410 |
[32m[20221213 15:01:22 @agent_ppo2.py:185][0m |          -0.0163 |           2.7118 |           0.2410 |
[32m[20221213 15:01:22 @agent_ppo2.py:185][0m |          -0.0186 |           2.6928 |           0.2408 |
[32m[20221213 15:01:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 93.07
[32m[20221213 15:01:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 101.83
[32m[20221213 15:01:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.07
[32m[20221213 15:01:22 @agent_ppo2.py:143][0m Total time:       8.44 min
[32m[20221213 15:01:22 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221213 15:01:22 @agent_ppo2.py:121][0m #------------------------ Iteration 371 --------------------------#
[32m[20221213 15:01:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:22 @agent_ppo2.py:185][0m |          -0.0012 |           2.9852 |           0.2444 |
[32m[20221213 15:01:22 @agent_ppo2.py:185][0m |          -0.0045 |           2.9306 |           0.2439 |
[32m[20221213 15:01:22 @agent_ppo2.py:185][0m |           0.0005 |           3.2083 |           0.2436 |
[32m[20221213 15:01:22 @agent_ppo2.py:185][0m |          -0.0145 |           2.9075 |           0.2434 |
[32m[20221213 15:01:23 @agent_ppo2.py:185][0m |          -0.0019 |           3.1114 |           0.2435 |
[32m[20221213 15:01:23 @agent_ppo2.py:185][0m |          -0.0056 |           2.9078 |           0.2433 |
[32m[20221213 15:01:23 @agent_ppo2.py:185][0m |          -0.0140 |           2.8227 |           0.2434 |
[32m[20221213 15:01:23 @agent_ppo2.py:185][0m |          -0.0155 |           2.8160 |           0.2434 |
[32m[20221213 15:01:23 @agent_ppo2.py:185][0m |          -0.0141 |           2.8103 |           0.2434 |
[32m[20221213 15:01:23 @agent_ppo2.py:185][0m |          -0.0184 |           2.7964 |           0.2435 |
[32m[20221213 15:01:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:01:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.81
[32m[20221213 15:01:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.40
[32m[20221213 15:01:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 91.90
[32m[20221213 15:01:23 @agent_ppo2.py:143][0m Total time:       8.47 min
[32m[20221213 15:01:23 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221213 15:01:23 @agent_ppo2.py:121][0m #------------------------ Iteration 372 --------------------------#
[32m[20221213 15:01:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0001 |           3.0229 |           0.2430 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |           0.0032 |           3.1152 |           0.2429 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |           0.0040 |           3.2450 |           0.2426 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0098 |           2.9390 |           0.2423 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0040 |           3.0694 |           0.2423 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0122 |           2.9075 |           0.2420 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0172 |           2.8817 |           0.2420 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0168 |           2.8737 |           0.2419 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0167 |           2.8718 |           0.2420 |
[32m[20221213 15:01:24 @agent_ppo2.py:185][0m |          -0.0188 |           2.8579 |           0.2418 |
[32m[20221213 15:01:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:01:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.60
[32m[20221213 15:01:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.52
[32m[20221213 15:01:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.38
[32m[20221213 15:01:25 @agent_ppo2.py:143][0m Total time:       8.49 min
[32m[20221213 15:01:25 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221213 15:01:25 @agent_ppo2.py:121][0m #------------------------ Iteration 373 --------------------------#
[32m[20221213 15:01:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:25 @agent_ppo2.py:185][0m |           0.0125 |           3.4291 |           0.2388 |
[32m[20221213 15:01:25 @agent_ppo2.py:185][0m |          -0.0007 |           3.0390 |           0.2383 |
[32m[20221213 15:01:25 @agent_ppo2.py:185][0m |          -0.0054 |           2.9509 |           0.2388 |
[32m[20221213 15:01:25 @agent_ppo2.py:185][0m |          -0.0117 |           2.9369 |           0.2389 |
[32m[20221213 15:01:25 @agent_ppo2.py:185][0m |          -0.0128 |           2.9220 |           0.2391 |
[32m[20221213 15:01:25 @agent_ppo2.py:185][0m |          -0.0053 |           3.0541 |           0.2392 |
[32m[20221213 15:01:25 @agent_ppo2.py:185][0m |           0.0005 |           3.3076 |           0.2390 |
[32m[20221213 15:01:26 @agent_ppo2.py:185][0m |          -0.0129 |           2.9041 |           0.2392 |
[32m[20221213 15:01:26 @agent_ppo2.py:185][0m |          -0.0148 |           2.8820 |           0.2393 |
[32m[20221213 15:01:26 @agent_ppo2.py:185][0m |          -0.0056 |           3.0196 |           0.2396 |
[32m[20221213 15:01:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:01:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 100.34
[32m[20221213 15:01:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.95
[32m[20221213 15:01:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.81
[32m[20221213 15:01:26 @agent_ppo2.py:143][0m Total time:       8.51 min
[32m[20221213 15:01:26 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221213 15:01:26 @agent_ppo2.py:121][0m #------------------------ Iteration 374 --------------------------#
[32m[20221213 15:01:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:26 @agent_ppo2.py:185][0m |          -0.0028 |           2.8229 |           0.2397 |
[32m[20221213 15:01:26 @agent_ppo2.py:185][0m |          -0.0063 |           2.6676 |           0.2394 |
[32m[20221213 15:01:26 @agent_ppo2.py:185][0m |          -0.0087 |           2.6247 |           0.2392 |
[32m[20221213 15:01:27 @agent_ppo2.py:185][0m |          -0.0111 |           2.5801 |           0.2391 |
[32m[20221213 15:01:27 @agent_ppo2.py:185][0m |          -0.0132 |           2.5501 |           0.2392 |
[32m[20221213 15:01:27 @agent_ppo2.py:185][0m |          -0.0141 |           2.5258 |           0.2392 |
[32m[20221213 15:01:27 @agent_ppo2.py:185][0m |          -0.0176 |           2.5041 |           0.2391 |
[32m[20221213 15:01:27 @agent_ppo2.py:185][0m |           0.0019 |           2.8218 |           0.2392 |
[32m[20221213 15:01:27 @agent_ppo2.py:185][0m |          -0.0163 |           2.4992 |           0.2389 |
[32m[20221213 15:01:27 @agent_ppo2.py:185][0m |          -0.0137 |           2.4742 |           0.2390 |
[32m[20221213 15:01:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.04
[32m[20221213 15:01:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 106.64
[32m[20221213 15:01:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 84.22
[32m[20221213 15:01:27 @agent_ppo2.py:143][0m Total time:       8.53 min
[32m[20221213 15:01:27 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221213 15:01:27 @agent_ppo2.py:121][0m #------------------------ Iteration 375 --------------------------#
[32m[20221213 15:01:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |           0.0015 |           3.0436 |           0.2404 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0095 |           2.9914 |           0.2406 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0114 |           2.9511 |           0.2406 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0077 |           2.9408 |           0.2407 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0130 |           2.9053 |           0.2408 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0123 |           2.8963 |           0.2408 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0201 |           2.8828 |           0.2410 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0181 |           2.8578 |           0.2411 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0165 |           2.8384 |           0.2412 |
[32m[20221213 15:01:28 @agent_ppo2.py:185][0m |          -0.0166 |           2.8246 |           0.2413 |
[32m[20221213 15:01:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:01:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.54
[32m[20221213 15:01:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 105.18
[32m[20221213 15:01:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 103.10
[32m[20221213 15:01:29 @agent_ppo2.py:143][0m Total time:       8.56 min
[32m[20221213 15:01:29 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221213 15:01:29 @agent_ppo2.py:121][0m #------------------------ Iteration 376 --------------------------#
[32m[20221213 15:01:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:29 @agent_ppo2.py:185][0m |          -0.0009 |           3.0179 |           0.2401 |
[32m[20221213 15:01:29 @agent_ppo2.py:185][0m |          -0.0075 |           2.9644 |           0.2398 |
[32m[20221213 15:01:29 @agent_ppo2.py:185][0m |          -0.0111 |           2.9282 |           0.2397 |
[32m[20221213 15:01:29 @agent_ppo2.py:185][0m |          -0.0116 |           2.9178 |           0.2394 |
[32m[20221213 15:01:29 @agent_ppo2.py:185][0m |          -0.0113 |           2.9058 |           0.2394 |
[32m[20221213 15:01:29 @agent_ppo2.py:185][0m |          -0.0189 |           2.8678 |           0.2391 |
[32m[20221213 15:01:29 @agent_ppo2.py:185][0m |          -0.0131 |           2.8652 |           0.2391 |
[32m[20221213 15:01:30 @agent_ppo2.py:185][0m |          -0.0144 |           2.8370 |           0.2389 |
[32m[20221213 15:01:30 @agent_ppo2.py:185][0m |          -0.0178 |           2.8312 |           0.2387 |
[32m[20221213 15:01:30 @agent_ppo2.py:185][0m |          -0.0181 |           2.8081 |           0.2387 |
[32m[20221213 15:01:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.50
[32m[20221213 15:01:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 103.64
[32m[20221213 15:01:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 63.39
[32m[20221213 15:01:30 @agent_ppo2.py:143][0m Total time:       8.58 min
[32m[20221213 15:01:30 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221213 15:01:30 @agent_ppo2.py:121][0m #------------------------ Iteration 377 --------------------------#
[32m[20221213 15:01:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:30 @agent_ppo2.py:185][0m |           0.0026 |           3.0470 |           0.2521 |
[32m[20221213 15:01:30 @agent_ppo2.py:185][0m |          -0.0079 |           2.9269 |           0.2515 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0030 |           3.0391 |           0.2511 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0117 |           2.8717 |           0.2513 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0134 |           2.8470 |           0.2511 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0041 |           3.2190 |           0.2511 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0083 |           2.8726 |           0.2512 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0174 |           2.8147 |           0.2510 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0115 |           2.8471 |           0.2510 |
[32m[20221213 15:01:31 @agent_ppo2.py:185][0m |          -0.0144 |           2.8261 |           0.2511 |
[32m[20221213 15:01:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:01:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 105.36
[32m[20221213 15:01:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 117.53
[32m[20221213 15:01:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.93
[32m[20221213 15:01:31 @agent_ppo2.py:143][0m Total time:       8.60 min
[32m[20221213 15:01:31 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221213 15:01:31 @agent_ppo2.py:121][0m #------------------------ Iteration 378 --------------------------#
[32m[20221213 15:01:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |           0.0003 |           3.3067 |           0.2466 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0083 |           3.2112 |           0.2460 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0091 |           3.1881 |           0.2460 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0106 |           3.1639 |           0.2458 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0123 |           3.1394 |           0.2457 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0124 |           3.1308 |           0.2455 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0144 |           3.1306 |           0.2455 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0141 |           3.1135 |           0.2453 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0162 |           3.1071 |           0.2453 |
[32m[20221213 15:01:32 @agent_ppo2.py:185][0m |          -0.0143 |           3.1119 |           0.2453 |
[32m[20221213 15:01:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.73
[32m[20221213 15:01:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.06
[32m[20221213 15:01:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 82.39
[32m[20221213 15:01:33 @agent_ppo2.py:143][0m Total time:       8.62 min
[32m[20221213 15:01:33 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221213 15:01:33 @agent_ppo2.py:121][0m #------------------------ Iteration 379 --------------------------#
[32m[20221213 15:01:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:33 @agent_ppo2.py:185][0m |          -0.0006 |           2.9537 |           0.2462 |
[32m[20221213 15:01:33 @agent_ppo2.py:185][0m |          -0.0102 |           2.8773 |           0.2456 |
[32m[20221213 15:01:33 @agent_ppo2.py:185][0m |          -0.0083 |           2.8292 |           0.2456 |
[32m[20221213 15:01:33 @agent_ppo2.py:185][0m |          -0.0131 |           2.7918 |           0.2455 |
[32m[20221213 15:01:33 @agent_ppo2.py:185][0m |          -0.0055 |           2.8790 |           0.2455 |
[32m[20221213 15:01:33 @agent_ppo2.py:185][0m |          -0.0012 |           3.0977 |           0.2453 |
[32m[20221213 15:01:34 @agent_ppo2.py:185][0m |          -0.0192 |           2.7639 |           0.2450 |
[32m[20221213 15:01:34 @agent_ppo2.py:185][0m |          -0.0127 |           2.7485 |           0.2452 |
[32m[20221213 15:01:34 @agent_ppo2.py:185][0m |          -0.0102 |           2.8291 |           0.2451 |
[32m[20221213 15:01:34 @agent_ppo2.py:185][0m |          -0.0157 |           2.6931 |           0.2449 |
[32m[20221213 15:01:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.42
[32m[20221213 15:01:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.24
[32m[20221213 15:01:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 81.71
[32m[20221213 15:01:34 @agent_ppo2.py:143][0m Total time:       8.65 min
[32m[20221213 15:01:34 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221213 15:01:34 @agent_ppo2.py:121][0m #------------------------ Iteration 380 --------------------------#
[32m[20221213 15:01:34 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:34 @agent_ppo2.py:185][0m |           0.0007 |           3.1069 |           0.2480 |
[32m[20221213 15:01:34 @agent_ppo2.py:185][0m |          -0.0049 |           2.9872 |           0.2478 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0089 |           2.9301 |           0.2476 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0121 |           2.8962 |           0.2472 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0107 |           2.8692 |           0.2468 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0124 |           2.8631 |           0.2466 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0157 |           2.8394 |           0.2465 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0043 |           3.0899 |           0.2463 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0140 |           2.8208 |           0.2458 |
[32m[20221213 15:01:35 @agent_ppo2.py:185][0m |          -0.0195 |           2.8035 |           0.2460 |
[32m[20221213 15:01:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.05
[32m[20221213 15:01:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.41
[32m[20221213 15:01:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 96.32
[32m[20221213 15:01:35 @agent_ppo2.py:143][0m Total time:       8.67 min
[32m[20221213 15:01:35 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221213 15:01:35 @agent_ppo2.py:121][0m #------------------------ Iteration 381 --------------------------#
[32m[20221213 15:01:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0000 |           3.0784 |           0.2489 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0038 |           3.0206 |           0.2487 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0101 |           2.9782 |           0.2486 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0124 |           2.9483 |           0.2486 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0068 |           2.9970 |           0.2485 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0134 |           2.9058 |           0.2489 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0162 |           2.8878 |           0.2491 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0156 |           2.8808 |           0.2492 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0162 |           2.8577 |           0.2495 |
[32m[20221213 15:01:36 @agent_ppo2.py:185][0m |          -0.0143 |           2.8406 |           0.2495 |
[32m[20221213 15:01:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 102.55
[32m[20221213 15:01:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.61
[32m[20221213 15:01:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 121.96
[32m[20221213 15:01:37 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 121.96
[32m[20221213 15:01:37 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 121.96
[32m[20221213 15:01:37 @agent_ppo2.py:143][0m Total time:       8.69 min
[32m[20221213 15:01:37 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221213 15:01:37 @agent_ppo2.py:121][0m #------------------------ Iteration 382 --------------------------#
[32m[20221213 15:01:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:37 @agent_ppo2.py:185][0m |           0.0017 |           3.3691 |           0.2495 |
[32m[20221213 15:01:37 @agent_ppo2.py:185][0m |          -0.0052 |           3.2819 |           0.2493 |
[32m[20221213 15:01:37 @agent_ppo2.py:185][0m |          -0.0122 |           3.2506 |           0.2489 |
[32m[20221213 15:01:37 @agent_ppo2.py:185][0m |          -0.0113 |           3.2294 |           0.2486 |
[32m[20221213 15:01:37 @agent_ppo2.py:185][0m |          -0.0097 |           3.2111 |           0.2486 |
[32m[20221213 15:01:37 @agent_ppo2.py:185][0m |          -0.0077 |           3.3321 |           0.2484 |
[32m[20221213 15:01:38 @agent_ppo2.py:185][0m |          -0.0176 |           3.1861 |           0.2481 |
[32m[20221213 15:01:38 @agent_ppo2.py:185][0m |          -0.0201 |           3.1698 |           0.2480 |
[32m[20221213 15:01:38 @agent_ppo2.py:185][0m |          -0.0159 |           3.1519 |           0.2479 |
[32m[20221213 15:01:38 @agent_ppo2.py:185][0m |          -0.0182 |           3.1382 |           0.2478 |
[32m[20221213 15:01:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:01:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 100.71
[32m[20221213 15:01:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.98
[32m[20221213 15:01:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 109.14
[32m[20221213 15:01:38 @agent_ppo2.py:143][0m Total time:       8.71 min
[32m[20221213 15:01:38 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221213 15:01:38 @agent_ppo2.py:121][0m #------------------------ Iteration 383 --------------------------#
[32m[20221213 15:01:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:38 @agent_ppo2.py:185][0m |           0.0085 |           3.4820 |           0.2404 |
[32m[20221213 15:01:38 @agent_ppo2.py:185][0m |          -0.0093 |           3.2879 |           0.2403 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0099 |           3.2426 |           0.2402 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0144 |           3.2165 |           0.2401 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0040 |           3.2634 |           0.2401 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0088 |           3.2063 |           0.2398 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0150 |           3.1663 |           0.2398 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0058 |           3.2473 |           0.2398 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0207 |           3.1450 |           0.2396 |
[32m[20221213 15:01:39 @agent_ppo2.py:185][0m |          -0.0196 |           3.1523 |           0.2397 |
[32m[20221213 15:01:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:01:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.78
[32m[20221213 15:01:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.54
[32m[20221213 15:01:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.03
[32m[20221213 15:01:39 @agent_ppo2.py:143][0m Total time:       8.74 min
[32m[20221213 15:01:39 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221213 15:01:39 @agent_ppo2.py:121][0m #------------------------ Iteration 384 --------------------------#
[32m[20221213 15:01:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |           0.0006 |           3.1138 |           0.2444 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0052 |           3.0407 |           0.2440 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0093 |           3.0075 |           0.2441 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0047 |           3.0409 |           0.2441 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0120 |           2.9686 |           0.2439 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0127 |           2.9480 |           0.2438 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0090 |           3.0098 |           0.2437 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0152 |           2.9337 |           0.2437 |
[32m[20221213 15:01:40 @agent_ppo2.py:185][0m |          -0.0170 |           2.9533 |           0.2437 |
[32m[20221213 15:01:41 @agent_ppo2.py:185][0m |          -0.0186 |           2.9218 |           0.2436 |
[32m[20221213 15:01:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:01:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.32
[32m[20221213 15:01:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 111.76
[32m[20221213 15:01:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 85.42
[32m[20221213 15:01:41 @agent_ppo2.py:143][0m Total time:       8.76 min
[32m[20221213 15:01:41 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221213 15:01:41 @agent_ppo2.py:121][0m #------------------------ Iteration 385 --------------------------#
[32m[20221213 15:01:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:01:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:41 @agent_ppo2.py:185][0m |           0.0113 |           3.4098 |           0.2393 |
[32m[20221213 15:01:41 @agent_ppo2.py:185][0m |          -0.0076 |           3.0369 |           0.2387 |
[32m[20221213 15:01:41 @agent_ppo2.py:185][0m |          -0.0071 |           2.9548 |           0.2386 |
[32m[20221213 15:01:41 @agent_ppo2.py:185][0m |          -0.0063 |           3.0076 |           0.2387 |
[32m[20221213 15:01:41 @agent_ppo2.py:185][0m |          -0.0123 |           2.8893 |           0.2386 |
[32m[20221213 15:01:42 @agent_ppo2.py:185][0m |          -0.0114 |           2.8810 |           0.2385 |
[32m[20221213 15:01:42 @agent_ppo2.py:185][0m |          -0.0064 |           2.9962 |           0.2385 |
[32m[20221213 15:01:42 @agent_ppo2.py:185][0m |          -0.0180 |           2.8203 |           0.2385 |
[32m[20221213 15:01:42 @agent_ppo2.py:185][0m |          -0.0137 |           2.8105 |           0.2386 |
[32m[20221213 15:01:42 @agent_ppo2.py:185][0m |          -0.0137 |           2.7801 |           0.2386 |
[32m[20221213 15:01:42 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 15:01:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.82
[32m[20221213 15:01:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.09
[32m[20221213 15:01:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.50
[32m[20221213 15:01:42 @agent_ppo2.py:143][0m Total time:       8.78 min
[32m[20221213 15:01:42 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221213 15:01:42 @agent_ppo2.py:121][0m #------------------------ Iteration 386 --------------------------#
[32m[20221213 15:01:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:01:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:43 @agent_ppo2.py:185][0m |          -0.0017 |           3.2012 |           0.2427 |
[32m[20221213 15:01:43 @agent_ppo2.py:185][0m |          -0.0051 |           3.0982 |           0.2424 |
[32m[20221213 15:01:43 @agent_ppo2.py:185][0m |          -0.0073 |           3.0574 |           0.2420 |
[32m[20221213 15:01:43 @agent_ppo2.py:185][0m |          -0.0097 |           3.0231 |           0.2418 |
[32m[20221213 15:01:43 @agent_ppo2.py:185][0m |          -0.0094 |           3.0459 |           0.2417 |
[32m[20221213 15:01:43 @agent_ppo2.py:185][0m |          -0.0144 |           2.9805 |           0.2414 |
[32m[20221213 15:01:43 @agent_ppo2.py:185][0m |          -0.0145 |           2.9803 |           0.2413 |
[32m[20221213 15:01:44 @agent_ppo2.py:185][0m |          -0.0127 |           2.9506 |           0.2413 |
[32m[20221213 15:01:44 @agent_ppo2.py:185][0m |          -0.0168 |           2.9353 |           0.2411 |
[32m[20221213 15:01:44 @agent_ppo2.py:185][0m |          -0.0065 |           3.0787 |           0.2411 |
[32m[20221213 15:01:44 @agent_ppo2.py:130][0m Policy update time: 1.61 s
[32m[20221213 15:01:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.79
[32m[20221213 15:01:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.89
[32m[20221213 15:01:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.07
[32m[20221213 15:01:44 @agent_ppo2.py:143][0m Total time:       8.82 min
[32m[20221213 15:01:44 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221213 15:01:44 @agent_ppo2.py:121][0m #------------------------ Iteration 387 --------------------------#
[32m[20221213 15:01:45 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:45 @agent_ppo2.py:185][0m |          -0.0019 |           3.1066 |           0.2449 |
[32m[20221213 15:01:45 @agent_ppo2.py:185][0m |          -0.0061 |           2.9841 |           0.2446 |
[32m[20221213 15:01:45 @agent_ppo2.py:185][0m |          -0.0107 |           2.9457 |           0.2445 |
[32m[20221213 15:01:45 @agent_ppo2.py:185][0m |          -0.0104 |           2.9020 |           0.2442 |
[32m[20221213 15:01:45 @agent_ppo2.py:185][0m |          -0.0134 |           2.8766 |           0.2440 |
[32m[20221213 15:01:45 @agent_ppo2.py:185][0m |          -0.0149 |           2.8524 |           0.2440 |
[32m[20221213 15:01:45 @agent_ppo2.py:185][0m |          -0.0151 |           2.8322 |           0.2438 |
[32m[20221213 15:01:46 @agent_ppo2.py:185][0m |          -0.0154 |           2.8081 |           0.2437 |
[32m[20221213 15:01:46 @agent_ppo2.py:185][0m |          -0.0163 |           2.7977 |           0.2438 |
[32m[20221213 15:01:46 @agent_ppo2.py:185][0m |          -0.0200 |           2.7767 |           0.2437 |
[32m[20221213 15:01:46 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 15:01:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 100.83
[32m[20221213 15:01:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 115.03
[32m[20221213 15:01:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 108.84
[32m[20221213 15:01:46 @agent_ppo2.py:143][0m Total time:       8.85 min
[32m[20221213 15:01:46 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221213 15:01:46 @agent_ppo2.py:121][0m #------------------------ Iteration 388 --------------------------#
[32m[20221213 15:01:46 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:46 @agent_ppo2.py:185][0m |          -0.0029 |           3.0861 |           0.2386 |
[32m[20221213 15:01:46 @agent_ppo2.py:185][0m |          -0.0111 |           2.9587 |           0.2383 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0092 |           2.9100 |           0.2383 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0127 |           2.8713 |           0.2382 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0182 |           2.8518 |           0.2379 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0114 |           2.8859 |           0.2377 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0157 |           2.8216 |           0.2377 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0189 |           2.8110 |           0.2374 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0183 |           2.7950 |           0.2373 |
[32m[20221213 15:01:47 @agent_ppo2.py:185][0m |          -0.0192 |           2.7804 |           0.2372 |
[32m[20221213 15:01:47 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 15:01:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 112.04
[32m[20221213 15:01:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 117.43
[32m[20221213 15:01:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 103.38
[32m[20221213 15:01:48 @agent_ppo2.py:143][0m Total time:       8.87 min
[32m[20221213 15:01:48 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221213 15:01:48 @agent_ppo2.py:121][0m #------------------------ Iteration 389 --------------------------#
[32m[20221213 15:01:48 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:48 @agent_ppo2.py:185][0m |          -0.0034 |           3.3493 |           0.2379 |
[32m[20221213 15:01:48 @agent_ppo2.py:185][0m |          -0.0087 |           3.2056 |           0.2376 |
[32m[20221213 15:01:48 @agent_ppo2.py:185][0m |          -0.0097 |           3.1619 |           0.2374 |
[32m[20221213 15:01:48 @agent_ppo2.py:185][0m |          -0.0089 |           3.1590 |           0.2371 |
[32m[20221213 15:01:48 @agent_ppo2.py:185][0m |          -0.0068 |           3.1400 |           0.2369 |
[32m[20221213 15:01:49 @agent_ppo2.py:185][0m |          -0.0155 |           3.0861 |           0.2371 |
[32m[20221213 15:01:49 @agent_ppo2.py:185][0m |          -0.0168 |           3.0706 |           0.2366 |
[32m[20221213 15:01:49 @agent_ppo2.py:185][0m |          -0.0144 |           3.0511 |           0.2367 |
[32m[20221213 15:01:49 @agent_ppo2.py:185][0m |          -0.0192 |           3.0410 |           0.2364 |
[32m[20221213 15:01:49 @agent_ppo2.py:185][0m |          -0.0186 |           3.0351 |           0.2363 |
[32m[20221213 15:01:49 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 15:01:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.49
[32m[20221213 15:01:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.39
[32m[20221213 15:01:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 97.26
[32m[20221213 15:01:49 @agent_ppo2.py:143][0m Total time:       8.90 min
[32m[20221213 15:01:49 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221213 15:01:49 @agent_ppo2.py:121][0m #------------------------ Iteration 390 --------------------------#
[32m[20221213 15:01:50 @agent_ppo2.py:127][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 15:01:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:50 @agent_ppo2.py:185][0m |          -0.0021 |           3.2151 |           0.2431 |
[32m[20221213 15:01:50 @agent_ppo2.py:185][0m |          -0.0086 |           3.1368 |           0.2431 |
[32m[20221213 15:01:50 @agent_ppo2.py:185][0m |          -0.0075 |           3.1177 |           0.2431 |
[32m[20221213 15:01:50 @agent_ppo2.py:185][0m |          -0.0130 |           3.0817 |           0.2431 |
[32m[20221213 15:01:50 @agent_ppo2.py:185][0m |          -0.0130 |           3.0690 |           0.2433 |
[32m[20221213 15:01:50 @agent_ppo2.py:185][0m |          -0.0145 |           3.0578 |           0.2435 |
[32m[20221213 15:01:50 @agent_ppo2.py:185][0m |          -0.0121 |           3.0683 |           0.2436 |
[32m[20221213 15:01:51 @agent_ppo2.py:185][0m |          -0.0140 |           3.0287 |           0.2436 |
[32m[20221213 15:01:51 @agent_ppo2.py:185][0m |          -0.0144 |           3.0218 |           0.2437 |
[32m[20221213 15:01:51 @agent_ppo2.py:185][0m |          -0.0204 |           3.0128 |           0.2440 |
[32m[20221213 15:01:51 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 15:01:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.29
[32m[20221213 15:01:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 115.40
[32m[20221213 15:01:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 105.58
[32m[20221213 15:01:51 @agent_ppo2.py:143][0m Total time:       8.93 min
[32m[20221213 15:01:51 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221213 15:01:51 @agent_ppo2.py:121][0m #------------------------ Iteration 391 --------------------------#
[32m[20221213 15:01:51 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:51 @agent_ppo2.py:185][0m |          -0.0005 |           3.2254 |           0.2461 |
[32m[20221213 15:01:51 @agent_ppo2.py:185][0m |          -0.0077 |           3.1698 |           0.2456 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0093 |           3.1376 |           0.2451 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0065 |           3.1385 |           0.2450 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0140 |           3.1045 |           0.2449 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0164 |           3.0845 |           0.2448 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0012 |           3.2470 |           0.2447 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0129 |           3.0939 |           0.2447 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0168 |           3.0345 |           0.2448 |
[32m[20221213 15:01:52 @agent_ppo2.py:185][0m |          -0.0148 |           3.0620 |           0.2446 |
[32m[20221213 15:01:52 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:01:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.71
[32m[20221213 15:01:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.97
[32m[20221213 15:01:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 104.85
[32m[20221213 15:01:53 @agent_ppo2.py:143][0m Total time:       8.96 min
[32m[20221213 15:01:53 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221213 15:01:53 @agent_ppo2.py:121][0m #------------------------ Iteration 392 --------------------------#
[32m[20221213 15:01:53 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:53 @agent_ppo2.py:185][0m |          -0.0017 |           3.3104 |           0.2385 |
[32m[20221213 15:01:53 @agent_ppo2.py:185][0m |          -0.0094 |           3.1980 |           0.2386 |
[32m[20221213 15:01:53 @agent_ppo2.py:185][0m |          -0.0114 |           3.1575 |           0.2384 |
[32m[20221213 15:01:53 @agent_ppo2.py:185][0m |          -0.0118 |           3.1139 |           0.2384 |
[32m[20221213 15:01:53 @agent_ppo2.py:185][0m |          -0.0136 |           3.0989 |           0.2385 |
[32m[20221213 15:01:54 @agent_ppo2.py:185][0m |          -0.0139 |           3.0895 |           0.2387 |
[32m[20221213 15:01:54 @agent_ppo2.py:185][0m |          -0.0139 |           3.0727 |           0.2386 |
[32m[20221213 15:01:54 @agent_ppo2.py:185][0m |          -0.0176 |           3.0430 |           0.2387 |
[32m[20221213 15:01:54 @agent_ppo2.py:185][0m |          -0.0158 |           3.0344 |           0.2386 |
[32m[20221213 15:01:54 @agent_ppo2.py:185][0m |          -0.0179 |           3.0412 |           0.2390 |
[32m[20221213 15:01:54 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 15:01:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.89
[32m[20221213 15:01:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.02
[32m[20221213 15:01:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.87
[32m[20221213 15:01:54 @agent_ppo2.py:143][0m Total time:       8.98 min
[32m[20221213 15:01:54 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221213 15:01:54 @agent_ppo2.py:121][0m #------------------------ Iteration 393 --------------------------#
[32m[20221213 15:01:54 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0011 |           3.2752 |           0.2453 |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0089 |           3.2051 |           0.2452 |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0120 |           3.1609 |           0.2451 |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0120 |           3.1315 |           0.2451 |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0118 |           3.1241 |           0.2451 |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0117 |           3.0901 |           0.2452 |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0144 |           3.0730 |           0.2452 |
[32m[20221213 15:01:55 @agent_ppo2.py:185][0m |          -0.0196 |           3.0565 |           0.2453 |
[32m[20221213 15:01:56 @agent_ppo2.py:185][0m |          -0.0176 |           3.0567 |           0.2453 |
[32m[20221213 15:01:56 @agent_ppo2.py:185][0m |          -0.0181 |           3.0416 |           0.2455 |
[32m[20221213 15:01:56 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:01:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 105.72
[32m[20221213 15:01:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.50
[32m[20221213 15:01:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 117.12
[32m[20221213 15:01:56 @agent_ppo2.py:143][0m Total time:       9.01 min
[32m[20221213 15:01:56 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221213 15:01:56 @agent_ppo2.py:121][0m #------------------------ Iteration 394 --------------------------#
[32m[20221213 15:01:56 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:56 @agent_ppo2.py:185][0m |          -0.0003 |           3.2200 |           0.2440 |
[32m[20221213 15:01:56 @agent_ppo2.py:185][0m |          -0.0080 |           3.1695 |           0.2430 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |          -0.0116 |           3.1455 |           0.2424 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |           0.0006 |           3.3908 |           0.2418 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |          -0.0144 |           3.0993 |           0.2412 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |          -0.0155 |           3.0882 |           0.2410 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |          -0.0167 |           3.0717 |           0.2406 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |          -0.0164 |           3.0622 |           0.2404 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |          -0.0064 |           3.2059 |           0.2402 |
[32m[20221213 15:01:57 @agent_ppo2.py:185][0m |          -0.0076 |           3.2293 |           0.2398 |
[32m[20221213 15:01:57 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 15:01:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.47
[32m[20221213 15:01:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 110.84
[32m[20221213 15:01:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 106.63
[32m[20221213 15:01:58 @agent_ppo2.py:143][0m Total time:       9.04 min
[32m[20221213 15:01:58 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221213 15:01:58 @agent_ppo2.py:121][0m #------------------------ Iteration 395 --------------------------#
[32m[20221213 15:01:58 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:01:58 @agent_ppo2.py:185][0m |          -0.0011 |           3.1048 |           0.2351 |
[32m[20221213 15:01:58 @agent_ppo2.py:185][0m |          -0.0073 |           3.0498 |           0.2351 |
[32m[20221213 15:01:58 @agent_ppo2.py:185][0m |          -0.0099 |           3.0257 |           0.2349 |
[32m[20221213 15:01:58 @agent_ppo2.py:185][0m |          -0.0088 |           3.0138 |           0.2350 |
[32m[20221213 15:01:58 @agent_ppo2.py:185][0m |          -0.0129 |           2.9859 |           0.2348 |
[32m[20221213 15:01:59 @agent_ppo2.py:185][0m |          -0.0136 |           2.9713 |           0.2348 |
[32m[20221213 15:01:59 @agent_ppo2.py:185][0m |          -0.0091 |           2.9759 |           0.2348 |
[32m[20221213 15:01:59 @agent_ppo2.py:185][0m |          -0.0096 |           3.0003 |           0.2345 |
[32m[20221213 15:01:59 @agent_ppo2.py:185][0m |          -0.0151 |           2.9537 |           0.2345 |
[32m[20221213 15:01:59 @agent_ppo2.py:185][0m |          -0.0149 |           2.9540 |           0.2345 |
[32m[20221213 15:01:59 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:01:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.46
[32m[20221213 15:01:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.76
[32m[20221213 15:01:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.47
[32m[20221213 15:01:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 127.47
[32m[20221213 15:01:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 127.47
[32m[20221213 15:01:59 @agent_ppo2.py:143][0m Total time:       9.07 min
[32m[20221213 15:01:59 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221213 15:01:59 @agent_ppo2.py:121][0m #------------------------ Iteration 396 --------------------------#
[32m[20221213 15:01:59 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:01:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |           0.0054 |           3.0042 |           0.2421 |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |           0.0058 |           3.1664 |           0.2420 |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |          -0.0084 |           2.8580 |           0.2417 |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |          -0.0102 |           2.8220 |           0.2414 |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |          -0.0094 |           2.8135 |           0.2411 |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |          -0.0142 |           2.8166 |           0.2408 |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |          -0.0064 |           2.8649 |           0.2406 |
[32m[20221213 15:02:00 @agent_ppo2.py:185][0m |          -0.0135 |           2.7753 |           0.2405 |
[32m[20221213 15:02:01 @agent_ppo2.py:185][0m |          -0.0031 |           3.1264 |           0.2402 |
[32m[20221213 15:02:01 @agent_ppo2.py:185][0m |          -0.0110 |           2.7717 |           0.2400 |
[32m[20221213 15:02:01 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 15:02:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 92.64
[32m[20221213 15:02:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 104.75
[32m[20221213 15:02:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.08
[32m[20221213 15:02:01 @agent_ppo2.py:143][0m Total time:       9.09 min
[32m[20221213 15:02:01 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221213 15:02:01 @agent_ppo2.py:121][0m #------------------------ Iteration 397 --------------------------#
[32m[20221213 15:02:01 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:02:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:01 @agent_ppo2.py:185][0m |           0.0009 |           3.1131 |           0.2438 |
[32m[20221213 15:02:01 @agent_ppo2.py:185][0m |          -0.0056 |           3.0263 |           0.2438 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0097 |           2.9867 |           0.2436 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0086 |           2.9790 |           0.2438 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0103 |           2.9527 |           0.2438 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0139 |           2.9346 |           0.2439 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0177 |           2.9002 |           0.2439 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0155 |           2.8782 |           0.2439 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0163 |           2.8655 |           0.2440 |
[32m[20221213 15:02:02 @agent_ppo2.py:185][0m |          -0.0166 |           2.8395 |           0.2439 |
[32m[20221213 15:02:02 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:02:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.13
[32m[20221213 15:02:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.65
[32m[20221213 15:02:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 117.28
[32m[20221213 15:02:03 @agent_ppo2.py:143][0m Total time:       9.12 min
[32m[20221213 15:02:03 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221213 15:02:03 @agent_ppo2.py:121][0m #------------------------ Iteration 398 --------------------------#
[32m[20221213 15:02:03 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:02:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:03 @agent_ppo2.py:185][0m |          -0.0028 |           3.1799 |           0.2373 |
[32m[20221213 15:02:03 @agent_ppo2.py:185][0m |          -0.0041 |           3.1187 |           0.2370 |
[32m[20221213 15:02:03 @agent_ppo2.py:185][0m |          -0.0110 |           3.0805 |           0.2370 |
[32m[20221213 15:02:03 @agent_ppo2.py:185][0m |          -0.0133 |           3.0525 |           0.2369 |
[32m[20221213 15:02:03 @agent_ppo2.py:185][0m |          -0.0151 |           3.0236 |           0.2369 |
[32m[20221213 15:02:04 @agent_ppo2.py:185][0m |          -0.0137 |           3.0096 |           0.2371 |
[32m[20221213 15:02:04 @agent_ppo2.py:185][0m |          -0.0100 |           3.0850 |           0.2371 |
[32m[20221213 15:02:04 @agent_ppo2.py:185][0m |          -0.0153 |           2.9902 |           0.2372 |
[32m[20221213 15:02:04 @agent_ppo2.py:185][0m |          -0.0164 |           2.9501 |           0.2372 |
[32m[20221213 15:02:04 @agent_ppo2.py:185][0m |          -0.0194 |           2.9358 |           0.2372 |
[32m[20221213 15:02:04 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221213 15:02:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 105.65
[32m[20221213 15:02:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.44
[32m[20221213 15:02:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 111.75
[32m[20221213 15:02:04 @agent_ppo2.py:143][0m Total time:       9.15 min
[32m[20221213 15:02:04 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221213 15:02:04 @agent_ppo2.py:121][0m #------------------------ Iteration 399 --------------------------#
[32m[20221213 15:02:04 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:02:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |           0.0030 |           3.3826 |           0.2412 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0044 |           3.2390 |           0.2412 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0079 |           3.1794 |           0.2407 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0113 |           3.1451 |           0.2407 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0051 |           3.1265 |           0.2403 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0138 |           3.0760 |           0.2402 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0124 |           3.0567 |           0.2401 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0147 |           3.0349 |           0.2400 |
[32m[20221213 15:02:05 @agent_ppo2.py:185][0m |          -0.0037 |           3.2527 |           0.2400 |
[32m[20221213 15:02:06 @agent_ppo2.py:185][0m |          -0.0176 |           3.0071 |           0.2394 |
[32m[20221213 15:02:06 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 15:02:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.45
[32m[20221213 15:02:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 115.83
[32m[20221213 15:02:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 105.69
[32m[20221213 15:02:06 @agent_ppo2.py:143][0m Total time:       9.18 min
[32m[20221213 15:02:06 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221213 15:02:06 @agent_ppo2.py:121][0m #------------------------ Iteration 400 --------------------------#
[32m[20221213 15:02:06 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 15:02:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:06 @agent_ppo2.py:185][0m |          -0.0038 |           3.2755 |           0.2390 |
[32m[20221213 15:02:06 @agent_ppo2.py:185][0m |          -0.0115 |           3.1659 |           0.2392 |
[32m[20221213 15:02:06 @agent_ppo2.py:185][0m |           0.0023 |           3.4457 |           0.2388 |
[32m[20221213 15:02:06 @agent_ppo2.py:185][0m |          -0.0155 |           3.1012 |           0.2386 |
[32m[20221213 15:02:07 @agent_ppo2.py:185][0m |          -0.0152 |           3.0641 |           0.2385 |
[32m[20221213 15:02:07 @agent_ppo2.py:185][0m |          -0.0159 |           3.0339 |           0.2385 |
[32m[20221213 15:02:07 @agent_ppo2.py:185][0m |          -0.0119 |           3.0315 |           0.2386 |
[32m[20221213 15:02:07 @agent_ppo2.py:185][0m |          -0.0155 |           2.9905 |           0.2386 |
[32m[20221213 15:02:07 @agent_ppo2.py:185][0m |          -0.0161 |           2.9697 |           0.2385 |
[32m[20221213 15:02:07 @agent_ppo2.py:185][0m |          -0.0180 |           2.9460 |           0.2385 |
[32m[20221213 15:02:07 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:02:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.24
[32m[20221213 15:02:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.60
[32m[20221213 15:02:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 114.79
[32m[20221213 15:02:07 @agent_ppo2.py:143][0m Total time:       9.20 min
[32m[20221213 15:02:07 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221213 15:02:07 @agent_ppo2.py:121][0m #------------------------ Iteration 401 --------------------------#
[32m[20221213 15:02:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |           0.0111 |           3.4905 |           0.2418 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0074 |           3.2315 |           0.2416 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0059 |           3.1919 |           0.2416 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0106 |           3.1682 |           0.2415 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0151 |           3.1360 |           0.2417 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0130 |           3.1166 |           0.2419 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0163 |           3.0850 |           0.2417 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0203 |           3.0745 |           0.2418 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0170 |           3.0566 |           0.2419 |
[32m[20221213 15:02:08 @agent_ppo2.py:185][0m |          -0.0183 |           3.0317 |           0.2420 |
[32m[20221213 15:02:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.03
[32m[20221213 15:02:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 120.36
[32m[20221213 15:02:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 113.14
[32m[20221213 15:02:09 @agent_ppo2.py:143][0m Total time:       9.22 min
[32m[20221213 15:02:09 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221213 15:02:09 @agent_ppo2.py:121][0m #------------------------ Iteration 402 --------------------------#
[32m[20221213 15:02:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:09 @agent_ppo2.py:185][0m |          -0.0048 |           3.3841 |           0.2337 |
[32m[20221213 15:02:09 @agent_ppo2.py:185][0m |          -0.0099 |           3.3191 |           0.2331 |
[32m[20221213 15:02:09 @agent_ppo2.py:185][0m |          -0.0114 |           3.2953 |           0.2330 |
[32m[20221213 15:02:09 @agent_ppo2.py:185][0m |          -0.0134 |           3.2587 |           0.2328 |
[32m[20221213 15:02:09 @agent_ppo2.py:185][0m |          -0.0128 |           3.2413 |           0.2328 |
[32m[20221213 15:02:09 @agent_ppo2.py:185][0m |          -0.0167 |           3.2402 |           0.2327 |
[32m[20221213 15:02:10 @agent_ppo2.py:185][0m |          -0.0166 |           3.2174 |           0.2325 |
[32m[20221213 15:02:10 @agent_ppo2.py:185][0m |          -0.0136 |           3.2043 |           0.2326 |
[32m[20221213 15:02:10 @agent_ppo2.py:185][0m |          -0.0126 |           3.2237 |           0.2324 |
[32m[20221213 15:02:10 @agent_ppo2.py:185][0m |          -0.0127 |           3.1805 |           0.2323 |
[32m[20221213 15:02:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:02:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 113.67
[32m[20221213 15:02:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 116.14
[32m[20221213 15:02:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 117.05
[32m[20221213 15:02:10 @agent_ppo2.py:143][0m Total time:       9.25 min
[32m[20221213 15:02:10 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221213 15:02:10 @agent_ppo2.py:121][0m #------------------------ Iteration 403 --------------------------#
[32m[20221213 15:02:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:10 @agent_ppo2.py:185][0m |          -0.0016 |           3.3655 |           0.2448 |
[32m[20221213 15:02:10 @agent_ppo2.py:185][0m |          -0.0001 |           3.3464 |           0.2443 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0081 |           3.2659 |           0.2440 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0104 |           3.2473 |           0.2439 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0117 |           3.2319 |           0.2437 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0129 |           3.2188 |           0.2436 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0120 |           3.2072 |           0.2438 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0149 |           3.1906 |           0.2436 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0154 |           3.1997 |           0.2437 |
[32m[20221213 15:02:11 @agent_ppo2.py:185][0m |          -0.0110 |           3.2158 |           0.2435 |
[32m[20221213 15:02:11 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:02:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.12
[32m[20221213 15:02:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.52
[32m[20221213 15:02:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.16
[32m[20221213 15:02:11 @agent_ppo2.py:143][0m Total time:       9.27 min
[32m[20221213 15:02:11 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221213 15:02:11 @agent_ppo2.py:121][0m #------------------------ Iteration 404 --------------------------#
[32m[20221213 15:02:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0014 |           3.3964 |           0.2384 |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0057 |           3.3222 |           0.2381 |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0119 |           3.2981 |           0.2380 |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0132 |           3.2743 |           0.2378 |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0102 |           3.2673 |           0.2378 |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0102 |           3.3818 |           0.2378 |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0137 |           3.2426 |           0.2378 |
[32m[20221213 15:02:12 @agent_ppo2.py:185][0m |          -0.0104 |           3.3037 |           0.2379 |
[32m[20221213 15:02:13 @agent_ppo2.py:185][0m |          -0.0172 |           3.1966 |           0.2377 |
[32m[20221213 15:02:13 @agent_ppo2.py:185][0m |          -0.0028 |           3.3915 |           0.2380 |
[32m[20221213 15:02:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:02:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.66
[32m[20221213 15:02:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.07
[32m[20221213 15:02:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 112.39
[32m[20221213 15:02:13 @agent_ppo2.py:143][0m Total time:       9.29 min
[32m[20221213 15:02:13 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221213 15:02:13 @agent_ppo2.py:121][0m #------------------------ Iteration 405 --------------------------#
[32m[20221213 15:02:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:13 @agent_ppo2.py:185][0m |          -0.0020 |           3.4339 |           0.2381 |
[32m[20221213 15:02:13 @agent_ppo2.py:185][0m |          -0.0045 |           3.2994 |           0.2379 |
[32m[20221213 15:02:13 @agent_ppo2.py:185][0m |          -0.0125 |           3.2702 |           0.2376 |
[32m[20221213 15:02:13 @agent_ppo2.py:185][0m |          -0.0058 |           3.3355 |           0.2374 |
[32m[20221213 15:02:14 @agent_ppo2.py:185][0m |          -0.0057 |           3.4435 |           0.2374 |
[32m[20221213 15:02:14 @agent_ppo2.py:185][0m |          -0.0106 |           3.1986 |           0.2374 |
[32m[20221213 15:02:14 @agent_ppo2.py:185][0m |          -0.0102 |           3.1674 |           0.2372 |
[32m[20221213 15:02:14 @agent_ppo2.py:185][0m |          -0.0130 |           3.1458 |           0.2372 |
[32m[20221213 15:02:14 @agent_ppo2.py:185][0m |          -0.0162 |           3.1113 |           0.2371 |
[32m[20221213 15:02:14 @agent_ppo2.py:185][0m |          -0.0111 |           3.1360 |           0.2371 |
[32m[20221213 15:02:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:02:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 104.66
[32m[20221213 15:02:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.56
[32m[20221213 15:02:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.25
[32m[20221213 15:02:14 @agent_ppo2.py:143][0m Total time:       9.32 min
[32m[20221213 15:02:14 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221213 15:02:14 @agent_ppo2.py:121][0m #------------------------ Iteration 406 --------------------------#
[32m[20221213 15:02:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0029 |           3.5767 |           0.2443 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0025 |           3.4481 |           0.2437 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0064 |           3.4031 |           0.2435 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0040 |           3.5287 |           0.2435 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0140 |           3.3405 |           0.2432 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0029 |           3.7872 |           0.2431 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0172 |           3.3070 |           0.2432 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0202 |           3.2836 |           0.2429 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0174 |           3.2596 |           0.2432 |
[32m[20221213 15:02:15 @agent_ppo2.py:185][0m |          -0.0150 |           3.2534 |           0.2428 |
[32m[20221213 15:02:15 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 107.38
[32m[20221213 15:02:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 113.70
[32m[20221213 15:02:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 108.63
[32m[20221213 15:02:16 @agent_ppo2.py:143][0m Total time:       9.34 min
[32m[20221213 15:02:16 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221213 15:02:16 @agent_ppo2.py:121][0m #------------------------ Iteration 407 --------------------------#
[32m[20221213 15:02:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:16 @agent_ppo2.py:185][0m |           0.0029 |           3.3004 |           0.2406 |
[32m[20221213 15:02:16 @agent_ppo2.py:185][0m |          -0.0057 |           3.1410 |           0.2404 |
[32m[20221213 15:02:16 @agent_ppo2.py:185][0m |          -0.0114 |           3.0899 |           0.2403 |
[32m[20221213 15:02:16 @agent_ppo2.py:185][0m |          -0.0124 |           3.0461 |           0.2404 |
[32m[20221213 15:02:16 @agent_ppo2.py:185][0m |          -0.0037 |           3.1915 |           0.2401 |
[32m[20221213 15:02:16 @agent_ppo2.py:185][0m |          -0.0134 |           2.9763 |           0.2402 |
[32m[20221213 15:02:17 @agent_ppo2.py:185][0m |          -0.0144 |           2.9411 |           0.2402 |
[32m[20221213 15:02:17 @agent_ppo2.py:185][0m |          -0.0123 |           2.9262 |           0.2403 |
[32m[20221213 15:02:17 @agent_ppo2.py:185][0m |          -0.0179 |           2.9060 |           0.2402 |
[32m[20221213 15:02:17 @agent_ppo2.py:185][0m |          -0.0156 |           2.8815 |           0.2404 |
[32m[20221213 15:02:17 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:02:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 107.60
[32m[20221213 15:02:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 121.82
[32m[20221213 15:02:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 101.89
[32m[20221213 15:02:17 @agent_ppo2.py:143][0m Total time:       9.36 min
[32m[20221213 15:02:17 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221213 15:02:17 @agent_ppo2.py:121][0m #------------------------ Iteration 408 --------------------------#
[32m[20221213 15:02:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:17 @agent_ppo2.py:185][0m |           0.0005 |           3.5041 |           0.2412 |
[32m[20221213 15:02:17 @agent_ppo2.py:185][0m |           0.0016 |           3.4073 |           0.2411 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0111 |           3.2891 |           0.2409 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0114 |           3.2422 |           0.2409 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0136 |           3.2129 |           0.2410 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0076 |           3.3806 |           0.2409 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0153 |           3.1552 |           0.2408 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0149 |           3.1439 |           0.2409 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0124 |           3.1263 |           0.2410 |
[32m[20221213 15:02:18 @agent_ppo2.py:185][0m |          -0.0168 |           3.1009 |           0.2410 |
[32m[20221213 15:02:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.45
[32m[20221213 15:02:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 114.09
[32m[20221213 15:02:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.69
[32m[20221213 15:02:18 @agent_ppo2.py:143][0m Total time:       9.39 min
[32m[20221213 15:02:18 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221213 15:02:18 @agent_ppo2.py:121][0m #------------------------ Iteration 409 --------------------------#
[32m[20221213 15:02:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0021 |           3.6629 |           0.2461 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0060 |           3.5046 |           0.2457 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0056 |           3.4768 |           0.2457 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0100 |           3.4034 |           0.2454 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0128 |           3.3738 |           0.2454 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0039 |           3.6464 |           0.2455 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0153 |           3.3470 |           0.2454 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0157 |           3.3164 |           0.2454 |
[32m[20221213 15:02:19 @agent_ppo2.py:185][0m |          -0.0156 |           3.2948 |           0.2453 |
[32m[20221213 15:02:20 @agent_ppo2.py:185][0m |          -0.0126 |           3.3144 |           0.2453 |
[32m[20221213 15:02:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:02:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 94.81
[32m[20221213 15:02:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.93
[32m[20221213 15:02:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 107.55
[32m[20221213 15:02:20 @agent_ppo2.py:143][0m Total time:       9.41 min
[32m[20221213 15:02:20 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221213 15:02:20 @agent_ppo2.py:121][0m #------------------------ Iteration 410 --------------------------#
[32m[20221213 15:02:20 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:02:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:20 @agent_ppo2.py:185][0m |           0.0005 |           3.4704 |           0.2475 |
[32m[20221213 15:02:20 @agent_ppo2.py:185][0m |          -0.0084 |           3.3761 |           0.2472 |
[32m[20221213 15:02:20 @agent_ppo2.py:185][0m |          -0.0111 |           3.3257 |           0.2472 |
[32m[20221213 15:02:20 @agent_ppo2.py:185][0m |          -0.0124 |           3.3090 |           0.2470 |
[32m[20221213 15:02:20 @agent_ppo2.py:185][0m |          -0.0143 |           3.2818 |           0.2474 |
[32m[20221213 15:02:21 @agent_ppo2.py:185][0m |          -0.0160 |           3.2568 |           0.2474 |
[32m[20221213 15:02:21 @agent_ppo2.py:185][0m |          -0.0088 |           3.3661 |           0.2477 |
[32m[20221213 15:02:21 @agent_ppo2.py:185][0m |          -0.0125 |           3.2989 |           0.2479 |
[32m[20221213 15:02:21 @agent_ppo2.py:185][0m |          -0.0130 |           3.2289 |           0.2480 |
[32m[20221213 15:02:21 @agent_ppo2.py:185][0m |          -0.0127 |           3.2210 |           0.2482 |
[32m[20221213 15:02:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.89
[32m[20221213 15:02:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 109.17
[32m[20221213 15:02:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.60
[32m[20221213 15:02:21 @agent_ppo2.py:143][0m Total time:       9.43 min
[32m[20221213 15:02:21 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221213 15:02:21 @agent_ppo2.py:121][0m #------------------------ Iteration 411 --------------------------#
[32m[20221213 15:02:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:21 @agent_ppo2.py:185][0m |           0.0004 |           3.2356 |           0.2516 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0082 |           3.1277 |           0.2513 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0102 |           3.0973 |           0.2513 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0126 |           3.0559 |           0.2514 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0133 |           3.0163 |           0.2514 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0162 |           3.0037 |           0.2515 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0183 |           2.9652 |           0.2515 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0145 |           2.9474 |           0.2517 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0147 |           2.9431 |           0.2517 |
[32m[20221213 15:02:22 @agent_ppo2.py:185][0m |          -0.0167 |           2.9092 |           0.2517 |
[32m[20221213 15:02:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:02:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.39
[32m[20221213 15:02:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 119.70
[32m[20221213 15:02:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 116.79
[32m[20221213 15:02:23 @agent_ppo2.py:143][0m Total time:       9.46 min
[32m[20221213 15:02:23 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221213 15:02:23 @agent_ppo2.py:121][0m #------------------------ Iteration 412 --------------------------#
[32m[20221213 15:02:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:23 @agent_ppo2.py:185][0m |          -0.0025 |           3.5104 |           0.2542 |
[32m[20221213 15:02:23 @agent_ppo2.py:185][0m |          -0.0065 |           3.3216 |           0.2538 |
[32m[20221213 15:02:23 @agent_ppo2.py:185][0m |          -0.0120 |           3.2593 |           0.2535 |
[32m[20221213 15:02:23 @agent_ppo2.py:185][0m |          -0.0118 |           3.2380 |           0.2533 |
[32m[20221213 15:02:23 @agent_ppo2.py:185][0m |          -0.0073 |           3.2209 |           0.2530 |
[32m[20221213 15:02:23 @agent_ppo2.py:185][0m |          -0.0045 |           3.2026 |           0.2528 |
[32m[20221213 15:02:23 @agent_ppo2.py:185][0m |          -0.0174 |           3.1622 |           0.2528 |
[32m[20221213 15:02:24 @agent_ppo2.py:185][0m |          -0.0157 |           3.1347 |           0.2528 |
[32m[20221213 15:02:24 @agent_ppo2.py:185][0m |          -0.0165 |           3.1284 |           0.2527 |
[32m[20221213 15:02:24 @agent_ppo2.py:185][0m |          -0.0172 |           3.1220 |           0.2524 |
[32m[20221213 15:02:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.74
[32m[20221213 15:02:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.39
[32m[20221213 15:02:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 43.82
[32m[20221213 15:02:24 @agent_ppo2.py:143][0m Total time:       9.48 min
[32m[20221213 15:02:24 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221213 15:02:24 @agent_ppo2.py:121][0m #------------------------ Iteration 413 --------------------------#
[32m[20221213 15:02:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:24 @agent_ppo2.py:185][0m |           0.0135 |           3.9202 |           0.2466 |
[32m[20221213 15:02:24 @agent_ppo2.py:185][0m |          -0.0043 |           3.4920 |           0.2463 |
[32m[20221213 15:02:24 @agent_ppo2.py:185][0m |          -0.0090 |           3.4247 |           0.2467 |
[32m[20221213 15:02:25 @agent_ppo2.py:185][0m |          -0.0176 |           3.3990 |           0.2469 |
[32m[20221213 15:02:25 @agent_ppo2.py:185][0m |          -0.0022 |           3.7168 |           0.2469 |
[32m[20221213 15:02:25 @agent_ppo2.py:185][0m |          -0.0153 |           3.3926 |           0.2470 |
[32m[20221213 15:02:25 @agent_ppo2.py:185][0m |          -0.0202 |           3.3251 |           0.2470 |
[32m[20221213 15:02:25 @agent_ppo2.py:185][0m |          -0.0177 |           3.3019 |           0.2473 |
[32m[20221213 15:02:25 @agent_ppo2.py:185][0m |          -0.0188 |           3.3209 |           0.2473 |
[32m[20221213 15:02:25 @agent_ppo2.py:185][0m |          -0.0168 |           3.2891 |           0.2474 |
[32m[20221213 15:02:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:02:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.99
[32m[20221213 15:02:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.82
[32m[20221213 15:02:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 103.97
[32m[20221213 15:02:25 @agent_ppo2.py:143][0m Total time:       9.50 min
[32m[20221213 15:02:25 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221213 15:02:25 @agent_ppo2.py:121][0m #------------------------ Iteration 414 --------------------------#
[32m[20221213 15:02:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |           0.0022 |           3.6843 |           0.2516 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0056 |           3.5222 |           0.2511 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0115 |           3.5045 |           0.2507 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0130 |           3.4241 |           0.2503 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0163 |           3.4056 |           0.2501 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0073 |           3.4426 |           0.2502 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0180 |           3.3462 |           0.2501 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0163 |           3.3147 |           0.2500 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0174 |           3.3170 |           0.2500 |
[32m[20221213 15:02:26 @agent_ppo2.py:185][0m |          -0.0132 |           3.3063 |           0.2500 |
[32m[20221213 15:02:26 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:02:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.51
[32m[20221213 15:02:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.61
[32m[20221213 15:02:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.09
[32m[20221213 15:02:27 @agent_ppo2.py:143][0m Total time:       9.52 min
[32m[20221213 15:02:27 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221213 15:02:27 @agent_ppo2.py:121][0m #------------------------ Iteration 415 --------------------------#
[32m[20221213 15:02:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:27 @agent_ppo2.py:185][0m |          -0.0028 |           3.5523 |           0.2579 |
[32m[20221213 15:02:27 @agent_ppo2.py:185][0m |          -0.0079 |           3.4282 |           0.2575 |
[32m[20221213 15:02:27 @agent_ppo2.py:185][0m |          -0.0084 |           3.3657 |           0.2576 |
[32m[20221213 15:02:27 @agent_ppo2.py:185][0m |          -0.0024 |           3.4667 |           0.2575 |
[32m[20221213 15:02:27 @agent_ppo2.py:185][0m |          -0.0123 |           3.3373 |           0.2574 |
[32m[20221213 15:02:28 @agent_ppo2.py:185][0m |          -0.0125 |           3.3091 |           0.2573 |
[32m[20221213 15:02:28 @agent_ppo2.py:185][0m |          -0.0133 |           3.2667 |           0.2572 |
[32m[20221213 15:02:28 @agent_ppo2.py:185][0m |          -0.0172 |           3.2384 |           0.2573 |
[32m[20221213 15:02:28 @agent_ppo2.py:185][0m |          -0.0141 |           3.2468 |           0.2574 |
[32m[20221213 15:02:28 @agent_ppo2.py:185][0m |          -0.0177 |           3.2025 |           0.2573 |
[32m[20221213 15:02:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:02:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.46
[32m[20221213 15:02:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 116.65
[32m[20221213 15:02:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 106.45
[32m[20221213 15:02:28 @agent_ppo2.py:143][0m Total time:       9.55 min
[32m[20221213 15:02:28 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221213 15:02:28 @agent_ppo2.py:121][0m #------------------------ Iteration 416 --------------------------#
[32m[20221213 15:02:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:28 @agent_ppo2.py:185][0m |           0.0025 |           3.7816 |           0.2560 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0072 |           3.6734 |           0.2559 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0097 |           3.6210 |           0.2559 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |           0.0017 |           3.9269 |           0.2559 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0076 |           3.5674 |           0.2557 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0098 |           3.5226 |           0.2559 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0121 |           3.4945 |           0.2558 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0130 |           3.4909 |           0.2561 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0166 |           3.4835 |           0.2563 |
[32m[20221213 15:02:29 @agent_ppo2.py:185][0m |          -0.0118 |           3.4640 |           0.2563 |
[32m[20221213 15:02:29 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.28
[32m[20221213 15:02:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 115.31
[32m[20221213 15:02:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.94
[32m[20221213 15:02:29 @agent_ppo2.py:143][0m Total time:       9.57 min
[32m[20221213 15:02:29 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221213 15:02:29 @agent_ppo2.py:121][0m #------------------------ Iteration 417 --------------------------#
[32m[20221213 15:02:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |           0.0024 |           3.6787 |           0.2506 |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |           0.0015 |           3.6590 |           0.2502 |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |          -0.0102 |           3.5330 |           0.2498 |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |          -0.0083 |           3.5133 |           0.2496 |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |          -0.0145 |           3.4567 |           0.2497 |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |          -0.0062 |           3.4916 |           0.2497 |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |          -0.0153 |           3.3988 |           0.2496 |
[32m[20221213 15:02:30 @agent_ppo2.py:185][0m |          -0.0154 |           3.3659 |           0.2496 |
[32m[20221213 15:02:31 @agent_ppo2.py:185][0m |          -0.0156 |           3.3624 |           0.2497 |
[32m[20221213 15:02:31 @agent_ppo2.py:185][0m |          -0.0119 |           3.3494 |           0.2495 |
[32m[20221213 15:02:31 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:02:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.22
[32m[20221213 15:02:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 116.58
[32m[20221213 15:02:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.33
[32m[20221213 15:02:31 @agent_ppo2.py:143][0m Total time:       9.59 min
[32m[20221213 15:02:31 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221213 15:02:31 @agent_ppo2.py:121][0m #------------------------ Iteration 418 --------------------------#
[32m[20221213 15:02:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:31 @agent_ppo2.py:185][0m |          -0.0023 |           3.7186 |           0.2547 |
[32m[20221213 15:02:31 @agent_ppo2.py:185][0m |          -0.0115 |           3.5775 |           0.2543 |
[32m[20221213 15:02:31 @agent_ppo2.py:185][0m |          -0.0104 |           3.5120 |           0.2540 |
[32m[20221213 15:02:32 @agent_ppo2.py:185][0m |          -0.0141 |           3.4728 |           0.2539 |
[32m[20221213 15:02:32 @agent_ppo2.py:185][0m |          -0.0133 |           3.4272 |           0.2538 |
[32m[20221213 15:02:32 @agent_ppo2.py:185][0m |          -0.0141 |           3.3890 |           0.2535 |
[32m[20221213 15:02:32 @agent_ppo2.py:185][0m |          -0.0172 |           3.3730 |           0.2535 |
[32m[20221213 15:02:32 @agent_ppo2.py:185][0m |          -0.0167 |           3.3418 |           0.2532 |
[32m[20221213 15:02:32 @agent_ppo2.py:185][0m |          -0.0151 |           3.3615 |           0.2531 |
[32m[20221213 15:02:32 @agent_ppo2.py:185][0m |          -0.0079 |           3.4683 |           0.2531 |
[32m[20221213 15:02:32 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:02:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.83
[32m[20221213 15:02:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.62
[32m[20221213 15:02:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 114.62
[32m[20221213 15:02:32 @agent_ppo2.py:143][0m Total time:       9.62 min
[32m[20221213 15:02:32 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221213 15:02:32 @agent_ppo2.py:121][0m #------------------------ Iteration 419 --------------------------#
[32m[20221213 15:02:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0035 |           3.6960 |           0.2567 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0077 |           3.5599 |           0.2562 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0007 |           3.9746 |           0.2558 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0124 |           3.4839 |           0.2553 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0132 |           3.4911 |           0.2552 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0085 |           3.5986 |           0.2553 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0156 |           3.4262 |           0.2548 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0177 |           3.3978 |           0.2547 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0186 |           3.3696 |           0.2544 |
[32m[20221213 15:02:33 @agent_ppo2.py:185][0m |          -0.0210 |           3.3633 |           0.2542 |
[32m[20221213 15:02:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.56
[32m[20221213 15:02:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 120.38
[32m[20221213 15:02:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.35
[32m[20221213 15:02:34 @agent_ppo2.py:143][0m Total time:       9.64 min
[32m[20221213 15:02:34 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221213 15:02:34 @agent_ppo2.py:121][0m #------------------------ Iteration 420 --------------------------#
[32m[20221213 15:02:34 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:02:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:34 @agent_ppo2.py:185][0m |          -0.0021 |           3.4348 |           0.2479 |
[32m[20221213 15:02:34 @agent_ppo2.py:185][0m |          -0.0101 |           3.3422 |           0.2479 |
[32m[20221213 15:02:34 @agent_ppo2.py:185][0m |          -0.0093 |           3.3027 |           0.2478 |
[32m[20221213 15:02:34 @agent_ppo2.py:185][0m |          -0.0109 |           3.2699 |           0.2479 |
[32m[20221213 15:02:34 @agent_ppo2.py:185][0m |          -0.0100 |           3.2598 |           0.2480 |
[32m[20221213 15:02:34 @agent_ppo2.py:185][0m |          -0.0175 |           3.2274 |           0.2482 |
[32m[20221213 15:02:35 @agent_ppo2.py:185][0m |          -0.0136 |           3.2148 |           0.2482 |
[32m[20221213 15:02:35 @agent_ppo2.py:185][0m |          -0.0178 |           3.1962 |           0.2483 |
[32m[20221213 15:02:35 @agent_ppo2.py:185][0m |          -0.0128 |           3.2187 |           0.2484 |
[32m[20221213 15:02:35 @agent_ppo2.py:185][0m |          -0.0159 |           3.1749 |           0.2486 |
[32m[20221213 15:02:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:02:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 107.82
[32m[20221213 15:02:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 117.10
[32m[20221213 15:02:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.29
[32m[20221213 15:02:35 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 130.29
[32m[20221213 15:02:35 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 130.29
[32m[20221213 15:02:35 @agent_ppo2.py:143][0m Total time:       9.66 min
[32m[20221213 15:02:35 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221213 15:02:35 @agent_ppo2.py:121][0m #------------------------ Iteration 421 --------------------------#
[32m[20221213 15:02:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:35 @agent_ppo2.py:185][0m |          -0.0042 |           3.6962 |           0.2516 |
[32m[20221213 15:02:35 @agent_ppo2.py:185][0m |          -0.0090 |           3.5945 |           0.2510 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0102 |           3.5666 |           0.2506 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0157 |           3.5287 |           0.2502 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0134 |           3.5050 |           0.2499 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0140 |           3.4791 |           0.2496 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0046 |           3.9437 |           0.2494 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0162 |           3.4647 |           0.2491 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0158 |           3.4559 |           0.2490 |
[32m[20221213 15:02:36 @agent_ppo2.py:185][0m |          -0.0117 |           3.4645 |           0.2490 |
[32m[20221213 15:02:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.97
[32m[20221213 15:02:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 130.37
[32m[20221213 15:02:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 45.55
[32m[20221213 15:02:36 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221213 15:02:36 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221213 15:02:36 @agent_ppo2.py:121][0m #------------------------ Iteration 422 --------------------------#
[32m[20221213 15:02:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |          -0.0012 |           3.5588 |           0.2465 |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |           0.0041 |           3.5755 |           0.2461 |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |          -0.0077 |           3.4257 |           0.2456 |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |          -0.0043 |           3.4731 |           0.2457 |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |          -0.0107 |           3.3635 |           0.2454 |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |          -0.0119 |           3.3521 |           0.2455 |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |           0.0084 |           3.8925 |           0.2455 |
[32m[20221213 15:02:37 @agent_ppo2.py:185][0m |          -0.0011 |           3.7046 |           0.2449 |
[32m[20221213 15:02:38 @agent_ppo2.py:185][0m |          -0.0102 |           3.3249 |           0.2450 |
[32m[20221213 15:02:38 @agent_ppo2.py:185][0m |          -0.0091 |           3.3073 |           0.2453 |
[32m[20221213 15:02:38 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:02:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 113.83
[32m[20221213 15:02:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.30
[32m[20221213 15:02:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.47
[32m[20221213 15:02:38 @agent_ppo2.py:143][0m Total time:       9.71 min
[32m[20221213 15:02:38 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221213 15:02:38 @agent_ppo2.py:121][0m #------------------------ Iteration 423 --------------------------#
[32m[20221213 15:02:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:38 @agent_ppo2.py:185][0m |          -0.0025 |           3.5640 |           0.2454 |
[32m[20221213 15:02:38 @agent_ppo2.py:185][0m |          -0.0059 |           3.4401 |           0.2454 |
[32m[20221213 15:02:38 @agent_ppo2.py:185][0m |          -0.0098 |           3.3790 |           0.2454 |
[32m[20221213 15:02:38 @agent_ppo2.py:185][0m |          -0.0090 |           3.3585 |           0.2454 |
[32m[20221213 15:02:39 @agent_ppo2.py:185][0m |          -0.0142 |           3.3341 |           0.2455 |
[32m[20221213 15:02:39 @agent_ppo2.py:185][0m |          -0.0131 |           3.3033 |           0.2456 |
[32m[20221213 15:02:39 @agent_ppo2.py:185][0m |          -0.0151 |           3.2703 |           0.2456 |
[32m[20221213 15:02:39 @agent_ppo2.py:185][0m |          -0.0149 |           3.2618 |           0.2458 |
[32m[20221213 15:02:39 @agent_ppo2.py:185][0m |          -0.0161 |           3.2538 |           0.2457 |
[32m[20221213 15:02:39 @agent_ppo2.py:185][0m |          -0.0130 |           3.2978 |           0.2460 |
[32m[20221213 15:02:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.35
[32m[20221213 15:02:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 125.23
[32m[20221213 15:02:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.70
[32m[20221213 15:02:39 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 131.70
[32m[20221213 15:02:39 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 131.70
[32m[20221213 15:02:39 @agent_ppo2.py:143][0m Total time:       9.73 min
[32m[20221213 15:02:39 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221213 15:02:39 @agent_ppo2.py:121][0m #------------------------ Iteration 424 --------------------------#
[32m[20221213 15:02:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0017 |           3.7627 |           0.2464 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0097 |           3.6472 |           0.2460 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0093 |           3.7218 |           0.2460 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0136 |           3.5855 |           0.2460 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0128 |           3.5523 |           0.2461 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0039 |           3.8224 |           0.2462 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0189 |           3.5431 |           0.2464 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0127 |           3.5581 |           0.2465 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0156 |           3.5261 |           0.2464 |
[32m[20221213 15:02:40 @agent_ppo2.py:185][0m |          -0.0060 |           3.9641 |           0.2465 |
[32m[20221213 15:02:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:02:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.23
[32m[20221213 15:02:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.57
[32m[20221213 15:02:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.44
[32m[20221213 15:02:41 @agent_ppo2.py:143][0m Total time:       9.76 min
[32m[20221213 15:02:41 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221213 15:02:41 @agent_ppo2.py:121][0m #------------------------ Iteration 425 --------------------------#
[32m[20221213 15:02:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:41 @agent_ppo2.py:185][0m |           0.0054 |           3.7671 |           0.2528 |
[32m[20221213 15:02:41 @agent_ppo2.py:185][0m |          -0.0011 |           3.6062 |           0.2523 |
[32m[20221213 15:02:41 @agent_ppo2.py:185][0m |          -0.0062 |           3.5139 |           0.2521 |
[32m[20221213 15:02:41 @agent_ppo2.py:185][0m |          -0.0081 |           3.4692 |           0.2518 |
[32m[20221213 15:02:41 @agent_ppo2.py:185][0m |          -0.0118 |           3.4374 |           0.2518 |
[32m[20221213 15:02:41 @agent_ppo2.py:185][0m |          -0.0130 |           3.4089 |           0.2515 |
[32m[20221213 15:02:41 @agent_ppo2.py:185][0m |          -0.0120 |           3.3775 |           0.2514 |
[32m[20221213 15:02:42 @agent_ppo2.py:185][0m |          -0.0098 |           3.3782 |           0.2513 |
[32m[20221213 15:02:42 @agent_ppo2.py:185][0m |          -0.0171 |           3.3300 |           0.2511 |
[32m[20221213 15:02:42 @agent_ppo2.py:185][0m |          -0.0153 |           3.3107 |           0.2510 |
[32m[20221213 15:02:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.31
[32m[20221213 15:02:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.49
[32m[20221213 15:02:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 44.67
[32m[20221213 15:02:42 @agent_ppo2.py:143][0m Total time:       9.78 min
[32m[20221213 15:02:42 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221213 15:02:42 @agent_ppo2.py:121][0m #------------------------ Iteration 426 --------------------------#
[32m[20221213 15:02:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:42 @agent_ppo2.py:185][0m |           0.0035 |           3.8285 |           0.2500 |
[32m[20221213 15:02:42 @agent_ppo2.py:185][0m |          -0.0044 |           3.6705 |           0.2497 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |           0.0003 |           3.8736 |           0.2499 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |          -0.0154 |           3.5707 |           0.2495 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |          -0.0189 |           3.5411 |           0.2498 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |          -0.0155 |           3.5088 |           0.2499 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |          -0.0049 |           3.7909 |           0.2497 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |          -0.0182 |           3.4687 |           0.2498 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |          -0.0181 |           3.4318 |           0.2497 |
[32m[20221213 15:02:43 @agent_ppo2.py:185][0m |          -0.0209 |           3.4179 |           0.2497 |
[32m[20221213 15:02:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:02:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.76
[32m[20221213 15:02:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 113.90
[32m[20221213 15:02:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.66
[32m[20221213 15:02:43 @agent_ppo2.py:143][0m Total time:       9.80 min
[32m[20221213 15:02:43 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221213 15:02:43 @agent_ppo2.py:121][0m #------------------------ Iteration 427 --------------------------#
[32m[20221213 15:02:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0010 |           3.6577 |           0.2488 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0031 |           3.5298 |           0.2487 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0049 |           3.4938 |           0.2482 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0117 |           3.3908 |           0.2483 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0138 |           3.3561 |           0.2481 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0091 |           3.3565 |           0.2476 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0166 |           3.3192 |           0.2475 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0157 |           3.2981 |           0.2471 |
[32m[20221213 15:02:44 @agent_ppo2.py:185][0m |          -0.0133 |           3.2869 |           0.2469 |
[32m[20221213 15:02:45 @agent_ppo2.py:185][0m |          -0.0188 |           3.2719 |           0.2467 |
[32m[20221213 15:02:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.44
[32m[20221213 15:02:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 119.31
[32m[20221213 15:02:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.07
[32m[20221213 15:02:45 @agent_ppo2.py:143][0m Total time:       9.83 min
[32m[20221213 15:02:45 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221213 15:02:45 @agent_ppo2.py:121][0m #------------------------ Iteration 428 --------------------------#
[32m[20221213 15:02:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:45 @agent_ppo2.py:185][0m |          -0.0028 |           3.8757 |           0.2421 |
[32m[20221213 15:02:45 @agent_ppo2.py:185][0m |          -0.0090 |           3.7765 |           0.2415 |
[32m[20221213 15:02:45 @agent_ppo2.py:185][0m |          -0.0115 |           3.7684 |           0.2415 |
[32m[20221213 15:02:45 @agent_ppo2.py:185][0m |          -0.0153 |           3.7228 |           0.2412 |
[32m[20221213 15:02:45 @agent_ppo2.py:185][0m |          -0.0150 |           3.7064 |           0.2412 |
[32m[20221213 15:02:46 @agent_ppo2.py:185][0m |          -0.0184 |           3.6973 |           0.2409 |
[32m[20221213 15:02:46 @agent_ppo2.py:185][0m |          -0.0195 |           3.6716 |           0.2410 |
[32m[20221213 15:02:46 @agent_ppo2.py:185][0m |          -0.0180 |           3.6631 |           0.2409 |
[32m[20221213 15:02:46 @agent_ppo2.py:185][0m |          -0.0169 |           3.6515 |           0.2408 |
[32m[20221213 15:02:46 @agent_ppo2.py:185][0m |          -0.0173 |           3.6452 |           0.2405 |
[32m[20221213 15:02:46 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.05
[32m[20221213 15:02:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.81
[32m[20221213 15:02:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 119.14
[32m[20221213 15:02:46 @agent_ppo2.py:143][0m Total time:       9.85 min
[32m[20221213 15:02:46 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221213 15:02:46 @agent_ppo2.py:121][0m #------------------------ Iteration 429 --------------------------#
[32m[20221213 15:02:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:46 @agent_ppo2.py:185][0m |          -0.0051 |           3.7347 |           0.2419 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0093 |           3.6374 |           0.2414 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0179 |           3.6030 |           0.2411 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0155 |           3.5685 |           0.2408 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0138 |           3.5142 |           0.2407 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0189 |           3.4921 |           0.2405 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0144 |           3.4657 |           0.2404 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0211 |           3.4451 |           0.2401 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0159 |           3.4797 |           0.2403 |
[32m[20221213 15:02:47 @agent_ppo2.py:185][0m |          -0.0207 |           3.4031 |           0.2401 |
[32m[20221213 15:02:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:02:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 106.00
[32m[20221213 15:02:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 115.49
[32m[20221213 15:02:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.59
[32m[20221213 15:02:47 @agent_ppo2.py:143][0m Total time:       9.87 min
[32m[20221213 15:02:47 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221213 15:02:47 @agent_ppo2.py:121][0m #------------------------ Iteration 430 --------------------------#
[32m[20221213 15:02:48 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:02:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |          -0.0038 |           3.3964 |           0.2425 |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |          -0.0025 |           3.3633 |           0.2423 |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |           0.0008 |           3.3135 |           0.2420 |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |          -0.0083 |           3.1561 |           0.2422 |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |          -0.0086 |           3.0840 |           0.2421 |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |          -0.0139 |           3.0040 |           0.2421 |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |          -0.0166 |           2.9861 |           0.2420 |
[32m[20221213 15:02:48 @agent_ppo2.py:185][0m |          -0.0161 |           2.9410 |           0.2419 |
[32m[20221213 15:02:49 @agent_ppo2.py:185][0m |          -0.0168 |           2.9175 |           0.2419 |
[32m[20221213 15:02:49 @agent_ppo2.py:185][0m |          -0.0181 |           2.8859 |           0.2418 |
[32m[20221213 15:02:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.63
[32m[20221213 15:02:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 119.80
[32m[20221213 15:02:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.97
[32m[20221213 15:02:49 @agent_ppo2.py:143][0m Total time:       9.89 min
[32m[20221213 15:02:49 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221213 15:02:49 @agent_ppo2.py:121][0m #------------------------ Iteration 431 --------------------------#
[32m[20221213 15:02:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:49 @agent_ppo2.py:185][0m |           0.0015 |           3.9668 |           0.2409 |
[32m[20221213 15:02:49 @agent_ppo2.py:185][0m |          -0.0021 |           3.8261 |           0.2410 |
[32m[20221213 15:02:49 @agent_ppo2.py:185][0m |          -0.0121 |           3.7598 |           0.2407 |
[32m[20221213 15:02:50 @agent_ppo2.py:185][0m |          -0.0098 |           3.6984 |           0.2411 |
[32m[20221213 15:02:50 @agent_ppo2.py:185][0m |          -0.0043 |           3.7984 |           0.2411 |
[32m[20221213 15:02:50 @agent_ppo2.py:185][0m |          -0.0130 |           3.6392 |           0.2413 |
[32m[20221213 15:02:50 @agent_ppo2.py:185][0m |          -0.0104 |           3.6153 |           0.2413 |
[32m[20221213 15:02:50 @agent_ppo2.py:185][0m |          -0.0164 |           3.6001 |           0.2414 |
[32m[20221213 15:02:50 @agent_ppo2.py:185][0m |          -0.0171 |           3.5923 |           0.2414 |
[32m[20221213 15:02:50 @agent_ppo2.py:185][0m |          -0.0196 |           3.5566 |           0.2415 |
[32m[20221213 15:02:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:02:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 99.90
[32m[20221213 15:02:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.02
[32m[20221213 15:02:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.29
[32m[20221213 15:02:50 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 135.29
[32m[20221213 15:02:50 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 135.29
[32m[20221213 15:02:50 @agent_ppo2.py:143][0m Total time:       9.92 min
[32m[20221213 15:02:50 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221213 15:02:50 @agent_ppo2.py:121][0m #------------------------ Iteration 432 --------------------------#
[32m[20221213 15:02:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0001 |           3.8478 |           0.2399 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0037 |           3.7607 |           0.2394 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0064 |           3.7423 |           0.2395 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |           0.0000 |           3.7621 |           0.2392 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0102 |           3.6949 |           0.2394 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0125 |           3.6686 |           0.2391 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0054 |           3.7244 |           0.2389 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0112 |           3.6349 |           0.2390 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |           0.0027 |           3.9824 |           0.2389 |
[32m[20221213 15:02:51 @agent_ppo2.py:185][0m |          -0.0117 |           3.6154 |           0.2386 |
[32m[20221213 15:02:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 112.40
[32m[20221213 15:02:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 123.22
[32m[20221213 15:02:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 113.86
[32m[20221213 15:02:52 @agent_ppo2.py:143][0m Total time:       9.94 min
[32m[20221213 15:02:52 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221213 15:02:52 @agent_ppo2.py:121][0m #------------------------ Iteration 433 --------------------------#
[32m[20221213 15:02:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:52 @agent_ppo2.py:185][0m |           0.0095 |           4.1772 |           0.2446 |
[32m[20221213 15:02:52 @agent_ppo2.py:185][0m |          -0.0029 |           3.8503 |           0.2445 |
[32m[20221213 15:02:52 @agent_ppo2.py:185][0m |          -0.0062 |           3.8082 |           0.2444 |
[32m[20221213 15:02:52 @agent_ppo2.py:185][0m |          -0.0115 |           3.7731 |           0.2444 |
[32m[20221213 15:02:52 @agent_ppo2.py:185][0m |          -0.0114 |           3.7737 |           0.2446 |
[32m[20221213 15:02:52 @agent_ppo2.py:185][0m |          -0.0090 |           3.7824 |           0.2446 |
[32m[20221213 15:02:53 @agent_ppo2.py:185][0m |          -0.0107 |           3.7304 |           0.2449 |
[32m[20221213 15:02:53 @agent_ppo2.py:185][0m |          -0.0113 |           3.8353 |           0.2450 |
[32m[20221213 15:02:53 @agent_ppo2.py:185][0m |          -0.0193 |           3.7212 |           0.2449 |
[32m[20221213 15:02:53 @agent_ppo2.py:185][0m |          -0.0174 |           3.6962 |           0.2452 |
[32m[20221213 15:02:53 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:02:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.42
[32m[20221213 15:02:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 120.04
[32m[20221213 15:02:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.47
[32m[20221213 15:02:53 @agent_ppo2.py:143][0m Total time:       9.96 min
[32m[20221213 15:02:53 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221213 15:02:53 @agent_ppo2.py:121][0m #------------------------ Iteration 434 --------------------------#
[32m[20221213 15:02:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:53 @agent_ppo2.py:185][0m |          -0.0018 |           3.7308 |           0.2446 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0076 |           3.6244 |           0.2447 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0127 |           3.5923 |           0.2445 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0127 |           3.5498 |           0.2446 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0130 |           3.5261 |           0.2443 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0159 |           3.5213 |           0.2444 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0183 |           3.4860 |           0.2444 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0178 |           3.4661 |           0.2443 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0052 |           3.8345 |           0.2443 |
[32m[20221213 15:02:54 @agent_ppo2.py:185][0m |          -0.0193 |           3.4620 |           0.2441 |
[32m[20221213 15:02:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:02:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.74
[32m[20221213 15:02:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.74
[32m[20221213 15:02:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.23
[32m[20221213 15:02:54 @agent_ppo2.py:143][0m Total time:       9.99 min
[32m[20221213 15:02:54 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221213 15:02:54 @agent_ppo2.py:121][0m #------------------------ Iteration 435 --------------------------#
[32m[20221213 15:02:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:02:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |           0.0000 |           3.9548 |           0.2577 |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |          -0.0052 |           3.8171 |           0.2578 |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |          -0.0109 |           3.7565 |           0.2577 |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |          -0.0097 |           3.7102 |           0.2576 |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |          -0.0141 |           3.6799 |           0.2577 |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |          -0.0118 |           3.6956 |           0.2577 |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |          -0.0193 |           3.6140 |           0.2578 |
[32m[20221213 15:02:55 @agent_ppo2.py:185][0m |          -0.0179 |           3.5954 |           0.2577 |
[32m[20221213 15:02:56 @agent_ppo2.py:185][0m |          -0.0187 |           3.5620 |           0.2578 |
[32m[20221213 15:02:56 @agent_ppo2.py:185][0m |          -0.0163 |           3.6068 |           0.2579 |
[32m[20221213 15:02:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:02:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.30
[32m[20221213 15:02:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.00
[32m[20221213 15:02:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.09
[32m[20221213 15:02:56 @agent_ppo2.py:143][0m Total time:      10.01 min
[32m[20221213 15:02:56 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221213 15:02:56 @agent_ppo2.py:121][0m #------------------------ Iteration 436 --------------------------#
[32m[20221213 15:02:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:56 @agent_ppo2.py:185][0m |          -0.0034 |           4.0627 |           0.2486 |
[32m[20221213 15:02:56 @agent_ppo2.py:185][0m |          -0.0055 |           3.9810 |           0.2486 |
[32m[20221213 15:02:56 @agent_ppo2.py:185][0m |          -0.0125 |           3.9498 |           0.2484 |
[32m[20221213 15:02:56 @agent_ppo2.py:185][0m |          -0.0004 |           4.1993 |           0.2482 |
[32m[20221213 15:02:57 @agent_ppo2.py:185][0m |          -0.0129 |           3.8883 |           0.2483 |
[32m[20221213 15:02:57 @agent_ppo2.py:185][0m |          -0.0082 |           3.9348 |           0.2482 |
[32m[20221213 15:02:57 @agent_ppo2.py:185][0m |          -0.0124 |           3.8336 |           0.2481 |
[32m[20221213 15:02:57 @agent_ppo2.py:185][0m |          -0.0132 |           3.8312 |           0.2482 |
[32m[20221213 15:02:57 @agent_ppo2.py:185][0m |          -0.0141 |           3.8080 |           0.2482 |
[32m[20221213 15:02:57 @agent_ppo2.py:185][0m |          -0.0155 |           3.8146 |           0.2482 |
[32m[20221213 15:02:57 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:02:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.10
[32m[20221213 15:02:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.66
[32m[20221213 15:02:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.60
[32m[20221213 15:02:57 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 139.60
[32m[20221213 15:02:57 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 139.60
[32m[20221213 15:02:57 @agent_ppo2.py:143][0m Total time:      10.03 min
[32m[20221213 15:02:57 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221213 15:02:57 @agent_ppo2.py:121][0m #------------------------ Iteration 437 --------------------------#
[32m[20221213 15:02:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0041 |           3.9183 |           0.2514 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0105 |           3.8229 |           0.2512 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0146 |           3.7748 |           0.2510 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0149 |           3.7189 |           0.2512 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0164 |           3.6918 |           0.2511 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0149 |           3.6634 |           0.2511 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0203 |           3.6404 |           0.2512 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0173 |           3.6209 |           0.2513 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0125 |           3.6260 |           0.2514 |
[32m[20221213 15:02:58 @agent_ppo2.py:185][0m |          -0.0159 |           3.5927 |           0.2515 |
[32m[20221213 15:02:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:02:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 106.99
[32m[20221213 15:02:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.93
[32m[20221213 15:02:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 118.36
[32m[20221213 15:02:59 @agent_ppo2.py:143][0m Total time:      10.06 min
[32m[20221213 15:02:59 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221213 15:02:59 @agent_ppo2.py:121][0m #------------------------ Iteration 438 --------------------------#
[32m[20221213 15:02:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:02:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:02:59 @agent_ppo2.py:185][0m |          -0.0011 |           4.1524 |           0.2502 |
[32m[20221213 15:02:59 @agent_ppo2.py:185][0m |          -0.0114 |           4.0111 |           0.2494 |
[32m[20221213 15:02:59 @agent_ppo2.py:185][0m |          -0.0113 |           3.9492 |           0.2492 |
[32m[20221213 15:02:59 @agent_ppo2.py:185][0m |          -0.0117 |           3.9215 |           0.2489 |
[32m[20221213 15:02:59 @agent_ppo2.py:185][0m |          -0.0105 |           3.8731 |           0.2488 |
[32m[20221213 15:02:59 @agent_ppo2.py:185][0m |          -0.0151 |           3.8680 |           0.2485 |
[32m[20221213 15:02:59 @agent_ppo2.py:185][0m |          -0.0129 |           3.8533 |           0.2483 |
[32m[20221213 15:03:00 @agent_ppo2.py:185][0m |          -0.0157 |           3.8242 |           0.2484 |
[32m[20221213 15:03:00 @agent_ppo2.py:185][0m |          -0.0105 |           3.8530 |           0.2482 |
[32m[20221213 15:03:00 @agent_ppo2.py:185][0m |          -0.0190 |           3.7954 |           0.2480 |
[32m[20221213 15:03:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:03:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.38
[32m[20221213 15:03:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.86
[32m[20221213 15:03:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.69
[32m[20221213 15:03:00 @agent_ppo2.py:143][0m Total time:      10.08 min
[32m[20221213 15:03:00 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221213 15:03:00 @agent_ppo2.py:121][0m #------------------------ Iteration 439 --------------------------#
[32m[20221213 15:03:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:00 @agent_ppo2.py:185][0m |          -0.0001 |           3.8640 |           0.2524 |
[32m[20221213 15:03:00 @agent_ppo2.py:185][0m |          -0.0054 |           3.8132 |           0.2522 |
[32m[20221213 15:03:00 @agent_ppo2.py:185][0m |          -0.0114 |           3.7492 |           0.2516 |
[32m[20221213 15:03:01 @agent_ppo2.py:185][0m |          -0.0100 |           3.7184 |           0.2514 |
[32m[20221213 15:03:01 @agent_ppo2.py:185][0m |          -0.0114 |           3.7318 |           0.2511 |
[32m[20221213 15:03:01 @agent_ppo2.py:185][0m |          -0.0141 |           3.6644 |           0.2509 |
[32m[20221213 15:03:01 @agent_ppo2.py:185][0m |          -0.0121 |           3.6758 |           0.2509 |
[32m[20221213 15:03:01 @agent_ppo2.py:185][0m |          -0.0173 |           3.6413 |           0.2506 |
[32m[20221213 15:03:01 @agent_ppo2.py:185][0m |          -0.0164 |           3.6077 |           0.2506 |
[32m[20221213 15:03:01 @agent_ppo2.py:185][0m |          -0.0173 |           3.6044 |           0.2502 |
[32m[20221213 15:03:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:03:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 111.26
[32m[20221213 15:03:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.45
[32m[20221213 15:03:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 116.81
[32m[20221213 15:03:01 @agent_ppo2.py:143][0m Total time:      10.10 min
[32m[20221213 15:03:01 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221213 15:03:01 @agent_ppo2.py:121][0m #------------------------ Iteration 440 --------------------------#
[32m[20221213 15:03:02 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:03:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0006 |           3.8794 |           0.2514 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0043 |           3.8091 |           0.2515 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0083 |           3.7594 |           0.2516 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0084 |           3.7330 |           0.2517 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0108 |           3.7025 |           0.2517 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0122 |           3.6935 |           0.2517 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0165 |           3.6610 |           0.2518 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0124 |           3.6314 |           0.2516 |
[32m[20221213 15:03:02 @agent_ppo2.py:185][0m |          -0.0028 |           4.0999 |           0.2516 |
[32m[20221213 15:03:03 @agent_ppo2.py:185][0m |          -0.0177 |           3.6543 |           0.2517 |
[32m[20221213 15:03:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:03:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.12
[32m[20221213 15:03:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.51
[32m[20221213 15:03:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 113.05
[32m[20221213 15:03:03 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221213 15:03:03 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221213 15:03:03 @agent_ppo2.py:121][0m #------------------------ Iteration 441 --------------------------#
[32m[20221213 15:03:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:03 @agent_ppo2.py:185][0m |           0.0115 |           4.5036 |           0.2518 |
[32m[20221213 15:03:03 @agent_ppo2.py:185][0m |          -0.0110 |           3.9234 |           0.2520 |
[32m[20221213 15:03:03 @agent_ppo2.py:185][0m |          -0.0014 |           3.9686 |           0.2519 |
[32m[20221213 15:03:03 @agent_ppo2.py:185][0m |          -0.0033 |           4.0855 |           0.2518 |
[32m[20221213 15:03:03 @agent_ppo2.py:185][0m |          -0.0135 |           3.7897 |           0.2514 |
[32m[20221213 15:03:04 @agent_ppo2.py:185][0m |          -0.0089 |           3.7460 |           0.2515 |
[32m[20221213 15:03:04 @agent_ppo2.py:185][0m |          -0.0137 |           3.7289 |           0.2515 |
[32m[20221213 15:03:04 @agent_ppo2.py:185][0m |          -0.0120 |           3.7016 |           0.2514 |
[32m[20221213 15:03:04 @agent_ppo2.py:185][0m |          -0.0187 |           3.6812 |           0.2515 |
[32m[20221213 15:03:04 @agent_ppo2.py:185][0m |          -0.0129 |           3.7001 |           0.2515 |
[32m[20221213 15:03:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:03:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.92
[32m[20221213 15:03:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.98
[32m[20221213 15:03:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.57
[32m[20221213 15:03:04 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221213 15:03:04 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221213 15:03:04 @agent_ppo2.py:121][0m #------------------------ Iteration 442 --------------------------#
[32m[20221213 15:03:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:04 @agent_ppo2.py:185][0m |          -0.0025 |           4.0446 |           0.2498 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0083 |           3.9516 |           0.2493 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0100 |           3.9199 |           0.2488 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0025 |           4.0447 |           0.2485 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0132 |           3.8937 |           0.2484 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0162 |           3.8481 |           0.2484 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0132 |           3.8351 |           0.2479 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0147 |           3.8087 |           0.2479 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0152 |           3.8009 |           0.2478 |
[32m[20221213 15:03:05 @agent_ppo2.py:185][0m |          -0.0168 |           3.8054 |           0.2475 |
[32m[20221213 15:03:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:03:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.87
[32m[20221213 15:03:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 113.63
[32m[20221213 15:03:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.06
[32m[20221213 15:03:05 @agent_ppo2.py:143][0m Total time:      10.17 min
[32m[20221213 15:03:05 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221213 15:03:05 @agent_ppo2.py:121][0m #------------------------ Iteration 443 --------------------------#
[32m[20221213 15:03:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0020 |           4.0841 |           0.2477 |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0104 |           4.0196 |           0.2474 |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0091 |           3.9840 |           0.2473 |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0124 |           3.9486 |           0.2470 |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0087 |           4.1422 |           0.2469 |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0112 |           3.9529 |           0.2467 |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0169 |           3.8862 |           0.2465 |
[32m[20221213 15:03:06 @agent_ppo2.py:185][0m |          -0.0043 |           4.1579 |           0.2464 |
[32m[20221213 15:03:07 @agent_ppo2.py:185][0m |          -0.0203 |           3.8780 |           0.2461 |
[32m[20221213 15:03:07 @agent_ppo2.py:185][0m |          -0.0195 |           3.8500 |           0.2459 |
[32m[20221213 15:03:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:03:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.44
[32m[20221213 15:03:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.35
[32m[20221213 15:03:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.72
[32m[20221213 15:03:07 @agent_ppo2.py:143][0m Total time:      10.19 min
[32m[20221213 15:03:07 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221213 15:03:07 @agent_ppo2.py:121][0m #------------------------ Iteration 444 --------------------------#
[32m[20221213 15:03:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:07 @agent_ppo2.py:185][0m |          -0.0024 |           3.9623 |           0.2431 |
[32m[20221213 15:03:07 @agent_ppo2.py:185][0m |          -0.0010 |           3.9174 |           0.2425 |
[32m[20221213 15:03:07 @agent_ppo2.py:185][0m |          -0.0093 |           3.8425 |           0.2422 |
[32m[20221213 15:03:08 @agent_ppo2.py:185][0m |          -0.0099 |           3.8066 |           0.2424 |
[32m[20221213 15:03:08 @agent_ppo2.py:185][0m |          -0.0132 |           3.7780 |           0.2426 |
[32m[20221213 15:03:08 @agent_ppo2.py:185][0m |          -0.0163 |           3.7430 |           0.2424 |
[32m[20221213 15:03:08 @agent_ppo2.py:185][0m |          -0.0163 |           3.7220 |           0.2426 |
[32m[20221213 15:03:08 @agent_ppo2.py:185][0m |          -0.0062 |           4.1475 |           0.2429 |
[32m[20221213 15:03:08 @agent_ppo2.py:185][0m |          -0.0185 |           3.7173 |           0.2429 |
[32m[20221213 15:03:08 @agent_ppo2.py:185][0m |          -0.0188 |           3.6532 |           0.2432 |
[32m[20221213 15:03:08 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:03:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.52
[32m[20221213 15:03:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.42
[32m[20221213 15:03:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 103.76
[32m[20221213 15:03:08 @agent_ppo2.py:143][0m Total time:      10.22 min
[32m[20221213 15:03:08 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221213 15:03:08 @agent_ppo2.py:121][0m #------------------------ Iteration 445 --------------------------#
[32m[20221213 15:03:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |           0.0030 |           4.0738 |           0.2405 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0045 |           3.9664 |           0.2399 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0098 |           3.9377 |           0.2394 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0068 |           3.8965 |           0.2392 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0099 |           3.8736 |           0.2389 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0082 |           3.8589 |           0.2389 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0001 |           4.1544 |           0.2385 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0141 |           3.8372 |           0.2382 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0170 |           3.8349 |           0.2380 |
[32m[20221213 15:03:09 @agent_ppo2.py:185][0m |          -0.0162 |           3.8141 |           0.2379 |
[32m[20221213 15:03:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:03:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.80
[32m[20221213 15:03:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.26
[32m[20221213 15:03:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 122.95
[32m[20221213 15:03:10 @agent_ppo2.py:143][0m Total time:      10.24 min
[32m[20221213 15:03:10 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221213 15:03:10 @agent_ppo2.py:121][0m #------------------------ Iteration 446 --------------------------#
[32m[20221213 15:03:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:10 @agent_ppo2.py:185][0m |           0.0070 |           4.3809 |           0.2471 |
[32m[20221213 15:03:10 @agent_ppo2.py:185][0m |          -0.0055 |           4.0665 |           0.2465 |
[32m[20221213 15:03:10 @agent_ppo2.py:185][0m |          -0.0093 |           4.0424 |           0.2464 |
[32m[20221213 15:03:10 @agent_ppo2.py:185][0m |          -0.0123 |           4.0082 |           0.2461 |
[32m[20221213 15:03:10 @agent_ppo2.py:185][0m |          -0.0028 |           4.2771 |           0.2461 |
[32m[20221213 15:03:10 @agent_ppo2.py:185][0m |          -0.0139 |           3.9897 |           0.2459 |
[32m[20221213 15:03:11 @agent_ppo2.py:185][0m |          -0.0139 |           3.9598 |           0.2461 |
[32m[20221213 15:03:11 @agent_ppo2.py:185][0m |          -0.0168 |           3.9443 |           0.2459 |
[32m[20221213 15:03:11 @agent_ppo2.py:185][0m |          -0.0115 |           3.9326 |           0.2461 |
[32m[20221213 15:03:11 @agent_ppo2.py:185][0m |          -0.0197 |           3.9072 |           0.2461 |
[32m[20221213 15:03:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:03:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.70
[32m[20221213 15:03:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 130.93
[32m[20221213 15:03:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.20
[32m[20221213 15:03:11 @agent_ppo2.py:143][0m Total time:      10.26 min
[32m[20221213 15:03:11 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221213 15:03:11 @agent_ppo2.py:121][0m #------------------------ Iteration 447 --------------------------#
[32m[20221213 15:03:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:11 @agent_ppo2.py:185][0m |           0.0097 |           4.8523 |           0.2467 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0102 |           4.2213 |           0.2466 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0041 |           4.1619 |           0.2464 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0119 |           4.0960 |           0.2464 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0040 |           4.2109 |           0.2462 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0129 |           4.0632 |           0.2463 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0153 |           4.0354 |           0.2461 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0140 |           4.0045 |           0.2461 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0171 |           3.9965 |           0.2460 |
[32m[20221213 15:03:12 @agent_ppo2.py:185][0m |          -0.0177 |           3.9994 |           0.2462 |
[32m[20221213 15:03:12 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:03:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.01
[32m[20221213 15:03:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.14
[32m[20221213 15:03:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 109.06
[32m[20221213 15:03:12 @agent_ppo2.py:143][0m Total time:      10.29 min
[32m[20221213 15:03:12 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221213 15:03:12 @agent_ppo2.py:121][0m #------------------------ Iteration 448 --------------------------#
[32m[20221213 15:03:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0012 |           4.1302 |           0.2505 |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0066 |           4.0538 |           0.2503 |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0052 |           4.0504 |           0.2503 |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0108 |           4.0050 |           0.2504 |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0035 |           4.1312 |           0.2504 |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0128 |           3.9715 |           0.2505 |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0040 |           4.2252 |           0.2505 |
[32m[20221213 15:03:13 @agent_ppo2.py:185][0m |          -0.0146 |           3.9797 |           0.2506 |
[32m[20221213 15:03:14 @agent_ppo2.py:185][0m |          -0.0082 |           4.0887 |           0.2506 |
[32m[20221213 15:03:14 @agent_ppo2.py:185][0m |          -0.0151 |           3.9337 |           0.2509 |
[32m[20221213 15:03:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:03:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.66
[32m[20221213 15:03:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.62
[32m[20221213 15:03:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.13
[32m[20221213 15:03:14 @agent_ppo2.py:143][0m Total time:      10.31 min
[32m[20221213 15:03:14 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221213 15:03:14 @agent_ppo2.py:121][0m #------------------------ Iteration 449 --------------------------#
[32m[20221213 15:03:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:14 @agent_ppo2.py:185][0m |          -0.0028 |           4.2162 |           0.2455 |
[32m[20221213 15:03:14 @agent_ppo2.py:185][0m |          -0.0064 |           4.1125 |           0.2450 |
[32m[20221213 15:03:14 @agent_ppo2.py:185][0m |          -0.0109 |           4.0861 |           0.2448 |
[32m[20221213 15:03:14 @agent_ppo2.py:185][0m |          -0.0058 |           4.0628 |           0.2445 |
[32m[20221213 15:03:15 @agent_ppo2.py:185][0m |           0.0029 |           4.5403 |           0.2444 |
[32m[20221213 15:03:15 @agent_ppo2.py:185][0m |          -0.0126 |           4.0193 |           0.2441 |
[32m[20221213 15:03:15 @agent_ppo2.py:185][0m |          -0.0167 |           3.9707 |           0.2441 |
[32m[20221213 15:03:15 @agent_ppo2.py:185][0m |          -0.0113 |           3.9604 |           0.2439 |
[32m[20221213 15:03:15 @agent_ppo2.py:185][0m |          -0.0102 |           3.9851 |           0.2437 |
[32m[20221213 15:03:15 @agent_ppo2.py:185][0m |          -0.0186 |           3.9036 |           0.2438 |
[32m[20221213 15:03:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:03:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.85
[32m[20221213 15:03:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.61
[32m[20221213 15:03:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 98.14
[32m[20221213 15:03:15 @agent_ppo2.py:143][0m Total time:      10.33 min
[32m[20221213 15:03:15 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221213 15:03:15 @agent_ppo2.py:121][0m #------------------------ Iteration 450 --------------------------#
[32m[20221213 15:03:15 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:03:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0050 |           4.1214 |           0.2410 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0050 |           3.9021 |           0.2411 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0080 |           3.7575 |           0.2412 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0125 |           3.6649 |           0.2415 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0139 |           3.6071 |           0.2416 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0118 |           3.6535 |           0.2417 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0155 |           3.5202 |           0.2418 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0158 |           3.4822 |           0.2419 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0174 |           3.4454 |           0.2421 |
[32m[20221213 15:03:16 @agent_ppo2.py:185][0m |          -0.0150 |           3.4020 |           0.2419 |
[32m[20221213 15:03:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:03:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.02
[32m[20221213 15:03:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.56
[32m[20221213 15:03:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.48
[32m[20221213 15:03:17 @agent_ppo2.py:143][0m Total time:      10.36 min
[32m[20221213 15:03:17 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221213 15:03:17 @agent_ppo2.py:121][0m #------------------------ Iteration 451 --------------------------#
[32m[20221213 15:03:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:17 @agent_ppo2.py:185][0m |           0.0039 |           4.4611 |           0.2529 |
[32m[20221213 15:03:17 @agent_ppo2.py:185][0m |          -0.0068 |           4.1669 |           0.2523 |
[32m[20221213 15:03:17 @agent_ppo2.py:185][0m |          -0.0080 |           4.0982 |           0.2522 |
[32m[20221213 15:03:17 @agent_ppo2.py:185][0m |          -0.0079 |           4.0653 |           0.2521 |
[32m[20221213 15:03:17 @agent_ppo2.py:185][0m |          -0.0096 |           4.0456 |           0.2520 |
[32m[20221213 15:03:17 @agent_ppo2.py:185][0m |          -0.0096 |           4.0568 |           0.2520 |
[32m[20221213 15:03:18 @agent_ppo2.py:185][0m |          -0.0115 |           4.0203 |           0.2519 |
[32m[20221213 15:03:18 @agent_ppo2.py:185][0m |          -0.0147 |           3.9879 |           0.2516 |
[32m[20221213 15:03:18 @agent_ppo2.py:185][0m |          -0.0155 |           3.9595 |           0.2515 |
[32m[20221213 15:03:18 @agent_ppo2.py:185][0m |          -0.0151 |           3.9436 |           0.2516 |
[32m[20221213 15:03:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:03:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 100.15
[32m[20221213 15:03:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.92
[32m[20221213 15:03:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.74
[32m[20221213 15:03:18 @agent_ppo2.py:143][0m Total time:      10.38 min
[32m[20221213 15:03:18 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221213 15:03:18 @agent_ppo2.py:121][0m #------------------------ Iteration 452 --------------------------#
[32m[20221213 15:03:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:18 @agent_ppo2.py:185][0m |          -0.0020 |           4.1628 |           0.2474 |
[32m[20221213 15:03:18 @agent_ppo2.py:185][0m |          -0.0112 |           4.0496 |           0.2469 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0118 |           4.0050 |           0.2470 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0138 |           3.9727 |           0.2469 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0107 |           3.9399 |           0.2470 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0124 |           3.9139 |           0.2470 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0146 |           3.8951 |           0.2471 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0131 |           3.8813 |           0.2472 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0155 |           3.8711 |           0.2472 |
[32m[20221213 15:03:19 @agent_ppo2.py:185][0m |          -0.0196 |           3.8481 |           0.2472 |
[32m[20221213 15:03:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:03:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.30
[32m[20221213 15:03:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 120.37
[32m[20221213 15:03:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.06
[32m[20221213 15:03:19 @agent_ppo2.py:143][0m Total time:      10.40 min
[32m[20221213 15:03:19 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221213 15:03:19 @agent_ppo2.py:121][0m #------------------------ Iteration 453 --------------------------#
[32m[20221213 15:03:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |           0.0029 |           3.9594 |           0.2456 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0048 |           3.7504 |           0.2457 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0098 |           3.6697 |           0.2456 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0126 |           3.6132 |           0.2456 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0099 |           3.5803 |           0.2456 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0114 |           3.5682 |           0.2454 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0156 |           3.5375 |           0.2454 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0085 |           3.5382 |           0.2454 |
[32m[20221213 15:03:20 @agent_ppo2.py:185][0m |          -0.0132 |           3.5036 |           0.2452 |
[32m[20221213 15:03:21 @agent_ppo2.py:185][0m |          -0.0182 |           3.4849 |           0.2454 |
[32m[20221213 15:03:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:03:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.90
[32m[20221213 15:03:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.40
[32m[20221213 15:03:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.43
[32m[20221213 15:03:21 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 150.43
[32m[20221213 15:03:21 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 150.43
[32m[20221213 15:03:21 @agent_ppo2.py:143][0m Total time:      10.43 min
[32m[20221213 15:03:21 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221213 15:03:21 @agent_ppo2.py:121][0m #------------------------ Iteration 454 --------------------------#
[32m[20221213 15:03:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:21 @agent_ppo2.py:185][0m |           0.0039 |           4.5498 |           0.2465 |
[32m[20221213 15:03:21 @agent_ppo2.py:185][0m |          -0.0122 |           4.1953 |           0.2460 |
[32m[20221213 15:03:21 @agent_ppo2.py:185][0m |          -0.0128 |           4.0886 |           0.2462 |
[32m[20221213 15:03:21 @agent_ppo2.py:185][0m |          -0.0176 |           4.0550 |           0.2461 |
[32m[20221213 15:03:21 @agent_ppo2.py:185][0m |          -0.0145 |           4.0236 |           0.2461 |
[32m[20221213 15:03:22 @agent_ppo2.py:185][0m |          -0.0139 |           4.0258 |           0.2462 |
[32m[20221213 15:03:22 @agent_ppo2.py:185][0m |          -0.0158 |           3.9485 |           0.2463 |
[32m[20221213 15:03:22 @agent_ppo2.py:185][0m |          -0.0174 |           3.9130 |           0.2463 |
[32m[20221213 15:03:22 @agent_ppo2.py:185][0m |          -0.0187 |           3.8785 |           0.2463 |
[32m[20221213 15:03:22 @agent_ppo2.py:185][0m |          -0.0177 |           3.8380 |           0.2466 |
[32m[20221213 15:03:22 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:03:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 112.25
[32m[20221213 15:03:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 123.51
[32m[20221213 15:03:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.96
[32m[20221213 15:03:22 @agent_ppo2.py:143][0m Total time:      10.45 min
[32m[20221213 15:03:22 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221213 15:03:22 @agent_ppo2.py:121][0m #------------------------ Iteration 455 --------------------------#
[32m[20221213 15:03:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |           0.0045 |           4.4892 |           0.2601 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0039 |           4.2916 |           0.2594 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0074 |           4.2229 |           0.2588 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0081 |           4.1692 |           0.2585 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0125 |           4.1370 |           0.2583 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0131 |           4.1123 |           0.2581 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0175 |           4.0739 |           0.2581 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0163 |           4.0663 |           0.2579 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0161 |           4.0316 |           0.2577 |
[32m[20221213 15:03:23 @agent_ppo2.py:185][0m |          -0.0156 |           4.0149 |           0.2575 |
[32m[20221213 15:03:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:03:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.38
[32m[20221213 15:03:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.56
[32m[20221213 15:03:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 103.18
[32m[20221213 15:03:24 @agent_ppo2.py:143][0m Total time:      10.47 min
[32m[20221213 15:03:24 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221213 15:03:24 @agent_ppo2.py:121][0m #------------------------ Iteration 456 --------------------------#
[32m[20221213 15:03:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0029 |           4.0513 |           0.2473 |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0057 |           3.9006 |           0.2467 |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0146 |           3.7873 |           0.2463 |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0156 |           3.7418 |           0.2460 |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0176 |           3.6881 |           0.2460 |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0159 |           3.6488 |           0.2458 |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0167 |           3.6182 |           0.2456 |
[32m[20221213 15:03:24 @agent_ppo2.py:185][0m |          -0.0196 |           3.5803 |           0.2457 |
[32m[20221213 15:03:25 @agent_ppo2.py:185][0m |          -0.0208 |           3.5417 |           0.2455 |
[32m[20221213 15:03:25 @agent_ppo2.py:185][0m |          -0.0220 |           3.5256 |           0.2455 |
[32m[20221213 15:03:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 112.47
[32m[20221213 15:03:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.67
[32m[20221213 15:03:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.71
[32m[20221213 15:03:25 @agent_ppo2.py:143][0m Total time:      10.49 min
[32m[20221213 15:03:25 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221213 15:03:25 @agent_ppo2.py:121][0m #------------------------ Iteration 457 --------------------------#
[32m[20221213 15:03:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:25 @agent_ppo2.py:185][0m |           0.0044 |           4.3179 |           0.2491 |
[32m[20221213 15:03:25 @agent_ppo2.py:185][0m |          -0.0055 |           4.0629 |           0.2491 |
[32m[20221213 15:03:25 @agent_ppo2.py:185][0m |          -0.0033 |           4.1496 |           0.2490 |
[32m[20221213 15:03:25 @agent_ppo2.py:185][0m |          -0.0070 |           3.9686 |           0.2491 |
[32m[20221213 15:03:26 @agent_ppo2.py:185][0m |          -0.0118 |           3.9191 |           0.2491 |
[32m[20221213 15:03:26 @agent_ppo2.py:185][0m |          -0.0109 |           3.9054 |           0.2491 |
[32m[20221213 15:03:26 @agent_ppo2.py:185][0m |          -0.0181 |           3.8777 |           0.2494 |
[32m[20221213 15:03:26 @agent_ppo2.py:185][0m |          -0.0135 |           3.8564 |           0.2494 |
[32m[20221213 15:03:26 @agent_ppo2.py:185][0m |          -0.0145 |           3.8376 |           0.2495 |
[32m[20221213 15:03:26 @agent_ppo2.py:185][0m |          -0.0167 |           3.8293 |           0.2498 |
[32m[20221213 15:03:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.62
[32m[20221213 15:03:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 121.93
[32m[20221213 15:03:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.32
[32m[20221213 15:03:26 @agent_ppo2.py:143][0m Total time:      10.52 min
[32m[20221213 15:03:26 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221213 15:03:26 @agent_ppo2.py:121][0m #------------------------ Iteration 458 --------------------------#
[32m[20221213 15:03:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0003 |           4.2949 |           0.2557 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0053 |           4.2159 |           0.2554 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0124 |           4.1276 |           0.2556 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0115 |           4.1034 |           0.2555 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0143 |           4.1042 |           0.2553 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0157 |           4.0589 |           0.2551 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0177 |           4.0610 |           0.2551 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0093 |           4.1876 |           0.2551 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0158 |           4.0234 |           0.2548 |
[32m[20221213 15:03:27 @agent_ppo2.py:185][0m |          -0.0206 |           4.0026 |           0.2549 |
[32m[20221213 15:03:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.94
[32m[20221213 15:03:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.06
[32m[20221213 15:03:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.86
[32m[20221213 15:03:28 @agent_ppo2.py:143][0m Total time:      10.54 min
[32m[20221213 15:03:28 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221213 15:03:28 @agent_ppo2.py:121][0m #------------------------ Iteration 459 --------------------------#
[32m[20221213 15:03:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |           0.0075 |           4.3050 |           0.2450 |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |          -0.0100 |           3.8583 |           0.2447 |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |          -0.0069 |           3.9456 |           0.2446 |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |          -0.0117 |           3.7766 |           0.2446 |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |          -0.0046 |           4.0057 |           0.2445 |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |          -0.0149 |           3.7309 |           0.2442 |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |          -0.0149 |           3.7119 |           0.2441 |
[32m[20221213 15:03:28 @agent_ppo2.py:185][0m |          -0.0159 |           3.6903 |           0.2442 |
[32m[20221213 15:03:29 @agent_ppo2.py:185][0m |          -0.0176 |           3.6617 |           0.2440 |
[32m[20221213 15:03:29 @agent_ppo2.py:185][0m |          -0.0188 |           3.6422 |           0.2437 |
[32m[20221213 15:03:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:03:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 112.70
[32m[20221213 15:03:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.66
[32m[20221213 15:03:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.94
[32m[20221213 15:03:29 @agent_ppo2.py:143][0m Total time:      10.56 min
[32m[20221213 15:03:29 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221213 15:03:29 @agent_ppo2.py:121][0m #------------------------ Iteration 460 --------------------------#
[32m[20221213 15:03:29 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:03:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:29 @agent_ppo2.py:185][0m |          -0.0014 |           3.9749 |           0.2467 |
[32m[20221213 15:03:29 @agent_ppo2.py:185][0m |          -0.0069 |           3.9196 |           0.2463 |
[32m[20221213 15:03:29 @agent_ppo2.py:185][0m |           0.0035 |           4.3587 |           0.2463 |
[32m[20221213 15:03:30 @agent_ppo2.py:185][0m |          -0.0091 |           3.9122 |           0.2463 |
[32m[20221213 15:03:30 @agent_ppo2.py:185][0m |          -0.0150 |           3.8409 |           0.2462 |
[32m[20221213 15:03:30 @agent_ppo2.py:185][0m |          -0.0156 |           3.8448 |           0.2465 |
[32m[20221213 15:03:30 @agent_ppo2.py:185][0m |          -0.0139 |           3.8112 |           0.2463 |
[32m[20221213 15:03:30 @agent_ppo2.py:185][0m |          -0.0148 |           3.8023 |           0.2465 |
[32m[20221213 15:03:30 @agent_ppo2.py:185][0m |          -0.0195 |           3.7822 |           0.2465 |
[32m[20221213 15:03:30 @agent_ppo2.py:185][0m |           0.0031 |           4.5077 |           0.2465 |
[32m[20221213 15:03:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.92
[32m[20221213 15:03:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.34
[32m[20221213 15:03:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.28
[32m[20221213 15:03:30 @agent_ppo2.py:143][0m Total time:      10.58 min
[32m[20221213 15:03:30 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221213 15:03:30 @agent_ppo2.py:121][0m #------------------------ Iteration 461 --------------------------#
[32m[20221213 15:03:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0017 |           4.0929 |           0.2431 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0071 |           3.9691 |           0.2427 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0113 |           3.9520 |           0.2428 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0086 |           3.9384 |           0.2428 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0161 |           3.8940 |           0.2428 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0120 |           3.8785 |           0.2426 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0174 |           3.8631 |           0.2428 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0182 |           3.8408 |           0.2428 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0214 |           3.8246 |           0.2426 |
[32m[20221213 15:03:31 @agent_ppo2.py:185][0m |          -0.0176 |           3.8090 |           0.2427 |
[32m[20221213 15:03:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.05
[32m[20221213 15:03:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 112.13
[32m[20221213 15:03:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.07
[32m[20221213 15:03:32 @agent_ppo2.py:143][0m Total time:      10.61 min
[32m[20221213 15:03:32 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221213 15:03:32 @agent_ppo2.py:121][0m #------------------------ Iteration 462 --------------------------#
[32m[20221213 15:03:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:32 @agent_ppo2.py:185][0m |          -0.0006 |           4.0044 |           0.2555 |
[32m[20221213 15:03:32 @agent_ppo2.py:185][0m |          -0.0059 |           3.9223 |           0.2549 |
[32m[20221213 15:03:32 @agent_ppo2.py:185][0m |          -0.0094 |           3.8901 |           0.2548 |
[32m[20221213 15:03:32 @agent_ppo2.py:185][0m |          -0.0144 |           3.8599 |           0.2547 |
[32m[20221213 15:03:32 @agent_ppo2.py:185][0m |          -0.0134 |           3.8356 |           0.2546 |
[32m[20221213 15:03:32 @agent_ppo2.py:185][0m |          -0.0164 |           3.8091 |           0.2544 |
[32m[20221213 15:03:32 @agent_ppo2.py:185][0m |          -0.0138 |           3.7872 |           0.2543 |
[32m[20221213 15:03:33 @agent_ppo2.py:185][0m |          -0.0173 |           3.7829 |           0.2542 |
[32m[20221213 15:03:33 @agent_ppo2.py:185][0m |          -0.0174 |           3.7622 |           0.2542 |
[32m[20221213 15:03:33 @agent_ppo2.py:185][0m |          -0.0167 |           3.7930 |           0.2540 |
[32m[20221213 15:03:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.42
[32m[20221213 15:03:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.70
[32m[20221213 15:03:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.18
[32m[20221213 15:03:33 @agent_ppo2.py:143][0m Total time:      10.63 min
[32m[20221213 15:03:33 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221213 15:03:33 @agent_ppo2.py:121][0m #------------------------ Iteration 463 --------------------------#
[32m[20221213 15:03:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:33 @agent_ppo2.py:185][0m |           0.0053 |           4.1893 |           0.2517 |
[32m[20221213 15:03:33 @agent_ppo2.py:185][0m |          -0.0096 |           4.0771 |           0.2516 |
[32m[20221213 15:03:33 @agent_ppo2.py:185][0m |          -0.0111 |           4.0451 |           0.2514 |
[32m[20221213 15:03:33 @agent_ppo2.py:185][0m |          -0.0121 |           3.9884 |           0.2512 |
[32m[20221213 15:03:34 @agent_ppo2.py:185][0m |          -0.0167 |           3.9687 |           0.2511 |
[32m[20221213 15:03:34 @agent_ppo2.py:185][0m |          -0.0116 |           3.9880 |           0.2510 |
[32m[20221213 15:03:34 @agent_ppo2.py:185][0m |          -0.0149 |           3.9171 |           0.2508 |
[32m[20221213 15:03:34 @agent_ppo2.py:185][0m |          -0.0192 |           3.8910 |           0.2509 |
[32m[20221213 15:03:34 @agent_ppo2.py:185][0m |          -0.0173 |           3.8657 |           0.2506 |
[32m[20221213 15:03:34 @agent_ppo2.py:185][0m |          -0.0157 |           3.8541 |           0.2507 |
[32m[20221213 15:03:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:03:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.29
[32m[20221213 15:03:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 135.44
[32m[20221213 15:03:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.61
[32m[20221213 15:03:34 @agent_ppo2.py:143][0m Total time:      10.65 min
[32m[20221213 15:03:34 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221213 15:03:34 @agent_ppo2.py:121][0m #------------------------ Iteration 464 --------------------------#
[32m[20221213 15:03:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0028 |           3.9772 |           0.2489 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0008 |           4.2319 |           0.2488 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0097 |           3.8415 |           0.2483 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0086 |           3.8378 |           0.2482 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0121 |           3.7844 |           0.2480 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0081 |           3.9233 |           0.2480 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0175 |           3.7438 |           0.2479 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0052 |           3.9159 |           0.2478 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0165 |           3.7151 |           0.2477 |
[32m[20221213 15:03:35 @agent_ppo2.py:185][0m |          -0.0179 |           3.6981 |           0.2477 |
[32m[20221213 15:03:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:03:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 100.13
[32m[20221213 15:03:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 123.86
[32m[20221213 15:03:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.51
[32m[20221213 15:03:36 @agent_ppo2.py:143][0m Total time:      10.67 min
[32m[20221213 15:03:36 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221213 15:03:36 @agent_ppo2.py:121][0m #------------------------ Iteration 465 --------------------------#
[32m[20221213 15:03:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:36 @agent_ppo2.py:185][0m |          -0.0045 |           3.9547 |           0.2475 |
[32m[20221213 15:03:36 @agent_ppo2.py:185][0m |          -0.0076 |           3.8811 |           0.2469 |
[32m[20221213 15:03:36 @agent_ppo2.py:185][0m |          -0.0075 |           3.8596 |           0.2466 |
[32m[20221213 15:03:36 @agent_ppo2.py:185][0m |          -0.0120 |           3.8227 |           0.2466 |
[32m[20221213 15:03:36 @agent_ppo2.py:185][0m |          -0.0160 |           3.8095 |           0.2465 |
[32m[20221213 15:03:36 @agent_ppo2.py:185][0m |          -0.0170 |           3.7904 |           0.2464 |
[32m[20221213 15:03:36 @agent_ppo2.py:185][0m |          -0.0140 |           3.7760 |           0.2462 |
[32m[20221213 15:03:37 @agent_ppo2.py:185][0m |          -0.0074 |           3.9638 |           0.2461 |
[32m[20221213 15:03:37 @agent_ppo2.py:185][0m |          -0.0175 |           3.7520 |           0.2461 |
[32m[20221213 15:03:37 @agent_ppo2.py:185][0m |          -0.0203 |           3.7273 |           0.2462 |
[32m[20221213 15:03:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:03:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.34
[32m[20221213 15:03:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 119.16
[32m[20221213 15:03:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.09
[32m[20221213 15:03:37 @agent_ppo2.py:143][0m Total time:      10.70 min
[32m[20221213 15:03:37 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221213 15:03:37 @agent_ppo2.py:121][0m #------------------------ Iteration 466 --------------------------#
[32m[20221213 15:03:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:37 @agent_ppo2.py:185][0m |           0.0022 |           3.9051 |           0.2484 |
[32m[20221213 15:03:37 @agent_ppo2.py:185][0m |          -0.0045 |           3.8521 |           0.2477 |
[32m[20221213 15:03:37 @agent_ppo2.py:185][0m |          -0.0102 |           3.6673 |           0.2473 |
[32m[20221213 15:03:38 @agent_ppo2.py:185][0m |          -0.0123 |           3.6170 |           0.2472 |
[32m[20221213 15:03:38 @agent_ppo2.py:185][0m |          -0.0166 |           3.5778 |           0.2469 |
[32m[20221213 15:03:38 @agent_ppo2.py:185][0m |          -0.0155 |           3.5465 |           0.2467 |
[32m[20221213 15:03:38 @agent_ppo2.py:185][0m |          -0.0160 |           3.5162 |           0.2467 |
[32m[20221213 15:03:38 @agent_ppo2.py:185][0m |          -0.0147 |           3.4737 |           0.2465 |
[32m[20221213 15:03:38 @agent_ppo2.py:185][0m |          -0.0177 |           3.4544 |           0.2463 |
[32m[20221213 15:03:38 @agent_ppo2.py:185][0m |          -0.0184 |           3.4366 |           0.2463 |
[32m[20221213 15:03:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.98
[32m[20221213 15:03:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.95
[32m[20221213 15:03:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.99
[32m[20221213 15:03:38 @agent_ppo2.py:143][0m Total time:      10.72 min
[32m[20221213 15:03:38 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221213 15:03:38 @agent_ppo2.py:121][0m #------------------------ Iteration 467 --------------------------#
[32m[20221213 15:03:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |           0.0028 |           4.3831 |           0.2496 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0003 |           4.2840 |           0.2493 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0096 |           4.0480 |           0.2492 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0113 |           4.0045 |           0.2488 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0191 |           3.9697 |           0.2490 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0063 |           4.2310 |           0.2488 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0167 |           3.8733 |           0.2489 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0185 |           3.8356 |           0.2489 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0180 |           3.8071 |           0.2488 |
[32m[20221213 15:03:39 @agent_ppo2.py:185][0m |          -0.0146 |           3.8267 |           0.2490 |
[32m[20221213 15:03:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.48
[32m[20221213 15:03:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 131.57
[32m[20221213 15:03:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.10
[32m[20221213 15:03:40 @agent_ppo2.py:143][0m Total time:      10.74 min
[32m[20221213 15:03:40 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221213 15:03:40 @agent_ppo2.py:121][0m #------------------------ Iteration 468 --------------------------#
[32m[20221213 15:03:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:40 @agent_ppo2.py:185][0m |           0.0040 |           4.0574 |           0.2488 |
[32m[20221213 15:03:40 @agent_ppo2.py:185][0m |          -0.0053 |           3.8833 |           0.2486 |
[32m[20221213 15:03:40 @agent_ppo2.py:185][0m |          -0.0086 |           3.7958 |           0.2485 |
[32m[20221213 15:03:40 @agent_ppo2.py:185][0m |          -0.0110 |           3.7605 |           0.2484 |
[32m[20221213 15:03:40 @agent_ppo2.py:185][0m |          -0.0098 |           3.6962 |           0.2483 |
[32m[20221213 15:03:40 @agent_ppo2.py:185][0m |          -0.0099 |           3.6760 |           0.2485 |
[32m[20221213 15:03:40 @agent_ppo2.py:185][0m |          -0.0140 |           3.6214 |           0.2484 |
[32m[20221213 15:03:41 @agent_ppo2.py:185][0m |          -0.0140 |           3.5971 |           0.2484 |
[32m[20221213 15:03:41 @agent_ppo2.py:185][0m |          -0.0153 |           3.5733 |           0.2485 |
[32m[20221213 15:03:41 @agent_ppo2.py:185][0m |          -0.0049 |           3.8225 |           0.2485 |
[32m[20221213 15:03:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.34
[32m[20221213 15:03:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.45
[32m[20221213 15:03:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.53
[32m[20221213 15:03:41 @agent_ppo2.py:143][0m Total time:      10.76 min
[32m[20221213 15:03:41 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221213 15:03:41 @agent_ppo2.py:121][0m #------------------------ Iteration 469 --------------------------#
[32m[20221213 15:03:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:41 @agent_ppo2.py:185][0m |          -0.0018 |           4.5152 |           0.2417 |
[32m[20221213 15:03:41 @agent_ppo2.py:185][0m |          -0.0080 |           4.3674 |           0.2416 |
[32m[20221213 15:03:41 @agent_ppo2.py:185][0m |          -0.0089 |           4.3048 |           0.2415 |
[32m[20221213 15:03:42 @agent_ppo2.py:185][0m |          -0.0129 |           4.2618 |           0.2413 |
[32m[20221213 15:03:42 @agent_ppo2.py:185][0m |          -0.0150 |           4.2333 |           0.2413 |
[32m[20221213 15:03:42 @agent_ppo2.py:185][0m |          -0.0134 |           4.2101 |           0.2414 |
[32m[20221213 15:03:42 @agent_ppo2.py:185][0m |          -0.0052 |           4.4449 |           0.2414 |
[32m[20221213 15:03:42 @agent_ppo2.py:185][0m |          -0.0154 |           4.1874 |           0.2413 |
[32m[20221213 15:03:42 @agent_ppo2.py:185][0m |          -0.0161 |           4.1634 |           0.2413 |
[32m[20221213 15:03:42 @agent_ppo2.py:185][0m |          -0.0175 |           4.1503 |           0.2413 |
[32m[20221213 15:03:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.96
[32m[20221213 15:03:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.59
[32m[20221213 15:03:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.54
[32m[20221213 15:03:42 @agent_ppo2.py:143][0m Total time:      10.78 min
[32m[20221213 15:03:42 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221213 15:03:42 @agent_ppo2.py:121][0m #------------------------ Iteration 470 --------------------------#
[32m[20221213 15:03:42 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:03:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0003 |           4.2133 |           0.2387 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0060 |           4.1020 |           0.2385 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0087 |           4.0317 |           0.2381 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0126 |           3.9888 |           0.2377 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0112 |           3.9403 |           0.2377 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0117 |           3.9158 |           0.2374 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0157 |           3.8726 |           0.2372 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0077 |           4.0291 |           0.2373 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0137 |           3.8397 |           0.2375 |
[32m[20221213 15:03:43 @agent_ppo2.py:185][0m |          -0.0073 |           3.8814 |           0.2371 |
[32m[20221213 15:03:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.04
[32m[20221213 15:03:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.49
[32m[20221213 15:03:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 101.18
[32m[20221213 15:03:44 @agent_ppo2.py:143][0m Total time:      10.81 min
[32m[20221213 15:03:44 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221213 15:03:44 @agent_ppo2.py:121][0m #------------------------ Iteration 471 --------------------------#
[32m[20221213 15:03:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:44 @agent_ppo2.py:185][0m |          -0.0018 |           4.2595 |           0.2451 |
[32m[20221213 15:03:44 @agent_ppo2.py:185][0m |          -0.0063 |           4.1097 |           0.2446 |
[32m[20221213 15:03:44 @agent_ppo2.py:185][0m |          -0.0030 |           4.1979 |           0.2439 |
[32m[20221213 15:03:44 @agent_ppo2.py:185][0m |          -0.0111 |           3.9953 |           0.2436 |
[32m[20221213 15:03:44 @agent_ppo2.py:185][0m |          -0.0106 |           3.9727 |           0.2433 |
[32m[20221213 15:03:44 @agent_ppo2.py:185][0m |          -0.0161 |           3.9255 |           0.2430 |
[32m[20221213 15:03:44 @agent_ppo2.py:185][0m |          -0.0172 |           3.9311 |           0.2428 |
[32m[20221213 15:03:45 @agent_ppo2.py:185][0m |          -0.0180 |           3.8932 |           0.2427 |
[32m[20221213 15:03:45 @agent_ppo2.py:185][0m |          -0.0174 |           3.8813 |           0.2423 |
[32m[20221213 15:03:45 @agent_ppo2.py:185][0m |          -0.0162 |           3.8527 |           0.2423 |
[32m[20221213 15:03:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.00
[32m[20221213 15:03:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.13
[32m[20221213 15:03:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 114.24
[32m[20221213 15:03:45 @agent_ppo2.py:143][0m Total time:      10.83 min
[32m[20221213 15:03:45 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221213 15:03:45 @agent_ppo2.py:121][0m #------------------------ Iteration 472 --------------------------#
[32m[20221213 15:03:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:45 @agent_ppo2.py:185][0m |          -0.0029 |           4.0973 |           0.2510 |
[32m[20221213 15:03:45 @agent_ppo2.py:185][0m |          -0.0043 |           3.8583 |           0.2505 |
[32m[20221213 15:03:45 @agent_ppo2.py:185][0m |           0.0050 |           4.3351 |           0.2503 |
[32m[20221213 15:03:46 @agent_ppo2.py:185][0m |          -0.0110 |           3.7455 |           0.2499 |
[32m[20221213 15:03:46 @agent_ppo2.py:185][0m |          -0.0128 |           3.6827 |           0.2496 |
[32m[20221213 15:03:46 @agent_ppo2.py:185][0m |          -0.0140 |           3.6668 |           0.2496 |
[32m[20221213 15:03:46 @agent_ppo2.py:185][0m |          -0.0156 |           3.6371 |           0.2494 |
[32m[20221213 15:03:46 @agent_ppo2.py:185][0m |          -0.0128 |           3.6142 |           0.2491 |
[32m[20221213 15:03:46 @agent_ppo2.py:185][0m |          -0.0136 |           3.5998 |           0.2492 |
[32m[20221213 15:03:46 @agent_ppo2.py:185][0m |          -0.0051 |           3.8457 |           0.2492 |
[32m[20221213 15:03:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.01
[32m[20221213 15:03:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.76
[32m[20221213 15:03:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 117.69
[32m[20221213 15:03:46 @agent_ppo2.py:143][0m Total time:      10.85 min
[32m[20221213 15:03:46 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221213 15:03:46 @agent_ppo2.py:121][0m #------------------------ Iteration 473 --------------------------#
[32m[20221213 15:03:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0029 |           4.5599 |           0.2346 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0077 |           4.3564 |           0.2342 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0118 |           4.2971 |           0.2341 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0131 |           4.2333 |           0.2338 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0113 |           4.2198 |           0.2338 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0144 |           4.1805 |           0.2337 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0176 |           4.1661 |           0.2336 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0167 |           4.1022 |           0.2335 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0160 |           4.3085 |           0.2334 |
[32m[20221213 15:03:47 @agent_ppo2.py:185][0m |          -0.0151 |           4.0969 |           0.2331 |
[32m[20221213 15:03:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.18
[32m[20221213 15:03:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.41
[32m[20221213 15:03:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.73
[32m[20221213 15:03:48 @agent_ppo2.py:143][0m Total time:      10.87 min
[32m[20221213 15:03:48 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221213 15:03:48 @agent_ppo2.py:121][0m #------------------------ Iteration 474 --------------------------#
[32m[20221213 15:03:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:48 @agent_ppo2.py:185][0m |          -0.0040 |           4.6974 |           0.2407 |
[32m[20221213 15:03:48 @agent_ppo2.py:185][0m |          -0.0023 |           4.6310 |           0.2405 |
[32m[20221213 15:03:48 @agent_ppo2.py:185][0m |          -0.0103 |           4.5026 |           0.2405 |
[32m[20221213 15:03:48 @agent_ppo2.py:185][0m |          -0.0123 |           4.4777 |           0.2406 |
[32m[20221213 15:03:48 @agent_ppo2.py:185][0m |          -0.0130 |           4.4096 |           0.2406 |
[32m[20221213 15:03:48 @agent_ppo2.py:185][0m |          -0.0143 |           4.3923 |           0.2407 |
[32m[20221213 15:03:48 @agent_ppo2.py:185][0m |          -0.0164 |           4.3540 |           0.2407 |
[32m[20221213 15:03:49 @agent_ppo2.py:185][0m |          -0.0170 |           4.3387 |           0.2409 |
[32m[20221213 15:03:49 @agent_ppo2.py:185][0m |          -0.0179 |           4.3132 |           0.2408 |
[32m[20221213 15:03:49 @agent_ppo2.py:185][0m |          -0.0159 |           4.3104 |           0.2409 |
[32m[20221213 15:03:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 96.93
[32m[20221213 15:03:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.79
[32m[20221213 15:03:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.03
[32m[20221213 15:03:49 @agent_ppo2.py:143][0m Total time:      10.90 min
[32m[20221213 15:03:49 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221213 15:03:49 @agent_ppo2.py:121][0m #------------------------ Iteration 475 --------------------------#
[32m[20221213 15:03:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:49 @agent_ppo2.py:185][0m |          -0.0022 |           4.2225 |           0.2404 |
[32m[20221213 15:03:49 @agent_ppo2.py:185][0m |          -0.0080 |           4.1048 |           0.2401 |
[32m[20221213 15:03:49 @agent_ppo2.py:185][0m |          -0.0111 |           4.0390 |           0.2399 |
[32m[20221213 15:03:50 @agent_ppo2.py:185][0m |          -0.0124 |           4.0017 |           0.2398 |
[32m[20221213 15:03:50 @agent_ppo2.py:185][0m |          -0.0149 |           3.9575 |           0.2396 |
[32m[20221213 15:03:50 @agent_ppo2.py:185][0m |          -0.0058 |           4.1831 |           0.2398 |
[32m[20221213 15:03:50 @agent_ppo2.py:185][0m |          -0.0059 |           4.0276 |           0.2395 |
[32m[20221213 15:03:50 @agent_ppo2.py:185][0m |          -0.0148 |           3.8558 |           0.2395 |
[32m[20221213 15:03:50 @agent_ppo2.py:185][0m |          -0.0153 |           3.8068 |           0.2393 |
[32m[20221213 15:03:50 @agent_ppo2.py:185][0m |          -0.0188 |           3.8018 |           0.2394 |
[32m[20221213 15:03:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.17
[32m[20221213 15:03:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.36
[32m[20221213 15:03:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 103.87
[32m[20221213 15:03:50 @agent_ppo2.py:143][0m Total time:      10.92 min
[32m[20221213 15:03:50 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221213 15:03:50 @agent_ppo2.py:121][0m #------------------------ Iteration 476 --------------------------#
[32m[20221213 15:03:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0002 |           4.3266 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0028 |           4.1789 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0068 |           4.2183 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0104 |           4.0862 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0149 |           4.0405 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0120 |           4.0468 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0063 |           4.2017 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0140 |           3.9885 |           0.2383 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0169 |           4.0160 |           0.2384 |
[32m[20221213 15:03:51 @agent_ppo2.py:185][0m |          -0.0145 |           3.9658 |           0.2385 |
[32m[20221213 15:03:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.54
[32m[20221213 15:03:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.41
[32m[20221213 15:03:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.37
[32m[20221213 15:03:52 @agent_ppo2.py:143][0m Total time:      10.94 min
[32m[20221213 15:03:52 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221213 15:03:52 @agent_ppo2.py:121][0m #------------------------ Iteration 477 --------------------------#
[32m[20221213 15:03:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:52 @agent_ppo2.py:185][0m |           0.0084 |           4.3283 |           0.2364 |
[32m[20221213 15:03:52 @agent_ppo2.py:185][0m |          -0.0063 |           4.0402 |           0.2361 |
[32m[20221213 15:03:52 @agent_ppo2.py:185][0m |          -0.0104 |           3.9906 |           0.2357 |
[32m[20221213 15:03:52 @agent_ppo2.py:185][0m |          -0.0108 |           3.9329 |           0.2354 |
[32m[20221213 15:03:52 @agent_ppo2.py:185][0m |          -0.0125 |           3.9287 |           0.2354 |
[32m[20221213 15:03:52 @agent_ppo2.py:185][0m |          -0.0153 |           3.8692 |           0.2351 |
[32m[20221213 15:03:53 @agent_ppo2.py:185][0m |          -0.0117 |           3.8492 |           0.2348 |
[32m[20221213 15:03:53 @agent_ppo2.py:185][0m |          -0.0050 |           4.1335 |           0.2348 |
[32m[20221213 15:03:53 @agent_ppo2.py:185][0m |          -0.0166 |           3.8042 |           0.2343 |
[32m[20221213 15:03:53 @agent_ppo2.py:185][0m |          -0.0167 |           3.7982 |           0.2343 |
[32m[20221213 15:03:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.72
[32m[20221213 15:03:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.86
[32m[20221213 15:03:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.25
[32m[20221213 15:03:53 @agent_ppo2.py:143][0m Total time:      10.96 min
[32m[20221213 15:03:53 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221213 15:03:53 @agent_ppo2.py:121][0m #------------------------ Iteration 478 --------------------------#
[32m[20221213 15:03:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:53 @agent_ppo2.py:185][0m |           0.0012 |           4.4608 |           0.2369 |
[32m[20221213 15:03:53 @agent_ppo2.py:185][0m |          -0.0051 |           4.2943 |           0.2366 |
[32m[20221213 15:03:53 @agent_ppo2.py:185][0m |          -0.0104 |           4.2110 |           0.2365 |
[32m[20221213 15:03:54 @agent_ppo2.py:185][0m |          -0.0120 |           4.2001 |           0.2362 |
[32m[20221213 15:03:54 @agent_ppo2.py:185][0m |          -0.0135 |           4.1446 |           0.2362 |
[32m[20221213 15:03:54 @agent_ppo2.py:185][0m |          -0.0078 |           4.1580 |           0.2361 |
[32m[20221213 15:03:54 @agent_ppo2.py:185][0m |          -0.0152 |           4.0853 |           0.2360 |
[32m[20221213 15:03:54 @agent_ppo2.py:185][0m |          -0.0140 |           4.0763 |           0.2360 |
[32m[20221213 15:03:54 @agent_ppo2.py:185][0m |          -0.0209 |           4.0645 |           0.2360 |
[32m[20221213 15:03:54 @agent_ppo2.py:185][0m |          -0.0177 |           4.0351 |           0.2358 |
[32m[20221213 15:03:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.18
[32m[20221213 15:03:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 120.66
[32m[20221213 15:03:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.71
[32m[20221213 15:03:54 @agent_ppo2.py:143][0m Total time:      10.98 min
[32m[20221213 15:03:54 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221213 15:03:54 @agent_ppo2.py:121][0m #------------------------ Iteration 479 --------------------------#
[32m[20221213 15:03:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0007 |           4.2580 |           0.2341 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0082 |           4.1603 |           0.2341 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0091 |           4.1220 |           0.2341 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0094 |           4.0991 |           0.2340 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0074 |           4.1260 |           0.2339 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0141 |           4.0177 |           0.2338 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0167 |           3.9884 |           0.2337 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0121 |           3.9806 |           0.2337 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0118 |           4.0580 |           0.2336 |
[32m[20221213 15:03:55 @agent_ppo2.py:185][0m |          -0.0166 |           3.9358 |           0.2336 |
[32m[20221213 15:03:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.38
[32m[20221213 15:03:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.57
[32m[20221213 15:03:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.23
[32m[20221213 15:03:56 @agent_ppo2.py:143][0m Total time:      11.01 min
[32m[20221213 15:03:56 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221213 15:03:56 @agent_ppo2.py:121][0m #------------------------ Iteration 480 --------------------------#
[32m[20221213 15:03:56 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:03:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:56 @agent_ppo2.py:185][0m |          -0.0011 |           4.5377 |           0.2404 |
[32m[20221213 15:03:56 @agent_ppo2.py:185][0m |           0.0059 |           5.0561 |           0.2400 |
[32m[20221213 15:03:56 @agent_ppo2.py:185][0m |          -0.0109 |           4.3561 |           0.2396 |
[32m[20221213 15:03:56 @agent_ppo2.py:185][0m |          -0.0132 |           4.2760 |           0.2397 |
[32m[20221213 15:03:56 @agent_ppo2.py:185][0m |          -0.0051 |           4.6259 |           0.2397 |
[32m[20221213 15:03:56 @agent_ppo2.py:185][0m |          -0.0125 |           4.2231 |           0.2397 |
[32m[20221213 15:03:57 @agent_ppo2.py:185][0m |          -0.0155 |           4.2049 |           0.2394 |
[32m[20221213 15:03:57 @agent_ppo2.py:185][0m |          -0.0132 |           4.1971 |           0.2396 |
[32m[20221213 15:03:57 @agent_ppo2.py:185][0m |          -0.0118 |           4.2117 |           0.2393 |
[32m[20221213 15:03:57 @agent_ppo2.py:185][0m |          -0.0198 |           4.1307 |           0.2395 |
[32m[20221213 15:03:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.53
[32m[20221213 15:03:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.94
[32m[20221213 15:03:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.15
[32m[20221213 15:03:57 @agent_ppo2.py:143][0m Total time:      11.03 min
[32m[20221213 15:03:57 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221213 15:03:57 @agent_ppo2.py:121][0m #------------------------ Iteration 481 --------------------------#
[32m[20221213 15:03:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:03:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:57 @agent_ppo2.py:185][0m |           0.0003 |           4.5380 |           0.2382 |
[32m[20221213 15:03:57 @agent_ppo2.py:185][0m |          -0.0072 |           4.4572 |           0.2379 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0063 |           4.3459 |           0.2380 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0122 |           4.2875 |           0.2379 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0128 |           4.2341 |           0.2379 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0088 |           4.1970 |           0.2380 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0124 |           4.1592 |           0.2381 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0152 |           4.1419 |           0.2380 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0173 |           4.0931 |           0.2381 |
[32m[20221213 15:03:58 @agent_ppo2.py:185][0m |          -0.0164 |           4.0717 |           0.2381 |
[32m[20221213 15:03:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:03:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.07
[32m[20221213 15:03:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.87
[32m[20221213 15:03:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 118.26
[32m[20221213 15:03:58 @agent_ppo2.py:143][0m Total time:      11.05 min
[32m[20221213 15:03:58 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221213 15:03:58 @agent_ppo2.py:121][0m #------------------------ Iteration 482 --------------------------#
[32m[20221213 15:03:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:03:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0041 |           4.8476 |           0.2389 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |           0.0036 |           4.8976 |           0.2383 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0096 |           4.6237 |           0.2384 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0093 |           4.6014 |           0.2381 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0059 |           5.0634 |           0.2383 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0159 |           4.4757 |           0.2382 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0178 |           4.4323 |           0.2382 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0130 |           4.4038 |           0.2383 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0168 |           4.3909 |           0.2380 |
[32m[20221213 15:03:59 @agent_ppo2.py:185][0m |          -0.0096 |           4.3671 |           0.2380 |
[32m[20221213 15:03:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.42
[32m[20221213 15:04:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 157.08
[32m[20221213 15:04:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.51
[32m[20221213 15:04:00 @agent_ppo2.py:143][0m Total time:      11.07 min
[32m[20221213 15:04:00 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221213 15:04:00 @agent_ppo2.py:121][0m #------------------------ Iteration 483 --------------------------#
[32m[20221213 15:04:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:00 @agent_ppo2.py:185][0m |          -0.0021 |           4.4429 |           0.2370 |
[32m[20221213 15:04:00 @agent_ppo2.py:185][0m |          -0.0091 |           4.2465 |           0.2363 |
[32m[20221213 15:04:00 @agent_ppo2.py:185][0m |          -0.0131 |           4.1386 |           0.2360 |
[32m[20221213 15:04:00 @agent_ppo2.py:185][0m |          -0.0108 |           4.0913 |           0.2361 |
[32m[20221213 15:04:00 @agent_ppo2.py:185][0m |          -0.0102 |           4.0937 |           0.2360 |
[32m[20221213 15:04:00 @agent_ppo2.py:185][0m |          -0.0182 |           3.9572 |           0.2359 |
[32m[20221213 15:04:00 @agent_ppo2.py:185][0m |          -0.0133 |           3.9241 |           0.2360 |
[32m[20221213 15:04:01 @agent_ppo2.py:185][0m |          -0.0183 |           3.8582 |           0.2360 |
[32m[20221213 15:04:01 @agent_ppo2.py:185][0m |          -0.0205 |           3.8215 |           0.2360 |
[32m[20221213 15:04:01 @agent_ppo2.py:185][0m |          -0.0203 |           3.7835 |           0.2361 |
[32m[20221213 15:04:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.54
[32m[20221213 15:04:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.62
[32m[20221213 15:04:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.19
[32m[20221213 15:04:01 @agent_ppo2.py:143][0m Total time:      11.10 min
[32m[20221213 15:04:01 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221213 15:04:01 @agent_ppo2.py:121][0m #------------------------ Iteration 484 --------------------------#
[32m[20221213 15:04:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:01 @agent_ppo2.py:185][0m |           0.0179 |           5.8055 |           0.2458 |
[32m[20221213 15:04:01 @agent_ppo2.py:185][0m |          -0.0107 |           4.9600 |           0.2452 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0093 |           4.8429 |           0.2449 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0148 |           4.7717 |           0.2449 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0114 |           4.7295 |           0.2450 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0183 |           4.6814 |           0.2447 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0044 |           5.1246 |           0.2447 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0154 |           4.6107 |           0.2444 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0161 |           4.5791 |           0.2449 |
[32m[20221213 15:04:02 @agent_ppo2.py:185][0m |          -0.0188 |           4.5647 |           0.2447 |
[32m[20221213 15:04:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 103.79
[32m[20221213 15:04:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.74
[32m[20221213 15:04:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.88
[32m[20221213 15:04:02 @agent_ppo2.py:143][0m Total time:      11.12 min
[32m[20221213 15:04:02 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221213 15:04:02 @agent_ppo2.py:121][0m #------------------------ Iteration 485 --------------------------#
[32m[20221213 15:04:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0019 |           4.4158 |           0.2374 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0024 |           4.2639 |           0.2375 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0089 |           4.0834 |           0.2376 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0088 |           4.0481 |           0.2376 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0124 |           3.9891 |           0.2377 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0071 |           4.1350 |           0.2379 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0132 |           3.9447 |           0.2379 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0177 |           3.9222 |           0.2380 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0181 |           3.8844 |           0.2381 |
[32m[20221213 15:04:03 @agent_ppo2.py:185][0m |          -0.0198 |           3.8927 |           0.2382 |
[32m[20221213 15:04:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.00
[32m[20221213 15:04:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.45
[32m[20221213 15:04:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 119.97
[32m[20221213 15:04:04 @agent_ppo2.py:143][0m Total time:      11.14 min
[32m[20221213 15:04:04 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221213 15:04:04 @agent_ppo2.py:121][0m #------------------------ Iteration 486 --------------------------#
[32m[20221213 15:04:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:04 @agent_ppo2.py:185][0m |          -0.0029 |           4.6113 |           0.2427 |
[32m[20221213 15:04:04 @agent_ppo2.py:185][0m |          -0.0118 |           4.4790 |           0.2426 |
[32m[20221213 15:04:04 @agent_ppo2.py:185][0m |          -0.0097 |           4.4359 |           0.2419 |
[32m[20221213 15:04:04 @agent_ppo2.py:185][0m |          -0.0116 |           4.4000 |           0.2425 |
[32m[20221213 15:04:04 @agent_ppo2.py:185][0m |          -0.0150 |           4.3792 |           0.2421 |
[32m[20221213 15:04:04 @agent_ppo2.py:185][0m |          -0.0171 |           4.3554 |           0.2423 |
[32m[20221213 15:04:05 @agent_ppo2.py:185][0m |          -0.0074 |           4.4977 |           0.2425 |
[32m[20221213 15:04:05 @agent_ppo2.py:185][0m |          -0.0151 |           4.3068 |           0.2423 |
[32m[20221213 15:04:05 @agent_ppo2.py:185][0m |          -0.0162 |           4.2867 |           0.2425 |
[32m[20221213 15:04:05 @agent_ppo2.py:185][0m |          -0.0123 |           4.3987 |           0.2426 |
[32m[20221213 15:04:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:04:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.78
[32m[20221213 15:04:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 130.10
[32m[20221213 15:04:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 121.76
[32m[20221213 15:04:05 @agent_ppo2.py:143][0m Total time:      11.16 min
[32m[20221213 15:04:05 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221213 15:04:05 @agent_ppo2.py:121][0m #------------------------ Iteration 487 --------------------------#
[32m[20221213 15:04:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:05 @agent_ppo2.py:185][0m |           0.0013 |           4.5052 |           0.2461 |
[32m[20221213 15:04:05 @agent_ppo2.py:185][0m |          -0.0069 |           4.4015 |           0.2461 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0031 |           4.5294 |           0.2460 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0099 |           4.3195 |           0.2460 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0115 |           4.2941 |           0.2461 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0140 |           4.2728 |           0.2463 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0060 |           4.3987 |           0.2461 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0155 |           4.2470 |           0.2461 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0002 |           4.6763 |           0.2460 |
[32m[20221213 15:04:06 @agent_ppo2.py:185][0m |          -0.0108 |           4.3259 |           0.2460 |
[32m[20221213 15:04:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.45
[32m[20221213 15:04:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.09
[32m[20221213 15:04:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.10
[32m[20221213 15:04:06 @agent_ppo2.py:143][0m Total time:      11.19 min
[32m[20221213 15:04:06 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221213 15:04:06 @agent_ppo2.py:121][0m #------------------------ Iteration 488 --------------------------#
[32m[20221213 15:04:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0028 |           4.4395 |           0.2448 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0019 |           4.3947 |           0.2441 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0112 |           4.2796 |           0.2442 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0112 |           4.2422 |           0.2441 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0141 |           4.2308 |           0.2437 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0140 |           4.2067 |           0.2439 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0049 |           4.3417 |           0.2440 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0139 |           4.1767 |           0.2438 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0148 |           4.1798 |           0.2441 |
[32m[20221213 15:04:07 @agent_ppo2.py:185][0m |          -0.0120 |           4.1993 |           0.2440 |
[32m[20221213 15:04:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.39
[32m[20221213 15:04:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.75
[32m[20221213 15:04:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.14
[32m[20221213 15:04:08 @agent_ppo2.py:143][0m Total time:      11.21 min
[32m[20221213 15:04:08 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221213 15:04:08 @agent_ppo2.py:121][0m #------------------------ Iteration 489 --------------------------#
[32m[20221213 15:04:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:08 @agent_ppo2.py:185][0m |           0.0043 |           4.0546 |           0.2489 |
[32m[20221213 15:04:08 @agent_ppo2.py:185][0m |           0.0067 |           4.3859 |           0.2483 |
[32m[20221213 15:04:08 @agent_ppo2.py:185][0m |          -0.0056 |           3.9332 |           0.2473 |
[32m[20221213 15:04:08 @agent_ppo2.py:185][0m |          -0.0095 |           3.8973 |           0.2473 |
[32m[20221213 15:04:08 @agent_ppo2.py:185][0m |          -0.0140 |           3.8968 |           0.2474 |
[32m[20221213 15:04:08 @agent_ppo2.py:185][0m |          -0.0099 |           3.8825 |           0.2470 |
[32m[20221213 15:04:09 @agent_ppo2.py:185][0m |          -0.0139 |           3.8682 |           0.2466 |
[32m[20221213 15:04:09 @agent_ppo2.py:185][0m |          -0.0117 |           3.8578 |           0.2465 |
[32m[20221213 15:04:09 @agent_ppo2.py:185][0m |          -0.0113 |           3.8463 |           0.2463 |
[32m[20221213 15:04:09 @agent_ppo2.py:185][0m |          -0.0123 |           3.8569 |           0.2464 |
[32m[20221213 15:04:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.35
[32m[20221213 15:04:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 121.70
[32m[20221213 15:04:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.64
[32m[20221213 15:04:09 @agent_ppo2.py:143][0m Total time:      11.23 min
[32m[20221213 15:04:09 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221213 15:04:09 @agent_ppo2.py:121][0m #------------------------ Iteration 490 --------------------------#
[32m[20221213 15:04:09 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:04:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:09 @agent_ppo2.py:185][0m |          -0.0031 |           4.4125 |           0.2509 |
[32m[20221213 15:04:09 @agent_ppo2.py:185][0m |          -0.0052 |           4.3348 |           0.2508 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0076 |           4.2908 |           0.2504 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0061 |           4.2980 |           0.2504 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0098 |           4.2430 |           0.2502 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0122 |           4.2066 |           0.2502 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0155 |           4.1993 |           0.2500 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0121 |           4.1886 |           0.2499 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0122 |           4.1681 |           0.2499 |
[32m[20221213 15:04:10 @agent_ppo2.py:185][0m |          -0.0143 |           4.1569 |           0.2499 |
[32m[20221213 15:04:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.94
[32m[20221213 15:04:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.67
[32m[20221213 15:04:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.58
[32m[20221213 15:04:10 @agent_ppo2.py:143][0m Total time:      11.25 min
[32m[20221213 15:04:10 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221213 15:04:10 @agent_ppo2.py:121][0m #------------------------ Iteration 491 --------------------------#
[32m[20221213 15:04:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |          -0.0030 |           4.4374 |           0.2447 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |           0.0003 |           4.4181 |           0.2447 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |           0.0032 |           4.5204 |           0.2445 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |          -0.0092 |           4.2817 |           0.2443 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |           0.0039 |           4.7386 |           0.2441 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |          -0.0127 |           4.2505 |           0.2440 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |          -0.0144 |           4.2276 |           0.2440 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |          -0.0163 |           4.2129 |           0.2439 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |          -0.0129 |           4.1900 |           0.2439 |
[32m[20221213 15:04:11 @agent_ppo2.py:185][0m |          -0.0109 |           4.2012 |           0.2439 |
[32m[20221213 15:04:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.34
[32m[20221213 15:04:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.67
[32m[20221213 15:04:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.74
[32m[20221213 15:04:12 @agent_ppo2.py:143][0m Total time:      11.27 min
[32m[20221213 15:04:12 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221213 15:04:12 @agent_ppo2.py:121][0m #------------------------ Iteration 492 --------------------------#
[32m[20221213 15:04:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:12 @agent_ppo2.py:185][0m |          -0.0011 |           4.3414 |           0.2488 |
[32m[20221213 15:04:12 @agent_ppo2.py:185][0m |          -0.0044 |           4.2561 |           0.2486 |
[32m[20221213 15:04:12 @agent_ppo2.py:185][0m |          -0.0055 |           4.2032 |           0.2486 |
[32m[20221213 15:04:12 @agent_ppo2.py:185][0m |          -0.0089 |           4.1717 |           0.2486 |
[32m[20221213 15:04:12 @agent_ppo2.py:185][0m |          -0.0115 |           4.1390 |           0.2485 |
[32m[20221213 15:04:12 @agent_ppo2.py:185][0m |          -0.0157 |           4.1194 |           0.2483 |
[32m[20221213 15:04:13 @agent_ppo2.py:185][0m |          -0.0132 |           4.0961 |           0.2483 |
[32m[20221213 15:04:13 @agent_ppo2.py:185][0m |          -0.0135 |           4.0822 |           0.2484 |
[32m[20221213 15:04:13 @agent_ppo2.py:185][0m |          -0.0156 |           4.0639 |           0.2484 |
[32m[20221213 15:04:13 @agent_ppo2.py:185][0m |          -0.0185 |           4.0406 |           0.2485 |
[32m[20221213 15:04:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.07
[32m[20221213 15:04:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.70
[32m[20221213 15:04:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 128.02
[32m[20221213 15:04:13 @agent_ppo2.py:143][0m Total time:      11.30 min
[32m[20221213 15:04:13 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221213 15:04:13 @agent_ppo2.py:121][0m #------------------------ Iteration 493 --------------------------#
[32m[20221213 15:04:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:13 @agent_ppo2.py:185][0m |          -0.0003 |           4.2011 |           0.2447 |
[32m[20221213 15:04:13 @agent_ppo2.py:185][0m |          -0.0012 |           4.1422 |           0.2447 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0108 |           4.0214 |           0.2447 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0133 |           3.9620 |           0.2446 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0070 |           4.0150 |           0.2445 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0148 |           3.9019 |           0.2444 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0173 |           3.8808 |           0.2447 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0177 |           3.8546 |           0.2447 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0190 |           3.8277 |           0.2446 |
[32m[20221213 15:04:14 @agent_ppo2.py:185][0m |          -0.0189 |           3.8071 |           0.2448 |
[32m[20221213 15:04:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 113.67
[32m[20221213 15:04:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.34
[32m[20221213 15:04:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 128.70
[32m[20221213 15:04:14 @agent_ppo2.py:143][0m Total time:      11.32 min
[32m[20221213 15:04:14 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221213 15:04:14 @agent_ppo2.py:121][0m #------------------------ Iteration 494 --------------------------#
[32m[20221213 15:04:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |           0.0019 |           4.5629 |           0.2487 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0081 |           4.4360 |           0.2480 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0111 |           4.3651 |           0.2480 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0083 |           4.3138 |           0.2479 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0140 |           4.2940 |           0.2478 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0160 |           4.2734 |           0.2477 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0085 |           4.4026 |           0.2477 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0212 |           4.2317 |           0.2478 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0100 |           4.2885 |           0.2478 |
[32m[20221213 15:04:15 @agent_ppo2.py:185][0m |          -0.0147 |           4.2182 |           0.2477 |
[32m[20221213 15:04:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 111.08
[32m[20221213 15:04:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 125.60
[32m[20221213 15:04:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.13
[32m[20221213 15:04:16 @agent_ppo2.py:143][0m Total time:      11.34 min
[32m[20221213 15:04:16 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221213 15:04:16 @agent_ppo2.py:121][0m #------------------------ Iteration 495 --------------------------#
[32m[20221213 15:04:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:16 @agent_ppo2.py:185][0m |          -0.0012 |           4.3027 |           0.2462 |
[32m[20221213 15:04:16 @agent_ppo2.py:185][0m |          -0.0048 |           4.1785 |           0.2459 |
[32m[20221213 15:04:16 @agent_ppo2.py:185][0m |          -0.0098 |           4.1332 |           0.2457 |
[32m[20221213 15:04:16 @agent_ppo2.py:185][0m |          -0.0110 |           4.0892 |           0.2453 |
[32m[20221213 15:04:16 @agent_ppo2.py:185][0m |          -0.0100 |           4.0561 |           0.2452 |
[32m[20221213 15:04:16 @agent_ppo2.py:185][0m |          -0.0114 |           4.0281 |           0.2450 |
[32m[20221213 15:04:17 @agent_ppo2.py:185][0m |          -0.0069 |           4.0442 |           0.2449 |
[32m[20221213 15:04:17 @agent_ppo2.py:185][0m |          -0.0121 |           4.0014 |           0.2450 |
[32m[20221213 15:04:17 @agent_ppo2.py:185][0m |          -0.0168 |           3.9521 |           0.2448 |
[32m[20221213 15:04:17 @agent_ppo2.py:185][0m |          -0.0109 |           3.9369 |           0.2448 |
[32m[20221213 15:04:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.74
[32m[20221213 15:04:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.49
[32m[20221213 15:04:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 111.35
[32m[20221213 15:04:17 @agent_ppo2.py:143][0m Total time:      11.36 min
[32m[20221213 15:04:17 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221213 15:04:17 @agent_ppo2.py:121][0m #------------------------ Iteration 496 --------------------------#
[32m[20221213 15:04:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:17 @agent_ppo2.py:185][0m |          -0.0020 |           4.0689 |           0.2399 |
[32m[20221213 15:04:17 @agent_ppo2.py:185][0m |          -0.0085 |           3.9096 |           0.2398 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0093 |           3.8399 |           0.2396 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0109 |           3.8220 |           0.2394 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0131 |           3.7760 |           0.2393 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0164 |           3.7555 |           0.2391 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0138 |           3.7263 |           0.2391 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0155 |           3.6933 |           0.2389 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0165 |           3.6837 |           0.2390 |
[32m[20221213 15:04:18 @agent_ppo2.py:185][0m |          -0.0131 |           3.7051 |           0.2390 |
[32m[20221213 15:04:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 101.89
[32m[20221213 15:04:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.16
[32m[20221213 15:04:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.21
[32m[20221213 15:04:18 @agent_ppo2.py:143][0m Total time:      11.39 min
[32m[20221213 15:04:18 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221213 15:04:18 @agent_ppo2.py:121][0m #------------------------ Iteration 497 --------------------------#
[32m[20221213 15:04:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0028 |           4.7782 |           0.2534 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0067 |           4.6323 |           0.2533 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |           0.0040 |           5.2223 |           0.2532 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0131 |           4.5367 |           0.2529 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0116 |           4.4975 |           0.2530 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0022 |           4.8397 |           0.2530 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0176 |           4.4460 |           0.2530 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0165 |           4.4178 |           0.2530 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0163 |           4.4075 |           0.2529 |
[32m[20221213 15:04:19 @agent_ppo2.py:185][0m |          -0.0079 |           4.6951 |           0.2529 |
[32m[20221213 15:04:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.45
[32m[20221213 15:04:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.25
[32m[20221213 15:04:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.98
[32m[20221213 15:04:20 @agent_ppo2.py:143][0m Total time:      11.41 min
[32m[20221213 15:04:20 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221213 15:04:20 @agent_ppo2.py:121][0m #------------------------ Iteration 498 --------------------------#
[32m[20221213 15:04:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:20 @agent_ppo2.py:185][0m |          -0.0014 |           4.4440 |           0.2454 |
[32m[20221213 15:04:20 @agent_ppo2.py:185][0m |          -0.0037 |           4.3075 |           0.2451 |
[32m[20221213 15:04:20 @agent_ppo2.py:185][0m |          -0.0094 |           4.1941 |           0.2450 |
[32m[20221213 15:04:20 @agent_ppo2.py:185][0m |          -0.0130 |           4.1198 |           0.2449 |
[32m[20221213 15:04:20 @agent_ppo2.py:185][0m |          -0.0158 |           4.0654 |           0.2451 |
[32m[20221213 15:04:20 @agent_ppo2.py:185][0m |          -0.0051 |           4.3984 |           0.2452 |
[32m[20221213 15:04:21 @agent_ppo2.py:185][0m |          -0.0177 |           3.9831 |           0.2449 |
[32m[20221213 15:04:21 @agent_ppo2.py:185][0m |          -0.0125 |           4.0571 |           0.2451 |
[32m[20221213 15:04:21 @agent_ppo2.py:185][0m |          -0.0137 |           3.9920 |           0.2452 |
[32m[20221213 15:04:21 @agent_ppo2.py:185][0m |          -0.0177 |           3.8892 |           0.2453 |
[32m[20221213 15:04:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.94
[32m[20221213 15:04:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.78
[32m[20221213 15:04:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.28
[32m[20221213 15:04:21 @agent_ppo2.py:143][0m Total time:      11.43 min
[32m[20221213 15:04:21 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221213 15:04:21 @agent_ppo2.py:121][0m #------------------------ Iteration 499 --------------------------#
[32m[20221213 15:04:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:21 @agent_ppo2.py:185][0m |          -0.0014 |           4.4285 |           0.2460 |
[32m[20221213 15:04:21 @agent_ppo2.py:185][0m |          -0.0072 |           4.3071 |           0.2460 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0119 |           4.2655 |           0.2458 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0093 |           4.2371 |           0.2460 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0131 |           4.1785 |           0.2458 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0100 |           4.1515 |           0.2460 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0070 |           4.3058 |           0.2459 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0147 |           4.0908 |           0.2459 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0092 |           4.2036 |           0.2458 |
[32m[20221213 15:04:22 @agent_ppo2.py:185][0m |          -0.0065 |           4.3250 |           0.2458 |
[32m[20221213 15:04:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.21
[32m[20221213 15:04:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.89
[32m[20221213 15:04:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.69
[32m[20221213 15:04:22 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 150.43
[32m[20221213 15:04:22 @agent_ppo2.py:143][0m Total time:      11.45 min
[32m[20221213 15:04:22 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221213 15:04:22 @agent_ppo2.py:121][0m #------------------------ Iteration 500 --------------------------#
[32m[20221213 15:04:23 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:04:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |           0.0015 |           4.5455 |           0.2510 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0036 |           4.4202 |           0.2506 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0096 |           4.3619 |           0.2504 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0098 |           4.3116 |           0.2503 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0118 |           4.2752 |           0.2504 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0082 |           4.2839 |           0.2503 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0036 |           4.2614 |           0.2501 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0132 |           4.1765 |           0.2504 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0110 |           4.1705 |           0.2504 |
[32m[20221213 15:04:23 @agent_ppo2.py:185][0m |          -0.0091 |           4.2462 |           0.2501 |
[32m[20221213 15:04:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.19
[32m[20221213 15:04:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.31
[32m[20221213 15:04:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.04
[32m[20221213 15:04:24 @agent_ppo2.py:143][0m Total time:      11.47 min
[32m[20221213 15:04:24 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221213 15:04:24 @agent_ppo2.py:121][0m #------------------------ Iteration 501 --------------------------#
[32m[20221213 15:04:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:24 @agent_ppo2.py:185][0m |          -0.0023 |           4.5674 |           0.2535 |
[32m[20221213 15:04:24 @agent_ppo2.py:185][0m |          -0.0139 |           4.4295 |           0.2533 |
[32m[20221213 15:04:24 @agent_ppo2.py:185][0m |          -0.0130 |           4.3673 |           0.2530 |
[32m[20221213 15:04:24 @agent_ppo2.py:185][0m |          -0.0135 |           4.3138 |           0.2530 |
[32m[20221213 15:04:24 @agent_ppo2.py:185][0m |          -0.0183 |           4.2507 |           0.2531 |
[32m[20221213 15:04:24 @agent_ppo2.py:185][0m |          -0.0187 |           4.2425 |           0.2529 |
[32m[20221213 15:04:25 @agent_ppo2.py:185][0m |          -0.0177 |           4.2091 |           0.2530 |
[32m[20221213 15:04:25 @agent_ppo2.py:185][0m |          -0.0158 |           4.1876 |           0.2530 |
[32m[20221213 15:04:25 @agent_ppo2.py:185][0m |          -0.0206 |           4.1468 |           0.2527 |
[32m[20221213 15:04:25 @agent_ppo2.py:185][0m |          -0.0211 |           4.1159 |           0.2527 |
[32m[20221213 15:04:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.35
[32m[20221213 15:04:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.30
[32m[20221213 15:04:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.65
[32m[20221213 15:04:25 @agent_ppo2.py:143][0m Total time:      11.50 min
[32m[20221213 15:04:25 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221213 15:04:25 @agent_ppo2.py:121][0m #------------------------ Iteration 502 --------------------------#
[32m[20221213 15:04:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:25 @agent_ppo2.py:185][0m |          -0.0021 |           4.4887 |           0.2510 |
[32m[20221213 15:04:25 @agent_ppo2.py:185][0m |           0.0017 |           4.4247 |           0.2502 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0104 |           4.1232 |           0.2496 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0105 |           4.0379 |           0.2496 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0105 |           3.9630 |           0.2494 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0158 |           3.9215 |           0.2494 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0179 |           3.8527 |           0.2490 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0147 |           3.8334 |           0.2492 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0165 |           3.7774 |           0.2491 |
[32m[20221213 15:04:26 @agent_ppo2.py:185][0m |          -0.0181 |           3.7441 |           0.2491 |
[32m[20221213 15:04:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.31
[32m[20221213 15:04:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.39
[32m[20221213 15:04:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 113.62
[32m[20221213 15:04:26 @agent_ppo2.py:143][0m Total time:      11.52 min
[32m[20221213 15:04:26 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221213 15:04:26 @agent_ppo2.py:121][0m #------------------------ Iteration 503 --------------------------#
[32m[20221213 15:04:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |           0.0050 |           5.5028 |           0.2499 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0115 |           4.9164 |           0.2499 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0098 |           4.8013 |           0.2497 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0152 |           4.7368 |           0.2495 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0141 |           4.6606 |           0.2493 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0163 |           4.6314 |           0.2492 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0218 |           4.5738 |           0.2491 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0198 |           4.5342 |           0.2493 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0211 |           4.4688 |           0.2490 |
[32m[20221213 15:04:27 @agent_ppo2.py:185][0m |          -0.0222 |           4.4639 |           0.2490 |
[32m[20221213 15:04:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.83
[32m[20221213 15:04:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.06
[32m[20221213 15:04:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.93
[32m[20221213 15:04:28 @agent_ppo2.py:143][0m Total time:      11.54 min
[32m[20221213 15:04:28 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221213 15:04:28 @agent_ppo2.py:121][0m #------------------------ Iteration 504 --------------------------#
[32m[20221213 15:04:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:28 @agent_ppo2.py:185][0m |          -0.0022 |           5.1464 |           0.2503 |
[32m[20221213 15:04:28 @agent_ppo2.py:185][0m |          -0.0062 |           4.9711 |           0.2497 |
[32m[20221213 15:04:28 @agent_ppo2.py:185][0m |          -0.0046 |           4.8760 |           0.2495 |
[32m[20221213 15:04:28 @agent_ppo2.py:185][0m |          -0.0144 |           4.8201 |           0.2494 |
[32m[20221213 15:04:28 @agent_ppo2.py:185][0m |          -0.0106 |           4.7906 |           0.2493 |
[32m[20221213 15:04:28 @agent_ppo2.py:185][0m |          -0.0164 |           4.7761 |           0.2492 |
[32m[20221213 15:04:29 @agent_ppo2.py:185][0m |          -0.0088 |           4.7669 |           0.2491 |
[32m[20221213 15:04:29 @agent_ppo2.py:185][0m |          -0.0130 |           4.7588 |           0.2490 |
[32m[20221213 15:04:29 @agent_ppo2.py:185][0m |          -0.0170 |           4.6943 |           0.2490 |
[32m[20221213 15:04:29 @agent_ppo2.py:185][0m |          -0.0162 |           4.6930 |           0.2490 |
[32m[20221213 15:04:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.40
[32m[20221213 15:04:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.77
[32m[20221213 15:04:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.06
[32m[20221213 15:04:29 @agent_ppo2.py:143][0m Total time:      11.56 min
[32m[20221213 15:04:29 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221213 15:04:29 @agent_ppo2.py:121][0m #------------------------ Iteration 505 --------------------------#
[32m[20221213 15:04:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:29 @agent_ppo2.py:185][0m |          -0.0022 |           4.7948 |           0.2467 |
[32m[20221213 15:04:29 @agent_ppo2.py:185][0m |          -0.0057 |           4.6230 |           0.2467 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0087 |           4.5803 |           0.2464 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0125 |           4.5051 |           0.2465 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0082 |           4.6295 |           0.2466 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0132 |           4.4349 |           0.2465 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0131 |           4.3882 |           0.2466 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0144 |           4.3777 |           0.2467 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0149 |           4.3477 |           0.2468 |
[32m[20221213 15:04:30 @agent_ppo2.py:185][0m |          -0.0141 |           4.3227 |           0.2469 |
[32m[20221213 15:04:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 114.47
[32m[20221213 15:04:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.38
[32m[20221213 15:04:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.26
[32m[20221213 15:04:30 @agent_ppo2.py:143][0m Total time:      11.59 min
[32m[20221213 15:04:30 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221213 15:04:30 @agent_ppo2.py:121][0m #------------------------ Iteration 506 --------------------------#
[32m[20221213 15:04:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |           0.0016 |           5.1808 |           0.2504 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0061 |           5.0208 |           0.2498 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0109 |           4.9548 |           0.2496 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0111 |           4.9155 |           0.2492 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0149 |           4.8602 |           0.2492 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0125 |           4.8439 |           0.2491 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0102 |           4.8359 |           0.2489 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0175 |           4.7736 |           0.2489 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0144 |           4.7469 |           0.2488 |
[32m[20221213 15:04:31 @agent_ppo2.py:185][0m |          -0.0106 |           4.9973 |           0.2488 |
[32m[20221213 15:04:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.11
[32m[20221213 15:04:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.99
[32m[20221213 15:04:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 39.60
[32m[20221213 15:04:32 @agent_ppo2.py:143][0m Total time:      11.61 min
[32m[20221213 15:04:32 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221213 15:04:32 @agent_ppo2.py:121][0m #------------------------ Iteration 507 --------------------------#
[32m[20221213 15:04:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:32 @agent_ppo2.py:185][0m |          -0.0044 |           5.1794 |           0.2475 |
[32m[20221213 15:04:32 @agent_ppo2.py:185][0m |          -0.0048 |           5.0255 |           0.2473 |
[32m[20221213 15:04:32 @agent_ppo2.py:185][0m |          -0.0085 |           4.9620 |           0.2469 |
[32m[20221213 15:04:32 @agent_ppo2.py:185][0m |          -0.0075 |           5.1838 |           0.2468 |
[32m[20221213 15:04:32 @agent_ppo2.py:185][0m |          -0.0132 |           4.8879 |           0.2467 |
[32m[20221213 15:04:32 @agent_ppo2.py:185][0m |          -0.0157 |           4.8619 |           0.2464 |
[32m[20221213 15:04:33 @agent_ppo2.py:185][0m |          -0.0172 |           4.8193 |           0.2464 |
[32m[20221213 15:04:33 @agent_ppo2.py:185][0m |          -0.0132 |           4.8030 |           0.2464 |
[32m[20221213 15:04:33 @agent_ppo2.py:185][0m |          -0.0181 |           4.7900 |           0.2462 |
[32m[20221213 15:04:33 @agent_ppo2.py:185][0m |          -0.0211 |           4.7631 |           0.2462 |
[32m[20221213 15:04:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.85
[32m[20221213 15:04:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 123.26
[32m[20221213 15:04:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.61
[32m[20221213 15:04:33 @agent_ppo2.py:143][0m Total time:      11.63 min
[32m[20221213 15:04:33 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221213 15:04:33 @agent_ppo2.py:121][0m #------------------------ Iteration 508 --------------------------#
[32m[20221213 15:04:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:33 @agent_ppo2.py:185][0m |          -0.0024 |           4.3971 |           0.2457 |
[32m[20221213 15:04:33 @agent_ppo2.py:185][0m |           0.0052 |           4.2764 |           0.2456 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0103 |           3.8607 |           0.2455 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0116 |           3.7491 |           0.2458 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0137 |           3.6553 |           0.2459 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0169 |           3.6066 |           0.2459 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0123 |           3.6343 |           0.2461 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0181 |           3.5336 |           0.2464 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0157 |           3.5147 |           0.2464 |
[32m[20221213 15:04:34 @agent_ppo2.py:185][0m |          -0.0159 |           3.4914 |           0.2467 |
[32m[20221213 15:04:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.07
[32m[20221213 15:04:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.50
[32m[20221213 15:04:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 46.31
[32m[20221213 15:04:34 @agent_ppo2.py:143][0m Total time:      11.65 min
[32m[20221213 15:04:34 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221213 15:04:34 @agent_ppo2.py:121][0m #------------------------ Iteration 509 --------------------------#
[32m[20221213 15:04:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0004 |           5.2543 |           0.2494 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0081 |           5.1463 |           0.2491 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0103 |           5.0811 |           0.2489 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0142 |           5.0351 |           0.2490 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0146 |           5.0234 |           0.2490 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0100 |           5.2685 |           0.2488 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0160 |           4.9749 |           0.2487 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0202 |           4.9446 |           0.2491 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0183 |           4.9309 |           0.2491 |
[32m[20221213 15:04:35 @agent_ppo2.py:185][0m |          -0.0186 |           4.9446 |           0.2490 |
[32m[20221213 15:04:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.85
[32m[20221213 15:04:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.00
[32m[20221213 15:04:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.49
[32m[20221213 15:04:36 @agent_ppo2.py:143][0m Total time:      11.67 min
[32m[20221213 15:04:36 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221213 15:04:36 @agent_ppo2.py:121][0m #------------------------ Iteration 510 --------------------------#
[32m[20221213 15:04:36 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:04:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:36 @agent_ppo2.py:185][0m |          -0.0013 |           5.7778 |           0.2507 |
[32m[20221213 15:04:36 @agent_ppo2.py:185][0m |          -0.0131 |           5.4324 |           0.2504 |
[32m[20221213 15:04:36 @agent_ppo2.py:185][0m |          -0.0142 |           5.3240 |           0.2502 |
[32m[20221213 15:04:36 @agent_ppo2.py:185][0m |          -0.0113 |           5.2373 |           0.2499 |
[32m[20221213 15:04:36 @agent_ppo2.py:185][0m |          -0.0075 |           5.3365 |           0.2498 |
[32m[20221213 15:04:37 @agent_ppo2.py:185][0m |          -0.0093 |           5.2856 |           0.2495 |
[32m[20221213 15:04:37 @agent_ppo2.py:185][0m |          -0.0175 |           5.0151 |           0.2494 |
[32m[20221213 15:04:37 @agent_ppo2.py:185][0m |          -0.0137 |           4.9741 |           0.2493 |
[32m[20221213 15:04:37 @agent_ppo2.py:185][0m |          -0.0215 |           4.9355 |           0.2494 |
[32m[20221213 15:04:37 @agent_ppo2.py:185][0m |          -0.0197 |           4.8881 |           0.2490 |
[32m[20221213 15:04:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.06
[32m[20221213 15:04:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.75
[32m[20221213 15:04:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.02
[32m[20221213 15:04:37 @agent_ppo2.py:143][0m Total time:      11.70 min
[32m[20221213 15:04:37 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221213 15:04:37 @agent_ppo2.py:121][0m #------------------------ Iteration 511 --------------------------#
[32m[20221213 15:04:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:37 @agent_ppo2.py:185][0m |          -0.0018 |           5.0302 |           0.2548 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0124 |           4.8312 |           0.2543 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0087 |           4.6910 |           0.2537 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |           0.0005 |           4.8736 |           0.2539 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0144 |           4.6124 |           0.2536 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0146 |           4.5441 |           0.2535 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0160 |           4.5160 |           0.2536 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0105 |           4.5341 |           0.2532 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0186 |           4.4686 |           0.2530 |
[32m[20221213 15:04:38 @agent_ppo2.py:185][0m |          -0.0180 |           4.4171 |           0.2530 |
[32m[20221213 15:04:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.11
[32m[20221213 15:04:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.16
[32m[20221213 15:04:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 112.03
[32m[20221213 15:04:38 @agent_ppo2.py:143][0m Total time:      11.72 min
[32m[20221213 15:04:38 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221213 15:04:38 @agent_ppo2.py:121][0m #------------------------ Iteration 512 --------------------------#
[32m[20221213 15:04:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |           0.0005 |           5.0660 |           0.2416 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0084 |           4.7718 |           0.2412 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0106 |           4.6725 |           0.2408 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0113 |           4.5875 |           0.2407 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0121 |           4.5094 |           0.2405 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0169 |           4.5161 |           0.2405 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0125 |           4.4219 |           0.2406 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0147 |           4.3656 |           0.2404 |
[32m[20221213 15:04:39 @agent_ppo2.py:185][0m |          -0.0196 |           4.3539 |           0.2405 |
[32m[20221213 15:04:40 @agent_ppo2.py:185][0m |          -0.0214 |           4.3187 |           0.2404 |
[32m[20221213 15:04:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:04:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 106.40
[32m[20221213 15:04:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 117.47
[32m[20221213 15:04:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.78
[32m[20221213 15:04:40 @agent_ppo2.py:143][0m Total time:      11.74 min
[32m[20221213 15:04:40 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221213 15:04:40 @agent_ppo2.py:121][0m #------------------------ Iteration 513 --------------------------#
[32m[20221213 15:04:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:40 @agent_ppo2.py:185][0m |          -0.0056 |           5.1341 |           0.2394 |
[32m[20221213 15:04:40 @agent_ppo2.py:185][0m |          -0.0088 |           4.8704 |           0.2391 |
[32m[20221213 15:04:40 @agent_ppo2.py:185][0m |          -0.0116 |           4.7316 |           0.2392 |
[32m[20221213 15:04:40 @agent_ppo2.py:185][0m |          -0.0165 |           4.6562 |           0.2392 |
[32m[20221213 15:04:40 @agent_ppo2.py:185][0m |          -0.0123 |           4.6040 |           0.2390 |
[32m[20221213 15:04:41 @agent_ppo2.py:185][0m |          -0.0144 |           4.5446 |           0.2393 |
[32m[20221213 15:04:41 @agent_ppo2.py:185][0m |          -0.0158 |           4.4964 |           0.2393 |
[32m[20221213 15:04:41 @agent_ppo2.py:185][0m |          -0.0069 |           4.6965 |           0.2393 |
[32m[20221213 15:04:41 @agent_ppo2.py:185][0m |          -0.0116 |           4.5014 |           0.2392 |
[32m[20221213 15:04:41 @agent_ppo2.py:185][0m |          -0.0234 |           4.3881 |           0.2389 |
[32m[20221213 15:04:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 107.73
[32m[20221213 15:04:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 123.52
[32m[20221213 15:04:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.25
[32m[20221213 15:04:41 @agent_ppo2.py:143][0m Total time:      11.76 min
[32m[20221213 15:04:41 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221213 15:04:41 @agent_ppo2.py:121][0m #------------------------ Iteration 514 --------------------------#
[32m[20221213 15:04:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:41 @agent_ppo2.py:185][0m |           0.0007 |           5.2770 |           0.2549 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0067 |           5.0682 |           0.2545 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0089 |           4.9803 |           0.2542 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0085 |           4.9066 |           0.2541 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0132 |           4.8654 |           0.2540 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0130 |           4.8189 |           0.2539 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0145 |           4.7795 |           0.2539 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0128 |           4.7717 |           0.2540 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0144 |           4.7525 |           0.2536 |
[32m[20221213 15:04:42 @agent_ppo2.py:185][0m |          -0.0167 |           4.7145 |           0.2537 |
[32m[20221213 15:04:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.10
[32m[20221213 15:04:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 135.62
[32m[20221213 15:04:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.52
[32m[20221213 15:04:42 @agent_ppo2.py:143][0m Total time:      11.79 min
[32m[20221213 15:04:42 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221213 15:04:42 @agent_ppo2.py:121][0m #------------------------ Iteration 515 --------------------------#
[32m[20221213 15:04:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |           0.0006 |           4.7914 |           0.2398 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0099 |           4.6351 |           0.2399 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0109 |           4.6324 |           0.2399 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0144 |           4.5382 |           0.2401 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0128 |           4.4869 |           0.2400 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0134 |           4.4761 |           0.2401 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0144 |           4.4588 |           0.2403 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0139 |           4.4159 |           0.2405 |
[32m[20221213 15:04:43 @agent_ppo2.py:185][0m |          -0.0161 |           4.3768 |           0.2404 |
[32m[20221213 15:04:44 @agent_ppo2.py:185][0m |          -0.0171 |           4.3833 |           0.2404 |
[32m[20221213 15:04:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.28
[32m[20221213 15:04:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.84
[32m[20221213 15:04:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.67
[32m[20221213 15:04:44 @agent_ppo2.py:143][0m Total time:      11.81 min
[32m[20221213 15:04:44 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221213 15:04:44 @agent_ppo2.py:121][0m #------------------------ Iteration 516 --------------------------#
[32m[20221213 15:04:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:44 @agent_ppo2.py:185][0m |           0.0049 |           4.9344 |           0.2536 |
[32m[20221213 15:04:44 @agent_ppo2.py:185][0m |          -0.0064 |           4.6954 |           0.2532 |
[32m[20221213 15:04:44 @agent_ppo2.py:185][0m |          -0.0082 |           4.6244 |           0.2529 |
[32m[20221213 15:04:44 @agent_ppo2.py:185][0m |          -0.0121 |           4.5502 |           0.2528 |
[32m[20221213 15:04:44 @agent_ppo2.py:185][0m |          -0.0126 |           4.4980 |           0.2529 |
[32m[20221213 15:04:45 @agent_ppo2.py:185][0m |          -0.0142 |           4.4724 |           0.2530 |
[32m[20221213 15:04:45 @agent_ppo2.py:185][0m |          -0.0150 |           4.4410 |           0.2529 |
[32m[20221213 15:04:45 @agent_ppo2.py:185][0m |          -0.0142 |           4.4068 |           0.2530 |
[32m[20221213 15:04:45 @agent_ppo2.py:185][0m |          -0.0155 |           4.3792 |           0.2530 |
[32m[20221213 15:04:45 @agent_ppo2.py:185][0m |          -0.0156 |           4.3422 |           0.2531 |
[32m[20221213 15:04:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 109.49
[32m[20221213 15:04:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 121.34
[32m[20221213 15:04:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.61
[32m[20221213 15:04:45 @agent_ppo2.py:143][0m Total time:      11.83 min
[32m[20221213 15:04:45 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221213 15:04:45 @agent_ppo2.py:121][0m #------------------------ Iteration 517 --------------------------#
[32m[20221213 15:04:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:45 @agent_ppo2.py:185][0m |          -0.0014 |           4.8869 |           0.2467 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0066 |           4.7226 |           0.2465 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0102 |           4.6224 |           0.2464 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0156 |           4.5553 |           0.2462 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0164 |           4.5157 |           0.2463 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0135 |           4.4722 |           0.2463 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0164 |           4.4275 |           0.2463 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0171 |           4.4141 |           0.2464 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0191 |           4.3855 |           0.2464 |
[32m[20221213 15:04:46 @agent_ppo2.py:185][0m |          -0.0158 |           4.4996 |           0.2465 |
[32m[20221213 15:04:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.72
[32m[20221213 15:04:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.59
[32m[20221213 15:04:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.60
[32m[20221213 15:04:46 @agent_ppo2.py:143][0m Total time:      11.85 min
[32m[20221213 15:04:46 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221213 15:04:46 @agent_ppo2.py:121][0m #------------------------ Iteration 518 --------------------------#
[32m[20221213 15:04:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0008 |           4.9962 |           0.2548 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0054 |           4.7763 |           0.2545 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0080 |           4.6669 |           0.2541 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0126 |           4.6164 |           0.2542 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0177 |           4.5871 |           0.2540 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0029 |           4.8775 |           0.2540 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0171 |           4.4788 |           0.2539 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0152 |           4.4495 |           0.2537 |
[32m[20221213 15:04:47 @agent_ppo2.py:185][0m |          -0.0208 |           4.4226 |           0.2538 |
[32m[20221213 15:04:48 @agent_ppo2.py:185][0m |          -0.0164 |           4.4083 |           0.2537 |
[32m[20221213 15:04:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:04:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 97.92
[32m[20221213 15:04:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.18
[32m[20221213 15:04:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 118.40
[32m[20221213 15:04:48 @agent_ppo2.py:143][0m Total time:      11.88 min
[32m[20221213 15:04:48 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221213 15:04:48 @agent_ppo2.py:121][0m #------------------------ Iteration 519 --------------------------#
[32m[20221213 15:04:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:48 @agent_ppo2.py:185][0m |          -0.0009 |           4.8037 |           0.2546 |
[32m[20221213 15:04:48 @agent_ppo2.py:185][0m |          -0.0084 |           4.5677 |           0.2540 |
[32m[20221213 15:04:48 @agent_ppo2.py:185][0m |          -0.0106 |           4.4914 |           0.2535 |
[32m[20221213 15:04:48 @agent_ppo2.py:185][0m |          -0.0110 |           4.4197 |           0.2532 |
[32m[20221213 15:04:48 @agent_ppo2.py:185][0m |          -0.0095 |           4.3838 |           0.2530 |
[32m[20221213 15:04:49 @agent_ppo2.py:185][0m |          -0.0143 |           4.3512 |           0.2529 |
[32m[20221213 15:04:49 @agent_ppo2.py:185][0m |          -0.0116 |           4.3455 |           0.2529 |
[32m[20221213 15:04:49 @agent_ppo2.py:185][0m |          -0.0180 |           4.2957 |           0.2526 |
[32m[20221213 15:04:49 @agent_ppo2.py:185][0m |          -0.0155 |           4.2729 |           0.2526 |
[32m[20221213 15:04:49 @agent_ppo2.py:185][0m |          -0.0191 |           4.2392 |           0.2526 |
[32m[20221213 15:04:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.28
[32m[20221213 15:04:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 135.26
[32m[20221213 15:04:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.91
[32m[20221213 15:04:49 @agent_ppo2.py:143][0m Total time:      11.90 min
[32m[20221213 15:04:49 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221213 15:04:49 @agent_ppo2.py:121][0m #------------------------ Iteration 520 --------------------------#
[32m[20221213 15:04:49 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:04:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:49 @agent_ppo2.py:185][0m |           0.0030 |           4.7377 |           0.2402 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0053 |           4.5098 |           0.2396 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0033 |           4.6094 |           0.2392 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0096 |           4.3826 |           0.2390 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0087 |           4.4030 |           0.2391 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0136 |           4.2800 |           0.2391 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0174 |           4.2391 |           0.2391 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0108 |           4.2687 |           0.2392 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0148 |           4.1868 |           0.2391 |
[32m[20221213 15:04:50 @agent_ppo2.py:185][0m |          -0.0190 |           4.1536 |           0.2390 |
[32m[20221213 15:04:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.54
[32m[20221213 15:04:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 120.06
[32m[20221213 15:04:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.18
[32m[20221213 15:04:50 @agent_ppo2.py:143][0m Total time:      11.92 min
[32m[20221213 15:04:50 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221213 15:04:50 @agent_ppo2.py:121][0m #------------------------ Iteration 521 --------------------------#
[32m[20221213 15:04:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0009 |           4.6282 |           0.2515 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0084 |           4.4690 |           0.2512 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |           0.0048 |           4.7060 |           0.2511 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0103 |           4.3651 |           0.2510 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0120 |           4.3355 |           0.2509 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0125 |           4.3242 |           0.2508 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0156 |           4.2981 |           0.2505 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0145 |           4.2660 |           0.2506 |
[32m[20221213 15:04:51 @agent_ppo2.py:185][0m |          -0.0110 |           4.2942 |           0.2505 |
[32m[20221213 15:04:52 @agent_ppo2.py:185][0m |          -0.0134 |           4.2489 |           0.2505 |
[32m[20221213 15:04:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.90
[32m[20221213 15:04:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.37
[32m[20221213 15:04:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 112.86
[32m[20221213 15:04:52 @agent_ppo2.py:143][0m Total time:      11.94 min
[32m[20221213 15:04:52 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221213 15:04:52 @agent_ppo2.py:121][0m #------------------------ Iteration 522 --------------------------#
[32m[20221213 15:04:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:52 @agent_ppo2.py:185][0m |          -0.0018 |           4.6524 |           0.2476 |
[32m[20221213 15:04:52 @agent_ppo2.py:185][0m |          -0.0028 |           4.5324 |           0.2470 |
[32m[20221213 15:04:52 @agent_ppo2.py:185][0m |          -0.0092 |           4.4704 |           0.2467 |
[32m[20221213 15:04:52 @agent_ppo2.py:185][0m |          -0.0106 |           4.4524 |           0.2465 |
[32m[20221213 15:04:52 @agent_ppo2.py:185][0m |           0.0106 |           5.1441 |           0.2464 |
[32m[20221213 15:04:53 @agent_ppo2.py:185][0m |          -0.0055 |           4.7155 |           0.2461 |
[32m[20221213 15:04:53 @agent_ppo2.py:185][0m |          -0.0140 |           4.4189 |           0.2459 |
[32m[20221213 15:04:53 @agent_ppo2.py:185][0m |          -0.0100 |           4.4346 |           0.2459 |
[32m[20221213 15:04:53 @agent_ppo2.py:185][0m |          -0.0130 |           4.3675 |           0.2459 |
[32m[20221213 15:04:53 @agent_ppo2.py:185][0m |          -0.0144 |           4.3208 |           0.2460 |
[32m[20221213 15:04:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.51
[32m[20221213 15:04:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.79
[32m[20221213 15:04:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 110.07
[32m[20221213 15:04:53 @agent_ppo2.py:143][0m Total time:      11.97 min
[32m[20221213 15:04:53 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221213 15:04:53 @agent_ppo2.py:121][0m #------------------------ Iteration 523 --------------------------#
[32m[20221213 15:04:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:53 @agent_ppo2.py:185][0m |          -0.0014 |           4.4980 |           0.2531 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0085 |           4.3912 |           0.2529 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0114 |           4.3322 |           0.2527 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0089 |           4.3019 |           0.2526 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0103 |           4.2602 |           0.2528 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0088 |           4.2359 |           0.2528 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0137 |           4.1891 |           0.2527 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0068 |           4.5967 |           0.2527 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0034 |           4.6421 |           0.2525 |
[32m[20221213 15:04:54 @agent_ppo2.py:185][0m |          -0.0168 |           4.1516 |           0.2524 |
[32m[20221213 15:04:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.35
[32m[20221213 15:04:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.56
[32m[20221213 15:04:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.52
[32m[20221213 15:04:54 @agent_ppo2.py:143][0m Total time:      11.99 min
[32m[20221213 15:04:54 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221213 15:04:54 @agent_ppo2.py:121][0m #------------------------ Iteration 524 --------------------------#
[32m[20221213 15:04:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |           0.0012 |           4.9929 |           0.2523 |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |          -0.0052 |           4.7643 |           0.2517 |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |          -0.0095 |           4.6803 |           0.2519 |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |          -0.0099 |           4.6588 |           0.2517 |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |          -0.0130 |           4.5697 |           0.2519 |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |          -0.0120 |           4.5352 |           0.2518 |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |          -0.0133 |           4.5053 |           0.2517 |
[32m[20221213 15:04:55 @agent_ppo2.py:185][0m |          -0.0184 |           4.4869 |           0.2517 |
[32m[20221213 15:04:56 @agent_ppo2.py:185][0m |          -0.0142 |           4.4706 |           0.2517 |
[32m[20221213 15:04:56 @agent_ppo2.py:185][0m |          -0.0163 |           4.3969 |           0.2517 |
[32m[20221213 15:04:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.29
[32m[20221213 15:04:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.12
[32m[20221213 15:04:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.97
[32m[20221213 15:04:56 @agent_ppo2.py:143][0m Total time:      12.01 min
[32m[20221213 15:04:56 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221213 15:04:56 @agent_ppo2.py:121][0m #------------------------ Iteration 525 --------------------------#
[32m[20221213 15:04:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:56 @agent_ppo2.py:185][0m |           0.0011 |           4.7954 |           0.2412 |
[32m[20221213 15:04:56 @agent_ppo2.py:185][0m |          -0.0079 |           4.6789 |           0.2404 |
[32m[20221213 15:04:56 @agent_ppo2.py:185][0m |          -0.0081 |           4.6410 |           0.2405 |
[32m[20221213 15:04:56 @agent_ppo2.py:185][0m |          -0.0127 |           4.5923 |           0.2404 |
[32m[20221213 15:04:56 @agent_ppo2.py:185][0m |          -0.0136 |           4.5439 |           0.2403 |
[32m[20221213 15:04:57 @agent_ppo2.py:185][0m |          -0.0151 |           4.5186 |           0.2403 |
[32m[20221213 15:04:57 @agent_ppo2.py:185][0m |          -0.0157 |           4.5125 |           0.2401 |
[32m[20221213 15:04:57 @agent_ppo2.py:185][0m |          -0.0171 |           4.4884 |           0.2402 |
[32m[20221213 15:04:57 @agent_ppo2.py:185][0m |          -0.0177 |           4.4573 |           0.2403 |
[32m[20221213 15:04:57 @agent_ppo2.py:185][0m |          -0.0167 |           4.4423 |           0.2401 |
[32m[20221213 15:04:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.76
[32m[20221213 15:04:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 130.95
[32m[20221213 15:04:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.41
[32m[20221213 15:04:57 @agent_ppo2.py:143][0m Total time:      12.03 min
[32m[20221213 15:04:57 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221213 15:04:57 @agent_ppo2.py:121][0m #------------------------ Iteration 526 --------------------------#
[32m[20221213 15:04:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:04:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:57 @agent_ppo2.py:185][0m |          -0.0035 |           4.8594 |           0.2521 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0051 |           4.7735 |           0.2520 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0119 |           4.7608 |           0.2517 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0112 |           4.7212 |           0.2518 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0122 |           4.6827 |           0.2517 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0132 |           4.6564 |           0.2516 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0157 |           4.6421 |           0.2515 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0149 |           4.6187 |           0.2516 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0087 |           4.7174 |           0.2516 |
[32m[20221213 15:04:58 @agent_ppo2.py:185][0m |          -0.0173 |           4.6106 |           0.2515 |
[32m[20221213 15:04:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:04:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.46
[32m[20221213 15:04:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.59
[32m[20221213 15:04:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.40
[32m[20221213 15:04:58 @agent_ppo2.py:143][0m Total time:      12.05 min
[32m[20221213 15:04:58 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221213 15:04:58 @agent_ppo2.py:121][0m #------------------------ Iteration 527 --------------------------#
[32m[20221213 15:04:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:04:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |           0.0067 |           4.8005 |           0.2489 |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |          -0.0062 |           4.5000 |           0.2485 |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |          -0.0030 |           4.5553 |           0.2483 |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |          -0.0064 |           4.4564 |           0.2485 |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |          -0.0101 |           4.3805 |           0.2482 |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |          -0.0123 |           4.3528 |           0.2482 |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |          -0.0145 |           4.3443 |           0.2479 |
[32m[20221213 15:04:59 @agent_ppo2.py:185][0m |          -0.0123 |           4.3066 |           0.2479 |
[32m[20221213 15:05:00 @agent_ppo2.py:185][0m |          -0.0152 |           4.2962 |           0.2478 |
[32m[20221213 15:05:00 @agent_ppo2.py:185][0m |          -0.0163 |           4.2692 |           0.2477 |
[32m[20221213 15:05:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:05:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.77
[32m[20221213 15:05:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 125.42
[32m[20221213 15:05:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.86
[32m[20221213 15:05:00 @agent_ppo2.py:143][0m Total time:      12.08 min
[32m[20221213 15:05:00 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221213 15:05:00 @agent_ppo2.py:121][0m #------------------------ Iteration 528 --------------------------#
[32m[20221213 15:05:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:00 @agent_ppo2.py:185][0m |          -0.0038 |           4.8105 |           0.2474 |
[32m[20221213 15:05:00 @agent_ppo2.py:185][0m |          -0.0093 |           4.7382 |           0.2471 |
[32m[20221213 15:05:00 @agent_ppo2.py:185][0m |          -0.0000 |           4.7716 |           0.2473 |
[32m[20221213 15:05:00 @agent_ppo2.py:185][0m |          -0.0128 |           4.6544 |           0.2471 |
[32m[20221213 15:05:01 @agent_ppo2.py:185][0m |          -0.0130 |           4.6082 |           0.2473 |
[32m[20221213 15:05:01 @agent_ppo2.py:185][0m |          -0.0147 |           4.5902 |           0.2472 |
[32m[20221213 15:05:01 @agent_ppo2.py:185][0m |          -0.0139 |           4.5707 |           0.2474 |
[32m[20221213 15:05:01 @agent_ppo2.py:185][0m |          -0.0152 |           4.5528 |           0.2474 |
[32m[20221213 15:05:01 @agent_ppo2.py:185][0m |          -0.0153 |           4.5492 |           0.2475 |
[32m[20221213 15:05:01 @agent_ppo2.py:185][0m |          -0.0145 |           4.5384 |           0.2476 |
[32m[20221213 15:05:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:05:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.46
[32m[20221213 15:05:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.28
[32m[20221213 15:05:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.73
[32m[20221213 15:05:01 @agent_ppo2.py:143][0m Total time:      12.10 min
[32m[20221213 15:05:01 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221213 15:05:01 @agent_ppo2.py:121][0m #------------------------ Iteration 529 --------------------------#
[32m[20221213 15:05:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |           0.0044 |           4.5733 |           0.2520 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0061 |           4.3348 |           0.2519 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0018 |           4.2936 |           0.2517 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0137 |           4.1976 |           0.2516 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0117 |           4.1606 |           0.2516 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0137 |           4.1455 |           0.2515 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0123 |           4.1153 |           0.2513 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0123 |           4.0941 |           0.2514 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0170 |           4.1047 |           0.2512 |
[32m[20221213 15:05:02 @agent_ppo2.py:185][0m |          -0.0162 |           4.0760 |           0.2512 |
[32m[20221213 15:05:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:05:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.02
[32m[20221213 15:05:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.27
[32m[20221213 15:05:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 121.04
[32m[20221213 15:05:03 @agent_ppo2.py:143][0m Total time:      12.12 min
[32m[20221213 15:05:03 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221213 15:05:03 @agent_ppo2.py:121][0m #------------------------ Iteration 530 --------------------------#
[32m[20221213 15:05:03 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:05:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:03 @agent_ppo2.py:185][0m |          -0.0024 |           4.9925 |           0.2505 |
[32m[20221213 15:05:03 @agent_ppo2.py:185][0m |          -0.0091 |           4.8526 |           0.2503 |
[32m[20221213 15:05:03 @agent_ppo2.py:185][0m |          -0.0089 |           4.8046 |           0.2500 |
[32m[20221213 15:05:03 @agent_ppo2.py:185][0m |          -0.0092 |           4.7878 |           0.2501 |
[32m[20221213 15:05:03 @agent_ppo2.py:185][0m |          -0.0107 |           4.7424 |           0.2500 |
[32m[20221213 15:05:04 @agent_ppo2.py:185][0m |          -0.0123 |           4.7296 |           0.2499 |
[32m[20221213 15:05:04 @agent_ppo2.py:185][0m |          -0.0113 |           4.7061 |           0.2500 |
[32m[20221213 15:05:04 @agent_ppo2.py:185][0m |          -0.0013 |           5.1218 |           0.2499 |
[32m[20221213 15:05:04 @agent_ppo2.py:185][0m |          -0.0142 |           4.6945 |           0.2497 |
[32m[20221213 15:05:04 @agent_ppo2.py:185][0m |          -0.0174 |           4.6807 |           0.2499 |
[32m[20221213 15:05:04 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:05:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 113.22
[32m[20221213 15:05:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 123.85
[32m[20221213 15:05:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 131.41
[32m[20221213 15:05:04 @agent_ppo2.py:143][0m Total time:      12.15 min
[32m[20221213 15:05:04 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221213 15:05:04 @agent_ppo2.py:121][0m #------------------------ Iteration 531 --------------------------#
[32m[20221213 15:05:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:04 @agent_ppo2.py:185][0m |           0.0002 |           4.9099 |           0.2490 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |           0.0010 |           4.9323 |           0.2486 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0111 |           4.7331 |           0.2485 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0127 |           4.6907 |           0.2483 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0118 |           4.6569 |           0.2481 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0150 |           4.6269 |           0.2479 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0118 |           4.6016 |           0.2479 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0090 |           4.6816 |           0.2478 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0176 |           4.5595 |           0.2478 |
[32m[20221213 15:05:05 @agent_ppo2.py:185][0m |          -0.0105 |           4.6201 |           0.2480 |
[32m[20221213 15:05:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.41
[32m[20221213 15:05:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.07
[32m[20221213 15:05:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.57
[32m[20221213 15:05:05 @agent_ppo2.py:143][0m Total time:      12.17 min
[32m[20221213 15:05:05 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221213 15:05:05 @agent_ppo2.py:121][0m #------------------------ Iteration 532 --------------------------#
[32m[20221213 15:05:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |           0.0106 |           5.2095 |           0.2556 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0083 |           4.8362 |           0.2553 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0098 |           4.7987 |           0.2552 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0117 |           4.7455 |           0.2550 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0070 |           4.9019 |           0.2550 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0146 |           4.7065 |           0.2547 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0151 |           4.6802 |           0.2547 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0154 |           4.6967 |           0.2550 |
[32m[20221213 15:05:06 @agent_ppo2.py:185][0m |          -0.0163 |           4.6897 |           0.2549 |
[32m[20221213 15:05:07 @agent_ppo2.py:185][0m |          -0.0197 |           4.6402 |           0.2551 |
[32m[20221213 15:05:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.49
[32m[20221213 15:05:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.30
[32m[20221213 15:05:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.88
[32m[20221213 15:05:07 @agent_ppo2.py:143][0m Total time:      12.19 min
[32m[20221213 15:05:07 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221213 15:05:07 @agent_ppo2.py:121][0m #------------------------ Iteration 533 --------------------------#
[32m[20221213 15:05:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:07 @agent_ppo2.py:185][0m |          -0.0016 |           4.8004 |           0.2446 |
[32m[20221213 15:05:07 @agent_ppo2.py:185][0m |          -0.0041 |           4.7095 |           0.2443 |
[32m[20221213 15:05:07 @agent_ppo2.py:185][0m |          -0.0097 |           4.6866 |           0.2444 |
[32m[20221213 15:05:07 @agent_ppo2.py:185][0m |           0.0020 |           4.8590 |           0.2442 |
[32m[20221213 15:05:07 @agent_ppo2.py:185][0m |          -0.0116 |           4.6255 |           0.2442 |
[32m[20221213 15:05:08 @agent_ppo2.py:185][0m |          -0.0028 |           4.8387 |           0.2442 |
[32m[20221213 15:05:08 @agent_ppo2.py:185][0m |          -0.0127 |           4.6230 |           0.2445 |
[32m[20221213 15:05:08 @agent_ppo2.py:185][0m |          -0.0165 |           4.5624 |           0.2443 |
[32m[20221213 15:05:08 @agent_ppo2.py:185][0m |          -0.0168 |           4.5574 |           0.2444 |
[32m[20221213 15:05:08 @agent_ppo2.py:185][0m |          -0.0152 |           4.5287 |           0.2443 |
[32m[20221213 15:05:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.97
[32m[20221213 15:05:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.04
[32m[20221213 15:05:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 122.41
[32m[20221213 15:05:08 @agent_ppo2.py:143][0m Total time:      12.21 min
[32m[20221213 15:05:08 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221213 15:05:08 @agent_ppo2.py:121][0m #------------------------ Iteration 534 --------------------------#
[32m[20221213 15:05:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:05:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:08 @agent_ppo2.py:185][0m |          -0.0030 |           4.5540 |           0.2576 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0090 |           4.4850 |           0.2576 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0094 |           4.4531 |           0.2575 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0071 |           4.4628 |           0.2576 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0137 |           4.3871 |           0.2577 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0104 |           4.3640 |           0.2579 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0125 |           4.3404 |           0.2579 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0147 |           4.3177 |           0.2580 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0160 |           4.3218 |           0.2580 |
[32m[20221213 15:05:09 @agent_ppo2.py:185][0m |          -0.0139 |           4.2920 |           0.2582 |
[32m[20221213 15:05:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.98
[32m[20221213 15:05:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.88
[32m[20221213 15:05:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.80
[32m[20221213 15:05:09 @agent_ppo2.py:143][0m Total time:      12.24 min
[32m[20221213 15:05:09 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221213 15:05:09 @agent_ppo2.py:121][0m #------------------------ Iteration 535 --------------------------#
[32m[20221213 15:05:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |           0.0015 |           4.4731 |           0.2632 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |          -0.0068 |           4.3148 |           0.2631 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |           0.0004 |           4.6416 |           0.2630 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |          -0.0095 |           4.2185 |           0.2631 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |          -0.0119 |           4.1757 |           0.2631 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |          -0.0159 |           4.1544 |           0.2633 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |          -0.0116 |           4.1824 |           0.2634 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |          -0.0135 |           4.0889 |           0.2634 |
[32m[20221213 15:05:10 @agent_ppo2.py:185][0m |          -0.0205 |           4.0605 |           0.2635 |
[32m[20221213 15:05:11 @agent_ppo2.py:185][0m |          -0.0126 |           4.0720 |           0.2636 |
[32m[20221213 15:05:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:05:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.91
[32m[20221213 15:05:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 127.23
[32m[20221213 15:05:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.18
[32m[20221213 15:05:11 @agent_ppo2.py:143][0m Total time:      12.26 min
[32m[20221213 15:05:11 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221213 15:05:11 @agent_ppo2.py:121][0m #------------------------ Iteration 536 --------------------------#
[32m[20221213 15:05:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:11 @agent_ppo2.py:185][0m |           0.0025 |           4.6763 |           0.2650 |
[32m[20221213 15:05:11 @agent_ppo2.py:185][0m |          -0.0002 |           4.6393 |           0.2651 |
[32m[20221213 15:05:11 @agent_ppo2.py:185][0m |          -0.0078 |           4.5444 |           0.2650 |
[32m[20221213 15:05:11 @agent_ppo2.py:185][0m |          -0.0078 |           4.5573 |           0.2649 |
[32m[20221213 15:05:11 @agent_ppo2.py:185][0m |          -0.0124 |           4.5041 |           0.2649 |
[32m[20221213 15:05:12 @agent_ppo2.py:185][0m |          -0.0096 |           4.5635 |           0.2650 |
[32m[20221213 15:05:12 @agent_ppo2.py:185][0m |          -0.0123 |           4.4840 |           0.2648 |
[32m[20221213 15:05:12 @agent_ppo2.py:185][0m |          -0.0145 |           4.4551 |           0.2651 |
[32m[20221213 15:05:12 @agent_ppo2.py:185][0m |          -0.0135 |           4.4383 |           0.2651 |
[32m[20221213 15:05:12 @agent_ppo2.py:185][0m |          -0.0153 |           4.4235 |           0.2652 |
[32m[20221213 15:05:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.81
[32m[20221213 15:05:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 146.41
[32m[20221213 15:05:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 148.89
[32m[20221213 15:05:12 @agent_ppo2.py:143][0m Total time:      12.28 min
[32m[20221213 15:05:12 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221213 15:05:12 @agent_ppo2.py:121][0m #------------------------ Iteration 537 --------------------------#
[32m[20221213 15:05:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:12 @agent_ppo2.py:185][0m |          -0.0021 |           4.4994 |           0.2624 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0080 |           4.3232 |           0.2623 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0119 |           4.2126 |           0.2622 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0153 |           4.1624 |           0.2625 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0154 |           4.1033 |           0.2623 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0135 |           4.0612 |           0.2624 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0162 |           4.0038 |           0.2625 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0107 |           4.1034 |           0.2625 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0206 |           3.9535 |           0.2626 |
[32m[20221213 15:05:13 @agent_ppo2.py:185][0m |          -0.0202 |           3.9289 |           0.2625 |
[32m[20221213 15:05:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.21
[32m[20221213 15:05:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.19
[32m[20221213 15:05:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.69
[32m[20221213 15:05:13 @agent_ppo2.py:143][0m Total time:      12.30 min
[32m[20221213 15:05:13 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221213 15:05:13 @agent_ppo2.py:121][0m #------------------------ Iteration 538 --------------------------#
[32m[20221213 15:05:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:05:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0004 |           4.6867 |           0.2670 |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0069 |           4.5105 |           0.2665 |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0095 |           4.3860 |           0.2661 |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0125 |           4.3078 |           0.2661 |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0140 |           4.2391 |           0.2659 |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0182 |           4.1779 |           0.2658 |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0064 |           4.2615 |           0.2655 |
[32m[20221213 15:05:14 @agent_ppo2.py:185][0m |          -0.0172 |           4.0741 |           0.2652 |
[32m[20221213 15:05:15 @agent_ppo2.py:185][0m |          -0.0177 |           4.0075 |           0.2652 |
[32m[20221213 15:05:15 @agent_ppo2.py:185][0m |          -0.0176 |           3.9842 |           0.2650 |
[32m[20221213 15:05:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.21
[32m[20221213 15:05:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 131.75
[32m[20221213 15:05:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.45
[32m[20221213 15:05:15 @agent_ppo2.py:143][0m Total time:      12.33 min
[32m[20221213 15:05:15 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221213 15:05:15 @agent_ppo2.py:121][0m #------------------------ Iteration 539 --------------------------#
[32m[20221213 15:05:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:15 @agent_ppo2.py:185][0m |          -0.0006 |           4.7911 |           0.2673 |
[32m[20221213 15:05:15 @agent_ppo2.py:185][0m |          -0.0042 |           4.6273 |           0.2668 |
[32m[20221213 15:05:15 @agent_ppo2.py:185][0m |          -0.0086 |           4.5118 |           0.2663 |
[32m[20221213 15:05:15 @agent_ppo2.py:185][0m |          -0.0030 |           4.6154 |           0.2665 |
[32m[20221213 15:05:15 @agent_ppo2.py:185][0m |          -0.0112 |           4.4634 |           0.2667 |
[32m[20221213 15:05:16 @agent_ppo2.py:185][0m |          -0.0142 |           4.3851 |           0.2665 |
[32m[20221213 15:05:16 @agent_ppo2.py:185][0m |          -0.0156 |           4.3622 |           0.2666 |
[32m[20221213 15:05:16 @agent_ppo2.py:185][0m |          -0.0171 |           4.3252 |           0.2668 |
[32m[20221213 15:05:16 @agent_ppo2.py:185][0m |          -0.0172 |           4.3435 |           0.2669 |
[32m[20221213 15:05:16 @agent_ppo2.py:185][0m |          -0.0174 |           4.2734 |           0.2670 |
[32m[20221213 15:05:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.71
[32m[20221213 15:05:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 125.63
[32m[20221213 15:05:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 97.70
[32m[20221213 15:05:16 @agent_ppo2.py:143][0m Total time:      12.35 min
[32m[20221213 15:05:16 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221213 15:05:16 @agent_ppo2.py:121][0m #------------------------ Iteration 540 --------------------------#
[32m[20221213 15:05:16 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:05:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0014 |           4.9290 |           0.2633 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0139 |           4.7340 |           0.2631 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0113 |           4.6496 |           0.2629 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0124 |           4.6158 |           0.2628 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0168 |           4.5448 |           0.2627 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0169 |           4.4983 |           0.2626 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0206 |           4.4858 |           0.2625 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0003 |           4.9940 |           0.2625 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0158 |           4.5077 |           0.2624 |
[32m[20221213 15:05:17 @agent_ppo2.py:185][0m |          -0.0205 |           4.4250 |           0.2623 |
[32m[20221213 15:05:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.59
[32m[20221213 15:05:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.23
[32m[20221213 15:05:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.32
[32m[20221213 15:05:17 @agent_ppo2.py:143][0m Total time:      12.37 min
[32m[20221213 15:05:17 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221213 15:05:17 @agent_ppo2.py:121][0m #------------------------ Iteration 541 --------------------------#
[32m[20221213 15:05:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0011 |           4.8161 |           0.2581 |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0064 |           4.6995 |           0.2581 |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0107 |           4.6567 |           0.2580 |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0073 |           4.7461 |           0.2580 |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0125 |           4.5606 |           0.2578 |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0133 |           4.5336 |           0.2580 |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0121 |           4.4952 |           0.2581 |
[32m[20221213 15:05:18 @agent_ppo2.py:185][0m |          -0.0178 |           4.4740 |           0.2581 |
[32m[20221213 15:05:19 @agent_ppo2.py:185][0m |          -0.0137 |           4.4822 |           0.2580 |
[32m[20221213 15:05:19 @agent_ppo2.py:185][0m |          -0.0193 |           4.4388 |           0.2584 |
[32m[20221213 15:05:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.93
[32m[20221213 15:05:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.19
[32m[20221213 15:05:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.12
[32m[20221213 15:05:19 @agent_ppo2.py:143][0m Total time:      12.39 min
[32m[20221213 15:05:19 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221213 15:05:19 @agent_ppo2.py:121][0m #------------------------ Iteration 542 --------------------------#
[32m[20221213 15:05:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:05:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:19 @agent_ppo2.py:185][0m |          -0.0003 |           5.1563 |           0.2686 |
[32m[20221213 15:05:19 @agent_ppo2.py:185][0m |          -0.0122 |           5.0026 |           0.2681 |
[32m[20221213 15:05:19 @agent_ppo2.py:185][0m |          -0.0105 |           4.9273 |           0.2677 |
[32m[20221213 15:05:19 @agent_ppo2.py:185][0m |          -0.0135 |           4.8944 |           0.2679 |
[32m[20221213 15:05:20 @agent_ppo2.py:185][0m |          -0.0156 |           4.8237 |           0.2675 |
[32m[20221213 15:05:20 @agent_ppo2.py:185][0m |          -0.0096 |           5.0175 |           0.2673 |
[32m[20221213 15:05:20 @agent_ppo2.py:185][0m |          -0.0151 |           4.7611 |           0.2672 |
[32m[20221213 15:05:20 @agent_ppo2.py:185][0m |          -0.0195 |           4.7492 |           0.2674 |
[32m[20221213 15:05:20 @agent_ppo2.py:185][0m |          -0.0186 |           4.7075 |           0.2674 |
[32m[20221213 15:05:20 @agent_ppo2.py:185][0m |          -0.0180 |           4.6989 |           0.2672 |
[32m[20221213 15:05:20 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:05:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.68
[32m[20221213 15:05:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.56
[32m[20221213 15:05:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.51
[32m[20221213 15:05:20 @agent_ppo2.py:143][0m Total time:      12.42 min
[32m[20221213 15:05:20 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221213 15:05:20 @agent_ppo2.py:121][0m #------------------------ Iteration 543 --------------------------#
[32m[20221213 15:05:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0025 |           4.8343 |           0.2683 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0051 |           4.7261 |           0.2683 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0061 |           4.6698 |           0.2680 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0088 |           4.6338 |           0.2678 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0131 |           4.6066 |           0.2680 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0039 |           4.7673 |           0.2677 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |           0.0004 |           4.8944 |           0.2678 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0154 |           4.5435 |           0.2676 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0166 |           4.5254 |           0.2677 |
[32m[20221213 15:05:21 @agent_ppo2.py:185][0m |          -0.0139 |           4.4955 |           0.2677 |
[32m[20221213 15:05:21 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:05:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.98
[32m[20221213 15:05:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.94
[32m[20221213 15:05:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.40
[32m[20221213 15:05:22 @agent_ppo2.py:143][0m Total time:      12.44 min
[32m[20221213 15:05:22 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221213 15:05:22 @agent_ppo2.py:121][0m #------------------------ Iteration 544 --------------------------#
[32m[20221213 15:05:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:22 @agent_ppo2.py:185][0m |           0.0061 |           5.0825 |           0.2590 |
[32m[20221213 15:05:22 @agent_ppo2.py:185][0m |          -0.0067 |           4.8650 |           0.2589 |
[32m[20221213 15:05:22 @agent_ppo2.py:185][0m |          -0.0105 |           4.8148 |           0.2584 |
[32m[20221213 15:05:22 @agent_ppo2.py:185][0m |          -0.0138 |           4.7656 |           0.2582 |
[32m[20221213 15:05:22 @agent_ppo2.py:185][0m |          -0.0088 |           4.8540 |           0.2580 |
[32m[20221213 15:05:22 @agent_ppo2.py:185][0m |          -0.0153 |           4.7027 |           0.2578 |
[32m[20221213 15:05:23 @agent_ppo2.py:185][0m |          -0.0167 |           4.6791 |           0.2576 |
[32m[20221213 15:05:23 @agent_ppo2.py:185][0m |          -0.0101 |           5.1180 |           0.2575 |
[32m[20221213 15:05:23 @agent_ppo2.py:185][0m |          -0.0138 |           4.6485 |           0.2573 |
[32m[20221213 15:05:23 @agent_ppo2.py:185][0m |          -0.0087 |           4.7543 |           0.2572 |
[32m[20221213 15:05:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:05:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.32
[32m[20221213 15:05:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.51
[32m[20221213 15:05:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.17
[32m[20221213 15:05:23 @agent_ppo2.py:143][0m Total time:      12.46 min
[32m[20221213 15:05:23 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221213 15:05:23 @agent_ppo2.py:121][0m #------------------------ Iteration 545 --------------------------#
[32m[20221213 15:05:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:23 @agent_ppo2.py:185][0m |          -0.0036 |           4.9470 |           0.2605 |
[32m[20221213 15:05:23 @agent_ppo2.py:185][0m |          -0.0027 |           4.8255 |           0.2605 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0090 |           4.7859 |           0.2606 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0146 |           4.7341 |           0.2605 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0074 |           4.7608 |           0.2606 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0137 |           4.6587 |           0.2606 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0170 |           4.6568 |           0.2605 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0076 |           5.0064 |           0.2604 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0195 |           4.6210 |           0.2602 |
[32m[20221213 15:05:24 @agent_ppo2.py:185][0m |          -0.0198 |           4.5931 |           0.2603 |
[32m[20221213 15:05:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:05:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.55
[32m[20221213 15:05:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.50
[32m[20221213 15:05:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.62
[32m[20221213 15:05:24 @agent_ppo2.py:143][0m Total time:      12.49 min
[32m[20221213 15:05:24 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221213 15:05:24 @agent_ppo2.py:121][0m #------------------------ Iteration 546 --------------------------#
[32m[20221213 15:05:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0036 |           4.8860 |           0.2647 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0118 |           4.7752 |           0.2639 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0063 |           4.7210 |           0.2637 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0095 |           4.6826 |           0.2634 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0119 |           4.6511 |           0.2632 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0156 |           4.6069 |           0.2630 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0166 |           4.5807 |           0.2628 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0182 |           4.5490 |           0.2627 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0182 |           4.5166 |           0.2624 |
[32m[20221213 15:05:25 @agent_ppo2.py:185][0m |          -0.0178 |           4.5007 |           0.2623 |
[32m[20221213 15:05:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:05:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.47
[32m[20221213 15:05:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 131.48
[32m[20221213 15:05:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 128.06
[32m[20221213 15:05:26 @agent_ppo2.py:143][0m Total time:      12.51 min
[32m[20221213 15:05:26 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221213 15:05:26 @agent_ppo2.py:121][0m #------------------------ Iteration 547 --------------------------#
[32m[20221213 15:05:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:26 @agent_ppo2.py:185][0m |          -0.0008 |           5.0579 |           0.2611 |
[32m[20221213 15:05:26 @agent_ppo2.py:185][0m |          -0.0103 |           4.9084 |           0.2608 |
[32m[20221213 15:05:26 @agent_ppo2.py:185][0m |           0.0015 |           5.1534 |           0.2608 |
[32m[20221213 15:05:26 @agent_ppo2.py:185][0m |          -0.0114 |           4.8098 |           0.2607 |
[32m[20221213 15:05:26 @agent_ppo2.py:185][0m |          -0.0138 |           4.7598 |           0.2609 |
[32m[20221213 15:05:27 @agent_ppo2.py:185][0m |          -0.0092 |           4.7152 |           0.2609 |
[32m[20221213 15:05:27 @agent_ppo2.py:185][0m |          -0.0172 |           4.6896 |           0.2611 |
[32m[20221213 15:05:27 @agent_ppo2.py:185][0m |          -0.0124 |           4.7432 |           0.2612 |
[32m[20221213 15:05:27 @agent_ppo2.py:185][0m |          -0.0157 |           4.6674 |           0.2612 |
[32m[20221213 15:05:27 @agent_ppo2.py:185][0m |          -0.0148 |           4.6471 |           0.2616 |
[32m[20221213 15:05:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.61
[32m[20221213 15:05:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.45
[32m[20221213 15:05:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.48
[32m[20221213 15:05:27 @agent_ppo2.py:143][0m Total time:      12.53 min
[32m[20221213 15:05:27 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221213 15:05:27 @agent_ppo2.py:121][0m #------------------------ Iteration 548 --------------------------#
[32m[20221213 15:05:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:27 @agent_ppo2.py:185][0m |           0.0039 |           4.7194 |           0.2649 |
[32m[20221213 15:05:27 @agent_ppo2.py:185][0m |          -0.0099 |           4.5647 |           0.2644 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0124 |           4.4758 |           0.2643 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0186 |           4.4021 |           0.2644 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0159 |           4.3494 |           0.2641 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0178 |           4.2884 |           0.2642 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0144 |           4.2411 |           0.2642 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0179 |           4.1853 |           0.2643 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0152 |           4.1737 |           0.2642 |
[32m[20221213 15:05:28 @agent_ppo2.py:185][0m |          -0.0209 |           4.1052 |           0.2642 |
[32m[20221213 15:05:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.28
[32m[20221213 15:05:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.83
[32m[20221213 15:05:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 148.53
[32m[20221213 15:05:28 @agent_ppo2.py:143][0m Total time:      12.55 min
[32m[20221213 15:05:28 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221213 15:05:28 @agent_ppo2.py:121][0m #------------------------ Iteration 549 --------------------------#
[32m[20221213 15:05:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0036 |           4.5650 |           0.2658 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |           0.0004 |           4.5123 |           0.2653 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0123 |           4.3617 |           0.2654 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0063 |           4.3395 |           0.2654 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0126 |           4.3114 |           0.2657 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0096 |           4.4861 |           0.2658 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0047 |           4.4494 |           0.2660 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0114 |           4.2748 |           0.2666 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0111 |           4.2382 |           0.2663 |
[32m[20221213 15:05:29 @agent_ppo2.py:185][0m |          -0.0069 |           4.4690 |           0.2665 |
[32m[20221213 15:05:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 115.09
[32m[20221213 15:05:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 130.85
[32m[20221213 15:05:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 128.85
[32m[20221213 15:05:30 @agent_ppo2.py:143][0m Total time:      12.57 min
[32m[20221213 15:05:30 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221213 15:05:30 @agent_ppo2.py:121][0m #------------------------ Iteration 550 --------------------------#
[32m[20221213 15:05:30 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:05:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:30 @agent_ppo2.py:185][0m |           0.0059 |           5.1382 |           0.2715 |
[32m[20221213 15:05:30 @agent_ppo2.py:185][0m |          -0.0091 |           4.6344 |           0.2720 |
[32m[20221213 15:05:30 @agent_ppo2.py:185][0m |          -0.0122 |           4.5567 |           0.2721 |
[32m[20221213 15:05:30 @agent_ppo2.py:185][0m |          -0.0124 |           4.5122 |           0.2721 |
[32m[20221213 15:05:30 @agent_ppo2.py:185][0m |          -0.0164 |           4.4484 |           0.2721 |
[32m[20221213 15:05:31 @agent_ppo2.py:185][0m |          -0.0174 |           4.4058 |           0.2724 |
[32m[20221213 15:05:31 @agent_ppo2.py:185][0m |          -0.0199 |           4.3830 |           0.2727 |
[32m[20221213 15:05:31 @agent_ppo2.py:185][0m |          -0.0161 |           4.3325 |           0.2727 |
[32m[20221213 15:05:31 @agent_ppo2.py:185][0m |          -0.0188 |           4.3174 |           0.2728 |
[32m[20221213 15:05:31 @agent_ppo2.py:185][0m |          -0.0194 |           4.2867 |           0.2729 |
[32m[20221213 15:05:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.09
[32m[20221213 15:05:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.81
[32m[20221213 15:05:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.62
[32m[20221213 15:05:31 @agent_ppo2.py:143][0m Total time:      12.60 min
[32m[20221213 15:05:31 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221213 15:05:31 @agent_ppo2.py:121][0m #------------------------ Iteration 551 --------------------------#
[32m[20221213 15:05:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:31 @agent_ppo2.py:185][0m |          -0.0038 |           4.7780 |           0.2721 |
[32m[20221213 15:05:31 @agent_ppo2.py:185][0m |          -0.0031 |           4.7036 |           0.2715 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0126 |           4.6078 |           0.2715 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0023 |           4.7522 |           0.2714 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0116 |           4.5527 |           0.2717 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0122 |           4.5066 |           0.2714 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0155 |           4.4852 |           0.2710 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0144 |           4.4726 |           0.2714 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0155 |           4.4530 |           0.2712 |
[32m[20221213 15:05:32 @agent_ppo2.py:185][0m |          -0.0157 |           4.4336 |           0.2709 |
[32m[20221213 15:05:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.60
[32m[20221213 15:05:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.53
[32m[20221213 15:05:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 117.27
[32m[20221213 15:05:32 @agent_ppo2.py:143][0m Total time:      12.62 min
[32m[20221213 15:05:32 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221213 15:05:32 @agent_ppo2.py:121][0m #------------------------ Iteration 552 --------------------------#
[32m[20221213 15:05:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0046 |           4.9467 |           0.2710 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0103 |           4.7899 |           0.2709 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0091 |           4.7810 |           0.2709 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0121 |           4.6954 |           0.2709 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0150 |           4.6096 |           0.2709 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0172 |           4.5747 |           0.2709 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0172 |           4.5456 |           0.2710 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0171 |           4.5180 |           0.2711 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0202 |           4.4682 |           0.2712 |
[32m[20221213 15:05:33 @agent_ppo2.py:185][0m |          -0.0141 |           4.6305 |           0.2711 |
[32m[20221213 15:05:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.69
[32m[20221213 15:05:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.37
[32m[20221213 15:05:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 146.45
[32m[20221213 15:05:34 @agent_ppo2.py:143][0m Total time:      12.64 min
[32m[20221213 15:05:34 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221213 15:05:34 @agent_ppo2.py:121][0m #------------------------ Iteration 553 --------------------------#
[32m[20221213 15:05:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:34 @agent_ppo2.py:185][0m |           0.0048 |           4.7412 |           0.2809 |
[32m[20221213 15:05:34 @agent_ppo2.py:185][0m |          -0.0074 |           4.5314 |           0.2806 |
[32m[20221213 15:05:34 @agent_ppo2.py:185][0m |          -0.0100 |           4.4596 |           0.2804 |
[32m[20221213 15:05:34 @agent_ppo2.py:185][0m |          -0.0123 |           4.4260 |           0.2802 |
[32m[20221213 15:05:34 @agent_ppo2.py:185][0m |          -0.0142 |           4.3967 |           0.2800 |
[32m[20221213 15:05:34 @agent_ppo2.py:185][0m |          -0.0153 |           4.3325 |           0.2798 |
[32m[20221213 15:05:35 @agent_ppo2.py:185][0m |          -0.0141 |           4.3137 |           0.2798 |
[32m[20221213 15:05:35 @agent_ppo2.py:185][0m |          -0.0152 |           4.2983 |           0.2793 |
[32m[20221213 15:05:35 @agent_ppo2.py:185][0m |          -0.0163 |           4.2856 |           0.2794 |
[32m[20221213 15:05:35 @agent_ppo2.py:185][0m |          -0.0177 |           4.2568 |           0.2792 |
[32m[20221213 15:05:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.91
[32m[20221213 15:05:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.94
[32m[20221213 15:05:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 103.82
[32m[20221213 15:05:35 @agent_ppo2.py:143][0m Total time:      12.66 min
[32m[20221213 15:05:35 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221213 15:05:35 @agent_ppo2.py:121][0m #------------------------ Iteration 554 --------------------------#
[32m[20221213 15:05:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:35 @agent_ppo2.py:185][0m |          -0.0022 |           4.8082 |           0.2804 |
[32m[20221213 15:05:35 @agent_ppo2.py:185][0m |          -0.0072 |           4.6180 |           0.2803 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0121 |           4.5458 |           0.2801 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0129 |           4.5179 |           0.2801 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0117 |           4.5095 |           0.2802 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0104 |           4.5087 |           0.2799 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0039 |           4.7711 |           0.2800 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0179 |           4.4184 |           0.2802 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0149 |           4.4053 |           0.2801 |
[32m[20221213 15:05:36 @agent_ppo2.py:185][0m |          -0.0140 |           4.4415 |           0.2803 |
[32m[20221213 15:05:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.39
[32m[20221213 15:05:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.84
[32m[20221213 15:05:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.39
[32m[20221213 15:05:36 @agent_ppo2.py:143][0m Total time:      12.69 min
[32m[20221213 15:05:36 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221213 15:05:36 @agent_ppo2.py:121][0m #------------------------ Iteration 555 --------------------------#
[32m[20221213 15:05:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |           0.0009 |           4.7609 |           0.2735 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0049 |           4.6326 |           0.2732 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0092 |           4.5770 |           0.2731 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0080 |           4.5447 |           0.2731 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0088 |           4.5339 |           0.2731 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0122 |           4.4443 |           0.2734 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0146 |           4.4033 |           0.2735 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0146 |           4.4070 |           0.2734 |
[32m[20221213 15:05:37 @agent_ppo2.py:185][0m |          -0.0137 |           4.3457 |           0.2734 |
[32m[20221213 15:05:38 @agent_ppo2.py:185][0m |          -0.0151 |           4.3336 |           0.2736 |
[32m[20221213 15:05:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.44
[32m[20221213 15:05:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 131.40
[32m[20221213 15:05:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.85
[32m[20221213 15:05:38 @agent_ppo2.py:143][0m Total time:      12.71 min
[32m[20221213 15:05:38 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221213 15:05:38 @agent_ppo2.py:121][0m #------------------------ Iteration 556 --------------------------#
[32m[20221213 15:05:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:05:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:38 @agent_ppo2.py:185][0m |           0.0100 |           5.5267 |           0.2773 |
[32m[20221213 15:05:38 @agent_ppo2.py:185][0m |          -0.0026 |           5.0067 |           0.2770 |
[32m[20221213 15:05:38 @agent_ppo2.py:185][0m |          -0.0103 |           4.9112 |           0.2769 |
[32m[20221213 15:05:38 @agent_ppo2.py:185][0m |          -0.0148 |           4.8868 |           0.2767 |
[32m[20221213 15:05:38 @agent_ppo2.py:185][0m |          -0.0125 |           4.8603 |           0.2766 |
[32m[20221213 15:05:38 @agent_ppo2.py:185][0m |          -0.0144 |           4.8357 |           0.2764 |
[32m[20221213 15:05:39 @agent_ppo2.py:185][0m |          -0.0162 |           4.8271 |           0.2764 |
[32m[20221213 15:05:39 @agent_ppo2.py:185][0m |          -0.0176 |           4.7930 |           0.2763 |
[32m[20221213 15:05:39 @agent_ppo2.py:185][0m |          -0.0176 |           4.7885 |           0.2764 |
[32m[20221213 15:05:39 @agent_ppo2.py:185][0m |          -0.0168 |           4.7676 |           0.2762 |
[32m[20221213 15:05:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.05
[32m[20221213 15:05:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.60
[32m[20221213 15:05:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 119.40
[32m[20221213 15:05:39 @agent_ppo2.py:143][0m Total time:      12.73 min
[32m[20221213 15:05:39 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221213 15:05:39 @agent_ppo2.py:121][0m #------------------------ Iteration 557 --------------------------#
[32m[20221213 15:05:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:05:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:39 @agent_ppo2.py:185][0m |           0.0004 |           4.6558 |           0.2812 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0026 |           4.4137 |           0.2811 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |           0.0021 |           4.4036 |           0.2810 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0076 |           4.3273 |           0.2810 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0103 |           4.2139 |           0.2808 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0126 |           4.2119 |           0.2806 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0133 |           4.1659 |           0.2806 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0074 |           4.2025 |           0.2807 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0144 |           4.1548 |           0.2805 |
[32m[20221213 15:05:40 @agent_ppo2.py:185][0m |          -0.0147 |           4.1418 |           0.2806 |
[32m[20221213 15:05:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.97
[32m[20221213 15:05:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.81
[32m[20221213 15:05:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.69
[32m[20221213 15:05:40 @agent_ppo2.py:143][0m Total time:      12.75 min
[32m[20221213 15:05:40 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221213 15:05:40 @agent_ppo2.py:121][0m #------------------------ Iteration 558 --------------------------#
[32m[20221213 15:05:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |          -0.0050 |           4.9998 |           0.2826 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |           0.0012 |           5.0314 |           0.2820 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |          -0.0009 |           5.0382 |           0.2818 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |           0.0020 |           5.2181 |           0.2816 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |          -0.0119 |           4.6503 |           0.2811 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |          -0.0090 |           4.5713 |           0.2813 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |          -0.0151 |           4.5478 |           0.2813 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |          -0.0141 |           4.4945 |           0.2813 |
[32m[20221213 15:05:41 @agent_ppo2.py:185][0m |          -0.0128 |           4.4694 |           0.2809 |
[32m[20221213 15:05:42 @agent_ppo2.py:185][0m |          -0.0153 |           4.4368 |           0.2810 |
[32m[20221213 15:05:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.40
[32m[20221213 15:05:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 129.70
[32m[20221213 15:05:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.09
[32m[20221213 15:05:42 @agent_ppo2.py:143][0m Total time:      12.78 min
[32m[20221213 15:05:42 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221213 15:05:42 @agent_ppo2.py:121][0m #------------------------ Iteration 559 --------------------------#
[32m[20221213 15:05:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:42 @agent_ppo2.py:185][0m |          -0.0031 |           4.8134 |           0.2767 |
[32m[20221213 15:05:42 @agent_ppo2.py:185][0m |          -0.0071 |           4.6528 |           0.2761 |
[32m[20221213 15:05:42 @agent_ppo2.py:185][0m |          -0.0115 |           4.5842 |           0.2758 |
[32m[20221213 15:05:42 @agent_ppo2.py:185][0m |          -0.0102 |           4.5335 |           0.2756 |
[32m[20221213 15:05:42 @agent_ppo2.py:185][0m |          -0.0145 |           4.5182 |           0.2755 |
[32m[20221213 15:05:43 @agent_ppo2.py:185][0m |          -0.0107 |           4.6136 |           0.2753 |
[32m[20221213 15:05:43 @agent_ppo2.py:185][0m |          -0.0151 |           4.4616 |           0.2752 |
[32m[20221213 15:05:43 @agent_ppo2.py:185][0m |          -0.0141 |           4.4463 |           0.2750 |
[32m[20221213 15:05:43 @agent_ppo2.py:185][0m |          -0.0193 |           4.4507 |           0.2752 |
[32m[20221213 15:05:43 @agent_ppo2.py:185][0m |          -0.0143 |           4.4284 |           0.2750 |
[32m[20221213 15:05:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.25
[32m[20221213 15:05:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.52
[32m[20221213 15:05:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 124.63
[32m[20221213 15:05:43 @agent_ppo2.py:143][0m Total time:      12.80 min
[32m[20221213 15:05:43 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221213 15:05:43 @agent_ppo2.py:121][0m #------------------------ Iteration 560 --------------------------#
[32m[20221213 15:05:43 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:05:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:43 @agent_ppo2.py:185][0m |          -0.0022 |           4.7769 |           0.2807 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |           0.0009 |           4.8759 |           0.2802 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0101 |           4.6233 |           0.2797 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0140 |           4.5740 |           0.2795 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0062 |           4.5804 |           0.2793 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0141 |           4.5218 |           0.2790 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0053 |           5.0626 |           0.2789 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0173 |           4.5149 |           0.2784 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0143 |           4.4804 |           0.2784 |
[32m[20221213 15:05:44 @agent_ppo2.py:185][0m |          -0.0194 |           4.4678 |           0.2781 |
[32m[20221213 15:05:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.74
[32m[20221213 15:05:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.75
[32m[20221213 15:05:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.54
[32m[20221213 15:05:44 @agent_ppo2.py:143][0m Total time:      12.82 min
[32m[20221213 15:05:44 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221213 15:05:44 @agent_ppo2.py:121][0m #------------------------ Iteration 561 --------------------------#
[32m[20221213 15:05:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0014 |           4.7228 |           0.2779 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0012 |           4.7325 |           0.2780 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0068 |           4.5436 |           0.2782 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0131 |           4.4865 |           0.2782 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0108 |           4.4452 |           0.2784 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0135 |           4.4161 |           0.2784 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0162 |           4.3945 |           0.2786 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0139 |           4.3610 |           0.2786 |
[32m[20221213 15:05:45 @agent_ppo2.py:185][0m |          -0.0158 |           4.3199 |           0.2788 |
[32m[20221213 15:05:46 @agent_ppo2.py:185][0m |          -0.0151 |           4.3038 |           0.2790 |
[32m[20221213 15:05:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 108.79
[32m[20221213 15:05:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.81
[32m[20221213 15:05:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.19
[32m[20221213 15:05:46 @agent_ppo2.py:143][0m Total time:      12.84 min
[32m[20221213 15:05:46 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221213 15:05:46 @agent_ppo2.py:121][0m #------------------------ Iteration 562 --------------------------#
[32m[20221213 15:05:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:46 @agent_ppo2.py:185][0m |          -0.0040 |           4.7346 |           0.2773 |
[32m[20221213 15:05:46 @agent_ppo2.py:185][0m |          -0.0084 |           4.6690 |           0.2765 |
[32m[20221213 15:05:46 @agent_ppo2.py:185][0m |          -0.0120 |           4.6375 |           0.2761 |
[32m[20221213 15:05:46 @agent_ppo2.py:185][0m |          -0.0142 |           4.5844 |           0.2762 |
[32m[20221213 15:05:46 @agent_ppo2.py:185][0m |          -0.0176 |           4.5591 |           0.2760 |
[32m[20221213 15:05:47 @agent_ppo2.py:185][0m |          -0.0181 |           4.5456 |           0.2760 |
[32m[20221213 15:05:47 @agent_ppo2.py:185][0m |          -0.0160 |           4.5226 |           0.2758 |
[32m[20221213 15:05:47 @agent_ppo2.py:185][0m |          -0.0148 |           4.4966 |           0.2758 |
[32m[20221213 15:05:47 @agent_ppo2.py:185][0m |          -0.0186 |           4.4793 |           0.2757 |
[32m[20221213 15:05:47 @agent_ppo2.py:185][0m |          -0.0181 |           4.4704 |           0.2757 |
[32m[20221213 15:05:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.58
[32m[20221213 15:05:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.19
[32m[20221213 15:05:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.10
[32m[20221213 15:05:47 @agent_ppo2.py:143][0m Total time:      12.86 min
[32m[20221213 15:05:47 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221213 15:05:47 @agent_ppo2.py:121][0m #------------------------ Iteration 563 --------------------------#
[32m[20221213 15:05:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:47 @agent_ppo2.py:185][0m |          -0.0002 |           4.9317 |           0.2819 |
[32m[20221213 15:05:47 @agent_ppo2.py:185][0m |          -0.0087 |           4.7269 |           0.2813 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0126 |           4.6406 |           0.2811 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0132 |           4.6036 |           0.2809 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0179 |           4.5734 |           0.2808 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0145 |           4.5165 |           0.2806 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0088 |           4.5992 |           0.2807 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0173 |           4.4695 |           0.2802 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0158 |           4.4482 |           0.2803 |
[32m[20221213 15:05:48 @agent_ppo2.py:185][0m |          -0.0182 |           4.4225 |           0.2800 |
[32m[20221213 15:05:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.58
[32m[20221213 15:05:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.46
[32m[20221213 15:05:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.90
[32m[20221213 15:05:48 @agent_ppo2.py:143][0m Total time:      12.89 min
[32m[20221213 15:05:48 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221213 15:05:48 @agent_ppo2.py:121][0m #------------------------ Iteration 564 --------------------------#
[32m[20221213 15:05:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0047 |           4.6320 |           0.2752 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0108 |           4.4211 |           0.2752 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0080 |           4.3336 |           0.2752 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0147 |           4.2726 |           0.2750 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0140 |           4.1894 |           0.2750 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0151 |           4.1611 |           0.2748 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0168 |           4.1088 |           0.2749 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0125 |           4.1590 |           0.2747 |
[32m[20221213 15:05:49 @agent_ppo2.py:185][0m |          -0.0120 |           4.0696 |           0.2748 |
[32m[20221213 15:05:50 @agent_ppo2.py:185][0m |          -0.0199 |           4.0168 |           0.2746 |
[32m[20221213 15:05:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.14
[32m[20221213 15:05:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.39
[32m[20221213 15:05:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.25
[32m[20221213 15:05:50 @agent_ppo2.py:143][0m Total time:      12.91 min
[32m[20221213 15:05:50 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221213 15:05:50 @agent_ppo2.py:121][0m #------------------------ Iteration 565 --------------------------#
[32m[20221213 15:05:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:50 @agent_ppo2.py:185][0m |           0.0006 |           4.9944 |           0.2713 |
[32m[20221213 15:05:50 @agent_ppo2.py:185][0m |          -0.0104 |           4.8307 |           0.2704 |
[32m[20221213 15:05:50 @agent_ppo2.py:185][0m |          -0.0089 |           4.7588 |           0.2701 |
[32m[20221213 15:05:50 @agent_ppo2.py:185][0m |          -0.0113 |           4.7385 |           0.2700 |
[32m[20221213 15:05:50 @agent_ppo2.py:185][0m |          -0.0016 |           5.2693 |           0.2700 |
[32m[20221213 15:05:51 @agent_ppo2.py:185][0m |          -0.0077 |           4.9189 |           0.2696 |
[32m[20221213 15:05:51 @agent_ppo2.py:185][0m |          -0.0172 |           4.6652 |           0.2699 |
[32m[20221213 15:05:51 @agent_ppo2.py:185][0m |          -0.0097 |           4.7630 |           0.2703 |
[32m[20221213 15:05:51 @agent_ppo2.py:185][0m |          -0.0146 |           4.6492 |           0.2703 |
[32m[20221213 15:05:51 @agent_ppo2.py:185][0m |          -0.0184 |           4.6262 |           0.2705 |
[32m[20221213 15:05:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.62
[32m[20221213 15:05:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.21
[32m[20221213 15:05:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.09
[32m[20221213 15:05:51 @agent_ppo2.py:143][0m Total time:      12.93 min
[32m[20221213 15:05:51 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221213 15:05:51 @agent_ppo2.py:121][0m #------------------------ Iteration 566 --------------------------#
[32m[20221213 15:05:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:51 @agent_ppo2.py:185][0m |           0.0078 |           5.0587 |           0.2804 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |           0.0112 |           5.3241 |           0.2799 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0102 |           4.6505 |           0.2795 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0105 |           4.5857 |           0.2799 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0134 |           4.5625 |           0.2798 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0154 |           4.5358 |           0.2798 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0164 |           4.4991 |           0.2799 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0145 |           4.4806 |           0.2798 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0182 |           4.4488 |           0.2800 |
[32m[20221213 15:05:52 @agent_ppo2.py:185][0m |          -0.0153 |           4.4631 |           0.2798 |
[32m[20221213 15:05:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.39
[32m[20221213 15:05:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.94
[32m[20221213 15:05:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 109.93
[32m[20221213 15:05:52 @agent_ppo2.py:143][0m Total time:      12.95 min
[32m[20221213 15:05:52 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221213 15:05:52 @agent_ppo2.py:121][0m #------------------------ Iteration 567 --------------------------#
[32m[20221213 15:05:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0009 |           5.2568 |           0.2823 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0038 |           5.1435 |           0.2817 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0118 |           5.0488 |           0.2815 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0134 |           5.0135 |           0.2817 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0154 |           4.9722 |           0.2817 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0147 |           4.9555 |           0.2815 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0114 |           5.0298 |           0.2818 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0048 |           5.1791 |           0.2816 |
[32m[20221213 15:05:53 @agent_ppo2.py:185][0m |          -0.0115 |           4.9359 |           0.2816 |
[32m[20221213 15:05:54 @agent_ppo2.py:185][0m |          -0.0173 |           4.8358 |           0.2814 |
[32m[20221213 15:05:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.91
[32m[20221213 15:05:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.34
[32m[20221213 15:05:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.10
[32m[20221213 15:05:54 @agent_ppo2.py:143][0m Total time:      12.98 min
[32m[20221213 15:05:54 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221213 15:05:54 @agent_ppo2.py:121][0m #------------------------ Iteration 568 --------------------------#
[32m[20221213 15:05:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:05:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:54 @agent_ppo2.py:185][0m |          -0.0059 |           5.1196 |           0.2783 |
[32m[20221213 15:05:54 @agent_ppo2.py:185][0m |          -0.0085 |           4.9959 |           0.2779 |
[32m[20221213 15:05:54 @agent_ppo2.py:185][0m |          -0.0129 |           4.9480 |           0.2778 |
[32m[20221213 15:05:54 @agent_ppo2.py:185][0m |          -0.0125 |           4.9097 |           0.2778 |
[32m[20221213 15:05:54 @agent_ppo2.py:185][0m |           0.0060 |           5.4963 |           0.2778 |
[32m[20221213 15:05:55 @agent_ppo2.py:185][0m |          -0.0059 |           5.3193 |           0.2775 |
[32m[20221213 15:05:55 @agent_ppo2.py:185][0m |          -0.0160 |           4.8786 |           0.2776 |
[32m[20221213 15:05:55 @agent_ppo2.py:185][0m |          -0.0171 |           4.8286 |           0.2774 |
[32m[20221213 15:05:55 @agent_ppo2.py:185][0m |          -0.0182 |           4.8101 |           0.2774 |
[32m[20221213 15:05:55 @agent_ppo2.py:185][0m |          -0.0197 |           4.7989 |           0.2776 |
[32m[20221213 15:05:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.33
[32m[20221213 15:05:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.73
[32m[20221213 15:05:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.99
[32m[20221213 15:05:55 @agent_ppo2.py:143][0m Total time:      13.00 min
[32m[20221213 15:05:55 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221213 15:05:55 @agent_ppo2.py:121][0m #------------------------ Iteration 569 --------------------------#
[32m[20221213 15:05:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:55 @agent_ppo2.py:185][0m |           0.0122 |           5.6489 |           0.2737 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |           0.0079 |           5.2271 |           0.2727 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0104 |           4.8529 |           0.2723 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0135 |           4.8128 |           0.2718 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0085 |           5.0127 |           0.2717 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0166 |           4.7532 |           0.2715 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0186 |           4.7216 |           0.2713 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0196 |           4.6709 |           0.2711 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0171 |           4.6561 |           0.2709 |
[32m[20221213 15:05:56 @agent_ppo2.py:185][0m |          -0.0184 |           4.6445 |           0.2709 |
[32m[20221213 15:05:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.73
[32m[20221213 15:05:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.70
[32m[20221213 15:05:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.67
[32m[20221213 15:05:56 @agent_ppo2.py:143][0m Total time:      13.02 min
[32m[20221213 15:05:56 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221213 15:05:56 @agent_ppo2.py:121][0m #------------------------ Iteration 570 --------------------------#
[32m[20221213 15:05:57 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:05:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0013 |           4.9835 |           0.2771 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0041 |           4.8628 |           0.2772 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |           0.0020 |           5.0148 |           0.2767 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0048 |           4.7353 |           0.2767 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0116 |           4.6355 |           0.2766 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0136 |           4.5598 |           0.2765 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0134 |           4.5167 |           0.2767 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0173 |           4.4747 |           0.2765 |
[32m[20221213 15:05:57 @agent_ppo2.py:185][0m |          -0.0090 |           4.6096 |           0.2766 |
[32m[20221213 15:05:58 @agent_ppo2.py:185][0m |          -0.0161 |           4.4288 |           0.2764 |
[32m[20221213 15:05:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:05:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.96
[32m[20221213 15:05:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.98
[32m[20221213 15:05:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.09
[32m[20221213 15:05:58 @agent_ppo2.py:143][0m Total time:      13.04 min
[32m[20221213 15:05:58 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221213 15:05:58 @agent_ppo2.py:121][0m #------------------------ Iteration 571 --------------------------#
[32m[20221213 15:05:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:58 @agent_ppo2.py:185][0m |           0.0012 |           5.1537 |           0.2708 |
[32m[20221213 15:05:58 @agent_ppo2.py:185][0m |          -0.0052 |           5.0147 |           0.2705 |
[32m[20221213 15:05:58 @agent_ppo2.py:185][0m |          -0.0093 |           4.9122 |           0.2700 |
[32m[20221213 15:05:58 @agent_ppo2.py:185][0m |          -0.0139 |           4.8534 |           0.2699 |
[32m[20221213 15:05:58 @agent_ppo2.py:185][0m |          -0.0159 |           4.7930 |           0.2698 |
[32m[20221213 15:05:59 @agent_ppo2.py:185][0m |          -0.0141 |           4.7406 |           0.2695 |
[32m[20221213 15:05:59 @agent_ppo2.py:185][0m |          -0.0165 |           4.7021 |           0.2695 |
[32m[20221213 15:05:59 @agent_ppo2.py:185][0m |          -0.0150 |           4.6861 |           0.2694 |
[32m[20221213 15:05:59 @agent_ppo2.py:185][0m |          -0.0180 |           4.6319 |           0.2695 |
[32m[20221213 15:05:59 @agent_ppo2.py:185][0m |          -0.0175 |           4.5948 |           0.2693 |
[32m[20221213 15:05:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:05:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.10
[32m[20221213 15:05:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.55
[32m[20221213 15:05:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.44
[32m[20221213 15:05:59 @agent_ppo2.py:143][0m Total time:      13.06 min
[32m[20221213 15:05:59 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221213 15:05:59 @agent_ppo2.py:121][0m #------------------------ Iteration 572 --------------------------#
[32m[20221213 15:05:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:05:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:05:59 @agent_ppo2.py:185][0m |          -0.0008 |           4.9356 |           0.2844 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0109 |           4.6501 |           0.2843 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0113 |           4.5511 |           0.2843 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0098 |           4.5211 |           0.2840 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0135 |           4.4259 |           0.2839 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0168 |           4.3720 |           0.2840 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0175 |           4.3325 |           0.2840 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0143 |           4.3462 |           0.2838 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0112 |           4.3138 |           0.2838 |
[32m[20221213 15:06:00 @agent_ppo2.py:185][0m |          -0.0201 |           4.2627 |           0.2836 |
[32m[20221213 15:06:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.80
[32m[20221213 15:06:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.53
[32m[20221213 15:06:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.71
[32m[20221213 15:06:00 @agent_ppo2.py:143][0m Total time:      13.09 min
[32m[20221213 15:06:00 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221213 15:06:00 @agent_ppo2.py:121][0m #------------------------ Iteration 573 --------------------------#
[32m[20221213 15:06:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |           0.0029 |           5.3572 |           0.2714 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0062 |           5.1793 |           0.2711 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0091 |           5.1042 |           0.2710 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0124 |           5.0710 |           0.2708 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0092 |           5.0786 |           0.2707 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0152 |           5.0139 |           0.2707 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0071 |           5.0100 |           0.2707 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0147 |           4.9652 |           0.2708 |
[32m[20221213 15:06:01 @agent_ppo2.py:185][0m |          -0.0126 |           4.9620 |           0.2710 |
[32m[20221213 15:06:02 @agent_ppo2.py:185][0m |          -0.0120 |           5.0048 |           0.2710 |
[32m[20221213 15:06:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.05
[32m[20221213 15:06:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.09
[32m[20221213 15:06:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.40
[32m[20221213 15:06:02 @agent_ppo2.py:143][0m Total time:      13.11 min
[32m[20221213 15:06:02 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221213 15:06:02 @agent_ppo2.py:121][0m #------------------------ Iteration 574 --------------------------#
[32m[20221213 15:06:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:02 @agent_ppo2.py:185][0m |          -0.0009 |           4.6646 |           0.2683 |
[32m[20221213 15:06:02 @agent_ppo2.py:185][0m |          -0.0072 |           4.5452 |           0.2683 |
[32m[20221213 15:06:02 @agent_ppo2.py:185][0m |          -0.0103 |           4.4855 |           0.2680 |
[32m[20221213 15:06:02 @agent_ppo2.py:185][0m |          -0.0116 |           4.4478 |           0.2676 |
[32m[20221213 15:06:02 @agent_ppo2.py:185][0m |          -0.0112 |           4.4064 |           0.2677 |
[32m[20221213 15:06:03 @agent_ppo2.py:185][0m |          -0.0126 |           4.3756 |           0.2676 |
[32m[20221213 15:06:03 @agent_ppo2.py:185][0m |          -0.0164 |           4.3461 |           0.2675 |
[32m[20221213 15:06:03 @agent_ppo2.py:185][0m |          -0.0148 |           4.3137 |           0.2673 |
[32m[20221213 15:06:03 @agent_ppo2.py:185][0m |          -0.0209 |           4.3170 |           0.2675 |
[32m[20221213 15:06:03 @agent_ppo2.py:185][0m |          -0.0206 |           4.2887 |           0.2674 |
[32m[20221213 15:06:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 111.04
[32m[20221213 15:06:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.35
[32m[20221213 15:06:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.96
[32m[20221213 15:06:03 @agent_ppo2.py:143][0m Total time:      13.13 min
[32m[20221213 15:06:03 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221213 15:06:03 @agent_ppo2.py:121][0m #------------------------ Iteration 575 --------------------------#
[32m[20221213 15:06:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:03 @agent_ppo2.py:185][0m |          -0.0014 |           4.6044 |           0.2686 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0063 |           4.4916 |           0.2679 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |           0.0025 |           4.8558 |           0.2675 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0086 |           4.4265 |           0.2676 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0114 |           4.4052 |           0.2675 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0100 |           4.3422 |           0.2674 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0127 |           4.3130 |           0.2676 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0170 |           4.2938 |           0.2673 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0129 |           4.2556 |           0.2674 |
[32m[20221213 15:06:04 @agent_ppo2.py:185][0m |          -0.0148 |           4.2285 |           0.2674 |
[32m[20221213 15:06:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.26
[32m[20221213 15:06:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 133.27
[32m[20221213 15:06:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.50
[32m[20221213 15:06:04 @agent_ppo2.py:143][0m Total time:      13.15 min
[32m[20221213 15:06:04 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221213 15:06:04 @agent_ppo2.py:121][0m #------------------------ Iteration 576 --------------------------#
[32m[20221213 15:06:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0002 |           5.1583 |           0.2691 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |           0.0018 |           5.5739 |           0.2690 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0108 |           4.9955 |           0.2688 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0107 |           4.9074 |           0.2689 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0163 |           4.9028 |           0.2685 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0141 |           4.8513 |           0.2684 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0168 |           4.8394 |           0.2685 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0097 |           4.9704 |           0.2684 |
[32m[20221213 15:06:05 @agent_ppo2.py:185][0m |          -0.0133 |           4.9102 |           0.2681 |
[32m[20221213 15:06:06 @agent_ppo2.py:185][0m |          -0.0137 |           4.7804 |           0.2681 |
[32m[20221213 15:06:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.53
[32m[20221213 15:06:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.58
[32m[20221213 15:06:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.07
[32m[20221213 15:06:06 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 154.07
[32m[20221213 15:06:06 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 154.07
[32m[20221213 15:06:06 @agent_ppo2.py:143][0m Total time:      13.18 min
[32m[20221213 15:06:06 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221213 15:06:06 @agent_ppo2.py:121][0m #------------------------ Iteration 577 --------------------------#
[32m[20221213 15:06:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:06 @agent_ppo2.py:185][0m |          -0.0040 |           5.3674 |           0.2702 |
[32m[20221213 15:06:06 @agent_ppo2.py:185][0m |          -0.0085 |           5.2016 |           0.2699 |
[32m[20221213 15:06:06 @agent_ppo2.py:185][0m |          -0.0088 |           5.1120 |           0.2702 |
[32m[20221213 15:06:06 @agent_ppo2.py:185][0m |          -0.0093 |           5.0647 |           0.2701 |
[32m[20221213 15:06:06 @agent_ppo2.py:185][0m |          -0.0120 |           5.0217 |           0.2702 |
[32m[20221213 15:06:07 @agent_ppo2.py:185][0m |          -0.0056 |           5.2271 |           0.2703 |
[32m[20221213 15:06:07 @agent_ppo2.py:185][0m |          -0.0084 |           5.0954 |           0.2705 |
[32m[20221213 15:06:07 @agent_ppo2.py:185][0m |          -0.0164 |           4.9240 |           0.2706 |
[32m[20221213 15:06:07 @agent_ppo2.py:185][0m |          -0.0068 |           5.1252 |           0.2709 |
[32m[20221213 15:06:07 @agent_ppo2.py:185][0m |          -0.0172 |           4.8451 |           0.2711 |
[32m[20221213 15:06:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 136.68
[32m[20221213 15:06:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.73
[32m[20221213 15:06:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 149.00
[32m[20221213 15:06:07 @agent_ppo2.py:143][0m Total time:      13.20 min
[32m[20221213 15:06:07 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221213 15:06:07 @agent_ppo2.py:121][0m #------------------------ Iteration 578 --------------------------#
[32m[20221213 15:06:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:07 @agent_ppo2.py:185][0m |           0.0132 |           5.3972 |           0.2791 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0079 |           4.8097 |           0.2783 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0104 |           4.7151 |           0.2783 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0119 |           4.6626 |           0.2782 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0126 |           4.6030 |           0.2780 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0135 |           4.5704 |           0.2779 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0094 |           4.5950 |           0.2779 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0141 |           4.5245 |           0.2779 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0187 |           4.4956 |           0.2777 |
[32m[20221213 15:06:08 @agent_ppo2.py:185][0m |          -0.0076 |           4.6315 |           0.2775 |
[32m[20221213 15:06:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.13
[32m[20221213 15:06:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.70
[32m[20221213 15:06:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.56
[32m[20221213 15:06:08 @agent_ppo2.py:143][0m Total time:      13.22 min
[32m[20221213 15:06:08 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221213 15:06:08 @agent_ppo2.py:121][0m #------------------------ Iteration 579 --------------------------#
[32m[20221213 15:06:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0016 |           4.9470 |           0.2742 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0012 |           4.7692 |           0.2737 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0025 |           4.7607 |           0.2732 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0110 |           4.5255 |           0.2730 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0124 |           4.4528 |           0.2733 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0108 |           4.4125 |           0.2734 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0162 |           4.3680 |           0.2733 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0165 |           4.3525 |           0.2734 |
[32m[20221213 15:06:09 @agent_ppo2.py:185][0m |          -0.0161 |           4.2887 |           0.2733 |
[32m[20221213 15:06:10 @agent_ppo2.py:185][0m |          -0.0211 |           4.2643 |           0.2733 |
[32m[20221213 15:06:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.30
[32m[20221213 15:06:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.83
[32m[20221213 15:06:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.12
[32m[20221213 15:06:10 @agent_ppo2.py:143][0m Total time:      13.24 min
[32m[20221213 15:06:10 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221213 15:06:10 @agent_ppo2.py:121][0m #------------------------ Iteration 580 --------------------------#
[32m[20221213 15:06:10 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:06:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:10 @agent_ppo2.py:185][0m |           0.0060 |           5.3006 |           0.2759 |
[32m[20221213 15:06:10 @agent_ppo2.py:185][0m |          -0.0039 |           5.1166 |           0.2758 |
[32m[20221213 15:06:10 @agent_ppo2.py:185][0m |          -0.0096 |           4.9287 |           0.2758 |
[32m[20221213 15:06:10 @agent_ppo2.py:185][0m |          -0.0125 |           4.9013 |           0.2755 |
[32m[20221213 15:06:11 @agent_ppo2.py:185][0m |          -0.0123 |           4.8364 |           0.2753 |
[32m[20221213 15:06:11 @agent_ppo2.py:185][0m |          -0.0108 |           4.8460 |           0.2755 |
[32m[20221213 15:06:11 @agent_ppo2.py:185][0m |          -0.0173 |           4.7666 |           0.2754 |
[32m[20221213 15:06:11 @agent_ppo2.py:185][0m |          -0.0132 |           4.7717 |           0.2754 |
[32m[20221213 15:06:11 @agent_ppo2.py:185][0m |          -0.0196 |           4.6745 |           0.2754 |
[32m[20221213 15:06:11 @agent_ppo2.py:185][0m |          -0.0203 |           4.6647 |           0.2756 |
[32m[20221213 15:06:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:06:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.43
[32m[20221213 15:06:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.52
[32m[20221213 15:06:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 123.67
[32m[20221213 15:06:11 @agent_ppo2.py:143][0m Total time:      13.27 min
[32m[20221213 15:06:11 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221213 15:06:11 @agent_ppo2.py:121][0m #------------------------ Iteration 581 --------------------------#
[32m[20221213 15:06:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0012 |           5.3239 |           0.2870 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0108 |           5.1681 |           0.2873 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0134 |           5.0500 |           0.2872 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0137 |           4.9689 |           0.2870 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0170 |           4.9190 |           0.2871 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0154 |           4.9129 |           0.2869 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0193 |           4.8256 |           0.2871 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0096 |           5.1135 |           0.2871 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0174 |           4.7685 |           0.2871 |
[32m[20221213 15:06:12 @agent_ppo2.py:185][0m |          -0.0201 |           4.7257 |           0.2868 |
[32m[20221213 15:06:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.74
[32m[20221213 15:06:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.85
[32m[20221213 15:06:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.68
[32m[20221213 15:06:12 @agent_ppo2.py:143][0m Total time:      13.29 min
[32m[20221213 15:06:12 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221213 15:06:12 @agent_ppo2.py:121][0m #------------------------ Iteration 582 --------------------------#
[32m[20221213 15:06:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |           0.0012 |           5.0724 |           0.2832 |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |          -0.0100 |           4.9018 |           0.2821 |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |          -0.0127 |           4.8296 |           0.2819 |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |          -0.0131 |           4.7706 |           0.2814 |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |          -0.0165 |           4.7321 |           0.2812 |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |          -0.0161 |           4.7008 |           0.2811 |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |          -0.0103 |           4.8501 |           0.2810 |
[32m[20221213 15:06:13 @agent_ppo2.py:185][0m |          -0.0167 |           4.6519 |           0.2805 |
[32m[20221213 15:06:14 @agent_ppo2.py:185][0m |          -0.0173 |           4.6264 |           0.2806 |
[32m[20221213 15:06:14 @agent_ppo2.py:185][0m |          -0.0191 |           4.5907 |           0.2806 |
[32m[20221213 15:06:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:06:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.55
[32m[20221213 15:06:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.73
[32m[20221213 15:06:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.06
[32m[20221213 15:06:14 @agent_ppo2.py:143][0m Total time:      13.31 min
[32m[20221213 15:06:14 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221213 15:06:14 @agent_ppo2.py:121][0m #------------------------ Iteration 583 --------------------------#
[32m[20221213 15:06:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:14 @agent_ppo2.py:185][0m |          -0.0011 |           5.1311 |           0.2821 |
[32m[20221213 15:06:14 @agent_ppo2.py:185][0m |          -0.0095 |           4.6355 |           0.2814 |
[32m[20221213 15:06:14 @agent_ppo2.py:185][0m |          -0.0076 |           4.3799 |           0.2812 |
[32m[20221213 15:06:14 @agent_ppo2.py:185][0m |          -0.0133 |           4.2480 |           0.2810 |
[32m[20221213 15:06:15 @agent_ppo2.py:185][0m |          -0.0133 |           4.1811 |           0.2806 |
[32m[20221213 15:06:15 @agent_ppo2.py:185][0m |          -0.0144 |           4.0988 |           0.2806 |
[32m[20221213 15:06:15 @agent_ppo2.py:185][0m |          -0.0147 |           4.0485 |           0.2804 |
[32m[20221213 15:06:15 @agent_ppo2.py:185][0m |          -0.0173 |           4.0125 |           0.2804 |
[32m[20221213 15:06:15 @agent_ppo2.py:185][0m |          -0.0203 |           3.9680 |           0.2802 |
[32m[20221213 15:06:15 @agent_ppo2.py:185][0m |          -0.0211 |           3.9239 |           0.2802 |
[32m[20221213 15:06:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.61
[32m[20221213 15:06:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 130.14
[32m[20221213 15:06:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.49
[32m[20221213 15:06:15 @agent_ppo2.py:143][0m Total time:      13.33 min
[32m[20221213 15:06:15 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221213 15:06:15 @agent_ppo2.py:121][0m #------------------------ Iteration 584 --------------------------#
[32m[20221213 15:06:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |           0.0069 |           6.2352 |           0.2781 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0071 |           5.5268 |           0.2777 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0130 |           5.3021 |           0.2774 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0145 |           5.1754 |           0.2773 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0152 |           5.0774 |           0.2772 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0050 |           5.5089 |           0.2769 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0165 |           4.9885 |           0.2765 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0186 |           4.8664 |           0.2765 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0258 |           4.8557 |           0.2765 |
[32m[20221213 15:06:16 @agent_ppo2.py:185][0m |          -0.0158 |           4.8393 |           0.2764 |
[32m[20221213 15:06:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:06:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.86
[32m[20221213 15:06:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 130.07
[32m[20221213 15:06:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.31
[32m[20221213 15:06:16 @agent_ppo2.py:143][0m Total time:      13.36 min
[32m[20221213 15:06:16 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221213 15:06:16 @agent_ppo2.py:121][0m #------------------------ Iteration 585 --------------------------#
[32m[20221213 15:06:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |           0.0028 |           5.5912 |           0.2663 |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |          -0.0127 |           5.2409 |           0.2660 |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |          -0.0169 |           5.1441 |           0.2658 |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |          -0.0170 |           5.0918 |           0.2657 |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |          -0.0160 |           5.0447 |           0.2658 |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |          -0.0186 |           5.0345 |           0.2658 |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |          -0.0201 |           4.9598 |           0.2658 |
[32m[20221213 15:06:17 @agent_ppo2.py:185][0m |          -0.0183 |           4.9457 |           0.2657 |
[32m[20221213 15:06:18 @agent_ppo2.py:185][0m |          -0.0218 |           4.9301 |           0.2659 |
[32m[20221213 15:06:18 @agent_ppo2.py:185][0m |          -0.0235 |           4.8919 |           0.2657 |
[32m[20221213 15:06:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.39
[32m[20221213 15:06:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.71
[32m[20221213 15:06:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.19
[32m[20221213 15:06:18 @agent_ppo2.py:143][0m Total time:      13.38 min
[32m[20221213 15:06:18 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221213 15:06:18 @agent_ppo2.py:121][0m #------------------------ Iteration 586 --------------------------#
[32m[20221213 15:06:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:18 @agent_ppo2.py:185][0m |          -0.0007 |           5.3020 |           0.2756 |
[32m[20221213 15:06:18 @agent_ppo2.py:185][0m |          -0.0091 |           5.0786 |           0.2751 |
[32m[20221213 15:06:18 @agent_ppo2.py:185][0m |          -0.0136 |           5.0059 |           0.2751 |
[32m[20221213 15:06:18 @agent_ppo2.py:185][0m |          -0.0077 |           5.0315 |           0.2751 |
[32m[20221213 15:06:19 @agent_ppo2.py:185][0m |          -0.0150 |           4.9109 |           0.2747 |
[32m[20221213 15:06:19 @agent_ppo2.py:185][0m |          -0.0133 |           4.8996 |           0.2748 |
[32m[20221213 15:06:19 @agent_ppo2.py:185][0m |          -0.0158 |           4.8410 |           0.2749 |
[32m[20221213 15:06:19 @agent_ppo2.py:185][0m |          -0.0190 |           4.8095 |           0.2747 |
[32m[20221213 15:06:19 @agent_ppo2.py:185][0m |          -0.0184 |           4.8114 |           0.2749 |
[32m[20221213 15:06:19 @agent_ppo2.py:185][0m |          -0.0218 |           4.7947 |           0.2747 |
[32m[20221213 15:06:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.95
[32m[20221213 15:06:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.15
[32m[20221213 15:06:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.60
[32m[20221213 15:06:19 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 158.60
[32m[20221213 15:06:19 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 158.60
[32m[20221213 15:06:19 @agent_ppo2.py:143][0m Total time:      13.40 min
[32m[20221213 15:06:19 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221213 15:06:19 @agent_ppo2.py:121][0m #------------------------ Iteration 587 --------------------------#
[32m[20221213 15:06:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0000 |           5.3068 |           0.2797 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0040 |           5.1944 |           0.2799 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0124 |           5.1590 |           0.2794 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0067 |           5.1729 |           0.2794 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0100 |           5.0904 |           0.2795 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0121 |           5.0590 |           0.2795 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0152 |           5.0316 |           0.2792 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0137 |           5.0122 |           0.2792 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0185 |           4.9923 |           0.2792 |
[32m[20221213 15:06:20 @agent_ppo2.py:185][0m |          -0.0056 |           5.4456 |           0.2792 |
[32m[20221213 15:06:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.29
[32m[20221213 15:06:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.75
[32m[20221213 15:06:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.12
[32m[20221213 15:06:20 @agent_ppo2.py:143][0m Total time:      13.42 min
[32m[20221213 15:06:20 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221213 15:06:20 @agent_ppo2.py:121][0m #------------------------ Iteration 588 --------------------------#
[32m[20221213 15:06:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |           0.0084 |           5.3262 |           0.2691 |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |          -0.0001 |           5.1450 |           0.2690 |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |          -0.0103 |           4.8774 |           0.2690 |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |          -0.0102 |           4.8358 |           0.2690 |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |          -0.0118 |           4.8150 |           0.2690 |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |          -0.0130 |           4.7822 |           0.2691 |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |          -0.0157 |           4.7558 |           0.2689 |
[32m[20221213 15:06:21 @agent_ppo2.py:185][0m |          -0.0123 |           4.7747 |           0.2690 |
[32m[20221213 15:06:22 @agent_ppo2.py:185][0m |          -0.0171 |           4.7276 |           0.2687 |
[32m[20221213 15:06:22 @agent_ppo2.py:185][0m |          -0.0096 |           5.1628 |           0.2688 |
[32m[20221213 15:06:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.36
[32m[20221213 15:06:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.36
[32m[20221213 15:06:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 115.64
[32m[20221213 15:06:22 @agent_ppo2.py:143][0m Total time:      13.44 min
[32m[20221213 15:06:22 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221213 15:06:22 @agent_ppo2.py:121][0m #------------------------ Iteration 589 --------------------------#
[32m[20221213 15:06:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:22 @agent_ppo2.py:185][0m |          -0.0020 |           5.0186 |           0.2783 |
[32m[20221213 15:06:22 @agent_ppo2.py:185][0m |          -0.0091 |           4.8345 |           0.2778 |
[32m[20221213 15:06:22 @agent_ppo2.py:185][0m |          -0.0106 |           4.7407 |           0.2776 |
[32m[20221213 15:06:22 @agent_ppo2.py:185][0m |           0.0040 |           5.3337 |           0.2776 |
[32m[20221213 15:06:23 @agent_ppo2.py:185][0m |          -0.0089 |           4.6184 |           0.2777 |
[32m[20221213 15:06:23 @agent_ppo2.py:185][0m |          -0.0131 |           4.5667 |           0.2779 |
[32m[20221213 15:06:23 @agent_ppo2.py:185][0m |          -0.0138 |           4.5289 |           0.2782 |
[32m[20221213 15:06:23 @agent_ppo2.py:185][0m |          -0.0148 |           4.5064 |           0.2781 |
[32m[20221213 15:06:23 @agent_ppo2.py:185][0m |          -0.0122 |           4.4855 |           0.2784 |
[32m[20221213 15:06:23 @agent_ppo2.py:185][0m |          -0.0152 |           4.4459 |           0.2783 |
[32m[20221213 15:06:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.46
[32m[20221213 15:06:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.48
[32m[20221213 15:06:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.65
[32m[20221213 15:06:23 @agent_ppo2.py:143][0m Total time:      13.47 min
[32m[20221213 15:06:23 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221213 15:06:23 @agent_ppo2.py:121][0m #------------------------ Iteration 590 --------------------------#
[32m[20221213 15:06:23 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:06:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0001 |           5.3164 |           0.2713 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0101 |           5.1842 |           0.2706 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |           0.0030 |           5.4819 |           0.2706 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0055 |           5.1330 |           0.2700 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0109 |           5.0199 |           0.2697 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0167 |           4.9953 |           0.2700 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0131 |           4.9566 |           0.2700 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0152 |           4.9468 |           0.2701 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0149 |           4.9034 |           0.2698 |
[32m[20221213 15:06:24 @agent_ppo2.py:185][0m |          -0.0159 |           4.8806 |           0.2698 |
[32m[20221213 15:06:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.84
[32m[20221213 15:06:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.39
[32m[20221213 15:06:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.58
[32m[20221213 15:06:25 @agent_ppo2.py:143][0m Total time:      13.49 min
[32m[20221213 15:06:25 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221213 15:06:25 @agent_ppo2.py:121][0m #------------------------ Iteration 591 --------------------------#
[32m[20221213 15:06:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0003 |           5.3495 |           0.2706 |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0083 |           5.1870 |           0.2700 |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0090 |           5.1388 |           0.2700 |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0089 |           5.0812 |           0.2697 |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0126 |           5.0527 |           0.2696 |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0054 |           5.2956 |           0.2696 |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0052 |           5.1875 |           0.2696 |
[32m[20221213 15:06:25 @agent_ppo2.py:185][0m |          -0.0157 |           4.9608 |           0.2696 |
[32m[20221213 15:06:26 @agent_ppo2.py:185][0m |          -0.0151 |           4.9467 |           0.2695 |
[32m[20221213 15:06:26 @agent_ppo2.py:185][0m |          -0.0058 |           5.2555 |           0.2697 |
[32m[20221213 15:06:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.15
[32m[20221213 15:06:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.79
[32m[20221213 15:06:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 146.31
[32m[20221213 15:06:26 @agent_ppo2.py:143][0m Total time:      13.51 min
[32m[20221213 15:06:26 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221213 15:06:26 @agent_ppo2.py:121][0m #------------------------ Iteration 592 --------------------------#
[32m[20221213 15:06:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:26 @agent_ppo2.py:185][0m |           0.0012 |           5.3280 |           0.2808 |
[32m[20221213 15:06:26 @agent_ppo2.py:185][0m |          -0.0079 |           5.1122 |           0.2807 |
[32m[20221213 15:06:26 @agent_ppo2.py:185][0m |          -0.0008 |           5.5223 |           0.2806 |
[32m[20221213 15:06:26 @agent_ppo2.py:185][0m |          -0.0135 |           4.9014 |           0.2805 |
[32m[20221213 15:06:27 @agent_ppo2.py:185][0m |          -0.0131 |           4.8643 |           0.2804 |
[32m[20221213 15:06:27 @agent_ppo2.py:185][0m |          -0.0156 |           4.7857 |           0.2808 |
[32m[20221213 15:06:27 @agent_ppo2.py:185][0m |          -0.0166 |           4.7523 |           0.2805 |
[32m[20221213 15:06:27 @agent_ppo2.py:185][0m |          -0.0178 |           4.7032 |           0.2806 |
[32m[20221213 15:06:27 @agent_ppo2.py:185][0m |          -0.0176 |           4.6586 |           0.2805 |
[32m[20221213 15:06:27 @agent_ppo2.py:185][0m |          -0.0179 |           4.6300 |           0.2806 |
[32m[20221213 15:06:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.77
[32m[20221213 15:06:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 135.22
[32m[20221213 15:06:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.69
[32m[20221213 15:06:27 @agent_ppo2.py:143][0m Total time:      13.53 min
[32m[20221213 15:06:27 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221213 15:06:27 @agent_ppo2.py:121][0m #------------------------ Iteration 593 --------------------------#
[32m[20221213 15:06:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0051 |           5.4005 |           0.2825 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0012 |           5.5248 |           0.2821 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0121 |           5.2321 |           0.2814 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0148 |           5.1880 |           0.2814 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0166 |           5.1415 |           0.2811 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0152 |           5.1012 |           0.2811 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0172 |           5.0726 |           0.2811 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0191 |           5.0324 |           0.2810 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0141 |           5.2186 |           0.2809 |
[32m[20221213 15:06:28 @agent_ppo2.py:185][0m |          -0.0191 |           5.0046 |           0.2808 |
[32m[20221213 15:06:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.94
[32m[20221213 15:06:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.95
[32m[20221213 15:06:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.67
[32m[20221213 15:06:29 @agent_ppo2.py:143][0m Total time:      13.56 min
[32m[20221213 15:06:29 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221213 15:06:29 @agent_ppo2.py:121][0m #------------------------ Iteration 594 --------------------------#
[32m[20221213 15:06:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |           0.0008 |           4.8097 |           0.2810 |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |          -0.0023 |           4.7811 |           0.2806 |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |          -0.0098 |           4.5672 |           0.2805 |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |          -0.0045 |           4.6166 |           0.2803 |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |          -0.0153 |           4.4630 |           0.2803 |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |          -0.0128 |           4.4085 |           0.2803 |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |          -0.0036 |           4.6209 |           0.2804 |
[32m[20221213 15:06:29 @agent_ppo2.py:185][0m |          -0.0157 |           4.3171 |           0.2801 |
[32m[20221213 15:06:30 @agent_ppo2.py:185][0m |          -0.0162 |           4.2844 |           0.2801 |
[32m[20221213 15:06:30 @agent_ppo2.py:185][0m |          -0.0167 |           4.2745 |           0.2801 |
[32m[20221213 15:06:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.52
[32m[20221213 15:06:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.70
[32m[20221213 15:06:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 152.80
[32m[20221213 15:06:30 @agent_ppo2.py:143][0m Total time:      13.58 min
[32m[20221213 15:06:30 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221213 15:06:30 @agent_ppo2.py:121][0m #------------------------ Iteration 595 --------------------------#
[32m[20221213 15:06:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:30 @agent_ppo2.py:185][0m |          -0.0020 |           4.9299 |           0.2761 |
[32m[20221213 15:06:30 @agent_ppo2.py:185][0m |          -0.0060 |           4.7347 |           0.2765 |
[32m[20221213 15:06:30 @agent_ppo2.py:185][0m |          -0.0127 |           4.6397 |           0.2761 |
[32m[20221213 15:06:30 @agent_ppo2.py:185][0m |          -0.0088 |           4.5910 |           0.2762 |
[32m[20221213 15:06:31 @agent_ppo2.py:185][0m |          -0.0127 |           4.5107 |           0.2761 |
[32m[20221213 15:06:31 @agent_ppo2.py:185][0m |          -0.0073 |           4.5567 |           0.2760 |
[32m[20221213 15:06:31 @agent_ppo2.py:185][0m |          -0.0186 |           4.4127 |           0.2762 |
[32m[20221213 15:06:31 @agent_ppo2.py:185][0m |          -0.0161 |           4.3864 |           0.2764 |
[32m[20221213 15:06:31 @agent_ppo2.py:185][0m |          -0.0189 |           4.3664 |           0.2762 |
[32m[20221213 15:06:31 @agent_ppo2.py:185][0m |          -0.0230 |           4.3504 |           0.2763 |
[32m[20221213 15:06:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.26
[32m[20221213 15:06:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.42
[32m[20221213 15:06:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.44
[32m[20221213 15:06:31 @agent_ppo2.py:143][0m Total time:      13.60 min
[32m[20221213 15:06:31 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221213 15:06:31 @agent_ppo2.py:121][0m #------------------------ Iteration 596 --------------------------#
[32m[20221213 15:06:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0020 |           5.6372 |           0.2835 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0102 |           5.5124 |           0.2832 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0048 |           5.4631 |           0.2831 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0114 |           5.4067 |           0.2829 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0132 |           5.3495 |           0.2827 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0114 |           5.3282 |           0.2825 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0145 |           5.2989 |           0.2825 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0091 |           5.3772 |           0.2823 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0164 |           5.2700 |           0.2823 |
[32m[20221213 15:06:32 @agent_ppo2.py:185][0m |          -0.0170 |           5.2457 |           0.2821 |
[32m[20221213 15:06:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.14
[32m[20221213 15:06:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.55
[32m[20221213 15:06:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.14
[32m[20221213 15:06:33 @agent_ppo2.py:143][0m Total time:      13.62 min
[32m[20221213 15:06:33 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221213 15:06:33 @agent_ppo2.py:121][0m #------------------------ Iteration 597 --------------------------#
[32m[20221213 15:06:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0036 |           4.9497 |           0.2756 |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0083 |           4.7906 |           0.2751 |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0120 |           4.7411 |           0.2750 |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0111 |           4.6916 |           0.2750 |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0127 |           4.6657 |           0.2747 |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0015 |           5.2319 |           0.2748 |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0142 |           4.6111 |           0.2744 |
[32m[20221213 15:06:33 @agent_ppo2.py:185][0m |          -0.0190 |           4.5907 |           0.2744 |
[32m[20221213 15:06:34 @agent_ppo2.py:185][0m |          -0.0155 |           4.5568 |           0.2744 |
[32m[20221213 15:06:34 @agent_ppo2.py:185][0m |          -0.0154 |           4.5234 |           0.2744 |
[32m[20221213 15:06:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.43
[32m[20221213 15:06:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 135.55
[32m[20221213 15:06:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.89
[32m[20221213 15:06:34 @agent_ppo2.py:143][0m Total time:      13.64 min
[32m[20221213 15:06:34 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221213 15:06:34 @agent_ppo2.py:121][0m #------------------------ Iteration 598 --------------------------#
[32m[20221213 15:06:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:34 @agent_ppo2.py:185][0m |           0.0023 |           5.2334 |           0.2818 |
[32m[20221213 15:06:34 @agent_ppo2.py:185][0m |          -0.0075 |           5.1372 |           0.2812 |
[32m[20221213 15:06:34 @agent_ppo2.py:185][0m |          -0.0102 |           5.0665 |           0.2811 |
[32m[20221213 15:06:34 @agent_ppo2.py:185][0m |           0.0017 |           5.4879 |           0.2807 |
[32m[20221213 15:06:35 @agent_ppo2.py:185][0m |           0.0034 |           5.6566 |           0.2805 |
[32m[20221213 15:06:35 @agent_ppo2.py:185][0m |          -0.0129 |           5.0171 |           0.2803 |
[32m[20221213 15:06:35 @agent_ppo2.py:185][0m |          -0.0145 |           4.9491 |           0.2806 |
[32m[20221213 15:06:35 @agent_ppo2.py:185][0m |          -0.0144 |           4.9272 |           0.2806 |
[32m[20221213 15:06:35 @agent_ppo2.py:185][0m |          -0.0159 |           4.8915 |           0.2804 |
[32m[20221213 15:06:35 @agent_ppo2.py:185][0m |          -0.0158 |           4.8866 |           0.2803 |
[32m[20221213 15:06:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.87
[32m[20221213 15:06:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 131.69
[32m[20221213 15:06:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.15
[32m[20221213 15:06:35 @agent_ppo2.py:143][0m Total time:      13.67 min
[32m[20221213 15:06:35 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221213 15:06:35 @agent_ppo2.py:121][0m #------------------------ Iteration 599 --------------------------#
[32m[20221213 15:06:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |           0.0002 |           5.4172 |           0.2669 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0085 |           5.2632 |           0.2664 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0089 |           5.2017 |           0.2660 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0108 |           5.1321 |           0.2656 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0144 |           5.0736 |           0.2657 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0126 |           5.0907 |           0.2655 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0153 |           4.9885 |           0.2653 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0163 |           4.9619 |           0.2650 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0179 |           4.9332 |           0.2650 |
[32m[20221213 15:06:36 @agent_ppo2.py:185][0m |          -0.0174 |           4.8895 |           0.2647 |
[32m[20221213 15:06:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.42
[32m[20221213 15:06:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.27
[32m[20221213 15:06:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.97
[32m[20221213 15:06:36 @agent_ppo2.py:143][0m Total time:      13.69 min
[32m[20221213 15:06:36 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221213 15:06:36 @agent_ppo2.py:121][0m #------------------------ Iteration 600 --------------------------#
[32m[20221213 15:06:37 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:06:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |          -0.0001 |           5.2616 |           0.2705 |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |           0.0014 |           5.2765 |           0.2699 |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |          -0.0041 |           5.0880 |           0.2696 |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |          -0.0110 |           4.9809 |           0.2692 |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |          -0.0124 |           4.9230 |           0.2689 |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |          -0.0149 |           4.9047 |           0.2685 |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |          -0.0160 |           4.8502 |           0.2683 |
[32m[20221213 15:06:37 @agent_ppo2.py:185][0m |          -0.0177 |           4.8242 |           0.2681 |
[32m[20221213 15:06:38 @agent_ppo2.py:185][0m |          -0.0165 |           4.7728 |           0.2677 |
[32m[20221213 15:06:38 @agent_ppo2.py:185][0m |          -0.0186 |           4.7562 |           0.2675 |
[32m[20221213 15:06:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.63
[32m[20221213 15:06:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.55
[32m[20221213 15:06:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.09
[32m[20221213 15:06:38 @agent_ppo2.py:143][0m Total time:      13.71 min
[32m[20221213 15:06:38 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221213 15:06:38 @agent_ppo2.py:121][0m #------------------------ Iteration 601 --------------------------#
[32m[20221213 15:06:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:38 @agent_ppo2.py:185][0m |          -0.0058 |           4.3601 |           0.2718 |
[32m[20221213 15:06:38 @agent_ppo2.py:185][0m |          -0.0102 |           4.2101 |           0.2717 |
[32m[20221213 15:06:38 @agent_ppo2.py:185][0m |          -0.0110 |           4.1445 |           0.2717 |
[32m[20221213 15:06:38 @agent_ppo2.py:185][0m |          -0.0126 |           4.1153 |           0.2716 |
[32m[20221213 15:06:39 @agent_ppo2.py:185][0m |          -0.0162 |           4.0983 |           0.2716 |
[32m[20221213 15:06:39 @agent_ppo2.py:185][0m |          -0.0147 |           4.0640 |           0.2719 |
[32m[20221213 15:06:39 @agent_ppo2.py:185][0m |          -0.0170 |           4.0514 |           0.2718 |
[32m[20221213 15:06:39 @agent_ppo2.py:185][0m |          -0.0104 |           4.2070 |           0.2721 |
[32m[20221213 15:06:39 @agent_ppo2.py:185][0m |          -0.0201 |           4.0128 |           0.2720 |
[32m[20221213 15:06:39 @agent_ppo2.py:185][0m |          -0.0036 |           4.4285 |           0.2721 |
[32m[20221213 15:06:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.00
[32m[20221213 15:06:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.77
[32m[20221213 15:06:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.12
[32m[20221213 15:06:39 @agent_ppo2.py:143][0m Total time:      13.73 min
[32m[20221213 15:06:39 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221213 15:06:39 @agent_ppo2.py:121][0m #------------------------ Iteration 602 --------------------------#
[32m[20221213 15:06:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0031 |           5.2117 |           0.2685 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0080 |           5.0510 |           0.2683 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0096 |           4.9761 |           0.2683 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0077 |           5.1585 |           0.2679 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0127 |           4.8849 |           0.2680 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0041 |           5.2760 |           0.2682 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0159 |           4.8662 |           0.2679 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0178 |           4.7897 |           0.2680 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0175 |           4.7714 |           0.2680 |
[32m[20221213 15:06:40 @agent_ppo2.py:185][0m |          -0.0170 |           4.7312 |           0.2680 |
[32m[20221213 15:06:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.91
[32m[20221213 15:06:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.02
[32m[20221213 15:06:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.12
[32m[20221213 15:06:41 @agent_ppo2.py:143][0m Total time:      13.76 min
[32m[20221213 15:06:41 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221213 15:06:41 @agent_ppo2.py:121][0m #------------------------ Iteration 603 --------------------------#
[32m[20221213 15:06:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0053 |           5.1712 |           0.2683 |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0015 |           5.1291 |           0.2682 |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0143 |           5.0164 |           0.2679 |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0138 |           4.9376 |           0.2679 |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0059 |           5.3148 |           0.2677 |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0126 |           4.8445 |           0.2671 |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0167 |           4.7973 |           0.2678 |
[32m[20221213 15:06:41 @agent_ppo2.py:185][0m |          -0.0193 |           4.7439 |           0.2680 |
[32m[20221213 15:06:42 @agent_ppo2.py:185][0m |          -0.0046 |           5.0228 |           0.2678 |
[32m[20221213 15:06:42 @agent_ppo2.py:185][0m |          -0.0209 |           4.6901 |           0.2676 |
[32m[20221213 15:06:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:06:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.89
[32m[20221213 15:06:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.26
[32m[20221213 15:06:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.23
[32m[20221213 15:06:42 @agent_ppo2.py:143][0m Total time:      13.78 min
[32m[20221213 15:06:42 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221213 15:06:42 @agent_ppo2.py:121][0m #------------------------ Iteration 604 --------------------------#
[32m[20221213 15:06:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:42 @agent_ppo2.py:185][0m |           0.0023 |           5.2477 |           0.2704 |
[32m[20221213 15:06:42 @agent_ppo2.py:185][0m |          -0.0108 |           5.1034 |           0.2702 |
[32m[20221213 15:06:42 @agent_ppo2.py:185][0m |          -0.0108 |           5.0281 |           0.2700 |
[32m[20221213 15:06:43 @agent_ppo2.py:185][0m |          -0.0118 |           4.9903 |           0.2697 |
[32m[20221213 15:06:43 @agent_ppo2.py:185][0m |          -0.0162 |           4.9429 |           0.2697 |
[32m[20221213 15:06:43 @agent_ppo2.py:185][0m |          -0.0160 |           4.9193 |           0.2697 |
[32m[20221213 15:06:43 @agent_ppo2.py:185][0m |          -0.0171 |           4.8745 |           0.2696 |
[32m[20221213 15:06:43 @agent_ppo2.py:185][0m |          -0.0170 |           4.8498 |           0.2696 |
[32m[20221213 15:06:43 @agent_ppo2.py:185][0m |          -0.0122 |           5.1451 |           0.2697 |
[32m[20221213 15:06:43 @agent_ppo2.py:185][0m |          -0.0190 |           4.8066 |           0.2694 |
[32m[20221213 15:06:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:06:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.35
[32m[20221213 15:06:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.55
[32m[20221213 15:06:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.41
[32m[20221213 15:06:43 @agent_ppo2.py:143][0m Total time:      13.80 min
[32m[20221213 15:06:43 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221213 15:06:43 @agent_ppo2.py:121][0m #------------------------ Iteration 605 --------------------------#
[32m[20221213 15:06:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0022 |           5.3085 |           0.2730 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0054 |           5.1322 |           0.2728 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0016 |           5.3902 |           0.2725 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0123 |           5.0692 |           0.2726 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0143 |           4.9301 |           0.2724 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0133 |           4.8923 |           0.2721 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0135 |           4.8757 |           0.2721 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0161 |           4.8382 |           0.2719 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0190 |           4.8061 |           0.2718 |
[32m[20221213 15:06:44 @agent_ppo2.py:185][0m |          -0.0196 |           4.7843 |           0.2718 |
[32m[20221213 15:06:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.48
[32m[20221213 15:06:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.16
[32m[20221213 15:06:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.38
[32m[20221213 15:06:45 @agent_ppo2.py:143][0m Total time:      13.82 min
[32m[20221213 15:06:45 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221213 15:06:45 @agent_ppo2.py:121][0m #------------------------ Iteration 606 --------------------------#
[32m[20221213 15:06:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:45 @agent_ppo2.py:185][0m |           0.0002 |           4.8798 |           0.2646 |
[32m[20221213 15:06:45 @agent_ppo2.py:185][0m |           0.0018 |           5.2062 |           0.2642 |
[32m[20221213 15:06:45 @agent_ppo2.py:185][0m |          -0.0128 |           4.6956 |           0.2636 |
[32m[20221213 15:06:45 @agent_ppo2.py:185][0m |          -0.0119 |           4.6369 |           0.2638 |
[32m[20221213 15:06:45 @agent_ppo2.py:185][0m |          -0.0147 |           4.5975 |           0.2640 |
[32m[20221213 15:06:45 @agent_ppo2.py:185][0m |          -0.0166 |           4.5751 |           0.2635 |
[32m[20221213 15:06:45 @agent_ppo2.py:185][0m |          -0.0186 |           4.5546 |           0.2637 |
[32m[20221213 15:06:46 @agent_ppo2.py:185][0m |          -0.0168 |           4.5532 |           0.2634 |
[32m[20221213 15:06:46 @agent_ppo2.py:185][0m |          -0.0161 |           4.5256 |           0.2634 |
[32m[20221213 15:06:46 @agent_ppo2.py:185][0m |          -0.0176 |           4.5011 |           0.2634 |
[32m[20221213 15:06:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.27
[32m[20221213 15:06:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.21
[32m[20221213 15:06:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 93.55
[32m[20221213 15:06:46 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221213 15:06:46 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221213 15:06:46 @agent_ppo2.py:121][0m #------------------------ Iteration 607 --------------------------#
[32m[20221213 15:06:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:46 @agent_ppo2.py:185][0m |          -0.0007 |           5.4104 |           0.2615 |
[32m[20221213 15:06:46 @agent_ppo2.py:185][0m |          -0.0106 |           5.2947 |           0.2611 |
[32m[20221213 15:06:46 @agent_ppo2.py:185][0m |          -0.0124 |           5.2434 |           0.2608 |
[32m[20221213 15:06:47 @agent_ppo2.py:185][0m |          -0.0122 |           5.2334 |           0.2606 |
[32m[20221213 15:06:47 @agent_ppo2.py:185][0m |          -0.0025 |           5.7609 |           0.2601 |
[32m[20221213 15:06:47 @agent_ppo2.py:185][0m |          -0.0107 |           5.3961 |           0.2600 |
[32m[20221213 15:06:47 @agent_ppo2.py:185][0m |          -0.0181 |           5.1373 |           0.2601 |
[32m[20221213 15:06:47 @agent_ppo2.py:185][0m |          -0.0193 |           5.1201 |           0.2600 |
[32m[20221213 15:06:47 @agent_ppo2.py:185][0m |          -0.0199 |           5.0956 |           0.2599 |
[32m[20221213 15:06:47 @agent_ppo2.py:185][0m |          -0.0193 |           5.0847 |           0.2600 |
[32m[20221213 15:06:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.75
[32m[20221213 15:06:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.35
[32m[20221213 15:06:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.10
[32m[20221213 15:06:47 @agent_ppo2.py:143][0m Total time:      13.87 min
[32m[20221213 15:06:47 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221213 15:06:47 @agent_ppo2.py:121][0m #------------------------ Iteration 608 --------------------------#
[32m[20221213 15:06:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0028 |           5.2472 |           0.2628 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0097 |           5.1669 |           0.2627 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0132 |           5.1330 |           0.2625 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0156 |           5.1121 |           0.2625 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0156 |           5.0824 |           0.2624 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0098 |           5.2101 |           0.2625 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0174 |           5.0480 |           0.2623 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0200 |           5.0386 |           0.2626 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0170 |           5.0217 |           0.2626 |
[32m[20221213 15:06:48 @agent_ppo2.py:185][0m |          -0.0191 |           5.0120 |           0.2626 |
[32m[20221213 15:06:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:06:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.08
[32m[20221213 15:06:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.56
[32m[20221213 15:06:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.60
[32m[20221213 15:06:49 @agent_ppo2.py:143][0m Total time:      13.89 min
[32m[20221213 15:06:49 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221213 15:06:49 @agent_ppo2.py:121][0m #------------------------ Iteration 609 --------------------------#
[32m[20221213 15:06:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:06:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:49 @agent_ppo2.py:185][0m |           0.0002 |           5.2095 |           0.2604 |
[32m[20221213 15:06:49 @agent_ppo2.py:185][0m |          -0.0084 |           5.1547 |           0.2602 |
[32m[20221213 15:06:49 @agent_ppo2.py:185][0m |          -0.0103 |           5.1404 |           0.2598 |
[32m[20221213 15:06:49 @agent_ppo2.py:185][0m |          -0.0133 |           5.1127 |           0.2596 |
[32m[20221213 15:06:49 @agent_ppo2.py:185][0m |          -0.0155 |           5.0886 |           0.2592 |
[32m[20221213 15:06:49 @agent_ppo2.py:185][0m |          -0.0142 |           5.0756 |           0.2592 |
[32m[20221213 15:06:49 @agent_ppo2.py:185][0m |          -0.0196 |           5.0757 |           0.2590 |
[32m[20221213 15:06:50 @agent_ppo2.py:185][0m |          -0.0186 |           5.0410 |           0.2590 |
[32m[20221213 15:06:50 @agent_ppo2.py:185][0m |          -0.0213 |           5.0630 |           0.2586 |
[32m[20221213 15:06:50 @agent_ppo2.py:185][0m |          -0.0166 |           5.0749 |           0.2586 |
[32m[20221213 15:06:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:06:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 119.26
[32m[20221213 15:06:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 124.32
[32m[20221213 15:06:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.08
[32m[20221213 15:06:50 @agent_ppo2.py:143][0m Total time:      13.91 min
[32m[20221213 15:06:50 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221213 15:06:50 @agent_ppo2.py:121][0m #------------------------ Iteration 610 --------------------------#
[32m[20221213 15:06:50 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:06:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:50 @agent_ppo2.py:185][0m |           0.0053 |           5.5980 |           0.2607 |
[32m[20221213 15:06:50 @agent_ppo2.py:185][0m |          -0.0097 |           5.2392 |           0.2595 |
[32m[20221213 15:06:50 @agent_ppo2.py:185][0m |          -0.0075 |           5.2211 |           0.2596 |
[32m[20221213 15:06:51 @agent_ppo2.py:185][0m |          -0.0117 |           5.1335 |           0.2591 |
[32m[20221213 15:06:51 @agent_ppo2.py:185][0m |          -0.0056 |           5.2011 |           0.2589 |
[32m[20221213 15:06:51 @agent_ppo2.py:185][0m |          -0.0146 |           5.0641 |           0.2586 |
[32m[20221213 15:06:51 @agent_ppo2.py:185][0m |          -0.0147 |           5.0256 |           0.2586 |
[32m[20221213 15:06:51 @agent_ppo2.py:185][0m |          -0.0172 |           5.0095 |           0.2583 |
[32m[20221213 15:06:51 @agent_ppo2.py:185][0m |          -0.0131 |           4.9952 |           0.2584 |
[32m[20221213 15:06:51 @agent_ppo2.py:185][0m |          -0.0163 |           4.9469 |           0.2581 |
[32m[20221213 15:06:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:06:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.13
[32m[20221213 15:06:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 154.46
[32m[20221213 15:06:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.71
[32m[20221213 15:06:51 @agent_ppo2.py:143][0m Total time:      13.93 min
[32m[20221213 15:06:51 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221213 15:06:51 @agent_ppo2.py:121][0m #------------------------ Iteration 611 --------------------------#
[32m[20221213 15:06:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:06:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0022 |           5.0628 |           0.2595 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0097 |           4.9036 |           0.2594 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0118 |           4.8513 |           0.2594 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0125 |           4.8279 |           0.2593 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0145 |           4.8030 |           0.2593 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0143 |           4.7840 |           0.2594 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0156 |           4.7550 |           0.2596 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0158 |           4.7254 |           0.2598 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0172 |           4.7145 |           0.2599 |
[32m[20221213 15:06:52 @agent_ppo2.py:185][0m |          -0.0212 |           4.7141 |           0.2600 |
[32m[20221213 15:06:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:06:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.88
[32m[20221213 15:06:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.89
[32m[20221213 15:06:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.88
[32m[20221213 15:06:53 @agent_ppo2.py:143][0m Total time:      13.96 min
[32m[20221213 15:06:53 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221213 15:06:53 @agent_ppo2.py:121][0m #------------------------ Iteration 612 --------------------------#
[32m[20221213 15:06:53 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 15:06:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:53 @agent_ppo2.py:185][0m |           0.0012 |           5.1585 |           0.2635 |
[32m[20221213 15:06:53 @agent_ppo2.py:185][0m |          -0.0061 |           4.8581 |           0.2628 |
[32m[20221213 15:06:53 @agent_ppo2.py:185][0m |          -0.0103 |           4.8010 |           0.2629 |
[32m[20221213 15:06:53 @agent_ppo2.py:185][0m |          -0.0100 |           4.7362 |           0.2627 |
[32m[20221213 15:06:54 @agent_ppo2.py:185][0m |          -0.0145 |           4.6922 |           0.2624 |
[32m[20221213 15:06:54 @agent_ppo2.py:185][0m |          -0.0147 |           4.6463 |           0.2624 |
[32m[20221213 15:06:54 @agent_ppo2.py:185][0m |          -0.0125 |           4.6352 |           0.2623 |
[32m[20221213 15:06:54 @agent_ppo2.py:185][0m |          -0.0183 |           4.5963 |           0.2622 |
[32m[20221213 15:06:54 @agent_ppo2.py:185][0m |          -0.0082 |           4.9600 |           0.2621 |
[32m[20221213 15:06:54 @agent_ppo2.py:185][0m |          -0.0168 |           4.5740 |           0.2620 |
[32m[20221213 15:06:54 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 15:06:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.66
[32m[20221213 15:06:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.47
[32m[20221213 15:06:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.04
[32m[20221213 15:06:54 @agent_ppo2.py:143][0m Total time:      13.98 min
[32m[20221213 15:06:54 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221213 15:06:54 @agent_ppo2.py:121][0m #------------------------ Iteration 613 --------------------------#
[32m[20221213 15:06:55 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 15:06:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |           0.0017 |           5.6935 |           0.2578 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0068 |           5.5510 |           0.2576 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0098 |           5.5236 |           0.2576 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0079 |           5.5700 |           0.2575 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0136 |           5.4606 |           0.2574 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0115 |           5.6489 |           0.2575 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0181 |           5.4222 |           0.2578 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0186 |           5.3901 |           0.2576 |
[32m[20221213 15:06:55 @agent_ppo2.py:185][0m |          -0.0177 |           5.3881 |           0.2575 |
[32m[20221213 15:06:56 @agent_ppo2.py:185][0m |          -0.0169 |           5.3582 |           0.2576 |
[32m[20221213 15:06:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:06:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.20
[32m[20221213 15:06:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.41
[32m[20221213 15:06:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.18
[32m[20221213 15:06:56 @agent_ppo2.py:143][0m Total time:      14.01 min
[32m[20221213 15:06:56 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221213 15:06:56 @agent_ppo2.py:121][0m #------------------------ Iteration 614 --------------------------#
[32m[20221213 15:06:56 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:06:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:56 @agent_ppo2.py:185][0m |          -0.0041 |           5.2528 |           0.2696 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0088 |           5.0281 |           0.2688 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0039 |           4.9648 |           0.2683 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |           0.0061 |           5.5271 |           0.2679 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0127 |           4.8353 |           0.2672 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0137 |           4.7646 |           0.2673 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0133 |           4.7264 |           0.2673 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0154 |           4.6951 |           0.2669 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0200 |           4.6726 |           0.2668 |
[32m[20221213 15:06:57 @agent_ppo2.py:185][0m |          -0.0171 |           4.6314 |           0.2665 |
[32m[20221213 15:06:57 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 15:06:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.08
[32m[20221213 15:06:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.48
[32m[20221213 15:06:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.43
[32m[20221213 15:06:58 @agent_ppo2.py:143][0m Total time:      14.04 min
[32m[20221213 15:06:58 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221213 15:06:58 @agent_ppo2.py:121][0m #------------------------ Iteration 615 --------------------------#
[32m[20221213 15:06:58 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:06:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:06:58 @agent_ppo2.py:185][0m |          -0.0031 |           5.2358 |           0.2611 |
[32m[20221213 15:06:58 @agent_ppo2.py:185][0m |          -0.0021 |           5.0516 |           0.2609 |
[32m[20221213 15:06:58 @agent_ppo2.py:185][0m |          -0.0093 |           4.9354 |           0.2605 |
[32m[20221213 15:06:58 @agent_ppo2.py:185][0m |          -0.0145 |           4.8992 |           0.2605 |
[32m[20221213 15:06:58 @agent_ppo2.py:185][0m |          -0.0147 |           4.8637 |           0.2603 |
[32m[20221213 15:06:59 @agent_ppo2.py:185][0m |          -0.0168 |           4.8366 |           0.2601 |
[32m[20221213 15:06:59 @agent_ppo2.py:185][0m |          -0.0166 |           4.7923 |           0.2601 |
[32m[20221213 15:06:59 @agent_ppo2.py:185][0m |          -0.0143 |           4.8073 |           0.2602 |
[32m[20221213 15:06:59 @agent_ppo2.py:185][0m |          -0.0191 |           4.7723 |           0.2600 |
[32m[20221213 15:06:59 @agent_ppo2.py:185][0m |          -0.0088 |           4.9728 |           0.2599 |
[32m[20221213 15:06:59 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 15:06:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.52
[32m[20221213 15:06:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.76
[32m[20221213 15:06:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 165.68
[32m[20221213 15:06:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 165.68
[32m[20221213 15:06:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 165.68
[32m[20221213 15:06:59 @agent_ppo2.py:143][0m Total time:      14.07 min
[32m[20221213 15:06:59 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221213 15:06:59 @agent_ppo2.py:121][0m #------------------------ Iteration 616 --------------------------#
[32m[20221213 15:06:59 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:07:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |           0.0078 |           6.0991 |           0.2641 |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |          -0.0042 |           5.6032 |           0.2640 |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |          -0.0083 |           5.5585 |           0.2638 |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |          -0.0065 |           5.4406 |           0.2635 |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |          -0.0120 |           5.4099 |           0.2635 |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |          -0.0127 |           5.3482 |           0.2635 |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |          -0.0125 |           5.3323 |           0.2634 |
[32m[20221213 15:07:00 @agent_ppo2.py:185][0m |          -0.0075 |           5.6513 |           0.2632 |
[32m[20221213 15:07:01 @agent_ppo2.py:185][0m |          -0.0188 |           5.2840 |           0.2635 |
[32m[20221213 15:07:01 @agent_ppo2.py:185][0m |          -0.0185 |           5.2317 |           0.2634 |
[32m[20221213 15:07:01 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 15:07:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 118.95
[32m[20221213 15:07:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 122.00
[32m[20221213 15:07:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 135.75
[32m[20221213 15:07:01 @agent_ppo2.py:143][0m Total time:      14.09 min
[32m[20221213 15:07:01 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221213 15:07:01 @agent_ppo2.py:121][0m #------------------------ Iteration 617 --------------------------#
[32m[20221213 15:07:01 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:07:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:01 @agent_ppo2.py:185][0m |          -0.0010 |           5.2464 |           0.2629 |
[32m[20221213 15:07:01 @agent_ppo2.py:185][0m |          -0.0097 |           5.1645 |           0.2630 |
[32m[20221213 15:07:01 @agent_ppo2.py:185][0m |          -0.0109 |           5.1269 |           0.2630 |
[32m[20221213 15:07:02 @agent_ppo2.py:185][0m |          -0.0131 |           5.0699 |           0.2631 |
[32m[20221213 15:07:02 @agent_ppo2.py:185][0m |          -0.0139 |           5.0426 |           0.2629 |
[32m[20221213 15:07:02 @agent_ppo2.py:185][0m |           0.0028 |           5.5357 |           0.2631 |
[32m[20221213 15:07:02 @agent_ppo2.py:185][0m |          -0.0161 |           5.0328 |           0.2628 |
[32m[20221213 15:07:02 @agent_ppo2.py:185][0m |          -0.0148 |           4.9641 |           0.2628 |
[32m[20221213 15:07:02 @agent_ppo2.py:185][0m |          -0.0126 |           4.9931 |           0.2630 |
[32m[20221213 15:07:02 @agent_ppo2.py:185][0m |          -0.0182 |           4.9178 |           0.2629 |
[32m[20221213 15:07:02 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:07:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 136.96
[32m[20221213 15:07:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.70
[32m[20221213 15:07:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.84
[32m[20221213 15:07:02 @agent_ppo2.py:143][0m Total time:      14.12 min
[32m[20221213 15:07:02 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221213 15:07:02 @agent_ppo2.py:121][0m #------------------------ Iteration 618 --------------------------#
[32m[20221213 15:07:03 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:07:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:03 @agent_ppo2.py:185][0m |           0.0002 |           5.7570 |           0.2629 |
[32m[20221213 15:07:03 @agent_ppo2.py:185][0m |           0.0077 |           6.0348 |           0.2625 |
[32m[20221213 15:07:03 @agent_ppo2.py:185][0m |          -0.0094 |           5.5218 |           0.2624 |
[32m[20221213 15:07:03 @agent_ppo2.py:185][0m |          -0.0076 |           5.4888 |           0.2623 |
[32m[20221213 15:07:03 @agent_ppo2.py:185][0m |          -0.0131 |           5.4577 |           0.2624 |
[32m[20221213 15:07:03 @agent_ppo2.py:185][0m |          -0.0141 |           5.4104 |           0.2623 |
[32m[20221213 15:07:04 @agent_ppo2.py:185][0m |          -0.0177 |           5.3963 |           0.2623 |
[32m[20221213 15:07:04 @agent_ppo2.py:185][0m |          -0.0186 |           5.3626 |           0.2622 |
[32m[20221213 15:07:04 @agent_ppo2.py:185][0m |          -0.0138 |           5.3777 |           0.2625 |
[32m[20221213 15:07:04 @agent_ppo2.py:185][0m |          -0.0204 |           5.3200 |           0.2623 |
[32m[20221213 15:07:04 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:07:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.95
[32m[20221213 15:07:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.10
[32m[20221213 15:07:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.23
[32m[20221213 15:07:04 @agent_ppo2.py:143][0m Total time:      14.15 min
[32m[20221213 15:07:04 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221213 15:07:04 @agent_ppo2.py:121][0m #------------------------ Iteration 619 --------------------------#
[32m[20221213 15:07:04 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:07:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:04 @agent_ppo2.py:185][0m |          -0.0024 |           5.2811 |           0.2612 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0042 |           5.1488 |           0.2606 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0099 |           5.0723 |           0.2603 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0125 |           5.0314 |           0.2603 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0149 |           4.9935 |           0.2603 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0148 |           4.9423 |           0.2598 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0150 |           4.9255 |           0.2598 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0138 |           4.8855 |           0.2597 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0177 |           4.8988 |           0.2596 |
[32m[20221213 15:07:05 @agent_ppo2.py:185][0m |          -0.0098 |           4.9586 |           0.2594 |
[32m[20221213 15:07:05 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 15:07:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.60
[32m[20221213 15:07:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.39
[32m[20221213 15:07:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.19
[32m[20221213 15:07:06 @agent_ppo2.py:143][0m Total time:      14.17 min
[32m[20221213 15:07:06 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221213 15:07:06 @agent_ppo2.py:121][0m #------------------------ Iteration 620 --------------------------#
[32m[20221213 15:07:06 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 15:07:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:06 @agent_ppo2.py:185][0m |          -0.0046 |           4.8655 |           0.2624 |
[32m[20221213 15:07:06 @agent_ppo2.py:185][0m |          -0.0044 |           4.6307 |           0.2620 |
[32m[20221213 15:07:06 @agent_ppo2.py:185][0m |          -0.0120 |           4.4893 |           0.2616 |
[32m[20221213 15:07:06 @agent_ppo2.py:185][0m |          -0.0151 |           4.4062 |           0.2615 |
[32m[20221213 15:07:07 @agent_ppo2.py:185][0m |          -0.0145 |           4.3640 |           0.2614 |
[32m[20221213 15:07:07 @agent_ppo2.py:185][0m |          -0.0141 |           4.3037 |           0.2614 |
[32m[20221213 15:07:07 @agent_ppo2.py:185][0m |          -0.0108 |           4.3771 |           0.2614 |
[32m[20221213 15:07:07 @agent_ppo2.py:185][0m |          -0.0143 |           4.1995 |           0.2613 |
[32m[20221213 15:07:07 @agent_ppo2.py:185][0m |          -0.0172 |           4.1607 |           0.2613 |
[32m[20221213 15:07:07 @agent_ppo2.py:185][0m |          -0.0181 |           4.1113 |           0.2615 |
[32m[20221213 15:07:07 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 15:07:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.43
[32m[20221213 15:07:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.52
[32m[20221213 15:07:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 119.86
[32m[20221213 15:07:07 @agent_ppo2.py:143][0m Total time:      14.20 min
[32m[20221213 15:07:07 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221213 15:07:07 @agent_ppo2.py:121][0m #------------------------ Iteration 621 --------------------------#
[32m[20221213 15:07:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0061 |           5.1163 |           0.2589 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0111 |           4.8943 |           0.2588 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0105 |           4.8028 |           0.2587 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0169 |           4.7637 |           0.2588 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0146 |           4.7165 |           0.2586 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0163 |           4.6625 |           0.2586 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0183 |           4.6328 |           0.2586 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0179 |           4.6093 |           0.2584 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0209 |           4.5737 |           0.2584 |
[32m[20221213 15:07:08 @agent_ppo2.py:185][0m |          -0.0173 |           4.5992 |           0.2583 |
[32m[20221213 15:07:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:07:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.38
[32m[20221213 15:07:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 128.46
[32m[20221213 15:07:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.66
[32m[20221213 15:07:09 @agent_ppo2.py:143][0m Total time:      14.22 min
[32m[20221213 15:07:09 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221213 15:07:09 @agent_ppo2.py:121][0m #------------------------ Iteration 622 --------------------------#
[32m[20221213 15:07:09 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:07:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:09 @agent_ppo2.py:185][0m |          -0.0001 |           5.3405 |           0.2581 |
[32m[20221213 15:07:09 @agent_ppo2.py:185][0m |          -0.0111 |           5.2496 |           0.2578 |
[32m[20221213 15:07:09 @agent_ppo2.py:185][0m |          -0.0140 |           5.1929 |           0.2576 |
[32m[20221213 15:07:09 @agent_ppo2.py:185][0m |          -0.0148 |           5.1719 |           0.2575 |
[32m[20221213 15:07:09 @agent_ppo2.py:185][0m |          -0.0132 |           5.1191 |           0.2574 |
[32m[20221213 15:07:09 @agent_ppo2.py:185][0m |          -0.0141 |           5.0971 |           0.2572 |
[32m[20221213 15:07:10 @agent_ppo2.py:185][0m |          -0.0150 |           5.0492 |           0.2570 |
[32m[20221213 15:07:10 @agent_ppo2.py:185][0m |          -0.0160 |           5.0438 |           0.2570 |
[32m[20221213 15:07:10 @agent_ppo2.py:185][0m |          -0.0213 |           5.0242 |           0.2568 |
[32m[20221213 15:07:10 @agent_ppo2.py:185][0m |          -0.0142 |           5.0019 |           0.2567 |
[32m[20221213 15:07:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 117.47
[32m[20221213 15:07:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.00
[32m[20221213 15:07:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.23
[32m[20221213 15:07:10 @agent_ppo2.py:143][0m Total time:      14.25 min
[32m[20221213 15:07:10 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221213 15:07:10 @agent_ppo2.py:121][0m #------------------------ Iteration 623 --------------------------#
[32m[20221213 15:07:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:10 @agent_ppo2.py:185][0m |           0.0019 |           5.2521 |           0.2582 |
[32m[20221213 15:07:10 @agent_ppo2.py:185][0m |          -0.0061 |           5.0940 |           0.2582 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0130 |           5.0443 |           0.2581 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0105 |           4.9940 |           0.2580 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0143 |           4.9742 |           0.2579 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0130 |           4.9477 |           0.2579 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0141 |           4.9060 |           0.2578 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0156 |           4.8927 |           0.2578 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0169 |           4.8748 |           0.2577 |
[32m[20221213 15:07:11 @agent_ppo2.py:185][0m |          -0.0146 |           4.8992 |           0.2576 |
[32m[20221213 15:07:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.91
[32m[20221213 15:07:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.72
[32m[20221213 15:07:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.59
[32m[20221213 15:07:11 @agent_ppo2.py:143][0m Total time:      14.27 min
[32m[20221213 15:07:11 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221213 15:07:11 @agent_ppo2.py:121][0m #------------------------ Iteration 624 --------------------------#
[32m[20221213 15:07:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0028 |           5.1509 |           0.2603 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0089 |           5.0424 |           0.2598 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0109 |           4.9680 |           0.2595 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0122 |           4.9183 |           0.2592 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0145 |           4.8885 |           0.2593 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0159 |           4.8709 |           0.2590 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0189 |           4.8315 |           0.2591 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0168 |           4.8189 |           0.2592 |
[32m[20221213 15:07:12 @agent_ppo2.py:185][0m |          -0.0180 |           4.7816 |           0.2592 |
[32m[20221213 15:07:13 @agent_ppo2.py:185][0m |          -0.0206 |           4.7650 |           0.2593 |
[32m[20221213 15:07:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 110.90
[32m[20221213 15:07:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 118.02
[32m[20221213 15:07:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 132.02
[32m[20221213 15:07:13 @agent_ppo2.py:143][0m Total time:      14.29 min
[32m[20221213 15:07:13 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221213 15:07:13 @agent_ppo2.py:121][0m #------------------------ Iteration 625 --------------------------#
[32m[20221213 15:07:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:13 @agent_ppo2.py:185][0m |          -0.0031 |           5.3735 |           0.2504 |
[32m[20221213 15:07:13 @agent_ppo2.py:185][0m |          -0.0076 |           5.1440 |           0.2499 |
[32m[20221213 15:07:13 @agent_ppo2.py:185][0m |          -0.0125 |           5.0819 |           0.2494 |
[32m[20221213 15:07:13 @agent_ppo2.py:185][0m |          -0.0072 |           5.0615 |           0.2491 |
[32m[20221213 15:07:13 @agent_ppo2.py:185][0m |           0.0004 |           5.1929 |           0.2490 |
[32m[20221213 15:07:13 @agent_ppo2.py:185][0m |          -0.0088 |           5.0242 |           0.2488 |
[32m[20221213 15:07:14 @agent_ppo2.py:185][0m |          -0.0132 |           4.9239 |           0.2485 |
[32m[20221213 15:07:14 @agent_ppo2.py:185][0m |          -0.0147 |           4.9025 |           0.2483 |
[32m[20221213 15:07:14 @agent_ppo2.py:185][0m |          -0.0179 |           4.8562 |           0.2483 |
[32m[20221213 15:07:14 @agent_ppo2.py:185][0m |          -0.0161 |           4.8450 |           0.2482 |
[32m[20221213 15:07:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.21
[32m[20221213 15:07:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.43
[32m[20221213 15:07:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.94
[32m[20221213 15:07:14 @agent_ppo2.py:143][0m Total time:      14.31 min
[32m[20221213 15:07:14 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221213 15:07:14 @agent_ppo2.py:121][0m #------------------------ Iteration 626 --------------------------#
[32m[20221213 15:07:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:14 @agent_ppo2.py:185][0m |          -0.0039 |           5.2915 |           0.2578 |
[32m[20221213 15:07:14 @agent_ppo2.py:185][0m |          -0.0095 |           5.1368 |           0.2574 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0132 |           5.0670 |           0.2571 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0122 |           5.0107 |           0.2570 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0129 |           4.9753 |           0.2570 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0146 |           4.9293 |           0.2571 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0139 |           4.8831 |           0.2568 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0171 |           4.8543 |           0.2569 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0167 |           4.8160 |           0.2570 |
[32m[20221213 15:07:15 @agent_ppo2.py:185][0m |          -0.0194 |           4.7808 |           0.2568 |
[32m[20221213 15:07:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.75
[32m[20221213 15:07:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.66
[32m[20221213 15:07:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.92
[32m[20221213 15:07:15 @agent_ppo2.py:143][0m Total time:      14.34 min
[32m[20221213 15:07:15 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221213 15:07:15 @agent_ppo2.py:121][0m #------------------------ Iteration 627 --------------------------#
[32m[20221213 15:07:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |           0.0062 |           5.2277 |           0.2527 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0063 |           4.9597 |           0.2523 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0069 |           4.8814 |           0.2519 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0110 |           4.8236 |           0.2517 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0113 |           4.7790 |           0.2517 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0154 |           4.7514 |           0.2516 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0139 |           4.7099 |           0.2516 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0147 |           4.6907 |           0.2515 |
[32m[20221213 15:07:16 @agent_ppo2.py:185][0m |          -0.0163 |           4.6314 |           0.2514 |
[32m[20221213 15:07:17 @agent_ppo2.py:185][0m |          -0.0045 |           5.0777 |           0.2516 |
[32m[20221213 15:07:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 141.08
[32m[20221213 15:07:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 146.92
[32m[20221213 15:07:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 111.45
[32m[20221213 15:07:17 @agent_ppo2.py:143][0m Total time:      14.36 min
[32m[20221213 15:07:17 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221213 15:07:17 @agent_ppo2.py:121][0m #------------------------ Iteration 628 --------------------------#
[32m[20221213 15:07:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:17 @agent_ppo2.py:185][0m |          -0.0006 |           4.9682 |           0.2594 |
[32m[20221213 15:07:17 @agent_ppo2.py:185][0m |          -0.0092 |           4.6867 |           0.2592 |
[32m[20221213 15:07:17 @agent_ppo2.py:185][0m |          -0.0154 |           4.5400 |           0.2588 |
[32m[20221213 15:07:17 @agent_ppo2.py:185][0m |          -0.0145 |           4.4263 |           0.2586 |
[32m[20221213 15:07:17 @agent_ppo2.py:185][0m |          -0.0144 |           4.3562 |           0.2583 |
[32m[20221213 15:07:18 @agent_ppo2.py:185][0m |          -0.0210 |           4.2708 |           0.2583 |
[32m[20221213 15:07:18 @agent_ppo2.py:185][0m |          -0.0170 |           4.2114 |           0.2584 |
[32m[20221213 15:07:18 @agent_ppo2.py:185][0m |          -0.0038 |           4.8642 |           0.2582 |
[32m[20221213 15:07:18 @agent_ppo2.py:185][0m |          -0.0137 |           4.1362 |           0.2579 |
[32m[20221213 15:07:18 @agent_ppo2.py:185][0m |          -0.0213 |           4.0729 |           0.2580 |
[32m[20221213 15:07:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:07:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 122.88
[32m[20221213 15:07:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.12
[32m[20221213 15:07:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.79
[32m[20221213 15:07:18 @agent_ppo2.py:143][0m Total time:      14.38 min
[32m[20221213 15:07:18 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221213 15:07:18 @agent_ppo2.py:121][0m #------------------------ Iteration 629 --------------------------#
[32m[20221213 15:07:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:18 @agent_ppo2.py:185][0m |           0.0009 |           4.3236 |           0.2540 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |           0.0020 |           3.7902 |           0.2540 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |          -0.0100 |           3.6514 |           0.2541 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |          -0.0116 |           3.5793 |           0.2540 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |           0.0050 |           4.0607 |           0.2539 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |          -0.0153 |           3.5460 |           0.2541 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |          -0.0138 |           3.4681 |           0.2540 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |          -0.0177 |           3.4683 |           0.2540 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |          -0.0194 |           3.4273 |           0.2542 |
[32m[20221213 15:07:19 @agent_ppo2.py:185][0m |          -0.0236 |           3.4056 |           0.2542 |
[32m[20221213 15:07:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.56
[32m[20221213 15:07:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.14
[32m[20221213 15:07:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.47
[32m[20221213 15:07:19 @agent_ppo2.py:143][0m Total time:      14.40 min
[32m[20221213 15:07:19 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221213 15:07:19 @agent_ppo2.py:121][0m #------------------------ Iteration 630 --------------------------#
[32m[20221213 15:07:20 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:07:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |           0.0105 |           4.9100 |           0.2530 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0079 |           4.5626 |           0.2525 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0096 |           4.4556 |           0.2522 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0064 |           4.4286 |           0.2520 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0089 |           4.3949 |           0.2518 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0102 |           4.3278 |           0.2517 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0127 |           4.2816 |           0.2514 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0146 |           4.2421 |           0.2511 |
[32m[20221213 15:07:20 @agent_ppo2.py:185][0m |          -0.0153 |           4.2160 |           0.2512 |
[32m[20221213 15:07:21 @agent_ppo2.py:185][0m |          -0.0172 |           4.2224 |           0.2509 |
[32m[20221213 15:07:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.64
[32m[20221213 15:07:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.26
[32m[20221213 15:07:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.75
[32m[20221213 15:07:21 @agent_ppo2.py:143][0m Total time:      14.43 min
[32m[20221213 15:07:21 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221213 15:07:21 @agent_ppo2.py:121][0m #------------------------ Iteration 631 --------------------------#
[32m[20221213 15:07:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:21 @agent_ppo2.py:185][0m |           0.0079 |           5.9007 |           0.2537 |
[32m[20221213 15:07:21 @agent_ppo2.py:185][0m |          -0.0065 |           5.3870 |           0.2538 |
[32m[20221213 15:07:21 @agent_ppo2.py:185][0m |          -0.0011 |           5.5494 |           0.2536 |
[32m[20221213 15:07:21 @agent_ppo2.py:185][0m |          -0.0155 |           5.2732 |           0.2533 |
[32m[20221213 15:07:21 @agent_ppo2.py:185][0m |          -0.0119 |           5.2074 |           0.2534 |
[32m[20221213 15:07:22 @agent_ppo2.py:185][0m |          -0.0162 |           5.1928 |           0.2533 |
[32m[20221213 15:07:22 @agent_ppo2.py:185][0m |          -0.0151 |           5.1312 |           0.2531 |
[32m[20221213 15:07:22 @agent_ppo2.py:185][0m |          -0.0094 |           5.3349 |           0.2530 |
[32m[20221213 15:07:22 @agent_ppo2.py:185][0m |          -0.0158 |           5.0802 |           0.2528 |
[32m[20221213 15:07:22 @agent_ppo2.py:185][0m |          -0.0181 |           5.0450 |           0.2526 |
[32m[20221213 15:07:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:07:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.18
[32m[20221213 15:07:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 157.27
[32m[20221213 15:07:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 149.55
[32m[20221213 15:07:22 @agent_ppo2.py:143][0m Total time:      14.45 min
[32m[20221213 15:07:22 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221213 15:07:22 @agent_ppo2.py:121][0m #------------------------ Iteration 632 --------------------------#
[32m[20221213 15:07:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |           0.0022 |           5.8276 |           0.2504 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0094 |           5.6095 |           0.2502 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0112 |           5.5275 |           0.2500 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0118 |           5.4637 |           0.2502 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0123 |           5.4285 |           0.2502 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0159 |           5.3969 |           0.2500 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0171 |           5.3776 |           0.2500 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0174 |           5.3397 |           0.2500 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0172 |           5.3014 |           0.2501 |
[32m[20221213 15:07:23 @agent_ppo2.py:185][0m |          -0.0096 |           5.5371 |           0.2499 |
[32m[20221213 15:07:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:07:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.34
[32m[20221213 15:07:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.98
[32m[20221213 15:07:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.63
[32m[20221213 15:07:24 @agent_ppo2.py:143][0m Total time:      14.47 min
[32m[20221213 15:07:24 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221213 15:07:24 @agent_ppo2.py:121][0m #------------------------ Iteration 633 --------------------------#
[32m[20221213 15:07:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:24 @agent_ppo2.py:185][0m |          -0.0029 |           6.1789 |           0.2510 |
[32m[20221213 15:07:24 @agent_ppo2.py:185][0m |          -0.0036 |           5.9637 |           0.2509 |
[32m[20221213 15:07:24 @agent_ppo2.py:185][0m |          -0.0125 |           5.8439 |           0.2509 |
[32m[20221213 15:07:24 @agent_ppo2.py:185][0m |          -0.0150 |           5.7690 |           0.2509 |
[32m[20221213 15:07:24 @agent_ppo2.py:185][0m |          -0.0127 |           5.7206 |           0.2506 |
[32m[20221213 15:07:24 @agent_ppo2.py:185][0m |          -0.0143 |           5.6325 |           0.2505 |
[32m[20221213 15:07:24 @agent_ppo2.py:185][0m |          -0.0168 |           5.5921 |           0.2507 |
[32m[20221213 15:07:25 @agent_ppo2.py:185][0m |          -0.0156 |           5.5564 |           0.2507 |
[32m[20221213 15:07:25 @agent_ppo2.py:185][0m |          -0.0178 |           5.5241 |           0.2506 |
[32m[20221213 15:07:25 @agent_ppo2.py:185][0m |          -0.0200 |           5.5072 |           0.2506 |
[32m[20221213 15:07:25 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:07:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.85
[32m[20221213 15:07:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.89
[32m[20221213 15:07:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.10
[32m[20221213 15:07:25 @agent_ppo2.py:143][0m Total time:      14.50 min
[32m[20221213 15:07:25 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221213 15:07:25 @agent_ppo2.py:121][0m #------------------------ Iteration 634 --------------------------#
[32m[20221213 15:07:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:25 @agent_ppo2.py:185][0m |          -0.0025 |           5.7312 |           0.2487 |
[32m[20221213 15:07:25 @agent_ppo2.py:185][0m |          -0.0014 |           5.6159 |           0.2487 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0067 |           5.4539 |           0.2485 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0093 |           5.3855 |           0.2484 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0125 |           5.3464 |           0.2483 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0153 |           5.3171 |           0.2482 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0136 |           5.2697 |           0.2481 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0175 |           5.2409 |           0.2481 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0193 |           5.2164 |           0.2480 |
[32m[20221213 15:07:26 @agent_ppo2.py:185][0m |          -0.0178 |           5.1779 |           0.2480 |
[32m[20221213 15:07:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:07:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.57
[32m[20221213 15:07:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.69
[32m[20221213 15:07:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.23
[32m[20221213 15:07:26 @agent_ppo2.py:143][0m Total time:      14.52 min
[32m[20221213 15:07:26 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221213 15:07:26 @agent_ppo2.py:121][0m #------------------------ Iteration 635 --------------------------#
[32m[20221213 15:07:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |           0.0021 |           5.5068 |           0.2555 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |           0.0039 |           5.5934 |           0.2556 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |          -0.0004 |           5.5139 |           0.2551 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |          -0.0100 |           5.1964 |           0.2551 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |          -0.0120 |           5.1647 |           0.2549 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |          -0.0173 |           5.1323 |           0.2547 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |          -0.0164 |           5.1108 |           0.2547 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |          -0.0172 |           5.0825 |           0.2546 |
[32m[20221213 15:07:27 @agent_ppo2.py:185][0m |          -0.0156 |           5.0682 |           0.2545 |
[32m[20221213 15:07:28 @agent_ppo2.py:185][0m |          -0.0184 |           5.0705 |           0.2544 |
[32m[20221213 15:07:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:07:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.48
[32m[20221213 15:07:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.99
[32m[20221213 15:07:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.94
[32m[20221213 15:07:28 @agent_ppo2.py:143][0m Total time:      14.54 min
[32m[20221213 15:07:28 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221213 15:07:28 @agent_ppo2.py:121][0m #------------------------ Iteration 636 --------------------------#
[32m[20221213 15:07:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:28 @agent_ppo2.py:185][0m |           0.0167 |           5.8581 |           0.2504 |
[32m[20221213 15:07:28 @agent_ppo2.py:185][0m |          -0.0071 |           5.1289 |           0.2496 |
[32m[20221213 15:07:28 @agent_ppo2.py:185][0m |          -0.0110 |           5.0202 |           0.2493 |
[32m[20221213 15:07:28 @agent_ppo2.py:185][0m |          -0.0123 |           4.9753 |           0.2495 |
[32m[20221213 15:07:28 @agent_ppo2.py:185][0m |          -0.0137 |           4.9451 |           0.2495 |
[32m[20221213 15:07:29 @agent_ppo2.py:185][0m |          -0.0161 |           4.9087 |           0.2494 |
[32m[20221213 15:07:29 @agent_ppo2.py:185][0m |          -0.0172 |           4.8792 |           0.2494 |
[32m[20221213 15:07:29 @agent_ppo2.py:185][0m |          -0.0167 |           4.8495 |           0.2496 |
[32m[20221213 15:07:29 @agent_ppo2.py:185][0m |          -0.0191 |           4.8321 |           0.2496 |
[32m[20221213 15:07:29 @agent_ppo2.py:185][0m |          -0.0186 |           4.7928 |           0.2495 |
[32m[20221213 15:07:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.96
[32m[20221213 15:07:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.16
[32m[20221213 15:07:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.39
[32m[20221213 15:07:29 @agent_ppo2.py:143][0m Total time:      14.57 min
[32m[20221213 15:07:29 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221213 15:07:29 @agent_ppo2.py:121][0m #------------------------ Iteration 637 --------------------------#
[32m[20221213 15:07:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:29 @agent_ppo2.py:185][0m |           0.0085 |           5.4017 |           0.2541 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0067 |           4.9780 |           0.2536 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0082 |           4.9171 |           0.2533 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0121 |           4.8916 |           0.2533 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0145 |           4.8495 |           0.2530 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0115 |           4.8407 |           0.2527 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0164 |           4.8071 |           0.2527 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0065 |           5.0434 |           0.2526 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0166 |           4.7704 |           0.2521 |
[32m[20221213 15:07:30 @agent_ppo2.py:185][0m |          -0.0051 |           5.2372 |           0.2522 |
[32m[20221213 15:07:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.48
[32m[20221213 15:07:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.18
[32m[20221213 15:07:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 157.03
[32m[20221213 15:07:30 @agent_ppo2.py:143][0m Total time:      14.59 min
[32m[20221213 15:07:30 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221213 15:07:30 @agent_ppo2.py:121][0m #------------------------ Iteration 638 --------------------------#
[32m[20221213 15:07:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |           0.0026 |           5.1506 |           0.2467 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |           0.0010 |           4.9868 |           0.2469 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |           0.0026 |           5.3171 |           0.2469 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |          -0.0113 |           4.8190 |           0.2469 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |          -0.0158 |           4.7738 |           0.2471 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |          -0.0131 |           4.7485 |           0.2472 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |          -0.0170 |           4.7103 |           0.2475 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |          -0.0188 |           4.7002 |           0.2475 |
[32m[20221213 15:07:31 @agent_ppo2.py:185][0m |          -0.0079 |           5.0755 |           0.2477 |
[32m[20221213 15:07:32 @agent_ppo2.py:185][0m |          -0.0184 |           4.6551 |           0.2480 |
[32m[20221213 15:07:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.77
[32m[20221213 15:07:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.70
[32m[20221213 15:07:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.45
[32m[20221213 15:07:32 @agent_ppo2.py:143][0m Total time:      14.61 min
[32m[20221213 15:07:32 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221213 15:07:32 @agent_ppo2.py:121][0m #------------------------ Iteration 639 --------------------------#
[32m[20221213 15:07:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:32 @agent_ppo2.py:185][0m |           0.0003 |           5.2989 |           0.2556 |
[32m[20221213 15:07:32 @agent_ppo2.py:185][0m |          -0.0098 |           5.1171 |           0.2553 |
[32m[20221213 15:07:32 @agent_ppo2.py:185][0m |          -0.0117 |           5.0365 |           0.2549 |
[32m[20221213 15:07:32 @agent_ppo2.py:185][0m |          -0.0031 |           5.1860 |           0.2547 |
[32m[20221213 15:07:32 @agent_ppo2.py:185][0m |          -0.0156 |           4.9406 |           0.2548 |
[32m[20221213 15:07:33 @agent_ppo2.py:185][0m |          -0.0118 |           4.9082 |           0.2546 |
[32m[20221213 15:07:33 @agent_ppo2.py:185][0m |          -0.0170 |           4.8860 |           0.2546 |
[32m[20221213 15:07:33 @agent_ppo2.py:185][0m |          -0.0153 |           4.8629 |           0.2547 |
[32m[20221213 15:07:33 @agent_ppo2.py:185][0m |          -0.0171 |           4.8375 |           0.2547 |
[32m[20221213 15:07:33 @agent_ppo2.py:185][0m |          -0.0175 |           4.8002 |           0.2548 |
[32m[20221213 15:07:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.40
[32m[20221213 15:07:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.58
[32m[20221213 15:07:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 126.17
[32m[20221213 15:07:33 @agent_ppo2.py:143][0m Total time:      14.63 min
[32m[20221213 15:07:33 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221213 15:07:33 @agent_ppo2.py:121][0m #------------------------ Iteration 640 --------------------------#
[32m[20221213 15:07:33 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:07:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:33 @agent_ppo2.py:185][0m |          -0.0026 |           5.6993 |           0.2555 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0111 |           5.5589 |           0.2548 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0105 |           5.4626 |           0.2547 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0135 |           5.4039 |           0.2546 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0172 |           5.3622 |           0.2546 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0184 |           5.2927 |           0.2546 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0159 |           5.2487 |           0.2544 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0187 |           5.2062 |           0.2544 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0145 |           5.2119 |           0.2542 |
[32m[20221213 15:07:34 @agent_ppo2.py:185][0m |          -0.0176 |           5.1512 |           0.2541 |
[32m[20221213 15:07:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.31
[32m[20221213 15:07:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.65
[32m[20221213 15:07:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 162.82
[32m[20221213 15:07:34 @agent_ppo2.py:143][0m Total time:      14.65 min
[32m[20221213 15:07:34 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221213 15:07:34 @agent_ppo2.py:121][0m #------------------------ Iteration 641 --------------------------#
[32m[20221213 15:07:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0018 |           5.6092 |           0.2507 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0050 |           5.4409 |           0.2504 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0109 |           5.2749 |           0.2502 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0119 |           5.2219 |           0.2502 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0135 |           5.1539 |           0.2498 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0095 |           5.2375 |           0.2499 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0066 |           5.2643 |           0.2500 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0182 |           5.0877 |           0.2495 |
[32m[20221213 15:07:35 @agent_ppo2.py:185][0m |          -0.0176 |           5.0227 |           0.2498 |
[32m[20221213 15:07:36 @agent_ppo2.py:185][0m |          -0.0198 |           5.0078 |           0.2497 |
[32m[20221213 15:07:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.90
[32m[20221213 15:07:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.29
[32m[20221213 15:07:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 149.67
[32m[20221213 15:07:36 @agent_ppo2.py:143][0m Total time:      14.68 min
[32m[20221213 15:07:36 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221213 15:07:36 @agent_ppo2.py:121][0m #------------------------ Iteration 642 --------------------------#
[32m[20221213 15:07:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:36 @agent_ppo2.py:185][0m |          -0.0030 |           5.7729 |           0.2461 |
[32m[20221213 15:07:36 @agent_ppo2.py:185][0m |          -0.0096 |           5.6222 |           0.2455 |
[32m[20221213 15:07:36 @agent_ppo2.py:185][0m |          -0.0097 |           5.5345 |           0.2455 |
[32m[20221213 15:07:36 @agent_ppo2.py:185][0m |          -0.0126 |           5.5205 |           0.2452 |
[32m[20221213 15:07:36 @agent_ppo2.py:185][0m |          -0.0146 |           5.4613 |           0.2451 |
[32m[20221213 15:07:37 @agent_ppo2.py:185][0m |          -0.0157 |           5.4267 |           0.2451 |
[32m[20221213 15:07:37 @agent_ppo2.py:185][0m |          -0.0158 |           5.4023 |           0.2451 |
[32m[20221213 15:07:37 @agent_ppo2.py:185][0m |          -0.0131 |           5.3783 |           0.2450 |
[32m[20221213 15:07:37 @agent_ppo2.py:185][0m |          -0.0079 |           6.0649 |           0.2450 |
[32m[20221213 15:07:37 @agent_ppo2.py:185][0m |          -0.0150 |           5.3592 |           0.2450 |
[32m[20221213 15:07:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 136.33
[32m[20221213 15:07:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.24
[32m[20221213 15:07:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.78
[32m[20221213 15:07:37 @agent_ppo2.py:143][0m Total time:      14.70 min
[32m[20221213 15:07:37 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221213 15:07:37 @agent_ppo2.py:121][0m #------------------------ Iteration 643 --------------------------#
[32m[20221213 15:07:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:37 @agent_ppo2.py:185][0m |           0.0013 |           5.5197 |           0.2541 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0043 |           5.3627 |           0.2540 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0082 |           5.2998 |           0.2541 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0103 |           5.2458 |           0.2538 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0089 |           5.2173 |           0.2537 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0093 |           5.1691 |           0.2536 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0111 |           5.1876 |           0.2536 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0156 |           5.1474 |           0.2535 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0145 |           5.1191 |           0.2534 |
[32m[20221213 15:07:38 @agent_ppo2.py:185][0m |          -0.0173 |           5.1080 |           0.2533 |
[32m[20221213 15:07:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.37
[32m[20221213 15:07:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.91
[32m[20221213 15:07:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.84
[32m[20221213 15:07:38 @agent_ppo2.py:143][0m Total time:      14.72 min
[32m[20221213 15:07:38 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221213 15:07:38 @agent_ppo2.py:121][0m #------------------------ Iteration 644 --------------------------#
[32m[20221213 15:07:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |           0.0064 |           6.1426 |           0.2564 |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |          -0.0071 |           5.8589 |           0.2561 |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |          -0.0031 |           6.1438 |           0.2560 |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |          -0.0109 |           5.7157 |           0.2556 |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |          -0.0098 |           5.6657 |           0.2556 |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |          -0.0167 |           5.6112 |           0.2556 |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |          -0.0196 |           5.5942 |           0.2555 |
[32m[20221213 15:07:39 @agent_ppo2.py:185][0m |          -0.0165 |           5.5440 |           0.2554 |
[32m[20221213 15:07:40 @agent_ppo2.py:185][0m |          -0.0191 |           5.5226 |           0.2554 |
[32m[20221213 15:07:40 @agent_ppo2.py:185][0m |          -0.0178 |           5.4733 |           0.2554 |
[32m[20221213 15:07:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 136.65
[32m[20221213 15:07:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.36
[32m[20221213 15:07:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.96
[32m[20221213 15:07:40 @agent_ppo2.py:143][0m Total time:      14.74 min
[32m[20221213 15:07:40 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221213 15:07:40 @agent_ppo2.py:121][0m #------------------------ Iteration 645 --------------------------#
[32m[20221213 15:07:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:40 @agent_ppo2.py:185][0m |          -0.0036 |           5.9508 |           0.2480 |
[32m[20221213 15:07:40 @agent_ppo2.py:185][0m |          -0.0093 |           5.8002 |           0.2477 |
[32m[20221213 15:07:40 @agent_ppo2.py:185][0m |          -0.0102 |           5.7020 |           0.2476 |
[32m[20221213 15:07:40 @agent_ppo2.py:185][0m |          -0.0130 |           5.6505 |           0.2474 |
[32m[20221213 15:07:41 @agent_ppo2.py:185][0m |          -0.0165 |           5.6106 |           0.2473 |
[32m[20221213 15:07:41 @agent_ppo2.py:185][0m |          -0.0150 |           5.5855 |           0.2470 |
[32m[20221213 15:07:41 @agent_ppo2.py:185][0m |          -0.0186 |           5.5672 |           0.2470 |
[32m[20221213 15:07:41 @agent_ppo2.py:185][0m |          -0.0217 |           5.5339 |           0.2469 |
[32m[20221213 15:07:41 @agent_ppo2.py:185][0m |          -0.0237 |           5.5754 |           0.2470 |
[32m[20221213 15:07:41 @agent_ppo2.py:185][0m |          -0.0112 |           5.8549 |           0.2469 |
[32m[20221213 15:07:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.20
[32m[20221213 15:07:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.47
[32m[20221213 15:07:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.54
[32m[20221213 15:07:41 @agent_ppo2.py:143][0m Total time:      14.77 min
[32m[20221213 15:07:41 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221213 15:07:41 @agent_ppo2.py:121][0m #------------------------ Iteration 646 --------------------------#
[32m[20221213 15:07:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:41 @agent_ppo2.py:185][0m |          -0.0022 |           5.7829 |           0.2514 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0023 |           5.5674 |           0.2510 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0104 |           5.4050 |           0.2506 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0098 |           5.3259 |           0.2507 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0049 |           5.4034 |           0.2504 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0150 |           5.2178 |           0.2506 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0126 |           5.1717 |           0.2504 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0119 |           5.3257 |           0.2503 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0167 |           5.1167 |           0.2502 |
[32m[20221213 15:07:42 @agent_ppo2.py:185][0m |          -0.0159 |           5.0676 |           0.2502 |
[32m[20221213 15:07:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.70
[32m[20221213 15:07:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 159.92
[32m[20221213 15:07:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.67
[32m[20221213 15:07:42 @agent_ppo2.py:143][0m Total time:      14.79 min
[32m[20221213 15:07:42 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221213 15:07:42 @agent_ppo2.py:121][0m #------------------------ Iteration 647 --------------------------#
[32m[20221213 15:07:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |           0.0020 |           5.7663 |           0.2549 |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |           0.0033 |           5.7633 |           0.2546 |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |          -0.0089 |           5.2986 |           0.2542 |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |          -0.0118 |           5.1733 |           0.2541 |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |          -0.0148 |           5.1158 |           0.2542 |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |          -0.0010 |           5.5718 |           0.2542 |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |          -0.0022 |           5.2998 |           0.2538 |
[32m[20221213 15:07:43 @agent_ppo2.py:185][0m |          -0.0155 |           4.9109 |           0.2536 |
[32m[20221213 15:07:44 @agent_ppo2.py:185][0m |          -0.0142 |           4.8212 |           0.2538 |
[32m[20221213 15:07:44 @agent_ppo2.py:185][0m |          -0.0173 |           4.7722 |           0.2538 |
[32m[20221213 15:07:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.53
[32m[20221213 15:07:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.10
[32m[20221213 15:07:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 162.79
[32m[20221213 15:07:44 @agent_ppo2.py:143][0m Total time:      14.81 min
[32m[20221213 15:07:44 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221213 15:07:44 @agent_ppo2.py:121][0m #------------------------ Iteration 648 --------------------------#
[32m[20221213 15:07:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:44 @agent_ppo2.py:185][0m |           0.0069 |           5.8392 |           0.2487 |
[32m[20221213 15:07:44 @agent_ppo2.py:185][0m |          -0.0109 |           5.4607 |           0.2482 |
[32m[20221213 15:07:44 @agent_ppo2.py:185][0m |          -0.0101 |           5.3754 |           0.2480 |
[32m[20221213 15:07:44 @agent_ppo2.py:185][0m |          -0.0137 |           5.3218 |           0.2480 |
[32m[20221213 15:07:45 @agent_ppo2.py:185][0m |          -0.0144 |           5.2643 |           0.2478 |
[32m[20221213 15:07:45 @agent_ppo2.py:185][0m |          -0.0146 |           5.2538 |           0.2476 |
[32m[20221213 15:07:45 @agent_ppo2.py:185][0m |          -0.0186 |           5.2278 |           0.2475 |
[32m[20221213 15:07:45 @agent_ppo2.py:185][0m |          -0.0172 |           5.2030 |           0.2474 |
[32m[20221213 15:07:45 @agent_ppo2.py:185][0m |          -0.0090 |           5.3380 |           0.2473 |
[32m[20221213 15:07:45 @agent_ppo2.py:185][0m |          -0.0169 |           5.2363 |           0.2473 |
[32m[20221213 15:07:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.67
[32m[20221213 15:07:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.79
[32m[20221213 15:07:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 118.69
[32m[20221213 15:07:45 @agent_ppo2.py:143][0m Total time:      14.83 min
[32m[20221213 15:07:45 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221213 15:07:45 @agent_ppo2.py:121][0m #------------------------ Iteration 649 --------------------------#
[32m[20221213 15:07:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |           0.0049 |           5.6905 |           0.2465 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0093 |           5.2636 |           0.2458 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0108 |           5.1920 |           0.2456 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0130 |           5.0986 |           0.2455 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0100 |           5.0942 |           0.2453 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0148 |           4.9979 |           0.2452 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0060 |           5.2770 |           0.2451 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0205 |           4.9934 |           0.2451 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0193 |           4.9139 |           0.2452 |
[32m[20221213 15:07:46 @agent_ppo2.py:185][0m |          -0.0204 |           4.8938 |           0.2453 |
[32m[20221213 15:07:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.54
[32m[20221213 15:07:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.79
[32m[20221213 15:07:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.63
[32m[20221213 15:07:46 @agent_ppo2.py:143][0m Total time:      14.85 min
[32m[20221213 15:07:46 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221213 15:07:46 @agent_ppo2.py:121][0m #------------------------ Iteration 650 --------------------------#
[32m[20221213 15:07:47 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:07:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |           0.0102 |           5.9778 |           0.2589 |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |          -0.0103 |           5.5411 |           0.2585 |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |          -0.0027 |           5.5143 |           0.2580 |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |          -0.0141 |           5.3976 |           0.2574 |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |          -0.0149 |           5.3516 |           0.2574 |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |          -0.0093 |           5.4411 |           0.2571 |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |          -0.0139 |           5.2531 |           0.2569 |
[32m[20221213 15:07:47 @agent_ppo2.py:185][0m |          -0.0160 |           5.2407 |           0.2568 |
[32m[20221213 15:07:48 @agent_ppo2.py:185][0m |          -0.0170 |           5.2113 |           0.2564 |
[32m[20221213 15:07:48 @agent_ppo2.py:185][0m |          -0.0178 |           5.1689 |           0.2563 |
[32m[20221213 15:07:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 116.88
[32m[20221213 15:07:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 132.10
[32m[20221213 15:07:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.43
[32m[20221213 15:07:48 @agent_ppo2.py:143][0m Total time:      14.88 min
[32m[20221213 15:07:48 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221213 15:07:48 @agent_ppo2.py:121][0m #------------------------ Iteration 651 --------------------------#
[32m[20221213 15:07:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:48 @agent_ppo2.py:185][0m |          -0.0047 |           5.1132 |           0.2492 |
[32m[20221213 15:07:48 @agent_ppo2.py:185][0m |          -0.0086 |           4.8155 |           0.2490 |
[32m[20221213 15:07:48 @agent_ppo2.py:185][0m |           0.0006 |           4.9967 |           0.2487 |
[32m[20221213 15:07:48 @agent_ppo2.py:185][0m |          -0.0088 |           4.6422 |           0.2484 |
[32m[20221213 15:07:49 @agent_ppo2.py:185][0m |          -0.0157 |           4.5808 |           0.2480 |
[32m[20221213 15:07:49 @agent_ppo2.py:185][0m |          -0.0114 |           4.5252 |           0.2480 |
[32m[20221213 15:07:49 @agent_ppo2.py:185][0m |          -0.0152 |           4.4944 |           0.2479 |
[32m[20221213 15:07:49 @agent_ppo2.py:185][0m |          -0.0172 |           4.4784 |           0.2477 |
[32m[20221213 15:07:49 @agent_ppo2.py:185][0m |          -0.0131 |           4.4478 |           0.2476 |
[32m[20221213 15:07:49 @agent_ppo2.py:185][0m |          -0.0035 |           4.4599 |           0.2474 |
[32m[20221213 15:07:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.54
[32m[20221213 15:07:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.64
[32m[20221213 15:07:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 152.03
[32m[20221213 15:07:49 @agent_ppo2.py:143][0m Total time:      14.90 min
[32m[20221213 15:07:49 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221213 15:07:49 @agent_ppo2.py:121][0m #------------------------ Iteration 652 --------------------------#
[32m[20221213 15:07:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |           0.0003 |           5.4913 |           0.2407 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0093 |           5.1940 |           0.2404 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0109 |           5.0965 |           0.2404 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0020 |           5.6712 |           0.2403 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0084 |           5.0618 |           0.2399 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0140 |           4.9222 |           0.2400 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0044 |           4.9798 |           0.2399 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0077 |           5.0502 |           0.2398 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0169 |           4.8526 |           0.2397 |
[32m[20221213 15:07:50 @agent_ppo2.py:185][0m |          -0.0169 |           4.8142 |           0.2395 |
[32m[20221213 15:07:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.59
[32m[20221213 15:07:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.32
[32m[20221213 15:07:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.05
[32m[20221213 15:07:50 @agent_ppo2.py:143][0m Total time:      14.92 min
[32m[20221213 15:07:50 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221213 15:07:50 @agent_ppo2.py:121][0m #------------------------ Iteration 653 --------------------------#
[32m[20221213 15:07:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |           0.0069 |           5.8606 |           0.2381 |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |          -0.0088 |           5.4045 |           0.2373 |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |          -0.0045 |           5.4542 |           0.2372 |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |          -0.0111 |           5.2628 |           0.2368 |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |          -0.0131 |           5.2221 |           0.2366 |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |          -0.0142 |           5.1913 |           0.2362 |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |          -0.0182 |           5.1657 |           0.2365 |
[32m[20221213 15:07:51 @agent_ppo2.py:185][0m |          -0.0178 |           5.1452 |           0.2361 |
[32m[20221213 15:07:52 @agent_ppo2.py:185][0m |          -0.0179 |           5.1034 |           0.2361 |
[32m[20221213 15:07:52 @agent_ppo2.py:185][0m |          -0.0185 |           5.0967 |           0.2362 |
[32m[20221213 15:07:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.93
[32m[20221213 15:07:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.80
[32m[20221213 15:07:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 152.23
[32m[20221213 15:07:52 @agent_ppo2.py:143][0m Total time:      14.94 min
[32m[20221213 15:07:52 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221213 15:07:52 @agent_ppo2.py:121][0m #------------------------ Iteration 654 --------------------------#
[32m[20221213 15:07:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:52 @agent_ppo2.py:185][0m |          -0.0016 |           5.6717 |           0.2367 |
[32m[20221213 15:07:52 @agent_ppo2.py:185][0m |          -0.0087 |           5.4705 |           0.2357 |
[32m[20221213 15:07:52 @agent_ppo2.py:185][0m |          -0.0110 |           5.3569 |           0.2355 |
[32m[20221213 15:07:52 @agent_ppo2.py:185][0m |          -0.0130 |           5.2842 |           0.2353 |
[32m[20221213 15:07:53 @agent_ppo2.py:185][0m |          -0.0129 |           5.2425 |           0.2351 |
[32m[20221213 15:07:53 @agent_ppo2.py:185][0m |          -0.0095 |           5.2131 |           0.2350 |
[32m[20221213 15:07:53 @agent_ppo2.py:185][0m |          -0.0154 |           5.1099 |           0.2349 |
[32m[20221213 15:07:53 @agent_ppo2.py:185][0m |          -0.0179 |           5.0712 |           0.2348 |
[32m[20221213 15:07:53 @agent_ppo2.py:185][0m |          -0.0082 |           5.3809 |           0.2345 |
[32m[20221213 15:07:53 @agent_ppo2.py:185][0m |          -0.0208 |           5.0011 |           0.2346 |
[32m[20221213 15:07:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 129.42
[32m[20221213 15:07:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 135.89
[32m[20221213 15:07:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 159.86
[32m[20221213 15:07:53 @agent_ppo2.py:143][0m Total time:      14.97 min
[32m[20221213 15:07:53 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221213 15:07:53 @agent_ppo2.py:121][0m #------------------------ Iteration 655 --------------------------#
[32m[20221213 15:07:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |           0.0107 |           5.8720 |           0.2329 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0081 |           5.3590 |           0.2322 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0102 |           5.2557 |           0.2321 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0106 |           5.2161 |           0.2319 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0120 |           5.1665 |           0.2318 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0146 |           5.1254 |           0.2317 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0152 |           5.0896 |           0.2317 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0168 |           5.0692 |           0.2316 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0182 |           5.0428 |           0.2315 |
[32m[20221213 15:07:54 @agent_ppo2.py:185][0m |          -0.0123 |           5.1408 |           0.2315 |
[32m[20221213 15:07:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.24
[32m[20221213 15:07:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.06
[32m[20221213 15:07:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 118.88
[32m[20221213 15:07:55 @agent_ppo2.py:143][0m Total time:      14.99 min
[32m[20221213 15:07:55 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221213 15:07:55 @agent_ppo2.py:121][0m #------------------------ Iteration 656 --------------------------#
[32m[20221213 15:07:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0018 |           5.2829 |           0.2361 |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0110 |           5.1247 |           0.2359 |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0139 |           5.0376 |           0.2358 |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0127 |           4.9751 |           0.2357 |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0088 |           5.0894 |           0.2358 |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0084 |           5.1285 |           0.2357 |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0173 |           4.8458 |           0.2357 |
[32m[20221213 15:07:55 @agent_ppo2.py:185][0m |          -0.0190 |           4.8296 |           0.2359 |
[32m[20221213 15:07:56 @agent_ppo2.py:185][0m |          -0.0171 |           4.7656 |           0.2359 |
[32m[20221213 15:07:56 @agent_ppo2.py:185][0m |          -0.0209 |           4.7367 |           0.2360 |
[32m[20221213 15:07:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 123.53
[32m[20221213 15:07:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.79
[32m[20221213 15:07:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.24
[32m[20221213 15:07:56 @agent_ppo2.py:143][0m Total time:      15.01 min
[32m[20221213 15:07:56 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221213 15:07:56 @agent_ppo2.py:121][0m #------------------------ Iteration 657 --------------------------#
[32m[20221213 15:07:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:07:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:56 @agent_ppo2.py:185][0m |           0.0028 |           5.4387 |           0.2368 |
[32m[20221213 15:07:56 @agent_ppo2.py:185][0m |          -0.0018 |           5.3004 |           0.2364 |
[32m[20221213 15:07:56 @agent_ppo2.py:185][0m |          -0.0083 |           5.1976 |           0.2361 |
[32m[20221213 15:07:56 @agent_ppo2.py:185][0m |          -0.0080 |           5.1238 |           0.2360 |
[32m[20221213 15:07:57 @agent_ppo2.py:185][0m |          -0.0137 |           5.0824 |           0.2358 |
[32m[20221213 15:07:57 @agent_ppo2.py:185][0m |          -0.0163 |           5.0492 |           0.2357 |
[32m[20221213 15:07:57 @agent_ppo2.py:185][0m |          -0.0163 |           4.9960 |           0.2354 |
[32m[20221213 15:07:57 @agent_ppo2.py:185][0m |          -0.0152 |           4.9512 |           0.2356 |
[32m[20221213 15:07:57 @agent_ppo2.py:185][0m |          -0.0173 |           4.9056 |           0.2352 |
[32m[20221213 15:07:57 @agent_ppo2.py:185][0m |          -0.0173 |           4.8924 |           0.2353 |
[32m[20221213 15:07:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:07:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.55
[32m[20221213 15:07:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 126.67
[32m[20221213 15:07:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.60
[32m[20221213 15:07:57 @agent_ppo2.py:143][0m Total time:      15.03 min
[32m[20221213 15:07:57 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221213 15:07:57 @agent_ppo2.py:121][0m #------------------------ Iteration 658 --------------------------#
[32m[20221213 15:07:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |           0.0027 |           5.5740 |           0.2379 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0061 |           5.3062 |           0.2374 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0088 |           5.1542 |           0.2372 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0047 |           5.3021 |           0.2373 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0061 |           5.2074 |           0.2369 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0121 |           4.8793 |           0.2367 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0039 |           5.0959 |           0.2366 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0047 |           5.2158 |           0.2365 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0077 |           4.9265 |           0.2366 |
[32m[20221213 15:07:58 @agent_ppo2.py:185][0m |          -0.0196 |           4.6283 |           0.2365 |
[32m[20221213 15:07:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:07:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.10
[32m[20221213 15:07:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.01
[32m[20221213 15:07:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.95
[32m[20221213 15:07:59 @agent_ppo2.py:143][0m Total time:      15.06 min
[32m[20221213 15:07:59 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221213 15:07:59 @agent_ppo2.py:121][0m #------------------------ Iteration 659 --------------------------#
[32m[20221213 15:07:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:07:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0020 |           4.9678 |           0.2388 |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0068 |           4.4776 |           0.2386 |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0086 |           4.2270 |           0.2386 |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0088 |           4.0672 |           0.2384 |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0079 |           4.0894 |           0.2382 |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0130 |           3.9104 |           0.2382 |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0188 |           3.8426 |           0.2383 |
[32m[20221213 15:07:59 @agent_ppo2.py:185][0m |          -0.0106 |           3.9979 |           0.2383 |
[32m[20221213 15:08:00 @agent_ppo2.py:185][0m |          -0.0188 |           3.7477 |           0.2380 |
[32m[20221213 15:08:00 @agent_ppo2.py:185][0m |          -0.0112 |           3.8645 |           0.2383 |
[32m[20221213 15:08:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.18
[32m[20221213 15:08:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.93
[32m[20221213 15:08:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.89
[32m[20221213 15:08:00 @agent_ppo2.py:143][0m Total time:      15.08 min
[32m[20221213 15:08:00 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221213 15:08:00 @agent_ppo2.py:121][0m #------------------------ Iteration 660 --------------------------#
[32m[20221213 15:08:00 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:08:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:00 @agent_ppo2.py:185][0m |           0.0008 |           6.9502 |           0.2298 |
[32m[20221213 15:08:00 @agent_ppo2.py:185][0m |          -0.0100 |           6.2086 |           0.2295 |
[32m[20221213 15:08:00 @agent_ppo2.py:185][0m |          -0.0118 |           6.0346 |           0.2294 |
[32m[20221213 15:08:00 @agent_ppo2.py:185][0m |          -0.0083 |           5.9658 |           0.2293 |
[32m[20221213 15:08:01 @agent_ppo2.py:185][0m |          -0.0189 |           5.7641 |           0.2294 |
[32m[20221213 15:08:01 @agent_ppo2.py:185][0m |          -0.0115 |           5.7105 |           0.2295 |
[32m[20221213 15:08:01 @agent_ppo2.py:185][0m |          -0.0166 |           5.6137 |           0.2294 |
[32m[20221213 15:08:01 @agent_ppo2.py:185][0m |          -0.0206 |           5.5763 |           0.2293 |
[32m[20221213 15:08:01 @agent_ppo2.py:185][0m |          -0.0160 |           5.5810 |           0.2295 |
[32m[20221213 15:08:01 @agent_ppo2.py:185][0m |          -0.0195 |           5.5159 |           0.2294 |
[32m[20221213 15:08:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.34
[32m[20221213 15:08:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 139.29
[32m[20221213 15:08:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.94
[32m[20221213 15:08:01 @agent_ppo2.py:143][0m Total time:      15.10 min
[32m[20221213 15:08:01 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221213 15:08:01 @agent_ppo2.py:121][0m #------------------------ Iteration 661 --------------------------#
[32m[20221213 15:08:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |           0.0059 |           6.1144 |           0.2372 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0093 |           5.7486 |           0.2373 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0061 |           5.5951 |           0.2374 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0067 |           5.4976 |           0.2373 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0156 |           5.4115 |           0.2374 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0129 |           5.3451 |           0.2376 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0137 |           5.2823 |           0.2376 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0155 |           5.2412 |           0.2376 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0168 |           5.1996 |           0.2376 |
[32m[20221213 15:08:02 @agent_ppo2.py:185][0m |          -0.0158 |           5.1974 |           0.2378 |
[32m[20221213 15:08:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.10
[32m[20221213 15:08:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.29
[32m[20221213 15:08:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 125.69
[32m[20221213 15:08:03 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221213 15:08:03 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221213 15:08:03 @agent_ppo2.py:121][0m #------------------------ Iteration 662 --------------------------#
[32m[20221213 15:08:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |           0.0019 |           6.0760 |           0.2391 |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |          -0.0096 |           5.8050 |           0.2385 |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |          -0.0131 |           5.6811 |           0.2383 |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |          -0.0111 |           5.5510 |           0.2382 |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |          -0.0185 |           5.5208 |           0.2380 |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |          -0.0132 |           5.5091 |           0.2384 |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |          -0.0170 |           5.3846 |           0.2381 |
[32m[20221213 15:08:03 @agent_ppo2.py:185][0m |          -0.0159 |           5.3381 |           0.2382 |
[32m[20221213 15:08:04 @agent_ppo2.py:185][0m |          -0.0209 |           5.3226 |           0.2382 |
[32m[20221213 15:08:04 @agent_ppo2.py:185][0m |          -0.0181 |           5.2703 |           0.2379 |
[32m[20221213 15:08:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.35
[32m[20221213 15:08:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.87
[32m[20221213 15:08:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.31
[32m[20221213 15:08:04 @agent_ppo2.py:143][0m Total time:      15.14 min
[32m[20221213 15:08:04 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221213 15:08:04 @agent_ppo2.py:121][0m #------------------------ Iteration 663 --------------------------#
[32m[20221213 15:08:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:04 @agent_ppo2.py:185][0m |           0.0138 |           5.5278 |           0.2335 |
[32m[20221213 15:08:04 @agent_ppo2.py:185][0m |          -0.0038 |           4.9771 |           0.2335 |
[32m[20221213 15:08:04 @agent_ppo2.py:185][0m |          -0.0064 |           4.9176 |           0.2335 |
[32m[20221213 15:08:04 @agent_ppo2.py:185][0m |          -0.0105 |           4.8429 |           0.2335 |
[32m[20221213 15:08:05 @agent_ppo2.py:185][0m |          -0.0109 |           4.7875 |           0.2336 |
[32m[20221213 15:08:05 @agent_ppo2.py:185][0m |          -0.0106 |           4.7613 |           0.2336 |
[32m[20221213 15:08:05 @agent_ppo2.py:185][0m |          -0.0046 |           4.7845 |           0.2336 |
[32m[20221213 15:08:05 @agent_ppo2.py:185][0m |          -0.0132 |           4.6874 |           0.2337 |
[32m[20221213 15:08:05 @agent_ppo2.py:185][0m |          -0.0131 |           4.6581 |           0.2336 |
[32m[20221213 15:08:05 @agent_ppo2.py:185][0m |          -0.0172 |           4.6616 |           0.2337 |
[32m[20221213 15:08:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.58
[32m[20221213 15:08:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 155.08
[32m[20221213 15:08:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 120.11
[32m[20221213 15:08:05 @agent_ppo2.py:143][0m Total time:      15.17 min
[32m[20221213 15:08:05 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221213 15:08:05 @agent_ppo2.py:121][0m #------------------------ Iteration 664 --------------------------#
[32m[20221213 15:08:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0019 |           5.3394 |           0.2452 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0070 |           5.0734 |           0.2449 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0098 |           5.0039 |           0.2448 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0128 |           4.9588 |           0.2447 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0180 |           4.9165 |           0.2448 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0049 |           5.1365 |           0.2447 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0204 |           4.8718 |           0.2449 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0092 |           5.2488 |           0.2447 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0035 |           5.1218 |           0.2449 |
[32m[20221213 15:08:06 @agent_ppo2.py:185][0m |          -0.0191 |           4.7919 |           0.2448 |
[32m[20221213 15:08:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.64
[32m[20221213 15:08:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.87
[32m[20221213 15:08:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 128.95
[32m[20221213 15:08:07 @agent_ppo2.py:143][0m Total time:      15.19 min
[32m[20221213 15:08:07 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221213 15:08:07 @agent_ppo2.py:121][0m #------------------------ Iteration 665 --------------------------#
[32m[20221213 15:08:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:07 @agent_ppo2.py:185][0m |           0.0012 |           5.3594 |           0.2343 |
[32m[20221213 15:08:07 @agent_ppo2.py:185][0m |          -0.0027 |           5.1809 |           0.2341 |
[32m[20221213 15:08:07 @agent_ppo2.py:185][0m |           0.0012 |           5.2660 |           0.2341 |
[32m[20221213 15:08:07 @agent_ppo2.py:185][0m |          -0.0026 |           5.4065 |           0.2340 |
[32m[20221213 15:08:07 @agent_ppo2.py:185][0m |          -0.0111 |           4.9265 |           0.2339 |
[32m[20221213 15:08:07 @agent_ppo2.py:185][0m |          -0.0037 |           5.1695 |           0.2338 |
[32m[20221213 15:08:07 @agent_ppo2.py:185][0m |          -0.0137 |           4.8511 |           0.2338 |
[32m[20221213 15:08:08 @agent_ppo2.py:185][0m |          -0.0156 |           4.7900 |           0.2338 |
[32m[20221213 15:08:08 @agent_ppo2.py:185][0m |          -0.0158 |           4.7783 |           0.2338 |
[32m[20221213 15:08:08 @agent_ppo2.py:185][0m |          -0.0101 |           4.8119 |           0.2337 |
[32m[20221213 15:08:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.03
[32m[20221213 15:08:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.81
[32m[20221213 15:08:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.09
[32m[20221213 15:08:08 @agent_ppo2.py:143][0m Total time:      15.21 min
[32m[20221213 15:08:08 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221213 15:08:08 @agent_ppo2.py:121][0m #------------------------ Iteration 666 --------------------------#
[32m[20221213 15:08:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:08 @agent_ppo2.py:185][0m |           0.0063 |           5.8738 |           0.2417 |
[32m[20221213 15:08:08 @agent_ppo2.py:185][0m |           0.0037 |           5.7952 |           0.2413 |
[32m[20221213 15:08:08 @agent_ppo2.py:185][0m |          -0.0053 |           5.3674 |           0.2410 |
[32m[20221213 15:08:09 @agent_ppo2.py:185][0m |          -0.0153 |           5.1548 |           0.2408 |
[32m[20221213 15:08:09 @agent_ppo2.py:185][0m |          -0.0208 |           5.0962 |           0.2407 |
[32m[20221213 15:08:09 @agent_ppo2.py:185][0m |          -0.0190 |           5.0385 |           0.2407 |
[32m[20221213 15:08:09 @agent_ppo2.py:185][0m |          -0.0240 |           5.0182 |           0.2405 |
[32m[20221213 15:08:09 @agent_ppo2.py:185][0m |          -0.0174 |           4.9839 |           0.2405 |
[32m[20221213 15:08:09 @agent_ppo2.py:185][0m |          -0.0188 |           4.9510 |           0.2405 |
[32m[20221213 15:08:09 @agent_ppo2.py:185][0m |          -0.0181 |           4.9869 |           0.2404 |
[32m[20221213 15:08:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.67
[32m[20221213 15:08:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.13
[32m[20221213 15:08:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.63
[32m[20221213 15:08:09 @agent_ppo2.py:143][0m Total time:      15.23 min
[32m[20221213 15:08:09 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221213 15:08:09 @agent_ppo2.py:121][0m #------------------------ Iteration 667 --------------------------#
[32m[20221213 15:08:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0005 |           5.5841 |           0.2396 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0104 |           5.2806 |           0.2399 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0132 |           5.0922 |           0.2399 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0140 |           5.0125 |           0.2398 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0037 |           5.0667 |           0.2399 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0165 |           4.7987 |           0.2397 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0164 |           4.6860 |           0.2396 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |           0.0063 |           5.3410 |           0.2398 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0186 |           4.5816 |           0.2399 |
[32m[20221213 15:08:10 @agent_ppo2.py:185][0m |          -0.0177 |           4.5046 |           0.2400 |
[32m[20221213 15:08:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 142.74
[32m[20221213 15:08:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.93
[32m[20221213 15:08:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.44
[32m[20221213 15:08:11 @agent_ppo2.py:143][0m Total time:      15.26 min
[32m[20221213 15:08:11 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221213 15:08:11 @agent_ppo2.py:121][0m #------------------------ Iteration 668 --------------------------#
[32m[20221213 15:08:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:11 @agent_ppo2.py:185][0m |          -0.0018 |           5.7184 |           0.2346 |
[32m[20221213 15:08:11 @agent_ppo2.py:185][0m |          -0.0080 |           5.3875 |           0.2343 |
[32m[20221213 15:08:11 @agent_ppo2.py:185][0m |          -0.0071 |           5.2566 |           0.2344 |
[32m[20221213 15:08:11 @agent_ppo2.py:185][0m |          -0.0104 |           5.1810 |           0.2345 |
[32m[20221213 15:08:11 @agent_ppo2.py:185][0m |          -0.0115 |           5.1288 |           0.2342 |
[32m[20221213 15:08:11 @agent_ppo2.py:185][0m |          -0.0120 |           5.0636 |           0.2341 |
[32m[20221213 15:08:11 @agent_ppo2.py:185][0m |          -0.0145 |           5.0422 |           0.2342 |
[32m[20221213 15:08:12 @agent_ppo2.py:185][0m |          -0.0119 |           5.0091 |           0.2342 |
[32m[20221213 15:08:12 @agent_ppo2.py:185][0m |          -0.0183 |           4.9725 |           0.2340 |
[32m[20221213 15:08:12 @agent_ppo2.py:185][0m |          -0.0154 |           4.9394 |           0.2341 |
[32m[20221213 15:08:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.27
[32m[20221213 15:08:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 134.37
[32m[20221213 15:08:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.67
[32m[20221213 15:08:12 @agent_ppo2.py:143][0m Total time:      15.28 min
[32m[20221213 15:08:12 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221213 15:08:12 @agent_ppo2.py:121][0m #------------------------ Iteration 669 --------------------------#
[32m[20221213 15:08:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:12 @agent_ppo2.py:185][0m |          -0.0042 |           5.3071 |           0.2389 |
[32m[20221213 15:08:12 @agent_ppo2.py:185][0m |          -0.0078 |           5.1875 |           0.2387 |
[32m[20221213 15:08:12 @agent_ppo2.py:185][0m |          -0.0091 |           5.1569 |           0.2386 |
[32m[20221213 15:08:13 @agent_ppo2.py:185][0m |          -0.0040 |           5.4587 |           0.2384 |
[32m[20221213 15:08:13 @agent_ppo2.py:185][0m |          -0.0050 |           5.6963 |           0.2382 |
[32m[20221213 15:08:13 @agent_ppo2.py:185][0m |          -0.0151 |           5.0381 |           0.2378 |
[32m[20221213 15:08:13 @agent_ppo2.py:185][0m |          -0.0157 |           4.9857 |           0.2378 |
[32m[20221213 15:08:13 @agent_ppo2.py:185][0m |          -0.0178 |           4.9743 |           0.2378 |
[32m[20221213 15:08:13 @agent_ppo2.py:185][0m |          -0.0128 |           4.9921 |           0.2377 |
[32m[20221213 15:08:13 @agent_ppo2.py:185][0m |          -0.0153 |           4.9600 |           0.2378 |
[32m[20221213 15:08:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:08:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.80
[32m[20221213 15:08:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.54
[32m[20221213 15:08:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 152.60
[32m[20221213 15:08:13 @agent_ppo2.py:143][0m Total time:      15.30 min
[32m[20221213 15:08:13 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221213 15:08:13 @agent_ppo2.py:121][0m #------------------------ Iteration 670 --------------------------#
[32m[20221213 15:08:13 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:08:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0023 |           5.3087 |           0.2406 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0041 |           5.2565 |           0.2402 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0112 |           5.1133 |           0.2401 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0147 |           5.0584 |           0.2401 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0142 |           5.0520 |           0.2400 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0170 |           5.0187 |           0.2402 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0109 |           5.0669 |           0.2401 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0176 |           4.9803 |           0.2401 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0184 |           4.9458 |           0.2401 |
[32m[20221213 15:08:14 @agent_ppo2.py:185][0m |          -0.0121 |           4.9890 |           0.2400 |
[32m[20221213 15:08:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.02
[32m[20221213 15:08:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 135.65
[32m[20221213 15:08:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 140.60
[32m[20221213 15:08:15 @agent_ppo2.py:143][0m Total time:      15.32 min
[32m[20221213 15:08:15 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221213 15:08:15 @agent_ppo2.py:121][0m #------------------------ Iteration 671 --------------------------#
[32m[20221213 15:08:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:15 @agent_ppo2.py:185][0m |           0.0017 |           5.3181 |           0.2377 |
[32m[20221213 15:08:15 @agent_ppo2.py:185][0m |          -0.0066 |           5.2010 |           0.2374 |
[32m[20221213 15:08:15 @agent_ppo2.py:185][0m |          -0.0092 |           5.1373 |           0.2373 |
[32m[20221213 15:08:15 @agent_ppo2.py:185][0m |          -0.0134 |           5.0934 |           0.2374 |
[32m[20221213 15:08:15 @agent_ppo2.py:185][0m |          -0.0085 |           5.2612 |           0.2372 |
[32m[20221213 15:08:15 @agent_ppo2.py:185][0m |          -0.0125 |           5.1063 |           0.2374 |
[32m[20221213 15:08:16 @agent_ppo2.py:185][0m |          -0.0162 |           4.9830 |           0.2375 |
[32m[20221213 15:08:16 @agent_ppo2.py:185][0m |          -0.0191 |           4.9577 |           0.2376 |
[32m[20221213 15:08:16 @agent_ppo2.py:185][0m |          -0.0112 |           5.2359 |           0.2376 |
[32m[20221213 15:08:16 @agent_ppo2.py:185][0m |          -0.0176 |           4.9258 |           0.2376 |
[32m[20221213 15:08:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.41
[32m[20221213 15:08:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.14
[32m[20221213 15:08:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.03
[32m[20221213 15:08:16 @agent_ppo2.py:143][0m Total time:      15.35 min
[32m[20221213 15:08:16 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221213 15:08:16 @agent_ppo2.py:121][0m #------------------------ Iteration 672 --------------------------#
[32m[20221213 15:08:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:16 @agent_ppo2.py:185][0m |          -0.0006 |           5.6993 |           0.2460 |
[32m[20221213 15:08:16 @agent_ppo2.py:185][0m |          -0.0019 |           5.5404 |           0.2456 |
[32m[20221213 15:08:16 @agent_ppo2.py:185][0m |          -0.0080 |           5.4683 |           0.2457 |
[32m[20221213 15:08:17 @agent_ppo2.py:185][0m |          -0.0084 |           5.4129 |           0.2457 |
[32m[20221213 15:08:17 @agent_ppo2.py:185][0m |          -0.0107 |           5.3982 |           0.2456 |
[32m[20221213 15:08:17 @agent_ppo2.py:185][0m |          -0.0079 |           5.6181 |           0.2457 |
[32m[20221213 15:08:17 @agent_ppo2.py:185][0m |          -0.0104 |           5.3404 |           0.2455 |
[32m[20221213 15:08:17 @agent_ppo2.py:185][0m |          -0.0084 |           5.4721 |           0.2456 |
[32m[20221213 15:08:17 @agent_ppo2.py:185][0m |          -0.0104 |           5.4161 |           0.2454 |
[32m[20221213 15:08:17 @agent_ppo2.py:185][0m |          -0.0147 |           5.2922 |           0.2454 |
[32m[20221213 15:08:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.39
[32m[20221213 15:08:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.32
[32m[20221213 15:08:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.48
[32m[20221213 15:08:17 @agent_ppo2.py:143][0m Total time:      15.37 min
[32m[20221213 15:08:17 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221213 15:08:17 @agent_ppo2.py:121][0m #------------------------ Iteration 673 --------------------------#
[32m[20221213 15:08:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |           0.0013 |           5.3742 |           0.2463 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0086 |           5.2476 |           0.2459 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0082 |           5.1871 |           0.2456 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0067 |           5.2440 |           0.2459 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0065 |           5.2349 |           0.2456 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |           0.0010 |           5.6072 |           0.2455 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0093 |           5.0700 |           0.2460 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0167 |           5.0224 |           0.2457 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0072 |           5.0802 |           0.2459 |
[32m[20221213 15:08:18 @agent_ppo2.py:185][0m |          -0.0146 |           4.9744 |           0.2455 |
[32m[20221213 15:08:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.87
[32m[20221213 15:08:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.32
[32m[20221213 15:08:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.87
[32m[20221213 15:08:19 @agent_ppo2.py:143][0m Total time:      15.39 min
[32m[20221213 15:08:19 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221213 15:08:19 @agent_ppo2.py:121][0m #------------------------ Iteration 674 --------------------------#
[32m[20221213 15:08:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:19 @agent_ppo2.py:185][0m |           0.0213 |           6.3194 |           0.2413 |
[32m[20221213 15:08:19 @agent_ppo2.py:185][0m |          -0.0045 |           4.7657 |           0.2404 |
[32m[20221213 15:08:19 @agent_ppo2.py:185][0m |          -0.0029 |           4.9258 |           0.2404 |
[32m[20221213 15:08:19 @agent_ppo2.py:185][0m |          -0.0123 |           4.4649 |           0.2401 |
[32m[20221213 15:08:19 @agent_ppo2.py:185][0m |          -0.0145 |           4.3971 |           0.2399 |
[32m[20221213 15:08:19 @agent_ppo2.py:185][0m |          -0.0130 |           4.3561 |           0.2398 |
[32m[20221213 15:08:20 @agent_ppo2.py:185][0m |          -0.0157 |           4.3188 |           0.2397 |
[32m[20221213 15:08:20 @agent_ppo2.py:185][0m |          -0.0168 |           4.2925 |           0.2396 |
[32m[20221213 15:08:20 @agent_ppo2.py:185][0m |          -0.0107 |           4.4721 |           0.2394 |
[32m[20221213 15:08:20 @agent_ppo2.py:185][0m |          -0.0166 |           4.2351 |           0.2393 |
[32m[20221213 15:08:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.26
[32m[20221213 15:08:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.06
[32m[20221213 15:08:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.18
[32m[20221213 15:08:20 @agent_ppo2.py:143][0m Total time:      15.41 min
[32m[20221213 15:08:20 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221213 15:08:20 @agent_ppo2.py:121][0m #------------------------ Iteration 675 --------------------------#
[32m[20221213 15:08:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:20 @agent_ppo2.py:185][0m |           0.0013 |           5.3071 |           0.2424 |
[32m[20221213 15:08:20 @agent_ppo2.py:185][0m |           0.0005 |           5.3574 |           0.2422 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0074 |           5.0729 |           0.2420 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0125 |           4.9939 |           0.2421 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0062 |           4.9796 |           0.2421 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0057 |           5.3079 |           0.2421 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0130 |           4.8358 |           0.2421 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0146 |           4.7741 |           0.2422 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0162 |           4.7435 |           0.2422 |
[32m[20221213 15:08:21 @agent_ppo2.py:185][0m |          -0.0147 |           4.6977 |           0.2424 |
[32m[20221213 15:08:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.78
[32m[20221213 15:08:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.52
[32m[20221213 15:08:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.91
[32m[20221213 15:08:21 @agent_ppo2.py:143][0m Total time:      15.44 min
[32m[20221213 15:08:21 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221213 15:08:21 @agent_ppo2.py:121][0m #------------------------ Iteration 676 --------------------------#
[32m[20221213 15:08:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |           0.0027 |           5.5992 |           0.2368 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0049 |           5.3756 |           0.2364 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0069 |           5.3098 |           0.2362 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0067 |           5.2907 |           0.2361 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0077 |           5.2287 |           0.2357 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0107 |           5.2015 |           0.2356 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0163 |           5.1720 |           0.2355 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0158 |           5.1648 |           0.2353 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0111 |           5.1362 |           0.2352 |
[32m[20221213 15:08:22 @agent_ppo2.py:185][0m |          -0.0151 |           5.1467 |           0.2350 |
[32m[20221213 15:08:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.13
[32m[20221213 15:08:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.25
[32m[20221213 15:08:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.48
[32m[20221213 15:08:23 @agent_ppo2.py:143][0m Total time:      15.46 min
[32m[20221213 15:08:23 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221213 15:08:23 @agent_ppo2.py:121][0m #------------------------ Iteration 677 --------------------------#
[32m[20221213 15:08:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:23 @agent_ppo2.py:185][0m |          -0.0046 |           5.1250 |           0.2385 |
[32m[20221213 15:08:23 @agent_ppo2.py:185][0m |          -0.0077 |           4.8710 |           0.2385 |
[32m[20221213 15:08:23 @agent_ppo2.py:185][0m |          -0.0111 |           4.7014 |           0.2384 |
[32m[20221213 15:08:23 @agent_ppo2.py:185][0m |          -0.0108 |           4.5403 |           0.2385 |
[32m[20221213 15:08:23 @agent_ppo2.py:185][0m |          -0.0126 |           4.4290 |           0.2385 |
[32m[20221213 15:08:23 @agent_ppo2.py:185][0m |          -0.0158 |           4.3063 |           0.2388 |
[32m[20221213 15:08:24 @agent_ppo2.py:185][0m |          -0.0167 |           4.2187 |           0.2391 |
[32m[20221213 15:08:24 @agent_ppo2.py:185][0m |          -0.0149 |           4.1499 |           0.2393 |
[32m[20221213 15:08:24 @agent_ppo2.py:185][0m |          -0.0175 |           4.0737 |           0.2393 |
[32m[20221213 15:08:24 @agent_ppo2.py:185][0m |          -0.0141 |           4.1016 |           0.2395 |
[32m[20221213 15:08:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 121.50
[32m[20221213 15:08:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.67
[32m[20221213 15:08:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.74
[32m[20221213 15:08:24 @agent_ppo2.py:143][0m Total time:      15.48 min
[32m[20221213 15:08:24 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221213 15:08:24 @agent_ppo2.py:121][0m #------------------------ Iteration 678 --------------------------#
[32m[20221213 15:08:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:24 @agent_ppo2.py:185][0m |          -0.0018 |           5.1346 |           0.2406 |
[32m[20221213 15:08:24 @agent_ppo2.py:185][0m |          -0.0090 |           4.9728 |           0.2406 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0125 |           4.8854 |           0.2405 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0110 |           4.8270 |           0.2404 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0111 |           4.7730 |           0.2405 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0140 |           4.7264 |           0.2404 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0172 |           4.6971 |           0.2404 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0169 |           4.6812 |           0.2405 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0143 |           4.7015 |           0.2405 |
[32m[20221213 15:08:25 @agent_ppo2.py:185][0m |          -0.0196 |           4.6216 |           0.2403 |
[32m[20221213 15:08:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 128.42
[32m[20221213 15:08:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.58
[32m[20221213 15:08:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.39
[32m[20221213 15:08:25 @agent_ppo2.py:143][0m Total time:      15.50 min
[32m[20221213 15:08:25 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221213 15:08:25 @agent_ppo2.py:121][0m #------------------------ Iteration 679 --------------------------#
[32m[20221213 15:08:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0019 |           5.4099 |           0.2545 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0065 |           5.3165 |           0.2542 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0012 |           5.4223 |           0.2538 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0143 |           5.1635 |           0.2535 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0158 |           5.0938 |           0.2536 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0162 |           5.0656 |           0.2534 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0185 |           5.0385 |           0.2531 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0096 |           5.5343 |           0.2531 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0146 |           5.0418 |           0.2528 |
[32m[20221213 15:08:26 @agent_ppo2.py:185][0m |          -0.0096 |           5.2385 |           0.2528 |
[32m[20221213 15:08:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.41
[32m[20221213 15:08:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.07
[32m[20221213 15:08:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 144.18
[32m[20221213 15:08:27 @agent_ppo2.py:143][0m Total time:      15.52 min
[32m[20221213 15:08:27 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221213 15:08:27 @agent_ppo2.py:121][0m #------------------------ Iteration 680 --------------------------#
[32m[20221213 15:08:27 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:08:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:27 @agent_ppo2.py:185][0m |          -0.0033 |           5.6124 |           0.2395 |
[32m[20221213 15:08:27 @agent_ppo2.py:185][0m |          -0.0142 |           5.3475 |           0.2388 |
[32m[20221213 15:08:27 @agent_ppo2.py:185][0m |          -0.0167 |           5.2446 |           0.2388 |
[32m[20221213 15:08:27 @agent_ppo2.py:185][0m |          -0.0173 |           5.1875 |           0.2386 |
[32m[20221213 15:08:27 @agent_ppo2.py:185][0m |          -0.0202 |           5.1261 |           0.2387 |
[32m[20221213 15:08:27 @agent_ppo2.py:185][0m |          -0.0205 |           5.0887 |           0.2387 |
[32m[20221213 15:08:28 @agent_ppo2.py:185][0m |          -0.0188 |           5.0385 |           0.2388 |
[32m[20221213 15:08:28 @agent_ppo2.py:185][0m |          -0.0201 |           5.0295 |           0.2388 |
[32m[20221213 15:08:28 @agent_ppo2.py:185][0m |          -0.0189 |           4.9750 |           0.2388 |
[32m[20221213 15:08:28 @agent_ppo2.py:185][0m |          -0.0192 |           4.9465 |           0.2388 |
[32m[20221213 15:08:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.43
[32m[20221213 15:08:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.44
[32m[20221213 15:08:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 146.97
[32m[20221213 15:08:28 @agent_ppo2.py:143][0m Total time:      15.55 min
[32m[20221213 15:08:28 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221213 15:08:28 @agent_ppo2.py:121][0m #------------------------ Iteration 681 --------------------------#
[32m[20221213 15:08:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:28 @agent_ppo2.py:185][0m |          -0.0010 |           5.6245 |           0.2449 |
[32m[20221213 15:08:28 @agent_ppo2.py:185][0m |          -0.0093 |           5.5410 |           0.2438 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0108 |           5.4846 |           0.2438 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0157 |           5.4850 |           0.2436 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0154 |           5.4227 |           0.2433 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0139 |           5.4236 |           0.2430 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0043 |           6.0652 |           0.2430 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0198 |           5.3851 |           0.2426 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0174 |           5.3476 |           0.2426 |
[32m[20221213 15:08:29 @agent_ppo2.py:185][0m |          -0.0182 |           5.3815 |           0.2426 |
[32m[20221213 15:08:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.55
[32m[20221213 15:08:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.53
[32m[20221213 15:08:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.24
[32m[20221213 15:08:29 @agent_ppo2.py:143][0m Total time:      15.57 min
[32m[20221213 15:08:29 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221213 15:08:29 @agent_ppo2.py:121][0m #------------------------ Iteration 682 --------------------------#
[32m[20221213 15:08:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0009 |           5.6706 |           0.2435 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0073 |           5.5032 |           0.2429 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0116 |           5.4329 |           0.2427 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0084 |           5.3835 |           0.2426 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0081 |           5.3677 |           0.2423 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0000 |           5.8962 |           0.2421 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0105 |           5.5706 |           0.2420 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0173 |           5.2007 |           0.2417 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0166 |           5.1672 |           0.2419 |
[32m[20221213 15:08:30 @agent_ppo2.py:185][0m |          -0.0159 |           5.1903 |           0.2418 |
[32m[20221213 15:08:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.03
[32m[20221213 15:08:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 146.45
[32m[20221213 15:08:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 152.19
[32m[20221213 15:08:31 @agent_ppo2.py:143][0m Total time:      15.59 min
[32m[20221213 15:08:31 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221213 15:08:31 @agent_ppo2.py:121][0m #------------------------ Iteration 683 --------------------------#
[32m[20221213 15:08:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:31 @agent_ppo2.py:185][0m |           0.0022 |           5.8514 |           0.2416 |
[32m[20221213 15:08:31 @agent_ppo2.py:185][0m |          -0.0009 |           5.6914 |           0.2412 |
[32m[20221213 15:08:31 @agent_ppo2.py:185][0m |          -0.0125 |           5.5276 |           0.2410 |
[32m[20221213 15:08:31 @agent_ppo2.py:185][0m |          -0.0102 |           5.4285 |           0.2409 |
[32m[20221213 15:08:31 @agent_ppo2.py:185][0m |          -0.0153 |           5.3762 |           0.2410 |
[32m[20221213 15:08:31 @agent_ppo2.py:185][0m |          -0.0083 |           5.4830 |           0.2411 |
[32m[20221213 15:08:32 @agent_ppo2.py:185][0m |          -0.0153 |           5.2856 |           0.2409 |
[32m[20221213 15:08:32 @agent_ppo2.py:185][0m |          -0.0092 |           5.5314 |           0.2410 |
[32m[20221213 15:08:32 @agent_ppo2.py:185][0m |          -0.0190 |           5.2329 |           0.2407 |
[32m[20221213 15:08:32 @agent_ppo2.py:185][0m |          -0.0193 |           5.1797 |           0.2408 |
[32m[20221213 15:08:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.47
[32m[20221213 15:08:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.84
[32m[20221213 15:08:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.77
[32m[20221213 15:08:32 @agent_ppo2.py:143][0m Total time:      15.61 min
[32m[20221213 15:08:32 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221213 15:08:32 @agent_ppo2.py:121][0m #------------------------ Iteration 684 --------------------------#
[32m[20221213 15:08:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:32 @agent_ppo2.py:185][0m |           0.0027 |           5.9945 |           0.2381 |
[32m[20221213 15:08:32 @agent_ppo2.py:185][0m |          -0.0112 |           5.8481 |           0.2380 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0114 |           5.7855 |           0.2378 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0059 |           5.8477 |           0.2377 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0135 |           5.6334 |           0.2377 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0132 |           5.5579 |           0.2377 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0147 |           5.5242 |           0.2376 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0086 |           5.7035 |           0.2379 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0193 |           5.4483 |           0.2377 |
[32m[20221213 15:08:33 @agent_ppo2.py:185][0m |          -0.0170 |           5.3557 |           0.2380 |
[32m[20221213 15:08:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.31
[32m[20221213 15:08:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.58
[32m[20221213 15:08:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.88
[32m[20221213 15:08:33 @agent_ppo2.py:143][0m Total time:      15.64 min
[32m[20221213 15:08:33 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221213 15:08:33 @agent_ppo2.py:121][0m #------------------------ Iteration 685 --------------------------#
[32m[20221213 15:08:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0000 |           5.9523 |           0.2410 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0015 |           5.8296 |           0.2408 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0081 |           5.7001 |           0.2406 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0096 |           5.6433 |           0.2407 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0086 |           5.5897 |           0.2406 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0110 |           5.5742 |           0.2405 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0066 |           5.7548 |           0.2406 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0171 |           5.4781 |           0.2407 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0199 |           5.4429 |           0.2405 |
[32m[20221213 15:08:34 @agent_ppo2.py:185][0m |          -0.0173 |           5.4385 |           0.2407 |
[32m[20221213 15:08:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.12
[32m[20221213 15:08:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.80
[32m[20221213 15:08:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.18
[32m[20221213 15:08:35 @agent_ppo2.py:143][0m Total time:      15.66 min
[32m[20221213 15:08:35 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221213 15:08:35 @agent_ppo2.py:121][0m #------------------------ Iteration 686 --------------------------#
[32m[20221213 15:08:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:35 @agent_ppo2.py:185][0m |           0.0024 |           5.9375 |           0.2406 |
[32m[20221213 15:08:35 @agent_ppo2.py:185][0m |          -0.0057 |           5.6447 |           0.2402 |
[32m[20221213 15:08:35 @agent_ppo2.py:185][0m |          -0.0021 |           5.7387 |           0.2405 |
[32m[20221213 15:08:35 @agent_ppo2.py:185][0m |          -0.0111 |           5.5677 |           0.2401 |
[32m[20221213 15:08:35 @agent_ppo2.py:185][0m |          -0.0001 |           5.9876 |           0.2401 |
[32m[20221213 15:08:35 @agent_ppo2.py:185][0m |          -0.0153 |           5.5181 |           0.2402 |
[32m[20221213 15:08:36 @agent_ppo2.py:185][0m |          -0.0092 |           5.6160 |           0.2402 |
[32m[20221213 15:08:36 @agent_ppo2.py:185][0m |          -0.0146 |           5.4589 |           0.2401 |
[32m[20221213 15:08:36 @agent_ppo2.py:185][0m |          -0.0157 |           5.4237 |           0.2403 |
[32m[20221213 15:08:36 @agent_ppo2.py:185][0m |          -0.0116 |           5.6904 |           0.2401 |
[32m[20221213 15:08:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.03
[32m[20221213 15:08:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 164.29
[32m[20221213 15:08:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.74
[32m[20221213 15:08:36 @agent_ppo2.py:143][0m Total time:      15.68 min
[32m[20221213 15:08:36 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221213 15:08:36 @agent_ppo2.py:121][0m #------------------------ Iteration 687 --------------------------#
[32m[20221213 15:08:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:36 @agent_ppo2.py:185][0m |          -0.0005 |           5.5257 |           0.2446 |
[32m[20221213 15:08:36 @agent_ppo2.py:185][0m |          -0.0097 |           5.2970 |           0.2441 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0096 |           5.2733 |           0.2438 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0123 |           5.2016 |           0.2434 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0160 |           5.1861 |           0.2430 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0109 |           5.1960 |           0.2429 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0182 |           5.1307 |           0.2427 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0152 |           5.1289 |           0.2424 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0171 |           5.0732 |           0.2421 |
[32m[20221213 15:08:37 @agent_ppo2.py:185][0m |          -0.0172 |           5.0761 |           0.2420 |
[32m[20221213 15:08:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 136.23
[32m[20221213 15:08:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.78
[32m[20221213 15:08:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.32
[32m[20221213 15:08:37 @agent_ppo2.py:143][0m Total time:      15.70 min
[32m[20221213 15:08:37 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221213 15:08:37 @agent_ppo2.py:121][0m #------------------------ Iteration 688 --------------------------#
[32m[20221213 15:08:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |           0.0004 |           5.3434 |           0.2366 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0061 |           5.0908 |           0.2362 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0111 |           4.9257 |           0.2360 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0126 |           4.8183 |           0.2361 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0160 |           4.7263 |           0.2362 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0120 |           4.6931 |           0.2360 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0150 |           4.6118 |           0.2359 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0155 |           4.5864 |           0.2360 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0171 |           4.5367 |           0.2360 |
[32m[20221213 15:08:38 @agent_ppo2.py:185][0m |          -0.0181 |           4.5282 |           0.2361 |
[32m[20221213 15:08:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.43
[32m[20221213 15:08:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 151.32
[32m[20221213 15:08:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.14
[32m[20221213 15:08:39 @agent_ppo2.py:143][0m Total time:      15.72 min
[32m[20221213 15:08:39 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221213 15:08:39 @agent_ppo2.py:121][0m #------------------------ Iteration 689 --------------------------#
[32m[20221213 15:08:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:39 @agent_ppo2.py:185][0m |          -0.0034 |           5.5226 |           0.2401 |
[32m[20221213 15:08:39 @agent_ppo2.py:185][0m |          -0.0078 |           5.3475 |           0.2400 |
[32m[20221213 15:08:39 @agent_ppo2.py:185][0m |          -0.0120 |           5.3100 |           0.2396 |
[32m[20221213 15:08:39 @agent_ppo2.py:185][0m |          -0.0150 |           5.2732 |           0.2395 |
[32m[20221213 15:08:39 @agent_ppo2.py:185][0m |          -0.0157 |           5.2361 |           0.2394 |
[32m[20221213 15:08:39 @agent_ppo2.py:185][0m |          -0.0160 |           5.2093 |           0.2392 |
[32m[20221213 15:08:40 @agent_ppo2.py:185][0m |          -0.0175 |           5.2115 |           0.2393 |
[32m[20221213 15:08:40 @agent_ppo2.py:185][0m |          -0.0195 |           5.1842 |           0.2390 |
[32m[20221213 15:08:40 @agent_ppo2.py:185][0m |          -0.0170 |           5.1528 |           0.2388 |
[32m[20221213 15:08:40 @agent_ppo2.py:185][0m |          -0.0182 |           5.1314 |           0.2389 |
[32m[20221213 15:08:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.26
[32m[20221213 15:08:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.45
[32m[20221213 15:08:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 159.79
[32m[20221213 15:08:40 @agent_ppo2.py:143][0m Total time:      15.75 min
[32m[20221213 15:08:40 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221213 15:08:40 @agent_ppo2.py:121][0m #------------------------ Iteration 690 --------------------------#
[32m[20221213 15:08:40 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:08:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:40 @agent_ppo2.py:185][0m |          -0.0036 |           5.7896 |           0.2398 |
[32m[20221213 15:08:40 @agent_ppo2.py:185][0m |          -0.0062 |           5.6717 |           0.2393 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0119 |           5.5773 |           0.2392 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0027 |           5.8639 |           0.2395 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0140 |           5.4650 |           0.2393 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0193 |           5.4309 |           0.2395 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0191 |           5.4151 |           0.2397 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0078 |           5.7787 |           0.2397 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0215 |           5.3706 |           0.2395 |
[32m[20221213 15:08:41 @agent_ppo2.py:185][0m |          -0.0107 |           5.5107 |           0.2398 |
[32m[20221213 15:08:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.24
[32m[20221213 15:08:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 155.55
[32m[20221213 15:08:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.48
[32m[20221213 15:08:41 @agent_ppo2.py:143][0m Total time:      15.77 min
[32m[20221213 15:08:41 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221213 15:08:41 @agent_ppo2.py:121][0m #------------------------ Iteration 691 --------------------------#
[32m[20221213 15:08:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0017 |           5.7818 |           0.2387 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0091 |           5.5804 |           0.2386 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0118 |           5.5129 |           0.2382 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0155 |           5.4438 |           0.2379 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0149 |           5.4024 |           0.2375 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0175 |           5.3884 |           0.2373 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0169 |           5.3391 |           0.2372 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0167 |           5.3327 |           0.2369 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0198 |           5.2788 |           0.2369 |
[32m[20221213 15:08:42 @agent_ppo2.py:185][0m |          -0.0203 |           5.2766 |           0.2367 |
[32m[20221213 15:08:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 127.88
[32m[20221213 15:08:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.43
[32m[20221213 15:08:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.04
[32m[20221213 15:08:43 @agent_ppo2.py:143][0m Total time:      15.79 min
[32m[20221213 15:08:43 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221213 15:08:43 @agent_ppo2.py:121][0m #------------------------ Iteration 692 --------------------------#
[32m[20221213 15:08:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:43 @agent_ppo2.py:185][0m |          -0.0021 |           5.4242 |           0.2389 |
[32m[20221213 15:08:43 @agent_ppo2.py:185][0m |          -0.0066 |           5.2656 |           0.2387 |
[32m[20221213 15:08:43 @agent_ppo2.py:185][0m |          -0.0004 |           5.3756 |           0.2381 |
[32m[20221213 15:08:43 @agent_ppo2.py:185][0m |          -0.0124 |           5.1284 |           0.2379 |
[32m[20221213 15:08:43 @agent_ppo2.py:185][0m |          -0.0158 |           5.0785 |           0.2375 |
[32m[20221213 15:08:43 @agent_ppo2.py:185][0m |          -0.0179 |           5.0338 |           0.2373 |
[32m[20221213 15:08:44 @agent_ppo2.py:185][0m |          -0.0156 |           4.9862 |           0.2367 |
[32m[20221213 15:08:44 @agent_ppo2.py:185][0m |          -0.0210 |           4.9407 |           0.2368 |
[32m[20221213 15:08:44 @agent_ppo2.py:185][0m |          -0.0134 |           4.9959 |           0.2365 |
[32m[20221213 15:08:44 @agent_ppo2.py:185][0m |          -0.0152 |           4.8958 |           0.2362 |
[32m[20221213 15:08:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:08:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.13
[32m[20221213 15:08:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.26
[32m[20221213 15:08:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 133.70
[32m[20221213 15:08:44 @agent_ppo2.py:143][0m Total time:      15.81 min
[32m[20221213 15:08:44 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221213 15:08:44 @agent_ppo2.py:121][0m #------------------------ Iteration 693 --------------------------#
[32m[20221213 15:08:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:44 @agent_ppo2.py:185][0m |           0.0112 |           6.1118 |           0.2374 |
[32m[20221213 15:08:44 @agent_ppo2.py:185][0m |          -0.0096 |           5.2765 |           0.2367 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |          -0.0063 |           5.1633 |           0.2368 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |           0.0072 |           5.5871 |           0.2366 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |          -0.0189 |           5.0034 |           0.2363 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |          -0.0163 |           4.9231 |           0.2362 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |          -0.0150 |           4.8420 |           0.2361 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |          -0.0182 |           4.7924 |           0.2360 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |          -0.0178 |           4.7508 |           0.2360 |
[32m[20221213 15:08:45 @agent_ppo2.py:185][0m |          -0.0240 |           4.7152 |           0.2358 |
[32m[20221213 15:08:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.26
[32m[20221213 15:08:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.27
[32m[20221213 15:08:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.04
[32m[20221213 15:08:45 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221213 15:08:45 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221213 15:08:45 @agent_ppo2.py:121][0m #------------------------ Iteration 694 --------------------------#
[32m[20221213 15:08:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0010 |           5.6832 |           0.2317 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0033 |           5.4521 |           0.2316 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0092 |           5.3784 |           0.2317 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0108 |           5.2942 |           0.2317 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0057 |           5.2808 |           0.2317 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0126 |           5.1939 |           0.2316 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0125 |           5.1705 |           0.2317 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0128 |           5.1406 |           0.2318 |
[32m[20221213 15:08:46 @agent_ppo2.py:185][0m |          -0.0150 |           5.0992 |           0.2318 |
[32m[20221213 15:08:47 @agent_ppo2.py:185][0m |          -0.0154 |           5.0722 |           0.2320 |
[32m[20221213 15:08:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.58
[32m[20221213 15:08:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 145.65
[32m[20221213 15:08:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 165.33
[32m[20221213 15:08:47 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221213 15:08:47 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221213 15:08:47 @agent_ppo2.py:121][0m #------------------------ Iteration 695 --------------------------#
[32m[20221213 15:08:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:47 @agent_ppo2.py:185][0m |          -0.0032 |           5.8084 |           0.2384 |
[32m[20221213 15:08:47 @agent_ppo2.py:185][0m |          -0.0086 |           5.4697 |           0.2379 |
[32m[20221213 15:08:47 @agent_ppo2.py:185][0m |          -0.0122 |           5.3481 |           0.2377 |
[32m[20221213 15:08:47 @agent_ppo2.py:185][0m |          -0.0139 |           5.2709 |           0.2378 |
[32m[20221213 15:08:47 @agent_ppo2.py:185][0m |          -0.0038 |           5.8924 |           0.2376 |
[32m[20221213 15:08:48 @agent_ppo2.py:185][0m |          -0.0178 |           5.1871 |           0.2375 |
[32m[20221213 15:08:48 @agent_ppo2.py:185][0m |          -0.0157 |           5.1273 |           0.2375 |
[32m[20221213 15:08:48 @agent_ppo2.py:185][0m |          -0.0189 |           5.0920 |           0.2374 |
[32m[20221213 15:08:48 @agent_ppo2.py:185][0m |          -0.0202 |           5.0472 |           0.2373 |
[32m[20221213 15:08:48 @agent_ppo2.py:185][0m |          -0.0217 |           5.0133 |           0.2373 |
[32m[20221213 15:08:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.50
[32m[20221213 15:08:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.25
[32m[20221213 15:08:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 151.04
[32m[20221213 15:08:48 @agent_ppo2.py:143][0m Total time:      15.88 min
[32m[20221213 15:08:48 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221213 15:08:48 @agent_ppo2.py:121][0m #------------------------ Iteration 696 --------------------------#
[32m[20221213 15:08:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:48 @agent_ppo2.py:185][0m |          -0.0007 |           5.4637 |           0.2396 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0041 |           5.0910 |           0.2389 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0128 |           4.9207 |           0.2387 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0128 |           4.8485 |           0.2386 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0128 |           4.7685 |           0.2384 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0117 |           4.6750 |           0.2381 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0153 |           4.6217 |           0.2381 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0156 |           4.5370 |           0.2380 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0184 |           4.4805 |           0.2380 |
[32m[20221213 15:08:49 @agent_ppo2.py:185][0m |          -0.0169 |           4.4247 |           0.2377 |
[32m[20221213 15:08:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 140.64
[32m[20221213 15:08:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.08
[32m[20221213 15:08:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.08
[32m[20221213 15:08:49 @agent_ppo2.py:143][0m Total time:      15.90 min
[32m[20221213 15:08:49 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221213 15:08:49 @agent_ppo2.py:121][0m #------------------------ Iteration 697 --------------------------#
[32m[20221213 15:08:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0014 |           5.7147 |           0.2361 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0005 |           5.5619 |           0.2360 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0100 |           5.3287 |           0.2358 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0134 |           5.2462 |           0.2357 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0151 |           5.2069 |           0.2357 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0163 |           5.1620 |           0.2357 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0176 |           5.1331 |           0.2356 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0179 |           5.1001 |           0.2357 |
[32m[20221213 15:08:50 @agent_ppo2.py:185][0m |          -0.0172 |           5.0758 |           0.2359 |
[32m[20221213 15:08:51 @agent_ppo2.py:185][0m |          -0.0233 |           5.0537 |           0.2358 |
[32m[20221213 15:08:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 125.43
[32m[20221213 15:08:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 137.75
[32m[20221213 15:08:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.39
[32m[20221213 15:08:51 @agent_ppo2.py:143][0m Total time:      15.93 min
[32m[20221213 15:08:51 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221213 15:08:51 @agent_ppo2.py:121][0m #------------------------ Iteration 698 --------------------------#
[32m[20221213 15:08:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:51 @agent_ppo2.py:185][0m |           0.0031 |           5.3844 |           0.2390 |
[32m[20221213 15:08:51 @agent_ppo2.py:185][0m |          -0.0062 |           5.0383 |           0.2386 |
[32m[20221213 15:08:51 @agent_ppo2.py:185][0m |          -0.0095 |           4.9576 |           0.2384 |
[32m[20221213 15:08:51 @agent_ppo2.py:185][0m |          -0.0049 |           5.0639 |           0.2382 |
[32m[20221213 15:08:51 @agent_ppo2.py:185][0m |          -0.0131 |           4.7914 |           0.2381 |
[32m[20221213 15:08:52 @agent_ppo2.py:185][0m |          -0.0158 |           4.7280 |           0.2381 |
[32m[20221213 15:08:52 @agent_ppo2.py:185][0m |          -0.0169 |           4.6608 |           0.2380 |
[32m[20221213 15:08:52 @agent_ppo2.py:185][0m |          -0.0154 |           4.6217 |           0.2380 |
[32m[20221213 15:08:52 @agent_ppo2.py:185][0m |          -0.0166 |           4.5900 |           0.2380 |
[32m[20221213 15:08:52 @agent_ppo2.py:185][0m |          -0.0163 |           4.5808 |           0.2377 |
[32m[20221213 15:08:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.92
[32m[20221213 15:08:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 154.26
[32m[20221213 15:08:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.92
[32m[20221213 15:08:52 @agent_ppo2.py:143][0m Total time:      15.95 min
[32m[20221213 15:08:52 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221213 15:08:52 @agent_ppo2.py:121][0m #------------------------ Iteration 699 --------------------------#
[32m[20221213 15:08:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:52 @agent_ppo2.py:185][0m |          -0.0026 |           5.5630 |           0.2408 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0082 |           5.4055 |           0.2408 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0102 |           5.3125 |           0.2407 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0155 |           5.2675 |           0.2405 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0145 |           5.2454 |           0.2404 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0155 |           5.1971 |           0.2404 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0175 |           5.1452 |           0.2404 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0162 |           5.1172 |           0.2405 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0171 |           5.0970 |           0.2404 |
[32m[20221213 15:08:53 @agent_ppo2.py:185][0m |          -0.0201 |           5.0784 |           0.2405 |
[32m[20221213 15:08:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.15
[32m[20221213 15:08:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 138.43
[32m[20221213 15:08:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 158.40
[32m[20221213 15:08:53 @agent_ppo2.py:143][0m Total time:      15.97 min
[32m[20221213 15:08:53 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221213 15:08:53 @agent_ppo2.py:121][0m #------------------------ Iteration 700 --------------------------#
[32m[20221213 15:08:54 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:08:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0011 |           5.6665 |           0.2406 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0058 |           5.5047 |           0.2401 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0104 |           5.3891 |           0.2400 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0117 |           5.2847 |           0.2399 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0145 |           5.2249 |           0.2397 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0058 |           5.6258 |           0.2397 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0137 |           5.0584 |           0.2393 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0143 |           5.0257 |           0.2394 |
[32m[20221213 15:08:54 @agent_ppo2.py:185][0m |          -0.0137 |           5.0217 |           0.2392 |
[32m[20221213 15:08:55 @agent_ppo2.py:185][0m |          -0.0191 |           4.9543 |           0.2393 |
[32m[20221213 15:08:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.58
[32m[20221213 15:08:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.24
[32m[20221213 15:08:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.71
[32m[20221213 15:08:55 @agent_ppo2.py:143][0m Total time:      15.99 min
[32m[20221213 15:08:55 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221213 15:08:55 @agent_ppo2.py:121][0m #------------------------ Iteration 701 --------------------------#
[32m[20221213 15:08:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:55 @agent_ppo2.py:185][0m |           0.0002 |           5.7674 |           0.2326 |
[32m[20221213 15:08:55 @agent_ppo2.py:185][0m |          -0.0066 |           5.3037 |           0.2326 |
[32m[20221213 15:08:55 @agent_ppo2.py:185][0m |          -0.0081 |           5.1554 |           0.2324 |
[32m[20221213 15:08:55 @agent_ppo2.py:185][0m |          -0.0101 |           5.0406 |           0.2321 |
[32m[20221213 15:08:55 @agent_ppo2.py:185][0m |          -0.0123 |           4.9571 |           0.2320 |
[32m[20221213 15:08:56 @agent_ppo2.py:185][0m |          -0.0173 |           4.9295 |           0.2322 |
[32m[20221213 15:08:56 @agent_ppo2.py:185][0m |          -0.0149 |           4.8530 |           0.2321 |
[32m[20221213 15:08:56 @agent_ppo2.py:185][0m |          -0.0132 |           4.8538 |           0.2320 |
[32m[20221213 15:08:56 @agent_ppo2.py:185][0m |          -0.0163 |           4.7822 |           0.2320 |
[32m[20221213 15:08:56 @agent_ppo2.py:185][0m |          -0.0187 |           4.7460 |           0.2321 |
[32m[20221213 15:08:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.25
[32m[20221213 15:08:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.40
[32m[20221213 15:08:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 29.51
[32m[20221213 15:08:56 @agent_ppo2.py:143][0m Total time:      16.01 min
[32m[20221213 15:08:56 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221213 15:08:56 @agent_ppo2.py:121][0m #------------------------ Iteration 702 --------------------------#
[32m[20221213 15:08:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:56 @agent_ppo2.py:185][0m |          -0.0017 |           5.8176 |           0.2428 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0075 |           5.7233 |           0.2424 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0090 |           5.6844 |           0.2423 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0124 |           5.6354 |           0.2421 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0150 |           5.6145 |           0.2419 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0142 |           5.5860 |           0.2421 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |           0.0002 |           6.2060 |           0.2421 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0068 |           5.8354 |           0.2421 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0158 |           5.4981 |           0.2421 |
[32m[20221213 15:08:57 @agent_ppo2.py:185][0m |          -0.0135 |           5.5564 |           0.2422 |
[32m[20221213 15:08:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:08:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 149.55
[32m[20221213 15:08:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 158.27
[32m[20221213 15:08:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.90
[32m[20221213 15:08:57 @agent_ppo2.py:143][0m Total time:      16.04 min
[32m[20221213 15:08:57 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221213 15:08:57 @agent_ppo2.py:121][0m #------------------------ Iteration 703 --------------------------#
[32m[20221213 15:08:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:08:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |           0.0038 |           5.8243 |           0.2392 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0134 |           5.5751 |           0.2386 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0119 |           5.5269 |           0.2384 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0146 |           5.4130 |           0.2381 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0115 |           5.4118 |           0.2382 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0161 |           5.3241 |           0.2378 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0178 |           5.2885 |           0.2376 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0160 |           5.2495 |           0.2376 |
[32m[20221213 15:08:58 @agent_ppo2.py:185][0m |          -0.0192 |           5.2254 |           0.2377 |
[32m[20221213 15:08:59 @agent_ppo2.py:185][0m |          -0.0189 |           5.2031 |           0.2375 |
[32m[20221213 15:08:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:08:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.78
[32m[20221213 15:08:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 142.64
[32m[20221213 15:08:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.01
[32m[20221213 15:08:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 166.01
[32m[20221213 15:08:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 166.01
[32m[20221213 15:08:59 @agent_ppo2.py:143][0m Total time:      16.06 min
[32m[20221213 15:08:59 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221213 15:08:59 @agent_ppo2.py:121][0m #------------------------ Iteration 704 --------------------------#
[32m[20221213 15:08:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:08:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:08:59 @agent_ppo2.py:185][0m |           0.0048 |           5.3248 |           0.2405 |
[32m[20221213 15:08:59 @agent_ppo2.py:185][0m |          -0.0006 |           4.7116 |           0.2405 |
[32m[20221213 15:08:59 @agent_ppo2.py:185][0m |          -0.0047 |           4.5877 |           0.2401 |
[32m[20221213 15:08:59 @agent_ppo2.py:185][0m |           0.0020 |           4.6103 |           0.2403 |
[32m[20221213 15:08:59 @agent_ppo2.py:185][0m |          -0.0054 |           4.3288 |           0.2402 |
[32m[20221213 15:09:00 @agent_ppo2.py:185][0m |          -0.0142 |           4.2897 |           0.2404 |
[32m[20221213 15:09:00 @agent_ppo2.py:185][0m |          -0.0173 |           4.2322 |           0.2402 |
[32m[20221213 15:09:00 @agent_ppo2.py:185][0m |          -0.0206 |           4.2169 |           0.2400 |
[32m[20221213 15:09:00 @agent_ppo2.py:185][0m |          -0.0115 |           4.3150 |           0.2403 |
[32m[20221213 15:09:00 @agent_ppo2.py:185][0m |          -0.0020 |           4.7099 |           0.2401 |
[32m[20221213 15:09:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.92
[32m[20221213 15:09:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.00
[32m[20221213 15:09:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.65
[32m[20221213 15:09:00 @agent_ppo2.py:143][0m Total time:      16.08 min
[32m[20221213 15:09:00 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221213 15:09:00 @agent_ppo2.py:121][0m #------------------------ Iteration 705 --------------------------#
[32m[20221213 15:09:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:09:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:00 @agent_ppo2.py:185][0m |          -0.0045 |           5.6496 |           0.2338 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0080 |           5.2646 |           0.2336 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0096 |           5.1237 |           0.2335 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0129 |           5.0465 |           0.2333 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0113 |           4.9778 |           0.2331 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0125 |           4.9249 |           0.2330 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0131 |           4.9605 |           0.2329 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0145 |           4.8929 |           0.2327 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0155 |           4.8578 |           0.2327 |
[32m[20221213 15:09:01 @agent_ppo2.py:185][0m |          -0.0166 |           4.8164 |           0.2329 |
[32m[20221213 15:09:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 120.57
[32m[20221213 15:09:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 136.11
[32m[20221213 15:09:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 143.63
[32m[20221213 15:09:01 @agent_ppo2.py:143][0m Total time:      16.10 min
[32m[20221213 15:09:01 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221213 15:09:01 @agent_ppo2.py:121][0m #------------------------ Iteration 706 --------------------------#
[32m[20221213 15:09:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0035 |           6.5216 |           0.2424 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0055 |           6.4664 |           0.2418 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0212 |           6.2085 |           0.2415 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0154 |           6.1656 |           0.2416 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0128 |           6.1294 |           0.2414 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0197 |           6.1225 |           0.2413 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0147 |           6.0849 |           0.2411 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0146 |           6.0875 |           0.2411 |
[32m[20221213 15:09:02 @agent_ppo2.py:185][0m |          -0.0047 |           6.7048 |           0.2409 |
[32m[20221213 15:09:03 @agent_ppo2.py:185][0m |          -0.0134 |           6.3534 |           0.2409 |
[32m[20221213 15:09:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 136.62
[32m[20221213 15:09:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 144.86
[32m[20221213 15:09:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.99
[32m[20221213 15:09:03 @agent_ppo2.py:143][0m Total time:      16.13 min
[32m[20221213 15:09:03 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221213 15:09:03 @agent_ppo2.py:121][0m #------------------------ Iteration 707 --------------------------#
[32m[20221213 15:09:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:03 @agent_ppo2.py:185][0m |          -0.0041 |           5.8043 |           0.2386 |
[32m[20221213 15:09:03 @agent_ppo2.py:185][0m |          -0.0060 |           5.5091 |           0.2387 |
[32m[20221213 15:09:03 @agent_ppo2.py:185][0m |          -0.0135 |           5.4282 |           0.2383 |
[32m[20221213 15:09:03 @agent_ppo2.py:185][0m |          -0.0147 |           5.3539 |           0.2383 |
[32m[20221213 15:09:03 @agent_ppo2.py:185][0m |          -0.0165 |           5.3219 |           0.2382 |
[32m[20221213 15:09:04 @agent_ppo2.py:185][0m |          -0.0153 |           5.2791 |           0.2380 |
[32m[20221213 15:09:04 @agent_ppo2.py:185][0m |          -0.0169 |           5.2154 |           0.2380 |
[32m[20221213 15:09:04 @agent_ppo2.py:185][0m |          -0.0199 |           5.1913 |           0.2379 |
[32m[20221213 15:09:04 @agent_ppo2.py:185][0m |          -0.0188 |           5.1510 |           0.2378 |
[32m[20221213 15:09:04 @agent_ppo2.py:185][0m |          -0.0218 |           5.1155 |           0.2378 |
[32m[20221213 15:09:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.98
[32m[20221213 15:09:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.81
[32m[20221213 15:09:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.01
[32m[20221213 15:09:04 @agent_ppo2.py:143][0m Total time:      16.15 min
[32m[20221213 15:09:04 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221213 15:09:04 @agent_ppo2.py:121][0m #------------------------ Iteration 708 --------------------------#
[32m[20221213 15:09:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:04 @agent_ppo2.py:185][0m |           0.0041 |           5.6260 |           0.2304 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0028 |           5.5110 |           0.2298 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0083 |           5.3402 |           0.2295 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0086 |           5.2847 |           0.2296 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0139 |           5.2282 |           0.2294 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0139 |           5.2060 |           0.2294 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0090 |           5.4798 |           0.2292 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0194 |           5.1527 |           0.2290 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0071 |           5.4549 |           0.2293 |
[32m[20221213 15:09:05 @agent_ppo2.py:185][0m |          -0.0156 |           5.1055 |           0.2289 |
[32m[20221213 15:09:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.51
[32m[20221213 15:09:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.88
[32m[20221213 15:09:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.73
[32m[20221213 15:09:05 @agent_ppo2.py:143][0m Total time:      16.17 min
[32m[20221213 15:09:05 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221213 15:09:05 @agent_ppo2.py:121][0m #------------------------ Iteration 709 --------------------------#
[32m[20221213 15:09:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0003 |           5.0586 |           0.2375 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0101 |           4.8210 |           0.2372 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0045 |           4.8265 |           0.2370 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0132 |           4.6344 |           0.2371 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0144 |           4.5653 |           0.2371 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0195 |           4.5315 |           0.2373 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0169 |           4.4493 |           0.2372 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0063 |           4.8594 |           0.2372 |
[32m[20221213 15:09:06 @agent_ppo2.py:185][0m |          -0.0146 |           4.4010 |           0.2373 |
[32m[20221213 15:09:07 @agent_ppo2.py:185][0m |          -0.0216 |           4.3513 |           0.2374 |
[32m[20221213 15:09:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 141.19
[32m[20221213 15:09:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 162.08
[32m[20221213 15:09:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 146.78
[32m[20221213 15:09:07 @agent_ppo2.py:143][0m Total time:      16.19 min
[32m[20221213 15:09:07 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221213 15:09:07 @agent_ppo2.py:121][0m #------------------------ Iteration 710 --------------------------#
[32m[20221213 15:09:07 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:09:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:07 @agent_ppo2.py:185][0m |          -0.0007 |           6.7303 |           0.2402 |
[32m[20221213 15:09:07 @agent_ppo2.py:185][0m |          -0.0110 |           6.3526 |           0.2399 |
[32m[20221213 15:09:07 @agent_ppo2.py:185][0m |          -0.0102 |           6.1791 |           0.2398 |
[32m[20221213 15:09:07 @agent_ppo2.py:185][0m |          -0.0119 |           6.0600 |           0.2397 |
[32m[20221213 15:09:08 @agent_ppo2.py:185][0m |          -0.0074 |           6.0337 |           0.2398 |
[32m[20221213 15:09:08 @agent_ppo2.py:185][0m |          -0.0195 |           5.9269 |           0.2398 |
[32m[20221213 15:09:08 @agent_ppo2.py:185][0m |          -0.0162 |           5.8681 |           0.2399 |
[32m[20221213 15:09:08 @agent_ppo2.py:185][0m |          -0.0157 |           5.8124 |           0.2396 |
[32m[20221213 15:09:08 @agent_ppo2.py:185][0m |          -0.0183 |           5.7885 |           0.2396 |
[32m[20221213 15:09:08 @agent_ppo2.py:185][0m |          -0.0264 |           5.7301 |           0.2398 |
[32m[20221213 15:09:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.58
[32m[20221213 15:09:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.17
[32m[20221213 15:09:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 153.24
[32m[20221213 15:09:08 @agent_ppo2.py:143][0m Total time:      16.22 min
[32m[20221213 15:09:08 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221213 15:09:08 @agent_ppo2.py:121][0m #------------------------ Iteration 711 --------------------------#
[32m[20221213 15:09:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:08 @agent_ppo2.py:185][0m |          -0.0011 |           6.1042 |           0.2350 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |           0.0008 |           6.1929 |           0.2350 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |          -0.0073 |           5.8719 |           0.2346 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |          -0.0071 |           5.8363 |           0.2344 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |          -0.0111 |           5.7430 |           0.2343 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |           0.0159 |           6.9407 |           0.2340 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |          -0.0130 |           5.7484 |           0.2336 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |          -0.0124 |           5.6339 |           0.2335 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |          -0.0159 |           5.6034 |           0.2334 |
[32m[20221213 15:09:09 @agent_ppo2.py:185][0m |          -0.0151 |           5.5512 |           0.2334 |
[32m[20221213 15:09:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 135.87
[32m[20221213 15:09:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.65
[32m[20221213 15:09:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 157.95
[32m[20221213 15:09:09 @agent_ppo2.py:143][0m Total time:      16.24 min
[32m[20221213 15:09:09 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221213 15:09:09 @agent_ppo2.py:121][0m #------------------------ Iteration 712 --------------------------#
[32m[20221213 15:09:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0024 |           6.3092 |           0.2399 |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0045 |           6.0346 |           0.2395 |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0096 |           5.8462 |           0.2390 |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0128 |           5.7154 |           0.2388 |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0076 |           5.9077 |           0.2386 |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0168 |           5.6053 |           0.2386 |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0126 |           5.5314 |           0.2384 |
[32m[20221213 15:09:10 @agent_ppo2.py:185][0m |          -0.0175 |           5.4822 |           0.2382 |
[32m[20221213 15:09:11 @agent_ppo2.py:185][0m |          -0.0119 |           5.4777 |           0.2380 |
[32m[20221213 15:09:11 @agent_ppo2.py:185][0m |          -0.0173 |           5.3848 |           0.2378 |
[32m[20221213 15:09:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:09:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.08
[32m[20221213 15:09:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 151.88
[32m[20221213 15:09:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.47
[32m[20221213 15:09:11 @agent_ppo2.py:143][0m Total time:      16.26 min
[32m[20221213 15:09:11 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221213 15:09:11 @agent_ppo2.py:121][0m #------------------------ Iteration 713 --------------------------#
[32m[20221213 15:09:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:11 @agent_ppo2.py:185][0m |           0.0017 |           6.8585 |           0.2342 |
[32m[20221213 15:09:11 @agent_ppo2.py:185][0m |           0.0022 |           6.8232 |           0.2340 |
[32m[20221213 15:09:11 @agent_ppo2.py:185][0m |          -0.0072 |           6.2648 |           0.2336 |
[32m[20221213 15:09:11 @agent_ppo2.py:185][0m |          -0.0094 |           6.1101 |           0.2336 |
[32m[20221213 15:09:12 @agent_ppo2.py:185][0m |          -0.0090 |           6.0574 |           0.2336 |
[32m[20221213 15:09:12 @agent_ppo2.py:185][0m |          -0.0161 |           5.9776 |           0.2334 |
[32m[20221213 15:09:12 @agent_ppo2.py:185][0m |          -0.0164 |           5.9632 |           0.2333 |
[32m[20221213 15:09:12 @agent_ppo2.py:185][0m |          -0.0167 |           5.8924 |           0.2334 |
[32m[20221213 15:09:12 @agent_ppo2.py:185][0m |          -0.0180 |           5.8323 |           0.2335 |
[32m[20221213 15:09:12 @agent_ppo2.py:185][0m |          -0.0182 |           5.7831 |           0.2332 |
[32m[20221213 15:09:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.32
[32m[20221213 15:09:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.24
[32m[20221213 15:09:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.02
[32m[20221213 15:09:12 @agent_ppo2.py:143][0m Total time:      16.28 min
[32m[20221213 15:09:12 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221213 15:09:12 @agent_ppo2.py:121][0m #------------------------ Iteration 714 --------------------------#
[32m[20221213 15:09:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |           0.0002 |           6.4257 |           0.2332 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |           0.0000 |           6.3226 |           0.2329 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0101 |           5.9983 |           0.2326 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0146 |           5.8957 |           0.2323 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0149 |           5.8196 |           0.2322 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0101 |           5.8307 |           0.2320 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0166 |           5.7020 |           0.2318 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0157 |           5.6735 |           0.2317 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0170 |           5.6214 |           0.2316 |
[32m[20221213 15:09:13 @agent_ppo2.py:185][0m |          -0.0171 |           5.5966 |           0.2315 |
[32m[20221213 15:09:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.89
[32m[20221213 15:09:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 161.83
[32m[20221213 15:09:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.39
[32m[20221213 15:09:13 @agent_ppo2.py:143][0m Total time:      16.31 min
[32m[20221213 15:09:13 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221213 15:09:13 @agent_ppo2.py:121][0m #------------------------ Iteration 715 --------------------------#
[32m[20221213 15:09:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0017 |           5.5806 |           0.2293 |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0079 |           5.4368 |           0.2296 |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0105 |           5.3875 |           0.2294 |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0008 |           5.6359 |           0.2295 |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0090 |           5.3993 |           0.2294 |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0143 |           5.2970 |           0.2295 |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0143 |           5.2646 |           0.2294 |
[32m[20221213 15:09:14 @agent_ppo2.py:185][0m |          -0.0127 |           5.2636 |           0.2296 |
[32m[20221213 15:09:15 @agent_ppo2.py:185][0m |          -0.0112 |           5.2694 |           0.2296 |
[32m[20221213 15:09:15 @agent_ppo2.py:185][0m |          -0.0169 |           5.2021 |           0.2297 |
[32m[20221213 15:09:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 133.75
[32m[20221213 15:09:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.86
[32m[20221213 15:09:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 24.97
[32m[20221213 15:09:15 @agent_ppo2.py:143][0m Total time:      16.33 min
[32m[20221213 15:09:15 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221213 15:09:15 @agent_ppo2.py:121][0m #------------------------ Iteration 716 --------------------------#
[32m[20221213 15:09:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:15 @agent_ppo2.py:185][0m |          -0.0012 |           5.2858 |           0.2315 |
[32m[20221213 15:09:15 @agent_ppo2.py:185][0m |          -0.0082 |           5.1930 |           0.2312 |
[32m[20221213 15:09:15 @agent_ppo2.py:185][0m |          -0.0108 |           5.0630 |           0.2309 |
[32m[20221213 15:09:15 @agent_ppo2.py:185][0m |          -0.0112 |           5.0027 |           0.2306 |
[32m[20221213 15:09:16 @agent_ppo2.py:185][0m |          -0.0130 |           4.9386 |           0.2306 |
[32m[20221213 15:09:16 @agent_ppo2.py:185][0m |          -0.0022 |           5.6550 |           0.2308 |
[32m[20221213 15:09:16 @agent_ppo2.py:185][0m |          -0.0155 |           4.8691 |           0.2304 |
[32m[20221213 15:09:16 @agent_ppo2.py:185][0m |          -0.0090 |           5.0145 |           0.2305 |
[32m[20221213 15:09:16 @agent_ppo2.py:185][0m |          -0.0174 |           4.7640 |           0.2302 |
[32m[20221213 15:09:16 @agent_ppo2.py:185][0m |          -0.0181 |           4.7101 |           0.2306 |
[32m[20221213 15:09:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 126.65
[32m[20221213 15:09:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 151.05
[32m[20221213 15:09:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.56
[32m[20221213 15:09:16 @agent_ppo2.py:143][0m Total time:      16.35 min
[32m[20221213 15:09:16 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221213 15:09:16 @agent_ppo2.py:121][0m #------------------------ Iteration 717 --------------------------#
[32m[20221213 15:09:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:09:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0017 |           6.0430 |           0.2269 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0106 |           5.8442 |           0.2266 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0169 |           5.6959 |           0.2264 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0111 |           5.6236 |           0.2262 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0164 |           5.5730 |           0.2261 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0187 |           5.4844 |           0.2261 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0097 |           5.6178 |           0.2261 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0173 |           5.4758 |           0.2262 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0199 |           5.3519 |           0.2263 |
[32m[20221213 15:09:17 @agent_ppo2.py:185][0m |          -0.0194 |           5.3064 |           0.2262 |
[32m[20221213 15:09:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 141.68
[32m[20221213 15:09:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.73
[32m[20221213 15:09:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 149.24
[32m[20221213 15:09:18 @agent_ppo2.py:143][0m Total time:      16.37 min
[32m[20221213 15:09:18 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221213 15:09:18 @agent_ppo2.py:121][0m #------------------------ Iteration 718 --------------------------#
[32m[20221213 15:09:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:09:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |          -0.0045 |           6.5273 |           0.2362 |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |          -0.0071 |           6.2078 |           0.2358 |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |           0.0000 |           6.2872 |           0.2358 |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |          -0.0090 |           5.9635 |           0.2359 |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |          -0.0132 |           5.8911 |           0.2360 |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |          -0.0127 |           5.8125 |           0.2360 |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |          -0.0215 |           5.7809 |           0.2362 |
[32m[20221213 15:09:18 @agent_ppo2.py:185][0m |          -0.0158 |           5.7316 |           0.2360 |
[32m[20221213 15:09:19 @agent_ppo2.py:185][0m |          -0.0164 |           5.7068 |           0.2361 |
[32m[20221213 15:09:19 @agent_ppo2.py:185][0m |          -0.0208 |           5.6545 |           0.2362 |
[32m[20221213 15:09:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 131.99
[32m[20221213 15:09:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.33
[32m[20221213 15:09:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 146.73
[32m[20221213 15:09:19 @agent_ppo2.py:143][0m Total time:      16.39 min
[32m[20221213 15:09:19 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221213 15:09:19 @agent_ppo2.py:121][0m #------------------------ Iteration 719 --------------------------#
[32m[20221213 15:09:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:19 @agent_ppo2.py:185][0m |           0.0072 |           6.5861 |           0.2350 |
[32m[20221213 15:09:19 @agent_ppo2.py:185][0m |          -0.0083 |           6.0115 |           0.2350 |
[32m[20221213 15:09:19 @agent_ppo2.py:185][0m |          -0.0108 |           5.9371 |           0.2347 |
[32m[20221213 15:09:19 @agent_ppo2.py:185][0m |          -0.0102 |           5.8651 |           0.2348 |
[32m[20221213 15:09:20 @agent_ppo2.py:185][0m |          -0.0146 |           5.8050 |           0.2346 |
[32m[20221213 15:09:20 @agent_ppo2.py:185][0m |          -0.0149 |           5.7702 |           0.2347 |
[32m[20221213 15:09:20 @agent_ppo2.py:185][0m |          -0.0152 |           5.7261 |           0.2345 |
[32m[20221213 15:09:20 @agent_ppo2.py:185][0m |          -0.0146 |           5.7066 |           0.2345 |
[32m[20221213 15:09:20 @agent_ppo2.py:185][0m |          -0.0146 |           5.7096 |           0.2346 |
[32m[20221213 15:09:20 @agent_ppo2.py:185][0m |          -0.0202 |           5.6450 |           0.2344 |
[32m[20221213 15:09:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.01
[32m[20221213 15:09:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.72
[32m[20221213 15:09:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 117.05
[32m[20221213 15:09:20 @agent_ppo2.py:143][0m Total time:      16.42 min
[32m[20221213 15:09:20 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221213 15:09:20 @agent_ppo2.py:121][0m #------------------------ Iteration 720 --------------------------#
[32m[20221213 15:09:20 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:09:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |           0.0007 |           6.1716 |           0.2293 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0110 |           5.9030 |           0.2290 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0148 |           5.7660 |           0.2287 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0173 |           5.6970 |           0.2284 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0156 |           5.6042 |           0.2284 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0169 |           5.5864 |           0.2283 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0166 |           5.5411 |           0.2285 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0217 |           5.4898 |           0.2283 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0207 |           5.4574 |           0.2283 |
[32m[20221213 15:09:21 @agent_ppo2.py:185][0m |          -0.0209 |           5.4375 |           0.2283 |
[32m[20221213 15:09:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.35
[32m[20221213 15:09:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 160.63
[32m[20221213 15:09:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.44
[32m[20221213 15:09:22 @agent_ppo2.py:143][0m Total time:      16.44 min
[32m[20221213 15:09:22 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221213 15:09:22 @agent_ppo2.py:121][0m #------------------------ Iteration 721 --------------------------#
[32m[20221213 15:09:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:22 @agent_ppo2.py:185][0m |          -0.0028 |           6.4289 |           0.2368 |
[32m[20221213 15:09:22 @agent_ppo2.py:185][0m |          -0.0084 |           6.1675 |           0.2368 |
[32m[20221213 15:09:22 @agent_ppo2.py:185][0m |          -0.0087 |           6.0669 |           0.2364 |
[32m[20221213 15:09:22 @agent_ppo2.py:185][0m |          -0.0150 |           5.9967 |           0.2362 |
[32m[20221213 15:09:22 @agent_ppo2.py:185][0m |          -0.0123 |           5.9452 |           0.2362 |
[32m[20221213 15:09:22 @agent_ppo2.py:185][0m |          -0.0145 |           5.9168 |           0.2359 |
[32m[20221213 15:09:22 @agent_ppo2.py:185][0m |          -0.0160 |           5.8522 |           0.2357 |
[32m[20221213 15:09:23 @agent_ppo2.py:185][0m |          -0.0156 |           5.8041 |           0.2356 |
[32m[20221213 15:09:23 @agent_ppo2.py:185][0m |          -0.0162 |           5.7737 |           0.2356 |
[32m[20221213 15:09:23 @agent_ppo2.py:185][0m |          -0.0242 |           5.7504 |           0.2354 |
[32m[20221213 15:09:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:09:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 132.60
[32m[20221213 15:09:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 143.11
[32m[20221213 15:09:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 137.41
[32m[20221213 15:09:23 @agent_ppo2.py:143][0m Total time:      16.46 min
[32m[20221213 15:09:23 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221213 15:09:23 @agent_ppo2.py:121][0m #------------------------ Iteration 722 --------------------------#
[32m[20221213 15:09:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:09:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:23 @agent_ppo2.py:185][0m |           0.0009 |           6.4419 |           0.2381 |
[32m[20221213 15:09:23 @agent_ppo2.py:185][0m |          -0.0141 |           6.0948 |           0.2378 |
[32m[20221213 15:09:23 @agent_ppo2.py:185][0m |          -0.0075 |           6.2936 |           0.2382 |
[32m[20221213 15:09:24 @agent_ppo2.py:185][0m |          -0.0143 |           5.8930 |           0.2380 |
[32m[20221213 15:09:24 @agent_ppo2.py:185][0m |          -0.0172 |           5.7798 |           0.2384 |
[32m[20221213 15:09:24 @agent_ppo2.py:185][0m |          -0.0138 |           5.7189 |           0.2386 |
[32m[20221213 15:09:24 @agent_ppo2.py:185][0m |          -0.0216 |           5.6394 |           0.2387 |
[32m[20221213 15:09:24 @agent_ppo2.py:185][0m |          -0.0237 |           5.5647 |           0.2388 |
[32m[20221213 15:09:24 @agent_ppo2.py:185][0m |          -0.0223 |           5.5392 |           0.2390 |
[32m[20221213 15:09:24 @agent_ppo2.py:185][0m |          -0.0200 |           5.4838 |           0.2391 |
[32m[20221213 15:09:24 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:09:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.86
[32m[20221213 15:09:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 163.35
[32m[20221213 15:09:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 174.81
[32m[20221213 15:09:24 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 174.81
[32m[20221213 15:09:24 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 174.81
[32m[20221213 15:09:24 @agent_ppo2.py:143][0m Total time:      16.49 min
[32m[20221213 15:09:24 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221213 15:09:24 @agent_ppo2.py:121][0m #------------------------ Iteration 723 --------------------------#
[32m[20221213 15:09:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:09:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0032 |           5.6375 |           0.2393 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0078 |           5.3548 |           0.2395 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0124 |           5.2333 |           0.2392 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0099 |           5.1450 |           0.2393 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0118 |           5.1055 |           0.2393 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0126 |           5.0530 |           0.2394 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0138 |           5.0289 |           0.2394 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0178 |           4.9921 |           0.2394 |
[32m[20221213 15:09:25 @agent_ppo2.py:185][0m |          -0.0091 |           5.3779 |           0.2393 |
[32m[20221213 15:09:26 @agent_ppo2.py:185][0m |          -0.0193 |           4.9450 |           0.2397 |
[32m[20221213 15:09:26 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:09:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 124.00
[32m[20221213 15:09:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 141.99
[32m[20221213 15:09:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.38
[32m[20221213 15:09:26 @agent_ppo2.py:143][0m Total time:      16.51 min
[32m[20221213 15:09:26 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221213 15:09:26 @agent_ppo2.py:121][0m #------------------------ Iteration 724 --------------------------#
[32m[20221213 15:09:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:26 @agent_ppo2.py:185][0m |          -0.0014 |           5.1949 |           0.2406 |
[32m[20221213 15:09:26 @agent_ppo2.py:185][0m |          -0.0106 |           5.0191 |           0.2399 |
[32m[20221213 15:09:26 @agent_ppo2.py:185][0m |          -0.0152 |           4.9511 |           0.2398 |
[32m[20221213 15:09:26 @agent_ppo2.py:185][0m |          -0.0111 |           4.9251 |           0.2395 |
[32m[20221213 15:09:26 @agent_ppo2.py:185][0m |          -0.0146 |           4.8661 |           0.2394 |
[32m[20221213 15:09:27 @agent_ppo2.py:185][0m |          -0.0169 |           4.8440 |           0.2394 |
[32m[20221213 15:09:27 @agent_ppo2.py:185][0m |          -0.0139 |           4.8834 |           0.2394 |
[32m[20221213 15:09:27 @agent_ppo2.py:185][0m |          -0.0171 |           4.7878 |           0.2394 |
[32m[20221213 15:09:27 @agent_ppo2.py:185][0m |          -0.0191 |           4.7604 |           0.2395 |
[32m[20221213 15:09:27 @agent_ppo2.py:185][0m |          -0.0177 |           4.7286 |           0.2394 |
[32m[20221213 15:09:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:09:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.18
[32m[20221213 15:09:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 151.85
[32m[20221213 15:09:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.13
[32m[20221213 15:09:27 @agent_ppo2.py:143][0m Total time:      16.53 min
[32m[20221213 15:09:27 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221213 15:09:27 @agent_ppo2.py:121][0m #------------------------ Iteration 725 --------------------------#
[32m[20221213 15:09:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:27 @agent_ppo2.py:185][0m |           0.0089 |           5.7217 |           0.2370 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0082 |           4.7546 |           0.2364 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0111 |           4.6041 |           0.2362 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0117 |           4.5539 |           0.2364 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0173 |           4.4452 |           0.2360 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0171 |           4.3672 |           0.2359 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0195 |           4.3097 |           0.2358 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0162 |           4.2364 |           0.2359 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0219 |           4.1989 |           0.2357 |
[32m[20221213 15:09:28 @agent_ppo2.py:185][0m |          -0.0212 |           4.1442 |           0.2357 |
[32m[20221213 15:09:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:09:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.12
[32m[20221213 15:09:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.94
[32m[20221213 15:09:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 146.73
[32m[20221213 15:09:28 @agent_ppo2.py:143][0m Total time:      16.55 min
[32m[20221213 15:09:28 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221213 15:09:28 @agent_ppo2.py:121][0m #------------------------ Iteration 726 --------------------------#
[32m[20221213 15:09:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0045 |           6.5832 |           0.2388 |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0090 |           6.3169 |           0.2384 |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0112 |           6.2170 |           0.2384 |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0139 |           6.1605 |           0.2381 |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0162 |           6.1189 |           0.2381 |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0177 |           6.0597 |           0.2380 |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0147 |           6.0391 |           0.2378 |
[32m[20221213 15:09:29 @agent_ppo2.py:185][0m |          -0.0190 |           5.9910 |           0.2378 |
[32m[20221213 15:09:30 @agent_ppo2.py:185][0m |          -0.0193 |           6.0334 |           0.2377 |
[32m[20221213 15:09:30 @agent_ppo2.py:185][0m |          -0.0200 |           5.9226 |           0.2378 |
[32m[20221213 15:09:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.96
[32m[20221213 15:09:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 160.72
[32m[20221213 15:09:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.15
[32m[20221213 15:09:30 @agent_ppo2.py:143][0m Total time:      16.58 min
[32m[20221213 15:09:30 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221213 15:09:30 @agent_ppo2.py:121][0m #------------------------ Iteration 727 --------------------------#
[32m[20221213 15:09:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:09:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:30 @agent_ppo2.py:185][0m |           0.0007 |           6.4671 |           0.2419 |
[32m[20221213 15:09:30 @agent_ppo2.py:185][0m |          -0.0059 |           6.3354 |           0.2412 |
[32m[20221213 15:09:30 @agent_ppo2.py:185][0m |          -0.0087 |           6.2414 |           0.2409 |
[32m[20221213 15:09:30 @agent_ppo2.py:185][0m |          -0.0131 |           6.1819 |           0.2409 |
[32m[20221213 15:09:31 @agent_ppo2.py:185][0m |          -0.0096 |           6.1481 |           0.2409 |
[32m[20221213 15:09:31 @agent_ppo2.py:185][0m |          -0.0155 |           6.0534 |           0.2407 |
[32m[20221213 15:09:31 @agent_ppo2.py:185][0m |          -0.0156 |           6.0245 |           0.2405 |
[32m[20221213 15:09:31 @agent_ppo2.py:185][0m |          -0.0182 |           5.9708 |           0.2405 |
[32m[20221213 15:09:31 @agent_ppo2.py:185][0m |          -0.0177 |           5.9578 |           0.2403 |
[32m[20221213 15:09:31 @agent_ppo2.py:185][0m |          -0.0200 |           5.9225 |           0.2404 |
[32m[20221213 15:09:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.32
[32m[20221213 15:09:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 159.93
[32m[20221213 15:09:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.18
[32m[20221213 15:09:31 @agent_ppo2.py:143][0m Total time:      16.60 min
[32m[20221213 15:09:31 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221213 15:09:31 @agent_ppo2.py:121][0m #------------------------ Iteration 728 --------------------------#
[32m[20221213 15:09:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0011 |           6.4565 |           0.2416 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0028 |           6.3482 |           0.2417 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0088 |           6.2472 |           0.2414 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0098 |           6.1945 |           0.2415 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0106 |           6.1583 |           0.2415 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0122 |           6.1247 |           0.2417 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0083 |           6.1524 |           0.2418 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0146 |           6.0499 |           0.2419 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0145 |           6.0346 |           0.2418 |
[32m[20221213 15:09:32 @agent_ppo2.py:185][0m |          -0.0171 |           6.0099 |           0.2419 |
[32m[20221213 15:09:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 142.06
[32m[20221213 15:09:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.48
[32m[20221213 15:09:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.28
[32m[20221213 15:09:32 @agent_ppo2.py:143][0m Total time:      16.62 min
[32m[20221213 15:09:32 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221213 15:09:32 @agent_ppo2.py:121][0m #------------------------ Iteration 729 --------------------------#
[32m[20221213 15:09:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |           0.0036 |           6.2112 |           0.2355 |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |          -0.0065 |           5.9121 |           0.2352 |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |          -0.0130 |           5.7448 |           0.2351 |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |          -0.0131 |           5.6856 |           0.2350 |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |          -0.0169 |           5.6189 |           0.2348 |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |          -0.0185 |           5.5178 |           0.2347 |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |          -0.0168 |           5.4655 |           0.2347 |
[32m[20221213 15:09:33 @agent_ppo2.py:185][0m |          -0.0146 |           5.3964 |           0.2347 |
[32m[20221213 15:09:34 @agent_ppo2.py:185][0m |          -0.0222 |           5.3683 |           0.2347 |
[32m[20221213 15:09:34 @agent_ppo2.py:185][0m |          -0.0255 |           5.3048 |           0.2344 |
[32m[20221213 15:09:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.24
[32m[20221213 15:09:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.17
[32m[20221213 15:09:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 130.04
[32m[20221213 15:09:34 @agent_ppo2.py:143][0m Total time:      16.64 min
[32m[20221213 15:09:34 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221213 15:09:34 @agent_ppo2.py:121][0m #------------------------ Iteration 730 --------------------------#
[32m[20221213 15:09:34 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:09:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:34 @agent_ppo2.py:185][0m |           0.0026 |           5.8812 |           0.2431 |
[32m[20221213 15:09:34 @agent_ppo2.py:185][0m |          -0.0047 |           5.4716 |           0.2429 |
[32m[20221213 15:09:34 @agent_ppo2.py:185][0m |          -0.0088 |           5.3472 |           0.2425 |
[32m[20221213 15:09:34 @agent_ppo2.py:185][0m |          -0.0128 |           5.2691 |           0.2427 |
[32m[20221213 15:09:35 @agent_ppo2.py:185][0m |          -0.0068 |           5.5036 |           0.2425 |
[32m[20221213 15:09:35 @agent_ppo2.py:185][0m |          -0.0156 |           5.1736 |           0.2425 |
[32m[20221213 15:09:35 @agent_ppo2.py:185][0m |          -0.0065 |           5.7262 |           0.2425 |
[32m[20221213 15:09:35 @agent_ppo2.py:185][0m |          -0.0175 |           5.0831 |           0.2422 |
[32m[20221213 15:09:35 @agent_ppo2.py:185][0m |          -0.0154 |           5.0619 |           0.2423 |
[32m[20221213 15:09:35 @agent_ppo2.py:185][0m |          -0.0219 |           5.0294 |           0.2424 |
[32m[20221213 15:09:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.03
[32m[20221213 15:09:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 159.29
[32m[20221213 15:09:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.35
[32m[20221213 15:09:35 @agent_ppo2.py:143][0m Total time:      16.67 min
[32m[20221213 15:09:35 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221213 15:09:35 @agent_ppo2.py:121][0m #------------------------ Iteration 731 --------------------------#
[32m[20221213 15:09:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0012 |           4.7227 |           0.2366 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0108 |           4.5777 |           0.2363 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0119 |           4.5038 |           0.2362 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0122 |           4.4641 |           0.2361 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0096 |           4.5119 |           0.2361 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0187 |           4.3862 |           0.2359 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0176 |           4.3544 |           0.2358 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0137 |           4.4238 |           0.2358 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0178 |           4.3076 |           0.2359 |
[32m[20221213 15:09:36 @agent_ppo2.py:185][0m |          -0.0211 |           4.2830 |           0.2358 |
[32m[20221213 15:09:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.31
[32m[20221213 15:09:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 167.07
[32m[20221213 15:09:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.63
[32m[20221213 15:09:37 @agent_ppo2.py:143][0m Total time:      16.69 min
[32m[20221213 15:09:37 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221213 15:09:37 @agent_ppo2.py:121][0m #------------------------ Iteration 732 --------------------------#
[32m[20221213 15:09:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0021 |           6.4140 |           0.2424 |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0064 |           6.2326 |           0.2416 |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0070 |           6.1326 |           0.2415 |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0125 |           6.0617 |           0.2413 |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0158 |           6.0185 |           0.2413 |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0120 |           5.9642 |           0.2410 |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0160 |           5.9067 |           0.2410 |
[32m[20221213 15:09:37 @agent_ppo2.py:185][0m |          -0.0146 |           5.8742 |           0.2408 |
[32m[20221213 15:09:38 @agent_ppo2.py:185][0m |          -0.0198 |           5.8555 |           0.2408 |
[32m[20221213 15:09:38 @agent_ppo2.py:185][0m |          -0.0204 |           5.7903 |           0.2405 |
[32m[20221213 15:09:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.80
[32m[20221213 15:09:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 166.61
[32m[20221213 15:09:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.65
[32m[20221213 15:09:38 @agent_ppo2.py:143][0m Total time:      16.71 min
[32m[20221213 15:09:38 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221213 15:09:38 @agent_ppo2.py:121][0m #------------------------ Iteration 733 --------------------------#
[32m[20221213 15:09:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:38 @agent_ppo2.py:185][0m |          -0.0035 |           6.4299 |           0.2477 |
[32m[20221213 15:09:38 @agent_ppo2.py:185][0m |          -0.0065 |           6.2693 |           0.2475 |
[32m[20221213 15:09:38 @agent_ppo2.py:185][0m |          -0.0115 |           6.1756 |           0.2470 |
[32m[20221213 15:09:38 @agent_ppo2.py:185][0m |          -0.0068 |           6.2645 |           0.2472 |
[32m[20221213 15:09:39 @agent_ppo2.py:185][0m |          -0.0142 |           6.0672 |           0.2472 |
[32m[20221213 15:09:39 @agent_ppo2.py:185][0m |          -0.0161 |           6.0165 |           0.2472 |
[32m[20221213 15:09:39 @agent_ppo2.py:185][0m |          -0.0106 |           6.1712 |           0.2471 |
[32m[20221213 15:09:39 @agent_ppo2.py:185][0m |          -0.0172 |           5.9277 |           0.2471 |
[32m[20221213 15:09:39 @agent_ppo2.py:185][0m |          -0.0197 |           5.9245 |           0.2473 |
[32m[20221213 15:09:39 @agent_ppo2.py:185][0m |          -0.0175 |           5.8538 |           0.2472 |
[32m[20221213 15:09:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.31
[32m[20221213 15:09:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.83
[32m[20221213 15:09:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 154.64
[32m[20221213 15:09:39 @agent_ppo2.py:143][0m Total time:      16.73 min
[32m[20221213 15:09:39 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221213 15:09:39 @agent_ppo2.py:121][0m #------------------------ Iteration 734 --------------------------#
[32m[20221213 15:09:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |           0.0009 |           6.4425 |           0.2412 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0078 |           6.2336 |           0.2411 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0098 |           6.1454 |           0.2407 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0137 |           6.0750 |           0.2406 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0032 |           6.3611 |           0.2404 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0147 |           6.0160 |           0.2400 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0144 |           5.9981 |           0.2400 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0161 |           5.9291 |           0.2399 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0168 |           5.9055 |           0.2399 |
[32m[20221213 15:09:40 @agent_ppo2.py:185][0m |          -0.0189 |           5.9239 |           0.2396 |
[32m[20221213 15:09:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 130.06
[32m[20221213 15:09:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 140.72
[32m[20221213 15:09:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.03
[32m[20221213 15:09:41 @agent_ppo2.py:143][0m Total time:      16.76 min
[32m[20221213 15:09:41 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221213 15:09:41 @agent_ppo2.py:121][0m #------------------------ Iteration 735 --------------------------#
[32m[20221213 15:09:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:41 @agent_ppo2.py:185][0m |           0.0013 |           6.3708 |           0.2415 |
[32m[20221213 15:09:41 @agent_ppo2.py:185][0m |           0.0036 |           6.3224 |           0.2417 |
[32m[20221213 15:09:41 @agent_ppo2.py:185][0m |          -0.0094 |           5.9859 |           0.2414 |
[32m[20221213 15:09:41 @agent_ppo2.py:185][0m |          -0.0090 |           5.9187 |           0.2414 |
[32m[20221213 15:09:41 @agent_ppo2.py:185][0m |          -0.0159 |           5.8717 |           0.2414 |
[32m[20221213 15:09:41 @agent_ppo2.py:185][0m |          -0.0164 |           5.8340 |           0.2414 |
[32m[20221213 15:09:41 @agent_ppo2.py:185][0m |          -0.0177 |           5.7957 |           0.2416 |
[32m[20221213 15:09:42 @agent_ppo2.py:185][0m |          -0.0188 |           5.7780 |           0.2414 |
[32m[20221213 15:09:42 @agent_ppo2.py:185][0m |          -0.0118 |           6.1822 |           0.2415 |
[32m[20221213 15:09:42 @agent_ppo2.py:185][0m |          -0.0212 |           5.7616 |           0.2416 |
[32m[20221213 15:09:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:09:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 152.05
[32m[20221213 15:09:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 158.83
[32m[20221213 15:09:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 148.15
[32m[20221213 15:09:42 @agent_ppo2.py:143][0m Total time:      16.78 min
[32m[20221213 15:09:42 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221213 15:09:42 @agent_ppo2.py:121][0m #------------------------ Iteration 736 --------------------------#
[32m[20221213 15:09:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:42 @agent_ppo2.py:185][0m |          -0.0027 |           5.3324 |           0.2354 |
[32m[20221213 15:09:42 @agent_ppo2.py:185][0m |          -0.0033 |           5.2756 |           0.2348 |
[32m[20221213 15:09:42 @agent_ppo2.py:185][0m |          -0.0053 |           5.2752 |           0.2347 |
[32m[20221213 15:09:43 @agent_ppo2.py:185][0m |          -0.0095 |           5.2429 |           0.2343 |
[32m[20221213 15:09:43 @agent_ppo2.py:185][0m |          -0.0152 |           5.0615 |           0.2342 |
[32m[20221213 15:09:43 @agent_ppo2.py:185][0m |          -0.0177 |           5.0239 |           0.2343 |
[32m[20221213 15:09:43 @agent_ppo2.py:185][0m |          -0.0126 |           5.2416 |           0.2343 |
[32m[20221213 15:09:43 @agent_ppo2.py:185][0m |          -0.0184 |           4.9735 |           0.2341 |
[32m[20221213 15:09:43 @agent_ppo2.py:185][0m |          -0.0194 |           4.9474 |           0.2341 |
[32m[20221213 15:09:43 @agent_ppo2.py:185][0m |          -0.0196 |           4.9036 |           0.2340 |
[32m[20221213 15:09:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 153.51
[32m[20221213 15:09:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 160.40
[32m[20221213 15:09:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 165.62
[32m[20221213 15:09:43 @agent_ppo2.py:143][0m Total time:      16.80 min
[32m[20221213 15:09:43 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221213 15:09:43 @agent_ppo2.py:121][0m #------------------------ Iteration 737 --------------------------#
[32m[20221213 15:09:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:09:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |           0.0009 |           5.5912 |           0.2446 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0092 |           5.4333 |           0.2439 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0043 |           5.4048 |           0.2438 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0118 |           5.2675 |           0.2435 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0133 |           5.1745 |           0.2435 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0160 |           5.1327 |           0.2433 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0182 |           5.0676 |           0.2431 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0157 |           5.0018 |           0.2429 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0196 |           4.9447 |           0.2428 |
[32m[20221213 15:09:44 @agent_ppo2.py:185][0m |          -0.0185 |           4.9089 |           0.2428 |
[32m[20221213 15:09:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.35
[32m[20221213 15:09:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 157.80
[32m[20221213 15:09:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 159.85
[32m[20221213 15:09:45 @agent_ppo2.py:143][0m Total time:      16.82 min
[32m[20221213 15:09:45 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221213 15:09:45 @agent_ppo2.py:121][0m #------------------------ Iteration 738 --------------------------#
[32m[20221213 15:09:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:45 @agent_ppo2.py:185][0m |           0.0001 |           6.7739 |           0.2346 |
[32m[20221213 15:09:45 @agent_ppo2.py:185][0m |          -0.0078 |           6.4907 |           0.2339 |
[32m[20221213 15:09:45 @agent_ppo2.py:185][0m |          -0.0119 |           6.4232 |           0.2333 |
[32m[20221213 15:09:45 @agent_ppo2.py:185][0m |          -0.0163 |           6.3581 |           0.2334 |
[32m[20221213 15:09:45 @agent_ppo2.py:185][0m |          -0.0124 |           6.3091 |           0.2332 |
[32m[20221213 15:09:45 @agent_ppo2.py:185][0m |          -0.0144 |           6.2650 |           0.2332 |
[32m[20221213 15:09:45 @agent_ppo2.py:185][0m |          -0.0169 |           6.2613 |           0.2330 |
[32m[20221213 15:09:46 @agent_ppo2.py:185][0m |          -0.0121 |           6.2685 |           0.2329 |
[32m[20221213 15:09:46 @agent_ppo2.py:185][0m |          -0.0046 |           6.8531 |           0.2327 |
[32m[20221213 15:09:46 @agent_ppo2.py:185][0m |           0.0012 |           7.4931 |           0.2325 |
[32m[20221213 15:09:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.97
[32m[20221213 15:09:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.16
[32m[20221213 15:09:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 150.15
[32m[20221213 15:09:46 @agent_ppo2.py:143][0m Total time:      16.85 min
[32m[20221213 15:09:46 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221213 15:09:46 @agent_ppo2.py:121][0m #------------------------ Iteration 739 --------------------------#
[32m[20221213 15:09:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:46 @agent_ppo2.py:185][0m |          -0.0064 |           6.3804 |           0.2283 |
[32m[20221213 15:09:46 @agent_ppo2.py:185][0m |          -0.0115 |           6.1855 |           0.2280 |
[32m[20221213 15:09:46 @agent_ppo2.py:185][0m |          -0.0128 |           6.1151 |           0.2278 |
[32m[20221213 15:09:47 @agent_ppo2.py:185][0m |          -0.0153 |           6.0292 |           0.2278 |
[32m[20221213 15:09:47 @agent_ppo2.py:185][0m |          -0.0156 |           5.9997 |           0.2277 |
[32m[20221213 15:09:47 @agent_ppo2.py:185][0m |          -0.0193 |           5.9222 |           0.2275 |
[32m[20221213 15:09:47 @agent_ppo2.py:185][0m |          -0.0190 |           5.8865 |           0.2276 |
[32m[20221213 15:09:47 @agent_ppo2.py:185][0m |          -0.0186 |           5.8092 |           0.2276 |
[32m[20221213 15:09:47 @agent_ppo2.py:185][0m |          -0.0206 |           5.7916 |           0.2276 |
[32m[20221213 15:09:47 @agent_ppo2.py:185][0m |          -0.0229 |           5.7434 |           0.2276 |
[32m[20221213 15:09:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.67
[32m[20221213 15:09:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 168.65
[32m[20221213 15:09:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.77
[32m[20221213 15:09:47 @agent_ppo2.py:143][0m Total time:      16.87 min
[32m[20221213 15:09:47 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221213 15:09:47 @agent_ppo2.py:121][0m #------------------------ Iteration 740 --------------------------#
[32m[20221213 15:09:47 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:09:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |           0.0006 |           5.8320 |           0.2292 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0092 |           5.7201 |           0.2289 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0102 |           5.6677 |           0.2288 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0112 |           5.6008 |           0.2285 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0130 |           5.5899 |           0.2284 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0148 |           5.5338 |           0.2284 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0137 |           5.5164 |           0.2281 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0157 |           5.5089 |           0.2280 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0191 |           5.4723 |           0.2279 |
[32m[20221213 15:09:48 @agent_ppo2.py:185][0m |          -0.0151 |           5.4628 |           0.2280 |
[32m[20221213 15:09:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.97
[32m[20221213 15:09:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.37
[32m[20221213 15:09:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.42
[32m[20221213 15:09:49 @agent_ppo2.py:143][0m Total time:      16.89 min
[32m[20221213 15:09:49 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221213 15:09:49 @agent_ppo2.py:121][0m #------------------------ Iteration 741 --------------------------#
[32m[20221213 15:09:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:49 @agent_ppo2.py:185][0m |          -0.0067 |           6.2831 |           0.2344 |
[32m[20221213 15:09:49 @agent_ppo2.py:185][0m |          -0.0080 |           6.1468 |           0.2339 |
[32m[20221213 15:09:49 @agent_ppo2.py:185][0m |           0.0024 |           6.6716 |           0.2333 |
[32m[20221213 15:09:49 @agent_ppo2.py:185][0m |          -0.0023 |           6.8798 |           0.2332 |
[32m[20221213 15:09:49 @agent_ppo2.py:185][0m |          -0.0141 |           5.9992 |           0.2329 |
[32m[20221213 15:09:49 @agent_ppo2.py:185][0m |          -0.0133 |           5.9363 |           0.2329 |
[32m[20221213 15:09:49 @agent_ppo2.py:185][0m |          -0.0145 |           5.8874 |           0.2327 |
[32m[20221213 15:09:50 @agent_ppo2.py:185][0m |          -0.0159 |           5.8755 |           0.2325 |
[32m[20221213 15:09:50 @agent_ppo2.py:185][0m |          -0.0169 |           5.8497 |           0.2327 |
[32m[20221213 15:09:50 @agent_ppo2.py:185][0m |          -0.0195 |           5.8022 |           0.2326 |
[32m[20221213 15:09:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.38
[32m[20221213 15:09:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 155.03
[32m[20221213 15:09:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.99
[32m[20221213 15:09:50 @agent_ppo2.py:143][0m Total time:      16.91 min
[32m[20221213 15:09:50 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221213 15:09:50 @agent_ppo2.py:121][0m #------------------------ Iteration 742 --------------------------#
[32m[20221213 15:09:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:50 @agent_ppo2.py:185][0m |          -0.0028 |           6.1633 |           0.2302 |
[32m[20221213 15:09:50 @agent_ppo2.py:185][0m |          -0.0092 |           5.9681 |           0.2299 |
[32m[20221213 15:09:50 @agent_ppo2.py:185][0m |          -0.0023 |           6.1958 |           0.2293 |
[32m[20221213 15:09:51 @agent_ppo2.py:185][0m |          -0.0147 |           5.7921 |           0.2293 |
[32m[20221213 15:09:51 @agent_ppo2.py:185][0m |          -0.0156 |           5.6804 |           0.2291 |
[32m[20221213 15:09:51 @agent_ppo2.py:185][0m |          -0.0159 |           5.6087 |           0.2293 |
[32m[20221213 15:09:51 @agent_ppo2.py:185][0m |          -0.0128 |           5.6774 |           0.2291 |
[32m[20221213 15:09:51 @agent_ppo2.py:185][0m |          -0.0178 |           5.5211 |           0.2288 |
[32m[20221213 15:09:51 @agent_ppo2.py:185][0m |          -0.0156 |           5.4708 |           0.2288 |
[32m[20221213 15:09:51 @agent_ppo2.py:185][0m |          -0.0082 |           5.9106 |           0.2289 |
[32m[20221213 15:09:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 143.57
[32m[20221213 15:09:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 157.07
[32m[20221213 15:09:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.17
[32m[20221213 15:09:51 @agent_ppo2.py:143][0m Total time:      16.93 min
[32m[20221213 15:09:51 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221213 15:09:51 @agent_ppo2.py:121][0m #------------------------ Iteration 743 --------------------------#
[32m[20221213 15:09:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0033 |           6.6846 |           0.2332 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0033 |           6.6139 |           0.2324 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0121 |           6.3379 |           0.2324 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0127 |           6.2353 |           0.2324 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0155 |           6.1608 |           0.2321 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0176 |           6.0983 |           0.2321 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0096 |           6.2547 |           0.2320 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0162 |           5.9559 |           0.2320 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0204 |           5.8937 |           0.2317 |
[32m[20221213 15:09:52 @agent_ppo2.py:185][0m |          -0.0214 |           5.8701 |           0.2316 |
[32m[20221213 15:09:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.02
[32m[20221213 15:09:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 158.46
[32m[20221213 15:09:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 138.31
[32m[20221213 15:09:53 @agent_ppo2.py:143][0m Total time:      16.96 min
[32m[20221213 15:09:53 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221213 15:09:53 @agent_ppo2.py:121][0m #------------------------ Iteration 744 --------------------------#
[32m[20221213 15:09:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:53 @agent_ppo2.py:185][0m |          -0.0027 |           6.7759 |           0.2318 |
[32m[20221213 15:09:53 @agent_ppo2.py:185][0m |          -0.0080 |           6.5843 |           0.2315 |
[32m[20221213 15:09:53 @agent_ppo2.py:185][0m |          -0.0107 |           6.5004 |           0.2313 |
[32m[20221213 15:09:53 @agent_ppo2.py:185][0m |          -0.0140 |           6.4416 |           0.2312 |
[32m[20221213 15:09:53 @agent_ppo2.py:185][0m |          -0.0146 |           6.3809 |           0.2309 |
[32m[20221213 15:09:53 @agent_ppo2.py:185][0m |          -0.0178 |           6.3242 |           0.2310 |
[32m[20221213 15:09:54 @agent_ppo2.py:185][0m |          -0.0181 |           6.2987 |           0.2311 |
[32m[20221213 15:09:54 @agent_ppo2.py:185][0m |          -0.0197 |           6.2514 |           0.2310 |
[32m[20221213 15:09:54 @agent_ppo2.py:185][0m |          -0.0195 |           6.2168 |           0.2312 |
[32m[20221213 15:09:54 @agent_ppo2.py:185][0m |          -0.0175 |           6.3644 |           0.2310 |
[32m[20221213 15:09:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:09:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 143.39
[32m[20221213 15:09:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 147.68
[32m[20221213 15:09:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.38
[32m[20221213 15:09:54 @agent_ppo2.py:143][0m Total time:      16.98 min
[32m[20221213 15:09:54 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221213 15:09:54 @agent_ppo2.py:121][0m #------------------------ Iteration 745 --------------------------#
[32m[20221213 15:09:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:54 @agent_ppo2.py:185][0m |          -0.0015 |           6.7235 |           0.2311 |
[32m[20221213 15:09:54 @agent_ppo2.py:185][0m |          -0.0072 |           6.4544 |           0.2311 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0099 |           6.3036 |           0.2312 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0103 |           6.2331 |           0.2310 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0122 |           6.1564 |           0.2310 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0162 |           6.1008 |           0.2310 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0127 |           6.0868 |           0.2309 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0130 |           6.0771 |           0.2309 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0182 |           5.9782 |           0.2310 |
[32m[20221213 15:09:55 @agent_ppo2.py:185][0m |          -0.0177 |           5.9516 |           0.2310 |
[32m[20221213 15:09:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 154.66
[32m[20221213 15:09:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 160.36
[32m[20221213 15:09:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.52
[32m[20221213 15:09:55 @agent_ppo2.py:143][0m Total time:      17.00 min
[32m[20221213 15:09:55 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221213 15:09:55 @agent_ppo2.py:121][0m #------------------------ Iteration 746 --------------------------#
[32m[20221213 15:09:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0025 |           6.5994 |           0.2302 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0093 |           6.3996 |           0.2300 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0066 |           6.3276 |           0.2298 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0103 |           6.2572 |           0.2296 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0146 |           6.2066 |           0.2297 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0130 |           6.1438 |           0.2298 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0145 |           6.0889 |           0.2298 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0118 |           6.0865 |           0.2299 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0053 |           6.4476 |           0.2297 |
[32m[20221213 15:09:56 @agent_ppo2.py:185][0m |          -0.0195 |           5.9523 |           0.2297 |
[32m[20221213 15:09:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.43
[32m[20221213 15:09:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 159.35
[32m[20221213 15:09:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.60
[32m[20221213 15:09:57 @agent_ppo2.py:143][0m Total time:      17.02 min
[32m[20221213 15:09:57 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221213 15:09:57 @agent_ppo2.py:121][0m #------------------------ Iteration 747 --------------------------#
[32m[20221213 15:09:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:57 @agent_ppo2.py:185][0m |          -0.0028 |           6.7086 |           0.2318 |
[32m[20221213 15:09:57 @agent_ppo2.py:185][0m |          -0.0084 |           6.5135 |           0.2314 |
[32m[20221213 15:09:57 @agent_ppo2.py:185][0m |          -0.0115 |           6.4241 |           0.2311 |
[32m[20221213 15:09:57 @agent_ppo2.py:185][0m |          -0.0085 |           6.3798 |           0.2309 |
[32m[20221213 15:09:57 @agent_ppo2.py:185][0m |          -0.0104 |           6.3065 |           0.2306 |
[32m[20221213 15:09:57 @agent_ppo2.py:185][0m |          -0.0114 |           6.2400 |           0.2303 |
[32m[20221213 15:09:58 @agent_ppo2.py:185][0m |          -0.0041 |           6.8743 |           0.2301 |
[32m[20221213 15:09:58 @agent_ppo2.py:185][0m |          -0.0170 |           6.1913 |           0.2300 |
[32m[20221213 15:09:58 @agent_ppo2.py:185][0m |          -0.0189 |           6.1381 |           0.2297 |
[32m[20221213 15:09:58 @agent_ppo2.py:185][0m |          -0.0183 |           6.1279 |           0.2296 |
[32m[20221213 15:09:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 134.87
[32m[20221213 15:09:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 149.43
[32m[20221213 15:09:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 160.35
[32m[20221213 15:09:58 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221213 15:09:58 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221213 15:09:58 @agent_ppo2.py:121][0m #------------------------ Iteration 748 --------------------------#
[32m[20221213 15:09:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:09:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:09:58 @agent_ppo2.py:185][0m |          -0.0002 |           6.6284 |           0.2371 |
[32m[20221213 15:09:58 @agent_ppo2.py:185][0m |          -0.0098 |           6.4494 |           0.2368 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |          -0.0110 |           6.3201 |           0.2365 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |          -0.0089 |           6.2652 |           0.2366 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |           0.0015 |           7.0241 |           0.2367 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |          -0.0136 |           6.1743 |           0.2364 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |          -0.0152 |           6.1184 |           0.2364 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |          -0.0163 |           6.0873 |           0.2366 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |          -0.0165 |           6.0470 |           0.2365 |
[32m[20221213 15:09:59 @agent_ppo2.py:185][0m |          -0.0173 |           6.0252 |           0.2366 |
[32m[20221213 15:09:59 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:09:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 142.79
[32m[20221213 15:09:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 148.01
[32m[20221213 15:09:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 167.30
[32m[20221213 15:09:59 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221213 15:09:59 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221213 15:09:59 @agent_ppo2.py:121][0m #------------------------ Iteration 749 --------------------------#
[32m[20221213 15:10:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |           0.0131 |           7.1902 |           0.2319 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0050 |           6.1676 |           0.2311 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0125 |           6.0659 |           0.2318 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0084 |           6.0896 |           0.2318 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0145 |           5.9553 |           0.2318 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0150 |           5.8950 |           0.2318 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0154 |           5.8668 |           0.2317 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0170 |           5.8235 |           0.2322 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0132 |           5.9124 |           0.2321 |
[32m[20221213 15:10:00 @agent_ppo2.py:185][0m |          -0.0188 |           5.7762 |           0.2322 |
[32m[20221213 15:10:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 138.98
[32m[20221213 15:10:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.76
[32m[20221213 15:10:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.91
[32m[20221213 15:10:01 @agent_ppo2.py:143][0m Total time:      17.09 min
[32m[20221213 15:10:01 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221213 15:10:01 @agent_ppo2.py:121][0m #------------------------ Iteration 750 --------------------------#
[32m[20221213 15:10:01 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:10:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:01 @agent_ppo2.py:185][0m |           0.0010 |           6.3206 |           0.2323 |
[32m[20221213 15:10:01 @agent_ppo2.py:185][0m |          -0.0081 |           6.0683 |           0.2321 |
[32m[20221213 15:10:01 @agent_ppo2.py:185][0m |          -0.0058 |           5.9694 |           0.2319 |
[32m[20221213 15:10:01 @agent_ppo2.py:185][0m |          -0.0082 |           5.9595 |           0.2315 |
[32m[20221213 15:10:01 @agent_ppo2.py:185][0m |          -0.0147 |           5.8672 |           0.2316 |
[32m[20221213 15:10:01 @agent_ppo2.py:185][0m |          -0.0122 |           5.8224 |           0.2315 |
[32m[20221213 15:10:02 @agent_ppo2.py:185][0m |          -0.0162 |           5.7718 |           0.2314 |
[32m[20221213 15:10:02 @agent_ppo2.py:185][0m |          -0.0156 |           5.7321 |           0.2313 |
[32m[20221213 15:10:02 @agent_ppo2.py:185][0m |          -0.0157 |           5.7006 |           0.2313 |
[32m[20221213 15:10:02 @agent_ppo2.py:185][0m |          -0.0188 |           5.6928 |           0.2313 |
[32m[20221213 15:10:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.68
[32m[20221213 15:10:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 162.07
[32m[20221213 15:10:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.24
[32m[20221213 15:10:02 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 182.24
[32m[20221213 15:10:02 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 182.24
[32m[20221213 15:10:02 @agent_ppo2.py:143][0m Total time:      17.11 min
[32m[20221213 15:10:02 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221213 15:10:02 @agent_ppo2.py:121][0m #------------------------ Iteration 751 --------------------------#
[32m[20221213 15:10:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:02 @agent_ppo2.py:185][0m |           0.0039 |           6.4337 |           0.2315 |
[32m[20221213 15:10:02 @agent_ppo2.py:185][0m |          -0.0138 |           5.8425 |           0.2305 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0109 |           5.7570 |           0.2302 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0173 |           5.6384 |           0.2303 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0160 |           5.5909 |           0.2305 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0160 |           5.5269 |           0.2304 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0173 |           5.4948 |           0.2305 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0199 |           5.4523 |           0.2305 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0169 |           5.4221 |           0.2307 |
[32m[20221213 15:10:03 @agent_ppo2.py:185][0m |          -0.0183 |           5.3907 |           0.2306 |
[32m[20221213 15:10:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.95
[32m[20221213 15:10:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.76
[32m[20221213 15:10:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.97
[32m[20221213 15:10:03 @agent_ppo2.py:143][0m Total time:      17.14 min
[32m[20221213 15:10:03 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221213 15:10:03 @agent_ppo2.py:121][0m #------------------------ Iteration 752 --------------------------#
[32m[20221213 15:10:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0029 |           5.9699 |           0.2323 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0050 |           5.4581 |           0.2322 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0115 |           5.3306 |           0.2318 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0083 |           5.4543 |           0.2316 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0132 |           5.1845 |           0.2315 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0141 |           5.1585 |           0.2314 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0160 |           5.0751 |           0.2313 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0171 |           5.0233 |           0.2311 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0191 |           5.0088 |           0.2309 |
[32m[20221213 15:10:04 @agent_ppo2.py:185][0m |          -0.0130 |           5.1555 |           0.2308 |
[32m[20221213 15:10:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.61
[32m[20221213 15:10:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 155.62
[32m[20221213 15:10:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 173.02
[32m[20221213 15:10:05 @agent_ppo2.py:143][0m Total time:      17.16 min
[32m[20221213 15:10:05 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221213 15:10:05 @agent_ppo2.py:121][0m #------------------------ Iteration 753 --------------------------#
[32m[20221213 15:10:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:05 @agent_ppo2.py:185][0m |           0.0045 |           6.8226 |           0.2312 |
[32m[20221213 15:10:05 @agent_ppo2.py:185][0m |          -0.0081 |           6.5718 |           0.2311 |
[32m[20221213 15:10:05 @agent_ppo2.py:185][0m |          -0.0097 |           6.4724 |           0.2308 |
[32m[20221213 15:10:05 @agent_ppo2.py:185][0m |          -0.0100 |           6.4317 |           0.2306 |
[32m[20221213 15:10:05 @agent_ppo2.py:185][0m |          -0.0132 |           6.3795 |           0.2305 |
[32m[20221213 15:10:05 @agent_ppo2.py:185][0m |          -0.0140 |           6.3300 |           0.2306 |
[32m[20221213 15:10:06 @agent_ppo2.py:185][0m |          -0.0087 |           6.3526 |           0.2308 |
[32m[20221213 15:10:06 @agent_ppo2.py:185][0m |          -0.0173 |           6.2709 |           0.2306 |
[32m[20221213 15:10:06 @agent_ppo2.py:185][0m |          -0.0188 |           6.2259 |           0.2305 |
[32m[20221213 15:10:06 @agent_ppo2.py:185][0m |          -0.0174 |           6.1715 |           0.2305 |
[32m[20221213 15:10:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 154.82
[32m[20221213 15:10:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 162.13
[32m[20221213 15:10:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 167.33
[32m[20221213 15:10:06 @agent_ppo2.py:143][0m Total time:      17.18 min
[32m[20221213 15:10:06 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221213 15:10:06 @agent_ppo2.py:121][0m #------------------------ Iteration 754 --------------------------#
[32m[20221213 15:10:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:06 @agent_ppo2.py:185][0m |           0.0089 |           6.9927 |           0.2318 |
[32m[20221213 15:10:06 @agent_ppo2.py:185][0m |          -0.0034 |           6.7072 |           0.2314 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0114 |           6.5542 |           0.2312 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0135 |           6.4908 |           0.2311 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0162 |           6.4348 |           0.2310 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0177 |           6.4067 |           0.2307 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0156 |           6.3640 |           0.2306 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0182 |           6.3304 |           0.2304 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0184 |           6.2982 |           0.2301 |
[32m[20221213 15:10:07 @agent_ppo2.py:185][0m |          -0.0192 |           6.2446 |           0.2299 |
[32m[20221213 15:10:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 151.08
[32m[20221213 15:10:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 164.20
[32m[20221213 15:10:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.86
[32m[20221213 15:10:07 @agent_ppo2.py:143][0m Total time:      17.20 min
[32m[20221213 15:10:07 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221213 15:10:07 @agent_ppo2.py:121][0m #------------------------ Iteration 755 --------------------------#
[32m[20221213 15:10:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0011 |           6.6353 |           0.2271 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0071 |           6.4159 |           0.2269 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0108 |           6.2950 |           0.2268 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0114 |           6.1901 |           0.2267 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0129 |           6.0787 |           0.2268 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0154 |           6.0147 |           0.2268 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0177 |           5.9346 |           0.2270 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0156 |           5.8551 |           0.2271 |
[32m[20221213 15:10:08 @agent_ppo2.py:185][0m |          -0.0168 |           5.7887 |           0.2269 |
[32m[20221213 15:10:09 @agent_ppo2.py:185][0m |          -0.0206 |           5.7314 |           0.2272 |
[32m[20221213 15:10:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:10:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.82
[32m[20221213 15:10:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.41
[32m[20221213 15:10:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 157.98
[32m[20221213 15:10:09 @agent_ppo2.py:143][0m Total time:      17.23 min
[32m[20221213 15:10:09 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221213 15:10:09 @agent_ppo2.py:121][0m #------------------------ Iteration 756 --------------------------#
[32m[20221213 15:10:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:09 @agent_ppo2.py:185][0m |           0.0022 |           6.1685 |           0.2341 |
[32m[20221213 15:10:09 @agent_ppo2.py:185][0m |          -0.0083 |           5.5157 |           0.2338 |
[32m[20221213 15:10:09 @agent_ppo2.py:185][0m |          -0.0090 |           5.3275 |           0.2334 |
[32m[20221213 15:10:09 @agent_ppo2.py:185][0m |          -0.0144 |           5.1999 |           0.2334 |
[32m[20221213 15:10:09 @agent_ppo2.py:185][0m |          -0.0132 |           5.0541 |           0.2332 |
[32m[20221213 15:10:10 @agent_ppo2.py:185][0m |          -0.0155 |           4.9522 |           0.2331 |
[32m[20221213 15:10:10 @agent_ppo2.py:185][0m |          -0.0174 |           4.8970 |           0.2332 |
[32m[20221213 15:10:10 @agent_ppo2.py:185][0m |          -0.0200 |           4.8441 |           0.2331 |
[32m[20221213 15:10:10 @agent_ppo2.py:185][0m |          -0.0178 |           4.7944 |           0.2330 |
[32m[20221213 15:10:10 @agent_ppo2.py:185][0m |          -0.0125 |           4.8722 |           0.2330 |
[32m[20221213 15:10:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:10:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.45
[32m[20221213 15:10:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 154.61
[32m[20221213 15:10:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 141.22
[32m[20221213 15:10:10 @agent_ppo2.py:143][0m Total time:      17.25 min
[32m[20221213 15:10:10 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221213 15:10:10 @agent_ppo2.py:121][0m #------------------------ Iteration 757 --------------------------#
[32m[20221213 15:10:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0028 |           6.9230 |           0.2336 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0090 |           6.6536 |           0.2336 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0105 |           6.5286 |           0.2334 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0126 |           6.5300 |           0.2335 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0155 |           6.3735 |           0.2336 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0157 |           6.3214 |           0.2336 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0184 |           6.2477 |           0.2336 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0185 |           6.1958 |           0.2335 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0173 |           6.1549 |           0.2338 |
[32m[20221213 15:10:11 @agent_ppo2.py:185][0m |          -0.0186 |           6.1061 |           0.2337 |
[32m[20221213 15:10:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:10:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 149.95
[32m[20221213 15:10:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 158.91
[32m[20221213 15:10:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.57
[32m[20221213 15:10:12 @agent_ppo2.py:143][0m Total time:      17.27 min
[32m[20221213 15:10:12 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221213 15:10:12 @agent_ppo2.py:121][0m #------------------------ Iteration 758 --------------------------#
[32m[20221213 15:10:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |           0.0019 |           6.8128 |           0.2279 |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |          -0.0012 |           6.6355 |           0.2279 |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |          -0.0091 |           6.4410 |           0.2279 |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |          -0.0141 |           6.3809 |           0.2279 |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |          -0.0157 |           6.3124 |           0.2279 |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |          -0.0058 |           7.1431 |           0.2277 |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |          -0.0177 |           6.2221 |           0.2275 |
[32m[20221213 15:10:12 @agent_ppo2.py:185][0m |          -0.0184 |           6.1712 |           0.2277 |
[32m[20221213 15:10:13 @agent_ppo2.py:185][0m |          -0.0178 |           6.1344 |           0.2276 |
[32m[20221213 15:10:13 @agent_ppo2.py:185][0m |          -0.0200 |           6.1063 |           0.2277 |
[32m[20221213 15:10:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 137.84
[32m[20221213 15:10:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 153.86
[32m[20221213 15:10:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.55
[32m[20221213 15:10:13 @agent_ppo2.py:143][0m Total time:      17.29 min
[32m[20221213 15:10:13 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221213 15:10:13 @agent_ppo2.py:121][0m #------------------------ Iteration 759 --------------------------#
[32m[20221213 15:10:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:13 @agent_ppo2.py:185][0m |          -0.0005 |           6.7948 |           0.2323 |
[32m[20221213 15:10:13 @agent_ppo2.py:185][0m |          -0.0083 |           6.5820 |           0.2322 |
[32m[20221213 15:10:13 @agent_ppo2.py:185][0m |          -0.0126 |           6.4425 |           0.2324 |
[32m[20221213 15:10:13 @agent_ppo2.py:185][0m |          -0.0139 |           6.3419 |           0.2325 |
[32m[20221213 15:10:14 @agent_ppo2.py:185][0m |          -0.0149 |           6.2835 |           0.2326 |
[32m[20221213 15:10:14 @agent_ppo2.py:185][0m |          -0.0155 |           6.2215 |           0.2327 |
[32m[20221213 15:10:14 @agent_ppo2.py:185][0m |          -0.0082 |           6.4981 |           0.2330 |
[32m[20221213 15:10:14 @agent_ppo2.py:185][0m |          -0.0192 |           6.0958 |           0.2330 |
[32m[20221213 15:10:14 @agent_ppo2.py:185][0m |          -0.0201 |           6.0516 |           0.2333 |
[32m[20221213 15:10:14 @agent_ppo2.py:185][0m |          -0.0225 |           5.9870 |           0.2333 |
[32m[20221213 15:10:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 142.61
[32m[20221213 15:10:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 154.03
[32m[20221213 15:10:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.04
[32m[20221213 15:10:14 @agent_ppo2.py:143][0m Total time:      17.32 min
[32m[20221213 15:10:14 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221213 15:10:14 @agent_ppo2.py:121][0m #------------------------ Iteration 760 --------------------------#
[32m[20221213 15:10:14 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:10:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0003 |           6.8314 |           0.2385 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0046 |           6.6165 |           0.2384 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0097 |           6.4904 |           0.2385 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0016 |           7.0580 |           0.2386 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0125 |           6.2913 |           0.2384 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0158 |           6.2416 |           0.2384 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0147 |           6.1669 |           0.2383 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0186 |           6.1260 |           0.2385 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0155 |           6.1214 |           0.2386 |
[32m[20221213 15:10:15 @agent_ppo2.py:185][0m |          -0.0205 |           6.0595 |           0.2387 |
[32m[20221213 15:10:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 149.93
[32m[20221213 15:10:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.54
[32m[20221213 15:10:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 161.20
[32m[20221213 15:10:16 @agent_ppo2.py:143][0m Total time:      17.34 min
[32m[20221213 15:10:16 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221213 15:10:16 @agent_ppo2.py:121][0m #------------------------ Iteration 761 --------------------------#
[32m[20221213 15:10:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |           0.0068 |           6.9504 |           0.2331 |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |           0.0013 |           7.0672 |           0.2327 |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |          -0.0013 |           6.7552 |           0.2324 |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |          -0.0121 |           6.4906 |           0.2321 |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |          -0.0118 |           6.4588 |           0.2321 |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |          -0.0121 |           6.4283 |           0.2320 |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |          -0.0170 |           6.4049 |           0.2320 |
[32m[20221213 15:10:16 @agent_ppo2.py:185][0m |          -0.0151 |           6.3847 |           0.2318 |
[32m[20221213 15:10:17 @agent_ppo2.py:185][0m |          -0.0134 |           6.4452 |           0.2317 |
[32m[20221213 15:10:17 @agent_ppo2.py:185][0m |          -0.0187 |           6.3374 |           0.2314 |
[32m[20221213 15:10:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 148.14
[32m[20221213 15:10:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.28
[32m[20221213 15:10:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.65
[32m[20221213 15:10:17 @agent_ppo2.py:143][0m Total time:      17.36 min
[32m[20221213 15:10:17 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221213 15:10:17 @agent_ppo2.py:121][0m #------------------------ Iteration 762 --------------------------#
[32m[20221213 15:10:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:17 @agent_ppo2.py:185][0m |          -0.0026 |           6.2607 |           0.2342 |
[32m[20221213 15:10:17 @agent_ppo2.py:185][0m |          -0.0078 |           5.9418 |           0.2331 |
[32m[20221213 15:10:17 @agent_ppo2.py:185][0m |          -0.0115 |           5.7533 |           0.2328 |
[32m[20221213 15:10:17 @agent_ppo2.py:185][0m |          -0.0019 |           5.8937 |           0.2323 |
[32m[20221213 15:10:18 @agent_ppo2.py:185][0m |          -0.0129 |           5.5560 |           0.2320 |
[32m[20221213 15:10:18 @agent_ppo2.py:185][0m |          -0.0136 |           5.4762 |           0.2319 |
[32m[20221213 15:10:18 @agent_ppo2.py:185][0m |          -0.0180 |           5.4123 |           0.2318 |
[32m[20221213 15:10:18 @agent_ppo2.py:185][0m |          -0.0149 |           5.3569 |           0.2317 |
[32m[20221213 15:10:18 @agent_ppo2.py:185][0m |          -0.0180 |           5.3159 |           0.2315 |
[32m[20221213 15:10:18 @agent_ppo2.py:185][0m |          -0.0189 |           5.2752 |           0.2315 |
[32m[20221213 15:10:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 153.52
[32m[20221213 15:10:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 171.69
[32m[20221213 15:10:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.56
[32m[20221213 15:10:18 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 182.56
[32m[20221213 15:10:18 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 182.56
[32m[20221213 15:10:18 @agent_ppo2.py:143][0m Total time:      17.38 min
[32m[20221213 15:10:18 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221213 15:10:18 @agent_ppo2.py:121][0m #------------------------ Iteration 763 --------------------------#
[32m[20221213 15:10:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |           0.0049 |           6.8750 |           0.2364 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0068 |           6.1909 |           0.2358 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0071 |           6.1651 |           0.2356 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0147 |           6.0382 |           0.2354 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0161 |           5.9635 |           0.2355 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0162 |           5.9126 |           0.2353 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0040 |           6.4465 |           0.2352 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0176 |           5.8674 |           0.2352 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0190 |           5.8146 |           0.2351 |
[32m[20221213 15:10:19 @agent_ppo2.py:185][0m |          -0.0139 |           6.1217 |           0.2352 |
[32m[20221213 15:10:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 151.41
[32m[20221213 15:10:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 167.66
[32m[20221213 15:10:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 157.90
[32m[20221213 15:10:20 @agent_ppo2.py:143][0m Total time:      17.41 min
[32m[20221213 15:10:20 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221213 15:10:20 @agent_ppo2.py:121][0m #------------------------ Iteration 764 --------------------------#
[32m[20221213 15:10:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:20 @agent_ppo2.py:185][0m |           0.0006 |           6.4316 |           0.2302 |
[32m[20221213 15:10:20 @agent_ppo2.py:185][0m |          -0.0086 |           6.1394 |           0.2301 |
[32m[20221213 15:10:20 @agent_ppo2.py:185][0m |          -0.0054 |           6.2350 |           0.2299 |
[32m[20221213 15:10:20 @agent_ppo2.py:185][0m |          -0.0100 |           5.8033 |           0.2299 |
[32m[20221213 15:10:20 @agent_ppo2.py:185][0m |          -0.0145 |           5.6909 |           0.2297 |
[32m[20221213 15:10:20 @agent_ppo2.py:185][0m |          -0.0158 |           5.5877 |           0.2296 |
[32m[20221213 15:10:20 @agent_ppo2.py:185][0m |          -0.0168 |           5.4947 |           0.2296 |
[32m[20221213 15:10:21 @agent_ppo2.py:185][0m |          -0.0127 |           5.4162 |           0.2296 |
[32m[20221213 15:10:21 @agent_ppo2.py:185][0m |          -0.0194 |           5.3643 |           0.2294 |
[32m[20221213 15:10:21 @agent_ppo2.py:185][0m |          -0.0165 |           5.2824 |           0.2294 |
[32m[20221213 15:10:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.23
[32m[20221213 15:10:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 163.27
[32m[20221213 15:10:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 171.74
[32m[20221213 15:10:21 @agent_ppo2.py:143][0m Total time:      17.43 min
[32m[20221213 15:10:21 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221213 15:10:21 @agent_ppo2.py:121][0m #------------------------ Iteration 765 --------------------------#
[32m[20221213 15:10:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:21 @agent_ppo2.py:185][0m |          -0.0011 |           7.0710 |           0.2326 |
[32m[20221213 15:10:21 @agent_ppo2.py:185][0m |          -0.0069 |           6.8795 |           0.2318 |
[32m[20221213 15:10:21 @agent_ppo2.py:185][0m |          -0.0105 |           6.8223 |           0.2317 |
[32m[20221213 15:10:22 @agent_ppo2.py:185][0m |          -0.0118 |           6.7824 |           0.2317 |
[32m[20221213 15:10:22 @agent_ppo2.py:185][0m |          -0.0140 |           6.7347 |           0.2313 |
[32m[20221213 15:10:22 @agent_ppo2.py:185][0m |          -0.0133 |           6.6995 |           0.2312 |
[32m[20221213 15:10:22 @agent_ppo2.py:185][0m |          -0.0135 |           6.7150 |           0.2309 |
[32m[20221213 15:10:22 @agent_ppo2.py:185][0m |          -0.0184 |           6.6518 |           0.2309 |
[32m[20221213 15:10:22 @agent_ppo2.py:185][0m |          -0.0125 |           6.9135 |           0.2308 |
[32m[20221213 15:10:22 @agent_ppo2.py:185][0m |          -0.0209 |           6.6239 |           0.2306 |
[32m[20221213 15:10:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 153.68
[32m[20221213 15:10:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.09
[32m[20221213 15:10:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 184.63
[32m[20221213 15:10:22 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 184.63
[32m[20221213 15:10:22 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 184.63
[32m[20221213 15:10:22 @agent_ppo2.py:143][0m Total time:      17.45 min
[32m[20221213 15:10:22 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221213 15:10:22 @agent_ppo2.py:121][0m #------------------------ Iteration 766 --------------------------#
[32m[20221213 15:10:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0000 |           7.0941 |           0.2236 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0071 |           6.9382 |           0.2238 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0121 |           6.8367 |           0.2237 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0145 |           6.7827 |           0.2236 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0140 |           6.7005 |           0.2234 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0156 |           6.6388 |           0.2234 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0178 |           6.5858 |           0.2233 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0219 |           6.5842 |           0.2233 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0203 |           6.4975 |           0.2232 |
[32m[20221213 15:10:23 @agent_ppo2.py:185][0m |          -0.0209 |           6.4537 |           0.2232 |
[32m[20221213 15:10:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.50
[32m[20221213 15:10:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 156.84
[32m[20221213 15:10:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 187.41
[32m[20221213 15:10:24 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 187.41
[32m[20221213 15:10:24 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 187.41
[32m[20221213 15:10:24 @agent_ppo2.py:143][0m Total time:      17.47 min
[32m[20221213 15:10:24 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221213 15:10:24 @agent_ppo2.py:121][0m #------------------------ Iteration 767 --------------------------#
[32m[20221213 15:10:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:24 @agent_ppo2.py:185][0m |          -0.0020 |           6.9084 |           0.2271 |
[32m[20221213 15:10:24 @agent_ppo2.py:185][0m |          -0.0092 |           6.6349 |           0.2269 |
[32m[20221213 15:10:24 @agent_ppo2.py:185][0m |          -0.0150 |           6.5125 |           0.2268 |
[32m[20221213 15:10:24 @agent_ppo2.py:185][0m |          -0.0084 |           6.4358 |           0.2265 |
[32m[20221213 15:10:24 @agent_ppo2.py:185][0m |          -0.0019 |           6.8915 |           0.2265 |
[32m[20221213 15:10:24 @agent_ppo2.py:185][0m |          -0.0151 |           6.2428 |           0.2263 |
[32m[20221213 15:10:24 @agent_ppo2.py:185][0m |          -0.0153 |           6.1734 |           0.2264 |
[32m[20221213 15:10:25 @agent_ppo2.py:185][0m |          -0.0142 |           6.1647 |           0.2262 |
[32m[20221213 15:10:25 @agent_ppo2.py:185][0m |          -0.0100 |           6.5189 |           0.2261 |
[32m[20221213 15:10:25 @agent_ppo2.py:185][0m |          -0.0189 |           6.0189 |           0.2260 |
[32m[20221213 15:10:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 143.03
[32m[20221213 15:10:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 164.53
[32m[20221213 15:10:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 175.26
[32m[20221213 15:10:25 @agent_ppo2.py:143][0m Total time:      17.50 min
[32m[20221213 15:10:25 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221213 15:10:25 @agent_ppo2.py:121][0m #------------------------ Iteration 768 --------------------------#
[32m[20221213 15:10:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:25 @agent_ppo2.py:185][0m |          -0.0013 |           6.7042 |           0.2274 |
[32m[20221213 15:10:25 @agent_ppo2.py:185][0m |          -0.0111 |           6.4347 |           0.2273 |
[32m[20221213 15:10:25 @agent_ppo2.py:185][0m |          -0.0130 |           6.2617 |           0.2271 |
[32m[20221213 15:10:26 @agent_ppo2.py:185][0m |          -0.0147 |           6.1632 |           0.2272 |
[32m[20221213 15:10:26 @agent_ppo2.py:185][0m |          -0.0118 |           6.1915 |           0.2273 |
[32m[20221213 15:10:26 @agent_ppo2.py:185][0m |          -0.0147 |           5.9960 |           0.2274 |
[32m[20221213 15:10:26 @agent_ppo2.py:185][0m |          -0.0146 |           5.9820 |           0.2275 |
[32m[20221213 15:10:26 @agent_ppo2.py:185][0m |          -0.0194 |           5.8680 |           0.2275 |
[32m[20221213 15:10:26 @agent_ppo2.py:185][0m |          -0.0217 |           5.7636 |           0.2277 |
[32m[20221213 15:10:26 @agent_ppo2.py:185][0m |          -0.0206 |           5.7127 |           0.2278 |
[32m[20221213 15:10:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.62
[32m[20221213 15:10:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 162.27
[32m[20221213 15:10:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 179.27
[32m[20221213 15:10:26 @agent_ppo2.py:143][0m Total time:      17.52 min
[32m[20221213 15:10:26 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221213 15:10:26 @agent_ppo2.py:121][0m #------------------------ Iteration 769 --------------------------#
[32m[20221213 15:10:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0033 |           6.6893 |           0.2351 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0066 |           6.4543 |           0.2349 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0036 |           6.6200 |           0.2345 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0078 |           6.3264 |           0.2343 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0111 |           6.2111 |           0.2345 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0048 |           6.7776 |           0.2340 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0137 |           6.0427 |           0.2343 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0188 |           5.9972 |           0.2340 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0185 |           5.9490 |           0.2339 |
[32m[20221213 15:10:27 @agent_ppo2.py:185][0m |          -0.0066 |           6.5455 |           0.2338 |
[32m[20221213 15:10:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 158.90
[32m[20221213 15:10:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 161.57
[32m[20221213 15:10:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.56
[32m[20221213 15:10:28 @agent_ppo2.py:143][0m Total time:      17.54 min
[32m[20221213 15:10:28 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221213 15:10:28 @agent_ppo2.py:121][0m #------------------------ Iteration 770 --------------------------#
[32m[20221213 15:10:28 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:10:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:28 @agent_ppo2.py:185][0m |           0.0016 |           7.2564 |           0.2280 |
[32m[20221213 15:10:28 @agent_ppo2.py:185][0m |          -0.0052 |           6.9829 |           0.2274 |
[32m[20221213 15:10:28 @agent_ppo2.py:185][0m |          -0.0068 |           6.9016 |           0.2276 |
[32m[20221213 15:10:28 @agent_ppo2.py:185][0m |          -0.0088 |           6.8599 |           0.2272 |
[32m[20221213 15:10:28 @agent_ppo2.py:185][0m |          -0.0116 |           6.8199 |           0.2271 |
[32m[20221213 15:10:28 @agent_ppo2.py:185][0m |          -0.0130 |           6.7679 |           0.2271 |
[32m[20221213 15:10:28 @agent_ppo2.py:185][0m |          -0.0049 |           6.9702 |           0.2270 |
[32m[20221213 15:10:29 @agent_ppo2.py:185][0m |          -0.0141 |           6.7139 |           0.2266 |
[32m[20221213 15:10:29 @agent_ppo2.py:185][0m |          -0.0144 |           6.6960 |           0.2268 |
[32m[20221213 15:10:29 @agent_ppo2.py:185][0m |          -0.0151 |           6.6645 |           0.2267 |
[32m[20221213 15:10:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 159.83
[32m[20221213 15:10:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.89
[32m[20221213 15:10:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 196.00
[32m[20221213 15:10:29 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 196.00
[32m[20221213 15:10:29 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 196.00
[32m[20221213 15:10:29 @agent_ppo2.py:143][0m Total time:      17.56 min
[32m[20221213 15:10:29 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221213 15:10:29 @agent_ppo2.py:121][0m #------------------------ Iteration 771 --------------------------#
[32m[20221213 15:10:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:29 @agent_ppo2.py:185][0m |           0.0005 |           6.8208 |           0.2327 |
[32m[20221213 15:10:29 @agent_ppo2.py:185][0m |          -0.0060 |           6.5136 |           0.2327 |
[32m[20221213 15:10:29 @agent_ppo2.py:185][0m |          -0.0133 |           6.3754 |           0.2325 |
[32m[20221213 15:10:30 @agent_ppo2.py:185][0m |          -0.0143 |           6.2727 |           0.2325 |
[32m[20221213 15:10:30 @agent_ppo2.py:185][0m |          -0.0158 |           6.2094 |           0.2322 |
[32m[20221213 15:10:30 @agent_ppo2.py:185][0m |          -0.0157 |           6.1396 |           0.2322 |
[32m[20221213 15:10:30 @agent_ppo2.py:185][0m |          -0.0169 |           6.0628 |           0.2320 |
[32m[20221213 15:10:30 @agent_ppo2.py:185][0m |          -0.0169 |           6.0187 |           0.2319 |
[32m[20221213 15:10:30 @agent_ppo2.py:185][0m |          -0.0207 |           5.9482 |           0.2317 |
[32m[20221213 15:10:30 @agent_ppo2.py:185][0m |          -0.0199 |           5.9204 |           0.2319 |
[32m[20221213 15:10:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.32
[32m[20221213 15:10:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 163.14
[32m[20221213 15:10:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.96
[32m[20221213 15:10:30 @agent_ppo2.py:143][0m Total time:      17.58 min
[32m[20221213 15:10:30 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221213 15:10:30 @agent_ppo2.py:121][0m #------------------------ Iteration 772 --------------------------#
[32m[20221213 15:10:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |           0.0087 |           7.3084 |           0.2277 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0115 |           6.6898 |           0.2272 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0113 |           6.5751 |           0.2270 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0117 |           6.5012 |           0.2268 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0153 |           6.4315 |           0.2267 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0148 |           6.4058 |           0.2267 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0188 |           6.3355 |           0.2265 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0201 |           6.3041 |           0.2263 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0198 |           6.2461 |           0.2262 |
[32m[20221213 15:10:31 @agent_ppo2.py:185][0m |          -0.0209 |           6.2157 |           0.2261 |
[32m[20221213 15:10:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 158.52
[32m[20221213 15:10:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.27
[32m[20221213 15:10:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.99
[32m[20221213 15:10:32 @agent_ppo2.py:143][0m Total time:      17.61 min
[32m[20221213 15:10:32 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221213 15:10:32 @agent_ppo2.py:121][0m #------------------------ Iteration 773 --------------------------#
[32m[20221213 15:10:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:32 @agent_ppo2.py:185][0m |           0.0126 |           7.1348 |           0.2255 |
[32m[20221213 15:10:32 @agent_ppo2.py:185][0m |          -0.0049 |           6.3412 |           0.2249 |
[32m[20221213 15:10:32 @agent_ppo2.py:185][0m |          -0.0106 |           6.1037 |           0.2247 |
[32m[20221213 15:10:32 @agent_ppo2.py:185][0m |          -0.0152 |           5.9273 |           0.2247 |
[32m[20221213 15:10:32 @agent_ppo2.py:185][0m |          -0.0032 |           6.3776 |           0.2247 |
[32m[20221213 15:10:32 @agent_ppo2.py:185][0m |          -0.0082 |           5.9076 |           0.2244 |
[32m[20221213 15:10:32 @agent_ppo2.py:185][0m |          -0.0147 |           5.5344 |           0.2246 |
[32m[20221213 15:10:33 @agent_ppo2.py:185][0m |          -0.0192 |           5.4352 |           0.2246 |
[32m[20221213 15:10:33 @agent_ppo2.py:185][0m |          -0.0168 |           5.3656 |           0.2246 |
[32m[20221213 15:10:33 @agent_ppo2.py:185][0m |          -0.0078 |           5.3806 |           0.2246 |
[32m[20221213 15:10:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 158.40
[32m[20221213 15:10:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.85
[32m[20221213 15:10:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 169.68
[32m[20221213 15:10:33 @agent_ppo2.py:143][0m Total time:      17.63 min
[32m[20221213 15:10:33 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221213 15:10:33 @agent_ppo2.py:121][0m #------------------------ Iteration 774 --------------------------#
[32m[20221213 15:10:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:33 @agent_ppo2.py:185][0m |          -0.0001 |           7.4485 |           0.2215 |
[32m[20221213 15:10:33 @agent_ppo2.py:185][0m |          -0.0091 |           7.0481 |           0.2212 |
[32m[20221213 15:10:33 @agent_ppo2.py:185][0m |          -0.0088 |           6.9356 |           0.2210 |
[32m[20221213 15:10:34 @agent_ppo2.py:185][0m |          -0.0124 |           6.8279 |           0.2210 |
[32m[20221213 15:10:34 @agent_ppo2.py:185][0m |          -0.0103 |           7.0894 |           0.2209 |
[32m[20221213 15:10:34 @agent_ppo2.py:185][0m |          -0.0177 |           6.7067 |           0.2207 |
[32m[20221213 15:10:34 @agent_ppo2.py:185][0m |          -0.0105 |           6.8908 |           0.2211 |
[32m[20221213 15:10:34 @agent_ppo2.py:185][0m |          -0.0180 |           6.6349 |           0.2208 |
[32m[20221213 15:10:34 @agent_ppo2.py:185][0m |          -0.0166 |           6.5494 |           0.2209 |
[32m[20221213 15:10:34 @agent_ppo2.py:185][0m |          -0.0204 |           6.5282 |           0.2209 |
[32m[20221213 15:10:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 156.76
[32m[20221213 15:10:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 171.37
[32m[20221213 15:10:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 127.15
[32m[20221213 15:10:34 @agent_ppo2.py:143][0m Total time:      17.65 min
[32m[20221213 15:10:34 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221213 15:10:34 @agent_ppo2.py:121][0m #------------------------ Iteration 775 --------------------------#
[32m[20221213 15:10:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0017 |           6.3702 |           0.2253 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |           0.0028 |           6.1520 |           0.2250 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0007 |           6.1334 |           0.2248 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |           0.0002 |           6.0410 |           0.2249 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0005 |           5.9174 |           0.2246 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0100 |           5.5191 |           0.2246 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0148 |           5.4517 |           0.2246 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0165 |           5.4285 |           0.2246 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0161 |           5.3582 |           0.2243 |
[32m[20221213 15:10:35 @agent_ppo2.py:185][0m |          -0.0184 |           5.3514 |           0.2244 |
[32m[20221213 15:10:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 160.48
[32m[20221213 15:10:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 175.62
[32m[20221213 15:10:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.12
[32m[20221213 15:10:36 @agent_ppo2.py:143][0m Total time:      17.67 min
[32m[20221213 15:10:36 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221213 15:10:36 @agent_ppo2.py:121][0m #------------------------ Iteration 776 --------------------------#
[32m[20221213 15:10:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:36 @agent_ppo2.py:185][0m |           0.0156 |           8.6501 |           0.2334 |
[32m[20221213 15:10:36 @agent_ppo2.py:185][0m |          -0.0098 |           7.4872 |           0.2333 |
[32m[20221213 15:10:36 @agent_ppo2.py:185][0m |          -0.0118 |           7.3287 |           0.2332 |
[32m[20221213 15:10:36 @agent_ppo2.py:185][0m |          -0.0141 |           7.2287 |           0.2331 |
[32m[20221213 15:10:36 @agent_ppo2.py:185][0m |          -0.0129 |           7.1662 |           0.2330 |
[32m[20221213 15:10:36 @agent_ppo2.py:185][0m |          -0.0062 |           7.7710 |           0.2328 |
[32m[20221213 15:10:36 @agent_ppo2.py:185][0m |          -0.0044 |           7.7757 |           0.2325 |
[32m[20221213 15:10:37 @agent_ppo2.py:185][0m |          -0.0212 |           7.0588 |           0.2323 |
[32m[20221213 15:10:37 @agent_ppo2.py:185][0m |          -0.0212 |           6.9930 |           0.2322 |
[32m[20221213 15:10:37 @agent_ppo2.py:185][0m |          -0.0202 |           6.9523 |           0.2322 |
[32m[20221213 15:10:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.52
[32m[20221213 15:10:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 157.70
[32m[20221213 15:10:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.75
[32m[20221213 15:10:37 @agent_ppo2.py:143][0m Total time:      17.70 min
[32m[20221213 15:10:37 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221213 15:10:37 @agent_ppo2.py:121][0m #------------------------ Iteration 777 --------------------------#
[32m[20221213 15:10:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:37 @agent_ppo2.py:185][0m |          -0.0010 |           7.8067 |           0.2265 |
[32m[20221213 15:10:37 @agent_ppo2.py:185][0m |          -0.0089 |           7.5018 |           0.2258 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0127 |           7.4604 |           0.2258 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0150 |           7.4064 |           0.2255 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0132 |           7.3586 |           0.2255 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0145 |           7.3215 |           0.2255 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0065 |           7.8887 |           0.2253 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0153 |           7.2982 |           0.2252 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0199 |           7.2506 |           0.2253 |
[32m[20221213 15:10:38 @agent_ppo2.py:185][0m |          -0.0177 |           7.2798 |           0.2253 |
[32m[20221213 15:10:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 153.55
[32m[20221213 15:10:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 161.51
[32m[20221213 15:10:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.43
[32m[20221213 15:10:38 @agent_ppo2.py:143][0m Total time:      17.72 min
[32m[20221213 15:10:38 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221213 15:10:38 @agent_ppo2.py:121][0m #------------------------ Iteration 778 --------------------------#
[32m[20221213 15:10:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0036 |           6.3544 |           0.2175 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0063 |           5.9083 |           0.2174 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0093 |           5.7220 |           0.2175 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0089 |           5.6158 |           0.2174 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0103 |           5.5342 |           0.2173 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0157 |           5.4450 |           0.2173 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0166 |           5.3442 |           0.2175 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0168 |           5.2725 |           0.2174 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0208 |           5.2199 |           0.2174 |
[32m[20221213 15:10:39 @agent_ppo2.py:185][0m |          -0.0209 |           5.1794 |           0.2175 |
[32m[20221213 15:10:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 157.94
[32m[20221213 15:10:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 166.14
[32m[20221213 15:10:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 157.37
[32m[20221213 15:10:40 @agent_ppo2.py:143][0m Total time:      17.74 min
[32m[20221213 15:10:40 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221213 15:10:40 @agent_ppo2.py:121][0m #------------------------ Iteration 779 --------------------------#
[32m[20221213 15:10:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:40 @agent_ppo2.py:185][0m |          -0.0061 |           8.5943 |           0.2194 |
[32m[20221213 15:10:40 @agent_ppo2.py:185][0m |          -0.0134 |           7.9747 |           0.2191 |
[32m[20221213 15:10:40 @agent_ppo2.py:185][0m |          -0.0149 |           7.7759 |           0.2187 |
[32m[20221213 15:10:40 @agent_ppo2.py:185][0m |          -0.0067 |           8.7334 |           0.2190 |
[32m[20221213 15:10:40 @agent_ppo2.py:185][0m |          -0.0178 |           7.6116 |           0.2186 |
[32m[20221213 15:10:40 @agent_ppo2.py:185][0m |          -0.0243 |           7.4459 |           0.2189 |
[32m[20221213 15:10:41 @agent_ppo2.py:185][0m |          -0.0124 |           8.2531 |           0.2189 |
[32m[20221213 15:10:41 @agent_ppo2.py:185][0m |          -0.0191 |           7.3495 |           0.2187 |
[32m[20221213 15:10:41 @agent_ppo2.py:185][0m |          -0.0207 |           7.2354 |           0.2188 |
[32m[20221213 15:10:41 @agent_ppo2.py:185][0m |          -0.0240 |           7.1683 |           0.2187 |
[32m[20221213 15:10:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.78
[32m[20221213 15:10:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 167.52
[32m[20221213 15:10:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.50
[32m[20221213 15:10:41 @agent_ppo2.py:143][0m Total time:      17.76 min
[32m[20221213 15:10:41 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221213 15:10:41 @agent_ppo2.py:121][0m #------------------------ Iteration 780 --------------------------#
[32m[20221213 15:10:41 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:10:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:41 @agent_ppo2.py:185][0m |           0.0102 |           8.0696 |           0.2228 |
[32m[20221213 15:10:41 @agent_ppo2.py:185][0m |          -0.0082 |           6.9825 |           0.2220 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0150 |           6.8522 |           0.2217 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0171 |           6.7007 |           0.2217 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0152 |           6.6195 |           0.2216 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0100 |           6.7094 |           0.2217 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0138 |           6.4807 |           0.2216 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0162 |           6.4316 |           0.2215 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0200 |           6.3683 |           0.2216 |
[32m[20221213 15:10:42 @agent_ppo2.py:185][0m |          -0.0184 |           6.3191 |           0.2217 |
[32m[20221213 15:10:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.97
[32m[20221213 15:10:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.44
[32m[20221213 15:10:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 163.55
[32m[20221213 15:10:42 @agent_ppo2.py:143][0m Total time:      17.79 min
[32m[20221213 15:10:42 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221213 15:10:42 @agent_ppo2.py:121][0m #------------------------ Iteration 781 --------------------------#
[32m[20221213 15:10:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:10:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |           0.0000 |           7.5104 |           0.2263 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0088 |           7.2449 |           0.2258 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0108 |           7.1254 |           0.2255 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0117 |           7.0293 |           0.2254 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0136 |           6.9508 |           0.2253 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0179 |           6.8647 |           0.2250 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0142 |           6.8105 |           0.2251 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0197 |           6.7587 |           0.2251 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0072 |           7.7498 |           0.2250 |
[32m[20221213 15:10:43 @agent_ppo2.py:185][0m |          -0.0090 |           7.0381 |           0.2252 |
[32m[20221213 15:10:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:10:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 139.44
[32m[20221213 15:10:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 151.89
[32m[20221213 15:10:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.13
[32m[20221213 15:10:44 @agent_ppo2.py:143][0m Total time:      17.81 min
[32m[20221213 15:10:44 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221213 15:10:44 @agent_ppo2.py:121][0m #------------------------ Iteration 782 --------------------------#
[32m[20221213 15:10:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:44 @agent_ppo2.py:185][0m |           0.0003 |           7.5393 |           0.2225 |
[32m[20221213 15:10:44 @agent_ppo2.py:185][0m |          -0.0058 |           7.2656 |           0.2223 |
[32m[20221213 15:10:44 @agent_ppo2.py:185][0m |          -0.0092 |           7.1744 |           0.2220 |
[32m[20221213 15:10:44 @agent_ppo2.py:185][0m |          -0.0115 |           7.1378 |           0.2220 |
[32m[20221213 15:10:44 @agent_ppo2.py:185][0m |          -0.0152 |           7.0933 |           0.2220 |
[32m[20221213 15:10:44 @agent_ppo2.py:185][0m |          -0.0137 |           7.0671 |           0.2219 |
[32m[20221213 15:10:45 @agent_ppo2.py:185][0m |          -0.0084 |           7.3786 |           0.2219 |
[32m[20221213 15:10:45 @agent_ppo2.py:185][0m |          -0.0169 |           7.0370 |           0.2218 |
[32m[20221213 15:10:45 @agent_ppo2.py:185][0m |          -0.0189 |           7.0107 |           0.2219 |
[32m[20221213 15:10:45 @agent_ppo2.py:185][0m |          -0.0151 |           6.9940 |           0.2217 |
[32m[20221213 15:10:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.22
[32m[20221213 15:10:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.56
[32m[20221213 15:10:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.33
[32m[20221213 15:10:45 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 198.33
[32m[20221213 15:10:45 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 198.33
[32m[20221213 15:10:45 @agent_ppo2.py:143][0m Total time:      17.83 min
[32m[20221213 15:10:45 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221213 15:10:45 @agent_ppo2.py:121][0m #------------------------ Iteration 783 --------------------------#
[32m[20221213 15:10:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:45 @agent_ppo2.py:185][0m |          -0.0034 |           7.3859 |           0.2229 |
[32m[20221213 15:10:45 @agent_ppo2.py:185][0m |          -0.0089 |           7.2399 |           0.2222 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |           0.0085 |           8.2983 |           0.2219 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |          -0.0072 |           7.1741 |           0.2212 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |          -0.0133 |           7.0711 |           0.2215 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |          -0.0159 |           7.0116 |           0.2213 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |          -0.0170 |           6.9830 |           0.2214 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |          -0.0195 |           6.9585 |           0.2212 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |          -0.0186 |           6.9284 |           0.2212 |
[32m[20221213 15:10:46 @agent_ppo2.py:185][0m |          -0.0165 |           6.8823 |           0.2212 |
[32m[20221213 15:10:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 162.42
[32m[20221213 15:10:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 186.54
[32m[20221213 15:10:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 173.83
[32m[20221213 15:10:46 @agent_ppo2.py:143][0m Total time:      17.85 min
[32m[20221213 15:10:46 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221213 15:10:46 @agent_ppo2.py:121][0m #------------------------ Iteration 784 --------------------------#
[32m[20221213 15:10:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0037 |           7.3490 |           0.2216 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0014 |           7.9790 |           0.2213 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0025 |           7.4645 |           0.2214 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0161 |           7.1323 |           0.2211 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0057 |           7.3824 |           0.2211 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0176 |           7.0554 |           0.2209 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0191 |           7.0181 |           0.2210 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0066 |           7.5390 |           0.2212 |
[32m[20221213 15:10:47 @agent_ppo2.py:185][0m |          -0.0137 |           7.0906 |           0.2212 |
[32m[20221213 15:10:48 @agent_ppo2.py:185][0m |          -0.0189 |           6.9664 |           0.2213 |
[32m[20221213 15:10:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.81
[32m[20221213 15:10:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 168.61
[32m[20221213 15:10:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 173.71
[32m[20221213 15:10:48 @agent_ppo2.py:143][0m Total time:      17.88 min
[32m[20221213 15:10:48 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221213 15:10:48 @agent_ppo2.py:121][0m #------------------------ Iteration 785 --------------------------#
[32m[20221213 15:10:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:48 @agent_ppo2.py:185][0m |          -0.0008 |           7.2514 |           0.2232 |
[32m[20221213 15:10:48 @agent_ppo2.py:185][0m |          -0.0108 |           7.0764 |           0.2228 |
[32m[20221213 15:10:48 @agent_ppo2.py:185][0m |          -0.0085 |           6.9302 |           0.2230 |
[32m[20221213 15:10:48 @agent_ppo2.py:185][0m |          -0.0120 |           6.8345 |           0.2229 |
[32m[20221213 15:10:48 @agent_ppo2.py:185][0m |          -0.0057 |           6.9994 |           0.2228 |
[32m[20221213 15:10:49 @agent_ppo2.py:185][0m |          -0.0120 |           6.6310 |           0.2228 |
[32m[20221213 15:10:49 @agent_ppo2.py:185][0m |          -0.0147 |           6.5698 |           0.2228 |
[32m[20221213 15:10:49 @agent_ppo2.py:185][0m |          -0.0175 |           6.4867 |           0.2228 |
[32m[20221213 15:10:49 @agent_ppo2.py:185][0m |          -0.0107 |           6.5192 |           0.2226 |
[32m[20221213 15:10:49 @agent_ppo2.py:185][0m |          -0.0173 |           6.3730 |           0.2225 |
[32m[20221213 15:10:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.50
[32m[20221213 15:10:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 173.81
[32m[20221213 15:10:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 136.04
[32m[20221213 15:10:49 @agent_ppo2.py:143][0m Total time:      17.90 min
[32m[20221213 15:10:49 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221213 15:10:49 @agent_ppo2.py:121][0m #------------------------ Iteration 786 --------------------------#
[32m[20221213 15:10:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:49 @agent_ppo2.py:185][0m |          -0.0041 |           7.8537 |           0.2284 |
[32m[20221213 15:10:49 @agent_ppo2.py:185][0m |          -0.0078 |           7.6320 |           0.2284 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0045 |           7.7712 |           0.2280 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0108 |           7.4784 |           0.2280 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0161 |           7.3927 |           0.2282 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0102 |           7.5108 |           0.2283 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0160 |           7.2882 |           0.2283 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0165 |           7.2287 |           0.2284 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0091 |           7.7071 |           0.2283 |
[32m[20221213 15:10:50 @agent_ppo2.py:185][0m |          -0.0167 |           7.2289 |           0.2283 |
[32m[20221213 15:10:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.29
[32m[20221213 15:10:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 173.33
[32m[20221213 15:10:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 191.72
[32m[20221213 15:10:50 @agent_ppo2.py:143][0m Total time:      17.92 min
[32m[20221213 15:10:50 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221213 15:10:50 @agent_ppo2.py:121][0m #------------------------ Iteration 787 --------------------------#
[32m[20221213 15:10:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |           0.0016 |           7.4444 |           0.2233 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0059 |           7.2224 |           0.2232 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0106 |           6.9343 |           0.2234 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0139 |           6.7902 |           0.2233 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0099 |           6.7566 |           0.2233 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0161 |           6.5721 |           0.2232 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0161 |           6.4987 |           0.2233 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0191 |           6.4198 |           0.2233 |
[32m[20221213 15:10:51 @agent_ppo2.py:185][0m |          -0.0154 |           6.3953 |           0.2231 |
[32m[20221213 15:10:52 @agent_ppo2.py:185][0m |          -0.0203 |           6.2858 |           0.2231 |
[32m[20221213 15:10:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 162.59
[32m[20221213 15:10:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 177.13
[32m[20221213 15:10:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 152.89
[32m[20221213 15:10:52 @agent_ppo2.py:143][0m Total time:      17.94 min
[32m[20221213 15:10:52 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221213 15:10:52 @agent_ppo2.py:121][0m #------------------------ Iteration 788 --------------------------#
[32m[20221213 15:10:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:52 @agent_ppo2.py:185][0m |           0.0028 |           7.4135 |           0.2235 |
[32m[20221213 15:10:52 @agent_ppo2.py:185][0m |          -0.0050 |           7.1957 |           0.2225 |
[32m[20221213 15:10:52 @agent_ppo2.py:185][0m |          -0.0117 |           7.1423 |           0.2223 |
[32m[20221213 15:10:52 @agent_ppo2.py:185][0m |          -0.0133 |           7.0928 |           0.2219 |
[32m[20221213 15:10:52 @agent_ppo2.py:185][0m |          -0.0121 |           7.0551 |           0.2218 |
[32m[20221213 15:10:53 @agent_ppo2.py:185][0m |          -0.0160 |           7.0375 |           0.2216 |
[32m[20221213 15:10:53 @agent_ppo2.py:185][0m |          -0.0164 |           7.0014 |           0.2213 |
[32m[20221213 15:10:53 @agent_ppo2.py:185][0m |          -0.0051 |           7.3954 |           0.2208 |
[32m[20221213 15:10:53 @agent_ppo2.py:185][0m |          -0.0175 |           6.9537 |           0.2209 |
[32m[20221213 15:10:53 @agent_ppo2.py:185][0m |          -0.0188 |           6.9280 |           0.2207 |
[32m[20221213 15:10:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 164.09
[32m[20221213 15:10:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 171.45
[32m[20221213 15:10:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.45
[32m[20221213 15:10:53 @agent_ppo2.py:143][0m Total time:      17.96 min
[32m[20221213 15:10:53 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221213 15:10:53 @agent_ppo2.py:121][0m #------------------------ Iteration 789 --------------------------#
[32m[20221213 15:10:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:53 @agent_ppo2.py:185][0m |          -0.0027 |           7.7830 |           0.2253 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0015 |           7.3035 |           0.2250 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0112 |           7.0671 |           0.2247 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0146 |           6.9001 |           0.2248 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0130 |           6.7632 |           0.2246 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0152 |           6.6639 |           0.2246 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0172 |           6.5502 |           0.2245 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0170 |           6.4833 |           0.2246 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0186 |           6.4214 |           0.2246 |
[32m[20221213 15:10:54 @agent_ppo2.py:185][0m |          -0.0165 |           6.4000 |           0.2247 |
[32m[20221213 15:10:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.59
[32m[20221213 15:10:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 150.50
[32m[20221213 15:10:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 178.68
[32m[20221213 15:10:54 @agent_ppo2.py:143][0m Total time:      17.99 min
[32m[20221213 15:10:54 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221213 15:10:54 @agent_ppo2.py:121][0m #------------------------ Iteration 790 --------------------------#
[32m[20221213 15:10:55 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:10:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |           0.0023 |           7.4702 |           0.2245 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |           0.0051 |           7.3972 |           0.2247 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |          -0.0086 |           6.9868 |           0.2245 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |          -0.0103 |           6.8443 |           0.2246 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |          -0.0165 |           6.7830 |           0.2247 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |          -0.0141 |           6.6946 |           0.2249 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |          -0.0149 |           6.6194 |           0.2250 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |          -0.0177 |           6.5715 |           0.2250 |
[32m[20221213 15:10:55 @agent_ppo2.py:185][0m |          -0.0172 |           6.5104 |           0.2250 |
[32m[20221213 15:10:56 @agent_ppo2.py:185][0m |          -0.0175 |           6.4607 |           0.2251 |
[32m[20221213 15:10:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 168.57
[32m[20221213 15:10:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 174.40
[32m[20221213 15:10:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.17
[32m[20221213 15:10:56 @agent_ppo2.py:143][0m Total time:      18.01 min
[32m[20221213 15:10:56 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221213 15:10:56 @agent_ppo2.py:121][0m #------------------------ Iteration 791 --------------------------#
[32m[20221213 15:10:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:56 @agent_ppo2.py:185][0m |          -0.0006 |           7.5749 |           0.2224 |
[32m[20221213 15:10:56 @agent_ppo2.py:185][0m |          -0.0096 |           7.1443 |           0.2218 |
[32m[20221213 15:10:56 @agent_ppo2.py:185][0m |          -0.0101 |           6.9120 |           0.2215 |
[32m[20221213 15:10:56 @agent_ppo2.py:185][0m |          -0.0131 |           6.7771 |           0.2216 |
[32m[20221213 15:10:56 @agent_ppo2.py:185][0m |          -0.0125 |           6.6453 |           0.2215 |
[32m[20221213 15:10:57 @agent_ppo2.py:185][0m |          -0.0096 |           7.1315 |           0.2213 |
[32m[20221213 15:10:57 @agent_ppo2.py:185][0m |           0.0027 |           7.3643 |           0.2209 |
[32m[20221213 15:10:57 @agent_ppo2.py:185][0m |          -0.0182 |           6.4816 |           0.2214 |
[32m[20221213 15:10:57 @agent_ppo2.py:185][0m |          -0.0168 |           6.4126 |           0.2213 |
[32m[20221213 15:10:57 @agent_ppo2.py:185][0m |          -0.0192 |           6.3926 |           0.2212 |
[32m[20221213 15:10:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.44
[32m[20221213 15:10:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 171.18
[32m[20221213 15:10:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 190.67
[32m[20221213 15:10:57 @agent_ppo2.py:143][0m Total time:      18.03 min
[32m[20221213 15:10:57 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221213 15:10:57 @agent_ppo2.py:121][0m #------------------------ Iteration 792 --------------------------#
[32m[20221213 15:10:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:57 @agent_ppo2.py:185][0m |          -0.0030 |           7.5726 |           0.2253 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0038 |           7.2632 |           0.2250 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |           0.0015 |           7.7832 |           0.2248 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0089 |           6.9684 |           0.2241 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0111 |           6.8924 |           0.2245 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0078 |           6.9815 |           0.2245 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0051 |           6.8350 |           0.2243 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0153 |           6.7450 |           0.2245 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0133 |           6.6920 |           0.2245 |
[32m[20221213 15:10:58 @agent_ppo2.py:185][0m |          -0.0134 |           6.6329 |           0.2244 |
[32m[20221213 15:10:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:10:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.96
[32m[20221213 15:10:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 175.49
[32m[20221213 15:10:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.60
[32m[20221213 15:10:58 @agent_ppo2.py:143][0m Total time:      18.05 min
[32m[20221213 15:10:58 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221213 15:10:58 @agent_ppo2.py:121][0m #------------------------ Iteration 793 --------------------------#
[32m[20221213 15:10:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:10:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |           0.0026 |           9.0712 |           0.2248 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0071 |           8.0401 |           0.2245 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0033 |           8.0697 |           0.2246 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0098 |           7.6252 |           0.2244 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0140 |           7.4772 |           0.2244 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0182 |           7.3716 |           0.2245 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0167 |           7.2899 |           0.2245 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0202 |           7.2509 |           0.2245 |
[32m[20221213 15:10:59 @agent_ppo2.py:185][0m |          -0.0164 |           7.1270 |           0.2244 |
[32m[20221213 15:11:00 @agent_ppo2.py:185][0m |          -0.0172 |           7.0541 |           0.2244 |
[32m[20221213 15:11:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 150.18
[32m[20221213 15:11:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 161.23
[32m[20221213 15:11:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 183.18
[32m[20221213 15:11:00 @agent_ppo2.py:143][0m Total time:      18.08 min
[32m[20221213 15:11:00 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221213 15:11:00 @agent_ppo2.py:121][0m #------------------------ Iteration 794 --------------------------#
[32m[20221213 15:11:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:00 @agent_ppo2.py:185][0m |          -0.0032 |           8.3498 |           0.2207 |
[32m[20221213 15:11:00 @agent_ppo2.py:185][0m |           0.0013 |           8.5603 |           0.2203 |
[32m[20221213 15:11:00 @agent_ppo2.py:185][0m |          -0.0137 |           7.7659 |           0.2198 |
[32m[20221213 15:11:00 @agent_ppo2.py:185][0m |          -0.0112 |           7.6226 |           0.2196 |
[32m[20221213 15:11:01 @agent_ppo2.py:185][0m |          -0.0134 |           7.5332 |           0.2195 |
[32m[20221213 15:11:01 @agent_ppo2.py:185][0m |          -0.0180 |           7.4422 |           0.2194 |
[32m[20221213 15:11:01 @agent_ppo2.py:185][0m |          -0.0171 |           7.4162 |           0.2194 |
[32m[20221213 15:11:01 @agent_ppo2.py:185][0m |          -0.0189 |           7.3418 |           0.2192 |
[32m[20221213 15:11:01 @agent_ppo2.py:185][0m |          -0.0138 |           7.3725 |           0.2191 |
[32m[20221213 15:11:01 @agent_ppo2.py:185][0m |          -0.0167 |           7.2826 |           0.2189 |
[32m[20221213 15:11:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 159.32
[32m[20221213 15:11:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 168.24
[32m[20221213 15:11:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.50
[32m[20221213 15:11:01 @agent_ppo2.py:143][0m Total time:      18.10 min
[32m[20221213 15:11:01 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221213 15:11:01 @agent_ppo2.py:121][0m #------------------------ Iteration 795 --------------------------#
[32m[20221213 15:11:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0008 |           8.5144 |           0.2207 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0081 |           8.2686 |           0.2204 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0102 |           8.1889 |           0.2200 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0112 |           8.0793 |           0.2197 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0110 |           8.0464 |           0.2195 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0119 |           8.0396 |           0.2194 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0151 |           7.9175 |           0.2192 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0189 |           7.8983 |           0.2191 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0157 |           7.8739 |           0.2191 |
[32m[20221213 15:11:02 @agent_ppo2.py:185][0m |          -0.0196 |           7.8140 |           0.2190 |
[32m[20221213 15:11:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 147.34
[32m[20221213 15:11:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 160.12
[32m[20221213 15:11:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 174.37
[32m[20221213 15:11:02 @agent_ppo2.py:143][0m Total time:      18.12 min
[32m[20221213 15:11:02 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221213 15:11:02 @agent_ppo2.py:121][0m #------------------------ Iteration 796 --------------------------#
[32m[20221213 15:11:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:11:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0026 |           7.4340 |           0.2150 |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0082 |           7.2409 |           0.2148 |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0085 |           7.0884 |           0.2147 |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0125 |           7.0241 |           0.2149 |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0127 |           6.9284 |           0.2148 |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0145 |           6.8646 |           0.2148 |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0162 |           6.7652 |           0.2148 |
[32m[20221213 15:11:03 @agent_ppo2.py:185][0m |          -0.0173 |           6.7243 |           0.2148 |
[32m[20221213 15:11:04 @agent_ppo2.py:185][0m |          -0.0152 |           6.6607 |           0.2147 |
[32m[20221213 15:11:04 @agent_ppo2.py:185][0m |          -0.0208 |           6.6477 |           0.2148 |
[32m[20221213 15:11:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.20
[32m[20221213 15:11:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 175.48
[32m[20221213 15:11:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 167.94
[32m[20221213 15:11:04 @agent_ppo2.py:143][0m Total time:      18.14 min
[32m[20221213 15:11:04 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221213 15:11:04 @agent_ppo2.py:121][0m #------------------------ Iteration 797 --------------------------#
[32m[20221213 15:11:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:04 @agent_ppo2.py:185][0m |          -0.0011 |           8.0826 |           0.2219 |
[32m[20221213 15:11:04 @agent_ppo2.py:185][0m |          -0.0102 |           7.9112 |           0.2213 |
[32m[20221213 15:11:04 @agent_ppo2.py:185][0m |          -0.0080 |           8.0792 |           0.2212 |
[32m[20221213 15:11:04 @agent_ppo2.py:185][0m |          -0.0129 |           7.7430 |           0.2211 |
[32m[20221213 15:11:05 @agent_ppo2.py:185][0m |          -0.0057 |           8.5097 |           0.2209 |
[32m[20221213 15:11:05 @agent_ppo2.py:185][0m |          -0.0157 |           7.6881 |           0.2210 |
[32m[20221213 15:11:05 @agent_ppo2.py:185][0m |          -0.0099 |           7.9678 |           0.2208 |
[32m[20221213 15:11:05 @agent_ppo2.py:185][0m |          -0.0183 |           7.6128 |           0.2207 |
[32m[20221213 15:11:05 @agent_ppo2.py:185][0m |          -0.0142 |           7.9103 |           0.2208 |
[32m[20221213 15:11:05 @agent_ppo2.py:185][0m |          -0.0222 |           7.5791 |           0.2208 |
[32m[20221213 15:11:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.93
[32m[20221213 15:11:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.07
[32m[20221213 15:11:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 190.53
[32m[20221213 15:11:05 @agent_ppo2.py:143][0m Total time:      18.17 min
[32m[20221213 15:11:05 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221213 15:11:05 @agent_ppo2.py:121][0m #------------------------ Iteration 798 --------------------------#
[32m[20221213 15:11:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:11:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0016 |           7.7984 |           0.2190 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0030 |           7.6794 |           0.2183 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0087 |           7.5006 |           0.2181 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0119 |           7.4560 |           0.2179 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0125 |           7.3915 |           0.2179 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0057 |           8.2127 |           0.2179 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0153 |           7.2943 |           0.2176 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0150 |           7.2627 |           0.2177 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0167 |           7.2161 |           0.2179 |
[32m[20221213 15:11:06 @agent_ppo2.py:185][0m |          -0.0187 |           7.1788 |           0.2180 |
[32m[20221213 15:11:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.52
[32m[20221213 15:11:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 162.51
[32m[20221213 15:11:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.46
[32m[20221213 15:11:06 @agent_ppo2.py:143][0m Total time:      18.19 min
[32m[20221213 15:11:06 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221213 15:11:06 @agent_ppo2.py:121][0m #------------------------ Iteration 799 --------------------------#
[32m[20221213 15:11:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |           0.0019 |           7.7475 |           0.2229 |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |          -0.0013 |           7.9496 |           0.2227 |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |          -0.0135 |           7.4706 |           0.2227 |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |          -0.0100 |           7.6733 |           0.2227 |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |          -0.0153 |           7.3581 |           0.2227 |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |          -0.0130 |           7.3110 |           0.2225 |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |          -0.0118 |           7.5733 |           0.2225 |
[32m[20221213 15:11:07 @agent_ppo2.py:185][0m |          -0.0197 |           7.2330 |           0.2225 |
[32m[20221213 15:11:08 @agent_ppo2.py:185][0m |          -0.0151 |           7.2116 |           0.2226 |
[32m[20221213 15:11:08 @agent_ppo2.py:185][0m |          -0.0205 |           7.1470 |           0.2225 |
[32m[20221213 15:11:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.04
[32m[20221213 15:11:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 162.25
[32m[20221213 15:11:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 185.91
[32m[20221213 15:11:08 @agent_ppo2.py:143][0m Total time:      18.21 min
[32m[20221213 15:11:08 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221213 15:11:08 @agent_ppo2.py:121][0m #------------------------ Iteration 800 --------------------------#
[32m[20221213 15:11:08 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:11:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:08 @agent_ppo2.py:185][0m |          -0.0002 |           7.4821 |           0.2197 |
[32m[20221213 15:11:08 @agent_ppo2.py:185][0m |          -0.0074 |           7.3100 |           0.2197 |
[32m[20221213 15:11:08 @agent_ppo2.py:185][0m |          -0.0116 |           7.2291 |           0.2194 |
[32m[20221213 15:11:08 @agent_ppo2.py:185][0m |          -0.0114 |           7.1766 |           0.2194 |
[32m[20221213 15:11:09 @agent_ppo2.py:185][0m |          -0.0113 |           7.1297 |           0.2192 |
[32m[20221213 15:11:09 @agent_ppo2.py:185][0m |          -0.0156 |           7.1337 |           0.2192 |
[32m[20221213 15:11:09 @agent_ppo2.py:185][0m |          -0.0162 |           7.0735 |           0.2191 |
[32m[20221213 15:11:09 @agent_ppo2.py:185][0m |          -0.0156 |           7.0319 |           0.2190 |
[32m[20221213 15:11:09 @agent_ppo2.py:185][0m |          -0.0170 |           7.0074 |           0.2190 |
[32m[20221213 15:11:09 @agent_ppo2.py:185][0m |          -0.0189 |           6.9812 |           0.2190 |
[32m[20221213 15:11:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.36
[32m[20221213 15:11:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.90
[32m[20221213 15:11:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 188.76
[32m[20221213 15:11:09 @agent_ppo2.py:143][0m Total time:      18.23 min
[32m[20221213 15:11:09 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221213 15:11:09 @agent_ppo2.py:121][0m #------------------------ Iteration 801 --------------------------#
[32m[20221213 15:11:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:11:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |           0.0001 |           8.0811 |           0.2208 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0104 |           7.8748 |           0.2209 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0149 |           7.7091 |           0.2207 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0169 |           7.6079 |           0.2207 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0055 |           7.7503 |           0.2208 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0112 |           7.5936 |           0.2208 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0185 |           7.4115 |           0.2208 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0219 |           7.3872 |           0.2205 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0190 |           7.3311 |           0.2206 |
[32m[20221213 15:11:10 @agent_ppo2.py:185][0m |          -0.0177 |           7.3424 |           0.2206 |
[32m[20221213 15:11:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 168.85
[32m[20221213 15:11:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 178.57
[32m[20221213 15:11:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 173.82
[32m[20221213 15:11:11 @agent_ppo2.py:143][0m Total time:      18.26 min
[32m[20221213 15:11:11 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221213 15:11:11 @agent_ppo2.py:121][0m #------------------------ Iteration 802 --------------------------#
[32m[20221213 15:11:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0024 |           8.0202 |           0.2180 |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0111 |           7.6074 |           0.2181 |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0136 |           7.4277 |           0.2181 |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0168 |           7.3374 |           0.2180 |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0161 |           7.2214 |           0.2182 |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0168 |           7.1234 |           0.2181 |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0184 |           7.0691 |           0.2183 |
[32m[20221213 15:11:11 @agent_ppo2.py:185][0m |          -0.0160 |           7.0587 |           0.2183 |
[32m[20221213 15:11:12 @agent_ppo2.py:185][0m |          -0.0193 |           6.9875 |           0.2183 |
[32m[20221213 15:11:12 @agent_ppo2.py:185][0m |          -0.0195 |           6.8981 |           0.2185 |
[32m[20221213 15:11:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 166.03
[32m[20221213 15:11:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.80
[32m[20221213 15:11:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 183.11
[32m[20221213 15:11:12 @agent_ppo2.py:143][0m Total time:      18.28 min
[32m[20221213 15:11:12 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221213 15:11:12 @agent_ppo2.py:121][0m #------------------------ Iteration 803 --------------------------#
[32m[20221213 15:11:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:12 @agent_ppo2.py:185][0m |           0.0038 |           8.5442 |           0.2268 |
[32m[20221213 15:11:12 @agent_ppo2.py:185][0m |          -0.0067 |           7.6605 |           0.2259 |
[32m[20221213 15:11:12 @agent_ppo2.py:185][0m |          -0.0160 |           7.5098 |           0.2261 |
[32m[20221213 15:11:12 @agent_ppo2.py:185][0m |          -0.0157 |           7.4140 |           0.2258 |
[32m[20221213 15:11:13 @agent_ppo2.py:185][0m |          -0.0169 |           7.3234 |           0.2257 |
[32m[20221213 15:11:13 @agent_ppo2.py:185][0m |          -0.0186 |           7.2551 |           0.2255 |
[32m[20221213 15:11:13 @agent_ppo2.py:185][0m |          -0.0218 |           7.1756 |           0.2255 |
[32m[20221213 15:11:13 @agent_ppo2.py:185][0m |          -0.0226 |           7.1030 |           0.2255 |
[32m[20221213 15:11:13 @agent_ppo2.py:185][0m |          -0.0077 |           8.2467 |           0.2255 |
[32m[20221213 15:11:13 @agent_ppo2.py:185][0m |          -0.0172 |           7.0043 |           0.2253 |
[32m[20221213 15:11:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:11:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 162.34
[32m[20221213 15:11:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 170.22
[32m[20221213 15:11:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 187.28
[32m[20221213 15:11:13 @agent_ppo2.py:143][0m Total time:      18.30 min
[32m[20221213 15:11:13 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221213 15:11:13 @agent_ppo2.py:121][0m #------------------------ Iteration 804 --------------------------#
[32m[20221213 15:11:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0037 |           8.1564 |           0.2218 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0113 |           7.7024 |           0.2219 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0113 |           7.4827 |           0.2218 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0130 |           7.3251 |           0.2217 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0194 |           7.1659 |           0.2215 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0022 |           7.6341 |           0.2214 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0188 |           6.9971 |           0.2212 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0211 |           6.8789 |           0.2213 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0194 |           6.7997 |           0.2213 |
[32m[20221213 15:11:14 @agent_ppo2.py:185][0m |          -0.0210 |           6.6930 |           0.2211 |
[32m[20221213 15:11:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.51
[32m[20221213 15:11:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.67
[32m[20221213 15:11:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 179.40
[32m[20221213 15:11:15 @agent_ppo2.py:143][0m Total time:      18.32 min
[32m[20221213 15:11:15 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221213 15:11:15 @agent_ppo2.py:121][0m #------------------------ Iteration 805 --------------------------#
[32m[20221213 15:11:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:15 @agent_ppo2.py:185][0m |           0.0021 |           8.1810 |           0.2248 |
[32m[20221213 15:11:15 @agent_ppo2.py:185][0m |          -0.0123 |           7.8587 |           0.2243 |
[32m[20221213 15:11:15 @agent_ppo2.py:185][0m |          -0.0110 |           7.6485 |           0.2242 |
[32m[20221213 15:11:15 @agent_ppo2.py:185][0m |          -0.0051 |           8.3655 |           0.2246 |
[32m[20221213 15:11:15 @agent_ppo2.py:185][0m |          -0.0143 |           7.4724 |           0.2244 |
[32m[20221213 15:11:15 @agent_ppo2.py:185][0m |          -0.0180 |           7.3355 |           0.2247 |
[32m[20221213 15:11:15 @agent_ppo2.py:185][0m |          -0.0107 |           7.7259 |           0.2250 |
[32m[20221213 15:11:16 @agent_ppo2.py:185][0m |          -0.0180 |           7.2461 |           0.2251 |
[32m[20221213 15:11:16 @agent_ppo2.py:185][0m |          -0.0202 |           7.1336 |           0.2252 |
[32m[20221213 15:11:16 @agent_ppo2.py:185][0m |          -0.0074 |           7.9724 |           0.2256 |
[32m[20221213 15:11:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.24
[32m[20221213 15:11:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 169.76
[32m[20221213 15:11:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 187.21
[32m[20221213 15:11:16 @agent_ppo2.py:143][0m Total time:      18.35 min
[32m[20221213 15:11:16 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221213 15:11:16 @agent_ppo2.py:121][0m #------------------------ Iteration 806 --------------------------#
[32m[20221213 15:11:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:16 @agent_ppo2.py:185][0m |          -0.0019 |           8.3849 |           0.2236 |
[32m[20221213 15:11:16 @agent_ppo2.py:185][0m |          -0.0060 |           8.0468 |           0.2234 |
[32m[20221213 15:11:16 @agent_ppo2.py:185][0m |          -0.0111 |           7.8725 |           0.2233 |
[32m[20221213 15:11:17 @agent_ppo2.py:185][0m |          -0.0134 |           7.7504 |           0.2233 |
[32m[20221213 15:11:17 @agent_ppo2.py:185][0m |          -0.0139 |           7.7850 |           0.2232 |
[32m[20221213 15:11:17 @agent_ppo2.py:185][0m |          -0.0125 |           7.6188 |           0.2231 |
[32m[20221213 15:11:17 @agent_ppo2.py:185][0m |          -0.0193 |           7.5948 |           0.2231 |
[32m[20221213 15:11:17 @agent_ppo2.py:185][0m |          -0.0165 |           7.5154 |           0.2230 |
[32m[20221213 15:11:17 @agent_ppo2.py:185][0m |          -0.0186 |           7.4575 |           0.2231 |
[32m[20221213 15:11:17 @agent_ppo2.py:185][0m |          -0.0176 |           7.4285 |           0.2230 |
[32m[20221213 15:11:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 159.06
[32m[20221213 15:11:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 166.37
[32m[20221213 15:11:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.04
[32m[20221213 15:11:17 @agent_ppo2.py:143][0m Total time:      18.37 min
[32m[20221213 15:11:17 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221213 15:11:17 @agent_ppo2.py:121][0m #------------------------ Iteration 807 --------------------------#
[32m[20221213 15:11:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:11:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |           0.0013 |           8.4445 |           0.2273 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0054 |           8.1497 |           0.2270 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0091 |           8.0353 |           0.2267 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0136 |           7.9524 |           0.2260 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0133 |           7.8772 |           0.2258 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0097 |           7.8743 |           0.2255 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0139 |           7.8123 |           0.2254 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0133 |           7.7712 |           0.2250 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0173 |           7.7099 |           0.2248 |
[32m[20221213 15:11:18 @agent_ppo2.py:185][0m |          -0.0172 |           7.6725 |           0.2245 |
[32m[20221213 15:11:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 164.97
[32m[20221213 15:11:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 170.05
[32m[20221213 15:11:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 180.91
[32m[20221213 15:11:19 @agent_ppo2.py:143][0m Total time:      18.39 min
[32m[20221213 15:11:19 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221213 15:11:19 @agent_ppo2.py:121][0m #------------------------ Iteration 808 --------------------------#
[32m[20221213 15:11:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:19 @agent_ppo2.py:185][0m |          -0.0022 |           8.0759 |           0.2210 |
[32m[20221213 15:11:19 @agent_ppo2.py:185][0m |           0.0040 |           8.3964 |           0.2203 |
[32m[20221213 15:11:19 @agent_ppo2.py:185][0m |          -0.0119 |           7.5280 |           0.2200 |
[32m[20221213 15:11:19 @agent_ppo2.py:185][0m |          -0.0145 |           7.3685 |           0.2199 |
[32m[20221213 15:11:19 @agent_ppo2.py:185][0m |          -0.0158 |           7.2577 |           0.2196 |
[32m[20221213 15:11:19 @agent_ppo2.py:185][0m |          -0.0162 |           7.1652 |           0.2194 |
[32m[20221213 15:11:19 @agent_ppo2.py:185][0m |          -0.0181 |           7.0651 |           0.2191 |
[32m[20221213 15:11:20 @agent_ppo2.py:185][0m |          -0.0147 |           7.0010 |           0.2190 |
[32m[20221213 15:11:20 @agent_ppo2.py:185][0m |          -0.0206 |           6.8413 |           0.2189 |
[32m[20221213 15:11:20 @agent_ppo2.py:185][0m |          -0.0154 |           6.8227 |           0.2187 |
[32m[20221213 15:11:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.79
[32m[20221213 15:11:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 171.15
[32m[20221213 15:11:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 197.78
[32m[20221213 15:11:20 @agent_ppo2.py:143][0m Total time:      18.41 min
[32m[20221213 15:11:20 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221213 15:11:20 @agent_ppo2.py:121][0m #------------------------ Iteration 809 --------------------------#
[32m[20221213 15:11:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:20 @agent_ppo2.py:185][0m |          -0.0032 |           8.1652 |           0.2155 |
[32m[20221213 15:11:20 @agent_ppo2.py:185][0m |          -0.0104 |           7.9390 |           0.2154 |
[32m[20221213 15:11:20 @agent_ppo2.py:185][0m |          -0.0107 |           7.8127 |           0.2153 |
[32m[20221213 15:11:21 @agent_ppo2.py:185][0m |          -0.0151 |           7.7083 |           0.2154 |
[32m[20221213 15:11:21 @agent_ppo2.py:185][0m |          -0.0137 |           7.6320 |           0.2155 |
[32m[20221213 15:11:21 @agent_ppo2.py:185][0m |          -0.0208 |           7.5785 |           0.2156 |
[32m[20221213 15:11:21 @agent_ppo2.py:185][0m |          -0.0209 |           7.5063 |           0.2155 |
[32m[20221213 15:11:21 @agent_ppo2.py:185][0m |          -0.0155 |           7.5417 |           0.2157 |
[32m[20221213 15:11:21 @agent_ppo2.py:185][0m |          -0.0202 |           7.3975 |           0.2158 |
[32m[20221213 15:11:21 @agent_ppo2.py:185][0m |          -0.0203 |           7.3502 |           0.2157 |
[32m[20221213 15:11:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 171.40
[32m[20221213 15:11:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 190.50
[32m[20221213 15:11:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 196.69
[32m[20221213 15:11:21 @agent_ppo2.py:143][0m Total time:      18.43 min
[32m[20221213 15:11:21 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221213 15:11:21 @agent_ppo2.py:121][0m #------------------------ Iteration 810 --------------------------#
[32m[20221213 15:11:21 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:11:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0018 |           7.9722 |           0.2260 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0083 |           7.6606 |           0.2255 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0082 |           7.5271 |           0.2254 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0050 |           7.6526 |           0.2253 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0095 |           7.2719 |           0.2252 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0133 |           7.1266 |           0.2255 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0141 |           7.0214 |           0.2253 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0160 |           6.9407 |           0.2255 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0185 |           6.8889 |           0.2255 |
[32m[20221213 15:11:22 @agent_ppo2.py:185][0m |          -0.0185 |           6.8350 |           0.2254 |
[32m[20221213 15:11:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 163.96
[32m[20221213 15:11:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 180.57
[32m[20221213 15:11:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 173.43
[32m[20221213 15:11:23 @agent_ppo2.py:143][0m Total time:      18.46 min
[32m[20221213 15:11:23 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221213 15:11:23 @agent_ppo2.py:121][0m #------------------------ Iteration 811 --------------------------#
[32m[20221213 15:11:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:23 @agent_ppo2.py:185][0m |          -0.0024 |           7.6933 |           0.2199 |
[32m[20221213 15:11:23 @agent_ppo2.py:185][0m |          -0.0073 |           7.3947 |           0.2195 |
[32m[20221213 15:11:23 @agent_ppo2.py:185][0m |          -0.0128 |           7.1920 |           0.2192 |
[32m[20221213 15:11:23 @agent_ppo2.py:185][0m |          -0.0015 |           7.8403 |           0.2188 |
[32m[20221213 15:11:23 @agent_ppo2.py:185][0m |          -0.0154 |           6.9573 |           0.2184 |
[32m[20221213 15:11:23 @agent_ppo2.py:185][0m |          -0.0160 |           6.8458 |           0.2184 |
[32m[20221213 15:11:24 @agent_ppo2.py:185][0m |          -0.0056 |           7.5246 |           0.2183 |
[32m[20221213 15:11:24 @agent_ppo2.py:185][0m |          -0.0215 |           6.7158 |           0.2179 |
[32m[20221213 15:11:24 @agent_ppo2.py:185][0m |          -0.0213 |           6.6245 |           0.2176 |
[32m[20221213 15:11:24 @agent_ppo2.py:185][0m |          -0.0197 |           6.5653 |           0.2175 |
[32m[20221213 15:11:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:11:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.18
[32m[20221213 15:11:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 167.86
[32m[20221213 15:11:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 167.44
[32m[20221213 15:11:24 @agent_ppo2.py:143][0m Total time:      18.48 min
[32m[20221213 15:11:24 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221213 15:11:24 @agent_ppo2.py:121][0m #------------------------ Iteration 812 --------------------------#
[32m[20221213 15:11:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:11:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:24 @agent_ppo2.py:185][0m |          -0.0013 |           8.0091 |           0.2114 |
[32m[20221213 15:11:24 @agent_ppo2.py:185][0m |          -0.0062 |           7.7305 |           0.2112 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0091 |           7.5974 |           0.2111 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0116 |           7.4987 |           0.2112 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0121 |           7.4416 |           0.2112 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0146 |           7.3978 |           0.2112 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0060 |           7.6548 |           0.2113 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0035 |           7.9831 |           0.2113 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0098 |           7.4138 |           0.2112 |
[32m[20221213 15:11:25 @agent_ppo2.py:185][0m |          -0.0175 |           7.2395 |           0.2113 |
[32m[20221213 15:11:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:11:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.92
[32m[20221213 15:11:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 168.55
[32m[20221213 15:11:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 194.04
[32m[20221213 15:11:25 @agent_ppo2.py:143][0m Total time:      18.50 min
[32m[20221213 15:11:25 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221213 15:11:25 @agent_ppo2.py:121][0m #------------------------ Iteration 813 --------------------------#
[32m[20221213 15:11:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0026 |           7.7074 |           0.2107 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0048 |           7.6296 |           0.2104 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0060 |           7.7493 |           0.2102 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0087 |           7.6284 |           0.2105 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0178 |           7.3392 |           0.2103 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0199 |           7.2921 |           0.2103 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0166 |           7.2682 |           0.2105 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0169 |           7.2465 |           0.2107 |
[32m[20221213 15:11:26 @agent_ppo2.py:185][0m |          -0.0194 |           7.2053 |           0.2107 |
[32m[20221213 15:11:27 @agent_ppo2.py:185][0m |          -0.0181 |           7.1627 |           0.2106 |
[32m[20221213 15:11:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:11:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.15
[32m[20221213 15:11:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 174.58
[32m[20221213 15:11:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 104.80
[32m[20221213 15:11:27 @agent_ppo2.py:143][0m Total time:      18.53 min
[32m[20221213 15:11:27 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221213 15:11:27 @agent_ppo2.py:121][0m #------------------------ Iteration 814 --------------------------#
[32m[20221213 15:11:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:27 @agent_ppo2.py:185][0m |          -0.0016 |           7.8520 |           0.2136 |
[32m[20221213 15:11:27 @agent_ppo2.py:185][0m |          -0.0082 |           7.6532 |           0.2131 |
[32m[20221213 15:11:27 @agent_ppo2.py:185][0m |          -0.0083 |           7.5825 |           0.2129 |
[32m[20221213 15:11:27 @agent_ppo2.py:185][0m |          -0.0151 |           7.3743 |           0.2123 |
[32m[20221213 15:11:27 @agent_ppo2.py:185][0m |          -0.0171 |           7.2937 |           0.2125 |
[32m[20221213 15:11:28 @agent_ppo2.py:185][0m |          -0.0172 |           7.2324 |           0.2125 |
[32m[20221213 15:11:28 @agent_ppo2.py:185][0m |          -0.0176 |           7.1621 |           0.2124 |
[32m[20221213 15:11:28 @agent_ppo2.py:185][0m |          -0.0188 |           7.1095 |           0.2124 |
[32m[20221213 15:11:28 @agent_ppo2.py:185][0m |          -0.0189 |           7.0270 |           0.2124 |
[32m[20221213 15:11:28 @agent_ppo2.py:185][0m |          -0.0191 |           6.9673 |           0.2124 |
[32m[20221213 15:11:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:11:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.11
[32m[20221213 15:11:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 182.14
[32m[20221213 15:11:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 89.50
[32m[20221213 15:11:28 @agent_ppo2.py:143][0m Total time:      18.55 min
[32m[20221213 15:11:28 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221213 15:11:28 @agent_ppo2.py:121][0m #------------------------ Iteration 815 --------------------------#
[32m[20221213 15:11:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:11:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0013 |           8.1692 |           0.2220 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0050 |           7.8688 |           0.2219 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0099 |           7.7193 |           0.2219 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0095 |           7.6004 |           0.2214 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0072 |           7.7685 |           0.2213 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0047 |           8.0848 |           0.2210 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0097 |           7.4257 |           0.2207 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0125 |           7.3503 |           0.2207 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0147 |           7.2504 |           0.2207 |
[32m[20221213 15:11:29 @agent_ppo2.py:185][0m |          -0.0176 |           7.2115 |           0.2206 |
[32m[20221213 15:11:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:11:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 172.05
[32m[20221213 15:11:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 189.34
[32m[20221213 15:11:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 191.80
[32m[20221213 15:11:30 @agent_ppo2.py:143][0m Total time:      18.57 min
[32m[20221213 15:11:30 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221213 15:11:30 @agent_ppo2.py:121][0m #------------------------ Iteration 816 --------------------------#
[32m[20221213 15:11:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:30 @agent_ppo2.py:185][0m |          -0.0038 |           8.0870 |           0.2172 |
[32m[20221213 15:11:30 @agent_ppo2.py:185][0m |          -0.0093 |           7.8178 |           0.2161 |
[32m[20221213 15:11:30 @agent_ppo2.py:185][0m |          -0.0107 |           7.7060 |           0.2163 |
[32m[20221213 15:11:30 @agent_ppo2.py:185][0m |          -0.0151 |           7.6132 |           0.2155 |
[32m[20221213 15:11:30 @agent_ppo2.py:185][0m |          -0.0170 |           7.5755 |           0.2153 |
[32m[20221213 15:11:30 @agent_ppo2.py:185][0m |          -0.0133 |           7.5001 |           0.2152 |
[32m[20221213 15:11:30 @agent_ppo2.py:185][0m |          -0.0180 |           7.4670 |           0.2151 |
[32m[20221213 15:11:31 @agent_ppo2.py:185][0m |          -0.0179 |           7.4171 |           0.2148 |
[32m[20221213 15:11:31 @agent_ppo2.py:185][0m |          -0.0111 |           7.4840 |           0.2147 |
[32m[20221213 15:11:31 @agent_ppo2.py:185][0m |          -0.0159 |           7.3454 |           0.2144 |
[32m[20221213 15:11:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:11:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 166.10
[32m[20221213 15:11:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 182.68
[32m[20221213 15:11:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 193.41
[32m[20221213 15:11:31 @agent_ppo2.py:143][0m Total time:      18.60 min
[32m[20221213 15:11:31 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221213 15:11:31 @agent_ppo2.py:121][0m #------------------------ Iteration 817 --------------------------#
[32m[20221213 15:11:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:31 @agent_ppo2.py:185][0m |          -0.0014 |           8.1667 |           0.2171 |
[32m[20221213 15:11:31 @agent_ppo2.py:185][0m |           0.0067 |           8.6231 |           0.2164 |
[32m[20221213 15:11:31 @agent_ppo2.py:185][0m |          -0.0093 |           7.9158 |           0.2162 |
[32m[20221213 15:11:32 @agent_ppo2.py:185][0m |          -0.0115 |           7.8586 |           0.2160 |
[32m[20221213 15:11:32 @agent_ppo2.py:185][0m |          -0.0140 |           7.8240 |           0.2159 |
[32m[20221213 15:11:32 @agent_ppo2.py:185][0m |          -0.0165 |           7.7823 |           0.2158 |
[32m[20221213 15:11:32 @agent_ppo2.py:185][0m |          -0.0101 |           7.8667 |           0.2157 |
[32m[20221213 15:11:32 @agent_ppo2.py:185][0m |          -0.0107 |           7.8622 |           0.2155 |
[32m[20221213 15:11:32 @agent_ppo2.py:185][0m |          -0.0158 |           7.6897 |           0.2154 |
[32m[20221213 15:11:32 @agent_ppo2.py:185][0m |          -0.0197 |           7.6515 |           0.2153 |
[32m[20221213 15:11:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.21
[32m[20221213 15:11:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.27
[32m[20221213 15:11:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 196.57
[32m[20221213 15:11:32 @agent_ppo2.py:143][0m Total time:      18.62 min
[32m[20221213 15:11:32 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221213 15:11:32 @agent_ppo2.py:121][0m #------------------------ Iteration 818 --------------------------#
[32m[20221213 15:11:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0023 |           7.9214 |           0.2123 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0070 |           7.4558 |           0.2123 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0100 |           7.2829 |           0.2120 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0099 |           7.2330 |           0.2116 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0137 |           7.1090 |           0.2114 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0139 |           7.0016 |           0.2113 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0157 |           6.9362 |           0.2112 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0123 |           6.9833 |           0.2111 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0090 |           7.3059 |           0.2110 |
[32m[20221213 15:11:33 @agent_ppo2.py:185][0m |          -0.0166 |           6.7737 |           0.2107 |
[32m[20221213 15:11:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.42
[32m[20221213 15:11:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 174.03
[32m[20221213 15:11:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 194.10
[32m[20221213 15:11:34 @agent_ppo2.py:143][0m Total time:      18.64 min
[32m[20221213 15:11:34 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221213 15:11:34 @agent_ppo2.py:121][0m #------------------------ Iteration 819 --------------------------#
[32m[20221213 15:11:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:34 @agent_ppo2.py:185][0m |           0.0003 |           8.4114 |           0.2096 |
[32m[20221213 15:11:34 @agent_ppo2.py:185][0m |          -0.0099 |           8.0740 |           0.2091 |
[32m[20221213 15:11:34 @agent_ppo2.py:185][0m |          -0.0110 |           7.9869 |           0.2090 |
[32m[20221213 15:11:34 @agent_ppo2.py:185][0m |          -0.0149 |           7.9216 |           0.2090 |
[32m[20221213 15:11:34 @agent_ppo2.py:185][0m |          -0.0178 |           7.8696 |           0.2090 |
[32m[20221213 15:11:34 @agent_ppo2.py:185][0m |          -0.0125 |           7.8657 |           0.2088 |
[32m[20221213 15:11:34 @agent_ppo2.py:185][0m |          -0.0130 |           8.0248 |           0.2090 |
[32m[20221213 15:11:35 @agent_ppo2.py:185][0m |          -0.0216 |           7.7385 |           0.2090 |
[32m[20221213 15:11:35 @agent_ppo2.py:185][0m |          -0.0208 |           7.7136 |           0.2091 |
[32m[20221213 15:11:35 @agent_ppo2.py:185][0m |          -0.0200 |           7.6845 |           0.2091 |
[32m[20221213 15:11:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 145.77
[32m[20221213 15:11:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 163.92
[32m[20221213 15:11:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.96
[32m[20221213 15:11:35 @agent_ppo2.py:143][0m Total time:      18.66 min
[32m[20221213 15:11:35 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221213 15:11:35 @agent_ppo2.py:121][0m #------------------------ Iteration 820 --------------------------#
[32m[20221213 15:11:35 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:11:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:35 @agent_ppo2.py:185][0m |          -0.0032 |           8.2627 |           0.2082 |
[32m[20221213 15:11:35 @agent_ppo2.py:185][0m |          -0.0012 |           8.3568 |           0.2079 |
[32m[20221213 15:11:35 @agent_ppo2.py:185][0m |          -0.0108 |           8.0308 |           0.2078 |
[32m[20221213 15:11:36 @agent_ppo2.py:185][0m |          -0.0076 |           8.0427 |           0.2078 |
[32m[20221213 15:11:36 @agent_ppo2.py:185][0m |          -0.0130 |           7.9224 |           0.2078 |
[32m[20221213 15:11:36 @agent_ppo2.py:185][0m |          -0.0163 |           7.8646 |           0.2080 |
[32m[20221213 15:11:36 @agent_ppo2.py:185][0m |          -0.0156 |           7.7929 |           0.2080 |
[32m[20221213 15:11:36 @agent_ppo2.py:185][0m |          -0.0172 |           7.7798 |           0.2080 |
[32m[20221213 15:11:36 @agent_ppo2.py:185][0m |          -0.0173 |           7.7025 |           0.2078 |
[32m[20221213 15:11:36 @agent_ppo2.py:185][0m |          -0.0102 |           8.0947 |           0.2080 |
[32m[20221213 15:11:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.70
[32m[20221213 15:11:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 177.03
[32m[20221213 15:11:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 176.09
[32m[20221213 15:11:36 @agent_ppo2.py:143][0m Total time:      18.68 min
[32m[20221213 15:11:36 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221213 15:11:36 @agent_ppo2.py:121][0m #------------------------ Iteration 821 --------------------------#
[32m[20221213 15:11:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0024 |           8.0656 |           0.2189 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0022 |           7.9539 |           0.2189 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0089 |           7.7870 |           0.2189 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0000 |           8.0911 |           0.2188 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0077 |           7.6571 |           0.2187 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0114 |           7.5714 |           0.2188 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0139 |           7.5206 |           0.2189 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0133 |           7.4906 |           0.2187 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0169 |           7.4242 |           0.2188 |
[32m[20221213 15:11:37 @agent_ppo2.py:185][0m |          -0.0167 |           7.4337 |           0.2190 |
[32m[20221213 15:11:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 168.39
[32m[20221213 15:11:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.13
[32m[20221213 15:11:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.48
[32m[20221213 15:11:38 @agent_ppo2.py:143][0m Total time:      18.71 min
[32m[20221213 15:11:38 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221213 15:11:38 @agent_ppo2.py:121][0m #------------------------ Iteration 822 --------------------------#
[32m[20221213 15:11:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:38 @agent_ppo2.py:185][0m |           0.0041 |           7.9920 |           0.2218 |
[32m[20221213 15:11:38 @agent_ppo2.py:185][0m |          -0.0073 |           7.4394 |           0.2210 |
[32m[20221213 15:11:38 @agent_ppo2.py:185][0m |          -0.0135 |           7.3149 |           0.2211 |
[32m[20221213 15:11:38 @agent_ppo2.py:185][0m |           0.0115 |           8.3306 |           0.2210 |
[32m[20221213 15:11:38 @agent_ppo2.py:185][0m |          -0.0164 |           7.1952 |           0.2210 |
[32m[20221213 15:11:38 @agent_ppo2.py:185][0m |          -0.0143 |           7.0744 |           0.2211 |
[32m[20221213 15:11:38 @agent_ppo2.py:185][0m |          -0.0071 |           7.8603 |           0.2211 |
[32m[20221213 15:11:39 @agent_ppo2.py:185][0m |          -0.0173 |           7.0308 |           0.2206 |
[32m[20221213 15:11:39 @agent_ppo2.py:185][0m |          -0.0112 |           7.0270 |           0.2209 |
[32m[20221213 15:11:39 @agent_ppo2.py:185][0m |          -0.0189 |           6.8816 |           0.2211 |
[32m[20221213 15:11:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.03
[32m[20221213 15:11:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.74
[32m[20221213 15:11:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.28
[32m[20221213 15:11:39 @agent_ppo2.py:143][0m Total time:      18.73 min
[32m[20221213 15:11:39 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221213 15:11:39 @agent_ppo2.py:121][0m #------------------------ Iteration 823 --------------------------#
[32m[20221213 15:11:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:39 @agent_ppo2.py:185][0m |          -0.0026 |           8.2617 |           0.2197 |
[32m[20221213 15:11:39 @agent_ppo2.py:185][0m |          -0.0075 |           7.9289 |           0.2194 |
[32m[20221213 15:11:39 @agent_ppo2.py:185][0m |           0.0065 |           8.7849 |           0.2190 |
[32m[20221213 15:11:40 @agent_ppo2.py:185][0m |          -0.0093 |           7.6333 |           0.2189 |
[32m[20221213 15:11:40 @agent_ppo2.py:185][0m |          -0.0130 |           7.5258 |           0.2190 |
[32m[20221213 15:11:40 @agent_ppo2.py:185][0m |          -0.0018 |           8.4974 |           0.2190 |
[32m[20221213 15:11:40 @agent_ppo2.py:185][0m |          -0.0151 |           7.4260 |           0.2191 |
[32m[20221213 15:11:40 @agent_ppo2.py:185][0m |          -0.0186 |           7.3277 |           0.2191 |
[32m[20221213 15:11:40 @agent_ppo2.py:185][0m |          -0.0157 |           7.2612 |           0.2192 |
[32m[20221213 15:11:40 @agent_ppo2.py:185][0m |          -0.0208 |           7.1994 |           0.2193 |
[32m[20221213 15:11:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 169.87
[32m[20221213 15:11:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 178.40
[32m[20221213 15:11:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.85
[32m[20221213 15:11:40 @agent_ppo2.py:143][0m Total time:      18.75 min
[32m[20221213 15:11:40 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221213 15:11:40 @agent_ppo2.py:121][0m #------------------------ Iteration 824 --------------------------#
[32m[20221213 15:11:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |           0.0041 |           9.1225 |           0.2180 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0110 |           8.4496 |           0.2176 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0098 |           8.2978 |           0.2178 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0105 |           8.2565 |           0.2179 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0093 |           8.1420 |           0.2181 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0045 |           9.0252 |           0.2184 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0150 |           8.0418 |           0.2179 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0169 |           7.9903 |           0.2184 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0199 |           7.9434 |           0.2185 |
[32m[20221213 15:11:41 @agent_ppo2.py:185][0m |          -0.0213 |           7.9248 |           0.2187 |
[32m[20221213 15:11:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 161.94
[32m[20221213 15:11:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 182.77
[32m[20221213 15:11:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 134.55
[32m[20221213 15:11:42 @agent_ppo2.py:143][0m Total time:      18.77 min
[32m[20221213 15:11:42 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221213 15:11:42 @agent_ppo2.py:121][0m #------------------------ Iteration 825 --------------------------#
[32m[20221213 15:11:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:42 @agent_ppo2.py:185][0m |          -0.0038 |           7.8116 |           0.2179 |
[32m[20221213 15:11:42 @agent_ppo2.py:185][0m |          -0.0081 |           7.5269 |           0.2177 |
[32m[20221213 15:11:42 @agent_ppo2.py:185][0m |          -0.0029 |           7.5663 |           0.2173 |
[32m[20221213 15:11:42 @agent_ppo2.py:185][0m |          -0.0142 |           7.2787 |           0.2174 |
[32m[20221213 15:11:42 @agent_ppo2.py:185][0m |          -0.0176 |           7.1775 |           0.2174 |
[32m[20221213 15:11:42 @agent_ppo2.py:185][0m |          -0.0127 |           7.1328 |           0.2172 |
[32m[20221213 15:11:43 @agent_ppo2.py:185][0m |          -0.0203 |           7.0480 |           0.2171 |
[32m[20221213 15:11:43 @agent_ppo2.py:185][0m |          -0.0173 |           6.9633 |           0.2171 |
[32m[20221213 15:11:43 @agent_ppo2.py:185][0m |          -0.0202 |           6.9294 |           0.2169 |
[32m[20221213 15:11:43 @agent_ppo2.py:185][0m |          -0.0177 |           6.8855 |           0.2171 |
[32m[20221213 15:11:43 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:11:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 164.37
[32m[20221213 15:11:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 174.93
[32m[20221213 15:11:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 192.22
[32m[20221213 15:11:43 @agent_ppo2.py:143][0m Total time:      18.80 min
[32m[20221213 15:11:43 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221213 15:11:43 @agent_ppo2.py:121][0m #------------------------ Iteration 826 --------------------------#
[32m[20221213 15:11:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:43 @agent_ppo2.py:185][0m |          -0.0057 |           8.2957 |           0.2178 |
[32m[20221213 15:11:43 @agent_ppo2.py:185][0m |          -0.0094 |           7.9222 |           0.2178 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0130 |           7.7039 |           0.2177 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0122 |           7.5806 |           0.2179 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0149 |           7.4563 |           0.2179 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0175 |           7.3507 |           0.2181 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0152 |           7.3647 |           0.2181 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0202 |           7.1585 |           0.2181 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0201 |           7.0885 |           0.2181 |
[32m[20221213 15:11:44 @agent_ppo2.py:185][0m |          -0.0112 |           7.4509 |           0.2181 |
[32m[20221213 15:11:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 168.83
[32m[20221213 15:11:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.12
[32m[20221213 15:11:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 171.64
[32m[20221213 15:11:44 @agent_ppo2.py:143][0m Total time:      18.82 min
[32m[20221213 15:11:44 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221213 15:11:44 @agent_ppo2.py:121][0m #------------------------ Iteration 827 --------------------------#
[32m[20221213 15:11:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0020 |           8.6920 |           0.2223 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0074 |           8.2167 |           0.2220 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0119 |           7.9668 |           0.2219 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0064 |           8.0405 |           0.2217 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0126 |           7.6788 |           0.2217 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0157 |           7.5702 |           0.2216 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0053 |           7.7378 |           0.2216 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0139 |           7.4098 |           0.2212 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0161 |           7.2834 |           0.2213 |
[32m[20221213 15:11:45 @agent_ppo2.py:185][0m |          -0.0169 |           7.2262 |           0.2210 |
[32m[20221213 15:11:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 172.26
[32m[20221213 15:11:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 180.60
[32m[20221213 15:11:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.80
[32m[20221213 15:11:46 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 204.80
[32m[20221213 15:11:46 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 204.80
[32m[20221213 15:11:46 @agent_ppo2.py:143][0m Total time:      18.84 min
[32m[20221213 15:11:46 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221213 15:11:46 @agent_ppo2.py:121][0m #------------------------ Iteration 828 --------------------------#
[32m[20221213 15:11:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:46 @agent_ppo2.py:185][0m |          -0.0025 |           8.9610 |           0.2181 |
[32m[20221213 15:11:46 @agent_ppo2.py:185][0m |          -0.0090 |           8.5445 |           0.2177 |
[32m[20221213 15:11:46 @agent_ppo2.py:185][0m |          -0.0111 |           8.4389 |           0.2174 |
[32m[20221213 15:11:46 @agent_ppo2.py:185][0m |          -0.0099 |           8.5335 |           0.2175 |
[32m[20221213 15:11:46 @agent_ppo2.py:185][0m |          -0.0171 |           8.3158 |           0.2175 |
[32m[20221213 15:11:46 @agent_ppo2.py:185][0m |          -0.0183 |           8.2223 |           0.2176 |
[32m[20221213 15:11:47 @agent_ppo2.py:185][0m |          -0.0179 |           8.1877 |           0.2176 |
[32m[20221213 15:11:47 @agent_ppo2.py:185][0m |          -0.0142 |           8.2250 |           0.2177 |
[32m[20221213 15:11:47 @agent_ppo2.py:185][0m |          -0.0190 |           8.0759 |           0.2179 |
[32m[20221213 15:11:47 @agent_ppo2.py:185][0m |          -0.0159 |           8.2068 |           0.2177 |
[32m[20221213 15:11:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 167.39
[32m[20221213 15:11:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 181.16
[32m[20221213 15:11:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.61
[32m[20221213 15:11:47 @agent_ppo2.py:143][0m Total time:      18.86 min
[32m[20221213 15:11:47 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221213 15:11:47 @agent_ppo2.py:121][0m #------------------------ Iteration 829 --------------------------#
[32m[20221213 15:11:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:47 @agent_ppo2.py:185][0m |           0.0177 |           9.4323 |           0.2285 |
[32m[20221213 15:11:47 @agent_ppo2.py:185][0m |          -0.0072 |           8.2847 |           0.2278 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0086 |           8.2070 |           0.2280 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0116 |           8.1448 |           0.2278 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0085 |           8.4480 |           0.2279 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0159 |           8.0529 |           0.2278 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0144 |           8.0373 |           0.2277 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0132 |           8.0949 |           0.2276 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0182 |           7.9414 |           0.2276 |
[32m[20221213 15:11:48 @agent_ppo2.py:185][0m |          -0.0141 |           7.9606 |           0.2276 |
[32m[20221213 15:11:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 177.01
[32m[20221213 15:11:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 186.56
[32m[20221213 15:11:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 203.06
[32m[20221213 15:11:48 @agent_ppo2.py:143][0m Total time:      18.89 min
[32m[20221213 15:11:48 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221213 15:11:48 @agent_ppo2.py:121][0m #------------------------ Iteration 830 --------------------------#
[32m[20221213 15:11:49 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:11:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |           0.0105 |           8.2129 |           0.2256 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0043 |           7.5002 |           0.2250 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0102 |           7.2259 |           0.2247 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0101 |           7.0415 |           0.2245 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0125 |           6.8947 |           0.2244 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0158 |           6.8041 |           0.2244 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0169 |           6.6991 |           0.2242 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0175 |           6.6019 |           0.2241 |
[32m[20221213 15:11:49 @agent_ppo2.py:185][0m |          -0.0167 |           6.5397 |           0.2240 |
[32m[20221213 15:11:50 @agent_ppo2.py:185][0m |          -0.0192 |           6.5100 |           0.2239 |
[32m[20221213 15:11:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:11:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 173.72
[32m[20221213 15:11:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 183.00
[32m[20221213 15:11:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 186.86
[32m[20221213 15:11:50 @agent_ppo2.py:143][0m Total time:      18.91 min
[32m[20221213 15:11:50 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221213 15:11:50 @agent_ppo2.py:121][0m #------------------------ Iteration 831 --------------------------#
[32m[20221213 15:11:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:50 @agent_ppo2.py:185][0m |           0.0123 |          10.5501 |           0.2142 |
[32m[20221213 15:11:50 @agent_ppo2.py:185][0m |          -0.0056 |           9.0134 |           0.2133 |
[32m[20221213 15:11:50 @agent_ppo2.py:185][0m |          -0.0072 |           8.9274 |           0.2139 |
[32m[20221213 15:11:50 @agent_ppo2.py:185][0m |          -0.0142 |           8.7341 |           0.2136 |
[32m[20221213 15:11:50 @agent_ppo2.py:185][0m |          -0.0117 |           8.6916 |           0.2137 |
[32m[20221213 15:11:50 @agent_ppo2.py:185][0m |          -0.0116 |           8.6723 |           0.2136 |
[32m[20221213 15:11:51 @agent_ppo2.py:185][0m |          -0.0173 |           8.5786 |           0.2136 |
[32m[20221213 15:11:51 @agent_ppo2.py:185][0m |          -0.0173 |           8.5214 |           0.2137 |
[32m[20221213 15:11:51 @agent_ppo2.py:185][0m |          -0.0187 |           8.4868 |           0.2137 |
[32m[20221213 15:11:51 @agent_ppo2.py:185][0m |          -0.0161 |           8.4610 |           0.2135 |
[32m[20221213 15:11:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 155.76
[32m[20221213 15:11:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 183.73
[32m[20221213 15:11:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 201.44
[32m[20221213 15:11:51 @agent_ppo2.py:143][0m Total time:      18.93 min
[32m[20221213 15:11:51 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221213 15:11:51 @agent_ppo2.py:121][0m #------------------------ Iteration 832 --------------------------#
[32m[20221213 15:11:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:51 @agent_ppo2.py:185][0m |          -0.0048 |           8.0495 |           0.2224 |
[32m[20221213 15:11:51 @agent_ppo2.py:185][0m |          -0.0070 |           7.5180 |           0.2220 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0058 |           7.4452 |           0.2216 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0118 |           7.0917 |           0.2215 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0152 |           6.9518 |           0.2214 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0194 |           6.8191 |           0.2213 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0179 |           6.6778 |           0.2214 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0177 |           6.5999 |           0.2213 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0178 |           6.5527 |           0.2213 |
[32m[20221213 15:11:52 @agent_ppo2.py:185][0m |          -0.0194 |           6.3745 |           0.2212 |
[32m[20221213 15:11:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 158.10
[32m[20221213 15:11:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 171.97
[32m[20221213 15:11:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.38
[32m[20221213 15:11:52 @agent_ppo2.py:143][0m Total time:      18.95 min
[32m[20221213 15:11:52 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221213 15:11:52 @agent_ppo2.py:121][0m #------------------------ Iteration 833 --------------------------#
[32m[20221213 15:11:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |           0.0108 |           8.7329 |           0.2190 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0076 |           7.6158 |           0.2181 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0096 |           7.5591 |           0.2182 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0123 |           7.3449 |           0.2179 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0129 |           7.3007 |           0.2179 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0157 |           7.2061 |           0.2177 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0183 |           7.1655 |           0.2180 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0189 |           7.0943 |           0.2178 |
[32m[20221213 15:11:53 @agent_ppo2.py:185][0m |          -0.0212 |           7.0496 |           0.2178 |
[32m[20221213 15:11:54 @agent_ppo2.py:185][0m |          -0.0188 |           7.0052 |           0.2180 |
[32m[20221213 15:11:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 142.69
[32m[20221213 15:11:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 152.97
[32m[20221213 15:11:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 147.96
[32m[20221213 15:11:54 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221213 15:11:54 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221213 15:11:54 @agent_ppo2.py:121][0m #------------------------ Iteration 834 --------------------------#
[32m[20221213 15:11:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:54 @agent_ppo2.py:185][0m |          -0.0009 |           8.7526 |           0.2220 |
[32m[20221213 15:11:54 @agent_ppo2.py:185][0m |          -0.0026 |           8.6635 |           0.2214 |
[32m[20221213 15:11:54 @agent_ppo2.py:185][0m |          -0.0078 |           8.4410 |           0.2215 |
[32m[20221213 15:11:54 @agent_ppo2.py:185][0m |          -0.0089 |           8.3403 |           0.2214 |
[32m[20221213 15:11:54 @agent_ppo2.py:185][0m |          -0.0149 |           8.2538 |           0.2213 |
[32m[20221213 15:11:55 @agent_ppo2.py:185][0m |          -0.0119 |           8.2003 |           0.2215 |
[32m[20221213 15:11:55 @agent_ppo2.py:185][0m |          -0.0094 |           8.3164 |           0.2213 |
[32m[20221213 15:11:55 @agent_ppo2.py:185][0m |          -0.0183 |           8.0861 |           0.2214 |
[32m[20221213 15:11:55 @agent_ppo2.py:185][0m |          -0.0093 |           8.7405 |           0.2215 |
[32m[20221213 15:11:55 @agent_ppo2.py:185][0m |          -0.0125 |           8.3435 |           0.2216 |
[32m[20221213 15:11:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 168.76
[32m[20221213 15:11:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 177.82
[32m[20221213 15:11:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.11
[32m[20221213 15:11:55 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221213 15:11:55 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221213 15:11:55 @agent_ppo2.py:121][0m #------------------------ Iteration 835 --------------------------#
[32m[20221213 15:11:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:55 @agent_ppo2.py:185][0m |          -0.0016 |           8.8165 |           0.2189 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0083 |           8.4850 |           0.2188 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0115 |           8.3179 |           0.2189 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0134 |           8.1935 |           0.2188 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0098 |           8.1432 |           0.2191 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0174 |           8.0212 |           0.2192 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0182 |           7.9541 |           0.2192 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0188 |           7.8835 |           0.2194 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0096 |           8.4828 |           0.2194 |
[32m[20221213 15:11:56 @agent_ppo2.py:185][0m |          -0.0168 |           7.8169 |           0.2195 |
[32m[20221213 15:11:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 172.21
[32m[20221213 15:11:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.55
[32m[20221213 15:11:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 197.59
[32m[20221213 15:11:56 @agent_ppo2.py:143][0m Total time:      19.02 min
[32m[20221213 15:11:56 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221213 15:11:56 @agent_ppo2.py:121][0m #------------------------ Iteration 836 --------------------------#
[32m[20221213 15:11:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |           0.0012 |           8.2782 |           0.2211 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0050 |           7.9664 |           0.2211 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0058 |           7.8654 |           0.2208 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0105 |           7.7503 |           0.2206 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0111 |           7.6820 |           0.2206 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0143 |           7.6172 |           0.2206 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0150 |           7.5676 |           0.2205 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0165 |           7.5273 |           0.2207 |
[32m[20221213 15:11:57 @agent_ppo2.py:185][0m |          -0.0165 |           7.4846 |           0.2207 |
[32m[20221213 15:11:58 @agent_ppo2.py:185][0m |          -0.0103 |           7.7922 |           0.2208 |
[32m[20221213 15:11:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.40
[32m[20221213 15:11:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 181.36
[32m[20221213 15:11:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 207.31
[32m[20221213 15:11:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 207.31
[32m[20221213 15:11:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 207.31
[32m[20221213 15:11:58 @agent_ppo2.py:143][0m Total time:      19.04 min
[32m[20221213 15:11:58 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221213 15:11:58 @agent_ppo2.py:121][0m #------------------------ Iteration 837 --------------------------#
[32m[20221213 15:11:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:11:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:58 @agent_ppo2.py:185][0m |           0.0022 |           8.6081 |           0.2217 |
[32m[20221213 15:11:58 @agent_ppo2.py:185][0m |          -0.0090 |           8.2795 |           0.2215 |
[32m[20221213 15:11:58 @agent_ppo2.py:185][0m |          -0.0099 |           8.1222 |           0.2211 |
[32m[20221213 15:11:58 @agent_ppo2.py:185][0m |          -0.0157 |           8.0003 |           0.2210 |
[32m[20221213 15:11:58 @agent_ppo2.py:185][0m |          -0.0022 |           8.5629 |           0.2210 |
[32m[20221213 15:11:59 @agent_ppo2.py:185][0m |          -0.0134 |           7.8189 |           0.2206 |
[32m[20221213 15:11:59 @agent_ppo2.py:185][0m |          -0.0204 |           7.7195 |           0.2209 |
[32m[20221213 15:11:59 @agent_ppo2.py:185][0m |          -0.0213 |           7.6699 |           0.2207 |
[32m[20221213 15:11:59 @agent_ppo2.py:185][0m |          -0.0185 |           7.5774 |           0.2208 |
[32m[20221213 15:11:59 @agent_ppo2.py:185][0m |          -0.0134 |           8.0485 |           0.2208 |
[32m[20221213 15:11:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:11:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 170.96
[32m[20221213 15:11:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 179.42
[32m[20221213 15:11:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 185.89
[32m[20221213 15:11:59 @agent_ppo2.py:143][0m Total time:      19.07 min
[32m[20221213 15:11:59 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221213 15:11:59 @agent_ppo2.py:121][0m #------------------------ Iteration 838 --------------------------#
[32m[20221213 15:11:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:11:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:11:59 @agent_ppo2.py:185][0m |           0.0012 |           9.1359 |           0.2223 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0050 |           8.6538 |           0.2221 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0103 |           8.4470 |           0.2219 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0036 |           8.8257 |           0.2222 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0135 |           8.1622 |           0.2224 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0145 |           8.0238 |           0.2225 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0034 |           8.3770 |           0.2226 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0138 |           7.8387 |           0.2223 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0179 |           7.7274 |           0.2228 |
[32m[20221213 15:12:00 @agent_ppo2.py:185][0m |          -0.0191 |           7.6754 |           0.2227 |
[32m[20221213 15:12:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:12:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 173.33
[32m[20221213 15:12:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 183.73
[32m[20221213 15:12:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 183.48
[32m[20221213 15:12:00 @agent_ppo2.py:143][0m Total time:      19.09 min
[32m[20221213 15:12:00 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221213 15:12:00 @agent_ppo2.py:121][0m #------------------------ Iteration 839 --------------------------#
[32m[20221213 15:12:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:01 @agent_ppo2.py:185][0m |           0.0042 |           9.1239 |           0.2287 |
[32m[20221213 15:12:01 @agent_ppo2.py:185][0m |           0.0020 |           9.2395 |           0.2281 |
[32m[20221213 15:12:01 @agent_ppo2.py:185][0m |          -0.0079 |           8.6898 |           0.2274 |
[32m[20221213 15:12:01 @agent_ppo2.py:185][0m |          -0.0126 |           8.6494 |           0.2273 |
[32m[20221213 15:12:01 @agent_ppo2.py:185][0m |          -0.0137 |           8.5779 |           0.2273 |
[32m[20221213 15:12:01 @agent_ppo2.py:185][0m |          -0.0177 |           8.5546 |           0.2272 |
[32m[20221213 15:12:01 @agent_ppo2.py:185][0m |           0.0006 |           9.5880 |           0.2272 |
[32m[20221213 15:12:02 @agent_ppo2.py:185][0m |          -0.0156 |           8.5309 |           0.2267 |
[32m[20221213 15:12:02 @agent_ppo2.py:185][0m |          -0.0159 |           8.4096 |           0.2268 |
[32m[20221213 15:12:02 @agent_ppo2.py:185][0m |          -0.0157 |           8.4075 |           0.2269 |
[32m[20221213 15:12:02 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 15:12:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 154.73
[32m[20221213 15:12:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 181.59
[32m[20221213 15:12:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 69.54
[32m[20221213 15:12:02 @agent_ppo2.py:143][0m Total time:      19.11 min
[32m[20221213 15:12:02 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221213 15:12:02 @agent_ppo2.py:121][0m #------------------------ Iteration 840 --------------------------#
[32m[20221213 15:12:02 @agent_ppo2.py:127][0m Sampling time: 0.36 s by 5 slaves
[32m[20221213 15:12:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0008 |           8.7010 |           0.2252 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0037 |           8.4655 |           0.2247 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0110 |           8.2292 |           0.2243 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0151 |           8.0815 |           0.2247 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0139 |           7.9935 |           0.2243 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0041 |           8.1426 |           0.2242 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0153 |           7.7740 |           0.2242 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0191 |           7.6654 |           0.2240 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0158 |           7.5824 |           0.2239 |
[32m[20221213 15:12:03 @agent_ppo2.py:185][0m |          -0.0091 |           8.2414 |           0.2239 |
[32m[20221213 15:12:03 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:12:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 142.39
[32m[20221213 15:12:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 161.37
[32m[20221213 15:12:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.34
[32m[20221213 15:12:04 @agent_ppo2.py:143][0m Total time:      19.14 min
[32m[20221213 15:12:04 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221213 15:12:04 @agent_ppo2.py:121][0m #------------------------ Iteration 841 --------------------------#
[32m[20221213 15:12:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:04 @agent_ppo2.py:185][0m |           0.0011 |           8.5324 |           0.2205 |
[32m[20221213 15:12:04 @agent_ppo2.py:185][0m |          -0.0065 |           8.1927 |           0.2206 |
[32m[20221213 15:12:04 @agent_ppo2.py:185][0m |          -0.0174 |           8.0847 |           0.2202 |
[32m[20221213 15:12:04 @agent_ppo2.py:185][0m |          -0.0142 |           7.9579 |           0.2201 |
[32m[20221213 15:12:04 @agent_ppo2.py:185][0m |          -0.0115 |           7.8803 |           0.2199 |
[32m[20221213 15:12:04 @agent_ppo2.py:185][0m |          -0.0178 |           7.8307 |           0.2198 |
[32m[20221213 15:12:04 @agent_ppo2.py:185][0m |          -0.0229 |           7.7903 |           0.2197 |
[32m[20221213 15:12:05 @agent_ppo2.py:185][0m |          -0.0193 |           7.6931 |           0.2196 |
[32m[20221213 15:12:05 @agent_ppo2.py:185][0m |          -0.0196 |           7.6256 |           0.2195 |
[32m[20221213 15:12:05 @agent_ppo2.py:185][0m |          -0.0243 |           7.5847 |           0.2193 |
[32m[20221213 15:12:05 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:12:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 156.36
[32m[20221213 15:12:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 170.00
[32m[20221213 15:12:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 185.68
[32m[20221213 15:12:05 @agent_ppo2.py:143][0m Total time:      19.16 min
[32m[20221213 15:12:05 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221213 15:12:05 @agent_ppo2.py:121][0m #------------------------ Iteration 842 --------------------------#
[32m[20221213 15:12:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:05 @agent_ppo2.py:185][0m |           0.0005 |           8.2279 |           0.2239 |
[32m[20221213 15:12:05 @agent_ppo2.py:185][0m |          -0.0109 |           7.4308 |           0.2234 |
[32m[20221213 15:12:05 @agent_ppo2.py:185][0m |          -0.0125 |           7.0835 |           0.2233 |
[32m[20221213 15:12:06 @agent_ppo2.py:185][0m |          -0.0169 |           6.8878 |           0.2232 |
[32m[20221213 15:12:06 @agent_ppo2.py:185][0m |          -0.0157 |           6.7455 |           0.2232 |
[32m[20221213 15:12:06 @agent_ppo2.py:185][0m |          -0.0200 |           6.6118 |           0.2233 |
[32m[20221213 15:12:06 @agent_ppo2.py:185][0m |          -0.0204 |           6.5488 |           0.2233 |
[32m[20221213 15:12:06 @agent_ppo2.py:185][0m |          -0.0172 |           6.4469 |           0.2233 |
[32m[20221213 15:12:06 @agent_ppo2.py:185][0m |          -0.0126 |           6.4755 |           0.2236 |
[32m[20221213 15:12:06 @agent_ppo2.py:185][0m |          -0.0202 |           6.3180 |           0.2234 |
[32m[20221213 15:12:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:12:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 157.67
[32m[20221213 15:12:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 165.35
[32m[20221213 15:12:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 181.52
[32m[20221213 15:12:06 @agent_ppo2.py:143][0m Total time:      19.19 min
[32m[20221213 15:12:06 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221213 15:12:06 @agent_ppo2.py:121][0m #------------------------ Iteration 843 --------------------------#
[32m[20221213 15:12:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0051 |           9.1099 |           0.2229 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0103 |           8.7277 |           0.2226 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0106 |           8.6255 |           0.2228 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0142 |           8.5207 |           0.2227 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0165 |           8.4483 |           0.2231 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0157 |           8.4181 |           0.2229 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0184 |           8.3456 |           0.2232 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0212 |           8.2990 |           0.2231 |
[32m[20221213 15:12:07 @agent_ppo2.py:185][0m |          -0.0207 |           8.2617 |           0.2235 |
[32m[20221213 15:12:08 @agent_ppo2.py:185][0m |          -0.0190 |           8.2418 |           0.2234 |
[32m[20221213 15:12:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:12:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.20
[32m[20221213 15:12:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 177.17
[32m[20221213 15:12:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 184.44
[32m[20221213 15:12:08 @agent_ppo2.py:143][0m Total time:      19.21 min
[32m[20221213 15:12:08 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221213 15:12:08 @agent_ppo2.py:121][0m #------------------------ Iteration 844 --------------------------#
[32m[20221213 15:12:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:08 @agent_ppo2.py:185][0m |           0.0051 |           8.9213 |           0.2290 |
[32m[20221213 15:12:08 @agent_ppo2.py:185][0m |          -0.0014 |           8.4596 |           0.2286 |
[32m[20221213 15:12:08 @agent_ppo2.py:185][0m |          -0.0125 |           8.1070 |           0.2287 |
[32m[20221213 15:12:08 @agent_ppo2.py:185][0m |          -0.0130 |           7.9747 |           0.2288 |
[32m[20221213 15:12:08 @agent_ppo2.py:185][0m |          -0.0168 |           7.9097 |           0.2286 |
[32m[20221213 15:12:09 @agent_ppo2.py:185][0m |          -0.0162 |           7.8224 |           0.2287 |
[32m[20221213 15:12:09 @agent_ppo2.py:185][0m |          -0.0156 |           7.7599 |           0.2287 |
[32m[20221213 15:12:09 @agent_ppo2.py:185][0m |          -0.0191 |           7.6817 |           0.2288 |
[32m[20221213 15:12:09 @agent_ppo2.py:185][0m |          -0.0183 |           7.6499 |           0.2288 |
[32m[20221213 15:12:09 @agent_ppo2.py:185][0m |          -0.0208 |           7.6236 |           0.2289 |
[32m[20221213 15:12:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:12:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.57
[32m[20221213 15:12:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.38
[32m[20221213 15:12:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 215.21
[32m[20221213 15:12:09 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 215.21
[32m[20221213 15:12:09 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 215.21
[32m[20221213 15:12:09 @agent_ppo2.py:143][0m Total time:      19.23 min
[32m[20221213 15:12:09 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221213 15:12:09 @agent_ppo2.py:121][0m #------------------------ Iteration 845 --------------------------#
[32m[20221213 15:12:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |           0.0099 |          10.4385 |           0.2268 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0061 |           9.3656 |           0.2263 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0062 |           9.2137 |           0.2263 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0116 |           9.1382 |           0.2259 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0108 |           9.0658 |           0.2260 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0140 |           9.0090 |           0.2258 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0160 |           8.9245 |           0.2259 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0131 |           8.8901 |           0.2259 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0184 |           8.8303 |           0.2260 |
[32m[20221213 15:12:10 @agent_ppo2.py:185][0m |          -0.0186 |           8.7485 |           0.2257 |
[32m[20221213 15:12:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:12:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.00
[32m[20221213 15:12:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 196.21
[32m[20221213 15:12:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.32
[32m[20221213 15:12:11 @agent_ppo2.py:143][0m Total time:      19.26 min
[32m[20221213 15:12:11 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221213 15:12:11 @agent_ppo2.py:121][0m #------------------------ Iteration 846 --------------------------#
[32m[20221213 15:12:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:11 @agent_ppo2.py:185][0m |          -0.0007 |           8.7968 |           0.2278 |
[32m[20221213 15:12:11 @agent_ppo2.py:185][0m |          -0.0096 |           8.4413 |           0.2275 |
[32m[20221213 15:12:11 @agent_ppo2.py:185][0m |          -0.0055 |           8.4746 |           0.2274 |
[32m[20221213 15:12:11 @agent_ppo2.py:185][0m |          -0.0101 |           8.2659 |           0.2272 |
[32m[20221213 15:12:11 @agent_ppo2.py:185][0m |          -0.0144 |           8.1268 |           0.2275 |
[32m[20221213 15:12:11 @agent_ppo2.py:185][0m |          -0.0178 |           8.0615 |           0.2273 |
[32m[20221213 15:12:11 @agent_ppo2.py:185][0m |          -0.0157 |           8.0053 |           0.2274 |
[32m[20221213 15:12:12 @agent_ppo2.py:185][0m |          -0.0167 |           7.9593 |           0.2273 |
[32m[20221213 15:12:12 @agent_ppo2.py:185][0m |          -0.0194 |           7.9066 |           0.2271 |
[32m[20221213 15:12:12 @agent_ppo2.py:185][0m |          -0.0158 |           7.9655 |           0.2273 |
[32m[20221213 15:12:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 183.59
[32m[20221213 15:12:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 201.42
[32m[20221213 15:12:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 173.74
[32m[20221213 15:12:12 @agent_ppo2.py:143][0m Total time:      19.28 min
[32m[20221213 15:12:12 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221213 15:12:12 @agent_ppo2.py:121][0m #------------------------ Iteration 847 --------------------------#
[32m[20221213 15:12:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:12 @agent_ppo2.py:185][0m |           0.0022 |           9.7602 |           0.2308 |
[32m[20221213 15:12:12 @agent_ppo2.py:185][0m |          -0.0054 |           8.9675 |           0.2307 |
[32m[20221213 15:12:12 @agent_ppo2.py:185][0m |          -0.0106 |           8.8071 |           0.2307 |
[32m[20221213 15:12:13 @agent_ppo2.py:185][0m |          -0.0078 |           8.8940 |           0.2307 |
[32m[20221213 15:12:13 @agent_ppo2.py:185][0m |          -0.0153 |           8.6158 |           0.2308 |
[32m[20221213 15:12:13 @agent_ppo2.py:185][0m |          -0.0160 |           8.5392 |           0.2306 |
[32m[20221213 15:12:13 @agent_ppo2.py:185][0m |          -0.0069 |           8.9069 |           0.2307 |
[32m[20221213 15:12:13 @agent_ppo2.py:185][0m |          -0.0154 |           8.4035 |           0.2304 |
[32m[20221213 15:12:13 @agent_ppo2.py:185][0m |          -0.0153 |           8.3674 |           0.2303 |
[32m[20221213 15:12:13 @agent_ppo2.py:185][0m |          -0.0132 |           8.3544 |           0.2306 |
[32m[20221213 15:12:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:12:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.18
[32m[20221213 15:12:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 183.88
[32m[20221213 15:12:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 108.46
[32m[20221213 15:12:13 @agent_ppo2.py:143][0m Total time:      19.30 min
[32m[20221213 15:12:13 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221213 15:12:13 @agent_ppo2.py:121][0m #------------------------ Iteration 848 --------------------------#
[32m[20221213 15:12:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0012 |           9.4167 |           0.2308 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |           0.0038 |          10.2875 |           0.2306 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0063 |           9.0036 |           0.2301 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0128 |           8.8196 |           0.2304 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0147 |           8.6948 |           0.2303 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0173 |           8.6193 |           0.2306 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0179 |           8.5520 |           0.2305 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0186 |           8.5185 |           0.2305 |
[32m[20221213 15:12:14 @agent_ppo2.py:185][0m |          -0.0188 |           8.3984 |           0.2307 |
[32m[20221213 15:12:15 @agent_ppo2.py:185][0m |          -0.0188 |           8.3507 |           0.2310 |
[32m[20221213 15:12:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:12:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 169.86
[32m[20221213 15:12:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 191.44
[32m[20221213 15:12:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.53
[32m[20221213 15:12:15 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 218.53
[32m[20221213 15:12:15 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 218.53
[32m[20221213 15:12:15 @agent_ppo2.py:143][0m Total time:      19.33 min
[32m[20221213 15:12:15 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221213 15:12:15 @agent_ppo2.py:121][0m #------------------------ Iteration 849 --------------------------#
[32m[20221213 15:12:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:15 @agent_ppo2.py:185][0m |          -0.0030 |           9.0611 |           0.2290 |
[32m[20221213 15:12:15 @agent_ppo2.py:185][0m |          -0.0089 |           8.8912 |           0.2288 |
[32m[20221213 15:12:15 @agent_ppo2.py:185][0m |          -0.0122 |           8.7887 |           0.2285 |
[32m[20221213 15:12:15 @agent_ppo2.py:185][0m |          -0.0144 |           8.7232 |           0.2283 |
[32m[20221213 15:12:15 @agent_ppo2.py:185][0m |          -0.0153 |           8.6507 |           0.2283 |
[32m[20221213 15:12:16 @agent_ppo2.py:185][0m |          -0.0174 |           8.5881 |           0.2281 |
[32m[20221213 15:12:16 @agent_ppo2.py:185][0m |          -0.0189 |           8.5583 |           0.2280 |
[32m[20221213 15:12:16 @agent_ppo2.py:185][0m |          -0.0131 |           8.7721 |           0.2278 |
[32m[20221213 15:12:16 @agent_ppo2.py:185][0m |          -0.0181 |           8.4587 |           0.2278 |
[32m[20221213 15:12:16 @agent_ppo2.py:185][0m |          -0.0193 |           8.4223 |           0.2278 |
[32m[20221213 15:12:16 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:12:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 179.46
[32m[20221213 15:12:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 196.32
[32m[20221213 15:12:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 202.29
[32m[20221213 15:12:16 @agent_ppo2.py:143][0m Total time:      19.35 min
[32m[20221213 15:12:16 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221213 15:12:16 @agent_ppo2.py:121][0m #------------------------ Iteration 850 --------------------------#
[32m[20221213 15:12:16 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:12:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0005 |           9.4410 |           0.2295 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0134 |           9.2549 |           0.2294 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0114 |           9.1395 |           0.2293 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0129 |           9.0533 |           0.2291 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0139 |           9.0120 |           0.2292 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0164 |           8.9575 |           0.2293 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0188 |           8.9179 |           0.2293 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0170 |           8.8871 |           0.2291 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0205 |           8.8601 |           0.2293 |
[32m[20221213 15:12:17 @agent_ppo2.py:185][0m |          -0.0150 |           8.8552 |           0.2293 |
[32m[20221213 15:12:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:12:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 170.11
[32m[20221213 15:12:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 180.47
[32m[20221213 15:12:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.74
[32m[20221213 15:12:17 @agent_ppo2.py:143][0m Total time:      19.37 min
[32m[20221213 15:12:17 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221213 15:12:18 @agent_ppo2.py:121][0m #------------------------ Iteration 851 --------------------------#
[32m[20221213 15:12:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0021 |           9.0566 |           0.2317 |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0076 |           8.8040 |           0.2319 |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0132 |           8.6634 |           0.2320 |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0058 |           8.9766 |           0.2321 |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0116 |           8.5331 |           0.2323 |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0073 |           8.6797 |           0.2324 |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0156 |           8.1908 |           0.2322 |
[32m[20221213 15:12:18 @agent_ppo2.py:185][0m |          -0.0161 |           8.0945 |           0.2326 |
[32m[20221213 15:12:19 @agent_ppo2.py:185][0m |          -0.0071 |           8.6855 |           0.2329 |
[32m[20221213 15:12:19 @agent_ppo2.py:185][0m |          -0.0131 |           8.1960 |           0.2327 |
[32m[20221213 15:12:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:12:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.57
[32m[20221213 15:12:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 196.70
[32m[20221213 15:12:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 177.30
[32m[20221213 15:12:19 @agent_ppo2.py:143][0m Total time:      19.39 min
[32m[20221213 15:12:19 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221213 15:12:19 @agent_ppo2.py:121][0m #------------------------ Iteration 852 --------------------------#
[32m[20221213 15:12:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:19 @agent_ppo2.py:185][0m |          -0.0000 |           9.5568 |           0.2324 |
[32m[20221213 15:12:19 @agent_ppo2.py:185][0m |           0.0076 |          10.0573 |           0.2320 |
[32m[20221213 15:12:19 @agent_ppo2.py:185][0m |          -0.0076 |           9.0137 |           0.2320 |
[32m[20221213 15:12:19 @agent_ppo2.py:185][0m |          -0.0128 |           8.7806 |           0.2319 |
[32m[20221213 15:12:20 @agent_ppo2.py:185][0m |          -0.0058 |           9.3168 |           0.2320 |
[32m[20221213 15:12:20 @agent_ppo2.py:185][0m |          -0.0133 |           8.6944 |           0.2319 |
[32m[20221213 15:12:20 @agent_ppo2.py:185][0m |          -0.0148 |           8.4737 |           0.2321 |
[32m[20221213 15:12:20 @agent_ppo2.py:185][0m |          -0.0134 |           8.4116 |           0.2318 |
[32m[20221213 15:12:20 @agent_ppo2.py:185][0m |          -0.0142 |           8.4759 |           0.2319 |
[32m[20221213 15:12:20 @agent_ppo2.py:185][0m |          -0.0139 |           8.3991 |           0.2319 |
[32m[20221213 15:12:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:12:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 157.62
[32m[20221213 15:12:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 174.57
[32m[20221213 15:12:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.48
[32m[20221213 15:12:20 @agent_ppo2.py:143][0m Total time:      19.42 min
[32m[20221213 15:12:20 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221213 15:12:20 @agent_ppo2.py:121][0m #------------------------ Iteration 853 --------------------------#
[32m[20221213 15:12:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |           0.0035 |           9.4289 |           0.2394 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0073 |           9.1797 |           0.2387 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0039 |           9.0641 |           0.2388 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0143 |           8.9516 |           0.2387 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0122 |           8.9282 |           0.2384 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0085 |           8.8768 |           0.2385 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0161 |           8.7306 |           0.2384 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0144 |           8.7290 |           0.2384 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0180 |           8.6278 |           0.2381 |
[32m[20221213 15:12:21 @agent_ppo2.py:185][0m |          -0.0188 |           8.5826 |           0.2384 |
[32m[20221213 15:12:21 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:12:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 171.28
[32m[20221213 15:12:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 177.59
[32m[20221213 15:12:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 166.51
[32m[20221213 15:12:22 @agent_ppo2.py:143][0m Total time:      19.44 min
[32m[20221213 15:12:22 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221213 15:12:22 @agent_ppo2.py:121][0m #------------------------ Iteration 854 --------------------------#
[32m[20221213 15:12:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:22 @agent_ppo2.py:185][0m |          -0.0013 |           8.9897 |           0.2323 |
[32m[20221213 15:12:22 @agent_ppo2.py:185][0m |          -0.0084 |           8.7259 |           0.2321 |
[32m[20221213 15:12:22 @agent_ppo2.py:185][0m |           0.0011 |           9.8399 |           0.2318 |
[32m[20221213 15:12:22 @agent_ppo2.py:185][0m |          -0.0106 |           8.6137 |           0.2315 |
[32m[20221213 15:12:22 @agent_ppo2.py:185][0m |          -0.0099 |           8.5546 |           0.2315 |
[32m[20221213 15:12:22 @agent_ppo2.py:185][0m |          -0.0159 |           8.3814 |           0.2315 |
[32m[20221213 15:12:23 @agent_ppo2.py:185][0m |          -0.0172 |           8.3428 |           0.2313 |
[32m[20221213 15:12:23 @agent_ppo2.py:185][0m |          -0.0172 |           8.3025 |           0.2313 |
[32m[20221213 15:12:23 @agent_ppo2.py:185][0m |          -0.0198 |           8.2315 |           0.2312 |
[32m[20221213 15:12:23 @agent_ppo2.py:185][0m |          -0.0174 |           8.2342 |           0.2311 |
[32m[20221213 15:12:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.93
[32m[20221213 15:12:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 191.64
[32m[20221213 15:12:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 196.75
[32m[20221213 15:12:23 @agent_ppo2.py:143][0m Total time:      19.46 min
[32m[20221213 15:12:23 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221213 15:12:23 @agent_ppo2.py:121][0m #------------------------ Iteration 855 --------------------------#
[32m[20221213 15:12:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:23 @agent_ppo2.py:185][0m |           0.0000 |           8.2604 |           0.2368 |
[32m[20221213 15:12:23 @agent_ppo2.py:185][0m |          -0.0118 |           7.6848 |           0.2366 |
[32m[20221213 15:12:23 @agent_ppo2.py:185][0m |          -0.0152 |           7.5120 |           0.2364 |
[32m[20221213 15:12:24 @agent_ppo2.py:185][0m |          -0.0181 |           7.3460 |           0.2362 |
[32m[20221213 15:12:24 @agent_ppo2.py:185][0m |          -0.0178 |           7.2407 |           0.2364 |
[32m[20221213 15:12:24 @agent_ppo2.py:185][0m |          -0.0187 |           7.1497 |           0.2362 |
[32m[20221213 15:12:24 @agent_ppo2.py:185][0m |          -0.0225 |           7.0511 |           0.2362 |
[32m[20221213 15:12:24 @agent_ppo2.py:185][0m |          -0.0217 |           6.9676 |           0.2361 |
[32m[20221213 15:12:24 @agent_ppo2.py:185][0m |          -0.0175 |           6.8833 |           0.2359 |
[32m[20221213 15:12:24 @agent_ppo2.py:185][0m |          -0.0229 |           6.8078 |           0.2361 |
[32m[20221213 15:12:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 181.04
[32m[20221213 15:12:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 192.05
[32m[20221213 15:12:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.23
[32m[20221213 15:12:24 @agent_ppo2.py:143][0m Total time:      19.49 min
[32m[20221213 15:12:24 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221213 15:12:24 @agent_ppo2.py:121][0m #------------------------ Iteration 856 --------------------------#
[32m[20221213 15:12:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:12:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0030 |           9.3880 |           0.2346 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0084 |           9.1242 |           0.2341 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0044 |           9.0993 |           0.2340 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0095 |           9.0133 |           0.2343 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0144 |           8.8729 |           0.2339 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0170 |           8.8132 |           0.2337 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0169 |           8.7094 |           0.2338 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0171 |           8.6536 |           0.2337 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0179 |           8.6301 |           0.2337 |
[32m[20221213 15:12:25 @agent_ppo2.py:185][0m |          -0.0198 |           8.5909 |           0.2336 |
[32m[20221213 15:12:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 177.56
[32m[20221213 15:12:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 199.35
[32m[20221213 15:12:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.07
[32m[20221213 15:12:26 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 225.07
[32m[20221213 15:12:26 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 225.07
[32m[20221213 15:12:26 @agent_ppo2.py:143][0m Total time:      19.51 min
[32m[20221213 15:12:26 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221213 15:12:26 @agent_ppo2.py:121][0m #------------------------ Iteration 857 --------------------------#
[32m[20221213 15:12:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:26 @agent_ppo2.py:185][0m |          -0.0001 |           9.3332 |           0.2360 |
[32m[20221213 15:12:26 @agent_ppo2.py:185][0m |           0.0073 |           9.1744 |           0.2357 |
[32m[20221213 15:12:26 @agent_ppo2.py:185][0m |          -0.0088 |           8.4087 |           0.2353 |
[32m[20221213 15:12:26 @agent_ppo2.py:185][0m |          -0.0112 |           8.2501 |           0.2353 |
[32m[20221213 15:12:26 @agent_ppo2.py:185][0m |          -0.0119 |           8.1223 |           0.2352 |
[32m[20221213 15:12:26 @agent_ppo2.py:185][0m |          -0.0082 |           8.5570 |           0.2352 |
[32m[20221213 15:12:27 @agent_ppo2.py:185][0m |          -0.0174 |           8.0107 |           0.2350 |
[32m[20221213 15:12:27 @agent_ppo2.py:185][0m |          -0.0125 |           7.9119 |           0.2350 |
[32m[20221213 15:12:27 @agent_ppo2.py:185][0m |          -0.0156 |           7.7873 |           0.2351 |
[32m[20221213 15:12:27 @agent_ppo2.py:185][0m |          -0.0160 |           7.7345 |           0.2349 |
[32m[20221213 15:12:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 180.12
[32m[20221213 15:12:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 190.91
[32m[20221213 15:12:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.73
[32m[20221213 15:12:27 @agent_ppo2.py:143][0m Total time:      19.53 min
[32m[20221213 15:12:27 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221213 15:12:27 @agent_ppo2.py:121][0m #------------------------ Iteration 858 --------------------------#
[32m[20221213 15:12:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:27 @agent_ppo2.py:185][0m |           0.0078 |          10.5983 |           0.2365 |
[32m[20221213 15:12:27 @agent_ppo2.py:185][0m |          -0.0049 |           9.2580 |           0.2355 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0109 |           8.9732 |           0.2357 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0116 |           8.8079 |           0.2356 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0164 |           8.6261 |           0.2355 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0172 |           8.5280 |           0.2355 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0154 |           8.4190 |           0.2357 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0160 |           8.3163 |           0.2356 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0192 |           8.2551 |           0.2354 |
[32m[20221213 15:12:28 @agent_ppo2.py:185][0m |          -0.0208 |           8.1611 |           0.2354 |
[32m[20221213 15:12:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 171.32
[32m[20221213 15:12:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 177.47
[32m[20221213 15:12:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.87
[32m[20221213 15:12:28 @agent_ppo2.py:143][0m Total time:      19.55 min
[32m[20221213 15:12:28 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221213 15:12:28 @agent_ppo2.py:121][0m #------------------------ Iteration 859 --------------------------#
[32m[20221213 15:12:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |           0.0005 |           9.6225 |           0.2385 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0058 |           9.2587 |           0.2383 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0103 |           9.0726 |           0.2382 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0093 |           8.9571 |           0.2383 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0124 |           8.8611 |           0.2381 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0132 |           8.7820 |           0.2385 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0075 |           9.0224 |           0.2383 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0181 |           8.6454 |           0.2382 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0195 |           8.6054 |           0.2386 |
[32m[20221213 15:12:29 @agent_ppo2.py:185][0m |          -0.0179 |           8.5225 |           0.2381 |
[32m[20221213 15:12:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 160.07
[32m[20221213 15:12:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 182.53
[32m[20221213 15:12:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 209.15
[32m[20221213 15:12:30 @agent_ppo2.py:143][0m Total time:      19.57 min
[32m[20221213 15:12:30 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221213 15:12:30 @agent_ppo2.py:121][0m #------------------------ Iteration 860 --------------------------#
[32m[20221213 15:12:30 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:12:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:30 @agent_ppo2.py:185][0m |           0.0003 |           9.2800 |           0.2315 |
[32m[20221213 15:12:30 @agent_ppo2.py:185][0m |          -0.0067 |           8.8327 |           0.2317 |
[32m[20221213 15:12:30 @agent_ppo2.py:185][0m |          -0.0134 |           8.6667 |           0.2318 |
[32m[20221213 15:12:30 @agent_ppo2.py:185][0m |          -0.0104 |           8.5192 |           0.2318 |
[32m[20221213 15:12:30 @agent_ppo2.py:185][0m |          -0.0129 |           8.4151 |           0.2317 |
[32m[20221213 15:12:30 @agent_ppo2.py:185][0m |          -0.0147 |           8.3567 |           0.2316 |
[32m[20221213 15:12:31 @agent_ppo2.py:185][0m |          -0.0187 |           8.2958 |           0.2316 |
[32m[20221213 15:12:31 @agent_ppo2.py:185][0m |          -0.0178 |           8.2258 |           0.2316 |
[32m[20221213 15:12:31 @agent_ppo2.py:185][0m |          -0.0197 |           8.1817 |           0.2318 |
[32m[20221213 15:12:31 @agent_ppo2.py:185][0m |          -0.0178 |           8.1269 |           0.2318 |
[32m[20221213 15:12:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.44
[32m[20221213 15:12:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.46
[32m[20221213 15:12:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 199.97
[32m[20221213 15:12:31 @agent_ppo2.py:143][0m Total time:      19.60 min
[32m[20221213 15:12:31 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221213 15:12:31 @agent_ppo2.py:121][0m #------------------------ Iteration 861 --------------------------#
[32m[20221213 15:12:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:31 @agent_ppo2.py:185][0m |          -0.0007 |           9.5091 |           0.2380 |
[32m[20221213 15:12:31 @agent_ppo2.py:185][0m |          -0.0039 |           9.3683 |           0.2381 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0140 |           9.1492 |           0.2380 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0124 |           9.0583 |           0.2378 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0158 |           8.9653 |           0.2379 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0100 |           9.4717 |           0.2380 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0128 |           8.9059 |           0.2378 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0153 |           8.8360 |           0.2378 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0197 |           8.7360 |           0.2380 |
[32m[20221213 15:12:32 @agent_ppo2.py:185][0m |          -0.0093 |           8.9764 |           0.2381 |
[32m[20221213 15:12:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.20
[32m[20221213 15:12:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 188.76
[32m[20221213 15:12:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 208.00
[32m[20221213 15:12:32 @agent_ppo2.py:143][0m Total time:      19.62 min
[32m[20221213 15:12:32 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221213 15:12:32 @agent_ppo2.py:121][0m #------------------------ Iteration 862 --------------------------#
[32m[20221213 15:12:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |           0.0095 |           9.9888 |           0.2385 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |           0.0021 |           9.8504 |           0.2377 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0128 |           8.7259 |           0.2378 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0146 |           8.5242 |           0.2375 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0143 |           8.4165 |           0.2373 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0179 |           8.3044 |           0.2373 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0138 |           8.1990 |           0.2372 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0151 |           8.1636 |           0.2370 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0172 |           8.0138 |           0.2370 |
[32m[20221213 15:12:33 @agent_ppo2.py:185][0m |          -0.0179 |           7.8918 |           0.2371 |
[32m[20221213 15:12:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:12:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.99
[32m[20221213 15:12:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 193.52
[32m[20221213 15:12:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.31
[32m[20221213 15:12:34 @agent_ppo2.py:143][0m Total time:      19.64 min
[32m[20221213 15:12:34 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221213 15:12:34 @agent_ppo2.py:121][0m #------------------------ Iteration 863 --------------------------#
[32m[20221213 15:12:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:34 @agent_ppo2.py:185][0m |          -0.0005 |           9.3682 |           0.2385 |
[32m[20221213 15:12:34 @agent_ppo2.py:185][0m |          -0.0056 |           9.0444 |           0.2382 |
[32m[20221213 15:12:34 @agent_ppo2.py:185][0m |           0.0057 |           9.8447 |           0.2378 |
[32m[20221213 15:12:34 @agent_ppo2.py:185][0m |          -0.0097 |           8.7986 |           0.2373 |
[32m[20221213 15:12:34 @agent_ppo2.py:185][0m |          -0.0164 |           8.7305 |           0.2376 |
[32m[20221213 15:12:34 @agent_ppo2.py:185][0m |          -0.0144 |           8.6323 |           0.2378 |
[32m[20221213 15:12:35 @agent_ppo2.py:185][0m |          -0.0134 |           8.6013 |           0.2376 |
[32m[20221213 15:12:35 @agent_ppo2.py:185][0m |          -0.0182 |           8.5045 |           0.2375 |
[32m[20221213 15:12:35 @agent_ppo2.py:185][0m |          -0.0163 |           8.5629 |           0.2377 |
[32m[20221213 15:12:35 @agent_ppo2.py:185][0m |          -0.0193 |           8.4556 |           0.2375 |
[32m[20221213 15:12:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 191.79
[32m[20221213 15:12:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 196.68
[32m[20221213 15:12:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.47
[32m[20221213 15:12:35 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221213 15:12:35 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221213 15:12:35 @agent_ppo2.py:121][0m #------------------------ Iteration 864 --------------------------#
[32m[20221213 15:12:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:35 @agent_ppo2.py:185][0m |          -0.0014 |           9.8053 |           0.2362 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0095 |           9.3684 |           0.2358 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0096 |           9.1099 |           0.2358 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0108 |           8.9865 |           0.2356 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0123 |           8.9094 |           0.2357 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0086 |           8.8549 |           0.2356 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0132 |           8.7320 |           0.2358 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0152 |           8.6592 |           0.2357 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0148 |           8.6290 |           0.2356 |
[32m[20221213 15:12:36 @agent_ppo2.py:185][0m |          -0.0172 |           8.5813 |           0.2356 |
[32m[20221213 15:12:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.92
[32m[20221213 15:12:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.49
[32m[20221213 15:12:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 208.19
[32m[20221213 15:12:36 @agent_ppo2.py:143][0m Total time:      19.69 min
[32m[20221213 15:12:36 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221213 15:12:36 @agent_ppo2.py:121][0m #------------------------ Iteration 865 --------------------------#
[32m[20221213 15:12:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0005 |          10.1707 |           0.2386 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0085 |           9.9458 |           0.2380 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0051 |          10.2175 |           0.2379 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0161 |           9.8071 |           0.2374 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0135 |           9.7195 |           0.2372 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0182 |           9.6814 |           0.2375 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0196 |           9.6628 |           0.2371 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0056 |          10.5181 |           0.2374 |
[32m[20221213 15:12:37 @agent_ppo2.py:185][0m |          -0.0212 |           9.6001 |           0.2372 |
[32m[20221213 15:12:38 @agent_ppo2.py:185][0m |          -0.0204 |           9.5488 |           0.2374 |
[32m[20221213 15:12:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.48
[32m[20221213 15:12:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.75
[32m[20221213 15:12:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 197.49
[32m[20221213 15:12:38 @agent_ppo2.py:143][0m Total time:      19.71 min
[32m[20221213 15:12:38 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221213 15:12:38 @agent_ppo2.py:121][0m #------------------------ Iteration 866 --------------------------#
[32m[20221213 15:12:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:38 @agent_ppo2.py:185][0m |           0.0014 |           9.7477 |           0.2343 |
[32m[20221213 15:12:38 @agent_ppo2.py:185][0m |          -0.0072 |           9.4466 |           0.2339 |
[32m[20221213 15:12:38 @agent_ppo2.py:185][0m |          -0.0108 |           9.3363 |           0.2336 |
[32m[20221213 15:12:38 @agent_ppo2.py:185][0m |          -0.0139 |           9.2494 |           0.2338 |
[32m[20221213 15:12:38 @agent_ppo2.py:185][0m |          -0.0177 |           9.1475 |           0.2334 |
[32m[20221213 15:12:39 @agent_ppo2.py:185][0m |          -0.0182 |           9.0706 |           0.2333 |
[32m[20221213 15:12:39 @agent_ppo2.py:185][0m |          -0.0169 |           8.9650 |           0.2331 |
[32m[20221213 15:12:39 @agent_ppo2.py:185][0m |          -0.0149 |           8.9483 |           0.2331 |
[32m[20221213 15:12:39 @agent_ppo2.py:185][0m |          -0.0209 |           8.8477 |           0.2331 |
[32m[20221213 15:12:39 @agent_ppo2.py:185][0m |          -0.0165 |           8.9960 |           0.2330 |
[32m[20221213 15:12:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.70
[32m[20221213 15:12:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 204.03
[32m[20221213 15:12:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 221.74
[32m[20221213 15:12:39 @agent_ppo2.py:143][0m Total time:      19.73 min
[32m[20221213 15:12:39 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221213 15:12:39 @agent_ppo2.py:121][0m #------------------------ Iteration 867 --------------------------#
[32m[20221213 15:12:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:39 @agent_ppo2.py:185][0m |          -0.0001 |          10.0698 |           0.2409 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0053 |           9.8247 |           0.2402 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0139 |           9.6805 |           0.2402 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0112 |           9.6078 |           0.2400 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0141 |           9.5256 |           0.2403 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0168 |           9.4794 |           0.2400 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0176 |           9.4297 |           0.2399 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0209 |           9.3637 |           0.2403 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0127 |           9.5564 |           0.2400 |
[32m[20221213 15:12:40 @agent_ppo2.py:185][0m |          -0.0202 |           9.2815 |           0.2401 |
[32m[20221213 15:12:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:12:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.94
[32m[20221213 15:12:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 195.97
[32m[20221213 15:12:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.39
[32m[20221213 15:12:40 @agent_ppo2.py:143][0m Total time:      19.75 min
[32m[20221213 15:12:40 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221213 15:12:40 @agent_ppo2.py:121][0m #------------------------ Iteration 868 --------------------------#
[32m[20221213 15:12:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0032 |           9.7384 |           0.2349 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0087 |           9.5544 |           0.2348 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0112 |           9.4511 |           0.2347 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |           0.0001 |          10.0834 |           0.2347 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0128 |           9.3381 |           0.2344 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0147 |           9.2790 |           0.2347 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0053 |           9.9121 |           0.2345 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0104 |           9.2325 |           0.2346 |
[32m[20221213 15:12:41 @agent_ppo2.py:185][0m |          -0.0130 |           9.2245 |           0.2348 |
[32m[20221213 15:12:42 @agent_ppo2.py:185][0m |          -0.0162 |           9.1256 |           0.2347 |
[32m[20221213 15:12:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.40
[32m[20221213 15:12:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 202.85
[32m[20221213 15:12:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.62
[32m[20221213 15:12:42 @agent_ppo2.py:143][0m Total time:      19.78 min
[32m[20221213 15:12:42 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221213 15:12:42 @agent_ppo2.py:121][0m #------------------------ Iteration 869 --------------------------#
[32m[20221213 15:12:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:42 @agent_ppo2.py:185][0m |          -0.0005 |          10.1414 |           0.2390 |
[32m[20221213 15:12:42 @agent_ppo2.py:185][0m |          -0.0081 |           9.7930 |           0.2382 |
[32m[20221213 15:12:42 @agent_ppo2.py:185][0m |          -0.0124 |           9.6574 |           0.2380 |
[32m[20221213 15:12:42 @agent_ppo2.py:185][0m |          -0.0152 |           9.5802 |           0.2379 |
[32m[20221213 15:12:42 @agent_ppo2.py:185][0m |          -0.0152 |           9.4965 |           0.2379 |
[32m[20221213 15:12:43 @agent_ppo2.py:185][0m |          -0.0152 |           9.4843 |           0.2381 |
[32m[20221213 15:12:43 @agent_ppo2.py:185][0m |          -0.0091 |           9.7214 |           0.2380 |
[32m[20221213 15:12:43 @agent_ppo2.py:185][0m |          -0.0175 |           9.3639 |           0.2379 |
[32m[20221213 15:12:43 @agent_ppo2.py:185][0m |          -0.0202 |           9.3006 |           0.2379 |
[32m[20221213 15:12:43 @agent_ppo2.py:185][0m |          -0.0181 |           9.2941 |           0.2381 |
[32m[20221213 15:12:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.38
[32m[20221213 15:12:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 191.33
[32m[20221213 15:12:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.44
[32m[20221213 15:12:43 @agent_ppo2.py:143][0m Total time:      19.80 min
[32m[20221213 15:12:43 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221213 15:12:43 @agent_ppo2.py:121][0m #------------------------ Iteration 870 --------------------------#
[32m[20221213 15:12:43 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:12:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:43 @agent_ppo2.py:185][0m |          -0.0023 |          10.0600 |           0.2405 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0071 |           9.8197 |           0.2401 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0081 |           9.7369 |           0.2397 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0120 |           9.6870 |           0.2398 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0077 |           9.7093 |           0.2398 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0115 |           9.9286 |           0.2397 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0171 |           9.5374 |           0.2397 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0155 |           9.5185 |           0.2398 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0158 |           9.5162 |           0.2400 |
[32m[20221213 15:12:44 @agent_ppo2.py:185][0m |          -0.0177 |           9.4889 |           0.2398 |
[32m[20221213 15:12:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:12:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 165.17
[32m[20221213 15:12:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 180.69
[32m[20221213 15:12:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.87
[32m[20221213 15:12:44 @agent_ppo2.py:143][0m Total time:      19.82 min
[32m[20221213 15:12:44 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221213 15:12:44 @agent_ppo2.py:121][0m #------------------------ Iteration 871 --------------------------#
[32m[20221213 15:12:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0016 |           9.9650 |           0.2419 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0112 |           9.7064 |           0.2414 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0100 |           9.5663 |           0.2410 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0147 |           9.4498 |           0.2409 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0161 |           9.4145 |           0.2409 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0180 |           9.3205 |           0.2407 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0152 |           9.2643 |           0.2407 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0165 |           9.2403 |           0.2405 |
[32m[20221213 15:12:45 @agent_ppo2.py:185][0m |          -0.0184 |           9.1499 |           0.2403 |
[32m[20221213 15:12:46 @agent_ppo2.py:185][0m |          -0.0141 |           9.3341 |           0.2404 |
[32m[20221213 15:12:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 177.03
[32m[20221213 15:12:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 198.55
[32m[20221213 15:12:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 219.83
[32m[20221213 15:12:46 @agent_ppo2.py:143][0m Total time:      19.84 min
[32m[20221213 15:12:46 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221213 15:12:46 @agent_ppo2.py:121][0m #------------------------ Iteration 872 --------------------------#
[32m[20221213 15:12:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:46 @agent_ppo2.py:185][0m |           0.0107 |          10.5789 |           0.2425 |
[32m[20221213 15:12:46 @agent_ppo2.py:185][0m |          -0.0061 |           9.7508 |           0.2422 |
[32m[20221213 15:12:46 @agent_ppo2.py:185][0m |          -0.0083 |           9.6147 |           0.2424 |
[32m[20221213 15:12:46 @agent_ppo2.py:185][0m |          -0.0066 |           9.5417 |           0.2426 |
[32m[20221213 15:12:46 @agent_ppo2.py:185][0m |          -0.0088 |           9.3576 |           0.2428 |
[32m[20221213 15:12:47 @agent_ppo2.py:185][0m |          -0.0115 |           9.2759 |           0.2430 |
[32m[20221213 15:12:47 @agent_ppo2.py:185][0m |          -0.0108 |           9.1641 |           0.2431 |
[32m[20221213 15:12:47 @agent_ppo2.py:185][0m |          -0.0133 |           9.1362 |           0.2432 |
[32m[20221213 15:12:47 @agent_ppo2.py:185][0m |          -0.0134 |           9.0482 |           0.2430 |
[32m[20221213 15:12:47 @agent_ppo2.py:185][0m |          -0.0069 |           9.4993 |           0.2436 |
[32m[20221213 15:12:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:12:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.96
[32m[20221213 15:12:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 199.25
[32m[20221213 15:12:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 199.34
[32m[20221213 15:12:47 @agent_ppo2.py:143][0m Total time:      19.87 min
[32m[20221213 15:12:47 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221213 15:12:47 @agent_ppo2.py:121][0m #------------------------ Iteration 873 --------------------------#
[32m[20221213 15:12:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:47 @agent_ppo2.py:185][0m |          -0.0010 |          10.3313 |           0.2462 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |           0.0046 |          10.4440 |           0.2459 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0111 |           9.8884 |           0.2455 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0124 |           9.7352 |           0.2455 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0155 |           9.6567 |           0.2451 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0135 |           9.5982 |           0.2451 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0183 |           9.5233 |           0.2451 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0184 |           9.4752 |           0.2448 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0178 |           9.4020 |           0.2447 |
[32m[20221213 15:12:48 @agent_ppo2.py:185][0m |          -0.0170 |           9.3375 |           0.2444 |
[32m[20221213 15:12:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:12:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 195.29
[32m[20221213 15:12:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 205.90
[32m[20221213 15:12:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 184.93
[32m[20221213 15:12:48 @agent_ppo2.py:143][0m Total time:      19.89 min
[32m[20221213 15:12:48 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221213 15:12:48 @agent_ppo2.py:121][0m #------------------------ Iteration 874 --------------------------#
[32m[20221213 15:12:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0024 |          10.0798 |           0.2476 |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0070 |           9.4815 |           0.2468 |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0119 |           9.2618 |           0.2469 |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0145 |           9.0733 |           0.2469 |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0152 |           8.9175 |           0.2466 |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0096 |           9.0643 |           0.2466 |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0103 |           8.8886 |           0.2462 |
[32m[20221213 15:12:49 @agent_ppo2.py:185][0m |          -0.0171 |           8.5067 |           0.2461 |
[32m[20221213 15:12:50 @agent_ppo2.py:185][0m |          -0.0154 |           8.3750 |           0.2460 |
[32m[20221213 15:12:50 @agent_ppo2.py:185][0m |          -0.0164 |           8.3017 |           0.2459 |
[32m[20221213 15:12:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.01
[32m[20221213 15:12:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 202.04
[32m[20221213 15:12:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 213.13
[32m[20221213 15:12:50 @agent_ppo2.py:143][0m Total time:      19.91 min
[32m[20221213 15:12:50 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221213 15:12:50 @agent_ppo2.py:121][0m #------------------------ Iteration 875 --------------------------#
[32m[20221213 15:12:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:50 @agent_ppo2.py:185][0m |           0.0019 |          10.9432 |           0.2426 |
[32m[20221213 15:12:50 @agent_ppo2.py:185][0m |          -0.0014 |          10.6804 |           0.2422 |
[32m[20221213 15:12:50 @agent_ppo2.py:185][0m |          -0.0101 |          10.3679 |           0.2421 |
[32m[20221213 15:12:50 @agent_ppo2.py:185][0m |          -0.0109 |          10.3007 |           0.2423 |
[32m[20221213 15:12:51 @agent_ppo2.py:185][0m |          -0.0036 |          11.0884 |           0.2420 |
[32m[20221213 15:12:51 @agent_ppo2.py:185][0m |          -0.0131 |          10.1997 |           0.2418 |
[32m[20221213 15:12:51 @agent_ppo2.py:185][0m |          -0.0176 |          10.1275 |           0.2420 |
[32m[20221213 15:12:51 @agent_ppo2.py:185][0m |          -0.0169 |          10.0935 |           0.2419 |
[32m[20221213 15:12:51 @agent_ppo2.py:185][0m |          -0.0185 |          10.0616 |           0.2416 |
[32m[20221213 15:12:51 @agent_ppo2.py:185][0m |          -0.0183 |          10.0250 |           0.2416 |
[32m[20221213 15:12:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.85
[32m[20221213 15:12:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.19
[32m[20221213 15:12:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 212.86
[32m[20221213 15:12:51 @agent_ppo2.py:143][0m Total time:      19.93 min
[32m[20221213 15:12:51 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221213 15:12:51 @agent_ppo2.py:121][0m #------------------------ Iteration 876 --------------------------#
[32m[20221213 15:12:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:51 @agent_ppo2.py:185][0m |          -0.0041 |           9.8245 |           0.2413 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0101 |           9.2873 |           0.2408 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0005 |           9.8341 |           0.2405 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0134 |           8.9628 |           0.2403 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0177 |           8.8324 |           0.2406 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0133 |           8.7915 |           0.2405 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0168 |           8.6266 |           0.2404 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0123 |           9.1428 |           0.2405 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0206 |           8.5023 |           0.2401 |
[32m[20221213 15:12:52 @agent_ppo2.py:185][0m |          -0.0186 |           8.4330 |           0.2404 |
[32m[20221213 15:12:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:12:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 185.04
[32m[20221213 15:12:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 203.74
[32m[20221213 15:12:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 213.16
[32m[20221213 15:12:52 @agent_ppo2.py:143][0m Total time:      19.95 min
[32m[20221213 15:12:52 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221213 15:12:52 @agent_ppo2.py:121][0m #------------------------ Iteration 877 --------------------------#
[32m[20221213 15:12:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0029 |          10.8514 |           0.2452 |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0092 |          10.4412 |           0.2452 |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0122 |          10.3300 |           0.2448 |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0138 |          10.1936 |           0.2446 |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0152 |          10.1067 |           0.2445 |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0135 |          10.0580 |           0.2447 |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0169 |           9.9671 |           0.2442 |
[32m[20221213 15:12:53 @agent_ppo2.py:185][0m |          -0.0133 |           9.9698 |           0.2441 |
[32m[20221213 15:12:54 @agent_ppo2.py:185][0m |          -0.0167 |           9.8795 |           0.2441 |
[32m[20221213 15:12:54 @agent_ppo2.py:185][0m |          -0.0171 |           9.8643 |           0.2440 |
[32m[20221213 15:12:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:12:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.31
[32m[20221213 15:12:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 208.28
[32m[20221213 15:12:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.18
[32m[20221213 15:12:54 @agent_ppo2.py:143][0m Total time:      19.98 min
[32m[20221213 15:12:54 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221213 15:12:54 @agent_ppo2.py:121][0m #------------------------ Iteration 878 --------------------------#
[32m[20221213 15:12:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:54 @agent_ppo2.py:185][0m |          -0.0023 |          10.1719 |           0.2438 |
[32m[20221213 15:12:54 @agent_ppo2.py:185][0m |          -0.0083 |           9.8034 |           0.2433 |
[32m[20221213 15:12:54 @agent_ppo2.py:185][0m |          -0.0089 |           9.5343 |           0.2429 |
[32m[20221213 15:12:54 @agent_ppo2.py:185][0m |          -0.0030 |           9.7587 |           0.2430 |
[32m[20221213 15:12:55 @agent_ppo2.py:185][0m |          -0.0077 |           9.7965 |           0.2432 |
[32m[20221213 15:12:55 @agent_ppo2.py:185][0m |          -0.0155 |           9.1583 |           0.2431 |
[32m[20221213 15:12:55 @agent_ppo2.py:185][0m |          -0.0137 |           8.9896 |           0.2428 |
[32m[20221213 15:12:55 @agent_ppo2.py:185][0m |          -0.0178 |           8.9082 |           0.2430 |
[32m[20221213 15:12:55 @agent_ppo2.py:185][0m |          -0.0124 |           8.8815 |           0.2428 |
[32m[20221213 15:12:55 @agent_ppo2.py:185][0m |          -0.0144 |           8.8774 |           0.2428 |
[32m[20221213 15:12:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.40
[32m[20221213 15:12:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 200.85
[32m[20221213 15:12:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.82
[32m[20221213 15:12:55 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 229.82
[32m[20221213 15:12:55 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 229.82
[32m[20221213 15:12:55 @agent_ppo2.py:143][0m Total time:      20.00 min
[32m[20221213 15:12:55 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221213 15:12:55 @agent_ppo2.py:121][0m #------------------------ Iteration 879 --------------------------#
[32m[20221213 15:12:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:12:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0029 |          10.5921 |           0.2418 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0078 |          10.3138 |           0.2410 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0105 |          10.1636 |           0.2409 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0020 |          10.6310 |           0.2408 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0137 |          10.0453 |           0.2404 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0029 |          10.6561 |           0.2404 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0166 |          10.0021 |           0.2404 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0124 |          10.3340 |           0.2402 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0147 |           9.9422 |           0.2398 |
[32m[20221213 15:12:56 @agent_ppo2.py:185][0m |          -0.0193 |           9.8458 |           0.2399 |
[32m[20221213 15:12:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 193.07
[32m[20221213 15:12:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.86
[32m[20221213 15:12:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.64
[32m[20221213 15:12:56 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 232.64
[32m[20221213 15:12:56 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 232.64
[32m[20221213 15:12:56 @agent_ppo2.py:143][0m Total time:      20.02 min
[32m[20221213 15:12:56 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221213 15:12:56 @agent_ppo2.py:121][0m #------------------------ Iteration 880 --------------------------#
[32m[20221213 15:12:57 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:12:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0040 |          10.5773 |           0.2439 |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0068 |          10.3993 |           0.2434 |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0104 |          10.1872 |           0.2430 |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0129 |          10.1074 |           0.2428 |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0145 |          10.0156 |           0.2425 |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0050 |          10.7025 |           0.2422 |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0087 |          10.0633 |           0.2421 |
[32m[20221213 15:12:57 @agent_ppo2.py:185][0m |          -0.0170 |           9.7896 |           0.2417 |
[32m[20221213 15:12:58 @agent_ppo2.py:185][0m |          -0.0182 |           9.7336 |           0.2415 |
[32m[20221213 15:12:58 @agent_ppo2.py:185][0m |          -0.0188 |           9.7018 |           0.2414 |
[32m[20221213 15:12:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:12:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.02
[32m[20221213 15:12:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 200.65
[32m[20221213 15:12:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.11
[32m[20221213 15:12:58 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 242.11
[32m[20221213 15:12:58 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 242.11
[32m[20221213 15:12:58 @agent_ppo2.py:143][0m Total time:      20.04 min
[32m[20221213 15:12:58 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221213 15:12:58 @agent_ppo2.py:121][0m #------------------------ Iteration 881 --------------------------#
[32m[20221213 15:12:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:12:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:12:58 @agent_ppo2.py:185][0m |           0.0011 |          10.6974 |           0.2301 |
[32m[20221213 15:12:58 @agent_ppo2.py:185][0m |          -0.0054 |          10.4330 |           0.2301 |
[32m[20221213 15:12:58 @agent_ppo2.py:185][0m |          -0.0063 |          10.2772 |           0.2303 |
[32m[20221213 15:12:58 @agent_ppo2.py:185][0m |          -0.0097 |          10.1810 |           0.2303 |
[32m[20221213 15:12:59 @agent_ppo2.py:185][0m |          -0.0072 |          10.1185 |           0.2304 |
[32m[20221213 15:12:59 @agent_ppo2.py:185][0m |          -0.0129 |          10.0729 |           0.2305 |
[32m[20221213 15:12:59 @agent_ppo2.py:185][0m |          -0.0171 |          10.0306 |           0.2306 |
[32m[20221213 15:12:59 @agent_ppo2.py:185][0m |          -0.0125 |           9.9943 |           0.2309 |
[32m[20221213 15:12:59 @agent_ppo2.py:185][0m |           0.0022 |          10.9809 |           0.2308 |
[32m[20221213 15:12:59 @agent_ppo2.py:185][0m |          -0.0143 |           9.9875 |           0.2309 |
[32m[20221213 15:12:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:12:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 181.38
[32m[20221213 15:12:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 190.63
[32m[20221213 15:12:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.49
[32m[20221213 15:12:59 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 244.49
[32m[20221213 15:12:59 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 244.49
[32m[20221213 15:12:59 @agent_ppo2.py:143][0m Total time:      20.07 min
[32m[20221213 15:12:59 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221213 15:12:59 @agent_ppo2.py:121][0m #------------------------ Iteration 882 --------------------------#
[32m[20221213 15:12:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:12:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |           0.0039 |          10.7565 |           0.2462 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0052 |          10.5296 |           0.2460 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0092 |          10.3829 |           0.2461 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0044 |          10.5023 |           0.2460 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0114 |          10.2864 |           0.2457 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0133 |          10.2153 |           0.2459 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0162 |          10.1714 |           0.2458 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0047 |          11.4647 |           0.2457 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0172 |          10.1167 |           0.2457 |
[32m[20221213 15:13:00 @agent_ppo2.py:185][0m |          -0.0141 |          10.0722 |           0.2457 |
[32m[20221213 15:13:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.34
[32m[20221213 15:13:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 211.77
[32m[20221213 15:13:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.26
[32m[20221213 15:13:01 @agent_ppo2.py:143][0m Total time:      20.09 min
[32m[20221213 15:13:01 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221213 15:13:01 @agent_ppo2.py:121][0m #------------------------ Iteration 883 --------------------------#
[32m[20221213 15:13:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |          -0.0012 |          10.8751 |           0.2440 |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |           0.0056 |          11.1517 |           0.2437 |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |          -0.0021 |          10.4099 |           0.2433 |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |          -0.0113 |          10.2239 |           0.2433 |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |          -0.0127 |          10.1754 |           0.2433 |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |          -0.0103 |          10.0679 |           0.2431 |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |          -0.0098 |          10.1229 |           0.2433 |
[32m[20221213 15:13:01 @agent_ppo2.py:185][0m |          -0.0028 |          10.5111 |           0.2431 |
[32m[20221213 15:13:02 @agent_ppo2.py:185][0m |          -0.0143 |           9.8817 |           0.2431 |
[32m[20221213 15:13:02 @agent_ppo2.py:185][0m |          -0.0160 |           9.8335 |           0.2430 |
[32m[20221213 15:13:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.40
[32m[20221213 15:13:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 210.53
[32m[20221213 15:13:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.59
[32m[20221213 15:13:02 @agent_ppo2.py:143][0m Total time:      20.11 min
[32m[20221213 15:13:02 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221213 15:13:02 @agent_ppo2.py:121][0m #------------------------ Iteration 884 --------------------------#
[32m[20221213 15:13:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:02 @agent_ppo2.py:185][0m |           0.0044 |          11.2798 |           0.2472 |
[32m[20221213 15:13:02 @agent_ppo2.py:185][0m |           0.0049 |          12.0313 |           0.2472 |
[32m[20221213 15:13:02 @agent_ppo2.py:185][0m |          -0.0010 |          11.6489 |           0.2470 |
[32m[20221213 15:13:03 @agent_ppo2.py:185][0m |          -0.0109 |          10.8226 |           0.2473 |
[32m[20221213 15:13:03 @agent_ppo2.py:185][0m |          -0.0113 |          10.6498 |           0.2475 |
[32m[20221213 15:13:03 @agent_ppo2.py:185][0m |          -0.0140 |          10.5682 |           0.2477 |
[32m[20221213 15:13:03 @agent_ppo2.py:185][0m |          -0.0164 |          10.5375 |           0.2479 |
[32m[20221213 15:13:03 @agent_ppo2.py:185][0m |          -0.0162 |          10.4827 |           0.2483 |
[32m[20221213 15:13:03 @agent_ppo2.py:185][0m |          -0.0195 |          10.4240 |           0.2485 |
[32m[20221213 15:13:03 @agent_ppo2.py:185][0m |          -0.0091 |          11.1866 |           0.2487 |
[32m[20221213 15:13:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 182.32
[32m[20221213 15:13:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.54
[32m[20221213 15:13:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.24
[32m[20221213 15:13:03 @agent_ppo2.py:143][0m Total time:      20.13 min
[32m[20221213 15:13:03 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221213 15:13:03 @agent_ppo2.py:121][0m #------------------------ Iteration 885 --------------------------#
[32m[20221213 15:13:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0028 |          11.3507 |           0.2461 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0063 |          10.9569 |           0.2460 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0129 |          10.7590 |           0.2462 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0126 |          10.6403 |           0.2463 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0138 |          10.5960 |           0.2464 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0137 |          10.5061 |           0.2462 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0177 |          10.4170 |           0.2464 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0141 |          10.3711 |           0.2465 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0177 |          10.3083 |           0.2463 |
[32m[20221213 15:13:04 @agent_ppo2.py:185][0m |          -0.0168 |          10.2901 |           0.2465 |
[32m[20221213 15:13:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.82
[32m[20221213 15:13:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 201.13
[32m[20221213 15:13:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.71
[32m[20221213 15:13:05 @agent_ppo2.py:143][0m Total time:      20.16 min
[32m[20221213 15:13:05 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221213 15:13:05 @agent_ppo2.py:121][0m #------------------------ Iteration 886 --------------------------#
[32m[20221213 15:13:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:05 @agent_ppo2.py:185][0m |          -0.0010 |          10.7005 |           0.2436 |
[32m[20221213 15:13:05 @agent_ppo2.py:185][0m |          -0.0057 |          10.4713 |           0.2436 |
[32m[20221213 15:13:05 @agent_ppo2.py:185][0m |          -0.0111 |          10.3560 |           0.2433 |
[32m[20221213 15:13:05 @agent_ppo2.py:185][0m |          -0.0065 |          10.4128 |           0.2437 |
[32m[20221213 15:13:05 @agent_ppo2.py:185][0m |          -0.0104 |          10.2694 |           0.2437 |
[32m[20221213 15:13:05 @agent_ppo2.py:185][0m |          -0.0130 |          10.2083 |           0.2438 |
[32m[20221213 15:13:05 @agent_ppo2.py:185][0m |          -0.0139 |          10.1962 |           0.2440 |
[32m[20221213 15:13:06 @agent_ppo2.py:185][0m |          -0.0138 |          10.1420 |           0.2441 |
[32m[20221213 15:13:06 @agent_ppo2.py:185][0m |          -0.0166 |          10.1180 |           0.2441 |
[32m[20221213 15:13:06 @agent_ppo2.py:185][0m |          -0.0174 |          10.1241 |           0.2444 |
[32m[20221213 15:13:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 177.85
[32m[20221213 15:13:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 184.90
[32m[20221213 15:13:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 221.02
[32m[20221213 15:13:06 @agent_ppo2.py:143][0m Total time:      20.18 min
[32m[20221213 15:13:06 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221213 15:13:06 @agent_ppo2.py:121][0m #------------------------ Iteration 887 --------------------------#
[32m[20221213 15:13:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:06 @agent_ppo2.py:185][0m |           0.0111 |          10.9235 |           0.2439 |
[32m[20221213 15:13:06 @agent_ppo2.py:185][0m |          -0.0044 |          10.3461 |           0.2439 |
[32m[20221213 15:13:06 @agent_ppo2.py:185][0m |          -0.0080 |          10.2458 |           0.2438 |
[32m[20221213 15:13:07 @agent_ppo2.py:185][0m |          -0.0081 |          10.2037 |           0.2435 |
[32m[20221213 15:13:07 @agent_ppo2.py:185][0m |          -0.0108 |          10.1197 |           0.2433 |
[32m[20221213 15:13:07 @agent_ppo2.py:185][0m |          -0.0115 |          10.0607 |           0.2430 |
[32m[20221213 15:13:07 @agent_ppo2.py:185][0m |          -0.0133 |          10.0179 |           0.2428 |
[32m[20221213 15:13:07 @agent_ppo2.py:185][0m |          -0.0122 |          10.0140 |           0.2427 |
[32m[20221213 15:13:07 @agent_ppo2.py:185][0m |          -0.0154 |           9.9506 |           0.2424 |
[32m[20221213 15:13:07 @agent_ppo2.py:185][0m |          -0.0159 |           9.9136 |           0.2424 |
[32m[20221213 15:13:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 177.25
[32m[20221213 15:13:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 184.89
[32m[20221213 15:13:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 129.11
[32m[20221213 15:13:07 @agent_ppo2.py:143][0m Total time:      20.20 min
[32m[20221213 15:13:07 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221213 15:13:07 @agent_ppo2.py:121][0m #------------------------ Iteration 888 --------------------------#
[32m[20221213 15:13:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |           0.0000 |          10.8181 |           0.2445 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0079 |          10.3449 |           0.2442 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0031 |          10.3566 |           0.2440 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0118 |          10.0972 |           0.2440 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0082 |          10.0909 |           0.2436 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0151 |           9.8878 |           0.2435 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0153 |           9.8385 |           0.2435 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0154 |           9.7638 |           0.2434 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0177 |           9.7258 |           0.2433 |
[32m[20221213 15:13:08 @agent_ppo2.py:185][0m |          -0.0161 |           9.6447 |           0.2431 |
[32m[20221213 15:13:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.25
[32m[20221213 15:13:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.15
[32m[20221213 15:13:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 205.53
[32m[20221213 15:13:09 @agent_ppo2.py:143][0m Total time:      20.22 min
[32m[20221213 15:13:09 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221213 15:13:09 @agent_ppo2.py:121][0m #------------------------ Iteration 889 --------------------------#
[32m[20221213 15:13:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:09 @agent_ppo2.py:185][0m |          -0.0001 |          10.6989 |           0.2483 |
[32m[20221213 15:13:09 @agent_ppo2.py:185][0m |          -0.0096 |          10.3124 |           0.2482 |
[32m[20221213 15:13:09 @agent_ppo2.py:185][0m |          -0.0114 |          10.1297 |           0.2481 |
[32m[20221213 15:13:09 @agent_ppo2.py:185][0m |          -0.0113 |           9.9643 |           0.2479 |
[32m[20221213 15:13:09 @agent_ppo2.py:185][0m |          -0.0137 |           9.8749 |           0.2478 |
[32m[20221213 15:13:09 @agent_ppo2.py:185][0m |          -0.0146 |           9.7951 |           0.2478 |
[32m[20221213 15:13:09 @agent_ppo2.py:185][0m |          -0.0151 |           9.7371 |           0.2477 |
[32m[20221213 15:13:10 @agent_ppo2.py:185][0m |          -0.0102 |           9.8225 |           0.2475 |
[32m[20221213 15:13:10 @agent_ppo2.py:185][0m |          -0.0145 |           9.6332 |           0.2474 |
[32m[20221213 15:13:10 @agent_ppo2.py:185][0m |          -0.0179 |           9.5274 |           0.2472 |
[32m[20221213 15:13:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 181.72
[32m[20221213 15:13:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 192.10
[32m[20221213 15:13:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.59
[32m[20221213 15:13:10 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 251.59
[32m[20221213 15:13:10 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 251.59
[32m[20221213 15:13:10 @agent_ppo2.py:143][0m Total time:      20.25 min
[32m[20221213 15:13:10 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221213 15:13:10 @agent_ppo2.py:121][0m #------------------------ Iteration 890 --------------------------#
[32m[20221213 15:13:10 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:13:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:10 @agent_ppo2.py:185][0m |          -0.0019 |          11.0567 |           0.2369 |
[32m[20221213 15:13:10 @agent_ppo2.py:185][0m |           0.0046 |          11.8178 |           0.2363 |
[32m[20221213 15:13:10 @agent_ppo2.py:185][0m |          -0.0089 |          10.5735 |           0.2360 |
[32m[20221213 15:13:11 @agent_ppo2.py:185][0m |          -0.0100 |          10.4244 |           0.2357 |
[32m[20221213 15:13:11 @agent_ppo2.py:185][0m |          -0.0098 |          10.3127 |           0.2359 |
[32m[20221213 15:13:11 @agent_ppo2.py:185][0m |          -0.0075 |          10.2722 |           0.2357 |
[32m[20221213 15:13:11 @agent_ppo2.py:185][0m |          -0.0120 |          10.1565 |           0.2358 |
[32m[20221213 15:13:11 @agent_ppo2.py:185][0m |          -0.0109 |          10.1516 |           0.2357 |
[32m[20221213 15:13:11 @agent_ppo2.py:185][0m |          -0.0177 |           9.9489 |           0.2356 |
[32m[20221213 15:13:11 @agent_ppo2.py:185][0m |          -0.0143 |           9.8423 |           0.2356 |
[32m[20221213 15:13:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.77
[32m[20221213 15:13:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 219.41
[32m[20221213 15:13:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 203.21
[32m[20221213 15:13:11 @agent_ppo2.py:143][0m Total time:      20.27 min
[32m[20221213 15:13:11 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221213 15:13:11 @agent_ppo2.py:121][0m #------------------------ Iteration 891 --------------------------#
[32m[20221213 15:13:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0028 |          11.2756 |           0.2414 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0043 |          10.9319 |           0.2406 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0101 |          10.7412 |           0.2407 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0124 |          10.6109 |           0.2406 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0163 |          10.5449 |           0.2405 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0118 |          10.4979 |           0.2404 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0108 |          10.8763 |           0.2404 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0176 |          10.3363 |           0.2403 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0181 |          10.2890 |           0.2405 |
[32m[20221213 15:13:12 @agent_ppo2.py:185][0m |          -0.0190 |          10.2421 |           0.2404 |
[32m[20221213 15:13:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.83
[32m[20221213 15:13:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 198.05
[32m[20221213 15:13:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.99
[32m[20221213 15:13:13 @agent_ppo2.py:143][0m Total time:      20.29 min
[32m[20221213 15:13:13 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221213 15:13:13 @agent_ppo2.py:121][0m #------------------------ Iteration 892 --------------------------#
[32m[20221213 15:13:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:13 @agent_ppo2.py:185][0m |          -0.0012 |          10.8805 |           0.2414 |
[32m[20221213 15:13:13 @agent_ppo2.py:185][0m |          -0.0059 |          10.6044 |           0.2410 |
[32m[20221213 15:13:13 @agent_ppo2.py:185][0m |          -0.0064 |          10.4465 |           0.2409 |
[32m[20221213 15:13:13 @agent_ppo2.py:185][0m |          -0.0127 |          10.3089 |           0.2406 |
[32m[20221213 15:13:13 @agent_ppo2.py:185][0m |          -0.0119 |          10.1885 |           0.2406 |
[32m[20221213 15:13:13 @agent_ppo2.py:185][0m |          -0.0165 |          10.0751 |           0.2404 |
[32m[20221213 15:13:13 @agent_ppo2.py:185][0m |          -0.0169 |           9.9681 |           0.2404 |
[32m[20221213 15:13:14 @agent_ppo2.py:185][0m |          -0.0171 |           9.8797 |           0.2403 |
[32m[20221213 15:13:14 @agent_ppo2.py:185][0m |          -0.0182 |           9.8096 |           0.2404 |
[32m[20221213 15:13:14 @agent_ppo2.py:185][0m |          -0.0172 |           9.7431 |           0.2404 |
[32m[20221213 15:13:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 181.53
[32m[20221213 15:13:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 200.42
[32m[20221213 15:13:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.26
[32m[20221213 15:13:14 @agent_ppo2.py:143][0m Total time:      20.31 min
[32m[20221213 15:13:14 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221213 15:13:14 @agent_ppo2.py:121][0m #------------------------ Iteration 893 --------------------------#
[32m[20221213 15:13:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:14 @agent_ppo2.py:185][0m |          -0.0019 |          10.8152 |           0.2464 |
[32m[20221213 15:13:14 @agent_ppo2.py:185][0m |          -0.0104 |          10.2809 |           0.2460 |
[32m[20221213 15:13:14 @agent_ppo2.py:185][0m |          -0.0114 |          10.0937 |           0.2452 |
[32m[20221213 15:13:15 @agent_ppo2.py:185][0m |          -0.0121 |           9.9477 |           0.2451 |
[32m[20221213 15:13:15 @agent_ppo2.py:185][0m |          -0.0143 |           9.8623 |           0.2449 |
[32m[20221213 15:13:15 @agent_ppo2.py:185][0m |          -0.0147 |           9.7881 |           0.2447 |
[32m[20221213 15:13:15 @agent_ppo2.py:185][0m |          -0.0154 |           9.7532 |           0.2445 |
[32m[20221213 15:13:15 @agent_ppo2.py:185][0m |          -0.0101 |           9.8838 |           0.2445 |
[32m[20221213 15:13:15 @agent_ppo2.py:185][0m |          -0.0179 |           9.6342 |           0.2443 |
[32m[20221213 15:13:15 @agent_ppo2.py:185][0m |          -0.0175 |           9.5880 |           0.2441 |
[32m[20221213 15:13:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:13:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.79
[32m[20221213 15:13:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 204.13
[32m[20221213 15:13:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.72
[32m[20221213 15:13:15 @agent_ppo2.py:143][0m Total time:      20.34 min
[32m[20221213 15:13:15 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221213 15:13:15 @agent_ppo2.py:121][0m #------------------------ Iteration 894 --------------------------#
[32m[20221213 15:13:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0015 |          11.4465 |           0.2443 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0051 |          11.1588 |           0.2444 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0120 |          10.9899 |           0.2441 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0099 |          10.8600 |           0.2444 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0142 |          10.7541 |           0.2442 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0148 |          10.6804 |           0.2441 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0144 |          10.6103 |           0.2439 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0096 |          10.7734 |           0.2440 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0148 |          10.5436 |           0.2439 |
[32m[20221213 15:13:16 @agent_ppo2.py:185][0m |          -0.0171 |          10.4089 |           0.2439 |
[32m[20221213 15:13:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:13:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 190.38
[32m[20221213 15:13:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.72
[32m[20221213 15:13:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.24
[32m[20221213 15:13:17 @agent_ppo2.py:143][0m Total time:      20.36 min
[32m[20221213 15:13:17 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221213 15:13:17 @agent_ppo2.py:121][0m #------------------------ Iteration 895 --------------------------#
[32m[20221213 15:13:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:17 @agent_ppo2.py:185][0m |          -0.0016 |          10.8714 |           0.2330 |
[32m[20221213 15:13:17 @agent_ppo2.py:185][0m |          -0.0075 |          10.4887 |           0.2327 |
[32m[20221213 15:13:17 @agent_ppo2.py:185][0m |          -0.0069 |          10.3295 |           0.2326 |
[32m[20221213 15:13:17 @agent_ppo2.py:185][0m |          -0.0097 |          10.0680 |           0.2322 |
[32m[20221213 15:13:17 @agent_ppo2.py:185][0m |          -0.0133 |           9.9417 |           0.2320 |
[32m[20221213 15:13:17 @agent_ppo2.py:185][0m |          -0.0136 |           9.7749 |           0.2320 |
[32m[20221213 15:13:18 @agent_ppo2.py:185][0m |          -0.0150 |           9.7115 |           0.2321 |
[32m[20221213 15:13:18 @agent_ppo2.py:185][0m |          -0.0164 |           9.5770 |           0.2318 |
[32m[20221213 15:13:18 @agent_ppo2.py:185][0m |          -0.0067 |          10.6325 |           0.2318 |
[32m[20221213 15:13:18 @agent_ppo2.py:185][0m |          -0.0179 |           9.4178 |           0.2312 |
[32m[20221213 15:13:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.69
[32m[20221213 15:13:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 211.98
[32m[20221213 15:13:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.16
[32m[20221213 15:13:18 @agent_ppo2.py:143][0m Total time:      20.38 min
[32m[20221213 15:13:18 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221213 15:13:18 @agent_ppo2.py:121][0m #------------------------ Iteration 896 --------------------------#
[32m[20221213 15:13:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:13:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:18 @agent_ppo2.py:185][0m |           0.0017 |          11.4122 |           0.2401 |
[32m[20221213 15:13:18 @agent_ppo2.py:185][0m |           0.0064 |          12.4854 |           0.2395 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0107 |          10.8247 |           0.2395 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0070 |          10.8158 |           0.2391 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0013 |          11.8872 |           0.2391 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0149 |          10.5758 |           0.2390 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0181 |          10.4383 |           0.2388 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0163 |          10.4047 |           0.2384 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0180 |          10.3212 |           0.2387 |
[32m[20221213 15:13:19 @agent_ppo2.py:185][0m |          -0.0221 |          10.2676 |           0.2388 |
[32m[20221213 15:13:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.79
[32m[20221213 15:13:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 203.16
[32m[20221213 15:13:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 221.22
[32m[20221213 15:13:19 @agent_ppo2.py:143][0m Total time:      20.40 min
[32m[20221213 15:13:19 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221213 15:13:19 @agent_ppo2.py:121][0m #------------------------ Iteration 897 --------------------------#
[32m[20221213 15:13:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |           0.0011 |          11.4863 |           0.2372 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0056 |          11.2061 |           0.2369 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0072 |          11.0766 |           0.2369 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0016 |          12.3751 |           0.2369 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0108 |          10.9142 |           0.2366 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0073 |          11.0373 |           0.2368 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0139 |          10.7709 |           0.2366 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0164 |          10.7354 |           0.2367 |
[32m[20221213 15:13:20 @agent_ppo2.py:185][0m |          -0.0122 |          10.7611 |           0.2368 |
[32m[20221213 15:13:21 @agent_ppo2.py:185][0m |          -0.0153 |          10.6927 |           0.2367 |
[32m[20221213 15:13:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.94
[32m[20221213 15:13:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 200.61
[32m[20221213 15:13:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.80
[32m[20221213 15:13:21 @agent_ppo2.py:143][0m Total time:      20.43 min
[32m[20221213 15:13:21 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221213 15:13:21 @agent_ppo2.py:121][0m #------------------------ Iteration 898 --------------------------#
[32m[20221213 15:13:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:21 @agent_ppo2.py:185][0m |           0.0005 |          11.0025 |           0.2427 |
[32m[20221213 15:13:21 @agent_ppo2.py:185][0m |          -0.0051 |          10.7346 |           0.2422 |
[32m[20221213 15:13:21 @agent_ppo2.py:185][0m |          -0.0065 |          10.6198 |           0.2422 |
[32m[20221213 15:13:21 @agent_ppo2.py:185][0m |          -0.0102 |          10.5503 |           0.2422 |
[32m[20221213 15:13:21 @agent_ppo2.py:185][0m |          -0.0107 |          10.4947 |           0.2422 |
[32m[20221213 15:13:21 @agent_ppo2.py:185][0m |          -0.0118 |          10.4616 |           0.2420 |
[32m[20221213 15:13:22 @agent_ppo2.py:185][0m |          -0.0136 |          10.4100 |           0.2420 |
[32m[20221213 15:13:22 @agent_ppo2.py:185][0m |          -0.0079 |          10.5925 |           0.2419 |
[32m[20221213 15:13:22 @agent_ppo2.py:185][0m |          -0.0116 |          10.3612 |           0.2417 |
[32m[20221213 15:13:22 @agent_ppo2.py:185][0m |          -0.0154 |          10.3213 |           0.2418 |
[32m[20221213 15:13:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.72
[32m[20221213 15:13:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 205.81
[32m[20221213 15:13:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 220.53
[32m[20221213 15:13:22 @agent_ppo2.py:143][0m Total time:      20.45 min
[32m[20221213 15:13:22 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221213 15:13:22 @agent_ppo2.py:121][0m #------------------------ Iteration 899 --------------------------#
[32m[20221213 15:13:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:22 @agent_ppo2.py:185][0m |           0.0111 |          12.0739 |           0.2432 |
[32m[20221213 15:13:22 @agent_ppo2.py:185][0m |          -0.0010 |          10.6071 |           0.2423 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0101 |          10.2814 |           0.2423 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0058 |          10.3938 |           0.2424 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0123 |          10.0736 |           0.2421 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0124 |          10.0188 |           0.2421 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0134 |           9.9685 |           0.2422 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0159 |           9.9026 |           0.2419 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0170 |           9.8742 |           0.2418 |
[32m[20221213 15:13:23 @agent_ppo2.py:185][0m |          -0.0187 |           9.8151 |           0.2415 |
[32m[20221213 15:13:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 178.13
[32m[20221213 15:13:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 184.37
[32m[20221213 15:13:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.87
[32m[20221213 15:13:23 @agent_ppo2.py:143][0m Total time:      20.47 min
[32m[20221213 15:13:23 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221213 15:13:23 @agent_ppo2.py:121][0m #------------------------ Iteration 900 --------------------------#
[32m[20221213 15:13:24 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:13:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0014 |          10.8364 |           0.2392 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0069 |          10.4720 |           0.2386 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0105 |          10.2238 |           0.2384 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0139 |          10.0524 |           0.2385 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0121 |           9.9351 |           0.2384 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0146 |           9.8752 |           0.2384 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0178 |           9.6927 |           0.2383 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0193 |           9.6335 |           0.2384 |
[32m[20221213 15:13:24 @agent_ppo2.py:185][0m |          -0.0205 |           9.5227 |           0.2382 |
[32m[20221213 15:13:25 @agent_ppo2.py:185][0m |          -0.0173 |           9.4710 |           0.2382 |
[32m[20221213 15:13:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.94
[32m[20221213 15:13:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 209.67
[32m[20221213 15:13:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.36
[32m[20221213 15:13:25 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 254.36
[32m[20221213 15:13:25 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 254.36
[32m[20221213 15:13:25 @agent_ppo2.py:143][0m Total time:      20.49 min
[32m[20221213 15:13:25 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221213 15:13:25 @agent_ppo2.py:121][0m #------------------------ Iteration 901 --------------------------#
[32m[20221213 15:13:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:25 @agent_ppo2.py:185][0m |          -0.0039 |          11.3915 |           0.2328 |
[32m[20221213 15:13:25 @agent_ppo2.py:185][0m |          -0.0084 |          11.1124 |           0.2325 |
[32m[20221213 15:13:25 @agent_ppo2.py:185][0m |          -0.0110 |          10.9699 |           0.2324 |
[32m[20221213 15:13:25 @agent_ppo2.py:185][0m |          -0.0163 |          10.8538 |           0.2325 |
[32m[20221213 15:13:25 @agent_ppo2.py:185][0m |          -0.0111 |          10.7942 |           0.2324 |
[32m[20221213 15:13:26 @agent_ppo2.py:185][0m |          -0.0163 |          10.6430 |           0.2325 |
[32m[20221213 15:13:26 @agent_ppo2.py:185][0m |          -0.0170 |          10.5616 |           0.2323 |
[32m[20221213 15:13:26 @agent_ppo2.py:185][0m |          -0.0208 |          10.4867 |           0.2326 |
[32m[20221213 15:13:26 @agent_ppo2.py:185][0m |          -0.0175 |          10.4667 |           0.2326 |
[32m[20221213 15:13:26 @agent_ppo2.py:185][0m |          -0.0187 |          10.3810 |           0.2326 |
[32m[20221213 15:13:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:13:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.15
[32m[20221213 15:13:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.11
[32m[20221213 15:13:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.45
[32m[20221213 15:13:26 @agent_ppo2.py:143][0m Total time:      20.51 min
[32m[20221213 15:13:26 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221213 15:13:26 @agent_ppo2.py:121][0m #------------------------ Iteration 902 --------------------------#
[32m[20221213 15:13:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:13:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:26 @agent_ppo2.py:185][0m |          -0.0013 |          11.0426 |           0.2365 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0107 |          10.8119 |           0.2367 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0105 |          10.6421 |           0.2366 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0108 |          10.5813 |           0.2363 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0053 |          11.1625 |           0.2366 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0159 |          10.4009 |           0.2362 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0106 |          10.7304 |           0.2363 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0148 |          10.2380 |           0.2362 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0189 |          10.1786 |           0.2363 |
[32m[20221213 15:13:27 @agent_ppo2.py:185][0m |          -0.0162 |          10.1048 |           0.2360 |
[32m[20221213 15:13:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:13:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 195.10
[32m[20221213 15:13:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 208.38
[32m[20221213 15:13:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.03
[32m[20221213 15:13:27 @agent_ppo2.py:143][0m Total time:      20.54 min
[32m[20221213 15:13:27 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221213 15:13:27 @agent_ppo2.py:121][0m #------------------------ Iteration 903 --------------------------#
[32m[20221213 15:13:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:13:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |          -0.0008 |          10.5624 |           0.2384 |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |           0.0001 |          10.5404 |           0.2380 |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |          -0.0097 |          10.0343 |           0.2379 |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |           0.0034 |          10.8188 |           0.2378 |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |          -0.0093 |           9.8269 |           0.2376 |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |          -0.0065 |          10.3421 |           0.2376 |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |          -0.0139 |           9.6026 |           0.2376 |
[32m[20221213 15:13:28 @agent_ppo2.py:185][0m |          -0.0168 |           9.5327 |           0.2378 |
[32m[20221213 15:13:29 @agent_ppo2.py:185][0m |          -0.0127 |           9.5642 |           0.2374 |
[32m[20221213 15:13:29 @agent_ppo2.py:185][0m |          -0.0160 |           9.4475 |           0.2378 |
[32m[20221213 15:13:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:13:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 190.66
[32m[20221213 15:13:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 204.85
[32m[20221213 15:13:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.82
[32m[20221213 15:13:29 @agent_ppo2.py:143][0m Total time:      20.56 min
[32m[20221213 15:13:29 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221213 15:13:29 @agent_ppo2.py:121][0m #------------------------ Iteration 904 --------------------------#
[32m[20221213 15:13:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:29 @agent_ppo2.py:185][0m |          -0.0018 |          10.8304 |           0.2334 |
[32m[20221213 15:13:29 @agent_ppo2.py:185][0m |          -0.0051 |          10.4400 |           0.2329 |
[32m[20221213 15:13:29 @agent_ppo2.py:185][0m |          -0.0087 |          10.3171 |           0.2328 |
[32m[20221213 15:13:30 @agent_ppo2.py:185][0m |          -0.0111 |          10.2268 |           0.2325 |
[32m[20221213 15:13:30 @agent_ppo2.py:185][0m |          -0.0077 |          10.3862 |           0.2328 |
[32m[20221213 15:13:30 @agent_ppo2.py:185][0m |          -0.0064 |          10.4820 |           0.2328 |
[32m[20221213 15:13:30 @agent_ppo2.py:185][0m |          -0.0174 |          10.0813 |           0.2327 |
[32m[20221213 15:13:30 @agent_ppo2.py:185][0m |          -0.0202 |          10.0159 |           0.2326 |
[32m[20221213 15:13:30 @agent_ppo2.py:185][0m |          -0.0146 |           9.9985 |           0.2326 |
[32m[20221213 15:13:30 @agent_ppo2.py:185][0m |          -0.0102 |          10.3184 |           0.2327 |
[32m[20221213 15:13:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:13:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.46
[32m[20221213 15:13:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 202.37
[32m[20221213 15:13:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.83
[32m[20221213 15:13:30 @agent_ppo2.py:143][0m Total time:      20.58 min
[32m[20221213 15:13:30 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221213 15:13:30 @agent_ppo2.py:121][0m #------------------------ Iteration 905 --------------------------#
[32m[20221213 15:13:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |           0.0003 |          10.6429 |           0.2359 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0093 |          10.3846 |           0.2355 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0105 |          10.3129 |           0.2351 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0113 |          10.2124 |           0.2354 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0146 |          10.1781 |           0.2350 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0143 |          10.1314 |           0.2350 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0168 |          10.1080 |           0.2351 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0165 |          10.0690 |           0.2351 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0104 |          10.1695 |           0.2350 |
[32m[20221213 15:13:31 @agent_ppo2.py:185][0m |          -0.0144 |          10.0426 |           0.2350 |
[32m[20221213 15:13:31 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:13:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.90
[32m[20221213 15:13:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.77
[32m[20221213 15:13:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.04
[32m[20221213 15:13:32 @agent_ppo2.py:143][0m Total time:      20.61 min
[32m[20221213 15:13:32 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221213 15:13:32 @agent_ppo2.py:121][0m #------------------------ Iteration 906 --------------------------#
[32m[20221213 15:13:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:32 @agent_ppo2.py:185][0m |          -0.0016 |          11.1898 |           0.2351 |
[32m[20221213 15:13:32 @agent_ppo2.py:185][0m |          -0.0028 |          11.0391 |           0.2349 |
[32m[20221213 15:13:32 @agent_ppo2.py:185][0m |          -0.0073 |          10.7771 |           0.2346 |
[32m[20221213 15:13:32 @agent_ppo2.py:185][0m |          -0.0051 |          10.7266 |           0.2346 |
[32m[20221213 15:13:32 @agent_ppo2.py:185][0m |          -0.0038 |          11.1301 |           0.2346 |
[32m[20221213 15:13:32 @agent_ppo2.py:185][0m |          -0.0131 |          10.4618 |           0.2345 |
[32m[20221213 15:13:33 @agent_ppo2.py:185][0m |          -0.0049 |          11.4542 |           0.2344 |
[32m[20221213 15:13:33 @agent_ppo2.py:185][0m |          -0.0122 |          10.3496 |           0.2341 |
[32m[20221213 15:13:33 @agent_ppo2.py:185][0m |          -0.0156 |          10.2651 |           0.2341 |
[32m[20221213 15:13:33 @agent_ppo2.py:185][0m |          -0.0165 |          10.2033 |           0.2341 |
[32m[20221213 15:13:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:13:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.01
[32m[20221213 15:13:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 198.99
[32m[20221213 15:13:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 234.67
[32m[20221213 15:13:33 @agent_ppo2.py:143][0m Total time:      20.63 min
[32m[20221213 15:13:33 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221213 15:13:33 @agent_ppo2.py:121][0m #------------------------ Iteration 907 --------------------------#
[32m[20221213 15:13:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:33 @agent_ppo2.py:185][0m |          -0.0035 |          10.6029 |           0.2363 |
[32m[20221213 15:13:33 @agent_ppo2.py:185][0m |          -0.0072 |          10.2966 |           0.2360 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0130 |          10.0397 |           0.2360 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0160 |           9.8902 |           0.2361 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0164 |           9.7652 |           0.2360 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0157 |           9.6388 |           0.2358 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0181 |           9.5664 |           0.2358 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0206 |           9.4999 |           0.2358 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0203 |           9.4305 |           0.2358 |
[32m[20221213 15:13:34 @agent_ppo2.py:185][0m |          -0.0215 |           9.3538 |           0.2356 |
[32m[20221213 15:13:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 181.89
[32m[20221213 15:13:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 188.96
[32m[20221213 15:13:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.17
[32m[20221213 15:13:34 @agent_ppo2.py:143][0m Total time:      20.65 min
[32m[20221213 15:13:34 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221213 15:13:34 @agent_ppo2.py:121][0m #------------------------ Iteration 908 --------------------------#
[32m[20221213 15:13:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0026 |          10.9082 |           0.2377 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0065 |          10.5926 |           0.2376 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0098 |          10.4108 |           0.2375 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0133 |          10.2571 |           0.2375 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0175 |          10.1531 |           0.2375 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0166 |          10.0286 |           0.2373 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0118 |          10.0453 |           0.2373 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0153 |           9.8859 |           0.2372 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |          -0.0180 |           9.7408 |           0.2371 |
[32m[20221213 15:13:35 @agent_ppo2.py:185][0m |           0.0033 |          12.7046 |           0.2370 |
[32m[20221213 15:13:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 190.89
[32m[20221213 15:13:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 206.25
[32m[20221213 15:13:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.69
[32m[20221213 15:13:36 @agent_ppo2.py:143][0m Total time:      20.67 min
[32m[20221213 15:13:36 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221213 15:13:36 @agent_ppo2.py:121][0m #------------------------ Iteration 909 --------------------------#
[32m[20221213 15:13:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:36 @agent_ppo2.py:185][0m |           0.0034 |          11.0815 |           0.2322 |
[32m[20221213 15:13:36 @agent_ppo2.py:185][0m |          -0.0111 |          10.5951 |           0.2319 |
[32m[20221213 15:13:36 @agent_ppo2.py:185][0m |          -0.0101 |          10.3923 |           0.2318 |
[32m[20221213 15:13:36 @agent_ppo2.py:185][0m |          -0.0102 |          10.3013 |           0.2316 |
[32m[20221213 15:13:36 @agent_ppo2.py:185][0m |          -0.0111 |          10.2383 |           0.2318 |
[32m[20221213 15:13:36 @agent_ppo2.py:185][0m |          -0.0137 |          10.0928 |           0.2316 |
[32m[20221213 15:13:37 @agent_ppo2.py:185][0m |          -0.0085 |          10.0783 |           0.2319 |
[32m[20221213 15:13:37 @agent_ppo2.py:185][0m |          -0.0174 |           9.9685 |           0.2319 |
[32m[20221213 15:13:37 @agent_ppo2.py:185][0m |          -0.0132 |          10.3422 |           0.2320 |
[32m[20221213 15:13:37 @agent_ppo2.py:185][0m |          -0.0174 |           9.8793 |           0.2319 |
[32m[20221213 15:13:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 174.34
[32m[20221213 15:13:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 187.59
[32m[20221213 15:13:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.80
[32m[20221213 15:13:37 @agent_ppo2.py:143][0m Total time:      20.70 min
[32m[20221213 15:13:37 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221213 15:13:37 @agent_ppo2.py:121][0m #------------------------ Iteration 910 --------------------------#
[32m[20221213 15:13:37 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:13:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:37 @agent_ppo2.py:185][0m |          -0.0012 |          10.7085 |           0.2359 |
[32m[20221213 15:13:37 @agent_ppo2.py:185][0m |          -0.0063 |          10.3490 |           0.2358 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0151 |           9.9947 |           0.2355 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0130 |           9.8277 |           0.2352 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0155 |           9.6485 |           0.2353 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0167 |           9.5403 |           0.2353 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0183 |           9.4144 |           0.2352 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0200 |           9.3280 |           0.2351 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0143 |           9.4257 |           0.2350 |
[32m[20221213 15:13:38 @agent_ppo2.py:185][0m |          -0.0157 |           9.1562 |           0.2347 |
[32m[20221213 15:13:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 181.12
[32m[20221213 15:13:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 187.38
[32m[20221213 15:13:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.61
[32m[20221213 15:13:38 @agent_ppo2.py:143][0m Total time:      20.72 min
[32m[20221213 15:13:38 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221213 15:13:38 @agent_ppo2.py:121][0m #------------------------ Iteration 911 --------------------------#
[32m[20221213 15:13:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |           0.0063 |          11.7447 |           0.2310 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0042 |          11.1081 |           0.2310 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0044 |          10.9933 |           0.2311 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0122 |          10.5976 |           0.2312 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0145 |          10.4600 |           0.2313 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0127 |          10.3898 |           0.2314 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0143 |          10.2903 |           0.2316 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0166 |          10.1781 |           0.2317 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0164 |          10.1239 |           0.2318 |
[32m[20221213 15:13:39 @agent_ppo2.py:185][0m |          -0.0205 |          10.0724 |           0.2318 |
[32m[20221213 15:13:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.94
[32m[20221213 15:13:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.36
[32m[20221213 15:13:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.48
[32m[20221213 15:13:40 @agent_ppo2.py:143][0m Total time:      20.74 min
[32m[20221213 15:13:40 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221213 15:13:40 @agent_ppo2.py:121][0m #------------------------ Iteration 912 --------------------------#
[32m[20221213 15:13:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:40 @agent_ppo2.py:185][0m |           0.0020 |          11.4834 |           0.2341 |
[32m[20221213 15:13:40 @agent_ppo2.py:185][0m |          -0.0072 |          11.0673 |           0.2336 |
[32m[20221213 15:13:40 @agent_ppo2.py:185][0m |           0.0028 |          11.8800 |           0.2333 |
[32m[20221213 15:13:40 @agent_ppo2.py:185][0m |          -0.0128 |          10.8725 |           0.2328 |
[32m[20221213 15:13:40 @agent_ppo2.py:185][0m |          -0.0138 |          10.7815 |           0.2330 |
[32m[20221213 15:13:40 @agent_ppo2.py:185][0m |          -0.0129 |          10.8418 |           0.2328 |
[32m[20221213 15:13:41 @agent_ppo2.py:185][0m |          -0.0176 |          10.6980 |           0.2329 |
[32m[20221213 15:13:41 @agent_ppo2.py:185][0m |          -0.0179 |          10.6310 |           0.2327 |
[32m[20221213 15:13:41 @agent_ppo2.py:185][0m |          -0.0153 |          10.5866 |           0.2326 |
[32m[20221213 15:13:41 @agent_ppo2.py:185][0m |          -0.0181 |          10.5356 |           0.2326 |
[32m[20221213 15:13:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.15
[32m[20221213 15:13:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 202.55
[32m[20221213 15:13:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.86
[32m[20221213 15:13:41 @agent_ppo2.py:143][0m Total time:      20.76 min
[32m[20221213 15:13:41 @agent_ppo2.py:145][0m 1869824 total steps have happened
[32m[20221213 15:13:41 @agent_ppo2.py:121][0m #------------------------ Iteration 913 --------------------------#
[32m[20221213 15:13:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:41 @agent_ppo2.py:185][0m |           0.0004 |          11.4712 |           0.2403 |
[32m[20221213 15:13:41 @agent_ppo2.py:185][0m |           0.0036 |          12.4333 |           0.2403 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0051 |          11.2525 |           0.2399 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0109 |          11.1209 |           0.2399 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0151 |          11.0805 |           0.2400 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0106 |          11.1106 |           0.2399 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0155 |          10.9971 |           0.2396 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0157 |          10.9922 |           0.2397 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0111 |          11.1409 |           0.2397 |
[32m[20221213 15:13:42 @agent_ppo2.py:185][0m |          -0.0174 |          10.9094 |           0.2396 |
[32m[20221213 15:13:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 195.32
[32m[20221213 15:13:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 203.72
[32m[20221213 15:13:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 207.30
[32m[20221213 15:13:42 @agent_ppo2.py:143][0m Total time:      20.79 min
[32m[20221213 15:13:42 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221213 15:13:42 @agent_ppo2.py:121][0m #------------------------ Iteration 914 --------------------------#
[32m[20221213 15:13:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0019 |          10.6994 |           0.2408 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0088 |          10.3098 |           0.2412 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0114 |          10.0521 |           0.2409 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0120 |           9.9286 |           0.2410 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0144 |           9.8108 |           0.2411 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0155 |           9.6816 |           0.2411 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0101 |           9.9069 |           0.2410 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0054 |          10.7885 |           0.2412 |
[32m[20221213 15:13:43 @agent_ppo2.py:185][0m |          -0.0173 |           9.3979 |           0.2410 |
[32m[20221213 15:13:44 @agent_ppo2.py:185][0m |          -0.0135 |           9.4729 |           0.2415 |
[32m[20221213 15:13:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 201.33
[32m[20221213 15:13:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.51
[32m[20221213 15:13:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.99
[32m[20221213 15:13:44 @agent_ppo2.py:143][0m Total time:      20.81 min
[32m[20221213 15:13:44 @agent_ppo2.py:145][0m 1873920 total steps have happened
[32m[20221213 15:13:44 @agent_ppo2.py:121][0m #------------------------ Iteration 915 --------------------------#
[32m[20221213 15:13:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:44 @agent_ppo2.py:185][0m |           0.0091 |          13.2618 |           0.2443 |
[32m[20221213 15:13:44 @agent_ppo2.py:185][0m |          -0.0076 |          11.2365 |           0.2431 |
[32m[20221213 15:13:44 @agent_ppo2.py:185][0m |          -0.0120 |          11.0387 |           0.2435 |
[32m[20221213 15:13:44 @agent_ppo2.py:185][0m |          -0.0119 |          10.8866 |           0.2433 |
[32m[20221213 15:13:44 @agent_ppo2.py:185][0m |          -0.0116 |          10.8030 |           0.2431 |
[32m[20221213 15:13:45 @agent_ppo2.py:185][0m |          -0.0176 |          10.6960 |           0.2430 |
[32m[20221213 15:13:45 @agent_ppo2.py:185][0m |          -0.0173 |          10.6389 |           0.2430 |
[32m[20221213 15:13:45 @agent_ppo2.py:185][0m |          -0.0177 |          10.5647 |           0.2427 |
[32m[20221213 15:13:45 @agent_ppo2.py:185][0m |          -0.0179 |          10.4792 |           0.2422 |
[32m[20221213 15:13:45 @agent_ppo2.py:185][0m |          -0.0195 |          10.4200 |           0.2423 |
[32m[20221213 15:13:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.48
[32m[20221213 15:13:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 190.16
[32m[20221213 15:13:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 209.42
[32m[20221213 15:13:45 @agent_ppo2.py:143][0m Total time:      20.83 min
[32m[20221213 15:13:45 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221213 15:13:45 @agent_ppo2.py:121][0m #------------------------ Iteration 916 --------------------------#
[32m[20221213 15:13:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:45 @agent_ppo2.py:185][0m |          -0.0025 |          11.4364 |           0.2418 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0074 |          11.1400 |           0.2412 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0107 |          10.9245 |           0.2406 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0151 |          10.7928 |           0.2403 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0144 |          10.6902 |           0.2400 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0122 |          10.6380 |           0.2400 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0114 |          10.7806 |           0.2396 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0172 |          10.4706 |           0.2394 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0175 |          10.3968 |           0.2392 |
[32m[20221213 15:13:46 @agent_ppo2.py:185][0m |          -0.0212 |          10.3180 |           0.2393 |
[32m[20221213 15:13:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 203.87
[32m[20221213 15:13:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.87
[32m[20221213 15:13:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.40
[32m[20221213 15:13:46 @agent_ppo2.py:143][0m Total time:      20.85 min
[32m[20221213 15:13:46 @agent_ppo2.py:145][0m 1878016 total steps have happened
[32m[20221213 15:13:46 @agent_ppo2.py:121][0m #------------------------ Iteration 917 --------------------------#
[32m[20221213 15:13:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |           0.0006 |          11.7174 |           0.2391 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0044 |          11.2962 |           0.2385 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0086 |          11.1233 |           0.2381 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0061 |          11.3472 |           0.2381 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0119 |          10.9149 |           0.2382 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0152 |          10.8301 |           0.2385 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0137 |          10.7677 |           0.2380 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0029 |          11.5730 |           0.2383 |
[32m[20221213 15:13:47 @agent_ppo2.py:185][0m |          -0.0158 |          10.5871 |           0.2375 |
[32m[20221213 15:13:48 @agent_ppo2.py:185][0m |          -0.0104 |          12.0935 |           0.2381 |
[32m[20221213 15:13:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:13:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 197.75
[32m[20221213 15:13:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 203.68
[32m[20221213 15:13:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.49
[32m[20221213 15:13:48 @agent_ppo2.py:143][0m Total time:      20.88 min
[32m[20221213 15:13:48 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221213 15:13:48 @agent_ppo2.py:121][0m #------------------------ Iteration 918 --------------------------#
[32m[20221213 15:13:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:13:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:48 @agent_ppo2.py:185][0m |          -0.0001 |          11.3451 |           0.2424 |
[32m[20221213 15:13:48 @agent_ppo2.py:185][0m |          -0.0049 |          10.9344 |           0.2421 |
[32m[20221213 15:13:48 @agent_ppo2.py:185][0m |          -0.0108 |          10.6773 |           0.2420 |
[32m[20221213 15:13:48 @agent_ppo2.py:185][0m |          -0.0130 |          10.4632 |           0.2417 |
[32m[20221213 15:13:48 @agent_ppo2.py:185][0m |          -0.0139 |          10.3103 |           0.2418 |
[32m[20221213 15:13:49 @agent_ppo2.py:185][0m |          -0.0161 |          10.1685 |           0.2415 |
[32m[20221213 15:13:49 @agent_ppo2.py:185][0m |          -0.0162 |          10.0571 |           0.2415 |
[32m[20221213 15:13:49 @agent_ppo2.py:185][0m |          -0.0152 |           9.9518 |           0.2416 |
[32m[20221213 15:13:49 @agent_ppo2.py:185][0m |          -0.0169 |           9.8568 |           0.2417 |
[32m[20221213 15:13:49 @agent_ppo2.py:185][0m |          -0.0161 |           9.7941 |           0.2416 |
[32m[20221213 15:13:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:13:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.97
[32m[20221213 15:13:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 204.39
[32m[20221213 15:13:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.92
[32m[20221213 15:13:49 @agent_ppo2.py:143][0m Total time:      20.90 min
[32m[20221213 15:13:49 @agent_ppo2.py:145][0m 1882112 total steps have happened
[32m[20221213 15:13:49 @agent_ppo2.py:121][0m #------------------------ Iteration 919 --------------------------#
[32m[20221213 15:13:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:49 @agent_ppo2.py:185][0m |          -0.0007 |          11.2062 |           0.2413 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0005 |          11.2867 |           0.2404 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0083 |          10.8285 |           0.2400 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0116 |          10.7162 |           0.2401 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0146 |          10.5901 |           0.2399 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0136 |          10.5080 |           0.2398 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0121 |          10.5701 |           0.2396 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0120 |          10.5512 |           0.2397 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0180 |          10.2888 |           0.2396 |
[32m[20221213 15:13:50 @agent_ppo2.py:185][0m |          -0.0201 |          10.2632 |           0.2393 |
[32m[20221213 15:13:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 193.17
[32m[20221213 15:13:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 199.70
[32m[20221213 15:13:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 212.55
[32m[20221213 15:13:50 @agent_ppo2.py:143][0m Total time:      20.92 min
[32m[20221213 15:13:50 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221213 15:13:50 @agent_ppo2.py:121][0m #------------------------ Iteration 920 --------------------------#
[32m[20221213 15:13:51 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:13:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0019 |          11.6979 |           0.2384 |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0086 |          11.3349 |           0.2380 |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0121 |          11.1709 |           0.2380 |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0118 |          11.0331 |           0.2379 |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0012 |          11.9977 |           0.2381 |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0136 |          10.8573 |           0.2377 |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0144 |          10.7002 |           0.2378 |
[32m[20221213 15:13:51 @agent_ppo2.py:185][0m |          -0.0213 |          10.6521 |           0.2381 |
[32m[20221213 15:13:52 @agent_ppo2.py:185][0m |          -0.0116 |          11.0513 |           0.2380 |
[32m[20221213 15:13:52 @agent_ppo2.py:185][0m |          -0.0093 |          10.9802 |           0.2378 |
[32m[20221213 15:13:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.21
[32m[20221213 15:13:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 211.39
[32m[20221213 15:13:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 226.18
[32m[20221213 15:13:52 @agent_ppo2.py:143][0m Total time:      20.94 min
[32m[20221213 15:13:52 @agent_ppo2.py:145][0m 1886208 total steps have happened
[32m[20221213 15:13:52 @agent_ppo2.py:121][0m #------------------------ Iteration 921 --------------------------#
[32m[20221213 15:13:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:52 @agent_ppo2.py:185][0m |           0.0199 |          12.1478 |           0.2341 |
[32m[20221213 15:13:52 @agent_ppo2.py:185][0m |          -0.0051 |          10.8600 |           0.2330 |
[32m[20221213 15:13:52 @agent_ppo2.py:185][0m |           0.0027 |          11.1757 |           0.2333 |
[32m[20221213 15:13:52 @agent_ppo2.py:185][0m |          -0.0115 |          10.5833 |           0.2329 |
[32m[20221213 15:13:52 @agent_ppo2.py:185][0m |          -0.0105 |          10.4983 |           0.2327 |
[32m[20221213 15:13:53 @agent_ppo2.py:185][0m |          -0.0122 |          10.4223 |           0.2325 |
[32m[20221213 15:13:53 @agent_ppo2.py:185][0m |          -0.0159 |          10.3250 |           0.2324 |
[32m[20221213 15:13:53 @agent_ppo2.py:185][0m |          -0.0164 |          10.2734 |           0.2322 |
[32m[20221213 15:13:53 @agent_ppo2.py:185][0m |          -0.0169 |          10.2095 |           0.2323 |
[32m[20221213 15:13:53 @agent_ppo2.py:185][0m |          -0.0164 |          10.1770 |           0.2318 |
[32m[20221213 15:13:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 201.37
[32m[20221213 15:13:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.49
[32m[20221213 15:13:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 210.78
[32m[20221213 15:13:53 @agent_ppo2.py:143][0m Total time:      20.97 min
[32m[20221213 15:13:53 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221213 15:13:53 @agent_ppo2.py:121][0m #------------------------ Iteration 922 --------------------------#
[32m[20221213 15:13:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:53 @agent_ppo2.py:185][0m |          -0.0009 |          11.2141 |           0.2357 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |          -0.0070 |          11.0261 |           0.2351 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |          -0.0080 |          10.9010 |           0.2352 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |           0.0013 |          11.9765 |           0.2352 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |          -0.0105 |          10.7878 |           0.2351 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |          -0.0138 |          10.6665 |           0.2352 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |           0.0054 |          12.0857 |           0.2351 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |          -0.0151 |          10.6060 |           0.2350 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |          -0.0171 |          10.5063 |           0.2350 |
[32m[20221213 15:13:54 @agent_ppo2.py:185][0m |          -0.0160 |          10.4428 |           0.2349 |
[32m[20221213 15:13:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:13:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 184.78
[32m[20221213 15:13:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 197.53
[32m[20221213 15:13:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.28
[32m[20221213 15:13:54 @agent_ppo2.py:143][0m Total time:      20.99 min
[32m[20221213 15:13:54 @agent_ppo2.py:145][0m 1890304 total steps have happened
[32m[20221213 15:13:54 @agent_ppo2.py:121][0m #------------------------ Iteration 923 --------------------------#
[32m[20221213 15:13:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:13:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |           0.0129 |          12.5856 |           0.2386 |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |          -0.0051 |          11.0006 |           0.2384 |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |          -0.0096 |          10.8283 |           0.2383 |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |          -0.0119 |          10.7205 |           0.2382 |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |          -0.0093 |          10.7760 |           0.2383 |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |          -0.0115 |          10.5006 |           0.2381 |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |          -0.0141 |          10.4183 |           0.2381 |
[32m[20221213 15:13:55 @agent_ppo2.py:185][0m |          -0.0153 |          10.3436 |           0.2382 |
[32m[20221213 15:13:56 @agent_ppo2.py:185][0m |          -0.0151 |          10.3362 |           0.2380 |
[32m[20221213 15:13:56 @agent_ppo2.py:185][0m |          -0.0168 |          10.2155 |           0.2381 |
[32m[20221213 15:13:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 201.24
[32m[20221213 15:13:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.95
[32m[20221213 15:13:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.45
[32m[20221213 15:13:56 @agent_ppo2.py:143][0m Total time:      21.01 min
[32m[20221213 15:13:56 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221213 15:13:56 @agent_ppo2.py:121][0m #------------------------ Iteration 924 --------------------------#
[32m[20221213 15:13:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:56 @agent_ppo2.py:185][0m |          -0.0028 |          10.8561 |           0.2355 |
[32m[20221213 15:13:56 @agent_ppo2.py:185][0m |          -0.0083 |          10.4546 |           0.2351 |
[32m[20221213 15:13:56 @agent_ppo2.py:185][0m |           0.0005 |          10.8582 |           0.2352 |
[32m[20221213 15:13:56 @agent_ppo2.py:185][0m |          -0.0153 |          10.0863 |           0.2353 |
[32m[20221213 15:13:57 @agent_ppo2.py:185][0m |          -0.0161 |           9.9049 |           0.2352 |
[32m[20221213 15:13:57 @agent_ppo2.py:185][0m |          -0.0164 |           9.7852 |           0.2353 |
[32m[20221213 15:13:57 @agent_ppo2.py:185][0m |          -0.0165 |           9.6435 |           0.2355 |
[32m[20221213 15:13:57 @agent_ppo2.py:185][0m |          -0.0175 |           9.5577 |           0.2355 |
[32m[20221213 15:13:57 @agent_ppo2.py:185][0m |          -0.0167 |           9.4631 |           0.2356 |
[32m[20221213 15:13:57 @agent_ppo2.py:185][0m |          -0.0219 |           9.3683 |           0.2357 |
[32m[20221213 15:13:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.57
[32m[20221213 15:13:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 194.44
[32m[20221213 15:13:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.58
[32m[20221213 15:13:57 @agent_ppo2.py:143][0m Total time:      21.03 min
[32m[20221213 15:13:57 @agent_ppo2.py:145][0m 1894400 total steps have happened
[32m[20221213 15:13:57 @agent_ppo2.py:121][0m #------------------------ Iteration 925 --------------------------#
[32m[20221213 15:13:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |           0.0045 |          11.3320 |           0.2365 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0079 |          10.7737 |           0.2365 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0051 |          10.6933 |           0.2363 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0101 |          10.4607 |           0.2365 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0105 |          10.3437 |           0.2366 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0128 |          10.2639 |           0.2368 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0152 |          10.1695 |           0.2368 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0071 |          10.4760 |           0.2368 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0176 |          10.0240 |           0.2368 |
[32m[20221213 15:13:58 @agent_ppo2.py:185][0m |          -0.0141 |          10.0172 |           0.2369 |
[32m[20221213 15:13:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:13:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.25
[32m[20221213 15:13:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 200.24
[32m[20221213 15:13:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.61
[32m[20221213 15:13:58 @agent_ppo2.py:143][0m Total time:      21.06 min
[32m[20221213 15:13:58 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221213 15:13:58 @agent_ppo2.py:121][0m #------------------------ Iteration 926 --------------------------#
[32m[20221213 15:13:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:13:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0029 |          10.8575 |           0.2431 |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0106 |          10.0515 |           0.2424 |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0053 |           9.8591 |           0.2421 |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0156 |           9.1687 |           0.2419 |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0155 |           8.7890 |           0.2421 |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0152 |           8.5341 |           0.2421 |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0171 |           8.2678 |           0.2418 |
[32m[20221213 15:13:59 @agent_ppo2.py:185][0m |          -0.0206 |           8.1486 |           0.2416 |
[32m[20221213 15:14:00 @agent_ppo2.py:185][0m |          -0.0210 |           7.9523 |           0.2417 |
[32m[20221213 15:14:00 @agent_ppo2.py:185][0m |          -0.0210 |           7.8145 |           0.2418 |
[32m[20221213 15:14:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 200.57
[32m[20221213 15:14:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.63
[32m[20221213 15:14:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.09
[32m[20221213 15:14:00 @agent_ppo2.py:143][0m Total time:      21.08 min
[32m[20221213 15:14:00 @agent_ppo2.py:145][0m 1898496 total steps have happened
[32m[20221213 15:14:00 @agent_ppo2.py:121][0m #------------------------ Iteration 927 --------------------------#
[32m[20221213 15:14:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:14:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:00 @agent_ppo2.py:185][0m |          -0.0033 |          13.1195 |           0.2392 |
[32m[20221213 15:14:00 @agent_ppo2.py:185][0m |          -0.0047 |          12.4090 |           0.2390 |
[32m[20221213 15:14:00 @agent_ppo2.py:185][0m |          -0.0087 |          12.1379 |           0.2390 |
[32m[20221213 15:14:00 @agent_ppo2.py:185][0m |          -0.0123 |          11.8728 |           0.2392 |
[32m[20221213 15:14:01 @agent_ppo2.py:185][0m |          -0.0155 |          11.7540 |           0.2391 |
[32m[20221213 15:14:01 @agent_ppo2.py:185][0m |          -0.0120 |          11.6379 |           0.2393 |
[32m[20221213 15:14:01 @agent_ppo2.py:185][0m |          -0.0180 |          11.4977 |           0.2393 |
[32m[20221213 15:14:01 @agent_ppo2.py:185][0m |          -0.0153 |          11.6319 |           0.2394 |
[32m[20221213 15:14:01 @agent_ppo2.py:185][0m |          -0.0189 |          11.2786 |           0.2391 |
[32m[20221213 15:14:01 @agent_ppo2.py:185][0m |          -0.0149 |          11.4355 |           0.2393 |
[32m[20221213 15:14:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 195.59
[32m[20221213 15:14:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.89
[32m[20221213 15:14:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.82
[32m[20221213 15:14:01 @agent_ppo2.py:143][0m Total time:      21.10 min
[32m[20221213 15:14:01 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221213 15:14:01 @agent_ppo2.py:121][0m #------------------------ Iteration 928 --------------------------#
[32m[20221213 15:14:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0010 |          12.3368 |           0.2421 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0052 |          11.8546 |           0.2415 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0084 |          11.8916 |           0.2409 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0131 |          11.6443 |           0.2409 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0126 |          11.6201 |           0.2406 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0159 |          11.5601 |           0.2404 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0118 |          11.6458 |           0.2401 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0166 |          11.4806 |           0.2397 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0077 |          11.7564 |           0.2397 |
[32m[20221213 15:14:02 @agent_ppo2.py:185][0m |          -0.0175 |          11.4112 |           0.2394 |
[32m[20221213 15:14:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.28
[32m[20221213 15:14:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.27
[32m[20221213 15:14:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.55
[32m[20221213 15:14:03 @agent_ppo2.py:143][0m Total time:      21.12 min
[32m[20221213 15:14:03 @agent_ppo2.py:145][0m 1902592 total steps have happened
[32m[20221213 15:14:03 @agent_ppo2.py:121][0m #------------------------ Iteration 929 --------------------------#
[32m[20221213 15:14:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |           0.0019 |          11.6651 |           0.2395 |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |          -0.0098 |          11.0783 |           0.2392 |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |          -0.0113 |          10.9151 |           0.2395 |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |          -0.0143 |          10.7631 |           0.2396 |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |          -0.0148 |          10.6445 |           0.2394 |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |          -0.0189 |          10.5452 |           0.2398 |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |          -0.0208 |          10.4428 |           0.2397 |
[32m[20221213 15:14:03 @agent_ppo2.py:185][0m |          -0.0206 |          10.3379 |           0.2397 |
[32m[20221213 15:14:04 @agent_ppo2.py:185][0m |          -0.0186 |          10.2658 |           0.2398 |
[32m[20221213 15:14:04 @agent_ppo2.py:185][0m |          -0.0223 |          10.1889 |           0.2399 |
[32m[20221213 15:14:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 187.76
[32m[20221213 15:14:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 195.19
[32m[20221213 15:14:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.66
[32m[20221213 15:14:04 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.66
[32m[20221213 15:14:04 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.66
[32m[20221213 15:14:04 @agent_ppo2.py:143][0m Total time:      21.14 min
[32m[20221213 15:14:04 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221213 15:14:04 @agent_ppo2.py:121][0m #------------------------ Iteration 930 --------------------------#
[32m[20221213 15:14:04 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:14:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:04 @agent_ppo2.py:185][0m |           0.0097 |          11.9922 |           0.2412 |
[32m[20221213 15:14:04 @agent_ppo2.py:185][0m |          -0.0080 |          10.6877 |           0.2409 |
[32m[20221213 15:14:04 @agent_ppo2.py:185][0m |          -0.0115 |          10.5528 |           0.2408 |
[32m[20221213 15:14:05 @agent_ppo2.py:185][0m |          -0.0108 |          10.4391 |           0.2405 |
[32m[20221213 15:14:05 @agent_ppo2.py:185][0m |          -0.0150 |          10.3813 |           0.2402 |
[32m[20221213 15:14:05 @agent_ppo2.py:185][0m |          -0.0138 |          10.3357 |           0.2398 |
[32m[20221213 15:14:05 @agent_ppo2.py:185][0m |          -0.0145 |          10.2615 |           0.2399 |
[32m[20221213 15:14:05 @agent_ppo2.py:185][0m |          -0.0192 |          10.2012 |           0.2395 |
[32m[20221213 15:14:05 @agent_ppo2.py:185][0m |          -0.0189 |          10.1458 |           0.2393 |
[32m[20221213 15:14:05 @agent_ppo2.py:185][0m |          -0.0178 |          10.1142 |           0.2392 |
[32m[20221213 15:14:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.47
[32m[20221213 15:14:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 199.81
[32m[20221213 15:14:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.54
[32m[20221213 15:14:05 @agent_ppo2.py:143][0m Total time:      21.17 min
[32m[20221213 15:14:05 @agent_ppo2.py:145][0m 1906688 total steps have happened
[32m[20221213 15:14:05 @agent_ppo2.py:121][0m #------------------------ Iteration 931 --------------------------#
[32m[20221213 15:14:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |           0.0048 |          11.1569 |           0.2326 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0078 |          10.7410 |           0.2319 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0104 |          10.5636 |           0.2317 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0120 |          10.4141 |           0.2315 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0169 |          10.3203 |           0.2314 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0147 |          10.2360 |           0.2314 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0166 |          10.1779 |           0.2312 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0185 |          10.0885 |           0.2313 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0169 |          10.0361 |           0.2310 |
[32m[20221213 15:14:06 @agent_ppo2.py:185][0m |          -0.0114 |          10.2742 |           0.2310 |
[32m[20221213 15:14:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 198.15
[32m[20221213 15:14:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.83
[32m[20221213 15:14:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.09
[32m[20221213 15:14:07 @agent_ppo2.py:143][0m Total time:      21.19 min
[32m[20221213 15:14:07 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221213 15:14:07 @agent_ppo2.py:121][0m #------------------------ Iteration 932 --------------------------#
[32m[20221213 15:14:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:07 @agent_ppo2.py:185][0m |          -0.0010 |          11.7498 |           0.2385 |
[32m[20221213 15:14:07 @agent_ppo2.py:185][0m |          -0.0041 |          11.3470 |           0.2378 |
[32m[20221213 15:14:07 @agent_ppo2.py:185][0m |          -0.0090 |          11.1623 |           0.2379 |
[32m[20221213 15:14:07 @agent_ppo2.py:185][0m |          -0.0114 |          10.9707 |           0.2377 |
[32m[20221213 15:14:07 @agent_ppo2.py:185][0m |          -0.0107 |          10.8963 |           0.2375 |
[32m[20221213 15:14:07 @agent_ppo2.py:185][0m |          -0.0145 |          10.8083 |           0.2374 |
[32m[20221213 15:14:07 @agent_ppo2.py:185][0m |          -0.0149 |          10.7360 |           0.2372 |
[32m[20221213 15:14:08 @agent_ppo2.py:185][0m |          -0.0099 |          10.7892 |           0.2373 |
[32m[20221213 15:14:08 @agent_ppo2.py:185][0m |          -0.0076 |          11.1129 |           0.2372 |
[32m[20221213 15:14:08 @agent_ppo2.py:185][0m |          -0.0070 |          11.7013 |           0.2370 |
[32m[20221213 15:14:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.25
[32m[20221213 15:14:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.14
[32m[20221213 15:14:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.67
[32m[20221213 15:14:08 @agent_ppo2.py:143][0m Total time:      21.21 min
[32m[20221213 15:14:08 @agent_ppo2.py:145][0m 1910784 total steps have happened
[32m[20221213 15:14:08 @agent_ppo2.py:121][0m #------------------------ Iteration 933 --------------------------#
[32m[20221213 15:14:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:08 @agent_ppo2.py:185][0m |           0.0006 |          11.7102 |           0.2401 |
[32m[20221213 15:14:08 @agent_ppo2.py:185][0m |          -0.0067 |          11.4349 |           0.2397 |
[32m[20221213 15:14:08 @agent_ppo2.py:185][0m |           0.0015 |          12.7954 |           0.2397 |
[32m[20221213 15:14:09 @agent_ppo2.py:185][0m |          -0.0108 |          11.2532 |           0.2395 |
[32m[20221213 15:14:09 @agent_ppo2.py:185][0m |          -0.0111 |          11.3190 |           0.2393 |
[32m[20221213 15:14:09 @agent_ppo2.py:185][0m |          -0.0135 |          11.0807 |           0.2391 |
[32m[20221213 15:14:09 @agent_ppo2.py:185][0m |          -0.0085 |          11.1564 |           0.2395 |
[32m[20221213 15:14:09 @agent_ppo2.py:185][0m |          -0.0164 |          10.9518 |           0.2393 |
[32m[20221213 15:14:09 @agent_ppo2.py:185][0m |          -0.0173 |          10.8874 |           0.2392 |
[32m[20221213 15:14:09 @agent_ppo2.py:185][0m |          -0.0177 |          10.8688 |           0.2393 |
[32m[20221213 15:14:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 201.52
[32m[20221213 15:14:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 206.67
[32m[20221213 15:14:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.72
[32m[20221213 15:14:09 @agent_ppo2.py:143][0m Total time:      21.23 min
[32m[20221213 15:14:09 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221213 15:14:09 @agent_ppo2.py:121][0m #------------------------ Iteration 934 --------------------------#
[32m[20221213 15:14:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0007 |          11.3536 |           0.2329 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0060 |          11.1071 |           0.2323 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0083 |          10.9432 |           0.2322 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0126 |          10.8573 |           0.2322 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0113 |          10.8196 |           0.2323 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0139 |          10.6509 |           0.2322 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0161 |          10.6049 |           0.2323 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0128 |          10.5456 |           0.2324 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0179 |          10.5114 |           0.2324 |
[32m[20221213 15:14:10 @agent_ppo2.py:185][0m |          -0.0156 |          10.5321 |           0.2325 |
[32m[20221213 15:14:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 195.39
[32m[20221213 15:14:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 211.20
[32m[20221213 15:14:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.79
[32m[20221213 15:14:11 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.79
[32m[20221213 15:14:11 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.79
[32m[20221213 15:14:11 @agent_ppo2.py:143][0m Total time:      21.26 min
[32m[20221213 15:14:11 @agent_ppo2.py:145][0m 1914880 total steps have happened
[32m[20221213 15:14:11 @agent_ppo2.py:121][0m #------------------------ Iteration 935 --------------------------#
[32m[20221213 15:14:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:11 @agent_ppo2.py:185][0m |          -0.0002 |          12.1333 |           0.2372 |
[32m[20221213 15:14:11 @agent_ppo2.py:185][0m |          -0.0029 |          11.8402 |           0.2367 |
[32m[20221213 15:14:11 @agent_ppo2.py:185][0m |          -0.0090 |          11.7404 |           0.2366 |
[32m[20221213 15:14:11 @agent_ppo2.py:185][0m |           0.0005 |          12.5376 |           0.2367 |
[32m[20221213 15:14:11 @agent_ppo2.py:185][0m |          -0.0138 |          11.5617 |           0.2367 |
[32m[20221213 15:14:11 @agent_ppo2.py:185][0m |          -0.0154 |          11.5302 |           0.2366 |
[32m[20221213 15:14:11 @agent_ppo2.py:185][0m |          -0.0162 |          11.4920 |           0.2368 |
[32m[20221213 15:14:12 @agent_ppo2.py:185][0m |          -0.0116 |          11.7029 |           0.2364 |
[32m[20221213 15:14:12 @agent_ppo2.py:185][0m |          -0.0188 |          11.4441 |           0.2366 |
[32m[20221213 15:14:12 @agent_ppo2.py:185][0m |          -0.0098 |          12.3248 |           0.2367 |
[32m[20221213 15:14:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.82
[32m[20221213 15:14:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 209.18
[32m[20221213 15:14:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.38
[32m[20221213 15:14:12 @agent_ppo2.py:143][0m Total time:      21.28 min
[32m[20221213 15:14:12 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221213 15:14:12 @agent_ppo2.py:121][0m #------------------------ Iteration 936 --------------------------#
[32m[20221213 15:14:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:12 @agent_ppo2.py:185][0m |          -0.0024 |          11.3435 |           0.2306 |
[32m[20221213 15:14:12 @agent_ppo2.py:185][0m |          -0.0081 |          11.0886 |           0.2305 |
[32m[20221213 15:14:12 @agent_ppo2.py:185][0m |          -0.0076 |          10.9620 |           0.2301 |
[32m[20221213 15:14:13 @agent_ppo2.py:185][0m |           0.0026 |          12.2127 |           0.2301 |
[32m[20221213 15:14:13 @agent_ppo2.py:185][0m |          -0.0126 |          10.8158 |           0.2296 |
[32m[20221213 15:14:13 @agent_ppo2.py:185][0m |          -0.0122 |          10.7333 |           0.2297 |
[32m[20221213 15:14:13 @agent_ppo2.py:185][0m |          -0.0138 |          10.6956 |           0.2296 |
[32m[20221213 15:14:13 @agent_ppo2.py:185][0m |          -0.0170 |          10.6339 |           0.2293 |
[32m[20221213 15:14:13 @agent_ppo2.py:185][0m |          -0.0169 |          10.5825 |           0.2292 |
[32m[20221213 15:14:13 @agent_ppo2.py:185][0m |          -0.0168 |          10.5430 |           0.2292 |
[32m[20221213 15:14:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.38
[32m[20221213 15:14:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 215.79
[32m[20221213 15:14:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.73
[32m[20221213 15:14:13 @agent_ppo2.py:143][0m Total time:      21.30 min
[32m[20221213 15:14:13 @agent_ppo2.py:145][0m 1918976 total steps have happened
[32m[20221213 15:14:13 @agent_ppo2.py:121][0m #------------------------ Iteration 937 --------------------------#
[32m[20221213 15:14:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:14:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0024 |          11.8317 |           0.2345 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0083 |          11.5168 |           0.2340 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0120 |          11.3663 |           0.2341 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0126 |          11.2274 |           0.2340 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0063 |          11.8292 |           0.2343 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0101 |          11.1316 |           0.2341 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0166 |          11.0131 |           0.2343 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0154 |          10.9264 |           0.2343 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0169 |          10.9084 |           0.2343 |
[32m[20221213 15:14:14 @agent_ppo2.py:185][0m |          -0.0174 |          10.8779 |           0.2343 |
[32m[20221213 15:14:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.84
[32m[20221213 15:14:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.22
[32m[20221213 15:14:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.27
[32m[20221213 15:14:15 @agent_ppo2.py:143][0m Total time:      21.32 min
[32m[20221213 15:14:15 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221213 15:14:15 @agent_ppo2.py:121][0m #------------------------ Iteration 938 --------------------------#
[32m[20221213 15:14:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:15 @agent_ppo2.py:185][0m |          -0.0015 |          11.8663 |           0.2355 |
[32m[20221213 15:14:15 @agent_ppo2.py:185][0m |          -0.0097 |          11.5869 |           0.2351 |
[32m[20221213 15:14:15 @agent_ppo2.py:185][0m |          -0.0085 |          11.4432 |           0.2346 |
[32m[20221213 15:14:15 @agent_ppo2.py:185][0m |          -0.0102 |          11.3925 |           0.2342 |
[32m[20221213 15:14:15 @agent_ppo2.py:185][0m |          -0.0127 |          11.2658 |           0.2343 |
[32m[20221213 15:14:15 @agent_ppo2.py:185][0m |          -0.0159 |          11.1798 |           0.2341 |
[32m[20221213 15:14:16 @agent_ppo2.py:185][0m |          -0.0128 |          11.0944 |           0.2336 |
[32m[20221213 15:14:16 @agent_ppo2.py:185][0m |          -0.0173 |          11.0337 |           0.2336 |
[32m[20221213 15:14:16 @agent_ppo2.py:185][0m |          -0.0200 |          10.9925 |           0.2333 |
[32m[20221213 15:14:16 @agent_ppo2.py:185][0m |          -0.0045 |          12.2715 |           0.2330 |
[32m[20221213 15:14:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 201.78
[32m[20221213 15:14:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.00
[32m[20221213 15:14:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.78
[32m[20221213 15:14:16 @agent_ppo2.py:143][0m Total time:      21.35 min
[32m[20221213 15:14:16 @agent_ppo2.py:145][0m 1923072 total steps have happened
[32m[20221213 15:14:16 @agent_ppo2.py:121][0m #------------------------ Iteration 939 --------------------------#
[32m[20221213 15:14:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:16 @agent_ppo2.py:185][0m |          -0.0019 |          11.9746 |           0.2243 |
[32m[20221213 15:14:16 @agent_ppo2.py:185][0m |          -0.0093 |          11.6196 |           0.2243 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0058 |          11.5406 |           0.2246 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0035 |          11.7620 |           0.2247 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0128 |          11.2608 |           0.2247 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0164 |          11.1407 |           0.2249 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0158 |          11.0614 |           0.2251 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0160 |          11.0101 |           0.2251 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0184 |          10.9812 |           0.2252 |
[32m[20221213 15:14:17 @agent_ppo2.py:185][0m |          -0.0197 |          10.9232 |           0.2255 |
[32m[20221213 15:14:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.21
[32m[20221213 15:14:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 209.51
[32m[20221213 15:14:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.47
[32m[20221213 15:14:17 @agent_ppo2.py:143][0m Total time:      21.37 min
[32m[20221213 15:14:17 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221213 15:14:17 @agent_ppo2.py:121][0m #------------------------ Iteration 940 --------------------------#
[32m[20221213 15:14:18 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:14:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |           0.0006 |          12.0180 |           0.2362 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |           0.0049 |          13.3817 |           0.2365 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0096 |          11.6615 |           0.2366 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0132 |          11.5435 |           0.2365 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0131 |          11.4365 |           0.2364 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0148 |          11.3211 |           0.2364 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0167 |          11.2412 |           0.2364 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0159 |          11.1821 |           0.2364 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0173 |          11.0708 |           0.2362 |
[32m[20221213 15:14:18 @agent_ppo2.py:185][0m |          -0.0195 |          11.0039 |           0.2362 |
[32m[20221213 15:14:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.15
[32m[20221213 15:14:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 201.33
[32m[20221213 15:14:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.30
[32m[20221213 15:14:19 @agent_ppo2.py:143][0m Total time:      21.39 min
[32m[20221213 15:14:19 @agent_ppo2.py:145][0m 1927168 total steps have happened
[32m[20221213 15:14:19 @agent_ppo2.py:121][0m #------------------------ Iteration 941 --------------------------#
[32m[20221213 15:14:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:19 @agent_ppo2.py:185][0m |           0.0066 |          13.3746 |           0.2324 |
[32m[20221213 15:14:19 @agent_ppo2.py:185][0m |          -0.0038 |          12.5223 |           0.2324 |
[32m[20221213 15:14:19 @agent_ppo2.py:185][0m |           0.0137 |          14.5925 |           0.2322 |
[32m[20221213 15:14:19 @agent_ppo2.py:185][0m |          -0.0108 |          12.4818 |           0.2312 |
[32m[20221213 15:14:19 @agent_ppo2.py:185][0m |          -0.0119 |          12.2495 |           0.2319 |
[32m[20221213 15:14:19 @agent_ppo2.py:185][0m |          -0.0157 |          12.2178 |           0.2319 |
[32m[20221213 15:14:20 @agent_ppo2.py:185][0m |          -0.0155 |          12.1729 |           0.2320 |
[32m[20221213 15:14:20 @agent_ppo2.py:185][0m |          -0.0128 |          12.2110 |           0.2319 |
[32m[20221213 15:14:20 @agent_ppo2.py:185][0m |          -0.0131 |          12.0958 |           0.2322 |
[32m[20221213 15:14:20 @agent_ppo2.py:185][0m |          -0.0207 |          12.0795 |           0.2321 |
[32m[20221213 15:14:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.00
[32m[20221213 15:14:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.14
[32m[20221213 15:14:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.15
[32m[20221213 15:14:20 @agent_ppo2.py:143][0m Total time:      21.41 min
[32m[20221213 15:14:20 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221213 15:14:20 @agent_ppo2.py:121][0m #------------------------ Iteration 942 --------------------------#
[32m[20221213 15:14:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:20 @agent_ppo2.py:185][0m |          -0.0013 |          11.9951 |           0.2353 |
[32m[20221213 15:14:20 @agent_ppo2.py:185][0m |          -0.0066 |          11.6024 |           0.2354 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |          -0.0099 |          11.3720 |           0.2355 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |          -0.0064 |          11.3674 |           0.2353 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |          -0.0135 |          11.1656 |           0.2354 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |           0.0042 |          12.4364 |           0.2355 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |          -0.0120 |          11.0188 |           0.2353 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |          -0.0154 |          10.9687 |           0.2355 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |          -0.0179 |          10.9321 |           0.2354 |
[32m[20221213 15:14:21 @agent_ppo2.py:185][0m |          -0.0150 |          11.0869 |           0.2354 |
[32m[20221213 15:14:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.75
[32m[20221213 15:14:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.45
[32m[20221213 15:14:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.47
[32m[20221213 15:14:21 @agent_ppo2.py:143][0m Total time:      21.44 min
[32m[20221213 15:14:21 @agent_ppo2.py:145][0m 1931264 total steps have happened
[32m[20221213 15:14:21 @agent_ppo2.py:121][0m #------------------------ Iteration 943 --------------------------#
[32m[20221213 15:14:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0010 |          12.1610 |           0.2394 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0091 |          11.7935 |           0.2392 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0121 |          11.5790 |           0.2392 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0166 |          11.4395 |           0.2391 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0152 |          11.3375 |           0.2389 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0156 |          11.2062 |           0.2390 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0172 |          11.1095 |           0.2390 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0184 |          11.0170 |           0.2390 |
[32m[20221213 15:14:22 @agent_ppo2.py:185][0m |          -0.0085 |          11.4556 |           0.2391 |
[32m[20221213 15:14:23 @agent_ppo2.py:185][0m |          -0.0201 |          10.8461 |           0.2390 |
[32m[20221213 15:14:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:14:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 189.55
[32m[20221213 15:14:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.75
[32m[20221213 15:14:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.99
[32m[20221213 15:14:23 @agent_ppo2.py:143][0m Total time:      21.46 min
[32m[20221213 15:14:23 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221213 15:14:23 @agent_ppo2.py:121][0m #------------------------ Iteration 944 --------------------------#
[32m[20221213 15:14:23 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:14:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:23 @agent_ppo2.py:185][0m |          -0.0056 |          11.4584 |           0.2409 |
[32m[20221213 15:14:23 @agent_ppo2.py:185][0m |          -0.0094 |          10.7160 |           0.2406 |
[32m[20221213 15:14:23 @agent_ppo2.py:185][0m |          -0.0074 |          10.5174 |           0.2408 |
[32m[20221213 15:14:23 @agent_ppo2.py:185][0m |          -0.0145 |          10.0930 |           0.2407 |
[32m[20221213 15:14:24 @agent_ppo2.py:185][0m |          -0.0160 |           9.8164 |           0.2408 |
[32m[20221213 15:14:24 @agent_ppo2.py:185][0m |          -0.0122 |           9.5642 |           0.2405 |
[32m[20221213 15:14:24 @agent_ppo2.py:185][0m |          -0.0188 |           9.3960 |           0.2405 |
[32m[20221213 15:14:24 @agent_ppo2.py:185][0m |          -0.0175 |           9.2488 |           0.2403 |
[32m[20221213 15:14:24 @agent_ppo2.py:185][0m |          -0.0176 |           9.0776 |           0.2403 |
[32m[20221213 15:14:24 @agent_ppo2.py:185][0m |          -0.0143 |           9.0839 |           0.2401 |
[32m[20221213 15:14:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:14:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.81
[32m[20221213 15:14:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 215.11
[32m[20221213 15:14:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.45
[32m[20221213 15:14:24 @agent_ppo2.py:143][0m Total time:      21.48 min
[32m[20221213 15:14:24 @agent_ppo2.py:145][0m 1935360 total steps have happened
[32m[20221213 15:14:24 @agent_ppo2.py:121][0m #------------------------ Iteration 945 --------------------------#
[32m[20221213 15:14:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0013 |          12.9883 |           0.2411 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0076 |          12.5867 |           0.2405 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0101 |          12.4084 |           0.2404 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0124 |          12.3047 |           0.2404 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0144 |          12.2244 |           0.2401 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0137 |          12.1364 |           0.2398 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0112 |          12.1300 |           0.2399 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0197 |          12.0361 |           0.2398 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0165 |          11.9516 |           0.2398 |
[32m[20221213 15:14:25 @agent_ppo2.py:185][0m |          -0.0174 |          11.9197 |           0.2396 |
[32m[20221213 15:14:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:14:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.69
[32m[20221213 15:14:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.06
[32m[20221213 15:14:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 211.99
[32m[20221213 15:14:25 @agent_ppo2.py:143][0m Total time:      21.51 min
[32m[20221213 15:14:25 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221213 15:14:26 @agent_ppo2.py:121][0m #------------------------ Iteration 946 --------------------------#
[32m[20221213 15:14:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |           0.0009 |          12.2334 |           0.2317 |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |           0.0009 |          12.9660 |           0.2317 |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |          -0.0117 |          11.8182 |           0.2311 |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |          -0.0124 |          11.7494 |           0.2313 |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |          -0.0128 |          11.6811 |           0.2313 |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |          -0.0155 |          11.6539 |           0.2315 |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |          -0.0151 |          11.6218 |           0.2311 |
[32m[20221213 15:14:26 @agent_ppo2.py:185][0m |          -0.0164 |          11.5880 |           0.2311 |
[32m[20221213 15:14:27 @agent_ppo2.py:185][0m |          -0.0164 |          11.5582 |           0.2310 |
[32m[20221213 15:14:27 @agent_ppo2.py:185][0m |          -0.0192 |          11.5381 |           0.2310 |
[32m[20221213 15:14:27 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:14:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.74
[32m[20221213 15:14:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.26
[32m[20221213 15:14:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.97
[32m[20221213 15:14:27 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 270.97
[32m[20221213 15:14:27 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 270.97
[32m[20221213 15:14:27 @agent_ppo2.py:143][0m Total time:      21.53 min
[32m[20221213 15:14:27 @agent_ppo2.py:145][0m 1939456 total steps have happened
[32m[20221213 15:14:27 @agent_ppo2.py:121][0m #------------------------ Iteration 947 --------------------------#
[32m[20221213 15:14:27 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:14:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:27 @agent_ppo2.py:185][0m |           0.0053 |          12.9959 |           0.2309 |
[32m[20221213 15:14:27 @agent_ppo2.py:185][0m |          -0.0070 |          12.3200 |           0.2304 |
[32m[20221213 15:14:27 @agent_ppo2.py:185][0m |          -0.0101 |          12.2248 |           0.2305 |
[32m[20221213 15:14:28 @agent_ppo2.py:185][0m |          -0.0137 |          12.1774 |           0.2303 |
[32m[20221213 15:14:28 @agent_ppo2.py:185][0m |          -0.0102 |          12.1898 |           0.2303 |
[32m[20221213 15:14:28 @agent_ppo2.py:185][0m |          -0.0138 |          12.1141 |           0.2303 |
[32m[20221213 15:14:28 @agent_ppo2.py:185][0m |          -0.0052 |          12.7935 |           0.2304 |
[32m[20221213 15:14:28 @agent_ppo2.py:185][0m |          -0.0162 |          12.1056 |           0.2302 |
[32m[20221213 15:14:28 @agent_ppo2.py:185][0m |          -0.0155 |          12.0345 |           0.2302 |
[32m[20221213 15:14:28 @agent_ppo2.py:185][0m |          -0.0129 |          12.1805 |           0.2304 |
[32m[20221213 15:14:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:14:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.28
[32m[20221213 15:14:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.38
[32m[20221213 15:14:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 224.76
[32m[20221213 15:14:28 @agent_ppo2.py:143][0m Total time:      21.55 min
[32m[20221213 15:14:28 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221213 15:14:28 @agent_ppo2.py:121][0m #------------------------ Iteration 948 --------------------------#
[32m[20221213 15:14:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |           0.0037 |          12.3868 |           0.2348 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0062 |          12.0145 |           0.2348 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0092 |          11.7865 |           0.2349 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0053 |          11.8429 |           0.2348 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0104 |          11.4279 |           0.2348 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0112 |          11.2349 |           0.2350 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0150 |          11.0654 |           0.2350 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0104 |          11.1162 |           0.2350 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0170 |          10.8810 |           0.2349 |
[32m[20221213 15:14:29 @agent_ppo2.py:185][0m |          -0.0170 |          10.7714 |           0.2350 |
[32m[20221213 15:14:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:14:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 188.92
[32m[20221213 15:14:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 202.35
[32m[20221213 15:14:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 218.86
[32m[20221213 15:14:30 @agent_ppo2.py:143][0m Total time:      21.57 min
[32m[20221213 15:14:30 @agent_ppo2.py:145][0m 1943552 total steps have happened
[32m[20221213 15:14:30 @agent_ppo2.py:121][0m #------------------------ Iteration 949 --------------------------#
[32m[20221213 15:14:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:30 @agent_ppo2.py:185][0m |           0.0008 |          13.3699 |           0.2339 |
[32m[20221213 15:14:30 @agent_ppo2.py:185][0m |          -0.0068 |          12.9869 |           0.2337 |
[32m[20221213 15:14:30 @agent_ppo2.py:185][0m |          -0.0089 |          12.8362 |           0.2334 |
[32m[20221213 15:14:30 @agent_ppo2.py:185][0m |          -0.0121 |          12.7688 |           0.2334 |
[32m[20221213 15:14:30 @agent_ppo2.py:185][0m |          -0.0139 |          12.6961 |           0.2330 |
[32m[20221213 15:14:30 @agent_ppo2.py:185][0m |          -0.0096 |          12.6278 |           0.2332 |
[32m[20221213 15:14:31 @agent_ppo2.py:185][0m |          -0.0101 |          12.6724 |           0.2329 |
[32m[20221213 15:14:31 @agent_ppo2.py:185][0m |          -0.0067 |          14.0855 |           0.2331 |
[32m[20221213 15:14:31 @agent_ppo2.py:185][0m |          -0.0121 |          12.5348 |           0.2329 |
[32m[20221213 15:14:31 @agent_ppo2.py:185][0m |          -0.0078 |          13.2080 |           0.2330 |
[32m[20221213 15:14:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.92
[32m[20221213 15:14:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 210.38
[32m[20221213 15:14:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.87
[32m[20221213 15:14:31 @agent_ppo2.py:143][0m Total time:      21.60 min
[32m[20221213 15:14:31 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221213 15:14:31 @agent_ppo2.py:121][0m #------------------------ Iteration 950 --------------------------#
[32m[20221213 15:14:31 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:14:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:31 @agent_ppo2.py:185][0m |          -0.0009 |          12.5861 |           0.2419 |
[32m[20221213 15:14:31 @agent_ppo2.py:185][0m |          -0.0043 |          12.3711 |           0.2422 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0074 |          12.3627 |           0.2421 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0145 |          12.0755 |           0.2418 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0110 |          11.9998 |           0.2417 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0079 |          12.1771 |           0.2416 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0176 |          11.7946 |           0.2414 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0201 |          11.7070 |           0.2414 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0121 |          11.9676 |           0.2414 |
[32m[20221213 15:14:32 @agent_ppo2.py:185][0m |          -0.0176 |          11.5374 |           0.2413 |
[32m[20221213 15:14:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:14:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.91
[32m[20221213 15:14:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 225.86
[32m[20221213 15:14:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.22
[32m[20221213 15:14:32 @agent_ppo2.py:143][0m Total time:      21.62 min
[32m[20221213 15:14:32 @agent_ppo2.py:145][0m 1947648 total steps have happened
[32m[20221213 15:14:32 @agent_ppo2.py:121][0m #------------------------ Iteration 951 --------------------------#
[32m[20221213 15:14:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |           0.0121 |          13.5905 |           0.2410 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0101 |          12.3161 |           0.2402 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0079 |          12.1408 |           0.2400 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0114 |          12.0666 |           0.2400 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0111 |          11.9532 |           0.2399 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0114 |          11.9709 |           0.2399 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0173 |          11.7909 |           0.2397 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0155 |          11.7430 |           0.2397 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0180 |          11.6975 |           0.2395 |
[32m[20221213 15:14:33 @agent_ppo2.py:185][0m |          -0.0170 |          11.6075 |           0.2395 |
[32m[20221213 15:14:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.49
[32m[20221213 15:14:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 226.92
[32m[20221213 15:14:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.83
[32m[20221213 15:14:34 @agent_ppo2.py:143][0m Total time:      21.64 min
[32m[20221213 15:14:34 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221213 15:14:34 @agent_ppo2.py:121][0m #------------------------ Iteration 952 --------------------------#
[32m[20221213 15:14:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:34 @agent_ppo2.py:185][0m |           0.0014 |          12.4284 |           0.2387 |
[32m[20221213 15:14:34 @agent_ppo2.py:185][0m |          -0.0072 |          12.0521 |           0.2384 |
[32m[20221213 15:14:34 @agent_ppo2.py:185][0m |          -0.0086 |          11.9092 |           0.2379 |
[32m[20221213 15:14:34 @agent_ppo2.py:185][0m |          -0.0105 |          11.8017 |           0.2377 |
[32m[20221213 15:14:34 @agent_ppo2.py:185][0m |          -0.0128 |          11.7672 |           0.2376 |
[32m[20221213 15:14:34 @agent_ppo2.py:185][0m |          -0.0130 |          11.6898 |           0.2374 |
[32m[20221213 15:14:35 @agent_ppo2.py:185][0m |          -0.0139 |          11.6175 |           0.2373 |
[32m[20221213 15:14:35 @agent_ppo2.py:185][0m |          -0.0160 |          11.6216 |           0.2369 |
[32m[20221213 15:14:35 @agent_ppo2.py:185][0m |          -0.0075 |          12.2658 |           0.2366 |
[32m[20221213 15:14:35 @agent_ppo2.py:185][0m |          -0.0154 |          11.5355 |           0.2363 |
[32m[20221213 15:14:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.77
[32m[20221213 15:14:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 226.30
[32m[20221213 15:14:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.21
[32m[20221213 15:14:35 @agent_ppo2.py:143][0m Total time:      21.66 min
[32m[20221213 15:14:35 @agent_ppo2.py:145][0m 1951744 total steps have happened
[32m[20221213 15:14:35 @agent_ppo2.py:121][0m #------------------------ Iteration 953 --------------------------#
[32m[20221213 15:14:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:35 @agent_ppo2.py:185][0m |          -0.0028 |          13.1001 |           0.2317 |
[32m[20221213 15:14:35 @agent_ppo2.py:185][0m |          -0.0093 |          12.7787 |           0.2316 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0047 |          13.2215 |           0.2316 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0077 |          12.8489 |           0.2315 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0166 |          12.3971 |           0.2312 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0130 |          12.4169 |           0.2314 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0189 |          12.2084 |           0.2312 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0194 |          12.1345 |           0.2312 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0192 |          12.1048 |           0.2313 |
[32m[20221213 15:14:36 @agent_ppo2.py:185][0m |          -0.0202 |          12.0110 |           0.2311 |
[32m[20221213 15:14:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:14:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.58
[32m[20221213 15:14:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.76
[32m[20221213 15:14:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.53
[32m[20221213 15:14:36 @agent_ppo2.py:143][0m Total time:      21.69 min
[32m[20221213 15:14:36 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221213 15:14:36 @agent_ppo2.py:121][0m #------------------------ Iteration 954 --------------------------#
[32m[20221213 15:14:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0024 |          13.1554 |           0.2348 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0096 |          12.8627 |           0.2345 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0087 |          12.6549 |           0.2347 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0090 |          12.5178 |           0.2342 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0123 |          12.4081 |           0.2345 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0155 |          12.2731 |           0.2346 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0077 |          12.9389 |           0.2346 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0156 |          12.0211 |           0.2344 |
[32m[20221213 15:14:37 @agent_ppo2.py:185][0m |          -0.0154 |          11.9112 |           0.2344 |
[32m[20221213 15:14:38 @agent_ppo2.py:185][0m |          -0.0183 |          11.7918 |           0.2347 |
[32m[20221213 15:14:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 186.40
[32m[20221213 15:14:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 213.48
[32m[20221213 15:14:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.86
[32m[20221213 15:14:38 @agent_ppo2.py:143][0m Total time:      21.71 min
[32m[20221213 15:14:38 @agent_ppo2.py:145][0m 1955840 total steps have happened
[32m[20221213 15:14:38 @agent_ppo2.py:121][0m #------------------------ Iteration 955 --------------------------#
[32m[20221213 15:14:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:38 @agent_ppo2.py:185][0m |          -0.0018 |          12.5242 |           0.2367 |
[32m[20221213 15:14:38 @agent_ppo2.py:185][0m |          -0.0018 |          12.2715 |           0.2363 |
[32m[20221213 15:14:38 @agent_ppo2.py:185][0m |          -0.0108 |          11.9853 |           0.2362 |
[32m[20221213 15:14:38 @agent_ppo2.py:185][0m |          -0.0138 |          11.8950 |           0.2359 |
[32m[20221213 15:14:38 @agent_ppo2.py:185][0m |          -0.0136 |          11.7799 |           0.2359 |
[32m[20221213 15:14:39 @agent_ppo2.py:185][0m |          -0.0173 |          11.7054 |           0.2360 |
[32m[20221213 15:14:39 @agent_ppo2.py:185][0m |          -0.0184 |          11.6516 |           0.2358 |
[32m[20221213 15:14:39 @agent_ppo2.py:185][0m |          -0.0173 |          11.6050 |           0.2357 |
[32m[20221213 15:14:39 @agent_ppo2.py:185][0m |          -0.0205 |          11.4861 |           0.2357 |
[32m[20221213 15:14:39 @agent_ppo2.py:185][0m |          -0.0201 |          11.4512 |           0.2356 |
[32m[20221213 15:14:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.00
[32m[20221213 15:14:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 215.88
[32m[20221213 15:14:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.39
[32m[20221213 15:14:39 @agent_ppo2.py:143][0m Total time:      21.73 min
[32m[20221213 15:14:39 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221213 15:14:39 @agent_ppo2.py:121][0m #------------------------ Iteration 956 --------------------------#
[32m[20221213 15:14:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:39 @agent_ppo2.py:185][0m |          -0.0052 |          12.4475 |           0.2258 |
[32m[20221213 15:14:39 @agent_ppo2.py:185][0m |          -0.0101 |          12.1707 |           0.2257 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0150 |          12.0533 |           0.2258 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0146 |          11.8519 |           0.2257 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0160 |          11.7580 |           0.2257 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0176 |          11.6362 |           0.2258 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0181 |          11.5392 |           0.2254 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0153 |          11.4608 |           0.2255 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0178 |          11.3794 |           0.2254 |
[32m[20221213 15:14:40 @agent_ppo2.py:185][0m |          -0.0162 |          11.2931 |           0.2254 |
[32m[20221213 15:14:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:14:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.82
[32m[20221213 15:14:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 223.60
[32m[20221213 15:14:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.06
[32m[20221213 15:14:40 @agent_ppo2.py:143][0m Total time:      21.75 min
[32m[20221213 15:14:40 @agent_ppo2.py:145][0m 1959936 total steps have happened
[32m[20221213 15:14:40 @agent_ppo2.py:121][0m #------------------------ Iteration 957 --------------------------#
[32m[20221213 15:14:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |           0.0015 |          12.5121 |           0.2358 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |          -0.0042 |          12.0233 |           0.2358 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |           0.0020 |          12.5707 |           0.2356 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |           0.0016 |          12.0795 |           0.2355 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |          -0.0035 |          11.6662 |           0.2356 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |          -0.0134 |          11.3273 |           0.2352 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |          -0.0131 |          11.2141 |           0.2352 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |          -0.0161 |          11.1341 |           0.2350 |
[32m[20221213 15:14:41 @agent_ppo2.py:185][0m |          -0.0167 |          11.0264 |           0.2352 |
[32m[20221213 15:14:42 @agent_ppo2.py:185][0m |          -0.0170 |          10.9452 |           0.2350 |
[32m[20221213 15:14:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.76
[32m[20221213 15:14:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.18
[32m[20221213 15:14:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.98
[32m[20221213 15:14:42 @agent_ppo2.py:143][0m Total time:      21.78 min
[32m[20221213 15:14:42 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221213 15:14:42 @agent_ppo2.py:121][0m #------------------------ Iteration 958 --------------------------#
[32m[20221213 15:14:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:42 @agent_ppo2.py:185][0m |           0.0085 |          14.3416 |           0.2275 |
[32m[20221213 15:14:42 @agent_ppo2.py:185][0m |          -0.0048 |          12.8945 |           0.2269 |
[32m[20221213 15:14:42 @agent_ppo2.py:185][0m |          -0.0063 |          12.6748 |           0.2270 |
[32m[20221213 15:14:42 @agent_ppo2.py:185][0m |          -0.0023 |          12.6859 |           0.2267 |
[32m[20221213 15:14:42 @agent_ppo2.py:185][0m |          -0.0105 |          12.4480 |           0.2267 |
[32m[20221213 15:14:43 @agent_ppo2.py:185][0m |          -0.0006 |          13.0691 |           0.2268 |
[32m[20221213 15:14:43 @agent_ppo2.py:185][0m |          -0.0142 |          12.2902 |           0.2265 |
[32m[20221213 15:14:43 @agent_ppo2.py:185][0m |          -0.0144 |          12.2293 |           0.2264 |
[32m[20221213 15:14:43 @agent_ppo2.py:185][0m |          -0.0123 |          12.1548 |           0.2264 |
[32m[20221213 15:14:43 @agent_ppo2.py:185][0m |          -0.0164 |          12.1107 |           0.2264 |
[32m[20221213 15:14:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:14:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.52
[32m[20221213 15:14:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 229.17
[32m[20221213 15:14:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.75
[32m[20221213 15:14:43 @agent_ppo2.py:143][0m Total time:      21.80 min
[32m[20221213 15:14:43 @agent_ppo2.py:145][0m 1964032 total steps have happened
[32m[20221213 15:14:43 @agent_ppo2.py:121][0m #------------------------ Iteration 959 --------------------------#
[32m[20221213 15:14:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:43 @agent_ppo2.py:185][0m |          -0.0011 |          13.0959 |           0.2259 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |           0.0033 |          14.3883 |           0.2250 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0084 |          12.8358 |           0.2242 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0101 |          12.7326 |           0.2246 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0115 |          12.6645 |           0.2247 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0053 |          13.3195 |           0.2245 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0137 |          12.6267 |           0.2245 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0145 |          12.5159 |           0.2245 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0163 |          12.5003 |           0.2244 |
[32m[20221213 15:14:44 @agent_ppo2.py:185][0m |          -0.0168 |          12.4750 |           0.2244 |
[32m[20221213 15:14:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.66
[32m[20221213 15:14:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 225.46
[32m[20221213 15:14:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.77
[32m[20221213 15:14:44 @agent_ppo2.py:143][0m Total time:      21.82 min
[32m[20221213 15:14:44 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221213 15:14:44 @agent_ppo2.py:121][0m #------------------------ Iteration 960 --------------------------#
[32m[20221213 15:14:45 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:14:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |           0.0006 |          13.5801 |           0.2294 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0073 |          13.3428 |           0.2290 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0093 |          13.2485 |           0.2287 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0057 |          13.3312 |           0.2287 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0110 |          13.1054 |           0.2285 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0145 |          13.0345 |           0.2284 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0139 |          12.9618 |           0.2284 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0173 |          12.9497 |           0.2283 |
[32m[20221213 15:14:45 @agent_ppo2.py:185][0m |          -0.0160 |          12.8735 |           0.2283 |
[32m[20221213 15:14:46 @agent_ppo2.py:185][0m |          -0.0178 |          12.8305 |           0.2282 |
[32m[20221213 15:14:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.13
[32m[20221213 15:14:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.07
[32m[20221213 15:14:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.03
[32m[20221213 15:14:46 @agent_ppo2.py:143][0m Total time:      21.84 min
[32m[20221213 15:14:46 @agent_ppo2.py:145][0m 1968128 total steps have happened
[32m[20221213 15:14:46 @agent_ppo2.py:121][0m #------------------------ Iteration 961 --------------------------#
[32m[20221213 15:14:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:14:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:46 @agent_ppo2.py:185][0m |          -0.0044 |          13.1504 |           0.2293 |
[32m[20221213 15:14:46 @agent_ppo2.py:185][0m |          -0.0091 |          12.8125 |           0.2289 |
[32m[20221213 15:14:46 @agent_ppo2.py:185][0m |          -0.0141 |          12.6579 |           0.2289 |
[32m[20221213 15:14:46 @agent_ppo2.py:185][0m |          -0.0165 |          12.5160 |           0.2288 |
[32m[20221213 15:14:46 @agent_ppo2.py:185][0m |          -0.0157 |          12.4313 |           0.2288 |
[32m[20221213 15:14:47 @agent_ppo2.py:185][0m |          -0.0046 |          13.3459 |           0.2288 |
[32m[20221213 15:14:47 @agent_ppo2.py:185][0m |          -0.0180 |          12.2527 |           0.2285 |
[32m[20221213 15:14:47 @agent_ppo2.py:185][0m |          -0.0180 |          12.1298 |           0.2289 |
[32m[20221213 15:14:47 @agent_ppo2.py:185][0m |          -0.0215 |          12.0612 |           0.2288 |
[32m[20221213 15:14:47 @agent_ppo2.py:185][0m |          -0.0197 |          12.0127 |           0.2287 |
[32m[20221213 15:14:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 196.74
[32m[20221213 15:14:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 212.35
[32m[20221213 15:14:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.38
[32m[20221213 15:14:47 @agent_ppo2.py:143][0m Total time:      21.86 min
[32m[20221213 15:14:47 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221213 15:14:47 @agent_ppo2.py:121][0m #------------------------ Iteration 962 --------------------------#
[32m[20221213 15:14:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:47 @agent_ppo2.py:185][0m |          -0.0029 |          13.3613 |           0.2328 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0090 |          12.9029 |           0.2324 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0109 |          12.6359 |           0.2324 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0133 |          12.4839 |           0.2322 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0153 |          12.3487 |           0.2321 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0150 |          12.1819 |           0.2321 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0159 |          12.1102 |           0.2320 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0175 |          12.0066 |           0.2322 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0113 |          12.1387 |           0.2321 |
[32m[20221213 15:14:48 @agent_ppo2.py:185][0m |          -0.0224 |          11.8309 |           0.2321 |
[32m[20221213 15:14:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.68
[32m[20221213 15:14:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 223.40
[32m[20221213 15:14:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.03
[32m[20221213 15:14:48 @agent_ppo2.py:143][0m Total time:      21.89 min
[32m[20221213 15:14:48 @agent_ppo2.py:145][0m 1972224 total steps have happened
[32m[20221213 15:14:48 @agent_ppo2.py:121][0m #------------------------ Iteration 963 --------------------------#
[32m[20221213 15:14:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:14:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |           0.0021 |          13.1964 |           0.2283 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0045 |          12.7094 |           0.2286 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0079 |          12.5598 |           0.2284 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0105 |          12.4536 |           0.2284 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0131 |          12.4069 |           0.2285 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0122 |          12.3359 |           0.2284 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0133 |          12.3166 |           0.2287 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0158 |          12.2649 |           0.2288 |
[32m[20221213 15:14:49 @agent_ppo2.py:185][0m |          -0.0155 |          12.2317 |           0.2288 |
[32m[20221213 15:14:50 @agent_ppo2.py:185][0m |          -0.0195 |          12.1815 |           0.2287 |
[32m[20221213 15:14:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.07
[32m[20221213 15:14:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.55
[32m[20221213 15:14:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.33
[32m[20221213 15:14:50 @agent_ppo2.py:143][0m Total time:      21.91 min
[32m[20221213 15:14:50 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221213 15:14:50 @agent_ppo2.py:121][0m #------------------------ Iteration 964 --------------------------#
[32m[20221213 15:14:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:50 @agent_ppo2.py:185][0m |           0.0016 |          13.5134 |           0.2396 |
[32m[20221213 15:14:50 @agent_ppo2.py:185][0m |          -0.0060 |          13.2370 |           0.2392 |
[32m[20221213 15:14:50 @agent_ppo2.py:185][0m |           0.0133 |          15.7380 |           0.2388 |
[32m[20221213 15:14:50 @agent_ppo2.py:185][0m |          -0.0097 |          12.9788 |           0.2379 |
[32m[20221213 15:14:50 @agent_ppo2.py:185][0m |           0.0005 |          13.9199 |           0.2381 |
[32m[20221213 15:14:51 @agent_ppo2.py:185][0m |          -0.0039 |          14.2298 |           0.2379 |
[32m[20221213 15:14:51 @agent_ppo2.py:185][0m |          -0.0147 |          12.6794 |           0.2377 |
[32m[20221213 15:14:51 @agent_ppo2.py:185][0m |          -0.0151 |          12.6270 |           0.2379 |
[32m[20221213 15:14:51 @agent_ppo2.py:185][0m |          -0.0146 |          12.5832 |           0.2379 |
[32m[20221213 15:14:51 @agent_ppo2.py:185][0m |           0.0001 |          13.5601 |           0.2376 |
[32m[20221213 15:14:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:14:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.85
[32m[20221213 15:14:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 223.83
[32m[20221213 15:14:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.24
[32m[20221213 15:14:51 @agent_ppo2.py:143][0m Total time:      21.93 min
[32m[20221213 15:14:51 @agent_ppo2.py:145][0m 1976320 total steps have happened
[32m[20221213 15:14:51 @agent_ppo2.py:121][0m #------------------------ Iteration 965 --------------------------#
[32m[20221213 15:14:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:14:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:51 @agent_ppo2.py:185][0m |          -0.0013 |          13.6346 |           0.2371 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |           0.0097 |          15.3574 |           0.2375 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0114 |          13.3865 |           0.2375 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0038 |          13.3845 |           0.2374 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0120 |          13.1699 |           0.2375 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0136 |          13.1326 |           0.2373 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0151 |          13.0391 |           0.2375 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0162 |          12.9837 |           0.2374 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0171 |          12.9421 |           0.2371 |
[32m[20221213 15:14:52 @agent_ppo2.py:185][0m |          -0.0164 |          12.8920 |           0.2373 |
[32m[20221213 15:14:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.20
[32m[20221213 15:14:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.66
[32m[20221213 15:14:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.83
[32m[20221213 15:14:52 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 276.83
[32m[20221213 15:14:52 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 276.83
[32m[20221213 15:14:52 @agent_ppo2.py:143][0m Total time:      21.95 min
[32m[20221213 15:14:52 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221213 15:14:52 @agent_ppo2.py:121][0m #------------------------ Iteration 966 --------------------------#
[32m[20221213 15:14:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0019 |          13.7869 |           0.2295 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0051 |          13.5563 |           0.2292 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0074 |          13.4772 |           0.2289 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0106 |          13.3776 |           0.2288 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0065 |          13.3011 |           0.2289 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0106 |          13.2517 |           0.2291 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0041 |          13.7194 |           0.2290 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0130 |          13.1544 |           0.2290 |
[32m[20221213 15:14:53 @agent_ppo2.py:185][0m |          -0.0107 |          13.1688 |           0.2290 |
[32m[20221213 15:14:54 @agent_ppo2.py:185][0m |          -0.0141 |          13.0733 |           0.2290 |
[32m[20221213 15:14:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 216.87
[32m[20221213 15:14:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 225.31
[32m[20221213 15:14:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.12
[32m[20221213 15:14:54 @agent_ppo2.py:143][0m Total time:      21.98 min
[32m[20221213 15:14:54 @agent_ppo2.py:145][0m 1980416 total steps have happened
[32m[20221213 15:14:54 @agent_ppo2.py:121][0m #------------------------ Iteration 967 --------------------------#
[32m[20221213 15:14:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:54 @agent_ppo2.py:185][0m |          -0.0005 |          13.1913 |           0.2344 |
[32m[20221213 15:14:54 @agent_ppo2.py:185][0m |          -0.0044 |          12.8614 |           0.2345 |
[32m[20221213 15:14:54 @agent_ppo2.py:185][0m |          -0.0092 |          12.6143 |           0.2346 |
[32m[20221213 15:14:54 @agent_ppo2.py:185][0m |          -0.0118 |          12.4725 |           0.2347 |
[32m[20221213 15:14:55 @agent_ppo2.py:185][0m |          -0.0064 |          12.4360 |           0.2350 |
[32m[20221213 15:14:55 @agent_ppo2.py:185][0m |          -0.0047 |          12.9589 |           0.2350 |
[32m[20221213 15:14:55 @agent_ppo2.py:185][0m |          -0.0154 |          12.0309 |           0.2350 |
[32m[20221213 15:14:55 @agent_ppo2.py:185][0m |          -0.0156 |          11.9389 |           0.2354 |
[32m[20221213 15:14:55 @agent_ppo2.py:185][0m |          -0.0151 |          11.8148 |           0.2352 |
[32m[20221213 15:14:55 @agent_ppo2.py:185][0m |          -0.0165 |          11.7398 |           0.2354 |
[32m[20221213 15:14:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:14:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 199.20
[32m[20221213 15:14:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 211.32
[32m[20221213 15:14:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.55
[32m[20221213 15:14:55 @agent_ppo2.py:143][0m Total time:      22.00 min
[32m[20221213 15:14:55 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221213 15:14:55 @agent_ppo2.py:121][0m #------------------------ Iteration 968 --------------------------#
[32m[20221213 15:14:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:55 @agent_ppo2.py:185][0m |          -0.0021 |          13.3717 |           0.2313 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0063 |          13.0339 |           0.2315 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0041 |          12.9610 |           0.2312 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0119 |          12.7193 |           0.2312 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0141 |          12.5829 |           0.2314 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0143 |          12.5519 |           0.2316 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0138 |          12.3870 |           0.2316 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0174 |          12.2750 |           0.2318 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0181 |          12.1626 |           0.2320 |
[32m[20221213 15:14:56 @agent_ppo2.py:185][0m |          -0.0185 |          12.1118 |           0.2318 |
[32m[20221213 15:14:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.15
[32m[20221213 15:14:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 217.95
[32m[20221213 15:14:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.21
[32m[20221213 15:14:56 @agent_ppo2.py:143][0m Total time:      22.02 min
[32m[20221213 15:14:56 @agent_ppo2.py:145][0m 1984512 total steps have happened
[32m[20221213 15:14:56 @agent_ppo2.py:121][0m #------------------------ Iteration 969 --------------------------#
[32m[20221213 15:14:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |           0.0040 |          13.8959 |           0.2425 |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |          -0.0031 |          12.9711 |           0.2420 |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |          -0.0094 |          12.6385 |           0.2419 |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |          -0.0123 |          12.4588 |           0.2421 |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |          -0.0132 |          12.3200 |           0.2420 |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |          -0.0155 |          12.2120 |           0.2419 |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |          -0.0145 |          12.0966 |           0.2420 |
[32m[20221213 15:14:57 @agent_ppo2.py:185][0m |          -0.0162 |          11.9632 |           0.2419 |
[32m[20221213 15:14:58 @agent_ppo2.py:185][0m |          -0.0166 |          11.9053 |           0.2419 |
[32m[20221213 15:14:58 @agent_ppo2.py:185][0m |          -0.0172 |          11.7837 |           0.2420 |
[32m[20221213 15:14:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.66
[32m[20221213 15:14:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 226.56
[32m[20221213 15:14:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.99
[32m[20221213 15:14:58 @agent_ppo2.py:143][0m Total time:      22.04 min
[32m[20221213 15:14:58 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221213 15:14:58 @agent_ppo2.py:121][0m #------------------------ Iteration 970 --------------------------#
[32m[20221213 15:14:58 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:14:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:14:58 @agent_ppo2.py:185][0m |           0.0009 |          13.9787 |           0.2406 |
[32m[20221213 15:14:58 @agent_ppo2.py:185][0m |          -0.0063 |          13.5684 |           0.2402 |
[32m[20221213 15:14:58 @agent_ppo2.py:185][0m |           0.0029 |          14.9977 |           0.2400 |
[32m[20221213 15:14:58 @agent_ppo2.py:185][0m |          -0.0128 |          13.3175 |           0.2399 |
[32m[20221213 15:14:59 @agent_ppo2.py:185][0m |          -0.0108 |          13.2384 |           0.2401 |
[32m[20221213 15:14:59 @agent_ppo2.py:185][0m |          -0.0159 |          13.1323 |           0.2400 |
[32m[20221213 15:14:59 @agent_ppo2.py:185][0m |          -0.0144 |          13.0647 |           0.2401 |
[32m[20221213 15:14:59 @agent_ppo2.py:185][0m |          -0.0179 |          13.0334 |           0.2403 |
[32m[20221213 15:14:59 @agent_ppo2.py:185][0m |          -0.0156 |          13.0611 |           0.2403 |
[32m[20221213 15:14:59 @agent_ppo2.py:185][0m |          -0.0175 |          12.9457 |           0.2403 |
[32m[20221213 15:14:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:14:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 200.29
[32m[20221213 15:14:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.41
[32m[20221213 15:14:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 217.41
[32m[20221213 15:14:59 @agent_ppo2.py:143][0m Total time:      22.07 min
[32m[20221213 15:14:59 @agent_ppo2.py:145][0m 1988608 total steps have happened
[32m[20221213 15:14:59 @agent_ppo2.py:121][0m #------------------------ Iteration 971 --------------------------#
[32m[20221213 15:14:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:14:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |           0.0066 |          13.8655 |           0.2412 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0088 |          13.0931 |           0.2403 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0093 |          12.9770 |           0.2405 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0110 |          12.8611 |           0.2403 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0084 |          12.9118 |           0.2399 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0146 |          12.6181 |           0.2403 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0169 |          12.5351 |           0.2403 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0154 |          12.3941 |           0.2403 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0180 |          12.2932 |           0.2403 |
[32m[20221213 15:15:00 @agent_ppo2.py:185][0m |          -0.0165 |          12.2093 |           0.2405 |
[32m[20221213 15:15:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 216.14
[32m[20221213 15:15:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.40
[32m[20221213 15:15:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.69
[32m[20221213 15:15:00 @agent_ppo2.py:143][0m Total time:      22.09 min
[32m[20221213 15:15:00 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221213 15:15:00 @agent_ppo2.py:121][0m #------------------------ Iteration 972 --------------------------#
[32m[20221213 15:15:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |           0.0043 |          13.6917 |           0.2523 |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |          -0.0076 |          13.0562 |           0.2517 |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |          -0.0113 |          12.7968 |           0.2516 |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |          -0.0136 |          12.6478 |           0.2514 |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |          -0.0151 |          12.4761 |           0.2518 |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |          -0.0145 |          12.3950 |           0.2520 |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |          -0.0178 |          12.1834 |           0.2518 |
[32m[20221213 15:15:01 @agent_ppo2.py:185][0m |          -0.0170 |          12.0510 |           0.2520 |
[32m[20221213 15:15:02 @agent_ppo2.py:185][0m |          -0.0186 |          11.9911 |           0.2520 |
[32m[20221213 15:15:02 @agent_ppo2.py:185][0m |          -0.0182 |          11.8550 |           0.2519 |
[32m[20221213 15:15:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.75
[32m[20221213 15:15:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 225.99
[32m[20221213 15:15:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.09
[32m[20221213 15:15:02 @agent_ppo2.py:143][0m Total time:      22.11 min
[32m[20221213 15:15:02 @agent_ppo2.py:145][0m 1992704 total steps have happened
[32m[20221213 15:15:02 @agent_ppo2.py:121][0m #------------------------ Iteration 973 --------------------------#
[32m[20221213 15:15:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:02 @agent_ppo2.py:185][0m |          -0.0038 |          14.0289 |           0.2463 |
[32m[20221213 15:15:02 @agent_ppo2.py:185][0m |          -0.0035 |          13.8509 |           0.2462 |
[32m[20221213 15:15:02 @agent_ppo2.py:185][0m |          -0.0119 |          13.6704 |           0.2460 |
[32m[20221213 15:15:02 @agent_ppo2.py:185][0m |          -0.0076 |          13.6634 |           0.2457 |
[32m[20221213 15:15:03 @agent_ppo2.py:185][0m |          -0.0063 |          13.7028 |           0.2458 |
[32m[20221213 15:15:03 @agent_ppo2.py:185][0m |          -0.0109 |          13.5168 |           0.2455 |
[32m[20221213 15:15:03 @agent_ppo2.py:185][0m |          -0.0086 |          13.5143 |           0.2454 |
[32m[20221213 15:15:03 @agent_ppo2.py:185][0m |          -0.0058 |          14.5077 |           0.2457 |
[32m[20221213 15:15:03 @agent_ppo2.py:185][0m |          -0.0013 |          14.9005 |           0.2454 |
[32m[20221213 15:15:03 @agent_ppo2.py:185][0m |          -0.0144 |          13.2865 |           0.2451 |
[32m[20221213 15:15:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.63
[32m[20221213 15:15:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.16
[32m[20221213 15:15:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.17
[32m[20221213 15:15:03 @agent_ppo2.py:143][0m Total time:      22.13 min
[32m[20221213 15:15:03 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221213 15:15:03 @agent_ppo2.py:121][0m #------------------------ Iteration 974 --------------------------#
[32m[20221213 15:15:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0019 |          12.6153 |           0.2441 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0068 |          12.3334 |           0.2437 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0099 |          12.1632 |           0.2435 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0110 |          11.9920 |           0.2435 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0131 |          11.8247 |           0.2435 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0133 |          11.7083 |           0.2433 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0089 |          11.8642 |           0.2433 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0120 |          11.5564 |           0.2429 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0141 |          11.4133 |           0.2428 |
[32m[20221213 15:15:04 @agent_ppo2.py:185][0m |          -0.0187 |          11.3846 |           0.2430 |
[32m[20221213 15:15:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.58
[32m[20221213 15:15:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.15
[32m[20221213 15:15:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.38
[32m[20221213 15:15:05 @agent_ppo2.py:143][0m Total time:      22.16 min
[32m[20221213 15:15:05 @agent_ppo2.py:145][0m 1996800 total steps have happened
[32m[20221213 15:15:05 @agent_ppo2.py:121][0m #------------------------ Iteration 975 --------------------------#
[32m[20221213 15:15:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |           0.0075 |          13.5660 |           0.2458 |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |          -0.0076 |          12.7632 |           0.2457 |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |          -0.0099 |          12.5497 |           0.2456 |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |          -0.0124 |          12.4808 |           0.2457 |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |          -0.0100 |          12.4113 |           0.2457 |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |          -0.0142 |          12.2994 |           0.2459 |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |          -0.0130 |          12.2021 |           0.2455 |
[32m[20221213 15:15:05 @agent_ppo2.py:185][0m |          -0.0160 |          12.1465 |           0.2456 |
[32m[20221213 15:15:06 @agent_ppo2.py:185][0m |          -0.0162 |          12.0830 |           0.2456 |
[32m[20221213 15:15:06 @agent_ppo2.py:185][0m |          -0.0190 |          12.0603 |           0.2458 |
[32m[20221213 15:15:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:15:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.53
[32m[20221213 15:15:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 224.70
[32m[20221213 15:15:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.94
[32m[20221213 15:15:06 @agent_ppo2.py:143][0m Total time:      22.18 min
[32m[20221213 15:15:06 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221213 15:15:06 @agent_ppo2.py:121][0m #------------------------ Iteration 976 --------------------------#
[32m[20221213 15:15:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:06 @agent_ppo2.py:185][0m |          -0.0029 |          13.0064 |           0.2450 |
[32m[20221213 15:15:06 @agent_ppo2.py:185][0m |          -0.0074 |          12.4904 |           0.2448 |
[32m[20221213 15:15:06 @agent_ppo2.py:185][0m |          -0.0107 |          12.2849 |           0.2445 |
[32m[20221213 15:15:06 @agent_ppo2.py:185][0m |          -0.0111 |          12.1122 |           0.2443 |
[32m[20221213 15:15:07 @agent_ppo2.py:185][0m |          -0.0121 |          11.9686 |           0.2447 |
[32m[20221213 15:15:07 @agent_ppo2.py:185][0m |          -0.0159 |          11.8376 |           0.2445 |
[32m[20221213 15:15:07 @agent_ppo2.py:185][0m |          -0.0162 |          11.7665 |           0.2447 |
[32m[20221213 15:15:07 @agent_ppo2.py:185][0m |          -0.0158 |          11.6152 |           0.2445 |
[32m[20221213 15:15:07 @agent_ppo2.py:185][0m |          -0.0176 |          11.5843 |           0.2447 |
[32m[20221213 15:15:07 @agent_ppo2.py:185][0m |          -0.0088 |          11.9944 |           0.2446 |
[32m[20221213 15:15:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.01
[32m[20221213 15:15:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.54
[32m[20221213 15:15:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 221.56
[32m[20221213 15:15:07 @agent_ppo2.py:143][0m Total time:      22.20 min
[32m[20221213 15:15:07 @agent_ppo2.py:145][0m 2000896 total steps have happened
[32m[20221213 15:15:07 @agent_ppo2.py:121][0m #------------------------ Iteration 977 --------------------------#
[32m[20221213 15:15:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0017 |          12.9775 |           0.2365 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0093 |          12.3259 |           0.2360 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0090 |          12.1248 |           0.2357 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0089 |          11.9465 |           0.2358 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0122 |          11.8348 |           0.2355 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0158 |          11.7563 |           0.2354 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0111 |          11.7642 |           0.2353 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0165 |          11.5581 |           0.2352 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0141 |          11.5192 |           0.2350 |
[32m[20221213 15:15:08 @agent_ppo2.py:185][0m |          -0.0079 |          11.6207 |           0.2352 |
[32m[20221213 15:15:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.65
[32m[20221213 15:15:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.03
[32m[20221213 15:15:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.13
[32m[20221213 15:15:09 @agent_ppo2.py:143][0m Total time:      22.22 min
[32m[20221213 15:15:09 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221213 15:15:09 @agent_ppo2.py:121][0m #------------------------ Iteration 978 --------------------------#
[32m[20221213 15:15:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:09 @agent_ppo2.py:185][0m |          -0.0007 |          13.4090 |           0.2531 |
[32m[20221213 15:15:09 @agent_ppo2.py:185][0m |          -0.0061 |          12.9998 |           0.2530 |
[32m[20221213 15:15:09 @agent_ppo2.py:185][0m |          -0.0108 |          12.7570 |           0.2528 |
[32m[20221213 15:15:09 @agent_ppo2.py:185][0m |          -0.0055 |          12.9277 |           0.2528 |
[32m[20221213 15:15:09 @agent_ppo2.py:185][0m |          -0.0077 |          12.5636 |           0.2528 |
[32m[20221213 15:15:09 @agent_ppo2.py:185][0m |          -0.0068 |          12.8421 |           0.2526 |
[32m[20221213 15:15:09 @agent_ppo2.py:185][0m |          -0.0130 |          12.2944 |           0.2524 |
[32m[20221213 15:15:10 @agent_ppo2.py:185][0m |          -0.0154 |          12.1717 |           0.2522 |
[32m[20221213 15:15:10 @agent_ppo2.py:185][0m |          -0.0178 |          12.0775 |           0.2522 |
[32m[20221213 15:15:10 @agent_ppo2.py:185][0m |          -0.0167 |          12.0037 |           0.2521 |
[32m[20221213 15:15:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.67
[32m[20221213 15:15:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.45
[32m[20221213 15:15:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.33
[32m[20221213 15:15:10 @agent_ppo2.py:143][0m Total time:      22.24 min
[32m[20221213 15:15:10 @agent_ppo2.py:145][0m 2004992 total steps have happened
[32m[20221213 15:15:10 @agent_ppo2.py:121][0m #------------------------ Iteration 979 --------------------------#
[32m[20221213 15:15:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:10 @agent_ppo2.py:185][0m |          -0.0000 |          14.1822 |           0.2480 |
[32m[20221213 15:15:10 @agent_ppo2.py:185][0m |          -0.0070 |          13.6731 |           0.2475 |
[32m[20221213 15:15:10 @agent_ppo2.py:185][0m |          -0.0085 |          13.4525 |           0.2473 |
[32m[20221213 15:15:11 @agent_ppo2.py:185][0m |          -0.0116 |          13.2627 |           0.2477 |
[32m[20221213 15:15:11 @agent_ppo2.py:185][0m |          -0.0149 |          13.1894 |           0.2476 |
[32m[20221213 15:15:11 @agent_ppo2.py:185][0m |          -0.0125 |          13.0617 |           0.2477 |
[32m[20221213 15:15:11 @agent_ppo2.py:185][0m |          -0.0147 |          13.0014 |           0.2478 |
[32m[20221213 15:15:11 @agent_ppo2.py:185][0m |          -0.0115 |          13.5660 |           0.2478 |
[32m[20221213 15:15:11 @agent_ppo2.py:185][0m |          -0.0127 |          12.9150 |           0.2475 |
[32m[20221213 15:15:11 @agent_ppo2.py:185][0m |          -0.0148 |          12.8547 |           0.2477 |
[32m[20221213 15:15:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 193.10
[32m[20221213 15:15:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 207.62
[32m[20221213 15:15:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.64
[32m[20221213 15:15:11 @agent_ppo2.py:143][0m Total time:      22.27 min
[32m[20221213 15:15:11 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221213 15:15:11 @agent_ppo2.py:121][0m #------------------------ Iteration 980 --------------------------#
[32m[20221213 15:15:11 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:15:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0026 |          13.1924 |           0.2424 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0079 |          12.8376 |           0.2414 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0107 |          12.6310 |           0.2412 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0123 |          12.4849 |           0.2409 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0130 |          12.3313 |           0.2406 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0143 |          12.2528 |           0.2407 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0147 |          12.1354 |           0.2405 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0167 |          12.0132 |           0.2400 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0165 |          11.9327 |           0.2400 |
[32m[20221213 15:15:12 @agent_ppo2.py:185][0m |          -0.0167 |          11.8199 |           0.2398 |
[32m[20221213 15:15:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 216.00
[32m[20221213 15:15:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.33
[32m[20221213 15:15:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.64
[32m[20221213 15:15:13 @agent_ppo2.py:143][0m Total time:      22.29 min
[32m[20221213 15:15:13 @agent_ppo2.py:145][0m 2009088 total steps have happened
[32m[20221213 15:15:13 @agent_ppo2.py:121][0m #------------------------ Iteration 981 --------------------------#
[32m[20221213 15:15:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:13 @agent_ppo2.py:185][0m |           0.0023 |          13.3329 |           0.2394 |
[32m[20221213 15:15:13 @agent_ppo2.py:185][0m |          -0.0031 |          12.8531 |           0.2395 |
[32m[20221213 15:15:13 @agent_ppo2.py:185][0m |          -0.0113 |          12.6799 |           0.2393 |
[32m[20221213 15:15:13 @agent_ppo2.py:185][0m |          -0.0095 |          12.5290 |           0.2395 |
[32m[20221213 15:15:13 @agent_ppo2.py:185][0m |          -0.0133 |          12.4289 |           0.2395 |
[32m[20221213 15:15:13 @agent_ppo2.py:185][0m |          -0.0110 |          12.3405 |           0.2395 |
[32m[20221213 15:15:13 @agent_ppo2.py:185][0m |          -0.0097 |          12.2186 |           0.2396 |
[32m[20221213 15:15:14 @agent_ppo2.py:185][0m |          -0.0130 |          12.1473 |           0.2397 |
[32m[20221213 15:15:14 @agent_ppo2.py:185][0m |          -0.0135 |          12.0573 |           0.2396 |
[32m[20221213 15:15:14 @agent_ppo2.py:185][0m |          -0.0090 |          12.0493 |           0.2398 |
[32m[20221213 15:15:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.67
[32m[20221213 15:15:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.12
[32m[20221213 15:15:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.22
[32m[20221213 15:15:14 @agent_ppo2.py:143][0m Total time:      22.31 min
[32m[20221213 15:15:14 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221213 15:15:14 @agent_ppo2.py:121][0m #------------------------ Iteration 982 --------------------------#
[32m[20221213 15:15:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:14 @agent_ppo2.py:185][0m |          -0.0026 |          13.3901 |           0.2455 |
[32m[20221213 15:15:14 @agent_ppo2.py:185][0m |          -0.0048 |          13.1114 |           0.2448 |
[32m[20221213 15:15:14 @agent_ppo2.py:185][0m |          -0.0149 |          12.5639 |           0.2448 |
[32m[20221213 15:15:15 @agent_ppo2.py:185][0m |          -0.0146 |          12.4309 |           0.2444 |
[32m[20221213 15:15:15 @agent_ppo2.py:185][0m |          -0.0150 |          12.2765 |           0.2443 |
[32m[20221213 15:15:15 @agent_ppo2.py:185][0m |          -0.0168 |          12.1386 |           0.2442 |
[32m[20221213 15:15:15 @agent_ppo2.py:185][0m |          -0.0118 |          12.2475 |           0.2442 |
[32m[20221213 15:15:15 @agent_ppo2.py:185][0m |          -0.0194 |          11.9990 |           0.2440 |
[32m[20221213 15:15:15 @agent_ppo2.py:185][0m |          -0.0196 |          11.9224 |           0.2439 |
[32m[20221213 15:15:15 @agent_ppo2.py:185][0m |          -0.0187 |          11.9594 |           0.2439 |
[32m[20221213 15:15:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 200.81
[32m[20221213 15:15:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 213.57
[32m[20221213 15:15:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.29
[32m[20221213 15:15:15 @agent_ppo2.py:143][0m Total time:      22.33 min
[32m[20221213 15:15:15 @agent_ppo2.py:145][0m 2013184 total steps have happened
[32m[20221213 15:15:15 @agent_ppo2.py:121][0m #------------------------ Iteration 983 --------------------------#
[32m[20221213 15:15:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |           0.0005 |          13.1026 |           0.2453 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0055 |          12.7704 |           0.2452 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0102 |          12.5764 |           0.2451 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0130 |          12.4099 |           0.2449 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0075 |          12.5563 |           0.2449 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0160 |          12.2249 |           0.2447 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0176 |          12.1187 |           0.2446 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0162 |          12.0570 |           0.2447 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0178 |          11.9860 |           0.2449 |
[32m[20221213 15:15:16 @agent_ppo2.py:185][0m |          -0.0190 |          11.9362 |           0.2447 |
[32m[20221213 15:15:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.13
[32m[20221213 15:15:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 216.45
[32m[20221213 15:15:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 70.54
[32m[20221213 15:15:17 @agent_ppo2.py:143][0m Total time:      22.36 min
[32m[20221213 15:15:17 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221213 15:15:17 @agent_ppo2.py:121][0m #------------------------ Iteration 984 --------------------------#
[32m[20221213 15:15:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:17 @agent_ppo2.py:185][0m |          -0.0011 |          13.1159 |           0.2468 |
[32m[20221213 15:15:17 @agent_ppo2.py:185][0m |          -0.0069 |          12.5322 |           0.2462 |
[32m[20221213 15:15:17 @agent_ppo2.py:185][0m |          -0.0113 |          12.2196 |           0.2459 |
[32m[20221213 15:15:17 @agent_ppo2.py:185][0m |          -0.0125 |          12.0932 |           0.2462 |
[32m[20221213 15:15:17 @agent_ppo2.py:185][0m |          -0.0143 |          11.8816 |           0.2460 |
[32m[20221213 15:15:17 @agent_ppo2.py:185][0m |          -0.0077 |          12.0909 |           0.2459 |
[32m[20221213 15:15:17 @agent_ppo2.py:185][0m |          -0.0133 |          11.6692 |           0.2457 |
[32m[20221213 15:15:18 @agent_ppo2.py:185][0m |          -0.0121 |          11.6651 |           0.2458 |
[32m[20221213 15:15:18 @agent_ppo2.py:185][0m |          -0.0169 |          11.4779 |           0.2455 |
[32m[20221213 15:15:18 @agent_ppo2.py:185][0m |          -0.0193 |          11.3503 |           0.2456 |
[32m[20221213 15:15:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.26
[32m[20221213 15:15:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.24
[32m[20221213 15:15:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.46
[32m[20221213 15:15:18 @agent_ppo2.py:143][0m Total time:      22.38 min
[32m[20221213 15:15:18 @agent_ppo2.py:145][0m 2017280 total steps have happened
[32m[20221213 15:15:18 @agent_ppo2.py:121][0m #------------------------ Iteration 985 --------------------------#
[32m[20221213 15:15:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:18 @agent_ppo2.py:185][0m |          -0.0030 |          13.9317 |           0.2373 |
[32m[20221213 15:15:18 @agent_ppo2.py:185][0m |          -0.0118 |          13.5605 |           0.2367 |
[32m[20221213 15:15:18 @agent_ppo2.py:185][0m |          -0.0132 |          13.3747 |           0.2367 |
[32m[20221213 15:15:19 @agent_ppo2.py:185][0m |          -0.0128 |          13.2583 |           0.2371 |
[32m[20221213 15:15:19 @agent_ppo2.py:185][0m |          -0.0134 |          13.1305 |           0.2370 |
[32m[20221213 15:15:19 @agent_ppo2.py:185][0m |          -0.0145 |          13.0818 |           0.2369 |
[32m[20221213 15:15:19 @agent_ppo2.py:185][0m |          -0.0122 |          13.0707 |           0.2368 |
[32m[20221213 15:15:19 @agent_ppo2.py:185][0m |          -0.0175 |          12.9340 |           0.2369 |
[32m[20221213 15:15:19 @agent_ppo2.py:185][0m |          -0.0202 |          12.8797 |           0.2371 |
[32m[20221213 15:15:19 @agent_ppo2.py:185][0m |          -0.0084 |          13.2412 |           0.2371 |
[32m[20221213 15:15:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:15:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.51
[32m[20221213 15:15:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.87
[32m[20221213 15:15:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.28
[32m[20221213 15:15:19 @agent_ppo2.py:143][0m Total time:      22.40 min
[32m[20221213 15:15:19 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221213 15:15:19 @agent_ppo2.py:121][0m #------------------------ Iteration 986 --------------------------#
[32m[20221213 15:15:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |           0.0099 |          15.1728 |           0.2393 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0071 |          13.0992 |           0.2387 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0094 |          12.9163 |           0.2386 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0114 |          12.8350 |           0.2388 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0022 |          13.5540 |           0.2384 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0126 |          12.7665 |           0.2387 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0141 |          12.7085 |           0.2388 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0149 |          12.6796 |           0.2389 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0153 |          12.6429 |           0.2389 |
[32m[20221213 15:15:20 @agent_ppo2.py:185][0m |          -0.0170 |          12.6150 |           0.2390 |
[32m[20221213 15:15:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.76
[32m[20221213 15:15:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 219.78
[32m[20221213 15:15:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.02
[32m[20221213 15:15:21 @agent_ppo2.py:143][0m Total time:      22.42 min
[32m[20221213 15:15:21 @agent_ppo2.py:145][0m 2021376 total steps have happened
[32m[20221213 15:15:21 @agent_ppo2.py:121][0m #------------------------ Iteration 987 --------------------------#
[32m[20221213 15:15:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:21 @agent_ppo2.py:185][0m |           0.0003 |          12.7477 |           0.2431 |
[32m[20221213 15:15:21 @agent_ppo2.py:185][0m |          -0.0073 |          12.5601 |           0.2428 |
[32m[20221213 15:15:21 @agent_ppo2.py:185][0m |          -0.0083 |          12.4535 |           0.2426 |
[32m[20221213 15:15:21 @agent_ppo2.py:185][0m |          -0.0053 |          12.7037 |           0.2429 |
[32m[20221213 15:15:21 @agent_ppo2.py:185][0m |          -0.0106 |          12.4171 |           0.2430 |
[32m[20221213 15:15:21 @agent_ppo2.py:185][0m |          -0.0074 |          12.4931 |           0.2429 |
[32m[20221213 15:15:22 @agent_ppo2.py:185][0m |          -0.0105 |          12.2684 |           0.2431 |
[32m[20221213 15:15:22 @agent_ppo2.py:185][0m |          -0.0122 |          12.2099 |           0.2431 |
[32m[20221213 15:15:22 @agent_ppo2.py:185][0m |          -0.0163 |          12.1583 |           0.2432 |
[32m[20221213 15:15:22 @agent_ppo2.py:185][0m |          -0.0160 |          12.1483 |           0.2436 |
[32m[20221213 15:15:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.48
[32m[20221213 15:15:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.65
[32m[20221213 15:15:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.79
[32m[20221213 15:15:22 @agent_ppo2.py:143][0m Total time:      22.45 min
[32m[20221213 15:15:22 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221213 15:15:22 @agent_ppo2.py:121][0m #------------------------ Iteration 988 --------------------------#
[32m[20221213 15:15:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:22 @agent_ppo2.py:185][0m |           0.0010 |          13.5294 |           0.2462 |
[32m[20221213 15:15:22 @agent_ppo2.py:185][0m |           0.0007 |          13.6505 |           0.2458 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0042 |          13.7086 |           0.2454 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0089 |          13.1545 |           0.2452 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0111 |          13.0926 |           0.2452 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0121 |          13.0565 |           0.2453 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0134 |          12.9621 |           0.2452 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0158 |          12.9227 |           0.2452 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0170 |          12.8570 |           0.2451 |
[32m[20221213 15:15:23 @agent_ppo2.py:185][0m |          -0.0155 |          12.7904 |           0.2448 |
[32m[20221213 15:15:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.62
[32m[20221213 15:15:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.91
[32m[20221213 15:15:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 68.81
[32m[20221213 15:15:23 @agent_ppo2.py:143][0m Total time:      22.47 min
[32m[20221213 15:15:23 @agent_ppo2.py:145][0m 2025472 total steps have happened
[32m[20221213 15:15:23 @agent_ppo2.py:121][0m #------------------------ Iteration 989 --------------------------#
[32m[20221213 15:15:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0007 |          13.8894 |           0.2512 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0055 |          13.4103 |           0.2502 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0096 |          13.1445 |           0.2504 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0091 |          12.9661 |           0.2502 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0064 |          12.8944 |           0.2501 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0085 |          13.1512 |           0.2500 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0150 |          12.6580 |           0.2494 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0157 |          12.5526 |           0.2499 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0141 |          12.5195 |           0.2497 |
[32m[20221213 15:15:24 @agent_ppo2.py:185][0m |          -0.0161 |          12.4234 |           0.2496 |
[32m[20221213 15:15:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:15:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.83
[32m[20221213 15:15:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.10
[32m[20221213 15:15:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.94
[32m[20221213 15:15:25 @agent_ppo2.py:143][0m Total time:      22.49 min
[32m[20221213 15:15:25 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221213 15:15:25 @agent_ppo2.py:121][0m #------------------------ Iteration 990 --------------------------#
[32m[20221213 15:15:25 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:15:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:25 @agent_ppo2.py:185][0m |          -0.0015 |          13.2445 |           0.2450 |
[32m[20221213 15:15:25 @agent_ppo2.py:185][0m |          -0.0089 |          12.7809 |           0.2440 |
[32m[20221213 15:15:25 @agent_ppo2.py:185][0m |          -0.0094 |          12.6042 |           0.2437 |
[32m[20221213 15:15:25 @agent_ppo2.py:185][0m |          -0.0087 |          12.4499 |           0.2436 |
[32m[20221213 15:15:25 @agent_ppo2.py:185][0m |          -0.0139 |          12.3500 |           0.2434 |
[32m[20221213 15:15:25 @agent_ppo2.py:185][0m |          -0.0151 |          12.1936 |           0.2433 |
[32m[20221213 15:15:26 @agent_ppo2.py:185][0m |           0.0010 |          13.3740 |           0.2429 |
[32m[20221213 15:15:26 @agent_ppo2.py:185][0m |          -0.0143 |          11.9639 |           0.2425 |
[32m[20221213 15:15:26 @agent_ppo2.py:185][0m |          -0.0161 |          11.8671 |           0.2427 |
[32m[20221213 15:15:26 @agent_ppo2.py:185][0m |          -0.0184 |          11.7973 |           0.2427 |
[32m[20221213 15:15:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:15:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.74
[32m[20221213 15:15:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.93
[32m[20221213 15:15:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.46
[32m[20221213 15:15:26 @agent_ppo2.py:143][0m Total time:      22.51 min
[32m[20221213 15:15:26 @agent_ppo2.py:145][0m 2029568 total steps have happened
[32m[20221213 15:15:26 @agent_ppo2.py:121][0m #------------------------ Iteration 991 --------------------------#
[32m[20221213 15:15:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:26 @agent_ppo2.py:185][0m |           0.0126 |          14.9094 |           0.2469 |
[32m[20221213 15:15:26 @agent_ppo2.py:185][0m |          -0.0047 |          13.2353 |           0.2463 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |           0.0005 |          14.7230 |           0.2464 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |          -0.0125 |          13.0252 |           0.2465 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |          -0.0151 |          12.9410 |           0.2466 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |          -0.0151 |          12.9085 |           0.2467 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |          -0.0154 |          12.8215 |           0.2466 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |          -0.0131 |          12.7825 |           0.2465 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |          -0.0124 |          12.9884 |           0.2470 |
[32m[20221213 15:15:27 @agent_ppo2.py:185][0m |          -0.0136 |          12.7065 |           0.2468 |
[32m[20221213 15:15:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 203.37
[32m[20221213 15:15:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.84
[32m[20221213 15:15:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.41
[32m[20221213 15:15:27 @agent_ppo2.py:143][0m Total time:      22.54 min
[32m[20221213 15:15:27 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221213 15:15:27 @agent_ppo2.py:121][0m #------------------------ Iteration 992 --------------------------#
[32m[20221213 15:15:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |           0.0042 |          14.1194 |           0.2466 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |           0.0097 |          15.1384 |           0.2462 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |          -0.0077 |          13.4664 |           0.2460 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |          -0.0109 |          13.3568 |           0.2466 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |          -0.0105 |          13.2966 |           0.2465 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |          -0.0120 |          13.3000 |           0.2468 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |          -0.0084 |          13.3476 |           0.2470 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |          -0.0078 |          13.5363 |           0.2471 |
[32m[20221213 15:15:28 @agent_ppo2.py:185][0m |          -0.0038 |          13.9343 |           0.2472 |
[32m[20221213 15:15:29 @agent_ppo2.py:185][0m |          -0.0096 |          13.2504 |           0.2473 |
[32m[20221213 15:15:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:15:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.71
[32m[20221213 15:15:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.35
[32m[20221213 15:15:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.30
[32m[20221213 15:15:29 @agent_ppo2.py:143][0m Total time:      22.56 min
[32m[20221213 15:15:29 @agent_ppo2.py:145][0m 2033664 total steps have happened
[32m[20221213 15:15:29 @agent_ppo2.py:121][0m #------------------------ Iteration 993 --------------------------#
[32m[20221213 15:15:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:29 @agent_ppo2.py:185][0m |           0.0026 |          13.5632 |           0.2513 |
[32m[20221213 15:15:29 @agent_ppo2.py:185][0m |          -0.0025 |          13.2681 |           0.2510 |
[32m[20221213 15:15:29 @agent_ppo2.py:185][0m |          -0.0088 |          12.9499 |           0.2506 |
[32m[20221213 15:15:29 @agent_ppo2.py:185][0m |           0.0011 |          14.4772 |           0.2506 |
[32m[20221213 15:15:29 @agent_ppo2.py:185][0m |          -0.0109 |          12.7123 |           0.2499 |
[32m[20221213 15:15:30 @agent_ppo2.py:185][0m |          -0.0134 |          12.5774 |           0.2500 |
[32m[20221213 15:15:30 @agent_ppo2.py:185][0m |          -0.0152 |          12.4825 |           0.2502 |
[32m[20221213 15:15:30 @agent_ppo2.py:185][0m |          -0.0130 |          12.4679 |           0.2501 |
[32m[20221213 15:15:30 @agent_ppo2.py:185][0m |          -0.0128 |          12.3337 |           0.2498 |
[32m[20221213 15:15:30 @agent_ppo2.py:185][0m |          -0.0174 |          12.2159 |           0.2498 |
[32m[20221213 15:15:30 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:15:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.08
[32m[20221213 15:15:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.17
[32m[20221213 15:15:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.66
[32m[20221213 15:15:30 @agent_ppo2.py:143][0m Total time:      22.58 min
[32m[20221213 15:15:30 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221213 15:15:30 @agent_ppo2.py:121][0m #------------------------ Iteration 994 --------------------------#
[32m[20221213 15:15:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:30 @agent_ppo2.py:185][0m |          -0.0008 |          13.7004 |           0.2445 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |           0.0003 |          13.5280 |           0.2439 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |          -0.0091 |          13.2312 |           0.2439 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |           0.0005 |          14.3419 |           0.2438 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |          -0.0134 |          13.0812 |           0.2436 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |          -0.0033 |          13.5044 |           0.2434 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |          -0.0149 |          12.9759 |           0.2435 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |          -0.0147 |          12.8917 |           0.2436 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |          -0.0162 |          12.8358 |           0.2435 |
[32m[20221213 15:15:31 @agent_ppo2.py:185][0m |          -0.0167 |          12.8057 |           0.2435 |
[32m[20221213 15:15:31 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:15:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.32
[32m[20221213 15:15:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.45
[32m[20221213 15:15:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.68
[32m[20221213 15:15:32 @agent_ppo2.py:143][0m Total time:      22.61 min
[32m[20221213 15:15:32 @agent_ppo2.py:145][0m 2037760 total steps have happened
[32m[20221213 15:15:32 @agent_ppo2.py:121][0m #------------------------ Iteration 995 --------------------------#
[32m[20221213 15:15:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:32 @agent_ppo2.py:185][0m |          -0.0002 |          13.5032 |           0.2464 |
[32m[20221213 15:15:32 @agent_ppo2.py:185][0m |           0.0047 |          14.4283 |           0.2461 |
[32m[20221213 15:15:32 @agent_ppo2.py:185][0m |          -0.0067 |          13.0488 |           0.2459 |
[32m[20221213 15:15:32 @agent_ppo2.py:185][0m |          -0.0100 |          12.8595 |           0.2459 |
[32m[20221213 15:15:32 @agent_ppo2.py:185][0m |          -0.0094 |          12.7569 |           0.2456 |
[32m[20221213 15:15:32 @agent_ppo2.py:185][0m |          -0.0095 |          12.6417 |           0.2454 |
[32m[20221213 15:15:32 @agent_ppo2.py:185][0m |          -0.0111 |          12.6752 |           0.2456 |
[32m[20221213 15:15:33 @agent_ppo2.py:185][0m |          -0.0153 |          12.4797 |           0.2454 |
[32m[20221213 15:15:33 @agent_ppo2.py:185][0m |          -0.0133 |          12.3880 |           0.2453 |
[32m[20221213 15:15:33 @agent_ppo2.py:185][0m |          -0.0141 |          12.3514 |           0.2455 |
[32m[20221213 15:15:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:15:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.98
[32m[20221213 15:15:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 220.77
[32m[20221213 15:15:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.72
[32m[20221213 15:15:33 @agent_ppo2.py:143][0m Total time:      22.63 min
[32m[20221213 15:15:33 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221213 15:15:33 @agent_ppo2.py:121][0m #------------------------ Iteration 996 --------------------------#
[32m[20221213 15:15:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:33 @agent_ppo2.py:185][0m |          -0.0028 |          13.3352 |           0.2406 |
[32m[20221213 15:15:33 @agent_ppo2.py:185][0m |          -0.0077 |          13.0492 |           0.2403 |
[32m[20221213 15:15:33 @agent_ppo2.py:185][0m |          -0.0059 |          12.9876 |           0.2404 |
[32m[20221213 15:15:34 @agent_ppo2.py:185][0m |          -0.0012 |          13.7693 |           0.2400 |
[32m[20221213 15:15:34 @agent_ppo2.py:185][0m |          -0.0096 |          12.8011 |           0.2402 |
[32m[20221213 15:15:34 @agent_ppo2.py:185][0m |          -0.0130 |          12.7177 |           0.2397 |
[32m[20221213 15:15:34 @agent_ppo2.py:185][0m |          -0.0156 |          12.6869 |           0.2396 |
[32m[20221213 15:15:34 @agent_ppo2.py:185][0m |          -0.0115 |          12.7378 |           0.2395 |
[32m[20221213 15:15:34 @agent_ppo2.py:185][0m |          -0.0160 |          12.5940 |           0.2396 |
[32m[20221213 15:15:34 @agent_ppo2.py:185][0m |          -0.0182 |          12.5689 |           0.2394 |
[32m[20221213 15:15:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:15:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.54
[32m[20221213 15:15:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.94
[32m[20221213 15:15:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.32
[32m[20221213 15:15:34 @agent_ppo2.py:143][0m Total time:      22.65 min
[32m[20221213 15:15:34 @agent_ppo2.py:145][0m 2041856 total steps have happened
[32m[20221213 15:15:34 @agent_ppo2.py:121][0m #------------------------ Iteration 997 --------------------------#
[32m[20221213 15:15:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |           0.0077 |          14.0409 |           0.2454 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0073 |          13.4651 |           0.2453 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0097 |          13.2549 |           0.2452 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0094 |          13.2118 |           0.2453 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0124 |          13.1278 |           0.2450 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0126 |          13.0739 |           0.2450 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0158 |          13.0444 |           0.2450 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0150 |          12.9280 |           0.2449 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0167 |          12.8839 |           0.2449 |
[32m[20221213 15:15:35 @agent_ppo2.py:185][0m |          -0.0179 |          12.8510 |           0.2450 |
[32m[20221213 15:15:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.37
[32m[20221213 15:15:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.86
[32m[20221213 15:15:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.73
[32m[20221213 15:15:36 @agent_ppo2.py:143][0m Total time:      22.67 min
[32m[20221213 15:15:36 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221213 15:15:36 @agent_ppo2.py:121][0m #------------------------ Iteration 998 --------------------------#
[32m[20221213 15:15:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:36 @agent_ppo2.py:185][0m |           0.0012 |          13.6904 |           0.2405 |
[32m[20221213 15:15:36 @agent_ppo2.py:185][0m |          -0.0033 |          13.3096 |           0.2400 |
[32m[20221213 15:15:36 @agent_ppo2.py:185][0m |          -0.0085 |          13.1603 |           0.2399 |
[32m[20221213 15:15:36 @agent_ppo2.py:185][0m |          -0.0114 |          13.0519 |           0.2394 |
[32m[20221213 15:15:36 @agent_ppo2.py:185][0m |          -0.0050 |          13.1646 |           0.2394 |
[32m[20221213 15:15:36 @agent_ppo2.py:185][0m |          -0.0107 |          12.9669 |           0.2395 |
[32m[20221213 15:15:37 @agent_ppo2.py:185][0m |          -0.0087 |          12.8808 |           0.2392 |
[32m[20221213 15:15:37 @agent_ppo2.py:185][0m |          -0.0127 |          12.8696 |           0.2394 |
[32m[20221213 15:15:37 @agent_ppo2.py:185][0m |          -0.0102 |          12.9866 |           0.2388 |
[32m[20221213 15:15:37 @agent_ppo2.py:185][0m |          -0.0173 |          12.7474 |           0.2392 |
[32m[20221213 15:15:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.95
[32m[20221213 15:15:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.88
[32m[20221213 15:15:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.03
[32m[20221213 15:15:37 @agent_ppo2.py:143][0m Total time:      22.70 min
[32m[20221213 15:15:37 @agent_ppo2.py:145][0m 2045952 total steps have happened
[32m[20221213 15:15:37 @agent_ppo2.py:121][0m #------------------------ Iteration 999 --------------------------#
[32m[20221213 15:15:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:37 @agent_ppo2.py:185][0m |          -0.0003 |          14.8490 |           0.2397 |
[32m[20221213 15:15:37 @agent_ppo2.py:185][0m |          -0.0071 |          14.4667 |           0.2393 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0112 |          14.2502 |           0.2392 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0112 |          14.1568 |           0.2390 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0109 |          14.0000 |           0.2387 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0118 |          13.9036 |           0.2388 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0133 |          13.8968 |           0.2388 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0149 |          13.7802 |           0.2388 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0161 |          13.6818 |           0.2384 |
[32m[20221213 15:15:38 @agent_ppo2.py:185][0m |          -0.0170 |          13.6477 |           0.2385 |
[32m[20221213 15:15:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.88
[32m[20221213 15:15:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.67
[32m[20221213 15:15:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.75
[32m[20221213 15:15:38 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 276.83
[32m[20221213 15:15:38 @agent_ppo2.py:143][0m Total time:      22.72 min
[32m[20221213 15:15:38 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221213 15:15:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221213 15:15:39 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:15:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |           0.0003 |          14.5413 |           0.2403 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0003 |          14.6305 |           0.2394 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0088 |          14.1436 |           0.2396 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0114 |          14.0313 |           0.2390 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0110 |          13.9544 |           0.2389 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0100 |          13.9638 |           0.2388 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0142 |          13.8113 |           0.2386 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0139 |          13.7308 |           0.2385 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0161 |          13.6667 |           0.2384 |
[32m[20221213 15:15:39 @agent_ppo2.py:185][0m |          -0.0167 |          13.6295 |           0.2383 |
[32m[20221213 15:15:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.22
[32m[20221213 15:15:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.81
[32m[20221213 15:15:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.92
[32m[20221213 15:15:40 @agent_ppo2.py:143][0m Total time:      22.74 min
[32m[20221213 15:15:40 @agent_ppo2.py:145][0m 2050048 total steps have happened
[32m[20221213 15:15:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221213 15:15:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:40 @agent_ppo2.py:185][0m |          -0.0009 |          14.7274 |           0.2326 |
[32m[20221213 15:15:40 @agent_ppo2.py:185][0m |          -0.0037 |          14.4402 |           0.2320 |
[32m[20221213 15:15:40 @agent_ppo2.py:185][0m |          -0.0061 |          14.3243 |           0.2318 |
[32m[20221213 15:15:40 @agent_ppo2.py:185][0m |          -0.0011 |          14.8099 |           0.2321 |
[32m[20221213 15:15:40 @agent_ppo2.py:185][0m |          -0.0088 |          14.0809 |           0.2322 |
[32m[20221213 15:15:40 @agent_ppo2.py:185][0m |          -0.0110 |          14.0444 |           0.2321 |
[32m[20221213 15:15:41 @agent_ppo2.py:185][0m |          -0.0035 |          14.5897 |           0.2320 |
[32m[20221213 15:15:41 @agent_ppo2.py:185][0m |          -0.0148 |          13.9410 |           0.2318 |
[32m[20221213 15:15:41 @agent_ppo2.py:185][0m |          -0.0148 |          13.8757 |           0.2319 |
[32m[20221213 15:15:41 @agent_ppo2.py:185][0m |          -0.0134 |          13.8023 |           0.2319 |
[32m[20221213 15:15:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:15:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.23
[32m[20221213 15:15:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.27
[32m[20221213 15:15:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.85
[32m[20221213 15:15:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 276.85
[32m[20221213 15:15:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 276.85
[32m[20221213 15:15:41 @agent_ppo2.py:143][0m Total time:      22.76 min
[32m[20221213 15:15:41 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221213 15:15:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221213 15:15:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:41 @agent_ppo2.py:185][0m |          -0.0005 |          13.7688 |           0.2365 |
[32m[20221213 15:15:41 @agent_ppo2.py:185][0m |          -0.0080 |          13.5795 |           0.2365 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0071 |          13.4713 |           0.2366 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0119 |          13.4042 |           0.2366 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0081 |          13.6188 |           0.2368 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0119 |          13.3689 |           0.2367 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0069 |          14.0965 |           0.2369 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0161 |          13.2187 |           0.2371 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0146 |          13.1859 |           0.2367 |
[32m[20221213 15:15:42 @agent_ppo2.py:185][0m |          -0.0176 |          13.0749 |           0.2370 |
[32m[20221213 15:15:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.57
[32m[20221213 15:15:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 224.57
[32m[20221213 15:15:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.01
[32m[20221213 15:15:42 @agent_ppo2.py:143][0m Total time:      22.79 min
[32m[20221213 15:15:42 @agent_ppo2.py:145][0m 2054144 total steps have happened
[32m[20221213 15:15:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221213 15:15:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |           0.0009 |          13.0021 |           0.2319 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0052 |          12.4622 |           0.2314 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0109 |          12.1316 |           0.2314 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0119 |          11.8413 |           0.2313 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0119 |          11.7075 |           0.2312 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0133 |          11.4990 |           0.2309 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0165 |          11.3970 |           0.2308 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0158 |          11.3282 |           0.2306 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0177 |          11.2138 |           0.2306 |
[32m[20221213 15:15:43 @agent_ppo2.py:185][0m |          -0.0164 |          11.1416 |           0.2305 |
[32m[20221213 15:15:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.82
[32m[20221213 15:15:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 227.06
[32m[20221213 15:15:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.10
[32m[20221213 15:15:44 @agent_ppo2.py:143][0m Total time:      22.81 min
[32m[20221213 15:15:44 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221213 15:15:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221213 15:15:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:44 @agent_ppo2.py:185][0m |          -0.0025 |          14.4110 |           0.2399 |
[32m[20221213 15:15:44 @agent_ppo2.py:185][0m |          -0.0077 |          13.9752 |           0.2393 |
[32m[20221213 15:15:44 @agent_ppo2.py:185][0m |          -0.0106 |          13.7321 |           0.2391 |
[32m[20221213 15:15:44 @agent_ppo2.py:185][0m |          -0.0101 |          13.5935 |           0.2388 |
[32m[20221213 15:15:44 @agent_ppo2.py:185][0m |          -0.0088 |          13.4962 |           0.2388 |
[32m[20221213 15:15:44 @agent_ppo2.py:185][0m |          -0.0090 |          13.5490 |           0.2388 |
[32m[20221213 15:15:45 @agent_ppo2.py:185][0m |          -0.0102 |          13.3535 |           0.2385 |
[32m[20221213 15:15:45 @agent_ppo2.py:185][0m |          -0.0148 |          13.2743 |           0.2384 |
[32m[20221213 15:15:45 @agent_ppo2.py:185][0m |          -0.0151 |          13.2135 |           0.2383 |
[32m[20221213 15:15:45 @agent_ppo2.py:185][0m |          -0.0115 |          13.4610 |           0.2383 |
[32m[20221213 15:15:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.55
[32m[20221213 15:15:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.89
[32m[20221213 15:15:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.59
[32m[20221213 15:15:45 @agent_ppo2.py:143][0m Total time:      22.83 min
[32m[20221213 15:15:45 @agent_ppo2.py:145][0m 2058240 total steps have happened
[32m[20221213 15:15:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221213 15:15:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:45 @agent_ppo2.py:185][0m |          -0.0022 |          13.9037 |           0.2431 |
[32m[20221213 15:15:45 @agent_ppo2.py:185][0m |          -0.0084 |          13.2187 |           0.2428 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0070 |          12.8908 |           0.2425 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0137 |          12.6273 |           0.2425 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0105 |          12.4426 |           0.2425 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0091 |          12.2497 |           0.2423 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0143 |          12.0318 |           0.2425 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0163 |          11.9047 |           0.2425 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0147 |          11.7833 |           0.2424 |
[32m[20221213 15:15:46 @agent_ppo2.py:185][0m |          -0.0190 |          11.6387 |           0.2425 |
[32m[20221213 15:15:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:15:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.10
[32m[20221213 15:15:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.21
[32m[20221213 15:15:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.07
[32m[20221213 15:15:46 @agent_ppo2.py:143][0m Total time:      22.85 min
[32m[20221213 15:15:46 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221213 15:15:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221213 15:15:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |           0.0066 |          15.2856 |           0.2428 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0057 |          14.1445 |           0.2425 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0106 |          13.9662 |           0.2429 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0180 |          13.8726 |           0.2429 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0133 |          13.7416 |           0.2428 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0133 |          13.6500 |           0.2429 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0145 |          13.5408 |           0.2430 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0125 |          13.4787 |           0.2431 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0158 |          13.4707 |           0.2432 |
[32m[20221213 15:15:47 @agent_ppo2.py:185][0m |          -0.0153 |          13.3580 |           0.2432 |
[32m[20221213 15:15:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.57
[32m[20221213 15:15:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.26
[32m[20221213 15:15:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.04
[32m[20221213 15:15:48 @agent_ppo2.py:143][0m Total time:      22.88 min
[32m[20221213 15:15:48 @agent_ppo2.py:145][0m 2062336 total steps have happened
[32m[20221213 15:15:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221213 15:15:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:48 @agent_ppo2.py:185][0m |          -0.0025 |          13.9487 |           0.2406 |
[32m[20221213 15:15:48 @agent_ppo2.py:185][0m |           0.0011 |          13.8130 |           0.2401 |
[32m[20221213 15:15:48 @agent_ppo2.py:185][0m |          -0.0071 |          13.2332 |           0.2395 |
[32m[20221213 15:15:48 @agent_ppo2.py:185][0m |          -0.0118 |          13.0395 |           0.2397 |
[32m[20221213 15:15:48 @agent_ppo2.py:185][0m |          -0.0068 |          13.0324 |           0.2399 |
[32m[20221213 15:15:49 @agent_ppo2.py:185][0m |          -0.0141 |          12.7787 |           0.2397 |
[32m[20221213 15:15:49 @agent_ppo2.py:185][0m |          -0.0068 |          12.9622 |           0.2396 |
[32m[20221213 15:15:49 @agent_ppo2.py:185][0m |          -0.0096 |          13.0356 |           0.2398 |
[32m[20221213 15:15:49 @agent_ppo2.py:185][0m |          -0.0134 |          12.4593 |           0.2397 |
[32m[20221213 15:15:49 @agent_ppo2.py:185][0m |          -0.0184 |          12.3853 |           0.2394 |
[32m[20221213 15:15:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.33
[32m[20221213 15:15:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.20
[32m[20221213 15:15:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.21
[32m[20221213 15:15:49 @agent_ppo2.py:143][0m Total time:      22.90 min
[32m[20221213 15:15:49 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221213 15:15:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221213 15:15:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:49 @agent_ppo2.py:185][0m |           0.0002 |          15.0174 |           0.2380 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0034 |          14.4504 |           0.2371 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0036 |          14.3922 |           0.2368 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0122 |          13.8776 |           0.2367 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0149 |          13.7183 |           0.2366 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0137 |          13.6416 |           0.2364 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0155 |          13.4687 |           0.2364 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0115 |          13.6031 |           0.2360 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0197 |          13.2875 |           0.2361 |
[32m[20221213 15:15:50 @agent_ppo2.py:185][0m |          -0.0165 |          13.4858 |           0.2359 |
[32m[20221213 15:15:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 207.41
[32m[20221213 15:15:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 218.75
[32m[20221213 15:15:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.27
[32m[20221213 15:15:50 @agent_ppo2.py:143][0m Total time:      22.92 min
[32m[20221213 15:15:50 @agent_ppo2.py:145][0m 2066432 total steps have happened
[32m[20221213 15:15:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221213 15:15:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |           0.0065 |          15.3694 |           0.2391 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0093 |          14.2616 |           0.2389 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0100 |          13.9946 |           0.2387 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0110 |          13.7804 |           0.2386 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0135 |          13.5834 |           0.2387 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0130 |          13.4435 |           0.2386 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0177 |          13.3398 |           0.2385 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0155 |          13.2116 |           0.2383 |
[32m[20221213 15:15:51 @agent_ppo2.py:185][0m |          -0.0178 |          13.1044 |           0.2383 |
[32m[20221213 15:15:52 @agent_ppo2.py:185][0m |          -0.0184 |          13.0929 |           0.2382 |
[32m[20221213 15:15:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.46
[32m[20221213 15:15:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.49
[32m[20221213 15:15:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.66
[32m[20221213 15:15:52 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 281.66
[32m[20221213 15:15:52 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 281.66
[32m[20221213 15:15:52 @agent_ppo2.py:143][0m Total time:      22.94 min
[32m[20221213 15:15:52 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221213 15:15:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221213 15:15:52 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:15:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:52 @agent_ppo2.py:185][0m |          -0.0011 |          15.2260 |           0.2379 |
[32m[20221213 15:15:52 @agent_ppo2.py:185][0m |          -0.0057 |          14.6738 |           0.2373 |
[32m[20221213 15:15:52 @agent_ppo2.py:185][0m |          -0.0098 |          14.4746 |           0.2368 |
[32m[20221213 15:15:52 @agent_ppo2.py:185][0m |          -0.0121 |          14.3291 |           0.2375 |
[32m[20221213 15:15:52 @agent_ppo2.py:185][0m |          -0.0105 |          14.2495 |           0.2371 |
[32m[20221213 15:15:53 @agent_ppo2.py:185][0m |          -0.0143 |          14.1913 |           0.2370 |
[32m[20221213 15:15:53 @agent_ppo2.py:185][0m |          -0.0045 |          14.5992 |           0.2371 |
[32m[20221213 15:15:53 @agent_ppo2.py:185][0m |          -0.0128 |          14.0731 |           0.2370 |
[32m[20221213 15:15:53 @agent_ppo2.py:185][0m |          -0.0135 |          13.9993 |           0.2372 |
[32m[20221213 15:15:53 @agent_ppo2.py:185][0m |          -0.0149 |          13.9812 |           0.2368 |
[32m[20221213 15:15:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:15:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.13
[32m[20221213 15:15:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 219.91
[32m[20221213 15:15:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.51
[32m[20221213 15:15:53 @agent_ppo2.py:143][0m Total time:      22.96 min
[32m[20221213 15:15:53 @agent_ppo2.py:145][0m 2070528 total steps have happened
[32m[20221213 15:15:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221213 15:15:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:53 @agent_ppo2.py:185][0m |           0.0000 |          15.2151 |           0.2395 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0054 |          14.9231 |           0.2394 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0057 |          14.7153 |           0.2393 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0068 |          14.6263 |           0.2390 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0112 |          14.5250 |           0.2389 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0118 |          14.4656 |           0.2388 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0146 |          14.3967 |           0.2388 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0142 |          14.3144 |           0.2389 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0075 |          14.4344 |           0.2386 |
[32m[20221213 15:15:54 @agent_ppo2.py:185][0m |          -0.0149 |          14.3163 |           0.2385 |
[32m[20221213 15:15:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.75
[32m[20221213 15:15:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.85
[32m[20221213 15:15:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.82
[32m[20221213 15:15:54 @agent_ppo2.py:143][0m Total time:      22.99 min
[32m[20221213 15:15:54 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221213 15:15:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221213 15:15:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:15:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0008 |          13.7046 |           0.2375 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0077 |          13.5499 |           0.2371 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0111 |          13.4757 |           0.2372 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0072 |          13.6102 |           0.2369 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0022 |          15.1389 |           0.2367 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0144 |          13.2735 |           0.2366 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0149 |          13.2108 |           0.2366 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0181 |          13.1897 |           0.2367 |
[32m[20221213 15:15:55 @agent_ppo2.py:185][0m |          -0.0177 |          13.1393 |           0.2367 |
[32m[20221213 15:15:56 @agent_ppo2.py:185][0m |          -0.0196 |          13.1206 |           0.2367 |
[32m[20221213 15:15:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.02
[32m[20221213 15:15:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.80
[32m[20221213 15:15:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.99
[32m[20221213 15:15:56 @agent_ppo2.py:143][0m Total time:      23.01 min
[32m[20221213 15:15:56 @agent_ppo2.py:145][0m 2074624 total steps have happened
[32m[20221213 15:15:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221213 15:15:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:56 @agent_ppo2.py:185][0m |          -0.0009 |          14.4089 |           0.2371 |
[32m[20221213 15:15:56 @agent_ppo2.py:185][0m |          -0.0054 |          14.1108 |           0.2370 |
[32m[20221213 15:15:56 @agent_ppo2.py:185][0m |          -0.0093 |          13.9730 |           0.2370 |
[32m[20221213 15:15:56 @agent_ppo2.py:185][0m |          -0.0024 |          14.2680 |           0.2371 |
[32m[20221213 15:15:56 @agent_ppo2.py:185][0m |          -0.0137 |          13.8340 |           0.2371 |
[32m[20221213 15:15:57 @agent_ppo2.py:185][0m |          -0.0134 |          13.7309 |           0.2372 |
[32m[20221213 15:15:57 @agent_ppo2.py:185][0m |          -0.0070 |          13.9389 |           0.2371 |
[32m[20221213 15:15:57 @agent_ppo2.py:185][0m |          -0.0121 |          13.6204 |           0.2372 |
[32m[20221213 15:15:57 @agent_ppo2.py:185][0m |          -0.0138 |          13.5479 |           0.2372 |
[32m[20221213 15:15:57 @agent_ppo2.py:185][0m |          -0.0164 |          13.5039 |           0.2372 |
[32m[20221213 15:15:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.46
[32m[20221213 15:15:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.95
[32m[20221213 15:15:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.78
[32m[20221213 15:15:57 @agent_ppo2.py:143][0m Total time:      23.03 min
[32m[20221213 15:15:57 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221213 15:15:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221213 15:15:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:57 @agent_ppo2.py:185][0m |          -0.0045 |          14.6952 |           0.2417 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0089 |          14.2653 |           0.2410 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0128 |          14.0968 |           0.2410 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0156 |          13.9542 |           0.2408 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0136 |          13.8316 |           0.2409 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0161 |          13.6948 |           0.2408 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0138 |          13.6745 |           0.2406 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0168 |          13.4377 |           0.2406 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0176 |          13.3685 |           0.2402 |
[32m[20221213 15:15:58 @agent_ppo2.py:185][0m |          -0.0146 |          13.3372 |           0.2404 |
[32m[20221213 15:15:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:15:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.12
[32m[20221213 15:15:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.50
[32m[20221213 15:15:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.91
[32m[20221213 15:15:58 @agent_ppo2.py:143][0m Total time:      23.05 min
[32m[20221213 15:15:58 @agent_ppo2.py:145][0m 2078720 total steps have happened
[32m[20221213 15:15:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221213 15:15:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:15:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0020 |          14.6937 |           0.2344 |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0085 |          14.3240 |           0.2340 |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0096 |          14.1010 |           0.2338 |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0064 |          14.3360 |           0.2333 |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0130 |          13.8806 |           0.2330 |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0166 |          13.7411 |           0.2327 |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0153 |          13.6785 |           0.2329 |
[32m[20221213 15:15:59 @agent_ppo2.py:185][0m |          -0.0166 |          13.5838 |           0.2324 |
[32m[20221213 15:16:00 @agent_ppo2.py:185][0m |          -0.0179 |          13.5706 |           0.2323 |
[32m[20221213 15:16:00 @agent_ppo2.py:185][0m |          -0.0163 |          13.5069 |           0.2324 |
[32m[20221213 15:16:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.03
[32m[20221213 15:16:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 233.26
[32m[20221213 15:16:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.80
[32m[20221213 15:16:00 @agent_ppo2.py:143][0m Total time:      23.08 min
[32m[20221213 15:16:00 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221213 15:16:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221213 15:16:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:00 @agent_ppo2.py:185][0m |          -0.0020 |          14.4563 |           0.2282 |
[32m[20221213 15:16:00 @agent_ppo2.py:185][0m |          -0.0085 |          14.2514 |           0.2276 |
[32m[20221213 15:16:00 @agent_ppo2.py:185][0m |          -0.0118 |          14.1196 |           0.2275 |
[32m[20221213 15:16:00 @agent_ppo2.py:185][0m |          -0.0116 |          14.0214 |           0.2273 |
[32m[20221213 15:16:01 @agent_ppo2.py:185][0m |          -0.0156 |          13.9012 |           0.2272 |
[32m[20221213 15:16:01 @agent_ppo2.py:185][0m |          -0.0159 |          13.8404 |           0.2271 |
[32m[20221213 15:16:01 @agent_ppo2.py:185][0m |          -0.0056 |          15.2480 |           0.2268 |
[32m[20221213 15:16:01 @agent_ppo2.py:185][0m |          -0.0166 |          13.7723 |           0.2264 |
[32m[20221213 15:16:01 @agent_ppo2.py:185][0m |          -0.0164 |          13.6767 |           0.2265 |
[32m[20221213 15:16:01 @agent_ppo2.py:185][0m |          -0.0191 |          13.6332 |           0.2262 |
[32m[20221213 15:16:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.53
[32m[20221213 15:16:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.73
[32m[20221213 15:16:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.63
[32m[20221213 15:16:01 @agent_ppo2.py:143][0m Total time:      23.10 min
[32m[20221213 15:16:01 @agent_ppo2.py:145][0m 2082816 total steps have happened
[32m[20221213 15:16:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221213 15:16:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |           0.0001 |          14.4963 |           0.2308 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0045 |          14.2278 |           0.2309 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0110 |          14.0557 |           0.2306 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0096 |          13.9196 |           0.2304 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0128 |          13.8650 |           0.2303 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0131 |          13.7753 |           0.2299 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0158 |          13.7479 |           0.2300 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0145 |          13.6907 |           0.2299 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0162 |          13.6476 |           0.2295 |
[32m[20221213 15:16:02 @agent_ppo2.py:185][0m |          -0.0119 |          13.8194 |           0.2294 |
[32m[20221213 15:16:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.39
[32m[20221213 15:16:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.06
[32m[20221213 15:16:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.44
[32m[20221213 15:16:02 @agent_ppo2.py:143][0m Total time:      23.12 min
[32m[20221213 15:16:02 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221213 15:16:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221213 15:16:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0019 |          15.3228 |           0.2291 |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0055 |          14.8853 |           0.2293 |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0108 |          14.6904 |           0.2293 |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0122 |          14.6058 |           0.2293 |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0135 |          14.5142 |           0.2293 |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0114 |          14.4252 |           0.2289 |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0141 |          14.3775 |           0.2291 |
[32m[20221213 15:16:03 @agent_ppo2.py:185][0m |          -0.0146 |          14.3210 |           0.2291 |
[32m[20221213 15:16:04 @agent_ppo2.py:185][0m |          -0.0173 |          14.2672 |           0.2291 |
[32m[20221213 15:16:04 @agent_ppo2.py:185][0m |          -0.0193 |          14.2113 |           0.2287 |
[32m[20221213 15:16:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 211.93
[32m[20221213 15:16:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.52
[32m[20221213 15:16:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.28
[32m[20221213 15:16:04 @agent_ppo2.py:143][0m Total time:      23.14 min
[32m[20221213 15:16:04 @agent_ppo2.py:145][0m 2086912 total steps have happened
[32m[20221213 15:16:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221213 15:16:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:04 @agent_ppo2.py:185][0m |          -0.0010 |          14.0889 |           0.2323 |
[32m[20221213 15:16:04 @agent_ppo2.py:185][0m |          -0.0069 |          13.5380 |           0.2318 |
[32m[20221213 15:16:04 @agent_ppo2.py:185][0m |          -0.0018 |          14.1074 |           0.2315 |
[32m[20221213 15:16:04 @agent_ppo2.py:185][0m |          -0.0105 |          13.2579 |           0.2307 |
[32m[20221213 15:16:05 @agent_ppo2.py:185][0m |          -0.0122 |          13.2817 |           0.2305 |
[32m[20221213 15:16:05 @agent_ppo2.py:185][0m |          -0.0156 |          13.0963 |           0.2308 |
[32m[20221213 15:16:05 @agent_ppo2.py:185][0m |          -0.0150 |          13.0144 |           0.2304 |
[32m[20221213 15:16:05 @agent_ppo2.py:185][0m |          -0.0102 |          13.4610 |           0.2305 |
[32m[20221213 15:16:05 @agent_ppo2.py:185][0m |          -0.0189 |          12.9217 |           0.2304 |
[32m[20221213 15:16:05 @agent_ppo2.py:185][0m |          -0.0153 |          12.9435 |           0.2304 |
[32m[20221213 15:16:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.85
[32m[20221213 15:16:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.93
[32m[20221213 15:16:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.21
[32m[20221213 15:16:05 @agent_ppo2.py:143][0m Total time:      23.17 min
[32m[20221213 15:16:05 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221213 15:16:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221213 15:16:05 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:16:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |           0.0180 |          17.7379 |           0.2236 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |           0.0066 |          15.9317 |           0.2233 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0033 |          14.9115 |           0.2234 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0105 |          14.2880 |           0.2233 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0138 |          14.2113 |           0.2233 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0155 |          14.1377 |           0.2233 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0171 |          14.0450 |           0.2234 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0173 |          13.9474 |           0.2231 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0182 |          13.8826 |           0.2232 |
[32m[20221213 15:16:06 @agent_ppo2.py:185][0m |          -0.0156 |          13.8920 |           0.2234 |
[32m[20221213 15:16:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:16:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 202.50
[32m[20221213 15:16:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 229.08
[32m[20221213 15:16:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.00
[32m[20221213 15:16:06 @agent_ppo2.py:143][0m Total time:      23.19 min
[32m[20221213 15:16:06 @agent_ppo2.py:145][0m 2091008 total steps have happened
[32m[20221213 15:16:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221213 15:16:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |           0.0125 |          16.7927 |           0.2325 |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |          -0.0069 |          14.7842 |           0.2319 |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |          -0.0109 |          14.5408 |           0.2316 |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |          -0.0084 |          14.4583 |           0.2314 |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |          -0.0137 |          14.3194 |           0.2317 |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |          -0.0154 |          14.2583 |           0.2316 |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |          -0.0144 |          14.1678 |           0.2314 |
[32m[20221213 15:16:07 @agent_ppo2.py:185][0m |          -0.0150 |          14.1125 |           0.2315 |
[32m[20221213 15:16:08 @agent_ppo2.py:185][0m |          -0.0150 |          14.0830 |           0.2314 |
[32m[20221213 15:16:08 @agent_ppo2.py:185][0m |          -0.0175 |          13.9722 |           0.2315 |
[32m[20221213 15:16:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.03
[32m[20221213 15:16:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.11
[32m[20221213 15:16:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.71
[32m[20221213 15:16:08 @agent_ppo2.py:143][0m Total time:      23.21 min
[32m[20221213 15:16:08 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221213 15:16:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221213 15:16:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:08 @agent_ppo2.py:185][0m |           0.0001 |          14.4366 |           0.2281 |
[32m[20221213 15:16:08 @agent_ppo2.py:185][0m |          -0.0076 |          14.2428 |           0.2282 |
[32m[20221213 15:16:08 @agent_ppo2.py:185][0m |          -0.0081 |          14.0962 |           0.2279 |
[32m[20221213 15:16:08 @agent_ppo2.py:185][0m |          -0.0003 |          15.0575 |           0.2280 |
[32m[20221213 15:16:09 @agent_ppo2.py:185][0m |          -0.0133 |          13.9832 |           0.2281 |
[32m[20221213 15:16:09 @agent_ppo2.py:185][0m |          -0.0125 |          13.8880 |           0.2281 |
[32m[20221213 15:16:09 @agent_ppo2.py:185][0m |          -0.0145 |          13.8750 |           0.2282 |
[32m[20221213 15:16:09 @agent_ppo2.py:185][0m |          -0.0145 |          13.7982 |           0.2282 |
[32m[20221213 15:16:09 @agent_ppo2.py:185][0m |          -0.0153 |          13.7619 |           0.2283 |
[32m[20221213 15:16:09 @agent_ppo2.py:185][0m |          -0.0166 |          13.7241 |           0.2284 |
[32m[20221213 15:16:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:16:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.92
[32m[20221213 15:16:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.26
[32m[20221213 15:16:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.62
[32m[20221213 15:16:09 @agent_ppo2.py:143][0m Total time:      23.23 min
[32m[20221213 15:16:09 @agent_ppo2.py:145][0m 2095104 total steps have happened
[32m[20221213 15:16:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221213 15:16:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |           0.0101 |          16.0189 |           0.2290 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0059 |          14.2973 |           0.2280 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0041 |          14.3273 |           0.2284 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0082 |          14.1357 |           0.2283 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0130 |          14.0419 |           0.2283 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0146 |          13.9922 |           0.2283 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0145 |          13.9448 |           0.2281 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0089 |          14.3918 |           0.2278 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0115 |          14.0611 |           0.2277 |
[32m[20221213 15:16:10 @agent_ppo2.py:185][0m |          -0.0173 |          13.8106 |           0.2276 |
[32m[20221213 15:16:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:16:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 213.00
[32m[20221213 15:16:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 229.24
[32m[20221213 15:16:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.09
[32m[20221213 15:16:11 @agent_ppo2.py:143][0m Total time:      23.26 min
[32m[20221213 15:16:11 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221213 15:16:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221213 15:16:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:11 @agent_ppo2.py:185][0m |           0.0015 |          14.2662 |           0.2307 |
[32m[20221213 15:16:11 @agent_ppo2.py:185][0m |          -0.0068 |          14.0327 |           0.2311 |
[32m[20221213 15:16:11 @agent_ppo2.py:185][0m |           0.0000 |          14.4490 |           0.2314 |
[32m[20221213 15:16:11 @agent_ppo2.py:185][0m |          -0.0099 |          13.7977 |           0.2314 |
[32m[20221213 15:16:11 @agent_ppo2.py:185][0m |          -0.0119 |          13.7113 |           0.2312 |
[32m[20221213 15:16:11 @agent_ppo2.py:185][0m |          -0.0112 |          13.6132 |           0.2313 |
[32m[20221213 15:16:12 @agent_ppo2.py:185][0m |          -0.0129 |          13.5804 |           0.2317 |
[32m[20221213 15:16:12 @agent_ppo2.py:185][0m |          -0.0152 |          13.5132 |           0.2315 |
[32m[20221213 15:16:12 @agent_ppo2.py:185][0m |          -0.0152 |          13.4813 |           0.2318 |
[32m[20221213 15:16:12 @agent_ppo2.py:185][0m |          -0.0091 |          14.0164 |           0.2320 |
[32m[20221213 15:16:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:16:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.49
[32m[20221213 15:16:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.75
[32m[20221213 15:16:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.08
[32m[20221213 15:16:12 @agent_ppo2.py:143][0m Total time:      23.28 min
[32m[20221213 15:16:12 @agent_ppo2.py:145][0m 2099200 total steps have happened
[32m[20221213 15:16:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221213 15:16:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:12 @agent_ppo2.py:185][0m |          -0.0026 |          14.7234 |           0.2383 |
[32m[20221213 15:16:12 @agent_ppo2.py:185][0m |          -0.0055 |          14.4771 |           0.2377 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0084 |          14.3248 |           0.2376 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0073 |          14.3111 |           0.2379 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0115 |          14.0268 |           0.2380 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0128 |          13.9208 |           0.2377 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0137 |          13.8481 |           0.2377 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0135 |          13.7015 |           0.2380 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0088 |          14.0129 |           0.2380 |
[32m[20221213 15:16:13 @agent_ppo2.py:185][0m |          -0.0106 |          13.9478 |           0.2379 |
[32m[20221213 15:16:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.29
[32m[20221213 15:16:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.95
[32m[20221213 15:16:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 234.30
[32m[20221213 15:16:13 @agent_ppo2.py:143][0m Total time:      23.30 min
[32m[20221213 15:16:13 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221213 15:16:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221213 15:16:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0011 |          14.8254 |           0.2324 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |           0.0010 |          15.4123 |           0.2325 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0041 |          14.6010 |           0.2322 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0117 |          14.0480 |           0.2322 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0098 |          13.9428 |           0.2324 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0159 |          13.8360 |           0.2322 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0145 |          13.7149 |           0.2320 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0087 |          13.9060 |           0.2320 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0146 |          13.5130 |           0.2319 |
[32m[20221213 15:16:14 @agent_ppo2.py:185][0m |          -0.0173 |          13.4698 |           0.2318 |
[32m[20221213 15:16:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.91
[32m[20221213 15:16:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 222.03
[32m[20221213 15:16:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.22
[32m[20221213 15:16:15 @agent_ppo2.py:143][0m Total time:      23.32 min
[32m[20221213 15:16:15 @agent_ppo2.py:145][0m 2103296 total steps have happened
[32m[20221213 15:16:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221213 15:16:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:15 @agent_ppo2.py:185][0m |          -0.0009 |          15.1305 |           0.2363 |
[32m[20221213 15:16:15 @agent_ppo2.py:185][0m |          -0.0054 |          14.7891 |           0.2356 |
[32m[20221213 15:16:15 @agent_ppo2.py:185][0m |          -0.0095 |          14.6215 |           0.2356 |
[32m[20221213 15:16:15 @agent_ppo2.py:185][0m |          -0.0099 |          14.5289 |           0.2352 |
[32m[20221213 15:16:15 @agent_ppo2.py:185][0m |          -0.0126 |          14.4342 |           0.2353 |
[32m[20221213 15:16:15 @agent_ppo2.py:185][0m |          -0.0130 |          14.4093 |           0.2351 |
[32m[20221213 15:16:16 @agent_ppo2.py:185][0m |          -0.0123 |          14.3329 |           0.2349 |
[32m[20221213 15:16:16 @agent_ppo2.py:185][0m |          -0.0139 |          14.3079 |           0.2349 |
[32m[20221213 15:16:16 @agent_ppo2.py:185][0m |          -0.0151 |          14.2768 |           0.2347 |
[32m[20221213 15:16:16 @agent_ppo2.py:185][0m |          -0.0177 |          14.2426 |           0.2347 |
[32m[20221213 15:16:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.16
[32m[20221213 15:16:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.85
[32m[20221213 15:16:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.95
[32m[20221213 15:16:16 @agent_ppo2.py:143][0m Total time:      23.35 min
[32m[20221213 15:16:16 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221213 15:16:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221213 15:16:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:16 @agent_ppo2.py:185][0m |           0.0011 |          14.9039 |           0.2321 |
[32m[20221213 15:16:16 @agent_ppo2.py:185][0m |          -0.0070 |          14.5123 |           0.2320 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0108 |          14.2519 |           0.2319 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0120 |          14.0547 |           0.2318 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0123 |          13.9249 |           0.2319 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0148 |          13.7926 |           0.2316 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0116 |          13.7784 |           0.2316 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0176 |          13.5512 |           0.2316 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0158 |          13.4620 |           0.2316 |
[32m[20221213 15:16:17 @agent_ppo2.py:185][0m |          -0.0184 |          13.3523 |           0.2315 |
[32m[20221213 15:16:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:16:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.49
[32m[20221213 15:16:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.01
[32m[20221213 15:16:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.12
[32m[20221213 15:16:17 @agent_ppo2.py:143][0m Total time:      23.37 min
[32m[20221213 15:16:17 @agent_ppo2.py:145][0m 2107392 total steps have happened
[32m[20221213 15:16:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221213 15:16:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |           0.0032 |          14.3693 |           0.2424 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0047 |          13.8237 |           0.2420 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0039 |          13.5878 |           0.2419 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0091 |          13.3397 |           0.2416 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0125 |          13.1614 |           0.2417 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0104 |          13.1321 |           0.2415 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0156 |          12.9574 |           0.2415 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0154 |          12.8702 |           0.2413 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0104 |          12.9368 |           0.2413 |
[32m[20221213 15:16:18 @agent_ppo2.py:185][0m |          -0.0187 |          12.6859 |           0.2409 |
[32m[20221213 15:16:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.19
[32m[20221213 15:16:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.33
[32m[20221213 15:16:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.51
[32m[20221213 15:16:19 @agent_ppo2.py:143][0m Total time:      23.39 min
[32m[20221213 15:16:19 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221213 15:16:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221213 15:16:19 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:16:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:19 @agent_ppo2.py:185][0m |           0.0012 |          15.0562 |           0.2326 |
[32m[20221213 15:16:19 @agent_ppo2.py:185][0m |          -0.0084 |          14.5079 |           0.2322 |
[32m[20221213 15:16:19 @agent_ppo2.py:185][0m |          -0.0049 |          14.2394 |           0.2321 |
[32m[20221213 15:16:19 @agent_ppo2.py:185][0m |          -0.0119 |          14.0403 |           0.2322 |
[32m[20221213 15:16:19 @agent_ppo2.py:185][0m |          -0.0117 |          13.9648 |           0.2324 |
[32m[20221213 15:16:20 @agent_ppo2.py:185][0m |          -0.0118 |          13.7989 |           0.2324 |
[32m[20221213 15:16:20 @agent_ppo2.py:185][0m |          -0.0162 |          13.6076 |           0.2323 |
[32m[20221213 15:16:20 @agent_ppo2.py:185][0m |          -0.0175 |          13.4844 |           0.2324 |
[32m[20221213 15:16:20 @agent_ppo2.py:185][0m |          -0.0131 |          13.9183 |           0.2323 |
[32m[20221213 15:16:20 @agent_ppo2.py:185][0m |          -0.0179 |          13.3206 |           0.2314 |
[32m[20221213 15:16:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:16:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.82
[32m[20221213 15:16:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 226.21
[32m[20221213 15:16:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.73
[32m[20221213 15:16:20 @agent_ppo2.py:143][0m Total time:      23.41 min
[32m[20221213 15:16:20 @agent_ppo2.py:145][0m 2111488 total steps have happened
[32m[20221213 15:16:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221213 15:16:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:20 @agent_ppo2.py:185][0m |          -0.0007 |          15.7360 |           0.2378 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0027 |          15.3429 |           0.2371 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0077 |          15.1230 |           0.2368 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0093 |          15.0706 |           0.2367 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0113 |          15.0214 |           0.2369 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0109 |          14.9505 |           0.2369 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0092 |          15.0761 |           0.2369 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0111 |          14.9228 |           0.2367 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0122 |          14.8549 |           0.2367 |
[32m[20221213 15:16:21 @agent_ppo2.py:185][0m |          -0.0086 |          15.0364 |           0.2366 |
[32m[20221213 15:16:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.95
[32m[20221213 15:16:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.53
[32m[20221213 15:16:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.26
[32m[20221213 15:16:21 @agent_ppo2.py:143][0m Total time:      23.44 min
[32m[20221213 15:16:21 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221213 15:16:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221213 15:16:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |           0.0035 |          16.1239 |           0.2359 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0066 |          15.1110 |           0.2357 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0104 |          14.9691 |           0.2356 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0098 |          14.9945 |           0.2355 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0112 |          14.8290 |           0.2352 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0125 |          14.7297 |           0.2353 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0090 |          14.9961 |           0.2352 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0146 |          14.6709 |           0.2352 |
[32m[20221213 15:16:22 @agent_ppo2.py:185][0m |          -0.0151 |          14.6207 |           0.2351 |
[32m[20221213 15:16:23 @agent_ppo2.py:185][0m |          -0.0154 |          14.5802 |           0.2351 |
[32m[20221213 15:16:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:16:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.09
[32m[20221213 15:16:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.27
[32m[20221213 15:16:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.81
[32m[20221213 15:16:23 @agent_ppo2.py:143][0m Total time:      23.46 min
[32m[20221213 15:16:23 @agent_ppo2.py:145][0m 2115584 total steps have happened
[32m[20221213 15:16:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221213 15:16:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:23 @agent_ppo2.py:185][0m |          -0.0024 |          14.7735 |           0.2244 |
[32m[20221213 15:16:23 @agent_ppo2.py:185][0m |          -0.0055 |          14.7013 |           0.2242 |
[32m[20221213 15:16:23 @agent_ppo2.py:185][0m |          -0.0089 |          14.1469 |           0.2239 |
[32m[20221213 15:16:23 @agent_ppo2.py:185][0m |          -0.0149 |          13.9055 |           0.2240 |
[32m[20221213 15:16:23 @agent_ppo2.py:185][0m |          -0.0153 |          13.7193 |           0.2240 |
[32m[20221213 15:16:24 @agent_ppo2.py:185][0m |          -0.0199 |          13.5574 |           0.2240 |
[32m[20221213 15:16:24 @agent_ppo2.py:185][0m |          -0.0187 |          13.3756 |           0.2239 |
[32m[20221213 15:16:24 @agent_ppo2.py:185][0m |          -0.0129 |          13.3527 |           0.2239 |
[32m[20221213 15:16:24 @agent_ppo2.py:185][0m |          -0.0181 |          13.1590 |           0.2239 |
[32m[20221213 15:16:24 @agent_ppo2.py:185][0m |          -0.0217 |          12.9883 |           0.2239 |
[32m[20221213 15:16:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:16:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 210.89
[32m[20221213 15:16:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 223.12
[32m[20221213 15:16:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.16
[32m[20221213 15:16:24 @agent_ppo2.py:143][0m Total time:      23.48 min
[32m[20221213 15:16:24 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221213 15:16:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221213 15:16:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:24 @agent_ppo2.py:185][0m |          -0.0023 |          14.8970 |           0.2352 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0098 |          14.4776 |           0.2343 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0096 |          14.1833 |           0.2343 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0119 |          13.9687 |           0.2342 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0138 |          13.8005 |           0.2341 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0137 |          13.7318 |           0.2341 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0162 |          13.6001 |           0.2338 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0188 |          13.4777 |           0.2338 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0182 |          13.4093 |           0.2339 |
[32m[20221213 15:16:25 @agent_ppo2.py:185][0m |          -0.0161 |          13.3116 |           0.2335 |
[32m[20221213 15:16:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.53
[32m[20221213 15:16:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.82
[32m[20221213 15:16:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.48
[32m[20221213 15:16:25 @agent_ppo2.py:143][0m Total time:      23.50 min
[32m[20221213 15:16:25 @agent_ppo2.py:145][0m 2119680 total steps have happened
[32m[20221213 15:16:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221213 15:16:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |           0.0017 |          14.6766 |           0.2351 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0073 |          14.3943 |           0.2349 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0112 |          14.2605 |           0.2347 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0108 |          14.1688 |           0.2345 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0132 |          14.0966 |           0.2343 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0093 |          14.0374 |           0.2342 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0125 |          14.0098 |           0.2342 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0157 |          13.9638 |           0.2340 |
[32m[20221213 15:16:26 @agent_ppo2.py:185][0m |          -0.0117 |          14.0133 |           0.2339 |
[32m[20221213 15:16:27 @agent_ppo2.py:185][0m |          -0.0158 |          13.8967 |           0.2339 |
[32m[20221213 15:16:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.06
[32m[20221213 15:16:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.91
[32m[20221213 15:16:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.42
[32m[20221213 15:16:27 @agent_ppo2.py:143][0m Total time:      23.53 min
[32m[20221213 15:16:27 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221213 15:16:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221213 15:16:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:27 @agent_ppo2.py:185][0m |           0.0010 |          14.7738 |           0.2279 |
[32m[20221213 15:16:27 @agent_ppo2.py:185][0m |          -0.0095 |          14.3197 |           0.2280 |
[32m[20221213 15:16:27 @agent_ppo2.py:185][0m |          -0.0114 |          14.0783 |           0.2279 |
[32m[20221213 15:16:27 @agent_ppo2.py:185][0m |          -0.0134 |          13.8981 |           0.2277 |
[32m[20221213 15:16:27 @agent_ppo2.py:185][0m |          -0.0124 |          13.7909 |           0.2275 |
[32m[20221213 15:16:28 @agent_ppo2.py:185][0m |          -0.0163 |          13.6759 |           0.2277 |
[32m[20221213 15:16:28 @agent_ppo2.py:185][0m |          -0.0179 |          13.5561 |           0.2275 |
[32m[20221213 15:16:28 @agent_ppo2.py:185][0m |          -0.0144 |          13.4295 |           0.2277 |
[32m[20221213 15:16:28 @agent_ppo2.py:185][0m |          -0.0186 |          13.3725 |           0.2276 |
[32m[20221213 15:16:28 @agent_ppo2.py:185][0m |          -0.0188 |          13.2673 |           0.2274 |
[32m[20221213 15:16:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 209.80
[32m[20221213 15:16:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 221.73
[32m[20221213 15:16:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.13
[32m[20221213 15:16:28 @agent_ppo2.py:143][0m Total time:      23.55 min
[32m[20221213 15:16:28 @agent_ppo2.py:145][0m 2123776 total steps have happened
[32m[20221213 15:16:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221213 15:16:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:28 @agent_ppo2.py:185][0m |          -0.0021 |          14.3796 |           0.2368 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0089 |          13.7234 |           0.2363 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0082 |          13.3427 |           0.2368 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |           0.0023 |          13.7204 |           0.2366 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0139 |          12.9505 |           0.2363 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0119 |          12.8063 |           0.2365 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0163 |          12.6304 |           0.2365 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0158 |          12.5242 |           0.2365 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0180 |          12.4246 |           0.2364 |
[32m[20221213 15:16:29 @agent_ppo2.py:185][0m |          -0.0182 |          12.3648 |           0.2363 |
[32m[20221213 15:16:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.81
[32m[20221213 15:16:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.57
[32m[20221213 15:16:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.02
[32m[20221213 15:16:29 @agent_ppo2.py:143][0m Total time:      23.57 min
[32m[20221213 15:16:29 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221213 15:16:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1038 --------------------------#
[32m[20221213 15:16:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |           0.0028 |          13.8336 |           0.2319 |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |          -0.0002 |          13.4227 |           0.2319 |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |          -0.0082 |          12.7757 |           0.2320 |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |          -0.0052 |          12.6081 |           0.2321 |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |          -0.0100 |          12.3458 |           0.2320 |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |          -0.0133 |          12.2272 |           0.2319 |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |          -0.0079 |          13.3889 |           0.2317 |
[32m[20221213 15:16:30 @agent_ppo2.py:185][0m |          -0.0059 |          12.9288 |           0.2315 |
[32m[20221213 15:16:31 @agent_ppo2.py:185][0m |          -0.0179 |          11.9376 |           0.2318 |
[32m[20221213 15:16:31 @agent_ppo2.py:185][0m |          -0.0156 |          11.8464 |           0.2317 |
[32m[20221213 15:16:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.61
[32m[20221213 15:16:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.64
[32m[20221213 15:16:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.47
[32m[20221213 15:16:31 @agent_ppo2.py:143][0m Total time:      23.59 min
[32m[20221213 15:16:31 @agent_ppo2.py:145][0m 2127872 total steps have happened
[32m[20221213 15:16:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1039 --------------------------#
[32m[20221213 15:16:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:31 @agent_ppo2.py:185][0m |           0.0090 |          17.1931 |           0.2348 |
[32m[20221213 15:16:31 @agent_ppo2.py:185][0m |          -0.0085 |          14.8551 |           0.2342 |
[32m[20221213 15:16:31 @agent_ppo2.py:185][0m |          -0.0101 |          14.5393 |           0.2343 |
[32m[20221213 15:16:31 @agent_ppo2.py:185][0m |          -0.0093 |          14.4045 |           0.2345 |
[32m[20221213 15:16:32 @agent_ppo2.py:185][0m |          -0.0129 |          14.0485 |           0.2345 |
[32m[20221213 15:16:32 @agent_ppo2.py:185][0m |          -0.0145 |          13.9261 |           0.2344 |
[32m[20221213 15:16:32 @agent_ppo2.py:185][0m |          -0.0137 |          13.7582 |           0.2346 |
[32m[20221213 15:16:32 @agent_ppo2.py:185][0m |          -0.0143 |          13.6293 |           0.2347 |
[32m[20221213 15:16:32 @agent_ppo2.py:185][0m |          -0.0165 |          13.5362 |           0.2346 |
[32m[20221213 15:16:32 @agent_ppo2.py:185][0m |          -0.0148 |          13.4278 |           0.2348 |
[32m[20221213 15:16:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.39
[32m[20221213 15:16:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.78
[32m[20221213 15:16:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 69.17
[32m[20221213 15:16:32 @agent_ppo2.py:143][0m Total time:      23.62 min
[32m[20221213 15:16:32 @agent_ppo2.py:145][0m 2129920 total steps have happened
[32m[20221213 15:16:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1040 --------------------------#
[32m[20221213 15:16:32 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:16:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0024 |          16.0776 |           0.2335 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0097 |          15.5357 |           0.2329 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0120 |          15.3263 |           0.2331 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0141 |          15.1902 |           0.2329 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0112 |          15.2479 |           0.2327 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0029 |          16.9075 |           0.2325 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0115 |          14.9827 |           0.2323 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0147 |          14.8625 |           0.2324 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0162 |          14.8153 |           0.2323 |
[32m[20221213 15:16:33 @agent_ppo2.py:185][0m |          -0.0181 |          14.7468 |           0.2321 |
[32m[20221213 15:16:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:16:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.38
[32m[20221213 15:16:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.48
[32m[20221213 15:16:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.14
[32m[20221213 15:16:33 @agent_ppo2.py:143][0m Total time:      23.64 min
[32m[20221213 15:16:33 @agent_ppo2.py:145][0m 2131968 total steps have happened
[32m[20221213 15:16:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1041 --------------------------#
[32m[20221213 15:16:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |           0.0017 |          15.2742 |           0.2367 |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |          -0.0087 |          14.4111 |           0.2365 |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |          -0.0091 |          14.1713 |           0.2368 |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |          -0.0136 |          14.0441 |           0.2367 |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |          -0.0104 |          13.9994 |           0.2369 |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |          -0.0136 |          13.8271 |           0.2370 |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |          -0.0143 |          13.7632 |           0.2369 |
[32m[20221213 15:16:34 @agent_ppo2.py:185][0m |          -0.0138 |          13.7020 |           0.2369 |
[32m[20221213 15:16:35 @agent_ppo2.py:185][0m |          -0.0171 |          13.6537 |           0.2372 |
[32m[20221213 15:16:35 @agent_ppo2.py:185][0m |          -0.0047 |          14.5648 |           0.2368 |
[32m[20221213 15:16:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.88
[32m[20221213 15:16:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.16
[32m[20221213 15:16:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.79
[32m[20221213 15:16:35 @agent_ppo2.py:143][0m Total time:      23.66 min
[32m[20221213 15:16:35 @agent_ppo2.py:145][0m 2134016 total steps have happened
[32m[20221213 15:16:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1042 --------------------------#
[32m[20221213 15:16:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:35 @agent_ppo2.py:185][0m |          -0.0021 |          15.3689 |           0.2408 |
[32m[20221213 15:16:35 @agent_ppo2.py:185][0m |          -0.0066 |          14.6632 |           0.2400 |
[32m[20221213 15:16:35 @agent_ppo2.py:185][0m |          -0.0116 |          14.4340 |           0.2397 |
[32m[20221213 15:16:35 @agent_ppo2.py:185][0m |          -0.0127 |          14.2486 |           0.2395 |
[32m[20221213 15:16:36 @agent_ppo2.py:185][0m |          -0.0136 |          14.0900 |           0.2391 |
[32m[20221213 15:16:36 @agent_ppo2.py:185][0m |          -0.0138 |          14.0237 |           0.2389 |
[32m[20221213 15:16:36 @agent_ppo2.py:185][0m |          -0.0165 |          13.8931 |           0.2384 |
[32m[20221213 15:16:36 @agent_ppo2.py:185][0m |          -0.0159 |          13.8109 |           0.2383 |
[32m[20221213 15:16:36 @agent_ppo2.py:185][0m |          -0.0129 |          13.9358 |           0.2381 |
[32m[20221213 15:16:36 @agent_ppo2.py:185][0m |          -0.0176 |          13.6896 |           0.2378 |
[32m[20221213 15:16:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 208.95
[32m[20221213 15:16:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.10
[32m[20221213 15:16:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.86
[32m[20221213 15:16:36 @agent_ppo2.py:143][0m Total time:      23.68 min
[32m[20221213 15:16:36 @agent_ppo2.py:145][0m 2136064 total steps have happened
[32m[20221213 15:16:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1043 --------------------------#
[32m[20221213 15:16:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0018 |          14.5555 |           0.2321 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0045 |          14.1938 |           0.2314 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0090 |          13.8378 |           0.2316 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0049 |          13.9033 |           0.2313 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0104 |          13.3931 |           0.2312 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0148 |          13.2244 |           0.2312 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0164 |          13.0585 |           0.2309 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0153 |          12.8602 |           0.2309 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0166 |          12.7441 |           0.2308 |
[32m[20221213 15:16:37 @agent_ppo2.py:185][0m |          -0.0165 |          12.5965 |           0.2305 |
[32m[20221213 15:16:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:16:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.49
[32m[20221213 15:16:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.63
[32m[20221213 15:16:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 213.44
[32m[20221213 15:16:38 @agent_ppo2.py:143][0m Total time:      23.71 min
[32m[20221213 15:16:38 @agent_ppo2.py:145][0m 2138112 total steps have happened
[32m[20221213 15:16:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1044 --------------------------#
[32m[20221213 15:16:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |           0.0017 |          16.0114 |           0.2351 |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |           0.0045 |          16.7341 |           0.2354 |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |          -0.0074 |          15.4255 |           0.2353 |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |          -0.0096 |          15.3321 |           0.2353 |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |          -0.0080 |          15.2082 |           0.2355 |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |          -0.0120 |          15.1807 |           0.2353 |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |          -0.0110 |          15.1107 |           0.2355 |
[32m[20221213 15:16:38 @agent_ppo2.py:185][0m |          -0.0055 |          15.4822 |           0.2354 |
[32m[20221213 15:16:39 @agent_ppo2.py:185][0m |          -0.0150 |          15.0352 |           0.2355 |
[32m[20221213 15:16:39 @agent_ppo2.py:185][0m |          -0.0149 |          14.9660 |           0.2356 |
[32m[20221213 15:16:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.44
[32m[20221213 15:16:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.92
[32m[20221213 15:16:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.48
[32m[20221213 15:16:39 @agent_ppo2.py:143][0m Total time:      23.73 min
[32m[20221213 15:16:39 @agent_ppo2.py:145][0m 2140160 total steps have happened
[32m[20221213 15:16:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1045 --------------------------#
[32m[20221213 15:16:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:39 @agent_ppo2.py:185][0m |           0.0079 |          16.7539 |           0.2282 |
[32m[20221213 15:16:39 @agent_ppo2.py:185][0m |          -0.0081 |          14.7370 |           0.2276 |
[32m[20221213 15:16:39 @agent_ppo2.py:185][0m |          -0.0096 |          14.5867 |           0.2281 |
[32m[20221213 15:16:39 @agent_ppo2.py:185][0m |          -0.0127 |          14.5101 |           0.2282 |
[32m[20221213 15:16:40 @agent_ppo2.py:185][0m |          -0.0116 |          14.4594 |           0.2283 |
[32m[20221213 15:16:40 @agent_ppo2.py:185][0m |          -0.0145 |          14.4432 |           0.2284 |
[32m[20221213 15:16:40 @agent_ppo2.py:185][0m |          -0.0135 |          14.3793 |           0.2283 |
[32m[20221213 15:16:40 @agent_ppo2.py:185][0m |          -0.0146 |          14.3563 |           0.2286 |
[32m[20221213 15:16:40 @agent_ppo2.py:185][0m |          -0.0174 |          14.2708 |           0.2288 |
[32m[20221213 15:16:40 @agent_ppo2.py:185][0m |          -0.0112 |          14.5113 |           0.2289 |
[32m[20221213 15:16:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.15
[32m[20221213 15:16:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 233.19
[32m[20221213 15:16:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.18
[32m[20221213 15:16:40 @agent_ppo2.py:143][0m Total time:      23.75 min
[32m[20221213 15:16:40 @agent_ppo2.py:145][0m 2142208 total steps have happened
[32m[20221213 15:16:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1046 --------------------------#
[32m[20221213 15:16:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |           0.0023 |          15.1385 |           0.2388 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0054 |          14.7136 |           0.2379 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0056 |          14.5421 |           0.2381 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0106 |          14.2999 |           0.2380 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0071 |          14.2393 |           0.2381 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0167 |          14.0113 |           0.2381 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0154 |          13.8911 |           0.2381 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |           0.0036 |          15.7160 |           0.2380 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0111 |          13.7845 |           0.2376 |
[32m[20221213 15:16:41 @agent_ppo2.py:185][0m |          -0.0180 |          13.5203 |           0.2379 |
[32m[20221213 15:16:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.64
[32m[20221213 15:16:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.58
[32m[20221213 15:16:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.31
[32m[20221213 15:16:42 @agent_ppo2.py:143][0m Total time:      23.77 min
[32m[20221213 15:16:42 @agent_ppo2.py:145][0m 2144256 total steps have happened
[32m[20221213 15:16:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1047 --------------------------#
[32m[20221213 15:16:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:42 @agent_ppo2.py:185][0m |          -0.0021 |          15.8642 |           0.2336 |
[32m[20221213 15:16:42 @agent_ppo2.py:185][0m |          -0.0069 |          15.5170 |           0.2332 |
[32m[20221213 15:16:42 @agent_ppo2.py:185][0m |          -0.0089 |          15.3700 |           0.2331 |
[32m[20221213 15:16:42 @agent_ppo2.py:185][0m |          -0.0113 |          15.2709 |           0.2332 |
[32m[20221213 15:16:42 @agent_ppo2.py:185][0m |          -0.0114 |          15.2358 |           0.2332 |
[32m[20221213 15:16:42 @agent_ppo2.py:185][0m |          -0.0124 |          15.1194 |           0.2330 |
[32m[20221213 15:16:42 @agent_ppo2.py:185][0m |          -0.0121 |          15.0576 |           0.2327 |
[32m[20221213 15:16:43 @agent_ppo2.py:185][0m |          -0.0153 |          15.0355 |           0.2327 |
[32m[20221213 15:16:43 @agent_ppo2.py:185][0m |          -0.0142 |          15.0071 |           0.2327 |
[32m[20221213 15:16:43 @agent_ppo2.py:185][0m |          -0.0194 |          14.9176 |           0.2326 |
[32m[20221213 15:16:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.56
[32m[20221213 15:16:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 248.67
[32m[20221213 15:16:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.35
[32m[20221213 15:16:43 @agent_ppo2.py:143][0m Total time:      23.80 min
[32m[20221213 15:16:43 @agent_ppo2.py:145][0m 2146304 total steps have happened
[32m[20221213 15:16:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1048 --------------------------#
[32m[20221213 15:16:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:43 @agent_ppo2.py:185][0m |          -0.0016 |          15.1288 |           0.2353 |
[32m[20221213 15:16:43 @agent_ppo2.py:185][0m |          -0.0095 |          14.7411 |           0.2344 |
[32m[20221213 15:16:43 @agent_ppo2.py:185][0m |          -0.0110 |          14.5064 |           0.2342 |
[32m[20221213 15:16:44 @agent_ppo2.py:185][0m |          -0.0049 |          14.6286 |           0.2337 |
[32m[20221213 15:16:44 @agent_ppo2.py:185][0m |          -0.0125 |          14.1157 |           0.2337 |
[32m[20221213 15:16:44 @agent_ppo2.py:185][0m |          -0.0153 |          13.9634 |           0.2335 |
[32m[20221213 15:16:44 @agent_ppo2.py:185][0m |          -0.0143 |          13.8034 |           0.2335 |
[32m[20221213 15:16:44 @agent_ppo2.py:185][0m |          -0.0159 |          13.6879 |           0.2333 |
[32m[20221213 15:16:44 @agent_ppo2.py:185][0m |          -0.0129 |          13.6479 |           0.2332 |
[32m[20221213 15:16:44 @agent_ppo2.py:185][0m |          -0.0165 |          13.4777 |           0.2330 |
[32m[20221213 15:16:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.27
[32m[20221213 15:16:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.11
[32m[20221213 15:16:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.14
[32m[20221213 15:16:44 @agent_ppo2.py:143][0m Total time:      23.82 min
[32m[20221213 15:16:44 @agent_ppo2.py:145][0m 2148352 total steps have happened
[32m[20221213 15:16:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1049 --------------------------#
[32m[20221213 15:16:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |           0.0005 |          15.2741 |           0.2330 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0050 |          14.6877 |           0.2326 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0039 |          14.5690 |           0.2324 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0133 |          14.1327 |           0.2322 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0149 |          13.8765 |           0.2322 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0169 |          13.7074 |           0.2320 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0077 |          14.6961 |           0.2323 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0174 |          13.3458 |           0.2321 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0051 |          14.6830 |           0.2320 |
[32m[20221213 15:16:45 @agent_ppo2.py:185][0m |          -0.0119 |          13.6332 |           0.2317 |
[32m[20221213 15:16:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:16:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.36
[32m[20221213 15:16:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.29
[32m[20221213 15:16:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.96
[32m[20221213 15:16:46 @agent_ppo2.py:143][0m Total time:      23.84 min
[32m[20221213 15:16:46 @agent_ppo2.py:145][0m 2150400 total steps have happened
[32m[20221213 15:16:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1050 --------------------------#
[32m[20221213 15:16:46 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:16:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:46 @agent_ppo2.py:185][0m |          -0.0005 |          15.8950 |           0.2337 |
[32m[20221213 15:16:46 @agent_ppo2.py:185][0m |          -0.0087 |          15.0966 |           0.2339 |
[32m[20221213 15:16:46 @agent_ppo2.py:185][0m |          -0.0118 |          14.6603 |           0.2336 |
[32m[20221213 15:16:46 @agent_ppo2.py:185][0m |          -0.0068 |          14.5569 |           0.2338 |
[32m[20221213 15:16:46 @agent_ppo2.py:185][0m |          -0.0100 |          14.3336 |           0.2335 |
[32m[20221213 15:16:46 @agent_ppo2.py:185][0m |          -0.0135 |          13.9876 |           0.2338 |
[32m[20221213 15:16:46 @agent_ppo2.py:185][0m |          -0.0199 |          13.7905 |           0.2337 |
[32m[20221213 15:16:47 @agent_ppo2.py:185][0m |          -0.0105 |          14.8462 |           0.2333 |
[32m[20221213 15:16:47 @agent_ppo2.py:185][0m |          -0.0209 |          13.4882 |           0.2337 |
[32m[20221213 15:16:47 @agent_ppo2.py:185][0m |          -0.0207 |          13.3920 |           0.2336 |
[32m[20221213 15:16:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 201.14
[32m[20221213 15:16:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.86
[32m[20221213 15:16:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.58
[32m[20221213 15:16:47 @agent_ppo2.py:143][0m Total time:      23.86 min
[32m[20221213 15:16:47 @agent_ppo2.py:145][0m 2152448 total steps have happened
[32m[20221213 15:16:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1051 --------------------------#
[32m[20221213 15:16:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:47 @agent_ppo2.py:185][0m |          -0.0028 |          16.5650 |           0.2297 |
[32m[20221213 15:16:47 @agent_ppo2.py:185][0m |          -0.0036 |          16.2177 |           0.2295 |
[32m[20221213 15:16:47 @agent_ppo2.py:185][0m |          -0.0089 |          15.8299 |           0.2293 |
[32m[20221213 15:16:48 @agent_ppo2.py:185][0m |          -0.0027 |          16.4970 |           0.2292 |
[32m[20221213 15:16:48 @agent_ppo2.py:185][0m |          -0.0085 |          15.8664 |           0.2287 |
[32m[20221213 15:16:48 @agent_ppo2.py:185][0m |          -0.0116 |          15.3696 |           0.2287 |
[32m[20221213 15:16:48 @agent_ppo2.py:185][0m |          -0.0152 |          15.0310 |           0.2293 |
[32m[20221213 15:16:48 @agent_ppo2.py:185][0m |          -0.0161 |          14.9221 |           0.2291 |
[32m[20221213 15:16:48 @agent_ppo2.py:185][0m |          -0.0192 |          14.8370 |           0.2288 |
[32m[20221213 15:16:48 @agent_ppo2.py:185][0m |          -0.0090 |          15.4663 |           0.2290 |
[32m[20221213 15:16:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:16:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 216.04
[32m[20221213 15:16:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.24
[32m[20221213 15:16:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.39
[32m[20221213 15:16:48 @agent_ppo2.py:143][0m Total time:      23.88 min
[32m[20221213 15:16:48 @agent_ppo2.py:145][0m 2154496 total steps have happened
[32m[20221213 15:16:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1052 --------------------------#
[32m[20221213 15:16:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |           0.0006 |          16.2196 |           0.2286 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |           0.0055 |          17.5340 |           0.2283 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0066 |          15.5434 |           0.2285 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0108 |          15.4230 |           0.2282 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0104 |          15.2570 |           0.2280 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0123 |          15.1348 |           0.2281 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0124 |          15.0954 |           0.2279 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0098 |          15.1430 |           0.2280 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0088 |          15.7174 |           0.2277 |
[32m[20221213 15:16:49 @agent_ppo2.py:185][0m |          -0.0133 |          14.9198 |           0.2276 |
[32m[20221213 15:16:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.19
[32m[20221213 15:16:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 248.97
[32m[20221213 15:16:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.56
[32m[20221213 15:16:50 @agent_ppo2.py:143][0m Total time:      23.91 min
[32m[20221213 15:16:50 @agent_ppo2.py:145][0m 2156544 total steps have happened
[32m[20221213 15:16:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1053 --------------------------#
[32m[20221213 15:16:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:50 @agent_ppo2.py:185][0m |          -0.0039 |          16.2135 |           0.2324 |
[32m[20221213 15:16:50 @agent_ppo2.py:185][0m |          -0.0024 |          16.0509 |           0.2317 |
[32m[20221213 15:16:50 @agent_ppo2.py:185][0m |           0.0076 |          17.4298 |           0.2317 |
[32m[20221213 15:16:50 @agent_ppo2.py:185][0m |          -0.0136 |          15.3843 |           0.2317 |
[32m[20221213 15:16:50 @agent_ppo2.py:185][0m |          -0.0136 |          15.1850 |           0.2315 |
[32m[20221213 15:16:50 @agent_ppo2.py:185][0m |          -0.0110 |          15.1229 |           0.2316 |
[32m[20221213 15:16:50 @agent_ppo2.py:185][0m |          -0.0165 |          14.8939 |           0.2314 |
[32m[20221213 15:16:51 @agent_ppo2.py:185][0m |          -0.0189 |          14.7563 |           0.2317 |
[32m[20221213 15:16:51 @agent_ppo2.py:185][0m |          -0.0166 |          14.6586 |           0.2314 |
[32m[20221213 15:16:51 @agent_ppo2.py:185][0m |          -0.0179 |          14.5782 |           0.2314 |
[32m[20221213 15:16:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:16:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.58
[32m[20221213 15:16:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 222.83
[32m[20221213 15:16:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.73
[32m[20221213 15:16:51 @agent_ppo2.py:143][0m Total time:      23.93 min
[32m[20221213 15:16:51 @agent_ppo2.py:145][0m 2158592 total steps have happened
[32m[20221213 15:16:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1054 --------------------------#
[32m[20221213 15:16:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:51 @agent_ppo2.py:185][0m |          -0.0004 |          15.1472 |           0.2326 |
[32m[20221213 15:16:51 @agent_ppo2.py:185][0m |          -0.0068 |          14.5527 |           0.2326 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0070 |          14.2057 |           0.2324 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0079 |          14.0373 |           0.2320 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0132 |          13.7608 |           0.2319 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0057 |          14.2328 |           0.2317 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0130 |          13.5120 |           0.2317 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0181 |          13.3607 |           0.2313 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0156 |          13.1936 |           0.2312 |
[32m[20221213 15:16:52 @agent_ppo2.py:185][0m |          -0.0158 |          13.0977 |           0.2310 |
[32m[20221213 15:16:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 215.22
[32m[20221213 15:16:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 223.93
[32m[20221213 15:16:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.96
[32m[20221213 15:16:52 @agent_ppo2.py:143][0m Total time:      23.95 min
[32m[20221213 15:16:52 @agent_ppo2.py:145][0m 2160640 total steps have happened
[32m[20221213 15:16:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1055 --------------------------#
[32m[20221213 15:16:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0056 |          16.4603 |           0.2279 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |           0.0046 |          17.1043 |           0.2276 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0124 |          15.8421 |           0.2278 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0123 |          15.7438 |           0.2278 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0141 |          15.6660 |           0.2281 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0165 |          15.5916 |           0.2280 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0171 |          15.5412 |           0.2281 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0193 |          15.5185 |           0.2283 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0094 |          17.2091 |           0.2281 |
[32m[20221213 15:16:53 @agent_ppo2.py:185][0m |          -0.0175 |          15.4670 |           0.2283 |
[32m[20221213 15:16:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.49
[32m[20221213 15:16:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.11
[32m[20221213 15:16:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.09
[32m[20221213 15:16:54 @agent_ppo2.py:143][0m Total time:      23.97 min
[32m[20221213 15:16:54 @agent_ppo2.py:145][0m 2162688 total steps have happened
[32m[20221213 15:16:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1056 --------------------------#
[32m[20221213 15:16:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:54 @agent_ppo2.py:185][0m |           0.0078 |          16.5768 |           0.2313 |
[32m[20221213 15:16:54 @agent_ppo2.py:185][0m |          -0.0022 |          15.0194 |           0.2311 |
[32m[20221213 15:16:54 @agent_ppo2.py:185][0m |          -0.0082 |          14.5342 |           0.2308 |
[32m[20221213 15:16:54 @agent_ppo2.py:185][0m |          -0.0107 |          14.3215 |           0.2308 |
[32m[20221213 15:16:54 @agent_ppo2.py:185][0m |          -0.0102 |          14.1054 |           0.2308 |
[32m[20221213 15:16:54 @agent_ppo2.py:185][0m |          -0.0090 |          14.1070 |           0.2304 |
[32m[20221213 15:16:55 @agent_ppo2.py:185][0m |           0.0026 |          14.5817 |           0.2303 |
[32m[20221213 15:16:55 @agent_ppo2.py:185][0m |          -0.0133 |          13.6366 |           0.2302 |
[32m[20221213 15:16:55 @agent_ppo2.py:185][0m |          -0.0170 |          13.4778 |           0.2301 |
[32m[20221213 15:16:55 @agent_ppo2.py:185][0m |          -0.0180 |          13.3368 |           0.2299 |
[32m[20221213 15:16:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 212.10
[32m[20221213 15:16:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 235.87
[32m[20221213 15:16:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.54
[32m[20221213 15:16:55 @agent_ppo2.py:143][0m Total time:      24.00 min
[32m[20221213 15:16:55 @agent_ppo2.py:145][0m 2164736 total steps have happened
[32m[20221213 15:16:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1057 --------------------------#
[32m[20221213 15:16:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:55 @agent_ppo2.py:185][0m |          -0.0031 |          14.5009 |           0.2247 |
[32m[20221213 15:16:55 @agent_ppo2.py:185][0m |          -0.0038 |          14.0645 |           0.2244 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0080 |          13.7309 |           0.2243 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0123 |          13.3843 |           0.2243 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0149 |          13.1624 |           0.2239 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0156 |          12.9393 |           0.2240 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0175 |          12.7978 |           0.2239 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0141 |          12.6884 |           0.2237 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0155 |          12.5189 |           0.2236 |
[32m[20221213 15:16:56 @agent_ppo2.py:185][0m |          -0.0188 |          12.3738 |           0.2236 |
[32m[20221213 15:16:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:16:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.06
[32m[20221213 15:16:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.21
[32m[20221213 15:16:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.51
[32m[20221213 15:16:56 @agent_ppo2.py:143][0m Total time:      24.02 min
[32m[20221213 15:16:56 @agent_ppo2.py:145][0m 2166784 total steps have happened
[32m[20221213 15:16:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1058 --------------------------#
[32m[20221213 15:16:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:16:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |           0.0006 |          15.8543 |           0.2262 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0077 |          14.9720 |           0.2257 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0126 |          14.4217 |           0.2254 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0107 |          14.0950 |           0.2253 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0129 |          13.8383 |           0.2250 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0147 |          13.6340 |           0.2247 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0162 |          13.4502 |           0.2246 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0182 |          13.2747 |           0.2243 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0180 |          13.1083 |           0.2241 |
[32m[20221213 15:16:57 @agent_ppo2.py:185][0m |          -0.0086 |          13.8692 |           0.2240 |
[32m[20221213 15:16:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:16:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.18
[32m[20221213 15:16:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.38
[32m[20221213 15:16:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.64
[32m[20221213 15:16:58 @agent_ppo2.py:143][0m Total time:      24.04 min
[32m[20221213 15:16:58 @agent_ppo2.py:145][0m 2168832 total steps have happened
[32m[20221213 15:16:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1059 --------------------------#
[32m[20221213 15:16:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:16:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:58 @agent_ppo2.py:185][0m |          -0.0016 |          16.9545 |           0.2248 |
[32m[20221213 15:16:58 @agent_ppo2.py:185][0m |          -0.0100 |          16.3005 |           0.2246 |
[32m[20221213 15:16:58 @agent_ppo2.py:185][0m |          -0.0044 |          16.3971 |           0.2247 |
[32m[20221213 15:16:58 @agent_ppo2.py:185][0m |          -0.0108 |          15.9557 |           0.2244 |
[32m[20221213 15:16:58 @agent_ppo2.py:185][0m |          -0.0167 |          15.8803 |           0.2244 |
[32m[20221213 15:16:58 @agent_ppo2.py:185][0m |          -0.0217 |          15.8135 |           0.2246 |
[32m[20221213 15:16:59 @agent_ppo2.py:185][0m |          -0.0093 |          16.4262 |           0.2245 |
[32m[20221213 15:16:59 @agent_ppo2.py:185][0m |          -0.0153 |          15.6009 |           0.2246 |
[32m[20221213 15:16:59 @agent_ppo2.py:185][0m |          -0.0144 |          15.6045 |           0.2248 |
[32m[20221213 15:16:59 @agent_ppo2.py:185][0m |          -0.0214 |          15.5257 |           0.2247 |
[32m[20221213 15:16:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:16:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 206.46
[32m[20221213 15:16:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 214.24
[32m[20221213 15:16:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.98
[32m[20221213 15:16:59 @agent_ppo2.py:143][0m Total time:      24.06 min
[32m[20221213 15:16:59 @agent_ppo2.py:145][0m 2170880 total steps have happened
[32m[20221213 15:16:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1060 --------------------------#
[32m[20221213 15:16:59 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:16:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:16:59 @agent_ppo2.py:185][0m |          -0.0008 |          15.7517 |           0.2269 |
[32m[20221213 15:16:59 @agent_ppo2.py:185][0m |          -0.0040 |          15.5417 |           0.2263 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0125 |          15.3871 |           0.2260 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0095 |          15.3118 |           0.2256 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0134 |          15.2065 |           0.2254 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0037 |          17.0488 |           0.2252 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0160 |          15.0600 |           0.2250 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0153 |          15.0293 |           0.2250 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0180 |          14.9688 |           0.2246 |
[32m[20221213 15:17:00 @agent_ppo2.py:185][0m |          -0.0162 |          14.9500 |           0.2247 |
[32m[20221213 15:17:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.58
[32m[20221213 15:17:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.06
[32m[20221213 15:17:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.04
[32m[20221213 15:17:00 @agent_ppo2.py:143][0m Total time:      24.09 min
[32m[20221213 15:17:00 @agent_ppo2.py:145][0m 2172928 total steps have happened
[32m[20221213 15:17:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1061 --------------------------#
[32m[20221213 15:17:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0031 |          15.4103 |           0.2210 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0061 |          15.0120 |           0.2211 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0133 |          14.7861 |           0.2210 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0121 |          14.5399 |           0.2210 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0128 |          14.3425 |           0.2210 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0163 |          14.2375 |           0.2210 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0194 |          14.1140 |           0.2210 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0125 |          14.1326 |           0.2210 |
[32m[20221213 15:17:01 @agent_ppo2.py:185][0m |          -0.0158 |          13.8885 |           0.2209 |
[32m[20221213 15:17:02 @agent_ppo2.py:185][0m |          -0.0193 |          13.8235 |           0.2212 |
[32m[20221213 15:17:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.83
[32m[20221213 15:17:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 217.99
[32m[20221213 15:17:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.17
[32m[20221213 15:17:02 @agent_ppo2.py:143][0m Total time:      24.11 min
[32m[20221213 15:17:02 @agent_ppo2.py:145][0m 2174976 total steps have happened
[32m[20221213 15:17:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1062 --------------------------#
[32m[20221213 15:17:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:02 @agent_ppo2.py:185][0m |          -0.0027 |          16.6360 |           0.2236 |
[32m[20221213 15:17:02 @agent_ppo2.py:185][0m |          -0.0090 |          16.0126 |           0.2234 |
[32m[20221213 15:17:02 @agent_ppo2.py:185][0m |          -0.0158 |          15.8594 |           0.2233 |
[32m[20221213 15:17:02 @agent_ppo2.py:185][0m |          -0.0010 |          17.4951 |           0.2233 |
[32m[20221213 15:17:02 @agent_ppo2.py:185][0m |          -0.0143 |          15.7565 |           0.2230 |
[32m[20221213 15:17:02 @agent_ppo2.py:185][0m |          -0.0059 |          16.6181 |           0.2231 |
[32m[20221213 15:17:03 @agent_ppo2.py:185][0m |          -0.0147 |          15.5559 |           0.2232 |
[32m[20221213 15:17:03 @agent_ppo2.py:185][0m |          -0.0152 |          15.5006 |           0.2230 |
[32m[20221213 15:17:03 @agent_ppo2.py:185][0m |          -0.0181 |          15.5735 |           0.2232 |
[32m[20221213 15:17:03 @agent_ppo2.py:185][0m |          -0.0182 |          15.4463 |           0.2231 |
[32m[20221213 15:17:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:17:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.97
[32m[20221213 15:17:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.32
[32m[20221213 15:17:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.94
[32m[20221213 15:17:03 @agent_ppo2.py:143][0m Total time:      24.13 min
[32m[20221213 15:17:03 @agent_ppo2.py:145][0m 2177024 total steps have happened
[32m[20221213 15:17:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1063 --------------------------#
[32m[20221213 15:17:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:03 @agent_ppo2.py:185][0m |           0.0034 |          15.6261 |           0.2211 |
[32m[20221213 15:17:03 @agent_ppo2.py:185][0m |          -0.0041 |          14.9000 |           0.2209 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0076 |          14.5523 |           0.2212 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0000 |          15.7156 |           0.2213 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0128 |          14.2202 |           0.2209 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0154 |          14.0316 |           0.2210 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0141 |          13.9234 |           0.2212 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0168 |          13.8566 |           0.2212 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0149 |          13.7858 |           0.2212 |
[32m[20221213 15:17:04 @agent_ppo2.py:185][0m |          -0.0176 |          13.6736 |           0.2211 |
[32m[20221213 15:17:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.93
[32m[20221213 15:17:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 228.98
[32m[20221213 15:17:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.98
[32m[20221213 15:17:04 @agent_ppo2.py:143][0m Total time:      24.15 min
[32m[20221213 15:17:04 @agent_ppo2.py:145][0m 2179072 total steps have happened
[32m[20221213 15:17:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1064 --------------------------#
[32m[20221213 15:17:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0017 |          14.5332 |           0.2251 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0087 |          13.8277 |           0.2250 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0111 |          13.6095 |           0.2249 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0112 |          13.5092 |           0.2248 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0129 |          13.3102 |           0.2250 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0170 |          13.2502 |           0.2250 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0160 |          13.1389 |           0.2250 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0181 |          13.1506 |           0.2251 |
[32m[20221213 15:17:05 @agent_ppo2.py:185][0m |          -0.0196 |          13.0228 |           0.2250 |
[32m[20221213 15:17:06 @agent_ppo2.py:185][0m |          -0.0185 |          12.9449 |           0.2251 |
[32m[20221213 15:17:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.67
[32m[20221213 15:17:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.24
[32m[20221213 15:17:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.53
[32m[20221213 15:17:06 @agent_ppo2.py:143][0m Total time:      24.18 min
[32m[20221213 15:17:06 @agent_ppo2.py:145][0m 2181120 total steps have happened
[32m[20221213 15:17:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1065 --------------------------#
[32m[20221213 15:17:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:06 @agent_ppo2.py:185][0m |          -0.0036 |          15.2796 |           0.2214 |
[32m[20221213 15:17:06 @agent_ppo2.py:185][0m |          -0.0111 |          15.0544 |           0.2215 |
[32m[20221213 15:17:06 @agent_ppo2.py:185][0m |          -0.0137 |          14.8710 |           0.2214 |
[32m[20221213 15:17:06 @agent_ppo2.py:185][0m |          -0.0138 |          14.7879 |           0.2215 |
[32m[20221213 15:17:06 @agent_ppo2.py:185][0m |          -0.0137 |          14.6887 |           0.2213 |
[32m[20221213 15:17:07 @agent_ppo2.py:185][0m |          -0.0142 |          14.7561 |           0.2213 |
[32m[20221213 15:17:07 @agent_ppo2.py:185][0m |          -0.0161 |          14.5690 |           0.2214 |
[32m[20221213 15:17:07 @agent_ppo2.py:185][0m |          -0.0152 |          14.5149 |           0.2212 |
[32m[20221213 15:17:07 @agent_ppo2.py:185][0m |          -0.0199 |          14.4340 |           0.2213 |
[32m[20221213 15:17:07 @agent_ppo2.py:185][0m |          -0.0189 |          14.3805 |           0.2212 |
[32m[20221213 15:17:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.32
[32m[20221213 15:17:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.28
[32m[20221213 15:17:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.19
[32m[20221213 15:17:07 @agent_ppo2.py:143][0m Total time:      24.20 min
[32m[20221213 15:17:07 @agent_ppo2.py:145][0m 2183168 total steps have happened
[32m[20221213 15:17:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1066 --------------------------#
[32m[20221213 15:17:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:07 @agent_ppo2.py:185][0m |           0.0033 |          14.4355 |           0.2292 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0041 |          13.8272 |           0.2293 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0021 |          13.7005 |           0.2293 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0087 |          13.3763 |           0.2295 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0145 |          13.0636 |           0.2295 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0141 |          12.8830 |           0.2297 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0178 |          12.7191 |           0.2299 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0192 |          12.6179 |           0.2299 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0110 |          12.7413 |           0.2301 |
[32m[20221213 15:17:08 @agent_ppo2.py:185][0m |          -0.0197 |          12.3765 |           0.2301 |
[32m[20221213 15:17:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 216.39
[32m[20221213 15:17:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 225.83
[32m[20221213 15:17:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.29
[32m[20221213 15:17:08 @agent_ppo2.py:143][0m Total time:      24.22 min
[32m[20221213 15:17:08 @agent_ppo2.py:145][0m 2185216 total steps have happened
[32m[20221213 15:17:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1067 --------------------------#
[32m[20221213 15:17:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |           0.0003 |          16.7451 |           0.2322 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0078 |          15.6047 |           0.2317 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0116 |          15.3607 |           0.2313 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0170 |          15.2065 |           0.2313 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0164 |          15.0655 |           0.2311 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0173 |          14.9696 |           0.2311 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0179 |          14.8495 |           0.2309 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0194 |          14.9543 |           0.2307 |
[32m[20221213 15:17:09 @agent_ppo2.py:185][0m |          -0.0182 |          14.6710 |           0.2307 |
[32m[20221213 15:17:10 @agent_ppo2.py:185][0m |          -0.0162 |          14.5962 |           0.2306 |
[32m[20221213 15:17:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.60
[32m[20221213 15:17:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 230.02
[32m[20221213 15:17:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.52
[32m[20221213 15:17:10 @agent_ppo2.py:143][0m Total time:      24.24 min
[32m[20221213 15:17:10 @agent_ppo2.py:145][0m 2187264 total steps have happened
[32m[20221213 15:17:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1068 --------------------------#
[32m[20221213 15:17:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:10 @agent_ppo2.py:185][0m |           0.0001 |          15.7952 |           0.2250 |
[32m[20221213 15:17:10 @agent_ppo2.py:185][0m |          -0.0058 |          15.5090 |           0.2248 |
[32m[20221213 15:17:10 @agent_ppo2.py:185][0m |          -0.0101 |          15.2991 |           0.2249 |
[32m[20221213 15:17:10 @agent_ppo2.py:185][0m |          -0.0077 |          15.3161 |           0.2246 |
[32m[20221213 15:17:10 @agent_ppo2.py:185][0m |          -0.0072 |          15.8297 |           0.2245 |
[32m[20221213 15:17:11 @agent_ppo2.py:185][0m |          -0.0117 |          15.3123 |           0.2245 |
[32m[20221213 15:17:11 @agent_ppo2.py:185][0m |          -0.0145 |          15.0537 |           0.2242 |
[32m[20221213 15:17:11 @agent_ppo2.py:185][0m |          -0.0071 |          16.8479 |           0.2244 |
[32m[20221213 15:17:11 @agent_ppo2.py:185][0m |          -0.0159 |          15.0005 |           0.2241 |
[32m[20221213 15:17:11 @agent_ppo2.py:185][0m |          -0.0140 |          15.2014 |           0.2241 |
[32m[20221213 15:17:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.36
[32m[20221213 15:17:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.02
[32m[20221213 15:17:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.78
[32m[20221213 15:17:11 @agent_ppo2.py:143][0m Total time:      24.26 min
[32m[20221213 15:17:11 @agent_ppo2.py:145][0m 2189312 total steps have happened
[32m[20221213 15:17:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1069 --------------------------#
[32m[20221213 15:17:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:11 @agent_ppo2.py:185][0m |          -0.0008 |          15.2038 |           0.2269 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |           0.0031 |          16.7632 |           0.2264 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0078 |          14.8580 |           0.2254 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0091 |          14.6463 |           0.2253 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0001 |          15.4878 |           0.2254 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0125 |          14.4638 |           0.2252 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0092 |          14.4866 |           0.2252 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0140 |          14.3136 |           0.2252 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0155 |          14.2387 |           0.2251 |
[32m[20221213 15:17:12 @agent_ppo2.py:185][0m |          -0.0179 |          14.1614 |           0.2250 |
[32m[20221213 15:17:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:17:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.08
[32m[20221213 15:17:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.57
[32m[20221213 15:17:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.10
[32m[20221213 15:17:12 @agent_ppo2.py:143][0m Total time:      24.29 min
[32m[20221213 15:17:12 @agent_ppo2.py:145][0m 2191360 total steps have happened
[32m[20221213 15:17:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1070 --------------------------#
[32m[20221213 15:17:13 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:17:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0020 |          16.0086 |           0.2244 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |           0.0082 |          17.2315 |           0.2239 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0074 |          15.5107 |           0.2235 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0146 |          15.3738 |           0.2235 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0121 |          15.3423 |           0.2235 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0109 |          15.2414 |           0.2233 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0129 |          15.1476 |           0.2231 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0155 |          15.0829 |           0.2231 |
[32m[20221213 15:17:13 @agent_ppo2.py:185][0m |          -0.0108 |          15.7524 |           0.2227 |
[32m[20221213 15:17:14 @agent_ppo2.py:185][0m |          -0.0121 |          14.9829 |           0.2226 |
[32m[20221213 15:17:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:17:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.34
[32m[20221213 15:17:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.51
[32m[20221213 15:17:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.61
[32m[20221213 15:17:14 @agent_ppo2.py:143][0m Total time:      24.31 min
[32m[20221213 15:17:14 @agent_ppo2.py:145][0m 2193408 total steps have happened
[32m[20221213 15:17:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1071 --------------------------#
[32m[20221213 15:17:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:14 @agent_ppo2.py:185][0m |           0.0037 |          16.0808 |           0.2220 |
[32m[20221213 15:17:14 @agent_ppo2.py:185][0m |          -0.0087 |          15.4809 |           0.2215 |
[32m[20221213 15:17:14 @agent_ppo2.py:185][0m |           0.0003 |          16.5472 |           0.2213 |
[32m[20221213 15:17:14 @agent_ppo2.py:185][0m |          -0.0041 |          16.3368 |           0.2215 |
[32m[20221213 15:17:14 @agent_ppo2.py:185][0m |          -0.0151 |          15.0352 |           0.2214 |
[32m[20221213 15:17:15 @agent_ppo2.py:185][0m |          -0.0152 |          14.9070 |           0.2215 |
[32m[20221213 15:17:15 @agent_ppo2.py:185][0m |          -0.0162 |          14.7833 |           0.2213 |
[32m[20221213 15:17:15 @agent_ppo2.py:185][0m |          -0.0179 |          14.6935 |           0.2215 |
[32m[20221213 15:17:15 @agent_ppo2.py:185][0m |          -0.0130 |          14.8505 |           0.2214 |
[32m[20221213 15:17:15 @agent_ppo2.py:185][0m |          -0.0166 |          14.6217 |           0.2216 |
[32m[20221213 15:17:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.98
[32m[20221213 15:17:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.20
[32m[20221213 15:17:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.16
[32m[20221213 15:17:15 @agent_ppo2.py:143][0m Total time:      24.33 min
[32m[20221213 15:17:15 @agent_ppo2.py:145][0m 2195456 total steps have happened
[32m[20221213 15:17:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1072 --------------------------#
[32m[20221213 15:17:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:15 @agent_ppo2.py:185][0m |           0.0047 |          14.7761 |           0.2311 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0060 |          13.3733 |           0.2308 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0088 |          12.8723 |           0.2306 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0102 |          12.5907 |           0.2304 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0141 |          12.3971 |           0.2303 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0146 |          12.1633 |           0.2303 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0182 |          12.0151 |           0.2298 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0182 |          11.8813 |           0.2300 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0170 |          11.8239 |           0.2299 |
[32m[20221213 15:17:16 @agent_ppo2.py:185][0m |          -0.0191 |          11.7601 |           0.2300 |
[32m[20221213 15:17:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.70
[32m[20221213 15:17:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.11
[32m[20221213 15:17:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.09
[32m[20221213 15:17:16 @agent_ppo2.py:143][0m Total time:      24.35 min
[32m[20221213 15:17:16 @agent_ppo2.py:145][0m 2197504 total steps have happened
[32m[20221213 15:17:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1073 --------------------------#
[32m[20221213 15:17:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |           0.0017 |          15.4288 |           0.2233 |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |          -0.0044 |          14.6180 |           0.2232 |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |          -0.0119 |          14.2768 |           0.2231 |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |          -0.0102 |          14.0515 |           0.2229 |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |          -0.0098 |          13.9781 |           0.2233 |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |          -0.0135 |          13.7668 |           0.2233 |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |          -0.0146 |          13.7104 |           0.2231 |
[32m[20221213 15:17:17 @agent_ppo2.py:185][0m |          -0.0172 |          13.5382 |           0.2236 |
[32m[20221213 15:17:18 @agent_ppo2.py:185][0m |          -0.0160 |          13.4821 |           0.2236 |
[32m[20221213 15:17:18 @agent_ppo2.py:185][0m |          -0.0171 |          13.3840 |           0.2238 |
[32m[20221213 15:17:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:17:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.92
[32m[20221213 15:17:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 236.35
[32m[20221213 15:17:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 233.33
[32m[20221213 15:17:18 @agent_ppo2.py:143][0m Total time:      24.38 min
[32m[20221213 15:17:18 @agent_ppo2.py:145][0m 2199552 total steps have happened
[32m[20221213 15:17:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1074 --------------------------#
[32m[20221213 15:17:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:18 @agent_ppo2.py:185][0m |           0.0058 |          16.4144 |           0.2238 |
[32m[20221213 15:17:18 @agent_ppo2.py:185][0m |          -0.0057 |          15.8558 |           0.2234 |
[32m[20221213 15:17:18 @agent_ppo2.py:185][0m |          -0.0086 |          15.7127 |           0.2231 |
[32m[20221213 15:17:18 @agent_ppo2.py:185][0m |          -0.0091 |          15.6469 |           0.2229 |
[32m[20221213 15:17:19 @agent_ppo2.py:185][0m |          -0.0089 |          15.5983 |           0.2227 |
[32m[20221213 15:17:19 @agent_ppo2.py:185][0m |          -0.0122 |          15.5251 |           0.2226 |
[32m[20221213 15:17:19 @agent_ppo2.py:185][0m |          -0.0098 |          15.5553 |           0.2223 |
[32m[20221213 15:17:19 @agent_ppo2.py:185][0m |          -0.0139 |          15.4555 |           0.2223 |
[32m[20221213 15:17:19 @agent_ppo2.py:185][0m |          -0.0138 |          15.4192 |           0.2219 |
[32m[20221213 15:17:19 @agent_ppo2.py:185][0m |          -0.0119 |          15.3765 |           0.2219 |
[32m[20221213 15:17:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.79
[32m[20221213 15:17:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.14
[32m[20221213 15:17:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.92
[32m[20221213 15:17:19 @agent_ppo2.py:143][0m Total time:      24.40 min
[32m[20221213 15:17:19 @agent_ppo2.py:145][0m 2201600 total steps have happened
[32m[20221213 15:17:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1075 --------------------------#
[32m[20221213 15:17:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0027 |          15.0152 |           0.2262 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0093 |          14.2763 |           0.2262 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0031 |          15.8643 |           0.2264 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0158 |          13.6293 |           0.2264 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0173 |          13.3182 |           0.2265 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0172 |          13.1281 |           0.2266 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0211 |          12.9391 |           0.2266 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0096 |          14.1370 |           0.2265 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0180 |          12.7538 |           0.2258 |
[32m[20221213 15:17:20 @agent_ppo2.py:185][0m |          -0.0214 |          12.6523 |           0.2268 |
[32m[20221213 15:17:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:17:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.14
[32m[20221213 15:17:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.61
[32m[20221213 15:17:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.29
[32m[20221213 15:17:20 @agent_ppo2.py:143][0m Total time:      24.42 min
[32m[20221213 15:17:20 @agent_ppo2.py:145][0m 2203648 total steps have happened
[32m[20221213 15:17:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1076 --------------------------#
[32m[20221213 15:17:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0021 |          18.1961 |           0.2298 |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0018 |          17.5108 |           0.2294 |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0103 |          16.7817 |           0.2288 |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0079 |          16.6182 |           0.2288 |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0081 |          16.6877 |           0.2285 |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0084 |          16.2899 |           0.2283 |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0165 |          16.1853 |           0.2282 |
[32m[20221213 15:17:21 @agent_ppo2.py:185][0m |          -0.0155 |          16.0558 |           0.2281 |
[32m[20221213 15:17:22 @agent_ppo2.py:185][0m |          -0.0177 |          15.9769 |           0.2281 |
[32m[20221213 15:17:22 @agent_ppo2.py:185][0m |          -0.0117 |          15.9402 |           0.2278 |
[32m[20221213 15:17:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.40
[32m[20221213 15:17:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.89
[32m[20221213 15:17:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.37
[32m[20221213 15:17:22 @agent_ppo2.py:143][0m Total time:      24.44 min
[32m[20221213 15:17:22 @agent_ppo2.py:145][0m 2205696 total steps have happened
[32m[20221213 15:17:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1077 --------------------------#
[32m[20221213 15:17:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:22 @agent_ppo2.py:185][0m |           0.0011 |          16.3559 |           0.2222 |
[32m[20221213 15:17:22 @agent_ppo2.py:185][0m |          -0.0097 |          15.9298 |           0.2218 |
[32m[20221213 15:17:22 @agent_ppo2.py:185][0m |          -0.0095 |          15.7154 |           0.2216 |
[32m[20221213 15:17:22 @agent_ppo2.py:185][0m |          -0.0130 |          15.6296 |           0.2213 |
[32m[20221213 15:17:23 @agent_ppo2.py:185][0m |          -0.0089 |          15.5892 |           0.2214 |
[32m[20221213 15:17:23 @agent_ppo2.py:185][0m |          -0.0153 |          15.4782 |           0.2215 |
[32m[20221213 15:17:23 @agent_ppo2.py:185][0m |          -0.0126 |          15.3961 |           0.2210 |
[32m[20221213 15:17:23 @agent_ppo2.py:185][0m |          -0.0108 |          15.7420 |           0.2214 |
[32m[20221213 15:17:23 @agent_ppo2.py:185][0m |          -0.0162 |          15.2838 |           0.2213 |
[32m[20221213 15:17:23 @agent_ppo2.py:185][0m |          -0.0168 |          15.2509 |           0.2213 |
[32m[20221213 15:17:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.69
[32m[20221213 15:17:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 240.88
[32m[20221213 15:17:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.63
[32m[20221213 15:17:23 @agent_ppo2.py:143][0m Total time:      24.47 min
[32m[20221213 15:17:23 @agent_ppo2.py:145][0m 2207744 total steps have happened
[32m[20221213 15:17:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1078 --------------------------#
[32m[20221213 15:17:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0002 |          16.1133 |           0.2198 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0077 |          15.8315 |           0.2194 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0103 |          15.7113 |           0.2193 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0100 |          15.6026 |           0.2191 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0128 |          15.5444 |           0.2191 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0125 |          15.4596 |           0.2191 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0139 |          15.4163 |           0.2192 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0151 |          15.3916 |           0.2191 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0149 |          15.2870 |           0.2191 |
[32m[20221213 15:17:24 @agent_ppo2.py:185][0m |          -0.0148 |          15.2533 |           0.2190 |
[32m[20221213 15:17:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 224.80
[32m[20221213 15:17:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.37
[32m[20221213 15:17:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.19
[32m[20221213 15:17:24 @agent_ppo2.py:143][0m Total time:      24.49 min
[32m[20221213 15:17:24 @agent_ppo2.py:145][0m 2209792 total steps have happened
[32m[20221213 15:17:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1079 --------------------------#
[32m[20221213 15:17:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |           0.0002 |          15.5125 |           0.2221 |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |          -0.0073 |          14.9975 |           0.2220 |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |          -0.0091 |          14.6493 |           0.2217 |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |           0.0043 |          15.9761 |           0.2219 |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |          -0.0027 |          14.8336 |           0.2213 |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |          -0.0120 |          13.9771 |           0.2214 |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |          -0.0150 |          13.8125 |           0.2214 |
[32m[20221213 15:17:25 @agent_ppo2.py:185][0m |          -0.0140 |          13.6583 |           0.2214 |
[32m[20221213 15:17:26 @agent_ppo2.py:185][0m |          -0.0151 |          13.5371 |           0.2213 |
[32m[20221213 15:17:26 @agent_ppo2.py:185][0m |          -0.0133 |          13.5413 |           0.2213 |
[32m[20221213 15:17:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.02
[32m[20221213 15:17:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.29
[32m[20221213 15:17:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.06
[32m[20221213 15:17:26 @agent_ppo2.py:143][0m Total time:      24.51 min
[32m[20221213 15:17:26 @agent_ppo2.py:145][0m 2211840 total steps have happened
[32m[20221213 15:17:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1080 --------------------------#
[32m[20221213 15:17:26 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:17:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:26 @agent_ppo2.py:185][0m |          -0.0030 |          15.2101 |           0.2240 |
[32m[20221213 15:17:26 @agent_ppo2.py:185][0m |          -0.0050 |          14.6531 |           0.2238 |
[32m[20221213 15:17:26 @agent_ppo2.py:185][0m |          -0.0062 |          14.2411 |           0.2239 |
[32m[20221213 15:17:26 @agent_ppo2.py:185][0m |          -0.0121 |          13.8999 |           0.2239 |
[32m[20221213 15:17:27 @agent_ppo2.py:185][0m |          -0.0061 |          14.0271 |           0.2241 |
[32m[20221213 15:17:27 @agent_ppo2.py:185][0m |          -0.0146 |          13.5008 |           0.2240 |
[32m[20221213 15:17:27 @agent_ppo2.py:185][0m |          -0.0160 |          13.3301 |           0.2241 |
[32m[20221213 15:17:27 @agent_ppo2.py:185][0m |          -0.0173 |          13.2123 |           0.2238 |
[32m[20221213 15:17:27 @agent_ppo2.py:185][0m |          -0.0169 |          13.1337 |           0.2241 |
[32m[20221213 15:17:27 @agent_ppo2.py:185][0m |          -0.0042 |          14.3037 |           0.2242 |
[32m[20221213 15:17:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.90
[32m[20221213 15:17:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.17
[32m[20221213 15:17:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.32
[32m[20221213 15:17:27 @agent_ppo2.py:143][0m Total time:      24.53 min
[32m[20221213 15:17:27 @agent_ppo2.py:145][0m 2213888 total steps have happened
[32m[20221213 15:17:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1081 --------------------------#
[32m[20221213 15:17:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |           0.0024 |          16.9937 |           0.2230 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0067 |          16.0481 |           0.2229 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0102 |          15.8243 |           0.2229 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0114 |          15.6998 |           0.2230 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0141 |          15.5858 |           0.2229 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0138 |          15.5005 |           0.2229 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0137 |          15.3663 |           0.2226 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0143 |          15.2138 |           0.2230 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0160 |          15.1155 |           0.2229 |
[32m[20221213 15:17:28 @agent_ppo2.py:185][0m |          -0.0152 |          15.0525 |           0.2229 |
[32m[20221213 15:17:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.26
[32m[20221213 15:17:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.47
[32m[20221213 15:17:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.50
[32m[20221213 15:17:29 @agent_ppo2.py:143][0m Total time:      24.56 min
[32m[20221213 15:17:29 @agent_ppo2.py:145][0m 2215936 total steps have happened
[32m[20221213 15:17:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1082 --------------------------#
[32m[20221213 15:17:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |           0.0017 |          15.3115 |           0.2256 |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |          -0.0057 |          14.8814 |           0.2250 |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |          -0.0080 |          14.6929 |           0.2251 |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |          -0.0132 |          14.4783 |           0.2249 |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |          -0.0075 |          14.7697 |           0.2249 |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |          -0.0159 |          14.1568 |           0.2245 |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |          -0.0178 |          14.0649 |           0.2243 |
[32m[20221213 15:17:29 @agent_ppo2.py:185][0m |          -0.0125 |          14.0744 |           0.2246 |
[32m[20221213 15:17:30 @agent_ppo2.py:185][0m |          -0.0111 |          14.3156 |           0.2243 |
[32m[20221213 15:17:30 @agent_ppo2.py:185][0m |          -0.0167 |          13.7449 |           0.2244 |
[32m[20221213 15:17:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:17:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.34
[32m[20221213 15:17:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.04
[32m[20221213 15:17:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.56
[32m[20221213 15:17:30 @agent_ppo2.py:143][0m Total time:      24.58 min
[32m[20221213 15:17:30 @agent_ppo2.py:145][0m 2217984 total steps have happened
[32m[20221213 15:17:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1083 --------------------------#
[32m[20221213 15:17:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:30 @agent_ppo2.py:185][0m |           0.0090 |          17.0158 |           0.2299 |
[32m[20221213 15:17:30 @agent_ppo2.py:185][0m |          -0.0098 |          14.9837 |           0.2295 |
[32m[20221213 15:17:30 @agent_ppo2.py:185][0m |          -0.0100 |          14.6032 |           0.2295 |
[32m[20221213 15:17:30 @agent_ppo2.py:185][0m |          -0.0146 |          14.2668 |           0.2297 |
[32m[20221213 15:17:31 @agent_ppo2.py:185][0m |          -0.0129 |          14.1250 |           0.2296 |
[32m[20221213 15:17:31 @agent_ppo2.py:185][0m |          -0.0174 |          13.8293 |           0.2297 |
[32m[20221213 15:17:31 @agent_ppo2.py:185][0m |          -0.0196 |          13.7439 |           0.2299 |
[32m[20221213 15:17:31 @agent_ppo2.py:185][0m |          -0.0157 |          13.5148 |           0.2301 |
[32m[20221213 15:17:31 @agent_ppo2.py:185][0m |          -0.0141 |          13.3972 |           0.2300 |
[32m[20221213 15:17:31 @agent_ppo2.py:185][0m |          -0.0172 |          13.2154 |           0.2302 |
[32m[20221213 15:17:31 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:17:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.15
[32m[20221213 15:17:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 231.33
[32m[20221213 15:17:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.58
[32m[20221213 15:17:31 @agent_ppo2.py:143][0m Total time:      24.60 min
[32m[20221213 15:17:31 @agent_ppo2.py:145][0m 2220032 total steps have happened
[32m[20221213 15:17:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1084 --------------------------#
[32m[20221213 15:17:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |           0.0032 |          18.5852 |           0.2233 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0040 |          17.6402 |           0.2235 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0068 |          17.6369 |           0.2232 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0012 |          18.0400 |           0.2230 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0049 |          17.4850 |           0.2226 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0140 |          16.9643 |           0.2228 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0123 |          16.9095 |           0.2227 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0178 |          16.7793 |           0.2226 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0185 |          16.7071 |           0.2224 |
[32m[20221213 15:17:32 @agent_ppo2.py:185][0m |          -0.0166 |          16.6199 |           0.2224 |
[32m[20221213 15:17:32 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:17:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 217.09
[32m[20221213 15:17:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.68
[32m[20221213 15:17:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.54
[32m[20221213 15:17:33 @agent_ppo2.py:143][0m Total time:      24.62 min
[32m[20221213 15:17:33 @agent_ppo2.py:145][0m 2222080 total steps have happened
[32m[20221213 15:17:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1085 --------------------------#
[32m[20221213 15:17:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:33 @agent_ppo2.py:185][0m |          -0.0009 |          16.9246 |           0.2255 |
[32m[20221213 15:17:33 @agent_ppo2.py:185][0m |          -0.0102 |          16.6430 |           0.2253 |
[32m[20221213 15:17:33 @agent_ppo2.py:185][0m |          -0.0118 |          16.4531 |           0.2252 |
[32m[20221213 15:17:33 @agent_ppo2.py:185][0m |          -0.0134 |          16.3235 |           0.2254 |
[32m[20221213 15:17:33 @agent_ppo2.py:185][0m |          -0.0132 |          16.2597 |           0.2253 |
[32m[20221213 15:17:33 @agent_ppo2.py:185][0m |          -0.0127 |          16.2573 |           0.2254 |
[32m[20221213 15:17:34 @agent_ppo2.py:185][0m |          -0.0123 |          16.1439 |           0.2256 |
[32m[20221213 15:17:34 @agent_ppo2.py:185][0m |          -0.0177 |          16.0397 |           0.2259 |
[32m[20221213 15:17:34 @agent_ppo2.py:185][0m |          -0.0167 |          15.9580 |           0.2258 |
[32m[20221213 15:17:34 @agent_ppo2.py:185][0m |          -0.0199 |          15.9001 |           0.2258 |
[32m[20221213 15:17:34 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:17:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.32
[32m[20221213 15:17:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.93
[32m[20221213 15:17:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.73
[32m[20221213 15:17:34 @agent_ppo2.py:143][0m Total time:      24.65 min
[32m[20221213 15:17:34 @agent_ppo2.py:145][0m 2224128 total steps have happened
[32m[20221213 15:17:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1086 --------------------------#
[32m[20221213 15:17:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:34 @agent_ppo2.py:185][0m |          -0.0006 |          16.5201 |           0.2251 |
[32m[20221213 15:17:34 @agent_ppo2.py:185][0m |          -0.0044 |          15.7812 |           0.2256 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0096 |          15.4454 |           0.2258 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0090 |          15.7031 |           0.2260 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0165 |          15.0875 |           0.2261 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0170 |          14.9267 |           0.2262 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0142 |          14.8212 |           0.2263 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0126 |          14.9330 |           0.2271 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0176 |          14.5137 |           0.2271 |
[32m[20221213 15:17:35 @agent_ppo2.py:185][0m |          -0.0160 |          14.4142 |           0.2270 |
[32m[20221213 15:17:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:17:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.58
[32m[20221213 15:17:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.18
[32m[20221213 15:17:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.87
[32m[20221213 15:17:35 @agent_ppo2.py:143][0m Total time:      24.67 min
[32m[20221213 15:17:35 @agent_ppo2.py:145][0m 2226176 total steps have happened
[32m[20221213 15:17:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1087 --------------------------#
[32m[20221213 15:17:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0009 |          16.5324 |           0.2371 |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0046 |          16.0532 |           0.2364 |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0113 |          15.7494 |           0.2362 |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0111 |          15.6190 |           0.2362 |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0103 |          15.5072 |           0.2362 |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0086 |          15.6025 |           0.2357 |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0156 |          15.3535 |           0.2360 |
[32m[20221213 15:17:36 @agent_ppo2.py:185][0m |          -0.0160 |          15.1722 |           0.2358 |
[32m[20221213 15:17:37 @agent_ppo2.py:185][0m |          -0.0151 |          15.1473 |           0.2360 |
[32m[20221213 15:17:37 @agent_ppo2.py:185][0m |          -0.0179 |          15.0933 |           0.2359 |
[32m[20221213 15:17:37 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:17:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.65
[32m[20221213 15:17:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 234.84
[32m[20221213 15:17:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.22
[32m[20221213 15:17:37 @agent_ppo2.py:143][0m Total time:      24.69 min
[32m[20221213 15:17:37 @agent_ppo2.py:145][0m 2228224 total steps have happened
[32m[20221213 15:17:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1088 --------------------------#
[32m[20221213 15:17:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:37 @agent_ppo2.py:185][0m |          -0.0024 |          17.1748 |           0.2359 |
[32m[20221213 15:17:37 @agent_ppo2.py:185][0m |          -0.0080 |          16.7181 |           0.2353 |
[32m[20221213 15:17:37 @agent_ppo2.py:185][0m |          -0.0104 |          16.4441 |           0.2354 |
[32m[20221213 15:17:37 @agent_ppo2.py:185][0m |          -0.0137 |          16.2924 |           0.2352 |
[32m[20221213 15:17:38 @agent_ppo2.py:185][0m |          -0.0138 |          16.2036 |           0.2351 |
[32m[20221213 15:17:38 @agent_ppo2.py:185][0m |          -0.0153 |          16.0964 |           0.2349 |
[32m[20221213 15:17:38 @agent_ppo2.py:185][0m |          -0.0165 |          15.9711 |           0.2348 |
[32m[20221213 15:17:38 @agent_ppo2.py:185][0m |          -0.0173 |          15.9294 |           0.2347 |
[32m[20221213 15:17:38 @agent_ppo2.py:185][0m |          -0.0181 |          15.8606 |           0.2346 |
[32m[20221213 15:17:38 @agent_ppo2.py:185][0m |          -0.0166 |          15.7648 |           0.2346 |
[32m[20221213 15:17:38 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:17:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.96
[32m[20221213 15:17:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.89
[32m[20221213 15:17:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.71
[32m[20221213 15:17:38 @agent_ppo2.py:143][0m Total time:      24.72 min
[32m[20221213 15:17:38 @agent_ppo2.py:145][0m 2230272 total steps have happened
[32m[20221213 15:17:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1089 --------------------------#
[32m[20221213 15:17:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0013 |          15.8913 |           0.2324 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0099 |          15.3657 |           0.2322 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0094 |          15.0249 |           0.2319 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0072 |          15.1580 |           0.2318 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0095 |          14.7863 |           0.2316 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0142 |          14.5029 |           0.2313 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0102 |          14.4192 |           0.2315 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0143 |          14.2458 |           0.2310 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0174 |          14.1623 |           0.2312 |
[32m[20221213 15:17:39 @agent_ppo2.py:185][0m |          -0.0166 |          14.0622 |           0.2310 |
[32m[20221213 15:17:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:17:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.61
[32m[20221213 15:17:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.90
[32m[20221213 15:17:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.17
[32m[20221213 15:17:40 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 283.17
[32m[20221213 15:17:40 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 283.17
[32m[20221213 15:17:40 @agent_ppo2.py:143][0m Total time:      24.74 min
[32m[20221213 15:17:40 @agent_ppo2.py:145][0m 2232320 total steps have happened
[32m[20221213 15:17:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1090 --------------------------#
[32m[20221213 15:17:40 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:17:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:40 @agent_ppo2.py:185][0m |          -0.0043 |          17.4137 |           0.2221 |
[32m[20221213 15:17:40 @agent_ppo2.py:185][0m |          -0.0094 |          16.6874 |           0.2219 |
[32m[20221213 15:17:40 @agent_ppo2.py:185][0m |          -0.0109 |          16.4187 |           0.2221 |
[32m[20221213 15:17:40 @agent_ppo2.py:185][0m |          -0.0132 |          16.2162 |           0.2222 |
[32m[20221213 15:17:40 @agent_ppo2.py:185][0m |          -0.0134 |          16.1789 |           0.2222 |
[32m[20221213 15:17:40 @agent_ppo2.py:185][0m |          -0.0132 |          16.0487 |           0.2223 |
[32m[20221213 15:17:41 @agent_ppo2.py:185][0m |          -0.0165 |          15.9109 |           0.2222 |
[32m[20221213 15:17:41 @agent_ppo2.py:185][0m |          -0.0154 |          15.8723 |           0.2222 |
[32m[20221213 15:17:41 @agent_ppo2.py:185][0m |          -0.0157 |          15.7663 |           0.2219 |
[32m[20221213 15:17:41 @agent_ppo2.py:185][0m |          -0.0191 |          15.7361 |           0.2220 |
[32m[20221213 15:17:41 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:17:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.74
[32m[20221213 15:17:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.29
[32m[20221213 15:17:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.62
[32m[20221213 15:17:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 285.62
[32m[20221213 15:17:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 285.62
[32m[20221213 15:17:41 @agent_ppo2.py:143][0m Total time:      24.76 min
[32m[20221213 15:17:41 @agent_ppo2.py:145][0m 2234368 total steps have happened
[32m[20221213 15:17:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1091 --------------------------#
[32m[20221213 15:17:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:41 @agent_ppo2.py:185][0m |          -0.0008 |          16.9360 |           0.2297 |
[32m[20221213 15:17:41 @agent_ppo2.py:185][0m |           0.0040 |          17.0626 |           0.2299 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0033 |          16.5243 |           0.2295 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0095 |          16.3406 |           0.2295 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0138 |          16.2890 |           0.2294 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0115 |          16.1923 |           0.2293 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0077 |          16.3867 |           0.2294 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0110 |          16.0687 |           0.2293 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0121 |          15.9785 |           0.2293 |
[32m[20221213 15:17:42 @agent_ppo2.py:185][0m |          -0.0146 |          15.9461 |           0.2294 |
[32m[20221213 15:17:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:17:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.55
[32m[20221213 15:17:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 248.09
[32m[20221213 15:17:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.42
[32m[20221213 15:17:42 @agent_ppo2.py:143][0m Total time:      24.79 min
[32m[20221213 15:17:42 @agent_ppo2.py:145][0m 2236416 total steps have happened
[32m[20221213 15:17:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1092 --------------------------#
[32m[20221213 15:17:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |           0.0020 |          16.8802 |           0.2318 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0063 |          16.1081 |           0.2315 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0012 |          16.2371 |           0.2317 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0113 |          15.5400 |           0.2316 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0129 |          15.3198 |           0.2316 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0137 |          15.1673 |           0.2314 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0077 |          15.6371 |           0.2316 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0160 |          14.9019 |           0.2317 |
[32m[20221213 15:17:43 @agent_ppo2.py:185][0m |          -0.0156 |          14.8042 |           0.2316 |
[32m[20221213 15:17:44 @agent_ppo2.py:185][0m |          -0.0148 |          14.7057 |           0.2317 |
[32m[20221213 15:17:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:17:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.04
[32m[20221213 15:17:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.59
[32m[20221213 15:17:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.41
[32m[20221213 15:17:44 @agent_ppo2.py:143][0m Total time:      24.81 min
[32m[20221213 15:17:44 @agent_ppo2.py:145][0m 2238464 total steps have happened
[32m[20221213 15:17:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1093 --------------------------#
[32m[20221213 15:17:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:44 @agent_ppo2.py:185][0m |           0.0001 |          17.3965 |           0.2273 |
[32m[20221213 15:17:44 @agent_ppo2.py:185][0m |          -0.0082 |          16.1625 |           0.2270 |
[32m[20221213 15:17:44 @agent_ppo2.py:185][0m |          -0.0083 |          15.8522 |           0.2268 |
[32m[20221213 15:17:44 @agent_ppo2.py:185][0m |          -0.0112 |          15.5792 |           0.2267 |
[32m[20221213 15:17:44 @agent_ppo2.py:185][0m |          -0.0106 |          15.3831 |           0.2265 |
[32m[20221213 15:17:45 @agent_ppo2.py:185][0m |          -0.0132 |          15.1981 |           0.2265 |
[32m[20221213 15:17:45 @agent_ppo2.py:185][0m |          -0.0134 |          15.0775 |           0.2263 |
[32m[20221213 15:17:45 @agent_ppo2.py:185][0m |          -0.0146 |          14.9414 |           0.2262 |
[32m[20221213 15:17:45 @agent_ppo2.py:185][0m |          -0.0162 |          14.7916 |           0.2262 |
[32m[20221213 15:17:45 @agent_ppo2.py:185][0m |          -0.0159 |          14.7303 |           0.2262 |
[32m[20221213 15:17:45 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:17:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.36
[32m[20221213 15:17:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.42
[32m[20221213 15:17:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.37
[32m[20221213 15:17:45 @agent_ppo2.py:143][0m Total time:      24.83 min
[32m[20221213 15:17:45 @agent_ppo2.py:145][0m 2240512 total steps have happened
[32m[20221213 15:17:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1094 --------------------------#
[32m[20221213 15:17:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0059 |          17.6808 |           0.2262 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0033 |          17.5336 |           0.2260 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0093 |          17.2636 |           0.2259 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0123 |          17.1719 |           0.2256 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0009 |          18.5717 |           0.2256 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0119 |          17.0354 |           0.2253 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0073 |          17.5547 |           0.2252 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0098 |          17.7216 |           0.2254 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0087 |          16.9735 |           0.2255 |
[32m[20221213 15:17:46 @agent_ppo2.py:185][0m |          -0.0094 |          17.0880 |           0.2249 |
[32m[20221213 15:17:46 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:17:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.33
[32m[20221213 15:17:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.27
[32m[20221213 15:17:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.82
[32m[20221213 15:17:47 @agent_ppo2.py:143][0m Total time:      24.86 min
[32m[20221213 15:17:47 @agent_ppo2.py:145][0m 2242560 total steps have happened
[32m[20221213 15:17:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1095 --------------------------#
[32m[20221213 15:17:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:47 @agent_ppo2.py:185][0m |          -0.0008 |          17.3077 |           0.2313 |
[32m[20221213 15:17:47 @agent_ppo2.py:185][0m |          -0.0050 |          16.6708 |           0.2307 |
[32m[20221213 15:17:47 @agent_ppo2.py:185][0m |          -0.0068 |          16.3310 |           0.2304 |
[32m[20221213 15:17:47 @agent_ppo2.py:185][0m |          -0.0125 |          16.1357 |           0.2303 |
[32m[20221213 15:17:47 @agent_ppo2.py:185][0m |          -0.0112 |          16.0159 |           0.2298 |
[32m[20221213 15:17:47 @agent_ppo2.py:185][0m |          -0.0120 |          15.9037 |           0.2300 |
[32m[20221213 15:17:47 @agent_ppo2.py:185][0m |          -0.0167 |          15.8230 |           0.2300 |
[32m[20221213 15:17:48 @agent_ppo2.py:185][0m |          -0.0154 |          15.7817 |           0.2296 |
[32m[20221213 15:17:48 @agent_ppo2.py:185][0m |          -0.0172 |          15.6912 |           0.2294 |
[32m[20221213 15:17:48 @agent_ppo2.py:185][0m |          -0.0167 |          15.6532 |           0.2294 |
[32m[20221213 15:17:48 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:17:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.34
[32m[20221213 15:17:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 233.46
[32m[20221213 15:17:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.22
[32m[20221213 15:17:48 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 287.22
[32m[20221213 15:17:48 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 287.22
[32m[20221213 15:17:48 @agent_ppo2.py:143][0m Total time:      24.88 min
[32m[20221213 15:17:48 @agent_ppo2.py:145][0m 2244608 total steps have happened
[32m[20221213 15:17:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1096 --------------------------#
[32m[20221213 15:17:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:48 @agent_ppo2.py:185][0m |          -0.0015 |          13.1089 |           0.2309 |
[32m[20221213 15:17:48 @agent_ppo2.py:185][0m |          -0.0042 |          12.6090 |           0.2304 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |           0.0125 |          14.3383 |           0.2307 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |          -0.0014 |          12.4456 |           0.2309 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |          -0.0068 |          12.3315 |           0.2308 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |          -0.0104 |          12.2738 |           0.2310 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |          -0.0101 |          12.1816 |           0.2312 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |          -0.0117 |          12.1133 |           0.2313 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |          -0.0135 |          12.0950 |           0.2312 |
[32m[20221213 15:17:49 @agent_ppo2.py:185][0m |          -0.0121 |          12.1279 |           0.2310 |
[32m[20221213 15:17:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:17:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 146.76
[32m[20221213 15:17:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.88
[32m[20221213 15:17:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.80
[32m[20221213 15:17:49 @agent_ppo2.py:143][0m Total time:      24.90 min
[32m[20221213 15:17:49 @agent_ppo2.py:145][0m 2246656 total steps have happened
[32m[20221213 15:17:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1097 --------------------------#
[32m[20221213 15:17:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0029 |          16.3780 |           0.2306 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0058 |          15.2678 |           0.2301 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0096 |          14.5883 |           0.2297 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0136 |          14.2948 |           0.2297 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0144 |          14.0408 |           0.2294 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0132 |          13.8565 |           0.2294 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0170 |          13.6500 |           0.2291 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0132 |          13.4998 |           0.2289 |
[32m[20221213 15:17:50 @agent_ppo2.py:185][0m |          -0.0155 |          13.3472 |           0.2289 |
[32m[20221213 15:17:51 @agent_ppo2.py:185][0m |          -0.0188 |          13.2066 |           0.2286 |
[32m[20221213 15:17:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:17:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.00
[32m[20221213 15:17:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.98
[32m[20221213 15:17:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.22
[32m[20221213 15:17:51 @agent_ppo2.py:143][0m Total time:      24.93 min
[32m[20221213 15:17:51 @agent_ppo2.py:145][0m 2248704 total steps have happened
[32m[20221213 15:17:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1098 --------------------------#
[32m[20221213 15:17:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:51 @agent_ppo2.py:185][0m |          -0.0034 |          17.6257 |           0.2251 |
[32m[20221213 15:17:51 @agent_ppo2.py:185][0m |          -0.0092 |          16.8735 |           0.2248 |
[32m[20221213 15:17:51 @agent_ppo2.py:185][0m |          -0.0116 |          16.6625 |           0.2249 |
[32m[20221213 15:17:51 @agent_ppo2.py:185][0m |          -0.0123 |          16.5380 |           0.2249 |
[32m[20221213 15:17:52 @agent_ppo2.py:185][0m |          -0.0121 |          16.4364 |           0.2250 |
[32m[20221213 15:17:52 @agent_ppo2.py:185][0m |          -0.0144 |          16.3525 |           0.2245 |
[32m[20221213 15:17:52 @agent_ppo2.py:185][0m |          -0.0156 |          16.3347 |           0.2248 |
[32m[20221213 15:17:52 @agent_ppo2.py:185][0m |          -0.0147 |          16.3444 |           0.2245 |
[32m[20221213 15:17:52 @agent_ppo2.py:185][0m |          -0.0129 |          16.4939 |           0.2249 |
[32m[20221213 15:17:52 @agent_ppo2.py:185][0m |          -0.0116 |          16.4790 |           0.2247 |
[32m[20221213 15:17:52 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:17:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.78
[32m[20221213 15:17:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.22
[32m[20221213 15:17:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.55
[32m[20221213 15:17:52 @agent_ppo2.py:143][0m Total time:      24.95 min
[32m[20221213 15:17:52 @agent_ppo2.py:145][0m 2250752 total steps have happened
[32m[20221213 15:17:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1099 --------------------------#
[32m[20221213 15:17:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0026 |          16.9285 |           0.2316 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0046 |          16.4860 |           0.2316 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0082 |          16.2150 |           0.2314 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0115 |          16.0493 |           0.2312 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0119 |          15.9027 |           0.2311 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0110 |          15.7993 |           0.2309 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0154 |          15.7034 |           0.2310 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0160 |          15.6070 |           0.2308 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0157 |          15.4653 |           0.2307 |
[32m[20221213 15:17:53 @agent_ppo2.py:185][0m |          -0.0182 |          15.3719 |           0.2307 |
[32m[20221213 15:17:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:17:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.52
[32m[20221213 15:17:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.43
[32m[20221213 15:17:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.58
[32m[20221213 15:17:54 @agent_ppo2.py:143][0m Total time:      24.97 min
[32m[20221213 15:17:54 @agent_ppo2.py:145][0m 2252800 total steps have happened
[32m[20221213 15:17:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1100 --------------------------#
[32m[20221213 15:17:54 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:17:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:54 @agent_ppo2.py:185][0m |           0.0072 |          17.7288 |           0.2296 |
[32m[20221213 15:17:54 @agent_ppo2.py:185][0m |          -0.0073 |          16.5055 |           0.2285 |
[32m[20221213 15:17:54 @agent_ppo2.py:185][0m |          -0.0063 |          16.3333 |           0.2286 |
[32m[20221213 15:17:54 @agent_ppo2.py:185][0m |          -0.0090 |          16.2115 |           0.2285 |
[32m[20221213 15:17:54 @agent_ppo2.py:185][0m |          -0.0101 |          16.1005 |           0.2284 |
[32m[20221213 15:17:54 @agent_ppo2.py:185][0m |          -0.0137 |          16.0304 |           0.2285 |
[32m[20221213 15:17:55 @agent_ppo2.py:185][0m |          -0.0124 |          16.0379 |           0.2280 |
[32m[20221213 15:17:55 @agent_ppo2.py:185][0m |          -0.0132 |          15.8802 |           0.2282 |
[32m[20221213 15:17:55 @agent_ppo2.py:185][0m |          -0.0113 |          15.9576 |           0.2279 |
[32m[20221213 15:17:55 @agent_ppo2.py:185][0m |          -0.0146 |          15.8090 |           0.2279 |
[32m[20221213 15:17:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:17:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.34
[32m[20221213 15:17:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.34
[32m[20221213 15:17:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.55
[32m[20221213 15:17:55 @agent_ppo2.py:143][0m Total time:      25.00 min
[32m[20221213 15:17:55 @agent_ppo2.py:145][0m 2254848 total steps have happened
[32m[20221213 15:17:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1101 --------------------------#
[32m[20221213 15:17:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:55 @agent_ppo2.py:185][0m |          -0.0017 |          15.2004 |           0.2182 |
[32m[20221213 15:17:55 @agent_ppo2.py:185][0m |          -0.0022 |          13.9233 |           0.2183 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0072 |          13.5067 |           0.2180 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0149 |          12.8635 |           0.2181 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0154 |          12.5938 |           0.2181 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0171 |          12.3258 |           0.2180 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0169 |          12.0972 |           0.2179 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0141 |          12.0058 |           0.2181 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0175 |          11.7382 |           0.2180 |
[32m[20221213 15:17:56 @agent_ppo2.py:185][0m |          -0.0193 |          11.6462 |           0.2179 |
[32m[20221213 15:17:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:17:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 219.54
[32m[20221213 15:17:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.05
[32m[20221213 15:17:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.59
[32m[20221213 15:17:56 @agent_ppo2.py:143][0m Total time:      25.02 min
[32m[20221213 15:17:56 @agent_ppo2.py:145][0m 2256896 total steps have happened
[32m[20221213 15:17:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1102 --------------------------#
[32m[20221213 15:17:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:17:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |           0.0028 |          18.1959 |           0.2222 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0068 |          17.2847 |           0.2215 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0139 |          17.0129 |           0.2211 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0157 |          16.7368 |           0.2212 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0132 |          16.6586 |           0.2214 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0190 |          16.3315 |           0.2211 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0082 |          17.6564 |           0.2212 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0091 |          17.0112 |           0.2207 |
[32m[20221213 15:17:57 @agent_ppo2.py:185][0m |          -0.0157 |          15.9204 |           0.2209 |
[32m[20221213 15:17:58 @agent_ppo2.py:185][0m |          -0.0207 |          15.7640 |           0.2210 |
[32m[20221213 15:17:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:17:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.59
[32m[20221213 15:17:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.99
[32m[20221213 15:17:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.07
[32m[20221213 15:17:58 @agent_ppo2.py:143][0m Total time:      25.04 min
[32m[20221213 15:17:58 @agent_ppo2.py:145][0m 2258944 total steps have happened
[32m[20221213 15:17:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1103 --------------------------#
[32m[20221213 15:17:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:17:58 @agent_ppo2.py:185][0m |          -0.0007 |          18.0370 |           0.2235 |
[32m[20221213 15:17:58 @agent_ppo2.py:185][0m |          -0.0087 |          17.0347 |           0.2230 |
[32m[20221213 15:17:58 @agent_ppo2.py:185][0m |          -0.0106 |          16.5603 |           0.2231 |
[32m[20221213 15:17:58 @agent_ppo2.py:185][0m |          -0.0113 |          16.2151 |           0.2232 |
[32m[20221213 15:17:58 @agent_ppo2.py:185][0m |          -0.0136 |          15.9075 |           0.2233 |
[32m[20221213 15:17:59 @agent_ppo2.py:185][0m |          -0.0188 |          15.7650 |           0.2234 |
[32m[20221213 15:17:59 @agent_ppo2.py:185][0m |          -0.0051 |          17.2660 |           0.2235 |
[32m[20221213 15:17:59 @agent_ppo2.py:185][0m |          -0.0160 |          15.5586 |           0.2236 |
[32m[20221213 15:17:59 @agent_ppo2.py:185][0m |          -0.0178 |          15.2913 |           0.2240 |
[32m[20221213 15:17:59 @agent_ppo2.py:185][0m |          -0.0199 |          15.1069 |           0.2238 |
[32m[20221213 15:17:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:17:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.89
[32m[20221213 15:17:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.75
[32m[20221213 15:17:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.78
[32m[20221213 15:17:59 @agent_ppo2.py:143][0m Total time:      25.07 min
[32m[20221213 15:17:59 @agent_ppo2.py:145][0m 2260992 total steps have happened
[32m[20221213 15:17:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1104 --------------------------#
[32m[20221213 15:17:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:17:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |           0.0026 |          17.8021 |           0.2301 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0061 |          17.0054 |           0.2298 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0101 |          16.7610 |           0.2293 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0067 |          16.7771 |           0.2293 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0109 |          16.5316 |           0.2294 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0154 |          16.4722 |           0.2297 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0131 |          16.3937 |           0.2293 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0164 |          16.3488 |           0.2293 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0173 |          16.2805 |           0.2292 |
[32m[20221213 15:18:00 @agent_ppo2.py:185][0m |          -0.0170 |          16.2342 |           0.2290 |
[32m[20221213 15:18:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:18:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.10
[32m[20221213 15:18:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.22
[32m[20221213 15:18:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.86
[32m[20221213 15:18:01 @agent_ppo2.py:143][0m Total time:      25.09 min
[32m[20221213 15:18:01 @agent_ppo2.py:145][0m 2263040 total steps have happened
[32m[20221213 15:18:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1105 --------------------------#
[32m[20221213 15:18:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:01 @agent_ppo2.py:185][0m |          -0.0002 |          16.9978 |           0.2327 |
[32m[20221213 15:18:01 @agent_ppo2.py:185][0m |          -0.0058 |          16.5920 |           0.2326 |
[32m[20221213 15:18:01 @agent_ppo2.py:185][0m |           0.0037 |          17.4046 |           0.2327 |
[32m[20221213 15:18:01 @agent_ppo2.py:185][0m |          -0.0084 |          16.1629 |           0.2327 |
[32m[20221213 15:18:01 @agent_ppo2.py:185][0m |           0.0043 |          17.7581 |           0.2327 |
[32m[20221213 15:18:01 @agent_ppo2.py:185][0m |          -0.0089 |          15.7806 |           0.2327 |
[32m[20221213 15:18:01 @agent_ppo2.py:185][0m |          -0.0124 |          15.6443 |           0.2327 |
[32m[20221213 15:18:02 @agent_ppo2.py:185][0m |          -0.0133 |          15.5392 |           0.2325 |
[32m[20221213 15:18:02 @agent_ppo2.py:185][0m |          -0.0125 |          15.4505 |           0.2323 |
[32m[20221213 15:18:02 @agent_ppo2.py:185][0m |           0.0014 |          16.5640 |           0.2322 |
[32m[20221213 15:18:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:18:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.88
[32m[20221213 15:18:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.30
[32m[20221213 15:18:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.55
[32m[20221213 15:18:02 @agent_ppo2.py:143][0m Total time:      25.11 min
[32m[20221213 15:18:02 @agent_ppo2.py:145][0m 2265088 total steps have happened
[32m[20221213 15:18:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1106 --------------------------#
[32m[20221213 15:18:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:02 @agent_ppo2.py:185][0m |           0.0007 |          17.6518 |           0.2322 |
[32m[20221213 15:18:02 @agent_ppo2.py:185][0m |          -0.0092 |          16.8590 |           0.2315 |
[32m[20221213 15:18:02 @agent_ppo2.py:185][0m |          -0.0024 |          18.6727 |           0.2320 |
[32m[20221213 15:18:03 @agent_ppo2.py:185][0m |          -0.0141 |          16.5139 |           0.2312 |
[32m[20221213 15:18:03 @agent_ppo2.py:185][0m |          -0.0130 |          16.4458 |           0.2317 |
[32m[20221213 15:18:03 @agent_ppo2.py:185][0m |          -0.0170 |          16.2956 |           0.2316 |
[32m[20221213 15:18:03 @agent_ppo2.py:185][0m |          -0.0160 |          16.1973 |           0.2317 |
[32m[20221213 15:18:03 @agent_ppo2.py:185][0m |          -0.0095 |          17.3858 |           0.2315 |
[32m[20221213 15:18:03 @agent_ppo2.py:185][0m |          -0.0069 |          17.6333 |           0.2310 |
[32m[20221213 15:18:03 @agent_ppo2.py:185][0m |          -0.0169 |          16.0697 |           0.2315 |
[32m[20221213 15:18:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:18:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.44
[32m[20221213 15:18:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.70
[32m[20221213 15:18:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.20
[32m[20221213 15:18:03 @agent_ppo2.py:143][0m Total time:      25.13 min
[32m[20221213 15:18:03 @agent_ppo2.py:145][0m 2267136 total steps have happened
[32m[20221213 15:18:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1107 --------------------------#
[32m[20221213 15:18:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0014 |          16.2184 |           0.2240 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0126 |          15.3775 |           0.2232 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0125 |          15.0832 |           0.2229 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0123 |          14.9077 |           0.2227 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0102 |          14.9717 |           0.2225 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0125 |          14.7060 |           0.2225 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0191 |          14.5437 |           0.2225 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0177 |          14.4846 |           0.2224 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0194 |          14.3812 |           0.2224 |
[32m[20221213 15:18:04 @agent_ppo2.py:185][0m |          -0.0215 |          14.3396 |           0.2224 |
[32m[20221213 15:18:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.36
[32m[20221213 15:18:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.08
[32m[20221213 15:18:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.82
[32m[20221213 15:18:05 @agent_ppo2.py:143][0m Total time:      25.16 min
[32m[20221213 15:18:05 @agent_ppo2.py:145][0m 2269184 total steps have happened
[32m[20221213 15:18:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1108 --------------------------#
[32m[20221213 15:18:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:05 @agent_ppo2.py:185][0m |          -0.0010 |          18.0381 |           0.2255 |
[32m[20221213 15:18:05 @agent_ppo2.py:185][0m |          -0.0087 |          17.6627 |           0.2257 |
[32m[20221213 15:18:05 @agent_ppo2.py:185][0m |          -0.0039 |          18.1138 |           0.2258 |
[32m[20221213 15:18:05 @agent_ppo2.py:185][0m |          -0.0088 |          17.7555 |           0.2261 |
[32m[20221213 15:18:05 @agent_ppo2.py:185][0m |          -0.0087 |          17.6678 |           0.2262 |
[32m[20221213 15:18:05 @agent_ppo2.py:185][0m |          -0.0080 |          18.1569 |           0.2263 |
[32m[20221213 15:18:06 @agent_ppo2.py:185][0m |          -0.0160 |          17.0968 |           0.2262 |
[32m[20221213 15:18:06 @agent_ppo2.py:185][0m |          -0.0154 |          16.9848 |           0.2265 |
[32m[20221213 15:18:06 @agent_ppo2.py:185][0m |          -0.0166 |          16.9576 |           0.2267 |
[32m[20221213 15:18:06 @agent_ppo2.py:185][0m |          -0.0189 |          16.8932 |           0.2266 |
[32m[20221213 15:18:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:18:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.90
[32m[20221213 15:18:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.21
[32m[20221213 15:18:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.59
[32m[20221213 15:18:06 @agent_ppo2.py:143][0m Total time:      25.18 min
[32m[20221213 15:18:06 @agent_ppo2.py:145][0m 2271232 total steps have happened
[32m[20221213 15:18:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1109 --------------------------#
[32m[20221213 15:18:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:06 @agent_ppo2.py:185][0m |          -0.0017 |          17.1296 |           0.2266 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0109 |          16.6180 |           0.2265 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0024 |          16.6817 |           0.2260 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0083 |          16.1787 |           0.2259 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0122 |          15.8085 |           0.2260 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0149 |          15.6008 |           0.2258 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0150 |          15.4624 |           0.2259 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0146 |          15.3516 |           0.2256 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0056 |          16.8697 |           0.2253 |
[32m[20221213 15:18:07 @agent_ppo2.py:185][0m |          -0.0146 |          15.1789 |           0.2254 |
[32m[20221213 15:18:07 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.26
[32m[20221213 15:18:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.11
[32m[20221213 15:18:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.66
[32m[20221213 15:18:07 @agent_ppo2.py:143][0m Total time:      25.20 min
[32m[20221213 15:18:07 @agent_ppo2.py:145][0m 2273280 total steps have happened
[32m[20221213 15:18:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1110 --------------------------#
[32m[20221213 15:18:08 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:18:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0017 |          17.5115 |           0.2284 |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0015 |          17.8834 |           0.2279 |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0116 |          16.7823 |           0.2277 |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0150 |          16.6439 |           0.2277 |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0184 |          16.5630 |           0.2283 |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0181 |          16.4471 |           0.2281 |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0192 |          16.3698 |           0.2280 |
[32m[20221213 15:18:08 @agent_ppo2.py:185][0m |          -0.0171 |          16.3122 |           0.2281 |
[32m[20221213 15:18:09 @agent_ppo2.py:185][0m |          -0.0174 |          16.2453 |           0.2282 |
[32m[20221213 15:18:09 @agent_ppo2.py:185][0m |          -0.0207 |          16.2086 |           0.2281 |
[32m[20221213 15:18:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:18:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.72
[32m[20221213 15:18:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.38
[32m[20221213 15:18:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 228.17
[32m[20221213 15:18:09 @agent_ppo2.py:143][0m Total time:      25.23 min
[32m[20221213 15:18:09 @agent_ppo2.py:145][0m 2275328 total steps have happened
[32m[20221213 15:18:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1111 --------------------------#
[32m[20221213 15:18:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:09 @agent_ppo2.py:185][0m |           0.0091 |          18.3373 |           0.2298 |
[32m[20221213 15:18:09 @agent_ppo2.py:185][0m |          -0.0077 |          16.4335 |           0.2290 |
[32m[20221213 15:18:09 @agent_ppo2.py:185][0m |          -0.0089 |          16.2799 |           0.2293 |
[32m[20221213 15:18:10 @agent_ppo2.py:185][0m |          -0.0058 |          16.4615 |           0.2292 |
[32m[20221213 15:18:10 @agent_ppo2.py:185][0m |          -0.0120 |          16.1059 |           0.2291 |
[32m[20221213 15:18:10 @agent_ppo2.py:185][0m |          -0.0054 |          16.4305 |           0.2290 |
[32m[20221213 15:18:10 @agent_ppo2.py:185][0m |          -0.0144 |          16.0234 |           0.2287 |
[32m[20221213 15:18:10 @agent_ppo2.py:185][0m |          -0.0140 |          15.9758 |           0.2288 |
[32m[20221213 15:18:10 @agent_ppo2.py:185][0m |          -0.0118 |          15.9485 |           0.2282 |
[32m[20221213 15:18:10 @agent_ppo2.py:185][0m |          -0.0168 |          15.8747 |           0.2285 |
[32m[20221213 15:18:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.62
[32m[20221213 15:18:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.98
[32m[20221213 15:18:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 234.50
[32m[20221213 15:18:10 @agent_ppo2.py:143][0m Total time:      25.25 min
[32m[20221213 15:18:10 @agent_ppo2.py:145][0m 2277376 total steps have happened
[32m[20221213 15:18:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1112 --------------------------#
[32m[20221213 15:18:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |           0.0099 |          18.8353 |           0.2239 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0061 |          16.7168 |           0.2234 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0115 |          16.4048 |           0.2234 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0050 |          16.5992 |           0.2232 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0147 |          16.1308 |           0.2231 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0166 |          16.0019 |           0.2229 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0162 |          15.8213 |           0.2228 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0172 |          15.7645 |           0.2226 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0182 |          15.6383 |           0.2224 |
[32m[20221213 15:18:11 @agent_ppo2.py:185][0m |          -0.0198 |          15.5092 |           0.2224 |
[32m[20221213 15:18:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:18:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.35
[32m[20221213 15:18:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.47
[32m[20221213 15:18:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.75
[32m[20221213 15:18:12 @agent_ppo2.py:143][0m Total time:      25.27 min
[32m[20221213 15:18:12 @agent_ppo2.py:145][0m 2279424 total steps have happened
[32m[20221213 15:18:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1113 --------------------------#
[32m[20221213 15:18:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:12 @agent_ppo2.py:185][0m |          -0.0006 |          17.1887 |           0.2262 |
[32m[20221213 15:18:12 @agent_ppo2.py:185][0m |          -0.0042 |          16.6522 |           0.2262 |
[32m[20221213 15:18:12 @agent_ppo2.py:185][0m |          -0.0107 |          16.3972 |           0.2261 |
[32m[20221213 15:18:12 @agent_ppo2.py:185][0m |          -0.0101 |          16.1498 |           0.2259 |
[32m[20221213 15:18:12 @agent_ppo2.py:185][0m |          -0.0127 |          15.9179 |           0.2261 |
[32m[20221213 15:18:12 @agent_ppo2.py:185][0m |          -0.0099 |          16.0441 |           0.2259 |
[32m[20221213 15:18:13 @agent_ppo2.py:185][0m |          -0.0112 |          15.6937 |           0.2260 |
[32m[20221213 15:18:13 @agent_ppo2.py:185][0m |          -0.0143 |          15.5027 |           0.2262 |
[32m[20221213 15:18:13 @agent_ppo2.py:185][0m |          -0.0156 |          15.3606 |           0.2260 |
[32m[20221213 15:18:13 @agent_ppo2.py:185][0m |          -0.0161 |          15.2013 |           0.2261 |
[32m[20221213 15:18:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.20
[32m[20221213 15:18:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.93
[32m[20221213 15:18:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.69
[32m[20221213 15:18:13 @agent_ppo2.py:143][0m Total time:      25.30 min
[32m[20221213 15:18:13 @agent_ppo2.py:145][0m 2281472 total steps have happened
[32m[20221213 15:18:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1114 --------------------------#
[32m[20221213 15:18:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:13 @agent_ppo2.py:185][0m |           0.0076 |          19.6584 |           0.2293 |
[32m[20221213 15:18:13 @agent_ppo2.py:185][0m |          -0.0011 |          18.5039 |           0.2293 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0145 |          17.2719 |           0.2294 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0134 |          17.1265 |           0.2295 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0179 |          17.0138 |           0.2293 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0189 |          16.9019 |           0.2293 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0171 |          17.0117 |           0.2292 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0080 |          18.6248 |           0.2295 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0184 |          16.7246 |           0.2292 |
[32m[20221213 15:18:14 @agent_ppo2.py:185][0m |          -0.0203 |          16.5972 |           0.2294 |
[32m[20221213 15:18:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:18:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.65
[32m[20221213 15:18:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.53
[32m[20221213 15:18:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 22.36
[32m[20221213 15:18:14 @agent_ppo2.py:143][0m Total time:      25.32 min
[32m[20221213 15:18:14 @agent_ppo2.py:145][0m 2283520 total steps have happened
[32m[20221213 15:18:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1115 --------------------------#
[32m[20221213 15:18:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0031 |          16.9783 |           0.2331 |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0112 |          16.4977 |           0.2328 |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0079 |          16.3370 |           0.2328 |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0112 |          16.0693 |           0.2329 |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0130 |          15.9211 |           0.2324 |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0126 |          15.8371 |           0.2326 |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0151 |          15.7417 |           0.2325 |
[32m[20221213 15:18:15 @agent_ppo2.py:185][0m |          -0.0144 |          15.6248 |           0.2325 |
[32m[20221213 15:18:16 @agent_ppo2.py:185][0m |          -0.0165 |          15.5400 |           0.2324 |
[32m[20221213 15:18:16 @agent_ppo2.py:185][0m |          -0.0167 |          15.4787 |           0.2324 |
[32m[20221213 15:18:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.39
[32m[20221213 15:18:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 241.44
[32m[20221213 15:18:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.97
[32m[20221213 15:18:16 @agent_ppo2.py:143][0m Total time:      25.34 min
[32m[20221213 15:18:16 @agent_ppo2.py:145][0m 2285568 total steps have happened
[32m[20221213 15:18:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1116 --------------------------#
[32m[20221213 15:18:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:16 @agent_ppo2.py:185][0m |          -0.0027 |          17.6155 |           0.2358 |
[32m[20221213 15:18:16 @agent_ppo2.py:185][0m |          -0.0064 |          17.1669 |           0.2354 |
[32m[20221213 15:18:16 @agent_ppo2.py:185][0m |          -0.0086 |          16.9671 |           0.2356 |
[32m[20221213 15:18:16 @agent_ppo2.py:185][0m |          -0.0117 |          16.8529 |           0.2357 |
[32m[20221213 15:18:17 @agent_ppo2.py:185][0m |           0.0009 |          17.8197 |           0.2354 |
[32m[20221213 15:18:17 @agent_ppo2.py:185][0m |          -0.0140 |          16.6944 |           0.2352 |
[32m[20221213 15:18:17 @agent_ppo2.py:185][0m |          -0.0052 |          17.6722 |           0.2351 |
[32m[20221213 15:18:17 @agent_ppo2.py:185][0m |          -0.0145 |          16.5133 |           0.2351 |
[32m[20221213 15:18:17 @agent_ppo2.py:185][0m |          -0.0083 |          17.5384 |           0.2350 |
[32m[20221213 15:18:17 @agent_ppo2.py:185][0m |          -0.0066 |          17.3739 |           0.2349 |
[32m[20221213 15:18:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:18:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.87
[32m[20221213 15:18:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.45
[32m[20221213 15:18:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.12
[32m[20221213 15:18:17 @agent_ppo2.py:143][0m Total time:      25.37 min
[32m[20221213 15:18:17 @agent_ppo2.py:145][0m 2287616 total steps have happened
[32m[20221213 15:18:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1117 --------------------------#
[32m[20221213 15:18:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0029 |          18.8711 |           0.2284 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0073 |          18.3258 |           0.2280 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0088 |          18.0567 |           0.2276 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0141 |          17.9131 |           0.2277 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0160 |          17.8650 |           0.2277 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0159 |          17.8341 |           0.2278 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0077 |          18.8765 |           0.2278 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0137 |          17.8858 |           0.2278 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0204 |          17.6359 |           0.2275 |
[32m[20221213 15:18:18 @agent_ppo2.py:185][0m |          -0.0174 |          17.5967 |           0.2276 |
[32m[20221213 15:18:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:18:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.46
[32m[20221213 15:18:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.83
[32m[20221213 15:18:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.98
[32m[20221213 15:18:19 @agent_ppo2.py:143][0m Total time:      25.39 min
[32m[20221213 15:18:19 @agent_ppo2.py:145][0m 2289664 total steps have happened
[32m[20221213 15:18:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1118 --------------------------#
[32m[20221213 15:18:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:19 @agent_ppo2.py:185][0m |          -0.0027 |          16.9908 |           0.2301 |
[32m[20221213 15:18:19 @agent_ppo2.py:185][0m |          -0.0067 |          16.4998 |           0.2304 |
[32m[20221213 15:18:19 @agent_ppo2.py:185][0m |          -0.0076 |          16.2563 |           0.2303 |
[32m[20221213 15:18:19 @agent_ppo2.py:185][0m |          -0.0103 |          16.0965 |           0.2303 |
[32m[20221213 15:18:19 @agent_ppo2.py:185][0m |          -0.0123 |          15.9800 |           0.2300 |
[32m[20221213 15:18:19 @agent_ppo2.py:185][0m |          -0.0100 |          15.9951 |           0.2302 |
[32m[20221213 15:18:20 @agent_ppo2.py:185][0m |          -0.0138 |          15.7297 |           0.2302 |
[32m[20221213 15:18:20 @agent_ppo2.py:185][0m |          -0.0169 |          15.6432 |           0.2301 |
[32m[20221213 15:18:20 @agent_ppo2.py:185][0m |          -0.0087 |          15.8047 |           0.2300 |
[32m[20221213 15:18:20 @agent_ppo2.py:185][0m |          -0.0087 |          16.2544 |           0.2300 |
[32m[20221213 15:18:20 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.12
[32m[20221213 15:18:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.38
[32m[20221213 15:18:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.20
[32m[20221213 15:18:20 @agent_ppo2.py:143][0m Total time:      25.41 min
[32m[20221213 15:18:20 @agent_ppo2.py:145][0m 2291712 total steps have happened
[32m[20221213 15:18:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1119 --------------------------#
[32m[20221213 15:18:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:20 @agent_ppo2.py:185][0m |          -0.0008 |          15.9621 |           0.2334 |
[32m[20221213 15:18:20 @agent_ppo2.py:185][0m |          -0.0076 |          15.0780 |           0.2335 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0124 |          14.7104 |           0.2336 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0119 |          14.4948 |           0.2337 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0142 |          14.3741 |           0.2338 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0144 |          14.1119 |           0.2337 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0152 |          13.8929 |           0.2339 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0162 |          13.6794 |           0.2339 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0174 |          13.4970 |           0.2337 |
[32m[20221213 15:18:21 @agent_ppo2.py:185][0m |          -0.0172 |          13.3128 |           0.2339 |
[32m[20221213 15:18:21 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:18:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.78
[32m[20221213 15:18:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.26
[32m[20221213 15:18:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.19
[32m[20221213 15:18:21 @agent_ppo2.py:143][0m Total time:      25.44 min
[32m[20221213 15:18:21 @agent_ppo2.py:145][0m 2293760 total steps have happened
[32m[20221213 15:18:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1120 --------------------------#
[32m[20221213 15:18:22 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:18:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0024 |          17.8100 |           0.2380 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0009 |          18.7558 |           0.2372 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0092 |          17.0453 |           0.2363 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0120 |          16.7269 |           0.2368 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0148 |          16.5000 |           0.2371 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0152 |          16.2997 |           0.2370 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0146 |          16.1551 |           0.2368 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0176 |          16.0623 |           0.2370 |
[32m[20221213 15:18:22 @agent_ppo2.py:185][0m |          -0.0170 |          15.9946 |           0.2367 |
[32m[20221213 15:18:23 @agent_ppo2.py:185][0m |          -0.0210 |          15.8608 |           0.2368 |
[32m[20221213 15:18:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.21
[32m[20221213 15:18:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.70
[32m[20221213 15:18:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.90
[32m[20221213 15:18:23 @agent_ppo2.py:143][0m Total time:      25.46 min
[32m[20221213 15:18:23 @agent_ppo2.py:145][0m 2295808 total steps have happened
[32m[20221213 15:18:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1121 --------------------------#
[32m[20221213 15:18:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:23 @agent_ppo2.py:185][0m |          -0.0034 |          17.3719 |           0.2340 |
[32m[20221213 15:18:23 @agent_ppo2.py:185][0m |          -0.0062 |          16.8019 |           0.2340 |
[32m[20221213 15:18:23 @agent_ppo2.py:185][0m |          -0.0110 |          16.4994 |           0.2340 |
[32m[20221213 15:18:23 @agent_ppo2.py:185][0m |          -0.0117 |          16.1862 |           0.2341 |
[32m[20221213 15:18:24 @agent_ppo2.py:185][0m |          -0.0112 |          16.1878 |           0.2342 |
[32m[20221213 15:18:24 @agent_ppo2.py:185][0m |          -0.0163 |          15.8219 |           0.2343 |
[32m[20221213 15:18:24 @agent_ppo2.py:185][0m |          -0.0158 |          15.6880 |           0.2342 |
[32m[20221213 15:18:24 @agent_ppo2.py:185][0m |          -0.0189 |          15.6429 |           0.2344 |
[32m[20221213 15:18:24 @agent_ppo2.py:185][0m |          -0.0185 |          15.4649 |           0.2347 |
[32m[20221213 15:18:24 @agent_ppo2.py:185][0m |          -0.0167 |          15.4340 |           0.2345 |
[32m[20221213 15:18:24 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:18:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.73
[32m[20221213 15:18:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.33
[32m[20221213 15:18:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.41
[32m[20221213 15:18:24 @agent_ppo2.py:143][0m Total time:      25.48 min
[32m[20221213 15:18:24 @agent_ppo2.py:145][0m 2297856 total steps have happened
[32m[20221213 15:18:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1122 --------------------------#
[32m[20221213 15:18:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |           0.0038 |          18.7753 |           0.2337 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0084 |          17.9027 |           0.2331 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0114 |          17.6969 |           0.2333 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0147 |          17.5677 |           0.2332 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0131 |          17.4754 |           0.2329 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0085 |          18.6115 |           0.2332 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0176 |          17.3147 |           0.2332 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0194 |          17.2338 |           0.2330 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0179 |          17.2312 |           0.2328 |
[32m[20221213 15:18:25 @agent_ppo2.py:185][0m |          -0.0179 |          17.1310 |           0.2328 |
[32m[20221213 15:18:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.61
[32m[20221213 15:18:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.95
[32m[20221213 15:18:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.35
[32m[20221213 15:18:26 @agent_ppo2.py:143][0m Total time:      25.51 min
[32m[20221213 15:18:26 @agent_ppo2.py:145][0m 2299904 total steps have happened
[32m[20221213 15:18:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1123 --------------------------#
[32m[20221213 15:18:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:26 @agent_ppo2.py:185][0m |          -0.0020 |          16.8427 |           0.2281 |
[32m[20221213 15:18:26 @agent_ppo2.py:185][0m |           0.0046 |          17.6976 |           0.2279 |
[32m[20221213 15:18:26 @agent_ppo2.py:185][0m |          -0.0085 |          16.4601 |           0.2282 |
[32m[20221213 15:18:26 @agent_ppo2.py:185][0m |          -0.0110 |          16.2986 |           0.2281 |
[32m[20221213 15:18:26 @agent_ppo2.py:185][0m |          -0.0077 |          16.7290 |           0.2279 |
[32m[20221213 15:18:26 @agent_ppo2.py:185][0m |          -0.0139 |          16.1361 |           0.2278 |
[32m[20221213 15:18:26 @agent_ppo2.py:185][0m |          -0.0093 |          16.2632 |           0.2279 |
[32m[20221213 15:18:27 @agent_ppo2.py:185][0m |          -0.0092 |          16.5585 |           0.2278 |
[32m[20221213 15:18:27 @agent_ppo2.py:185][0m |          -0.0167 |          16.0198 |           0.2276 |
[32m[20221213 15:18:27 @agent_ppo2.py:185][0m |          -0.0177 |          15.9825 |           0.2276 |
[32m[20221213 15:18:27 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:18:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.40
[32m[20221213 15:18:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.42
[32m[20221213 15:18:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.27
[32m[20221213 15:18:27 @agent_ppo2.py:143][0m Total time:      25.53 min
[32m[20221213 15:18:27 @agent_ppo2.py:145][0m 2301952 total steps have happened
[32m[20221213 15:18:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1124 --------------------------#
[32m[20221213 15:18:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:27 @agent_ppo2.py:185][0m |          -0.0046 |          17.1869 |           0.2351 |
[32m[20221213 15:18:27 @agent_ppo2.py:185][0m |          -0.0099 |          16.8731 |           0.2347 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0122 |          16.6937 |           0.2345 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0052 |          17.0284 |           0.2344 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0145 |          16.4219 |           0.2342 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0145 |          16.2987 |           0.2340 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0178 |          16.2614 |           0.2339 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0173 |          16.1961 |           0.2337 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0185 |          16.0739 |           0.2336 |
[32m[20221213 15:18:28 @agent_ppo2.py:185][0m |          -0.0060 |          18.1098 |           0.2331 |
[32m[20221213 15:18:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:18:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.76
[32m[20221213 15:18:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.86
[32m[20221213 15:18:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.96
[32m[20221213 15:18:28 @agent_ppo2.py:143][0m Total time:      25.55 min
[32m[20221213 15:18:28 @agent_ppo2.py:145][0m 2304000 total steps have happened
[32m[20221213 15:18:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1125 --------------------------#
[32m[20221213 15:18:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0021 |          17.2305 |           0.2344 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0064 |          16.7134 |           0.2346 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0106 |          16.4885 |           0.2344 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0138 |          16.4441 |           0.2348 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0112 |          16.2759 |           0.2350 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0064 |          16.9111 |           0.2352 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0149 |          16.1659 |           0.2353 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0096 |          16.2263 |           0.2356 |
[32m[20221213 15:18:29 @agent_ppo2.py:185][0m |          -0.0140 |          15.9924 |           0.2355 |
[32m[20221213 15:18:30 @agent_ppo2.py:185][0m |          -0.0183 |          15.9018 |           0.2358 |
[32m[20221213 15:18:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.33
[32m[20221213 15:18:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.54
[32m[20221213 15:18:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.47
[32m[20221213 15:18:30 @agent_ppo2.py:143][0m Total time:      25.58 min
[32m[20221213 15:18:30 @agent_ppo2.py:145][0m 2306048 total steps have happened
[32m[20221213 15:18:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1126 --------------------------#
[32m[20221213 15:18:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:30 @agent_ppo2.py:185][0m |          -0.0004 |          17.3473 |           0.2435 |
[32m[20221213 15:18:30 @agent_ppo2.py:185][0m |          -0.0069 |          17.0432 |           0.2436 |
[32m[20221213 15:18:30 @agent_ppo2.py:185][0m |          -0.0099 |          16.8347 |           0.2435 |
[32m[20221213 15:18:30 @agent_ppo2.py:185][0m |          -0.0124 |          16.7168 |           0.2435 |
[32m[20221213 15:18:30 @agent_ppo2.py:185][0m |          -0.0104 |          16.6863 |           0.2432 |
[32m[20221213 15:18:31 @agent_ppo2.py:185][0m |          -0.0135 |          16.5617 |           0.2434 |
[32m[20221213 15:18:31 @agent_ppo2.py:185][0m |          -0.0032 |          17.6314 |           0.2431 |
[32m[20221213 15:18:31 @agent_ppo2.py:185][0m |          -0.0112 |          16.5859 |           0.2431 |
[32m[20221213 15:18:31 @agent_ppo2.py:185][0m |          -0.0171 |          16.3752 |           0.2431 |
[32m[20221213 15:18:31 @agent_ppo2.py:185][0m |          -0.0144 |          16.4212 |           0.2431 |
[32m[20221213 15:18:31 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:18:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.83
[32m[20221213 15:18:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.80
[32m[20221213 15:18:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.68
[32m[20221213 15:18:31 @agent_ppo2.py:143][0m Total time:      25.60 min
[32m[20221213 15:18:31 @agent_ppo2.py:145][0m 2308096 total steps have happened
[32m[20221213 15:18:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1127 --------------------------#
[32m[20221213 15:18:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0016 |          16.6552 |           0.2436 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0089 |          16.0275 |           0.2435 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0053 |          15.7771 |           0.2434 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0104 |          15.2342 |           0.2431 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0147 |          14.8281 |           0.2431 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0134 |          14.5692 |           0.2433 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0166 |          14.3709 |           0.2430 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0157 |          14.2101 |           0.2431 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0176 |          14.1111 |           0.2430 |
[32m[20221213 15:18:32 @agent_ppo2.py:185][0m |          -0.0158 |          13.9849 |           0.2430 |
[32m[20221213 15:18:32 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.60
[32m[20221213 15:18:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.50
[32m[20221213 15:18:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.82
[32m[20221213 15:18:33 @agent_ppo2.py:143][0m Total time:      25.62 min
[32m[20221213 15:18:33 @agent_ppo2.py:145][0m 2310144 total steps have happened
[32m[20221213 15:18:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1128 --------------------------#
[32m[20221213 15:18:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:33 @agent_ppo2.py:185][0m |          -0.0021 |          18.1563 |           0.2426 |
[32m[20221213 15:18:33 @agent_ppo2.py:185][0m |          -0.0043 |          16.3105 |           0.2421 |
[32m[20221213 15:18:33 @agent_ppo2.py:185][0m |          -0.0092 |          15.8892 |           0.2418 |
[32m[20221213 15:18:33 @agent_ppo2.py:185][0m |          -0.0076 |          15.6211 |           0.2419 |
[32m[20221213 15:18:33 @agent_ppo2.py:185][0m |          -0.0091 |          15.4424 |           0.2419 |
[32m[20221213 15:18:33 @agent_ppo2.py:185][0m |          -0.0115 |          15.3017 |           0.2420 |
[32m[20221213 15:18:33 @agent_ppo2.py:185][0m |          -0.0148 |          15.1302 |           0.2421 |
[32m[20221213 15:18:34 @agent_ppo2.py:185][0m |          -0.0146 |          15.0388 |           0.2419 |
[32m[20221213 15:18:34 @agent_ppo2.py:185][0m |          -0.0140 |          14.9051 |           0.2422 |
[32m[20221213 15:18:34 @agent_ppo2.py:185][0m |          -0.0099 |          15.4435 |           0.2421 |
[32m[20221213 15:18:34 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 176.01
[32m[20221213 15:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 239.96
[32m[20221213 15:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.62
[32m[20221213 15:18:34 @agent_ppo2.py:143][0m Total time:      25.65 min
[32m[20221213 15:18:34 @agent_ppo2.py:145][0m 2312192 total steps have happened
[32m[20221213 15:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1129 --------------------------#
[32m[20221213 15:18:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:34 @agent_ppo2.py:185][0m |           0.0049 |          18.5668 |           0.2492 |
[32m[20221213 15:18:34 @agent_ppo2.py:185][0m |          -0.0041 |          17.9562 |           0.2492 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0086 |          17.4971 |           0.2496 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0093 |          17.4190 |           0.2495 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0135 |          17.2968 |           0.2498 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0109 |          17.5102 |           0.2499 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0061 |          17.5107 |           0.2499 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0165 |          17.1285 |           0.2502 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0093 |          17.3539 |           0.2503 |
[32m[20221213 15:18:35 @agent_ppo2.py:185][0m |          -0.0167 |          17.0250 |           0.2503 |
[32m[20221213 15:18:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.43
[32m[20221213 15:18:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.82
[32m[20221213 15:18:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.85
[32m[20221213 15:18:35 @agent_ppo2.py:143][0m Total time:      25.67 min
[32m[20221213 15:18:35 @agent_ppo2.py:145][0m 2314240 total steps have happened
[32m[20221213 15:18:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1130 --------------------------#
[32m[20221213 15:18:36 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:18:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0004 |          17.0997 |           0.2533 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0034 |          16.6921 |           0.2531 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0102 |          16.4708 |           0.2533 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0097 |          16.4083 |           0.2534 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0142 |          16.2134 |           0.2532 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0150 |          16.1009 |           0.2535 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0105 |          16.1953 |           0.2535 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0122 |          16.3889 |           0.2535 |
[32m[20221213 15:18:36 @agent_ppo2.py:185][0m |          -0.0180 |          15.9299 |           0.2534 |
[32m[20221213 15:18:37 @agent_ppo2.py:185][0m |          -0.0140 |          15.8704 |           0.2537 |
[32m[20221213 15:18:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:18:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.16
[32m[20221213 15:18:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.20
[32m[20221213 15:18:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.12
[32m[20221213 15:18:37 @agent_ppo2.py:143][0m Total time:      25.69 min
[32m[20221213 15:18:37 @agent_ppo2.py:145][0m 2316288 total steps have happened
[32m[20221213 15:18:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1131 --------------------------#
[32m[20221213 15:18:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:37 @agent_ppo2.py:185][0m |           0.0120 |          19.9162 |           0.2467 |
[32m[20221213 15:18:37 @agent_ppo2.py:185][0m |          -0.0063 |          17.6249 |           0.2465 |
[32m[20221213 15:18:37 @agent_ppo2.py:185][0m |          -0.0082 |          17.5006 |           0.2461 |
[32m[20221213 15:18:37 @agent_ppo2.py:185][0m |          -0.0109 |          17.3637 |           0.2462 |
[32m[20221213 15:18:37 @agent_ppo2.py:185][0m |          -0.0125 |          17.3034 |           0.2461 |
[32m[20221213 15:18:38 @agent_ppo2.py:185][0m |          -0.0159 |          17.2314 |           0.2463 |
[32m[20221213 15:18:38 @agent_ppo2.py:185][0m |          -0.0172 |          17.2182 |           0.2463 |
[32m[20221213 15:18:38 @agent_ppo2.py:185][0m |          -0.0060 |          18.6412 |           0.2464 |
[32m[20221213 15:18:38 @agent_ppo2.py:185][0m |          -0.0124 |          17.5264 |           0.2460 |
[32m[20221213 15:18:38 @agent_ppo2.py:185][0m |          -0.0170 |          17.0813 |           0.2462 |
[32m[20221213 15:18:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.46
[32m[20221213 15:18:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.04
[32m[20221213 15:18:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.31
[32m[20221213 15:18:38 @agent_ppo2.py:143][0m Total time:      25.72 min
[32m[20221213 15:18:38 @agent_ppo2.py:145][0m 2318336 total steps have happened
[32m[20221213 15:18:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1132 --------------------------#
[32m[20221213 15:18:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |           0.0139 |          19.6047 |           0.2502 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0080 |          17.0608 |           0.2495 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0096 |          16.7746 |           0.2496 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0106 |          16.5829 |           0.2497 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0127 |          16.5022 |           0.2496 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0093 |          16.5992 |           0.2497 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0140 |          16.2209 |           0.2500 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0149 |          16.1177 |           0.2497 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0171 |          16.0333 |           0.2499 |
[32m[20221213 15:18:39 @agent_ppo2.py:185][0m |          -0.0168 |          15.9532 |           0.2502 |
[32m[20221213 15:18:39 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:18:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.94
[32m[20221213 15:18:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.90
[32m[20221213 15:18:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.40
[32m[20221213 15:18:40 @agent_ppo2.py:143][0m Total time:      25.74 min
[32m[20221213 15:18:40 @agent_ppo2.py:145][0m 2320384 total steps have happened
[32m[20221213 15:18:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1133 --------------------------#
[32m[20221213 15:18:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:40 @agent_ppo2.py:185][0m |           0.0004 |          17.4562 |           0.2551 |
[32m[20221213 15:18:40 @agent_ppo2.py:185][0m |          -0.0072 |          16.9193 |           0.2545 |
[32m[20221213 15:18:40 @agent_ppo2.py:185][0m |          -0.0101 |          16.6542 |           0.2544 |
[32m[20221213 15:18:40 @agent_ppo2.py:185][0m |          -0.0133 |          16.4414 |           0.2542 |
[32m[20221213 15:18:40 @agent_ppo2.py:185][0m |          -0.0130 |          16.2896 |           0.2539 |
[32m[20221213 15:18:40 @agent_ppo2.py:185][0m |          -0.0171 |          16.1097 |           0.2536 |
[32m[20221213 15:18:40 @agent_ppo2.py:185][0m |          -0.0121 |          16.0728 |           0.2536 |
[32m[20221213 15:18:41 @agent_ppo2.py:185][0m |          -0.0173 |          15.9293 |           0.2533 |
[32m[20221213 15:18:41 @agent_ppo2.py:185][0m |          -0.0170 |          15.7724 |           0.2534 |
[32m[20221213 15:18:41 @agent_ppo2.py:185][0m |           0.0032 |          17.8496 |           0.2532 |
[32m[20221213 15:18:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.81
[32m[20221213 15:18:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.93
[32m[20221213 15:18:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.05
[32m[20221213 15:18:41 @agent_ppo2.py:143][0m Total time:      25.76 min
[32m[20221213 15:18:41 @agent_ppo2.py:145][0m 2322432 total steps have happened
[32m[20221213 15:18:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1134 --------------------------#
[32m[20221213 15:18:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:41 @agent_ppo2.py:185][0m |           0.0073 |          19.1787 |           0.2527 |
[32m[20221213 15:18:41 @agent_ppo2.py:185][0m |          -0.0069 |          17.5956 |           0.2514 |
[32m[20221213 15:18:41 @agent_ppo2.py:185][0m |          -0.0104 |          17.3894 |           0.2518 |
[32m[20221213 15:18:42 @agent_ppo2.py:185][0m |          -0.0119 |          17.3035 |           0.2518 |
[32m[20221213 15:18:42 @agent_ppo2.py:185][0m |          -0.0131 |          17.0887 |           0.2519 |
[32m[20221213 15:18:42 @agent_ppo2.py:185][0m |          -0.0151 |          16.9558 |           0.2517 |
[32m[20221213 15:18:42 @agent_ppo2.py:185][0m |          -0.0125 |          16.9709 |           0.2518 |
[32m[20221213 15:18:42 @agent_ppo2.py:185][0m |          -0.0183 |          16.7242 |           0.2518 |
[32m[20221213 15:18:42 @agent_ppo2.py:185][0m |          -0.0075 |          18.8664 |           0.2520 |
[32m[20221213 15:18:42 @agent_ppo2.py:185][0m |          -0.0176 |          16.6417 |           0.2519 |
[32m[20221213 15:18:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:18:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.24
[32m[20221213 15:18:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.09
[32m[20221213 15:18:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.08
[32m[20221213 15:18:42 @agent_ppo2.py:143][0m Total time:      25.79 min
[32m[20221213 15:18:42 @agent_ppo2.py:145][0m 2324480 total steps have happened
[32m[20221213 15:18:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1135 --------------------------#
[32m[20221213 15:18:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |           0.0003 |          17.2134 |           0.2511 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0037 |          17.0098 |           0.2510 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0103 |          16.7950 |           0.2509 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0104 |          16.6637 |           0.2510 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0076 |          16.7170 |           0.2512 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0136 |          16.4091 |           0.2511 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0121 |          16.3078 |           0.2508 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0164 |          16.2605 |           0.2509 |
[32m[20221213 15:18:43 @agent_ppo2.py:185][0m |          -0.0168 |          16.1590 |           0.2510 |
[32m[20221213 15:18:44 @agent_ppo2.py:185][0m |          -0.0025 |          18.3245 |           0.2509 |
[32m[20221213 15:18:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:18:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.96
[32m[20221213 15:18:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.26
[32m[20221213 15:18:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.20
[32m[20221213 15:18:44 @agent_ppo2.py:143][0m Total time:      25.81 min
[32m[20221213 15:18:44 @agent_ppo2.py:145][0m 2326528 total steps have happened
[32m[20221213 15:18:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1136 --------------------------#
[32m[20221213 15:18:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:44 @agent_ppo2.py:185][0m |           0.0047 |          19.4271 |           0.2541 |
[32m[20221213 15:18:44 @agent_ppo2.py:185][0m |          -0.0079 |          18.4331 |           0.2542 |
[32m[20221213 15:18:44 @agent_ppo2.py:185][0m |          -0.0094 |          18.2427 |           0.2538 |
[32m[20221213 15:18:44 @agent_ppo2.py:185][0m |          -0.0112 |          18.1150 |           0.2534 |
[32m[20221213 15:18:44 @agent_ppo2.py:185][0m |          -0.0022 |          18.7727 |           0.2535 |
[32m[20221213 15:18:45 @agent_ppo2.py:185][0m |          -0.0164 |          17.9671 |           0.2534 |
[32m[20221213 15:18:45 @agent_ppo2.py:185][0m |          -0.0156 |          17.8909 |           0.2535 |
[32m[20221213 15:18:45 @agent_ppo2.py:185][0m |          -0.0152 |          17.8314 |           0.2537 |
[32m[20221213 15:18:45 @agent_ppo2.py:185][0m |          -0.0165 |          17.8322 |           0.2538 |
[32m[20221213 15:18:45 @agent_ppo2.py:185][0m |          -0.0170 |          17.6980 |           0.2538 |
[32m[20221213 15:18:45 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 204.62
[32m[20221213 15:18:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.02
[32m[20221213 15:18:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.92
[32m[20221213 15:18:45 @agent_ppo2.py:143][0m Total time:      25.83 min
[32m[20221213 15:18:45 @agent_ppo2.py:145][0m 2328576 total steps have happened
[32m[20221213 15:18:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1137 --------------------------#
[32m[20221213 15:18:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:45 @agent_ppo2.py:185][0m |          -0.0035 |          17.4029 |           0.2591 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0077 |          17.1285 |           0.2586 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0128 |          16.8554 |           0.2585 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0151 |          16.6549 |           0.2583 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0159 |          16.5227 |           0.2581 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0149 |          16.3526 |           0.2578 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0184 |          16.2092 |           0.2579 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0094 |          17.7694 |           0.2579 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0201 |          16.0511 |           0.2575 |
[32m[20221213 15:18:46 @agent_ppo2.py:185][0m |          -0.0162 |          15.8960 |           0.2575 |
[32m[20221213 15:18:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:18:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.50
[32m[20221213 15:18:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.16
[32m[20221213 15:18:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.32
[32m[20221213 15:18:47 @agent_ppo2.py:143][0m Total time:      25.86 min
[32m[20221213 15:18:47 @agent_ppo2.py:145][0m 2330624 total steps have happened
[32m[20221213 15:18:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1138 --------------------------#
[32m[20221213 15:18:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:47 @agent_ppo2.py:185][0m |          -0.0004 |          17.6119 |           0.2474 |
[32m[20221213 15:18:47 @agent_ppo2.py:185][0m |          -0.0051 |          17.0550 |           0.2470 |
[32m[20221213 15:18:47 @agent_ppo2.py:185][0m |           0.0004 |          17.9903 |           0.2469 |
[32m[20221213 15:18:47 @agent_ppo2.py:185][0m |          -0.0044 |          17.0826 |           0.2466 |
[32m[20221213 15:18:47 @agent_ppo2.py:185][0m |          -0.0114 |          16.6696 |           0.2465 |
[32m[20221213 15:18:47 @agent_ppo2.py:185][0m |          -0.0057 |          17.1988 |           0.2464 |
[32m[20221213 15:18:47 @agent_ppo2.py:185][0m |          -0.0068 |          16.6690 |           0.2459 |
[32m[20221213 15:18:48 @agent_ppo2.py:185][0m |          -0.0062 |          16.9257 |           0.2461 |
[32m[20221213 15:18:48 @agent_ppo2.py:185][0m |          -0.0069 |          17.4867 |           0.2461 |
[32m[20221213 15:18:48 @agent_ppo2.py:185][0m |          -0.0140 |          16.4186 |           0.2459 |
[32m[20221213 15:18:48 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.00
[32m[20221213 15:18:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.06
[32m[20221213 15:18:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 21.64
[32m[20221213 15:18:48 @agent_ppo2.py:143][0m Total time:      25.88 min
[32m[20221213 15:18:48 @agent_ppo2.py:145][0m 2332672 total steps have happened
[32m[20221213 15:18:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1139 --------------------------#
[32m[20221213 15:18:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:48 @agent_ppo2.py:185][0m |          -0.0016 |          17.4151 |           0.2513 |
[32m[20221213 15:18:48 @agent_ppo2.py:185][0m |          -0.0062 |          17.1616 |           0.2512 |
[32m[20221213 15:18:48 @agent_ppo2.py:185][0m |          -0.0103 |          17.0929 |           0.2512 |
[32m[20221213 15:18:49 @agent_ppo2.py:185][0m |          -0.0122 |          17.0008 |           0.2510 |
[32m[20221213 15:18:49 @agent_ppo2.py:185][0m |          -0.0134 |          16.9821 |           0.2507 |
[32m[20221213 15:18:49 @agent_ppo2.py:185][0m |          -0.0049 |          17.8521 |           0.2507 |
[32m[20221213 15:18:49 @agent_ppo2.py:185][0m |          -0.0119 |          16.9237 |           0.2502 |
[32m[20221213 15:18:49 @agent_ppo2.py:185][0m |          -0.0159 |          16.8183 |           0.2507 |
[32m[20221213 15:18:49 @agent_ppo2.py:185][0m |          -0.0111 |          17.0797 |           0.2504 |
[32m[20221213 15:18:49 @agent_ppo2.py:185][0m |          -0.0057 |          17.7362 |           0.2504 |
[32m[20221213 15:18:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:18:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.76
[32m[20221213 15:18:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.01
[32m[20221213 15:18:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.24
[32m[20221213 15:18:49 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 288.24
[32m[20221213 15:18:49 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 288.24
[32m[20221213 15:18:49 @agent_ppo2.py:143][0m Total time:      25.90 min
[32m[20221213 15:18:49 @agent_ppo2.py:145][0m 2334720 total steps have happened
[32m[20221213 15:18:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1140 --------------------------#
[32m[20221213 15:18:50 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:18:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0003 |          16.5047 |           0.2549 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0045 |          16.1260 |           0.2548 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0099 |          15.8571 |           0.2545 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0126 |          15.7200 |           0.2543 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0125 |          15.5345 |           0.2542 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0143 |          15.4066 |           0.2539 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0151 |          15.3004 |           0.2537 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0168 |          15.2175 |           0.2538 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0169 |          15.0868 |           0.2535 |
[32m[20221213 15:18:50 @agent_ppo2.py:185][0m |          -0.0170 |          15.0541 |           0.2535 |
[32m[20221213 15:18:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.27
[32m[20221213 15:18:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.96
[32m[20221213 15:18:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.12
[32m[20221213 15:18:51 @agent_ppo2.py:143][0m Total time:      25.92 min
[32m[20221213 15:18:51 @agent_ppo2.py:145][0m 2336768 total steps have happened
[32m[20221213 15:18:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1141 --------------------------#
[32m[20221213 15:18:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:51 @agent_ppo2.py:185][0m |           0.0086 |          15.3608 |           0.2477 |
[32m[20221213 15:18:51 @agent_ppo2.py:185][0m |          -0.0066 |          14.3182 |           0.2470 |
[32m[20221213 15:18:51 @agent_ppo2.py:185][0m |          -0.0122 |          14.2343 |           0.2474 |
[32m[20221213 15:18:51 @agent_ppo2.py:185][0m |           0.0028 |          14.6368 |           0.2473 |
[32m[20221213 15:18:51 @agent_ppo2.py:185][0m |          -0.0084 |          14.0440 |           0.2476 |
[32m[20221213 15:18:52 @agent_ppo2.py:185][0m |          -0.0121 |          14.0480 |           0.2474 |
[32m[20221213 15:18:52 @agent_ppo2.py:185][0m |          -0.0125 |          13.8982 |           0.2474 |
[32m[20221213 15:18:52 @agent_ppo2.py:185][0m |          -0.0117 |          13.9294 |           0.2476 |
[32m[20221213 15:18:52 @agent_ppo2.py:185][0m |          -0.0173 |          13.8066 |           0.2474 |
[32m[20221213 15:18:52 @agent_ppo2.py:185][0m |          -0.0148 |          13.7544 |           0.2476 |
[32m[20221213 15:18:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:18:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.61
[32m[20221213 15:18:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.89
[32m[20221213 15:18:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.02
[32m[20221213 15:18:52 @agent_ppo2.py:143][0m Total time:      25.95 min
[32m[20221213 15:18:52 @agent_ppo2.py:145][0m 2338816 total steps have happened
[32m[20221213 15:18:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1142 --------------------------#
[32m[20221213 15:18:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:52 @agent_ppo2.py:185][0m |          -0.0016 |          17.1224 |           0.2524 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0053 |          16.9465 |           0.2515 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0089 |          16.7224 |           0.2511 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0104 |          16.6301 |           0.2510 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0120 |          16.5983 |           0.2510 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0145 |          16.5380 |           0.2507 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0134 |          16.4879 |           0.2507 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0140 |          16.4136 |           0.2506 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0150 |          16.3565 |           0.2507 |
[32m[20221213 15:18:53 @agent_ppo2.py:185][0m |          -0.0156 |          16.3302 |           0.2506 |
[32m[20221213 15:18:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:18:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.14
[32m[20221213 15:18:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.78
[32m[20221213 15:18:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 237.45
[32m[20221213 15:18:53 @agent_ppo2.py:143][0m Total time:      25.97 min
[32m[20221213 15:18:53 @agent_ppo2.py:145][0m 2340864 total steps have happened
[32m[20221213 15:18:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1143 --------------------------#
[32m[20221213 15:18:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0006 |          16.9011 |           0.2456 |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0055 |          16.7238 |           0.2452 |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0047 |          16.6811 |           0.2451 |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0119 |          16.4560 |           0.2448 |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0092 |          16.5025 |           0.2449 |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0129 |          16.3343 |           0.2447 |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0071 |          16.8577 |           0.2449 |
[32m[20221213 15:18:54 @agent_ppo2.py:185][0m |          -0.0158 |          16.2787 |           0.2448 |
[32m[20221213 15:18:55 @agent_ppo2.py:185][0m |          -0.0167 |          16.1984 |           0.2450 |
[32m[20221213 15:18:55 @agent_ppo2.py:185][0m |          -0.0158 |          16.1819 |           0.2448 |
[32m[20221213 15:18:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.12
[32m[20221213 15:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.44
[32m[20221213 15:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.30
[32m[20221213 15:18:55 @agent_ppo2.py:143][0m Total time:      25.99 min
[32m[20221213 15:18:55 @agent_ppo2.py:145][0m 2342912 total steps have happened
[32m[20221213 15:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1144 --------------------------#
[32m[20221213 15:18:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:55 @agent_ppo2.py:185][0m |           0.0054 |          17.7683 |           0.2485 |
[32m[20221213 15:18:55 @agent_ppo2.py:185][0m |          -0.0049 |          17.0477 |           0.2482 |
[32m[20221213 15:18:55 @agent_ppo2.py:185][0m |          -0.0082 |          16.7861 |           0.2487 |
[32m[20221213 15:18:56 @agent_ppo2.py:185][0m |          -0.0101 |          16.6771 |           0.2488 |
[32m[20221213 15:18:56 @agent_ppo2.py:185][0m |          -0.0101 |          16.6335 |           0.2485 |
[32m[20221213 15:18:56 @agent_ppo2.py:185][0m |          -0.0142 |          16.6087 |           0.2486 |
[32m[20221213 15:18:56 @agent_ppo2.py:185][0m |          -0.0136 |          16.5493 |           0.2490 |
[32m[20221213 15:18:56 @agent_ppo2.py:185][0m |          -0.0162 |          16.4639 |           0.2490 |
[32m[20221213 15:18:56 @agent_ppo2.py:185][0m |          -0.0148 |          16.4349 |           0.2492 |
[32m[20221213 15:18:56 @agent_ppo2.py:185][0m |          -0.0129 |          16.5034 |           0.2490 |
[32m[20221213 15:18:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.33
[32m[20221213 15:18:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.65
[32m[20221213 15:18:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.83
[32m[20221213 15:18:56 @agent_ppo2.py:143][0m Total time:      26.02 min
[32m[20221213 15:18:56 @agent_ppo2.py:145][0m 2344960 total steps have happened
[32m[20221213 15:18:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1145 --------------------------#
[32m[20221213 15:18:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |           0.0010 |          17.1759 |           0.2594 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0062 |          16.8410 |           0.2591 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0104 |          16.6594 |           0.2589 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0103 |          16.5603 |           0.2590 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0153 |          16.4586 |           0.2590 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0137 |          16.3551 |           0.2590 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0137 |          16.2989 |           0.2589 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0024 |          18.1517 |           0.2588 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0146 |          16.2288 |           0.2585 |
[32m[20221213 15:18:57 @agent_ppo2.py:185][0m |          -0.0140 |          16.1382 |           0.2588 |
[32m[20221213 15:18:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:18:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.11
[32m[20221213 15:18:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.43
[32m[20221213 15:18:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.81
[32m[20221213 15:18:58 @agent_ppo2.py:143][0m Total time:      26.04 min
[32m[20221213 15:18:58 @agent_ppo2.py:145][0m 2347008 total steps have happened
[32m[20221213 15:18:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1146 --------------------------#
[32m[20221213 15:18:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:18:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:58 @agent_ppo2.py:185][0m |           0.0020 |          17.5968 |           0.2568 |
[32m[20221213 15:18:58 @agent_ppo2.py:185][0m |          -0.0096 |          16.6717 |           0.2562 |
[32m[20221213 15:18:58 @agent_ppo2.py:185][0m |          -0.0140 |          16.5368 |           0.2566 |
[32m[20221213 15:18:58 @agent_ppo2.py:185][0m |          -0.0093 |          16.7354 |           0.2567 |
[32m[20221213 15:18:58 @agent_ppo2.py:185][0m |          -0.0156 |          16.3624 |           0.2564 |
[32m[20221213 15:18:58 @agent_ppo2.py:185][0m |          -0.0158 |          16.2873 |           0.2569 |
[32m[20221213 15:18:59 @agent_ppo2.py:185][0m |          -0.0185 |          16.2268 |           0.2569 |
[32m[20221213 15:18:59 @agent_ppo2.py:185][0m |          -0.0182 |          16.1625 |           0.2574 |
[32m[20221213 15:18:59 @agent_ppo2.py:185][0m |          -0.0173 |          16.1023 |           0.2572 |
[32m[20221213 15:18:59 @agent_ppo2.py:185][0m |          -0.0190 |          16.0529 |           0.2575 |
[32m[20221213 15:18:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:18:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.25
[32m[20221213 15:18:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.76
[32m[20221213 15:18:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.07
[32m[20221213 15:18:59 @agent_ppo2.py:143][0m Total time:      26.06 min
[32m[20221213 15:18:59 @agent_ppo2.py:145][0m 2349056 total steps have happened
[32m[20221213 15:18:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1147 --------------------------#
[32m[20221213 15:18:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:18:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:18:59 @agent_ppo2.py:185][0m |           0.0036 |          17.1821 |           0.2641 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0062 |          16.9091 |           0.2639 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0074 |          16.7789 |           0.2642 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0088 |          16.6643 |           0.2638 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0093 |          16.5772 |           0.2641 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0125 |          16.5466 |           0.2640 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0101 |          16.4874 |           0.2636 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0144 |          16.4541 |           0.2638 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0118 |          16.3921 |           0.2639 |
[32m[20221213 15:19:00 @agent_ppo2.py:185][0m |          -0.0150 |          16.3574 |           0.2640 |
[32m[20221213 15:19:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:19:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 221.73
[32m[20221213 15:19:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.99
[32m[20221213 15:19:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.10
[32m[20221213 15:19:00 @agent_ppo2.py:143][0m Total time:      26.09 min
[32m[20221213 15:19:00 @agent_ppo2.py:145][0m 2351104 total steps have happened
[32m[20221213 15:19:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1148 --------------------------#
[32m[20221213 15:19:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0005 |          17.2155 |           0.2570 |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0044 |          16.8346 |           0.2570 |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0125 |          16.5894 |           0.2571 |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0126 |          16.3481 |           0.2571 |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0131 |          16.2495 |           0.2573 |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0159 |          16.1163 |           0.2572 |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0153 |          15.9742 |           0.2574 |
[32m[20221213 15:19:01 @agent_ppo2.py:185][0m |          -0.0099 |          17.1663 |           0.2573 |
[32m[20221213 15:19:02 @agent_ppo2.py:185][0m |          -0.0132 |          15.8417 |           0.2570 |
[32m[20221213 15:19:02 @agent_ppo2.py:185][0m |          -0.0165 |          15.7099 |           0.2573 |
[32m[20221213 15:19:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:19:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.34
[32m[20221213 15:19:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.35
[32m[20221213 15:19:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.72
[32m[20221213 15:19:02 @agent_ppo2.py:143][0m Total time:      26.11 min
[32m[20221213 15:19:02 @agent_ppo2.py:145][0m 2353152 total steps have happened
[32m[20221213 15:19:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1149 --------------------------#
[32m[20221213 15:19:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:02 @agent_ppo2.py:185][0m |           0.0012 |          15.8588 |           0.2669 |
[32m[20221213 15:19:02 @agent_ppo2.py:185][0m |          -0.0098 |          15.4352 |           0.2660 |
[32m[20221213 15:19:02 @agent_ppo2.py:185][0m |          -0.0114 |          15.2715 |           0.2663 |
[32m[20221213 15:19:02 @agent_ppo2.py:185][0m |          -0.0055 |          15.3039 |           0.2665 |
[32m[20221213 15:19:03 @agent_ppo2.py:185][0m |          -0.0093 |          15.2032 |           0.2662 |
[32m[20221213 15:19:03 @agent_ppo2.py:185][0m |          -0.0124 |          15.1443 |           0.2664 |
[32m[20221213 15:19:03 @agent_ppo2.py:185][0m |          -0.0101 |          15.0474 |           0.2666 |
[32m[20221213 15:19:03 @agent_ppo2.py:185][0m |          -0.0128 |          15.0228 |           0.2666 |
[32m[20221213 15:19:03 @agent_ppo2.py:185][0m |          -0.0150 |          14.9294 |           0.2664 |
[32m[20221213 15:19:03 @agent_ppo2.py:185][0m |          -0.0192 |          14.9562 |           0.2667 |
[32m[20221213 15:19:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:19:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 192.27
[32m[20221213 15:19:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 243.50
[32m[20221213 15:19:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.80
[32m[20221213 15:19:03 @agent_ppo2.py:143][0m Total time:      26.13 min
[32m[20221213 15:19:03 @agent_ppo2.py:145][0m 2355200 total steps have happened
[32m[20221213 15:19:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1150 --------------------------#
[32m[20221213 15:19:03 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:19:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0018 |          17.0976 |           0.2740 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0062 |          16.8399 |           0.2740 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0117 |          16.6907 |           0.2739 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0144 |          16.5541 |           0.2739 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0134 |          16.4126 |           0.2741 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0115 |          16.4099 |           0.2740 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0153 |          16.2354 |           0.2742 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0156 |          16.0786 |           0.2742 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0066 |          17.3810 |           0.2745 |
[32m[20221213 15:19:04 @agent_ppo2.py:185][0m |          -0.0162 |          15.9102 |           0.2742 |
[32m[20221213 15:19:04 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:19:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 229.87
[32m[20221213 15:19:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.79
[32m[20221213 15:19:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.61
[32m[20221213 15:19:05 @agent_ppo2.py:143][0m Total time:      26.16 min
[32m[20221213 15:19:05 @agent_ppo2.py:145][0m 2357248 total steps have happened
[32m[20221213 15:19:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1151 --------------------------#
[32m[20221213 15:19:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:05 @agent_ppo2.py:185][0m |          -0.0004 |          12.7468 |           0.2711 |
[32m[20221213 15:19:05 @agent_ppo2.py:185][0m |          -0.0034 |          12.2657 |           0.2711 |
[32m[20221213 15:19:05 @agent_ppo2.py:185][0m |          -0.0107 |          12.0935 |           0.2708 |
[32m[20221213 15:19:05 @agent_ppo2.py:185][0m |          -0.0110 |          12.0355 |           0.2707 |
[32m[20221213 15:19:05 @agent_ppo2.py:185][0m |          -0.0011 |          12.9455 |           0.2704 |
[32m[20221213 15:19:05 @agent_ppo2.py:185][0m |          -0.0050 |          11.9600 |           0.2701 |
[32m[20221213 15:19:06 @agent_ppo2.py:185][0m |          -0.0098 |          11.8730 |           0.2706 |
[32m[20221213 15:19:06 @agent_ppo2.py:185][0m |          -0.0149 |          11.8160 |           0.2701 |
[32m[20221213 15:19:06 @agent_ppo2.py:185][0m |          -0.0144 |          11.7663 |           0.2702 |
[32m[20221213 15:19:06 @agent_ppo2.py:185][0m |          -0.0117 |          11.7205 |           0.2703 |
[32m[20221213 15:19:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:19:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 144.41
[32m[20221213 15:19:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.18
[32m[20221213 15:19:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.03
[32m[20221213 15:19:06 @agent_ppo2.py:143][0m Total time:      26.18 min
[32m[20221213 15:19:06 @agent_ppo2.py:145][0m 2359296 total steps have happened
[32m[20221213 15:19:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1152 --------------------------#
[32m[20221213 15:19:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:06 @agent_ppo2.py:185][0m |           0.0004 |          16.1233 |           0.2699 |
[32m[20221213 15:19:06 @agent_ppo2.py:185][0m |          -0.0060 |          15.8095 |           0.2699 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0086 |          15.5580 |           0.2697 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0113 |          15.4269 |           0.2694 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0124 |          15.2829 |           0.2695 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0108 |          15.2094 |           0.2695 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0138 |          15.0949 |           0.2693 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0152 |          14.9877 |           0.2691 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0151 |          14.9385 |           0.2691 |
[32m[20221213 15:19:07 @agent_ppo2.py:185][0m |          -0.0147 |          14.8475 |           0.2693 |
[32m[20221213 15:19:07 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:19:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.41
[32m[20221213 15:19:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.60
[32m[20221213 15:19:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.68
[32m[20221213 15:19:07 @agent_ppo2.py:143][0m Total time:      26.20 min
[32m[20221213 15:19:07 @agent_ppo2.py:145][0m 2361344 total steps have happened
[32m[20221213 15:19:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1153 --------------------------#
[32m[20221213 15:19:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |           0.0021 |          16.9292 |           0.2692 |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |          -0.0048 |          16.5601 |           0.2693 |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |          -0.0089 |          16.3723 |           0.2694 |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |          -0.0116 |          16.2847 |           0.2693 |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |          -0.0110 |          16.1586 |           0.2695 |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |          -0.0125 |          16.0430 |           0.2697 |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |          -0.0145 |          15.9461 |           0.2698 |
[32m[20221213 15:19:08 @agent_ppo2.py:185][0m |          -0.0145 |          15.8975 |           0.2697 |
[32m[20221213 15:19:09 @agent_ppo2.py:185][0m |          -0.0041 |          16.9987 |           0.2700 |
[32m[20221213 15:19:09 @agent_ppo2.py:185][0m |          -0.0160 |          15.7968 |           0.2697 |
[32m[20221213 15:19:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:19:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.69
[32m[20221213 15:19:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.20
[32m[20221213 15:19:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.66
[32m[20221213 15:19:09 @agent_ppo2.py:143][0m Total time:      26.23 min
[32m[20221213 15:19:09 @agent_ppo2.py:145][0m 2363392 total steps have happened
[32m[20221213 15:19:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1154 --------------------------#
[32m[20221213 15:19:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:09 @agent_ppo2.py:185][0m |          -0.0008 |          16.8573 |           0.2620 |
[32m[20221213 15:19:09 @agent_ppo2.py:185][0m |          -0.0067 |          16.2137 |           0.2611 |
[32m[20221213 15:19:09 @agent_ppo2.py:185][0m |          -0.0080 |          15.9278 |           0.2607 |
[32m[20221213 15:19:09 @agent_ppo2.py:185][0m |          -0.0093 |          15.7457 |           0.2607 |
[32m[20221213 15:19:10 @agent_ppo2.py:185][0m |          -0.0043 |          15.8332 |           0.2609 |
[32m[20221213 15:19:10 @agent_ppo2.py:185][0m |          -0.0020 |          16.1301 |           0.2601 |
[32m[20221213 15:19:10 @agent_ppo2.py:185][0m |          -0.0151 |          15.4662 |           0.2608 |
[32m[20221213 15:19:10 @agent_ppo2.py:185][0m |          -0.0155 |          15.3169 |           0.2608 |
[32m[20221213 15:19:10 @agent_ppo2.py:185][0m |          -0.0032 |          16.3422 |           0.2606 |
[32m[20221213 15:19:10 @agent_ppo2.py:185][0m |          -0.0176 |          15.2618 |           0.2607 |
[32m[20221213 15:19:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:19:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.19
[32m[20221213 15:19:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.48
[32m[20221213 15:19:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.82
[32m[20221213 15:19:10 @agent_ppo2.py:143][0m Total time:      26.25 min
[32m[20221213 15:19:10 @agent_ppo2.py:145][0m 2365440 total steps have happened
[32m[20221213 15:19:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1155 --------------------------#
[32m[20221213 15:19:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |           0.0010 |          17.3975 |           0.2688 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0060 |          16.9188 |           0.2685 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0042 |          16.9577 |           0.2682 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0123 |          16.3941 |           0.2681 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0145 |          16.1152 |           0.2679 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0049 |          17.3535 |           0.2678 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0140 |          15.7058 |           0.2675 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0184 |          15.5941 |           0.2675 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0155 |          15.4979 |           0.2675 |
[32m[20221213 15:19:11 @agent_ppo2.py:185][0m |          -0.0162 |          15.3802 |           0.2673 |
[32m[20221213 15:19:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.49
[32m[20221213 15:19:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.73
[32m[20221213 15:19:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.92
[32m[20221213 15:19:12 @agent_ppo2.py:143][0m Total time:      26.27 min
[32m[20221213 15:19:12 @agent_ppo2.py:145][0m 2367488 total steps have happened
[32m[20221213 15:19:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1156 --------------------------#
[32m[20221213 15:19:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:12 @agent_ppo2.py:185][0m |          -0.0024 |          16.9955 |           0.2678 |
[32m[20221213 15:19:12 @agent_ppo2.py:185][0m |          -0.0079 |          16.5900 |           0.2675 |
[32m[20221213 15:19:12 @agent_ppo2.py:185][0m |          -0.0085 |          16.3760 |           0.2678 |
[32m[20221213 15:19:12 @agent_ppo2.py:185][0m |          -0.0100 |          16.2640 |           0.2678 |
[32m[20221213 15:19:12 @agent_ppo2.py:185][0m |          -0.0126 |          16.1623 |           0.2678 |
[32m[20221213 15:19:12 @agent_ppo2.py:185][0m |          -0.0167 |          16.1060 |           0.2679 |
[32m[20221213 15:19:12 @agent_ppo2.py:185][0m |          -0.0131 |          16.0557 |           0.2681 |
[32m[20221213 15:19:13 @agent_ppo2.py:185][0m |          -0.0124 |          16.0249 |           0.2680 |
[32m[20221213 15:19:13 @agent_ppo2.py:185][0m |          -0.0139 |          15.9892 |           0.2682 |
[32m[20221213 15:19:13 @agent_ppo2.py:185][0m |          -0.0186 |          16.0078 |           0.2680 |
[32m[20221213 15:19:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.63
[32m[20221213 15:19:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.52
[32m[20221213 15:19:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.86
[32m[20221213 15:19:13 @agent_ppo2.py:143][0m Total time:      26.30 min
[32m[20221213 15:19:13 @agent_ppo2.py:145][0m 2369536 total steps have happened
[32m[20221213 15:19:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1157 --------------------------#
[32m[20221213 15:19:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:13 @agent_ppo2.py:185][0m |           0.0000 |          16.9084 |           0.2688 |
[32m[20221213 15:19:13 @agent_ppo2.py:185][0m |          -0.0051 |          16.6444 |           0.2678 |
[32m[20221213 15:19:13 @agent_ppo2.py:185][0m |          -0.0020 |          16.8796 |           0.2676 |
[32m[20221213 15:19:14 @agent_ppo2.py:185][0m |          -0.0114 |          16.4537 |           0.2677 |
[32m[20221213 15:19:14 @agent_ppo2.py:185][0m |          -0.0129 |          16.4265 |           0.2676 |
[32m[20221213 15:19:14 @agent_ppo2.py:185][0m |          -0.0114 |          16.3395 |           0.2675 |
[32m[20221213 15:19:14 @agent_ppo2.py:185][0m |          -0.0137 |          16.2601 |           0.2673 |
[32m[20221213 15:19:14 @agent_ppo2.py:185][0m |          -0.0157 |          16.2029 |           0.2673 |
[32m[20221213 15:19:14 @agent_ppo2.py:185][0m |          -0.0147 |          16.1610 |           0.2674 |
[32m[20221213 15:19:14 @agent_ppo2.py:185][0m |          -0.0165 |          16.1193 |           0.2671 |
[32m[20221213 15:19:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.61
[32m[20221213 15:19:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.14
[32m[20221213 15:19:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.91
[32m[20221213 15:19:14 @agent_ppo2.py:143][0m Total time:      26.32 min
[32m[20221213 15:19:14 @agent_ppo2.py:145][0m 2371584 total steps have happened
[32m[20221213 15:19:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1158 --------------------------#
[32m[20221213 15:19:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0003 |          16.3551 |           0.2693 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0072 |          16.1066 |           0.2693 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0037 |          16.3413 |           0.2692 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0111 |          15.8883 |           0.2692 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0134 |          15.8161 |           0.2693 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0011 |          16.7734 |           0.2694 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0149 |          15.7417 |           0.2689 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0134 |          15.6791 |           0.2689 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0139 |          15.6081 |           0.2691 |
[32m[20221213 15:19:15 @agent_ppo2.py:185][0m |          -0.0138 |          15.6095 |           0.2690 |
[32m[20221213 15:19:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 226.04
[32m[20221213 15:19:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.41
[32m[20221213 15:19:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.01
[32m[20221213 15:19:16 @agent_ppo2.py:143][0m Total time:      26.34 min
[32m[20221213 15:19:16 @agent_ppo2.py:145][0m 2373632 total steps have happened
[32m[20221213 15:19:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1159 --------------------------#
[32m[20221213 15:19:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:16 @agent_ppo2.py:185][0m |           0.0004 |          16.5656 |           0.2716 |
[32m[20221213 15:19:16 @agent_ppo2.py:185][0m |          -0.0065 |          16.1493 |           0.2715 |
[32m[20221213 15:19:16 @agent_ppo2.py:185][0m |          -0.0061 |          15.9847 |           0.2715 |
[32m[20221213 15:19:16 @agent_ppo2.py:185][0m |          -0.0088 |          15.8673 |           0.2714 |
[32m[20221213 15:19:16 @agent_ppo2.py:185][0m |          -0.0111 |          15.7816 |           0.2717 |
[32m[20221213 15:19:16 @agent_ppo2.py:185][0m |          -0.0134 |          15.7373 |           0.2714 |
[32m[20221213 15:19:16 @agent_ppo2.py:185][0m |          -0.0144 |          15.7041 |           0.2716 |
[32m[20221213 15:19:17 @agent_ppo2.py:185][0m |          -0.0134 |          15.6587 |           0.2719 |
[32m[20221213 15:19:17 @agent_ppo2.py:185][0m |          -0.0146 |          15.6804 |           0.2719 |
[32m[20221213 15:19:17 @agent_ppo2.py:185][0m |          -0.0171 |          15.6336 |           0.2719 |
[32m[20221213 15:19:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.25
[32m[20221213 15:19:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.58
[32m[20221213 15:19:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.95
[32m[20221213 15:19:17 @agent_ppo2.py:143][0m Total time:      26.36 min
[32m[20221213 15:19:17 @agent_ppo2.py:145][0m 2375680 total steps have happened
[32m[20221213 15:19:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1160 --------------------------#
[32m[20221213 15:19:17 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:19:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:17 @agent_ppo2.py:185][0m |           0.0048 |          17.5967 |           0.2757 |
[32m[20221213 15:19:17 @agent_ppo2.py:185][0m |          -0.0091 |          16.1052 |           0.2755 |
[32m[20221213 15:19:17 @agent_ppo2.py:185][0m |          -0.0075 |          15.8224 |           0.2755 |
[32m[20221213 15:19:18 @agent_ppo2.py:185][0m |          -0.0143 |          15.6051 |           0.2755 |
[32m[20221213 15:19:18 @agent_ppo2.py:185][0m |          -0.0152 |          15.4848 |           0.2753 |
[32m[20221213 15:19:18 @agent_ppo2.py:185][0m |          -0.0147 |          15.3457 |           0.2755 |
[32m[20221213 15:19:18 @agent_ppo2.py:185][0m |          -0.0132 |          15.3196 |           0.2753 |
[32m[20221213 15:19:18 @agent_ppo2.py:185][0m |          -0.0170 |          15.1220 |           0.2751 |
[32m[20221213 15:19:18 @agent_ppo2.py:185][0m |          -0.0186 |          15.0568 |           0.2751 |
[32m[20221213 15:19:18 @agent_ppo2.py:185][0m |          -0.0186 |          14.9497 |           0.2753 |
[32m[20221213 15:19:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 214.03
[32m[20221213 15:19:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 233.18
[32m[20221213 15:19:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.78
[32m[20221213 15:19:18 @agent_ppo2.py:143][0m Total time:      26.38 min
[32m[20221213 15:19:18 @agent_ppo2.py:145][0m 2377728 total steps have happened
[32m[20221213 15:19:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1161 --------------------------#
[32m[20221213 15:19:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |           0.0000 |          16.9642 |           0.2676 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0097 |          16.5807 |           0.2670 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0111 |          16.3918 |           0.2669 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0126 |          16.2661 |           0.2670 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0142 |          16.1765 |           0.2672 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0174 |          16.1226 |           0.2673 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0167 |          16.0560 |           0.2671 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0063 |          18.1670 |           0.2672 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0203 |          16.0201 |           0.2670 |
[32m[20221213 15:19:19 @agent_ppo2.py:185][0m |          -0.0209 |          15.9236 |           0.2675 |
[32m[20221213 15:19:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:19:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.31
[32m[20221213 15:19:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.12
[32m[20221213 15:19:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.92
[32m[20221213 15:19:20 @agent_ppo2.py:143][0m Total time:      26.41 min
[32m[20221213 15:19:20 @agent_ppo2.py:145][0m 2379776 total steps have happened
[32m[20221213 15:19:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1162 --------------------------#
[32m[20221213 15:19:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:20 @agent_ppo2.py:185][0m |          -0.0021 |          16.7601 |           0.2812 |
[32m[20221213 15:19:20 @agent_ppo2.py:185][0m |           0.0054 |          17.6511 |           0.2807 |
[32m[20221213 15:19:20 @agent_ppo2.py:185][0m |          -0.0114 |          15.1448 |           0.2803 |
[32m[20221213 15:19:20 @agent_ppo2.py:185][0m |           0.0014 |          16.0100 |           0.2800 |
[32m[20221213 15:19:20 @agent_ppo2.py:185][0m |          -0.0173 |          14.5623 |           0.2802 |
[32m[20221213 15:19:20 @agent_ppo2.py:185][0m |          -0.0156 |          14.3952 |           0.2802 |
[32m[20221213 15:19:20 @agent_ppo2.py:185][0m |          -0.0109 |          15.5107 |           0.2803 |
[32m[20221213 15:19:21 @agent_ppo2.py:185][0m |          -0.0139 |          14.4402 |           0.2801 |
[32m[20221213 15:19:21 @agent_ppo2.py:185][0m |          -0.0178 |          14.0904 |           0.2800 |
[32m[20221213 15:19:21 @agent_ppo2.py:185][0m |          -0.0203 |          14.0331 |           0.2803 |
[32m[20221213 15:19:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.64
[32m[20221213 15:19:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.13
[32m[20221213 15:19:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.94
[32m[20221213 15:19:21 @agent_ppo2.py:143][0m Total time:      26.43 min
[32m[20221213 15:19:21 @agent_ppo2.py:145][0m 2381824 total steps have happened
[32m[20221213 15:19:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1163 --------------------------#
[32m[20221213 15:19:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:21 @agent_ppo2.py:185][0m |          -0.0025 |          17.6815 |           0.2756 |
[32m[20221213 15:19:21 @agent_ppo2.py:185][0m |          -0.0020 |          17.8725 |           0.2748 |
[32m[20221213 15:19:21 @agent_ppo2.py:185][0m |          -0.0109 |          17.1027 |           0.2746 |
[32m[20221213 15:19:22 @agent_ppo2.py:185][0m |          -0.0148 |          17.0333 |           0.2744 |
[32m[20221213 15:19:22 @agent_ppo2.py:185][0m |          -0.0130 |          16.9831 |           0.2746 |
[32m[20221213 15:19:22 @agent_ppo2.py:185][0m |          -0.0091 |          18.4121 |           0.2743 |
[32m[20221213 15:19:22 @agent_ppo2.py:185][0m |          -0.0151 |          16.9205 |           0.2744 |
[32m[20221213 15:19:22 @agent_ppo2.py:185][0m |          -0.0156 |          16.9236 |           0.2744 |
[32m[20221213 15:19:22 @agent_ppo2.py:185][0m |          -0.0096 |          17.6499 |           0.2745 |
[32m[20221213 15:19:22 @agent_ppo2.py:185][0m |          -0.0068 |          18.0399 |           0.2745 |
[32m[20221213 15:19:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.83
[32m[20221213 15:19:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.21
[32m[20221213 15:19:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.49
[32m[20221213 15:19:22 @agent_ppo2.py:143][0m Total time:      26.45 min
[32m[20221213 15:19:22 @agent_ppo2.py:145][0m 2383872 total steps have happened
[32m[20221213 15:19:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1164 --------------------------#
[32m[20221213 15:19:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0031 |          17.9347 |           0.2831 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0064 |          17.6577 |           0.2824 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0104 |          17.5123 |           0.2827 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0043 |          17.9487 |           0.2829 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0121 |          17.3266 |           0.2824 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0056 |          18.3295 |           0.2823 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0158 |          17.1841 |           0.2825 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0170 |          17.1041 |           0.2824 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0183 |          17.0381 |           0.2824 |
[32m[20221213 15:19:23 @agent_ppo2.py:185][0m |          -0.0129 |          17.1062 |           0.2826 |
[32m[20221213 15:19:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.65
[32m[20221213 15:19:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.97
[32m[20221213 15:19:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.87
[32m[20221213 15:19:24 @agent_ppo2.py:143][0m Total time:      26.47 min
[32m[20221213 15:19:24 @agent_ppo2.py:145][0m 2385920 total steps have happened
[32m[20221213 15:19:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1165 --------------------------#
[32m[20221213 15:19:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:24 @agent_ppo2.py:185][0m |          -0.0014 |          15.2689 |           0.2801 |
[32m[20221213 15:19:24 @agent_ppo2.py:185][0m |          -0.0012 |          14.9692 |           0.2800 |
[32m[20221213 15:19:24 @agent_ppo2.py:185][0m |          -0.0070 |          14.7557 |           0.2801 |
[32m[20221213 15:19:24 @agent_ppo2.py:185][0m |          -0.0094 |          14.6369 |           0.2803 |
[32m[20221213 15:19:24 @agent_ppo2.py:185][0m |          -0.0124 |          14.5257 |           0.2804 |
[32m[20221213 15:19:24 @agent_ppo2.py:185][0m |          -0.0130 |          14.4784 |           0.2807 |
[32m[20221213 15:19:25 @agent_ppo2.py:185][0m |          -0.0114 |          14.3902 |           0.2808 |
[32m[20221213 15:19:25 @agent_ppo2.py:185][0m |          -0.0147 |          14.3750 |           0.2809 |
[32m[20221213 15:19:25 @agent_ppo2.py:185][0m |          -0.0133 |          14.2962 |           0.2808 |
[32m[20221213 15:19:25 @agent_ppo2.py:185][0m |          -0.0156 |          14.2068 |           0.2811 |
[32m[20221213 15:19:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 194.41
[32m[20221213 15:19:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.59
[32m[20221213 15:19:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.75
[32m[20221213 15:19:25 @agent_ppo2.py:143][0m Total time:      26.50 min
[32m[20221213 15:19:25 @agent_ppo2.py:145][0m 2387968 total steps have happened
[32m[20221213 15:19:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1166 --------------------------#
[32m[20221213 15:19:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:25 @agent_ppo2.py:185][0m |           0.0007 |          17.3890 |           0.2815 |
[32m[20221213 15:19:25 @agent_ppo2.py:185][0m |          -0.0067 |          17.1594 |           0.2807 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0039 |          17.4022 |           0.2811 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0091 |          17.0206 |           0.2811 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0132 |          16.9887 |           0.2810 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0147 |          16.9029 |           0.2809 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0014 |          18.9913 |           0.2806 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0141 |          16.8440 |           0.2805 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0164 |          16.7890 |           0.2805 |
[32m[20221213 15:19:26 @agent_ppo2.py:185][0m |          -0.0108 |          17.1027 |           0.2805 |
[32m[20221213 15:19:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.18
[32m[20221213 15:19:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.38
[32m[20221213 15:19:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.75
[32m[20221213 15:19:26 @agent_ppo2.py:143][0m Total time:      26.52 min
[32m[20221213 15:19:26 @agent_ppo2.py:145][0m 2390016 total steps have happened
[32m[20221213 15:19:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1167 --------------------------#
[32m[20221213 15:19:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0017 |          17.2392 |           0.2776 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0066 |          16.4812 |           0.2772 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0098 |          16.1478 |           0.2772 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0084 |          15.8664 |           0.2768 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0138 |          15.6325 |           0.2769 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0142 |          15.4123 |           0.2767 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0156 |          15.2598 |           0.2768 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0169 |          15.0860 |           0.2764 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0162 |          15.0016 |           0.2764 |
[32m[20221213 15:19:27 @agent_ppo2.py:185][0m |          -0.0198 |          14.9057 |           0.2763 |
[32m[20221213 15:19:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.36
[32m[20221213 15:19:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.16
[32m[20221213 15:19:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.27
[32m[20221213 15:19:28 @agent_ppo2.py:143][0m Total time:      26.54 min
[32m[20221213 15:19:28 @agent_ppo2.py:145][0m 2392064 total steps have happened
[32m[20221213 15:19:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1168 --------------------------#
[32m[20221213 15:19:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:28 @agent_ppo2.py:185][0m |           0.0033 |          18.4136 |           0.2787 |
[32m[20221213 15:19:28 @agent_ppo2.py:185][0m |          -0.0049 |          17.8003 |           0.2778 |
[32m[20221213 15:19:28 @agent_ppo2.py:185][0m |          -0.0086 |          17.6453 |           0.2774 |
[32m[20221213 15:19:28 @agent_ppo2.py:185][0m |          -0.0112 |          17.5003 |           0.2774 |
[32m[20221213 15:19:28 @agent_ppo2.py:185][0m |          -0.0084 |          17.6975 |           0.2773 |
[32m[20221213 15:19:28 @agent_ppo2.py:185][0m |          -0.0134 |          17.3661 |           0.2772 |
[32m[20221213 15:19:29 @agent_ppo2.py:185][0m |          -0.0137 |          17.2896 |           0.2768 |
[32m[20221213 15:19:29 @agent_ppo2.py:185][0m |          -0.0149 |          17.2486 |           0.2769 |
[32m[20221213 15:19:29 @agent_ppo2.py:185][0m |          -0.0160 |          17.1587 |           0.2767 |
[32m[20221213 15:19:29 @agent_ppo2.py:185][0m |          -0.0052 |          18.7223 |           0.2763 |
[32m[20221213 15:19:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.90
[32m[20221213 15:19:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.23
[32m[20221213 15:19:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.95
[32m[20221213 15:19:29 @agent_ppo2.py:143][0m Total time:      26.56 min
[32m[20221213 15:19:29 @agent_ppo2.py:145][0m 2394112 total steps have happened
[32m[20221213 15:19:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1169 --------------------------#
[32m[20221213 15:19:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:29 @agent_ppo2.py:185][0m |           0.0032 |          17.0620 |           0.2823 |
[32m[20221213 15:19:29 @agent_ppo2.py:185][0m |          -0.0012 |          16.2891 |           0.2826 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0107 |          15.8706 |           0.2824 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0123 |          15.5068 |           0.2823 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0117 |          15.2296 |           0.2823 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0154 |          14.9715 |           0.2822 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0140 |          14.8073 |           0.2821 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0133 |          14.6340 |           0.2818 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0132 |          14.4517 |           0.2817 |
[32m[20221213 15:19:30 @agent_ppo2.py:185][0m |          -0.0163 |          14.2503 |           0.2818 |
[32m[20221213 15:19:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.15
[32m[20221213 15:19:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.14
[32m[20221213 15:19:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.78
[32m[20221213 15:19:30 @agent_ppo2.py:143][0m Total time:      26.59 min
[32m[20221213 15:19:30 @agent_ppo2.py:145][0m 2396160 total steps have happened
[32m[20221213 15:19:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1170 --------------------------#
[32m[20221213 15:19:31 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:19:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0026 |          18.5022 |           0.2796 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0084 |          17.9407 |           0.2781 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0085 |          17.7059 |           0.2781 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0107 |          17.5985 |           0.2777 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0128 |          17.4602 |           0.2778 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0144 |          17.3739 |           0.2773 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0147 |          17.3084 |           0.2776 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0155 |          17.2202 |           0.2773 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0167 |          17.2213 |           0.2772 |
[32m[20221213 15:19:31 @agent_ppo2.py:185][0m |          -0.0105 |          17.3421 |           0.2772 |
[32m[20221213 15:19:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.35
[32m[20221213 15:19:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.53
[32m[20221213 15:19:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.31
[32m[20221213 15:19:32 @agent_ppo2.py:143][0m Total time:      26.61 min
[32m[20221213 15:19:32 @agent_ppo2.py:145][0m 2398208 total steps have happened
[32m[20221213 15:19:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1171 --------------------------#
[32m[20221213 15:19:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:32 @agent_ppo2.py:185][0m |          -0.0040 |          17.7961 |           0.2741 |
[32m[20221213 15:19:32 @agent_ppo2.py:185][0m |          -0.0061 |          17.5589 |           0.2739 |
[32m[20221213 15:19:32 @agent_ppo2.py:185][0m |          -0.0089 |          17.3998 |           0.2741 |
[32m[20221213 15:19:32 @agent_ppo2.py:185][0m |          -0.0006 |          18.4969 |           0.2737 |
[32m[20221213 15:19:32 @agent_ppo2.py:185][0m |          -0.0100 |          17.1904 |           0.2743 |
[32m[20221213 15:19:32 @agent_ppo2.py:185][0m |          -0.0129 |          17.0411 |           0.2738 |
[32m[20221213 15:19:33 @agent_ppo2.py:185][0m |          -0.0102 |          17.1019 |           0.2737 |
[32m[20221213 15:19:33 @agent_ppo2.py:185][0m |          -0.0151 |          16.9408 |           0.2740 |
[32m[20221213 15:19:33 @agent_ppo2.py:185][0m |          -0.0135 |          16.8704 |           0.2739 |
[32m[20221213 15:19:33 @agent_ppo2.py:185][0m |          -0.0104 |          17.1183 |           0.2742 |
[32m[20221213 15:19:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.49
[32m[20221213 15:19:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.74
[32m[20221213 15:19:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.17
[32m[20221213 15:19:33 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 298.17
[32m[20221213 15:19:33 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 298.17
[32m[20221213 15:19:33 @agent_ppo2.py:143][0m Total time:      26.63 min
[32m[20221213 15:19:33 @agent_ppo2.py:145][0m 2400256 total steps have happened
[32m[20221213 15:19:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1172 --------------------------#
[32m[20221213 15:19:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:33 @agent_ppo2.py:185][0m |          -0.0032 |          17.1034 |           0.2768 |
[32m[20221213 15:19:33 @agent_ppo2.py:185][0m |          -0.0076 |          16.6946 |           0.2766 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0043 |          16.8191 |           0.2760 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0085 |          16.4939 |           0.2763 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0119 |          16.2004 |           0.2764 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0054 |          17.0641 |           0.2765 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0131 |          16.0378 |           0.2765 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0161 |          15.9587 |           0.2764 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0124 |          15.9267 |           0.2763 |
[32m[20221213 15:19:34 @agent_ppo2.py:185][0m |          -0.0161 |          15.8376 |           0.2765 |
[32m[20221213 15:19:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.31
[32m[20221213 15:19:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.87
[32m[20221213 15:19:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.08
[32m[20221213 15:19:34 @agent_ppo2.py:143][0m Total time:      26.65 min
[32m[20221213 15:19:34 @agent_ppo2.py:145][0m 2402304 total steps have happened
[32m[20221213 15:19:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1173 --------------------------#
[32m[20221213 15:19:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |           0.0026 |          17.2394 |           0.2877 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0071 |          16.6369 |           0.2877 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |           0.0005 |          18.3500 |           0.2872 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0085 |          15.9628 |           0.2874 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0107 |          15.6901 |           0.2872 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0114 |          15.4954 |           0.2872 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0130 |          15.2893 |           0.2874 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0149 |          15.1124 |           0.2873 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0161 |          15.0156 |           0.2872 |
[32m[20221213 15:19:35 @agent_ppo2.py:185][0m |          -0.0164 |          14.8749 |           0.2873 |
[32m[20221213 15:19:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.30
[32m[20221213 15:19:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.19
[32m[20221213 15:19:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.07
[32m[20221213 15:19:36 @agent_ppo2.py:143][0m Total time:      26.68 min
[32m[20221213 15:19:36 @agent_ppo2.py:145][0m 2404352 total steps have happened
[32m[20221213 15:19:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1174 --------------------------#
[32m[20221213 15:19:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:36 @agent_ppo2.py:185][0m |          -0.0005 |          18.5395 |           0.2873 |
[32m[20221213 15:19:36 @agent_ppo2.py:185][0m |          -0.0084 |          17.7529 |           0.2873 |
[32m[20221213 15:19:36 @agent_ppo2.py:185][0m |          -0.0064 |          17.6486 |           0.2876 |
[32m[20221213 15:19:36 @agent_ppo2.py:185][0m |          -0.0098 |          17.3897 |           0.2878 |
[32m[20221213 15:19:36 @agent_ppo2.py:185][0m |          -0.0095 |          17.3912 |           0.2880 |
[32m[20221213 15:19:37 @agent_ppo2.py:185][0m |          -0.0137 |          17.2714 |           0.2883 |
[32m[20221213 15:19:37 @agent_ppo2.py:185][0m |          -0.0094 |          17.2728 |           0.2882 |
[32m[20221213 15:19:37 @agent_ppo2.py:185][0m |          -0.0142 |          17.1195 |           0.2886 |
[32m[20221213 15:19:37 @agent_ppo2.py:185][0m |          -0.0133 |          17.0741 |           0.2884 |
[32m[20221213 15:19:37 @agent_ppo2.py:185][0m |          -0.0042 |          18.7329 |           0.2890 |
[32m[20221213 15:19:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.04
[32m[20221213 15:19:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.08
[32m[20221213 15:19:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.85
[32m[20221213 15:19:37 @agent_ppo2.py:143][0m Total time:      26.70 min
[32m[20221213 15:19:37 @agent_ppo2.py:145][0m 2406400 total steps have happened
[32m[20221213 15:19:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1175 --------------------------#
[32m[20221213 15:19:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:37 @agent_ppo2.py:185][0m |           0.0030 |          18.8965 |           0.2843 |
[32m[20221213 15:19:37 @agent_ppo2.py:185][0m |          -0.0074 |          18.3837 |           0.2840 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0113 |          18.2760 |           0.2842 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0114 |          18.1576 |           0.2837 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0142 |          18.0918 |           0.2839 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0144 |          18.0590 |           0.2838 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0130 |          18.0718 |           0.2838 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0158 |          17.9645 |           0.2834 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0165 |          17.9523 |           0.2836 |
[32m[20221213 15:19:38 @agent_ppo2.py:185][0m |          -0.0158 |          17.9629 |           0.2835 |
[32m[20221213 15:19:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.92
[32m[20221213 15:19:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.57
[32m[20221213 15:19:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.94
[32m[20221213 15:19:38 @agent_ppo2.py:143][0m Total time:      26.72 min
[32m[20221213 15:19:38 @agent_ppo2.py:145][0m 2408448 total steps have happened
[32m[20221213 15:19:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1176 --------------------------#
[32m[20221213 15:19:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0013 |          18.1860 |           0.2845 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0062 |          17.8424 |           0.2844 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0059 |          17.7680 |           0.2840 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0118 |          17.5360 |           0.2840 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0164 |          17.4178 |           0.2837 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0131 |          17.3234 |           0.2835 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0144 |          17.3418 |           0.2834 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0181 |          17.0973 |           0.2833 |
[32m[20221213 15:19:39 @agent_ppo2.py:185][0m |          -0.0037 |          18.7226 |           0.2830 |
[32m[20221213 15:19:40 @agent_ppo2.py:185][0m |          -0.0145 |          17.1204 |           0.2823 |
[32m[20221213 15:19:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 225.58
[32m[20221213 15:19:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.59
[32m[20221213 15:19:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.54
[32m[20221213 15:19:40 @agent_ppo2.py:143][0m Total time:      26.74 min
[32m[20221213 15:19:40 @agent_ppo2.py:145][0m 2410496 total steps have happened
[32m[20221213 15:19:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1177 --------------------------#
[32m[20221213 15:19:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:40 @agent_ppo2.py:185][0m |           0.0014 |          18.3810 |           0.2895 |
[32m[20221213 15:19:40 @agent_ppo2.py:185][0m |           0.0025 |          18.9543 |           0.2891 |
[32m[20221213 15:19:40 @agent_ppo2.py:185][0m |          -0.0082 |          17.9492 |           0.2890 |
[32m[20221213 15:19:40 @agent_ppo2.py:185][0m |          -0.0074 |          17.9215 |           0.2888 |
[32m[20221213 15:19:40 @agent_ppo2.py:185][0m |          -0.0116 |          17.8013 |           0.2892 |
[32m[20221213 15:19:41 @agent_ppo2.py:185][0m |          -0.0134 |          17.7530 |           0.2889 |
[32m[20221213 15:19:41 @agent_ppo2.py:185][0m |          -0.0131 |          17.6860 |           0.2888 |
[32m[20221213 15:19:41 @agent_ppo2.py:185][0m |          -0.0155 |          17.6433 |           0.2887 |
[32m[20221213 15:19:41 @agent_ppo2.py:185][0m |          -0.0124 |          17.8228 |           0.2890 |
[32m[20221213 15:19:41 @agent_ppo2.py:185][0m |          -0.0172 |          17.5834 |           0.2890 |
[32m[20221213 15:19:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.34
[32m[20221213 15:19:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.42
[32m[20221213 15:19:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.95
[32m[20221213 15:19:41 @agent_ppo2.py:143][0m Total time:      26.76 min
[32m[20221213 15:19:41 @agent_ppo2.py:145][0m 2412544 total steps have happened
[32m[20221213 15:19:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1178 --------------------------#
[32m[20221213 15:19:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:41 @agent_ppo2.py:185][0m |           0.0019 |          17.9625 |           0.2857 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |          -0.0057 |          17.6446 |           0.2851 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |          -0.0069 |          17.5120 |           0.2846 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |           0.0005 |          18.8991 |           0.2846 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |          -0.0085 |          17.3958 |           0.2835 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |          -0.0053 |          17.7679 |           0.2836 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |           0.0004 |          19.2235 |           0.2838 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |          -0.0135 |          17.2599 |           0.2831 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |          -0.0137 |          17.1907 |           0.2834 |
[32m[20221213 15:19:42 @agent_ppo2.py:185][0m |          -0.0156 |          17.1437 |           0.2832 |
[32m[20221213 15:19:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.67
[32m[20221213 15:19:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.72
[32m[20221213 15:19:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.68
[32m[20221213 15:19:42 @agent_ppo2.py:143][0m Total time:      26.79 min
[32m[20221213 15:19:42 @agent_ppo2.py:145][0m 2414592 total steps have happened
[32m[20221213 15:19:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1179 --------------------------#
[32m[20221213 15:19:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |           0.0019 |          17.7430 |           0.2818 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |          -0.0115 |          17.0290 |           0.2816 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |           0.0060 |          19.9047 |           0.2820 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |          -0.0062 |          17.7866 |           0.2827 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |          -0.0152 |          16.7679 |           0.2827 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |          -0.0157 |          16.6396 |           0.2826 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |          -0.0156 |          16.5713 |           0.2824 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |          -0.0179 |          16.5594 |           0.2826 |
[32m[20221213 15:19:43 @agent_ppo2.py:185][0m |          -0.0172 |          16.4563 |           0.2824 |
[32m[20221213 15:19:44 @agent_ppo2.py:185][0m |          -0.0023 |          18.9896 |           0.2825 |
[32m[20221213 15:19:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.29
[32m[20221213 15:19:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.38
[32m[20221213 15:19:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.97
[32m[20221213 15:19:44 @agent_ppo2.py:143][0m Total time:      26.81 min
[32m[20221213 15:19:44 @agent_ppo2.py:145][0m 2416640 total steps have happened
[32m[20221213 15:19:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1180 --------------------------#
[32m[20221213 15:19:44 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:19:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:44 @agent_ppo2.py:185][0m |          -0.0034 |          18.1796 |           0.2802 |
[32m[20221213 15:19:44 @agent_ppo2.py:185][0m |          -0.0080 |          17.5521 |           0.2801 |
[32m[20221213 15:19:44 @agent_ppo2.py:185][0m |          -0.0132 |          17.3975 |           0.2801 |
[32m[20221213 15:19:44 @agent_ppo2.py:185][0m |          -0.0124 |          17.2834 |           0.2800 |
[32m[20221213 15:19:44 @agent_ppo2.py:185][0m |          -0.0135 |          17.1657 |           0.2801 |
[32m[20221213 15:19:45 @agent_ppo2.py:185][0m |          -0.0154 |          17.0831 |           0.2800 |
[32m[20221213 15:19:45 @agent_ppo2.py:185][0m |          -0.0162 |          17.0024 |           0.2803 |
[32m[20221213 15:19:45 @agent_ppo2.py:185][0m |          -0.0147 |          17.0543 |           0.2804 |
[32m[20221213 15:19:45 @agent_ppo2.py:185][0m |          -0.0129 |          17.0609 |           0.2802 |
[32m[20221213 15:19:45 @agent_ppo2.py:185][0m |          -0.0183 |          16.8195 |           0.2804 |
[32m[20221213 15:19:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.31
[32m[20221213 15:19:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.42
[32m[20221213 15:19:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.99
[32m[20221213 15:19:45 @agent_ppo2.py:143][0m Total time:      26.83 min
[32m[20221213 15:19:45 @agent_ppo2.py:145][0m 2418688 total steps have happened
[32m[20221213 15:19:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1181 --------------------------#
[32m[20221213 15:19:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:45 @agent_ppo2.py:185][0m |          -0.0003 |          17.8484 |           0.2875 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0073 |          17.5195 |           0.2866 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0113 |          17.3477 |           0.2870 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0118 |          17.2538 |           0.2863 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0096 |          17.1683 |           0.2865 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0131 |          17.1296 |           0.2867 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0157 |          17.0554 |           0.2871 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0166 |          17.0186 |           0.2871 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0187 |          16.9543 |           0.2868 |
[32m[20221213 15:19:46 @agent_ppo2.py:185][0m |          -0.0152 |          16.8938 |           0.2871 |
[32m[20221213 15:19:46 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.62
[32m[20221213 15:19:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.58
[32m[20221213 15:19:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.88
[32m[20221213 15:19:46 @agent_ppo2.py:143][0m Total time:      26.85 min
[32m[20221213 15:19:46 @agent_ppo2.py:145][0m 2420736 total steps have happened
[32m[20221213 15:19:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1182 --------------------------#
[32m[20221213 15:19:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |           0.0009 |          18.0047 |           0.2980 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0061 |          17.7742 |           0.2977 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0092 |          17.5964 |           0.2976 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0090 |          17.4939 |           0.2972 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0100 |          17.3800 |           0.2971 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0091 |          17.3650 |           0.2967 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0114 |          17.2425 |           0.2963 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0052 |          18.1639 |           0.2964 |
[32m[20221213 15:19:47 @agent_ppo2.py:185][0m |          -0.0086 |          17.2436 |           0.2959 |
[32m[20221213 15:19:48 @agent_ppo2.py:185][0m |          -0.0140 |          17.0596 |           0.2957 |
[32m[20221213 15:19:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.94
[32m[20221213 15:19:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.92
[32m[20221213 15:19:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.63
[32m[20221213 15:19:48 @agent_ppo2.py:143][0m Total time:      26.88 min
[32m[20221213 15:19:48 @agent_ppo2.py:145][0m 2422784 total steps have happened
[32m[20221213 15:19:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1183 --------------------------#
[32m[20221213 15:19:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:48 @agent_ppo2.py:185][0m |           0.0003 |          17.4840 |           0.2902 |
[32m[20221213 15:19:48 @agent_ppo2.py:185][0m |          -0.0035 |          17.2428 |           0.2899 |
[32m[20221213 15:19:48 @agent_ppo2.py:185][0m |          -0.0093 |          17.0300 |           0.2896 |
[32m[20221213 15:19:48 @agent_ppo2.py:185][0m |          -0.0083 |          16.9268 |           0.2897 |
[32m[20221213 15:19:48 @agent_ppo2.py:185][0m |          -0.0139 |          16.8397 |           0.2898 |
[32m[20221213 15:19:49 @agent_ppo2.py:185][0m |          -0.0134 |          16.6264 |           0.2899 |
[32m[20221213 15:19:49 @agent_ppo2.py:185][0m |          -0.0140 |          16.6025 |           0.2896 |
[32m[20221213 15:19:49 @agent_ppo2.py:185][0m |          -0.0150 |          16.5404 |           0.2902 |
[32m[20221213 15:19:49 @agent_ppo2.py:185][0m |          -0.0150 |          16.5002 |           0.2901 |
[32m[20221213 15:19:49 @agent_ppo2.py:185][0m |          -0.0187 |          16.4722 |           0.2902 |
[32m[20221213 15:19:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.40
[32m[20221213 15:19:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.77
[32m[20221213 15:19:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.68
[32m[20221213 15:19:49 @agent_ppo2.py:143][0m Total time:      26.90 min
[32m[20221213 15:19:49 @agent_ppo2.py:145][0m 2424832 total steps have happened
[32m[20221213 15:19:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1184 --------------------------#
[32m[20221213 15:19:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:49 @agent_ppo2.py:185][0m |          -0.0029 |          18.4352 |           0.2907 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0049 |          18.2919 |           0.2908 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0123 |          17.8746 |           0.2903 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0102 |          17.8177 |           0.2905 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0151 |          17.6212 |           0.2903 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0126 |          17.4570 |           0.2902 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0138 |          17.3715 |           0.2902 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0132 |          17.4151 |           0.2905 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0180 |          17.1780 |           0.2908 |
[32m[20221213 15:19:50 @agent_ppo2.py:185][0m |          -0.0177 |          17.0734 |           0.2904 |
[32m[20221213 15:19:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.20
[32m[20221213 15:19:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.68
[32m[20221213 15:19:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.89
[32m[20221213 15:19:50 @agent_ppo2.py:143][0m Total time:      26.92 min
[32m[20221213 15:19:50 @agent_ppo2.py:145][0m 2426880 total steps have happened
[32m[20221213 15:19:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1185 --------------------------#
[32m[20221213 15:19:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0025 |          16.8306 |           0.2927 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0057 |          16.4658 |           0.2920 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0080 |          16.1947 |           0.2916 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0110 |          16.0444 |           0.2912 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0129 |          15.9338 |           0.2910 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0152 |          15.8458 |           0.2904 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0162 |          15.7654 |           0.2905 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0159 |          15.7205 |           0.2905 |
[32m[20221213 15:19:51 @agent_ppo2.py:185][0m |          -0.0142 |          15.7217 |           0.2896 |
[32m[20221213 15:19:52 @agent_ppo2.py:185][0m |          -0.0181 |          15.6062 |           0.2899 |
[32m[20221213 15:19:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.39
[32m[20221213 15:19:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.57
[32m[20221213 15:19:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.54
[32m[20221213 15:19:52 @agent_ppo2.py:143][0m Total time:      26.94 min
[32m[20221213 15:19:52 @agent_ppo2.py:145][0m 2428928 total steps have happened
[32m[20221213 15:19:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1186 --------------------------#
[32m[20221213 15:19:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:52 @agent_ppo2.py:185][0m |           0.0113 |          21.1203 |           0.2864 |
[32m[20221213 15:19:52 @agent_ppo2.py:185][0m |          -0.0041 |          18.5764 |           0.2858 |
[32m[20221213 15:19:52 @agent_ppo2.py:185][0m |           0.0027 |          19.8664 |           0.2858 |
[32m[20221213 15:19:52 @agent_ppo2.py:185][0m |           0.0041 |          20.2757 |           0.2858 |
[32m[20221213 15:19:52 @agent_ppo2.py:185][0m |          -0.0087 |          18.2380 |           0.2855 |
[32m[20221213 15:19:53 @agent_ppo2.py:185][0m |          -0.0123 |          18.0733 |           0.2857 |
[32m[20221213 15:19:53 @agent_ppo2.py:185][0m |          -0.0136 |          17.9971 |           0.2858 |
[32m[20221213 15:19:53 @agent_ppo2.py:185][0m |          -0.0119 |          17.9567 |           0.2860 |
[32m[20221213 15:19:53 @agent_ppo2.py:185][0m |          -0.0105 |          17.8703 |           0.2857 |
[32m[20221213 15:19:53 @agent_ppo2.py:185][0m |          -0.0146 |          17.8499 |           0.2858 |
[32m[20221213 15:19:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.05
[32m[20221213 15:19:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.57
[32m[20221213 15:19:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.48
[32m[20221213 15:19:53 @agent_ppo2.py:143][0m Total time:      26.96 min
[32m[20221213 15:19:53 @agent_ppo2.py:145][0m 2430976 total steps have happened
[32m[20221213 15:19:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1187 --------------------------#
[32m[20221213 15:19:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:53 @agent_ppo2.py:185][0m |           0.0020 |          17.7434 |           0.2936 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0099 |          17.1423 |           0.2928 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0106 |          16.9505 |           0.2922 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |           0.0004 |          17.9283 |           0.2920 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0164 |          16.3554 |           0.2916 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0159 |          16.0816 |           0.2908 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0162 |          15.8970 |           0.2910 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0177 |          15.6976 |           0.2906 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0190 |          15.5534 |           0.2906 |
[32m[20221213 15:19:54 @agent_ppo2.py:185][0m |          -0.0207 |          15.4001 |           0.2903 |
[32m[20221213 15:19:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.36
[32m[20221213 15:19:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.91
[32m[20221213 15:19:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.04
[32m[20221213 15:19:54 @agent_ppo2.py:143][0m Total time:      26.99 min
[32m[20221213 15:19:54 @agent_ppo2.py:145][0m 2433024 total steps have happened
[32m[20221213 15:19:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1188 --------------------------#
[32m[20221213 15:19:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:19:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |          -0.0023 |          17.4865 |           0.2947 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |           0.0014 |          17.1378 |           0.2942 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |          -0.0075 |          16.5188 |           0.2941 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |          -0.0001 |          16.6171 |           0.2932 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |          -0.0024 |          16.6317 |           0.2930 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |           0.0015 |          18.1209 |           0.2929 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |          -0.0144 |          15.9456 |           0.2922 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |          -0.0124 |          15.7845 |           0.2917 |
[32m[20221213 15:19:55 @agent_ppo2.py:185][0m |          -0.0127 |          15.7283 |           0.2916 |
[32m[20221213 15:19:56 @agent_ppo2.py:185][0m |          -0.0140 |          15.6245 |           0.2911 |
[32m[20221213 15:19:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.08
[32m[20221213 15:19:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.90
[32m[20221213 15:19:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.24
[32m[20221213 15:19:56 @agent_ppo2.py:143][0m Total time:      27.01 min
[32m[20221213 15:19:56 @agent_ppo2.py:145][0m 2435072 total steps have happened
[32m[20221213 15:19:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1189 --------------------------#
[32m[20221213 15:19:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:56 @agent_ppo2.py:185][0m |           0.0011 |          18.3995 |           0.2856 |
[32m[20221213 15:19:56 @agent_ppo2.py:185][0m |          -0.0090 |          17.4702 |           0.2851 |
[32m[20221213 15:19:56 @agent_ppo2.py:185][0m |          -0.0106 |          17.1041 |           0.2851 |
[32m[20221213 15:19:56 @agent_ppo2.py:185][0m |          -0.0045 |          18.4458 |           0.2850 |
[32m[20221213 15:19:56 @agent_ppo2.py:185][0m |          -0.0139 |          16.7180 |           0.2843 |
[32m[20221213 15:19:57 @agent_ppo2.py:185][0m |          -0.0147 |          16.5227 |           0.2843 |
[32m[20221213 15:19:57 @agent_ppo2.py:185][0m |          -0.0128 |          16.4758 |           0.2842 |
[32m[20221213 15:19:57 @agent_ppo2.py:185][0m |          -0.0154 |          16.3523 |           0.2840 |
[32m[20221213 15:19:57 @agent_ppo2.py:185][0m |          -0.0226 |          16.1932 |           0.2841 |
[32m[20221213 15:19:57 @agent_ppo2.py:185][0m |          -0.0207 |          16.1192 |           0.2838 |
[32m[20221213 15:19:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:19:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.62
[32m[20221213 15:19:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.43
[32m[20221213 15:19:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.86
[32m[20221213 15:19:57 @agent_ppo2.py:143][0m Total time:      27.03 min
[32m[20221213 15:19:57 @agent_ppo2.py:145][0m 2437120 total steps have happened
[32m[20221213 15:19:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1190 --------------------------#
[32m[20221213 15:19:57 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:19:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:57 @agent_ppo2.py:185][0m |          -0.0006 |          19.0966 |           0.2830 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |           0.0083 |          20.3331 |           0.2824 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0066 |          18.4160 |           0.2816 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0088 |          18.2141 |           0.2817 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0113 |          18.0577 |           0.2821 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0131 |          17.9289 |           0.2820 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0140 |          17.8075 |           0.2816 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0139 |          17.7293 |           0.2816 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0081 |          18.4008 |           0.2816 |
[32m[20221213 15:19:58 @agent_ppo2.py:185][0m |          -0.0142 |          17.6090 |           0.2811 |
[32m[20221213 15:19:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:19:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.86
[32m[20221213 15:19:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.28
[32m[20221213 15:19:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.18
[32m[20221213 15:19:58 @agent_ppo2.py:143][0m Total time:      27.05 min
[32m[20221213 15:19:58 @agent_ppo2.py:145][0m 2439168 total steps have happened
[32m[20221213 15:19:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1191 --------------------------#
[32m[20221213 15:19:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:19:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0035 |          18.2314 |           0.2803 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0078 |          17.4637 |           0.2802 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0103 |          16.9195 |           0.2799 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0134 |          16.5467 |           0.2800 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0138 |          16.2852 |           0.2801 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0157 |          15.9624 |           0.2799 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0086 |          16.5213 |           0.2801 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0162 |          15.5891 |           0.2799 |
[32m[20221213 15:19:59 @agent_ppo2.py:185][0m |          -0.0188 |          15.4104 |           0.2803 |
[32m[20221213 15:20:00 @agent_ppo2.py:185][0m |          -0.0189 |          15.2528 |           0.2806 |
[32m[20221213 15:20:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.08
[32m[20221213 15:20:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.89
[32m[20221213 15:20:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.45
[32m[20221213 15:20:00 @agent_ppo2.py:143][0m Total time:      27.08 min
[32m[20221213 15:20:00 @agent_ppo2.py:145][0m 2441216 total steps have happened
[32m[20221213 15:20:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1192 --------------------------#
[32m[20221213 15:20:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:00 @agent_ppo2.py:185][0m |          -0.0014 |          19.9904 |           0.2820 |
[32m[20221213 15:20:00 @agent_ppo2.py:185][0m |           0.0006 |          19.8302 |           0.2814 |
[32m[20221213 15:20:00 @agent_ppo2.py:185][0m |          -0.0077 |          19.1536 |           0.2818 |
[32m[20221213 15:20:00 @agent_ppo2.py:185][0m |          -0.0127 |          19.0581 |           0.2814 |
[32m[20221213 15:20:00 @agent_ppo2.py:185][0m |          -0.0132 |          18.9564 |           0.2815 |
[32m[20221213 15:20:01 @agent_ppo2.py:185][0m |          -0.0132 |          18.8375 |           0.2818 |
[32m[20221213 15:20:01 @agent_ppo2.py:185][0m |          -0.0146 |          18.8388 |           0.2819 |
[32m[20221213 15:20:01 @agent_ppo2.py:185][0m |          -0.0134 |          18.8000 |           0.2819 |
[32m[20221213 15:20:01 @agent_ppo2.py:185][0m |          -0.0119 |          18.9263 |           0.2822 |
[32m[20221213 15:20:01 @agent_ppo2.py:185][0m |          -0.0086 |          19.4072 |           0.2823 |
[32m[20221213 15:20:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.44
[32m[20221213 15:20:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.98
[32m[20221213 15:20:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.49
[32m[20221213 15:20:01 @agent_ppo2.py:143][0m Total time:      27.10 min
[32m[20221213 15:20:01 @agent_ppo2.py:145][0m 2443264 total steps have happened
[32m[20221213 15:20:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1193 --------------------------#
[32m[20221213 15:20:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:01 @agent_ppo2.py:185][0m |           0.0000 |          19.3118 |           0.2794 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0036 |          19.8609 |           0.2788 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0102 |          18.6764 |           0.2790 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0055 |          19.3460 |           0.2790 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0108 |          18.4564 |           0.2790 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0147 |          18.2350 |           0.2791 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0157 |          18.1180 |           0.2788 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0180 |          17.9912 |           0.2788 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0180 |          17.8403 |           0.2787 |
[32m[20221213 15:20:02 @agent_ppo2.py:185][0m |          -0.0198 |          17.7240 |           0.2788 |
[32m[20221213 15:20:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.17
[32m[20221213 15:20:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.33
[32m[20221213 15:20:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.79
[32m[20221213 15:20:02 @agent_ppo2.py:143][0m Total time:      27.12 min
[32m[20221213 15:20:02 @agent_ppo2.py:145][0m 2445312 total steps have happened
[32m[20221213 15:20:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1194 --------------------------#
[32m[20221213 15:20:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0024 |          18.6628 |           0.2788 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |           0.0017 |          19.3271 |           0.2778 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0121 |          18.0983 |           0.2772 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0129 |          18.0580 |           0.2770 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0151 |          17.8760 |           0.2767 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0115 |          17.9130 |           0.2764 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0159 |          17.7452 |           0.2764 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0183 |          17.6615 |           0.2761 |
[32m[20221213 15:20:03 @agent_ppo2.py:185][0m |          -0.0176 |          17.6129 |           0.2759 |
[32m[20221213 15:20:04 @agent_ppo2.py:185][0m |          -0.0177 |          17.5724 |           0.2759 |
[32m[20221213 15:20:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.60
[32m[20221213 15:20:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.34
[32m[20221213 15:20:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.78
[32m[20221213 15:20:04 @agent_ppo2.py:143][0m Total time:      27.14 min
[32m[20221213 15:20:04 @agent_ppo2.py:145][0m 2447360 total steps have happened
[32m[20221213 15:20:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1195 --------------------------#
[32m[20221213 15:20:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:04 @agent_ppo2.py:185][0m |          -0.0008 |          19.4016 |           0.2891 |
[32m[20221213 15:20:04 @agent_ppo2.py:185][0m |          -0.0033 |          19.2066 |           0.2891 |
[32m[20221213 15:20:04 @agent_ppo2.py:185][0m |          -0.0086 |          18.9170 |           0.2891 |
[32m[20221213 15:20:04 @agent_ppo2.py:185][0m |          -0.0153 |          18.7924 |           0.2889 |
[32m[20221213 15:20:04 @agent_ppo2.py:185][0m |          -0.0165 |          18.6538 |           0.2893 |
[32m[20221213 15:20:05 @agent_ppo2.py:185][0m |          -0.0153 |          18.6125 |           0.2895 |
[32m[20221213 15:20:05 @agent_ppo2.py:185][0m |          -0.0187 |          18.5133 |           0.2896 |
[32m[20221213 15:20:05 @agent_ppo2.py:185][0m |          -0.0199 |          18.4660 |           0.2897 |
[32m[20221213 15:20:05 @agent_ppo2.py:185][0m |          -0.0104 |          19.5150 |           0.2900 |
[32m[20221213 15:20:05 @agent_ppo2.py:185][0m |          -0.0192 |          18.3812 |           0.2901 |
[32m[20221213 15:20:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.49
[32m[20221213 15:20:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.48
[32m[20221213 15:20:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.93
[32m[20221213 15:20:05 @agent_ppo2.py:143][0m Total time:      27.17 min
[32m[20221213 15:20:05 @agent_ppo2.py:145][0m 2449408 total steps have happened
[32m[20221213 15:20:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1196 --------------------------#
[32m[20221213 15:20:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:05 @agent_ppo2.py:185][0m |          -0.0031 |          18.4860 |           0.2903 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0043 |          18.6092 |           0.2898 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0113 |          18.0705 |           0.2893 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0125 |          17.9560 |           0.2889 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0127 |          17.8787 |           0.2888 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0134 |          17.8586 |           0.2888 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0154 |          17.7643 |           0.2884 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0030 |          19.3391 |           0.2885 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0131 |          17.6512 |           0.2880 |
[32m[20221213 15:20:06 @agent_ppo2.py:185][0m |          -0.0093 |          18.8228 |           0.2880 |
[32m[20221213 15:20:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.56
[32m[20221213 15:20:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.11
[32m[20221213 15:20:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.51
[32m[20221213 15:20:06 @agent_ppo2.py:143][0m Total time:      27.19 min
[32m[20221213 15:20:06 @agent_ppo2.py:145][0m 2451456 total steps have happened
[32m[20221213 15:20:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1197 --------------------------#
[32m[20221213 15:20:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0014 |          19.6797 |           0.2794 |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0084 |          19.2291 |           0.2788 |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0003 |          20.5895 |           0.2786 |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0113 |          18.9923 |           0.2785 |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0128 |          18.9320 |           0.2785 |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0145 |          18.8367 |           0.2788 |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0156 |          18.8018 |           0.2788 |
[32m[20221213 15:20:07 @agent_ppo2.py:185][0m |          -0.0110 |          18.9372 |           0.2787 |
[32m[20221213 15:20:08 @agent_ppo2.py:185][0m |          -0.0065 |          21.0910 |           0.2788 |
[32m[20221213 15:20:08 @agent_ppo2.py:185][0m |          -0.0174 |          18.6881 |           0.2786 |
[32m[20221213 15:20:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.85
[32m[20221213 15:20:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.56
[32m[20221213 15:20:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.38
[32m[20221213 15:20:08 @agent_ppo2.py:143][0m Total time:      27.21 min
[32m[20221213 15:20:08 @agent_ppo2.py:145][0m 2453504 total steps have happened
[32m[20221213 15:20:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1198 --------------------------#
[32m[20221213 15:20:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:08 @agent_ppo2.py:185][0m |          -0.0029 |          19.1377 |           0.2837 |
[32m[20221213 15:20:08 @agent_ppo2.py:185][0m |          -0.0073 |          18.6674 |           0.2834 |
[32m[20221213 15:20:08 @agent_ppo2.py:185][0m |          -0.0092 |          18.3435 |           0.2835 |
[32m[20221213 15:20:08 @agent_ppo2.py:185][0m |          -0.0100 |          18.2447 |           0.2829 |
[32m[20221213 15:20:08 @agent_ppo2.py:185][0m |          -0.0148 |          17.9578 |           0.2829 |
[32m[20221213 15:20:09 @agent_ppo2.py:185][0m |          -0.0147 |          17.8391 |           0.2827 |
[32m[20221213 15:20:09 @agent_ppo2.py:185][0m |          -0.0136 |          17.8009 |           0.2829 |
[32m[20221213 15:20:09 @agent_ppo2.py:185][0m |          -0.0171 |          17.5847 |           0.2828 |
[32m[20221213 15:20:09 @agent_ppo2.py:185][0m |          -0.0140 |          17.5798 |           0.2828 |
[32m[20221213 15:20:09 @agent_ppo2.py:185][0m |          -0.0092 |          18.0606 |           0.2828 |
[32m[20221213 15:20:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.00
[32m[20221213 15:20:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.24
[32m[20221213 15:20:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.73
[32m[20221213 15:20:09 @agent_ppo2.py:143][0m Total time:      27.23 min
[32m[20221213 15:20:09 @agent_ppo2.py:145][0m 2455552 total steps have happened
[32m[20221213 15:20:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1199 --------------------------#
[32m[20221213 15:20:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:09 @agent_ppo2.py:185][0m |           0.0017 |          18.8278 |           0.2887 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0078 |          18.6329 |           0.2888 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0110 |          18.5047 |           0.2884 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0126 |          18.4474 |           0.2885 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0133 |          18.3820 |           0.2886 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0134 |          18.2906 |           0.2887 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0100 |          19.3651 |           0.2887 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0169 |          18.2354 |           0.2885 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0096 |          18.6835 |           0.2887 |
[32m[20221213 15:20:10 @agent_ppo2.py:185][0m |          -0.0168 |          18.1259 |           0.2883 |
[32m[20221213 15:20:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.41
[32m[20221213 15:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.25
[32m[20221213 15:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.24
[32m[20221213 15:20:10 @agent_ppo2.py:143][0m Total time:      27.25 min
[32m[20221213 15:20:10 @agent_ppo2.py:145][0m 2457600 total steps have happened
[32m[20221213 15:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1200 --------------------------#
[32m[20221213 15:20:11 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:20:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |           0.0026 |          18.4025 |           0.2843 |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |          -0.0057 |          17.9646 |           0.2839 |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |          -0.0066 |          17.6982 |           0.2840 |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |          -0.0113 |          17.3170 |           0.2840 |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |          -0.0119 |          17.0539 |           0.2837 |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |          -0.0136 |          16.8511 |           0.2836 |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |          -0.0143 |          16.6554 |           0.2836 |
[32m[20221213 15:20:11 @agent_ppo2.py:185][0m |          -0.0162 |          16.5681 |           0.2833 |
[32m[20221213 15:20:12 @agent_ppo2.py:185][0m |          -0.0170 |          16.4401 |           0.2830 |
[32m[20221213 15:20:12 @agent_ppo2.py:185][0m |          -0.0160 |          16.3284 |           0.2833 |
[32m[20221213 15:20:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.62
[32m[20221213 15:20:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.44
[32m[20221213 15:20:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.59
[32m[20221213 15:20:12 @agent_ppo2.py:143][0m Total time:      27.28 min
[32m[20221213 15:20:12 @agent_ppo2.py:145][0m 2459648 total steps have happened
[32m[20221213 15:20:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1201 --------------------------#
[32m[20221213 15:20:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:12 @agent_ppo2.py:185][0m |           0.0054 |          19.0513 |           0.2876 |
[32m[20221213 15:20:12 @agent_ppo2.py:185][0m |           0.0058 |          19.3654 |           0.2870 |
[32m[20221213 15:20:12 @agent_ppo2.py:185][0m |          -0.0053 |          17.9060 |           0.2862 |
[32m[20221213 15:20:12 @agent_ppo2.py:185][0m |          -0.0079 |          17.5683 |           0.2873 |
[32m[20221213 15:20:12 @agent_ppo2.py:185][0m |           0.0001 |          19.2483 |           0.2875 |
[32m[20221213 15:20:13 @agent_ppo2.py:185][0m |          -0.0101 |          17.1986 |           0.2873 |
[32m[20221213 15:20:13 @agent_ppo2.py:185][0m |          -0.0114 |          16.9773 |           0.2874 |
[32m[20221213 15:20:13 @agent_ppo2.py:185][0m |          -0.0132 |          16.8854 |           0.2876 |
[32m[20221213 15:20:13 @agent_ppo2.py:185][0m |          -0.0036 |          17.2806 |           0.2876 |
[32m[20221213 15:20:13 @agent_ppo2.py:185][0m |          -0.0127 |          16.6872 |           0.2876 |
[32m[20221213 15:20:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.83
[32m[20221213 15:20:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.26
[32m[20221213 15:20:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.83
[32m[20221213 15:20:13 @agent_ppo2.py:143][0m Total time:      27.30 min
[32m[20221213 15:20:13 @agent_ppo2.py:145][0m 2461696 total steps have happened
[32m[20221213 15:20:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1202 --------------------------#
[32m[20221213 15:20:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:13 @agent_ppo2.py:185][0m |           0.0112 |          21.1816 |           0.2960 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0066 |          19.0172 |           0.2951 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0137 |          18.5467 |           0.2947 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0043 |          18.9840 |           0.2943 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0068 |          18.7173 |           0.2939 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0142 |          18.0352 |           0.2937 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0081 |          18.1401 |           0.2931 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0168 |          17.8229 |           0.2933 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0171 |          17.7400 |           0.2929 |
[32m[20221213 15:20:14 @agent_ppo2.py:185][0m |          -0.0148 |          17.6151 |           0.2923 |
[32m[20221213 15:20:14 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.02
[32m[20221213 15:20:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.97
[32m[20221213 15:20:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.78
[32m[20221213 15:20:14 @agent_ppo2.py:143][0m Total time:      27.32 min
[32m[20221213 15:20:14 @agent_ppo2.py:145][0m 2463744 total steps have happened
[32m[20221213 15:20:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1203 --------------------------#
[32m[20221213 15:20:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0006 |          19.9212 |           0.2912 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0066 |          19.5407 |           0.2906 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0102 |          19.4007 |           0.2904 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0097 |          19.3958 |           0.2905 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0126 |          19.1663 |           0.2904 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0130 |          19.1162 |           0.2902 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0126 |          19.0878 |           0.2900 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0123 |          19.0513 |           0.2900 |
[32m[20221213 15:20:15 @agent_ppo2.py:185][0m |          -0.0167 |          18.9829 |           0.2898 |
[32m[20221213 15:20:16 @agent_ppo2.py:185][0m |          -0.0133 |          19.1179 |           0.2900 |
[32m[20221213 15:20:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.98
[32m[20221213 15:20:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.25
[32m[20221213 15:20:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.95
[32m[20221213 15:20:16 @agent_ppo2.py:143][0m Total time:      27.34 min
[32m[20221213 15:20:16 @agent_ppo2.py:145][0m 2465792 total steps have happened
[32m[20221213 15:20:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1204 --------------------------#
[32m[20221213 15:20:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:16 @agent_ppo2.py:185][0m |           0.0129 |          19.9771 |           0.2814 |
[32m[20221213 15:20:16 @agent_ppo2.py:185][0m |          -0.0053 |          17.6349 |           0.2807 |
[32m[20221213 15:20:16 @agent_ppo2.py:185][0m |          -0.0116 |          17.4826 |           0.2805 |
[32m[20221213 15:20:16 @agent_ppo2.py:185][0m |          -0.0015 |          19.6878 |           0.2809 |
[32m[20221213 15:20:16 @agent_ppo2.py:185][0m |          -0.0142 |          17.3196 |           0.2807 |
[32m[20221213 15:20:17 @agent_ppo2.py:185][0m |          -0.0133 |          17.1728 |           0.2804 |
[32m[20221213 15:20:17 @agent_ppo2.py:185][0m |          -0.0153 |          17.0788 |           0.2805 |
[32m[20221213 15:20:17 @agent_ppo2.py:185][0m |          -0.0160 |          17.0085 |           0.2802 |
[32m[20221213 15:20:17 @agent_ppo2.py:185][0m |          -0.0155 |          16.9600 |           0.2804 |
[32m[20221213 15:20:17 @agent_ppo2.py:185][0m |          -0.0148 |          17.2011 |           0.2803 |
[32m[20221213 15:20:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.47
[32m[20221213 15:20:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 247.42
[32m[20221213 15:20:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.21
[32m[20221213 15:20:17 @agent_ppo2.py:143][0m Total time:      27.37 min
[32m[20221213 15:20:17 @agent_ppo2.py:145][0m 2467840 total steps have happened
[32m[20221213 15:20:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1205 --------------------------#
[32m[20221213 15:20:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:17 @agent_ppo2.py:185][0m |           0.0088 |          20.0803 |           0.2848 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0078 |          18.1074 |           0.2837 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0129 |          17.8245 |           0.2843 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0115 |          17.6181 |           0.2841 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0136 |          17.3739 |           0.2839 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0162 |          17.1947 |           0.2839 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0184 |          17.0005 |           0.2838 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0184 |          16.8333 |           0.2838 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0141 |          16.7260 |           0.2834 |
[32m[20221213 15:20:18 @agent_ppo2.py:185][0m |          -0.0182 |          16.5104 |           0.2838 |
[32m[20221213 15:20:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.69
[32m[20221213 15:20:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.03
[32m[20221213 15:20:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.23
[32m[20221213 15:20:18 @agent_ppo2.py:143][0m Total time:      27.39 min
[32m[20221213 15:20:18 @agent_ppo2.py:145][0m 2469888 total steps have happened
[32m[20221213 15:20:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1206 --------------------------#
[32m[20221213 15:20:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0020 |          18.8713 |           0.2831 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0084 |          18.4901 |           0.2823 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0051 |          18.3664 |           0.2821 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0109 |          18.1566 |           0.2820 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0149 |          18.0694 |           0.2819 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0131 |          17.9832 |           0.2819 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0153 |          17.8659 |           0.2818 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0203 |          17.8096 |           0.2817 |
[32m[20221213 15:20:19 @agent_ppo2.py:185][0m |          -0.0163 |          17.7495 |           0.2815 |
[32m[20221213 15:20:20 @agent_ppo2.py:185][0m |          -0.0192 |          17.6577 |           0.2818 |
[32m[20221213 15:20:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.98
[32m[20221213 15:20:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 238.65
[32m[20221213 15:20:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.03
[32m[20221213 15:20:20 @agent_ppo2.py:143][0m Total time:      27.41 min
[32m[20221213 15:20:20 @agent_ppo2.py:145][0m 2471936 total steps have happened
[32m[20221213 15:20:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1207 --------------------------#
[32m[20221213 15:20:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:20 @agent_ppo2.py:185][0m |          -0.0002 |          18.1374 |           0.2923 |
[32m[20221213 15:20:20 @agent_ppo2.py:185][0m |          -0.0084 |          17.6533 |           0.2915 |
[32m[20221213 15:20:20 @agent_ppo2.py:185][0m |          -0.0118 |          17.2770 |           0.2914 |
[32m[20221213 15:20:20 @agent_ppo2.py:185][0m |          -0.0129 |          16.8702 |           0.2916 |
[32m[20221213 15:20:21 @agent_ppo2.py:185][0m |          -0.0125 |          16.5653 |           0.2914 |
[32m[20221213 15:20:21 @agent_ppo2.py:185][0m |          -0.0100 |          16.4765 |           0.2914 |
[32m[20221213 15:20:21 @agent_ppo2.py:185][0m |          -0.0161 |          16.0535 |           0.2913 |
[32m[20221213 15:20:21 @agent_ppo2.py:185][0m |          -0.0139 |          16.1657 |           0.2912 |
[32m[20221213 15:20:21 @agent_ppo2.py:185][0m |          -0.0172 |          15.7156 |           0.2912 |
[32m[20221213 15:20:21 @agent_ppo2.py:185][0m |          -0.0202 |          15.5402 |           0.2908 |
[32m[20221213 15:20:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.69
[32m[20221213 15:20:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.55
[32m[20221213 15:20:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.63
[32m[20221213 15:20:21 @agent_ppo2.py:143][0m Total time:      27.43 min
[32m[20221213 15:20:21 @agent_ppo2.py:145][0m 2473984 total steps have happened
[32m[20221213 15:20:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1208 --------------------------#
[32m[20221213 15:20:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:21 @agent_ppo2.py:185][0m |           0.0043 |          20.4205 |           0.2814 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0063 |          19.3643 |           0.2810 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0114 |          19.0244 |           0.2808 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0125 |          18.8337 |           0.2807 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0135 |          18.7343 |           0.2807 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0145 |          18.6434 |           0.2811 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0156 |          18.5540 |           0.2810 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0154 |          18.5277 |           0.2811 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0158 |          18.4453 |           0.2811 |
[32m[20221213 15:20:22 @agent_ppo2.py:185][0m |          -0.0180 |          18.4032 |           0.2811 |
[32m[20221213 15:20:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.86
[32m[20221213 15:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.42
[32m[20221213 15:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 189.29
[32m[20221213 15:20:22 @agent_ppo2.py:143][0m Total time:      27.45 min
[32m[20221213 15:20:22 @agent_ppo2.py:145][0m 2476032 total steps have happened
[32m[20221213 15:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1209 --------------------------#
[32m[20221213 15:20:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |           0.0005 |          18.1205 |           0.2946 |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |          -0.0102 |          17.6573 |           0.2940 |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |          -0.0086 |          17.6672 |           0.2939 |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |          -0.0126 |          17.4445 |           0.2938 |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |          -0.0128 |          17.3968 |           0.2940 |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |          -0.0145 |          17.3050 |           0.2939 |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |          -0.0138 |          17.2516 |           0.2942 |
[32m[20221213 15:20:23 @agent_ppo2.py:185][0m |          -0.0163 |          17.1993 |           0.2938 |
[32m[20221213 15:20:24 @agent_ppo2.py:185][0m |          -0.0156 |          17.1823 |           0.2939 |
[32m[20221213 15:20:24 @agent_ppo2.py:185][0m |          -0.0175 |          17.1149 |           0.2940 |
[32m[20221213 15:20:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.22
[32m[20221213 15:20:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.42
[32m[20221213 15:20:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.07
[32m[20221213 15:20:24 @agent_ppo2.py:143][0m Total time:      27.48 min
[32m[20221213 15:20:24 @agent_ppo2.py:145][0m 2478080 total steps have happened
[32m[20221213 15:20:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1210 --------------------------#
[32m[20221213 15:20:24 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:20:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:24 @agent_ppo2.py:185][0m |           0.0001 |          17.7155 |           0.2855 |
[32m[20221213 15:20:24 @agent_ppo2.py:185][0m |          -0.0047 |          17.2154 |           0.2849 |
[32m[20221213 15:20:24 @agent_ppo2.py:185][0m |          -0.0109 |          16.9692 |           0.2842 |
[32m[20221213 15:20:24 @agent_ppo2.py:185][0m |          -0.0145 |          16.6992 |           0.2840 |
[32m[20221213 15:20:25 @agent_ppo2.py:185][0m |          -0.0161 |          16.5593 |           0.2837 |
[32m[20221213 15:20:25 @agent_ppo2.py:185][0m |          -0.0168 |          16.4021 |           0.2833 |
[32m[20221213 15:20:25 @agent_ppo2.py:185][0m |          -0.0157 |          16.2722 |           0.2832 |
[32m[20221213 15:20:25 @agent_ppo2.py:185][0m |          -0.0171 |          16.1232 |           0.2830 |
[32m[20221213 15:20:25 @agent_ppo2.py:185][0m |          -0.0199 |          16.0426 |           0.2826 |
[32m[20221213 15:20:25 @agent_ppo2.py:185][0m |          -0.0175 |          15.9502 |           0.2826 |
[32m[20221213 15:20:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 236.50
[32m[20221213 15:20:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 242.08
[32m[20221213 15:20:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.00
[32m[20221213 15:20:25 @agent_ppo2.py:143][0m Total time:      27.50 min
[32m[20221213 15:20:25 @agent_ppo2.py:145][0m 2480128 total steps have happened
[32m[20221213 15:20:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1211 --------------------------#
[32m[20221213 15:20:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:25 @agent_ppo2.py:185][0m |          -0.0021 |          19.5728 |           0.2861 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0101 |          19.1784 |           0.2855 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0111 |          19.0029 |           0.2848 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0115 |          18.8240 |           0.2851 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0138 |          18.6952 |           0.2846 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0164 |          18.6234 |           0.2844 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0083 |          19.5276 |           0.2846 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0174 |          18.4369 |           0.2838 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0171 |          18.3402 |           0.2837 |
[32m[20221213 15:20:26 @agent_ppo2.py:185][0m |          -0.0180 |          18.2676 |           0.2840 |
[32m[20221213 15:20:26 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.70
[32m[20221213 15:20:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.26
[32m[20221213 15:20:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.05
[32m[20221213 15:20:26 @agent_ppo2.py:143][0m Total time:      27.52 min
[32m[20221213 15:20:26 @agent_ppo2.py:145][0m 2482176 total steps have happened
[32m[20221213 15:20:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1212 --------------------------#
[32m[20221213 15:20:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |           0.0031 |          18.7293 |           0.2809 |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |          -0.0056 |          17.6886 |           0.2798 |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |          -0.0093 |          17.3608 |           0.2797 |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |          -0.0108 |          17.1069 |           0.2790 |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |          -0.0168 |          16.9186 |           0.2788 |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |          -0.0139 |          16.7909 |           0.2790 |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |          -0.0127 |          16.5550 |           0.2785 |
[32m[20221213 15:20:27 @agent_ppo2.py:185][0m |          -0.0135 |          16.5392 |           0.2785 |
[32m[20221213 15:20:28 @agent_ppo2.py:185][0m |          -0.0167 |          16.3534 |           0.2783 |
[32m[20221213 15:20:28 @agent_ppo2.py:185][0m |          -0.0167 |          16.2677 |           0.2779 |
[32m[20221213 15:20:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.72
[32m[20221213 15:20:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.38
[32m[20221213 15:20:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.56
[32m[20221213 15:20:28 @agent_ppo2.py:143][0m Total time:      27.54 min
[32m[20221213 15:20:28 @agent_ppo2.py:145][0m 2484224 total steps have happened
[32m[20221213 15:20:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1213 --------------------------#
[32m[20221213 15:20:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:28 @agent_ppo2.py:185][0m |           0.0022 |          17.7455 |           0.2812 |
[32m[20221213 15:20:28 @agent_ppo2.py:185][0m |           0.0089 |          19.1692 |           0.2812 |
[32m[20221213 15:20:28 @agent_ppo2.py:185][0m |          -0.0107 |          16.9772 |           0.2809 |
[32m[20221213 15:20:28 @agent_ppo2.py:185][0m |          -0.0110 |          16.7165 |           0.2810 |
[32m[20221213 15:20:28 @agent_ppo2.py:185][0m |          -0.0119 |          16.5996 |           0.2812 |
[32m[20221213 15:20:29 @agent_ppo2.py:185][0m |          -0.0137 |          16.4931 |           0.2809 |
[32m[20221213 15:20:29 @agent_ppo2.py:185][0m |          -0.0133 |          16.6386 |           0.2811 |
[32m[20221213 15:20:29 @agent_ppo2.py:185][0m |          -0.0145 |          16.3233 |           0.2811 |
[32m[20221213 15:20:29 @agent_ppo2.py:185][0m |          -0.0187 |          16.2849 |           0.2811 |
[32m[20221213 15:20:29 @agent_ppo2.py:185][0m |          -0.0180 |          16.2065 |           0.2811 |
[32m[20221213 15:20:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.50
[32m[20221213 15:20:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.88
[32m[20221213 15:20:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.84
[32m[20221213 15:20:29 @agent_ppo2.py:143][0m Total time:      27.57 min
[32m[20221213 15:20:29 @agent_ppo2.py:145][0m 2486272 total steps have happened
[32m[20221213 15:20:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1214 --------------------------#
[32m[20221213 15:20:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |           0.0066 |          19.7850 |           0.2732 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0033 |          18.9315 |           0.2729 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0101 |          18.6507 |           0.2724 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0074 |          18.6220 |           0.2727 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0125 |          18.3788 |           0.2728 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |           0.0107 |          22.2434 |           0.2727 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0022 |          19.3761 |           0.2729 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0142 |          18.2164 |           0.2728 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0188 |          18.0818 |           0.2728 |
[32m[20221213 15:20:30 @agent_ppo2.py:185][0m |          -0.0170 |          18.0425 |           0.2731 |
[32m[20221213 15:20:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.52
[32m[20221213 15:20:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.75
[32m[20221213 15:20:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.64
[32m[20221213 15:20:30 @agent_ppo2.py:143][0m Total time:      27.59 min
[32m[20221213 15:20:30 @agent_ppo2.py:145][0m 2488320 total steps have happened
[32m[20221213 15:20:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1215 --------------------------#
[32m[20221213 15:20:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0013 |          19.0783 |           0.2854 |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0065 |          18.6888 |           0.2848 |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0109 |          18.3749 |           0.2845 |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0070 |          18.6306 |           0.2844 |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0159 |          17.9892 |           0.2834 |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0168 |          17.8664 |           0.2837 |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0166 |          17.6914 |           0.2831 |
[32m[20221213 15:20:31 @agent_ppo2.py:185][0m |          -0.0037 |          19.4453 |           0.2829 |
[32m[20221213 15:20:32 @agent_ppo2.py:185][0m |          -0.0039 |          20.3178 |           0.2826 |
[32m[20221213 15:20:32 @agent_ppo2.py:185][0m |          -0.0096 |          18.5458 |           0.2822 |
[32m[20221213 15:20:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.55
[32m[20221213 15:20:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.22
[32m[20221213 15:20:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.33
[32m[20221213 15:20:32 @agent_ppo2.py:143][0m Total time:      27.61 min
[32m[20221213 15:20:32 @agent_ppo2.py:145][0m 2490368 total steps have happened
[32m[20221213 15:20:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1216 --------------------------#
[32m[20221213 15:20:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:32 @agent_ppo2.py:185][0m |          -0.0021 |          18.7549 |           0.2763 |
[32m[20221213 15:20:32 @agent_ppo2.py:185][0m |          -0.0077 |          18.3449 |           0.2757 |
[32m[20221213 15:20:32 @agent_ppo2.py:185][0m |          -0.0153 |          18.1649 |           0.2762 |
[32m[20221213 15:20:32 @agent_ppo2.py:185][0m |          -0.0122 |          17.9621 |           0.2762 |
[32m[20221213 15:20:33 @agent_ppo2.py:185][0m |          -0.0137 |          17.8535 |           0.2769 |
[32m[20221213 15:20:33 @agent_ppo2.py:185][0m |          -0.0145 |          17.7408 |           0.2766 |
[32m[20221213 15:20:33 @agent_ppo2.py:185][0m |          -0.0177 |          17.7124 |           0.2771 |
[32m[20221213 15:20:33 @agent_ppo2.py:185][0m |          -0.0131 |          17.8840 |           0.2774 |
[32m[20221213 15:20:33 @agent_ppo2.py:185][0m |          -0.0164 |          17.5529 |           0.2771 |
[32m[20221213 15:20:33 @agent_ppo2.py:185][0m |          -0.0147 |          17.5044 |           0.2778 |
[32m[20221213 15:20:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.28
[32m[20221213 15:20:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.78
[32m[20221213 15:20:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.61
[32m[20221213 15:20:33 @agent_ppo2.py:143][0m Total time:      27.63 min
[32m[20221213 15:20:33 @agent_ppo2.py:145][0m 2492416 total steps have happened
[32m[20221213 15:20:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1217 --------------------------#
[32m[20221213 15:20:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |           0.0007 |          18.5760 |           0.2897 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0064 |          18.4960 |           0.2889 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0077 |          18.4394 |           0.2885 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |           0.0025 |          20.3547 |           0.2883 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0132 |          18.3616 |           0.2883 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0122 |          18.2859 |           0.2882 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0140 |          18.2675 |           0.2878 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0068 |          18.8819 |           0.2879 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0151 |          18.2422 |           0.2877 |
[32m[20221213 15:20:34 @agent_ppo2.py:185][0m |          -0.0148 |          18.1898 |           0.2875 |
[32m[20221213 15:20:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.18
[32m[20221213 15:20:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.89
[32m[20221213 15:20:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.07
[32m[20221213 15:20:34 @agent_ppo2.py:143][0m Total time:      27.65 min
[32m[20221213 15:20:34 @agent_ppo2.py:145][0m 2494464 total steps have happened
[32m[20221213 15:20:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1218 --------------------------#
[32m[20221213 15:20:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0021 |          19.2190 |           0.2941 |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0080 |          18.6389 |           0.2936 |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0068 |          18.8430 |           0.2928 |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0135 |          18.2851 |           0.2930 |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0150 |          18.0877 |           0.2926 |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0151 |          17.9947 |           0.2923 |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0135 |          17.9107 |           0.2923 |
[32m[20221213 15:20:35 @agent_ppo2.py:185][0m |          -0.0181 |          17.7681 |           0.2922 |
[32m[20221213 15:20:36 @agent_ppo2.py:185][0m |          -0.0178 |          17.6391 |           0.2917 |
[32m[20221213 15:20:36 @agent_ppo2.py:185][0m |          -0.0175 |          17.5966 |           0.2921 |
[32m[20221213 15:20:36 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.21
[32m[20221213 15:20:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.04
[32m[20221213 15:20:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.81
[32m[20221213 15:20:36 @agent_ppo2.py:143][0m Total time:      27.68 min
[32m[20221213 15:20:36 @agent_ppo2.py:145][0m 2496512 total steps have happened
[32m[20221213 15:20:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1219 --------------------------#
[32m[20221213 15:20:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:36 @agent_ppo2.py:185][0m |           0.0029 |          18.9932 |           0.2801 |
[32m[20221213 15:20:36 @agent_ppo2.py:185][0m |          -0.0066 |          18.5548 |           0.2804 |
[32m[20221213 15:20:36 @agent_ppo2.py:185][0m |          -0.0072 |          18.3859 |           0.2802 |
[32m[20221213 15:20:36 @agent_ppo2.py:185][0m |          -0.0105 |          18.2956 |           0.2804 |
[32m[20221213 15:20:37 @agent_ppo2.py:185][0m |          -0.0150 |          18.1945 |           0.2807 |
[32m[20221213 15:20:37 @agent_ppo2.py:185][0m |          -0.0137 |          18.0990 |           0.2808 |
[32m[20221213 15:20:37 @agent_ppo2.py:185][0m |          -0.0137 |          18.0033 |           0.2809 |
[32m[20221213 15:20:37 @agent_ppo2.py:185][0m |          -0.0125 |          18.0026 |           0.2809 |
[32m[20221213 15:20:37 @agent_ppo2.py:185][0m |          -0.0171 |          17.9709 |           0.2810 |
[32m[20221213 15:20:37 @agent_ppo2.py:185][0m |          -0.0181 |          17.9558 |           0.2811 |
[32m[20221213 15:20:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.62
[32m[20221213 15:20:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.40
[32m[20221213 15:20:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.68
[32m[20221213 15:20:37 @agent_ppo2.py:143][0m Total time:      27.70 min
[32m[20221213 15:20:37 @agent_ppo2.py:145][0m 2498560 total steps have happened
[32m[20221213 15:20:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1220 --------------------------#
[32m[20221213 15:20:37 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:20:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |           0.0008 |          18.9249 |           0.2864 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0037 |          18.6926 |           0.2866 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0075 |          18.5127 |           0.2865 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0110 |          18.4301 |           0.2865 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |           0.0023 |          19.8930 |           0.2866 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0104 |          18.2593 |           0.2863 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0142 |          18.1569 |           0.2865 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0110 |          18.0730 |           0.2866 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0163 |          18.0689 |           0.2866 |
[32m[20221213 15:20:38 @agent_ppo2.py:185][0m |          -0.0146 |          17.9992 |           0.2867 |
[32m[20221213 15:20:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.75
[32m[20221213 15:20:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.38
[32m[20221213 15:20:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.82
[32m[20221213 15:20:38 @agent_ppo2.py:143][0m Total time:      27.72 min
[32m[20221213 15:20:38 @agent_ppo2.py:145][0m 2500608 total steps have happened
[32m[20221213 15:20:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1221 --------------------------#
[32m[20221213 15:20:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0006 |          18.4390 |           0.2801 |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0039 |          18.2323 |           0.2797 |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0062 |          18.1722 |           0.2790 |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0106 |          18.1538 |           0.2789 |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0102 |          18.0753 |           0.2784 |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0105 |          18.0297 |           0.2785 |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0119 |          18.0479 |           0.2786 |
[32m[20221213 15:20:39 @agent_ppo2.py:185][0m |          -0.0125 |          18.0031 |           0.2787 |
[32m[20221213 15:20:40 @agent_ppo2.py:185][0m |          -0.0113 |          17.9704 |           0.2785 |
[32m[20221213 15:20:40 @agent_ppo2.py:185][0m |          -0.0133 |          17.9802 |           0.2788 |
[32m[20221213 15:20:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.71
[32m[20221213 15:20:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.60
[32m[20221213 15:20:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.26
[32m[20221213 15:20:40 @agent_ppo2.py:143][0m Total time:      27.74 min
[32m[20221213 15:20:40 @agent_ppo2.py:145][0m 2502656 total steps have happened
[32m[20221213 15:20:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1222 --------------------------#
[32m[20221213 15:20:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:40 @agent_ppo2.py:185][0m |          -0.0015 |          18.4990 |           0.2924 |
[32m[20221213 15:20:40 @agent_ppo2.py:185][0m |          -0.0003 |          18.7200 |           0.2922 |
[32m[20221213 15:20:40 @agent_ppo2.py:185][0m |          -0.0068 |          18.2454 |           0.2921 |
[32m[20221213 15:20:40 @agent_ppo2.py:185][0m |           0.0128 |          21.8926 |           0.2919 |
[32m[20221213 15:20:41 @agent_ppo2.py:185][0m |           0.0070 |          20.9632 |           0.2906 |
[32m[20221213 15:20:41 @agent_ppo2.py:185][0m |           0.0144 |          20.6602 |           0.2905 |
[32m[20221213 15:20:41 @agent_ppo2.py:185][0m |          -0.0102 |          18.1324 |           0.2906 |
[32m[20221213 15:20:41 @agent_ppo2.py:185][0m |          -0.0074 |          18.3960 |           0.2906 |
[32m[20221213 15:20:41 @agent_ppo2.py:185][0m |          -0.0126 |          18.0326 |           0.2901 |
[32m[20221213 15:20:41 @agent_ppo2.py:185][0m |          -0.0032 |          19.4505 |           0.2899 |
[32m[20221213 15:20:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.06
[32m[20221213 15:20:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.37
[32m[20221213 15:20:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.18
[32m[20221213 15:20:41 @agent_ppo2.py:143][0m Total time:      27.77 min
[32m[20221213 15:20:41 @agent_ppo2.py:145][0m 2504704 total steps have happened
[32m[20221213 15:20:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1223 --------------------------#
[32m[20221213 15:20:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:41 @agent_ppo2.py:185][0m |           0.0076 |          20.4925 |           0.2916 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0066 |          18.9207 |           0.2912 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0085 |          18.7828 |           0.2911 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0112 |          18.7357 |           0.2913 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0123 |          18.6674 |           0.2914 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0137 |          18.6360 |           0.2915 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0142 |          18.6104 |           0.2916 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0060 |          19.1646 |           0.2914 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0138 |          18.5869 |           0.2914 |
[32m[20221213 15:20:42 @agent_ppo2.py:185][0m |          -0.0123 |          18.5515 |           0.2920 |
[32m[20221213 15:20:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.17
[32m[20221213 15:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.34
[32m[20221213 15:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.21
[32m[20221213 15:20:42 @agent_ppo2.py:143][0m Total time:      27.79 min
[32m[20221213 15:20:42 @agent_ppo2.py:145][0m 2506752 total steps have happened
[32m[20221213 15:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1224 --------------------------#
[32m[20221213 15:20:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |          -0.0010 |          17.7668 |           0.2801 |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |           0.0035 |          17.8924 |           0.2801 |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |          -0.0100 |          17.2427 |           0.2800 |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |          -0.0123 |          17.0772 |           0.2801 |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |          -0.0129 |          16.9346 |           0.2804 |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |          -0.0148 |          16.8018 |           0.2806 |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |          -0.0142 |          16.6751 |           0.2806 |
[32m[20221213 15:20:43 @agent_ppo2.py:185][0m |          -0.0133 |          16.5580 |           0.2806 |
[32m[20221213 15:20:44 @agent_ppo2.py:185][0m |          -0.0169 |          16.4582 |           0.2808 |
[32m[20221213 15:20:44 @agent_ppo2.py:185][0m |          -0.0190 |          16.3933 |           0.2808 |
[32m[20221213 15:20:44 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.67
[32m[20221213 15:20:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.72
[32m[20221213 15:20:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.35
[32m[20221213 15:20:44 @agent_ppo2.py:143][0m Total time:      27.81 min
[32m[20221213 15:20:44 @agent_ppo2.py:145][0m 2508800 total steps have happened
[32m[20221213 15:20:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1225 --------------------------#
[32m[20221213 15:20:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:44 @agent_ppo2.py:185][0m |          -0.0010 |          18.7275 |           0.2945 |
[32m[20221213 15:20:44 @agent_ppo2.py:185][0m |          -0.0056 |          18.4170 |           0.2941 |
[32m[20221213 15:20:44 @agent_ppo2.py:185][0m |          -0.0088 |          18.2881 |           0.2938 |
[32m[20221213 15:20:44 @agent_ppo2.py:185][0m |          -0.0081 |          18.4282 |           0.2938 |
[32m[20221213 15:20:45 @agent_ppo2.py:185][0m |          -0.0123 |          18.0766 |           0.2933 |
[32m[20221213 15:20:45 @agent_ppo2.py:185][0m |          -0.0121 |          18.0267 |           0.2934 |
[32m[20221213 15:20:45 @agent_ppo2.py:185][0m |          -0.0147 |          17.9552 |           0.2930 |
[32m[20221213 15:20:45 @agent_ppo2.py:185][0m |          -0.0165 |          17.9132 |           0.2929 |
[32m[20221213 15:20:45 @agent_ppo2.py:185][0m |          -0.0147 |          17.8661 |           0.2927 |
[32m[20221213 15:20:45 @agent_ppo2.py:185][0m |          -0.0163 |          17.7995 |           0.2929 |
[32m[20221213 15:20:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.48
[32m[20221213 15:20:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.27
[32m[20221213 15:20:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.43
[32m[20221213 15:20:45 @agent_ppo2.py:143][0m Total time:      27.83 min
[32m[20221213 15:20:45 @agent_ppo2.py:145][0m 2510848 total steps have happened
[32m[20221213 15:20:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1226 --------------------------#
[32m[20221213 15:20:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0007 |          17.0223 |           0.2904 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0114 |          16.0657 |           0.2898 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0143 |          15.3539 |           0.2895 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0126 |          14.8487 |           0.2894 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0168 |          14.4987 |           0.2896 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0181 |          14.2483 |           0.2892 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0181 |          14.0456 |           0.2891 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0193 |          13.8909 |           0.2891 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0188 |          13.7626 |           0.2891 |
[32m[20221213 15:20:46 @agent_ppo2.py:185][0m |          -0.0190 |          13.6471 |           0.2893 |
[32m[20221213 15:20:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.32
[32m[20221213 15:20:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.99
[32m[20221213 15:20:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.03
[32m[20221213 15:20:46 @agent_ppo2.py:143][0m Total time:      27.86 min
[32m[20221213 15:20:46 @agent_ppo2.py:145][0m 2512896 total steps have happened
[32m[20221213 15:20:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1227 --------------------------#
[32m[20221213 15:20:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0020 |          19.4103 |           0.2971 |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0111 |          18.9926 |           0.2977 |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0125 |          18.8273 |           0.2971 |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0169 |          18.6247 |           0.2975 |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0182 |          18.4924 |           0.2974 |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0177 |          18.5718 |           0.2974 |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0175 |          18.2792 |           0.2976 |
[32m[20221213 15:20:47 @agent_ppo2.py:185][0m |          -0.0205 |          18.2459 |           0.2977 |
[32m[20221213 15:20:48 @agent_ppo2.py:185][0m |          -0.0021 |          21.8262 |           0.2978 |
[32m[20221213 15:20:48 @agent_ppo2.py:185][0m |          -0.0173 |          18.3892 |           0.2970 |
[32m[20221213 15:20:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.61
[32m[20221213 15:20:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.92
[32m[20221213 15:20:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.46
[32m[20221213 15:20:48 @agent_ppo2.py:143][0m Total time:      27.88 min
[32m[20221213 15:20:48 @agent_ppo2.py:145][0m 2514944 total steps have happened
[32m[20221213 15:20:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1228 --------------------------#
[32m[20221213 15:20:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:48 @agent_ppo2.py:185][0m |          -0.0029 |          18.7843 |           0.2958 |
[32m[20221213 15:20:48 @agent_ppo2.py:185][0m |          -0.0082 |          18.2186 |           0.2946 |
[32m[20221213 15:20:48 @agent_ppo2.py:185][0m |          -0.0126 |          17.8425 |           0.2950 |
[32m[20221213 15:20:48 @agent_ppo2.py:185][0m |          -0.0033 |          18.4674 |           0.2941 |
[32m[20221213 15:20:49 @agent_ppo2.py:185][0m |          -0.0157 |          17.4157 |           0.2942 |
[32m[20221213 15:20:49 @agent_ppo2.py:185][0m |          -0.0121 |          17.2795 |           0.2943 |
[32m[20221213 15:20:49 @agent_ppo2.py:185][0m |          -0.0160 |          17.0222 |           0.2942 |
[32m[20221213 15:20:49 @agent_ppo2.py:185][0m |          -0.0069 |          18.2024 |           0.2944 |
[32m[20221213 15:20:49 @agent_ppo2.py:185][0m |          -0.0188 |          16.7489 |           0.2942 |
[32m[20221213 15:20:49 @agent_ppo2.py:185][0m |          -0.0153 |          16.5999 |           0.2937 |
[32m[20221213 15:20:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.12
[32m[20221213 15:20:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.79
[32m[20221213 15:20:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 234.29
[32m[20221213 15:20:49 @agent_ppo2.py:143][0m Total time:      27.90 min
[32m[20221213 15:20:49 @agent_ppo2.py:145][0m 2516992 total steps have happened
[32m[20221213 15:20:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1229 --------------------------#
[32m[20221213 15:20:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0045 |          19.7732 |           0.2979 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0071 |          19.2833 |           0.2970 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0106 |          19.1081 |           0.2969 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0121 |          18.8842 |           0.2973 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0111 |          18.7774 |           0.2970 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0078 |          18.9830 |           0.2970 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0139 |          18.5688 |           0.2970 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0136 |          18.8282 |           0.2968 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0162 |          18.3888 |           0.2967 |
[32m[20221213 15:20:50 @agent_ppo2.py:185][0m |          -0.0181 |          18.3722 |           0.2963 |
[32m[20221213 15:20:50 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.16
[32m[20221213 15:20:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.06
[32m[20221213 15:20:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.68
[32m[20221213 15:20:50 @agent_ppo2.py:143][0m Total time:      27.92 min
[32m[20221213 15:20:50 @agent_ppo2.py:145][0m 2519040 total steps have happened
[32m[20221213 15:20:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1230 --------------------------#
[32m[20221213 15:20:51 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:20:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0001 |          18.8779 |           0.2884 |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0055 |          17.9081 |           0.2882 |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0071 |          17.8409 |           0.2878 |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0099 |          17.4243 |           0.2876 |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0129 |          17.1998 |           0.2875 |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0146 |          17.1089 |           0.2872 |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0012 |          19.0866 |           0.2873 |
[32m[20221213 15:20:51 @agent_ppo2.py:185][0m |          -0.0135 |          16.9513 |           0.2864 |
[32m[20221213 15:20:52 @agent_ppo2.py:185][0m |          -0.0199 |          16.8164 |           0.2871 |
[32m[20221213 15:20:52 @agent_ppo2.py:185][0m |          -0.0159 |          16.6826 |           0.2870 |
[32m[20221213 15:20:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.45
[32m[20221213 15:20:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.29
[32m[20221213 15:20:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.94
[32m[20221213 15:20:52 @agent_ppo2.py:143][0m Total time:      27.94 min
[32m[20221213 15:20:52 @agent_ppo2.py:145][0m 2521088 total steps have happened
[32m[20221213 15:20:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1231 --------------------------#
[32m[20221213 15:20:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:52 @agent_ppo2.py:185][0m |          -0.0023 |          20.1619 |           0.2959 |
[32m[20221213 15:20:52 @agent_ppo2.py:185][0m |          -0.0053 |          19.5703 |           0.2960 |
[32m[20221213 15:20:52 @agent_ppo2.py:185][0m |          -0.0067 |          19.3981 |           0.2959 |
[32m[20221213 15:20:52 @agent_ppo2.py:185][0m |          -0.0064 |          19.2645 |           0.2959 |
[32m[20221213 15:20:53 @agent_ppo2.py:185][0m |           0.0048 |          21.0869 |           0.2965 |
[32m[20221213 15:20:53 @agent_ppo2.py:185][0m |          -0.0110 |          19.2395 |           0.2955 |
[32m[20221213 15:20:53 @agent_ppo2.py:185][0m |          -0.0105 |          19.1042 |           0.2959 |
[32m[20221213 15:20:53 @agent_ppo2.py:185][0m |           0.0010 |          20.7028 |           0.2960 |
[32m[20221213 15:20:53 @agent_ppo2.py:185][0m |          -0.0114 |          18.9624 |           0.2957 |
[32m[20221213 15:20:53 @agent_ppo2.py:185][0m |          -0.0106 |          19.0231 |           0.2960 |
[32m[20221213 15:20:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.63
[32m[20221213 15:20:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.85
[32m[20221213 15:20:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.93
[32m[20221213 15:20:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 305.93
[32m[20221213 15:20:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 305.93
[32m[20221213 15:20:53 @agent_ppo2.py:143][0m Total time:      27.97 min
[32m[20221213 15:20:53 @agent_ppo2.py:145][0m 2523136 total steps have happened
[32m[20221213 15:20:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1232 --------------------------#
[32m[20221213 15:20:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0003 |          19.7338 |           0.2973 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0087 |          19.4147 |           0.2970 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0096 |          19.2048 |           0.2969 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0093 |          19.0202 |           0.2973 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0130 |          18.9258 |           0.2969 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0120 |          18.8084 |           0.2973 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0148 |          18.7371 |           0.2974 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0063 |          19.5715 |           0.2972 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0152 |          18.5468 |           0.2973 |
[32m[20221213 15:20:54 @agent_ppo2.py:185][0m |          -0.0141 |          18.4357 |           0.2973 |
[32m[20221213 15:20:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.56
[32m[20221213 15:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.64
[32m[20221213 15:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.45
[32m[20221213 15:20:54 @agent_ppo2.py:143][0m Total time:      27.99 min
[32m[20221213 15:20:54 @agent_ppo2.py:145][0m 2525184 total steps have happened
[32m[20221213 15:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1233 --------------------------#
[32m[20221213 15:20:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0018 |          17.8861 |           0.2928 |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0051 |          17.3886 |           0.2927 |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0120 |          17.1103 |           0.2924 |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0127 |          16.8672 |           0.2919 |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0127 |          16.6494 |           0.2917 |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0107 |          16.5036 |           0.2917 |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0151 |          16.2689 |           0.2913 |
[32m[20221213 15:20:55 @agent_ppo2.py:185][0m |          -0.0154 |          16.1254 |           0.2912 |
[32m[20221213 15:20:56 @agent_ppo2.py:185][0m |          -0.0128 |          16.1533 |           0.2911 |
[32m[20221213 15:20:56 @agent_ppo2.py:185][0m |          -0.0177 |          15.8836 |           0.2909 |
[32m[20221213 15:20:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.08
[32m[20221213 15:20:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.12
[32m[20221213 15:20:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.92
[32m[20221213 15:20:56 @agent_ppo2.py:143][0m Total time:      28.01 min
[32m[20221213 15:20:56 @agent_ppo2.py:145][0m 2527232 total steps have happened
[32m[20221213 15:20:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1234 --------------------------#
[32m[20221213 15:20:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:56 @agent_ppo2.py:185][0m |          -0.0025 |          17.8783 |           0.2918 |
[32m[20221213 15:20:56 @agent_ppo2.py:185][0m |           0.0041 |          17.9090 |           0.2916 |
[32m[20221213 15:20:56 @agent_ppo2.py:185][0m |          -0.0127 |          16.6321 |           0.2910 |
[32m[20221213 15:20:56 @agent_ppo2.py:185][0m |          -0.0137 |          16.2503 |           0.2913 |
[32m[20221213 15:20:57 @agent_ppo2.py:185][0m |          -0.0152 |          16.0168 |           0.2912 |
[32m[20221213 15:20:57 @agent_ppo2.py:185][0m |          -0.0172 |          15.8501 |           0.2908 |
[32m[20221213 15:20:57 @agent_ppo2.py:185][0m |          -0.0165 |          15.7834 |           0.2910 |
[32m[20221213 15:20:57 @agent_ppo2.py:185][0m |          -0.0039 |          17.8477 |           0.2910 |
[32m[20221213 15:20:57 @agent_ppo2.py:185][0m |          -0.0158 |          15.4789 |           0.2898 |
[32m[20221213 15:20:57 @agent_ppo2.py:185][0m |          -0.0208 |          15.3500 |           0.2909 |
[32m[20221213 15:20:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:20:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.83
[32m[20221213 15:20:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.26
[32m[20221213 15:20:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.35
[32m[20221213 15:20:57 @agent_ppo2.py:143][0m Total time:      28.03 min
[32m[20221213 15:20:57 @agent_ppo2.py:145][0m 2529280 total steps have happened
[32m[20221213 15:20:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1235 --------------------------#
[32m[20221213 15:20:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:20:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0007 |          19.9718 |           0.2918 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0088 |          18.7140 |           0.2919 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0090 |          18.3817 |           0.2912 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0083 |          18.1787 |           0.2913 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0094 |          18.1811 |           0.2908 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0108 |          18.0556 |           0.2905 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0179 |          17.7274 |           0.2905 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0094 |          18.0953 |           0.2902 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0207 |          17.5915 |           0.2901 |
[32m[20221213 15:20:58 @agent_ppo2.py:185][0m |          -0.0093 |          19.7125 |           0.2900 |
[32m[20221213 15:20:58 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:20:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 220.89
[32m[20221213 15:20:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 237.47
[32m[20221213 15:20:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.48
[32m[20221213 15:20:58 @agent_ppo2.py:143][0m Total time:      28.05 min
[32m[20221213 15:20:58 @agent_ppo2.py:145][0m 2531328 total steps have happened
[32m[20221213 15:20:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1236 --------------------------#
[32m[20221213 15:20:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:20:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |           0.0144 |          20.6866 |           0.2916 |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |          -0.0045 |          18.1141 |           0.2917 |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |          -0.0046 |          17.9482 |           0.2918 |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |          -0.0103 |          17.8008 |           0.2914 |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |          -0.0106 |          17.7021 |           0.2916 |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |          -0.0123 |          17.6131 |           0.2915 |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |          -0.0137 |          17.5297 |           0.2912 |
[32m[20221213 15:20:59 @agent_ppo2.py:185][0m |          -0.0121 |          17.6243 |           0.2911 |
[32m[20221213 15:21:00 @agent_ppo2.py:185][0m |          -0.0156 |          17.3977 |           0.2915 |
[32m[20221213 15:21:00 @agent_ppo2.py:185][0m |          -0.0072 |          17.8732 |           0.2913 |
[32m[20221213 15:21:00 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:21:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.63
[32m[20221213 15:21:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.69
[32m[20221213 15:21:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.61
[32m[20221213 15:21:00 @agent_ppo2.py:143][0m Total time:      28.08 min
[32m[20221213 15:21:00 @agent_ppo2.py:145][0m 2533376 total steps have happened
[32m[20221213 15:21:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1237 --------------------------#
[32m[20221213 15:21:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:00 @agent_ppo2.py:185][0m |          -0.0009 |          19.6648 |           0.2863 |
[32m[20221213 15:21:00 @agent_ppo2.py:185][0m |          -0.0070 |          19.1895 |           0.2858 |
[32m[20221213 15:21:00 @agent_ppo2.py:185][0m |          -0.0020 |          19.3171 |           0.2858 |
[32m[20221213 15:21:00 @agent_ppo2.py:185][0m |          -0.0112 |          18.8756 |           0.2855 |
[32m[20221213 15:21:01 @agent_ppo2.py:185][0m |          -0.0108 |          18.7702 |           0.2856 |
[32m[20221213 15:21:01 @agent_ppo2.py:185][0m |          -0.0109 |          18.6992 |           0.2857 |
[32m[20221213 15:21:01 @agent_ppo2.py:185][0m |          -0.0153 |          18.6459 |           0.2858 |
[32m[20221213 15:21:01 @agent_ppo2.py:185][0m |          -0.0127 |          18.5741 |           0.2857 |
[32m[20221213 15:21:01 @agent_ppo2.py:185][0m |          -0.0156 |          18.5267 |           0.2859 |
[32m[20221213 15:21:01 @agent_ppo2.py:185][0m |          -0.0151 |          18.5080 |           0.2860 |
[32m[20221213 15:21:01 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.29
[32m[20221213 15:21:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.38
[32m[20221213 15:21:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.56
[32m[20221213 15:21:01 @agent_ppo2.py:143][0m Total time:      28.10 min
[32m[20221213 15:21:01 @agent_ppo2.py:145][0m 2535424 total steps have happened
[32m[20221213 15:21:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1238 --------------------------#
[32m[20221213 15:21:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:01 @agent_ppo2.py:185][0m |          -0.0005 |          20.0062 |           0.2931 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0089 |          19.6811 |           0.2928 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0119 |          19.5901 |           0.2927 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0143 |          19.4518 |           0.2924 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0122 |          19.3744 |           0.2925 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0117 |          19.4986 |           0.2925 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0134 |          19.3387 |           0.2923 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0161 |          19.2433 |           0.2923 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0175 |          19.1839 |           0.2920 |
[32m[20221213 15:21:02 @agent_ppo2.py:185][0m |          -0.0175 |          19.1462 |           0.2922 |
[32m[20221213 15:21:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.52
[32m[20221213 15:21:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.11
[32m[20221213 15:21:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.77
[32m[20221213 15:21:02 @agent_ppo2.py:143][0m Total time:      28.12 min
[32m[20221213 15:21:02 @agent_ppo2.py:145][0m 2537472 total steps have happened
[32m[20221213 15:21:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1239 --------------------------#
[32m[20221213 15:21:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |           0.0025 |          19.9401 |           0.2973 |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |          -0.0066 |          19.3690 |           0.2963 |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |          -0.0098 |          19.1935 |           0.2966 |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |          -0.0117 |          19.0683 |           0.2967 |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |          -0.0124 |          18.9418 |           0.2970 |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |          -0.0042 |          19.9812 |           0.2967 |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |          -0.0155 |          18.7983 |           0.2963 |
[32m[20221213 15:21:03 @agent_ppo2.py:185][0m |          -0.0164 |          18.6978 |           0.2968 |
[32m[20221213 15:21:04 @agent_ppo2.py:185][0m |          -0.0177 |          18.6096 |           0.2968 |
[32m[20221213 15:21:04 @agent_ppo2.py:185][0m |          -0.0170 |          18.5108 |           0.2968 |
[32m[20221213 15:21:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.45
[32m[20221213 15:21:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.19
[32m[20221213 15:21:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.78
[32m[20221213 15:21:04 @agent_ppo2.py:143][0m Total time:      28.14 min
[32m[20221213 15:21:04 @agent_ppo2.py:145][0m 2539520 total steps have happened
[32m[20221213 15:21:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1240 --------------------------#
[32m[20221213 15:21:04 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:21:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:04 @agent_ppo2.py:185][0m |           0.0000 |          19.7483 |           0.2982 |
[32m[20221213 15:21:04 @agent_ppo2.py:185][0m |          -0.0080 |          19.4771 |           0.2974 |
[32m[20221213 15:21:04 @agent_ppo2.py:185][0m |          -0.0090 |          19.3637 |           0.2973 |
[32m[20221213 15:21:04 @agent_ppo2.py:185][0m |           0.0010 |          22.0321 |           0.2975 |
[32m[20221213 15:21:05 @agent_ppo2.py:185][0m |          -0.0096 |          19.1852 |           0.2965 |
[32m[20221213 15:21:05 @agent_ppo2.py:185][0m |          -0.0104 |          19.1396 |           0.2974 |
[32m[20221213 15:21:05 @agent_ppo2.py:185][0m |          -0.0128 |          19.0860 |           0.2976 |
[32m[20221213 15:21:05 @agent_ppo2.py:185][0m |          -0.0113 |          19.1042 |           0.2978 |
[32m[20221213 15:21:05 @agent_ppo2.py:185][0m |          -0.0147 |          18.9933 |           0.2980 |
[32m[20221213 15:21:05 @agent_ppo2.py:185][0m |          -0.0139 |          19.0097 |           0.2981 |
[32m[20221213 15:21:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.74
[32m[20221213 15:21:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.20
[32m[20221213 15:21:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.94
[32m[20221213 15:21:05 @agent_ppo2.py:143][0m Total time:      28.17 min
[32m[20221213 15:21:05 @agent_ppo2.py:145][0m 2541568 total steps have happened
[32m[20221213 15:21:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1241 --------------------------#
[32m[20221213 15:21:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0014 |          18.8382 |           0.3018 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0026 |          18.5008 |           0.3011 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0079 |          18.1221 |           0.3011 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0092 |          17.9497 |           0.3008 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0110 |          17.7678 |           0.3008 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0092 |          17.7025 |           0.3003 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0129 |          17.5795 |           0.3001 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0139 |          17.5215 |           0.3005 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0148 |          17.4282 |           0.3002 |
[32m[20221213 15:21:06 @agent_ppo2.py:185][0m |          -0.0146 |          17.3443 |           0.2999 |
[32m[20221213 15:21:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:21:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.69
[32m[20221213 15:21:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.48
[32m[20221213 15:21:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.14
[32m[20221213 15:21:06 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 306.14
[32m[20221213 15:21:06 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 306.14
[32m[20221213 15:21:06 @agent_ppo2.py:143][0m Total time:      28.19 min
[32m[20221213 15:21:06 @agent_ppo2.py:145][0m 2543616 total steps have happened
[32m[20221213 15:21:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1242 --------------------------#
[32m[20221213 15:21:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |           0.0090 |          22.1006 |           0.2942 |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |          -0.0062 |          20.1859 |           0.2939 |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |          -0.0114 |          19.8299 |           0.2937 |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |          -0.0120 |          19.7417 |           0.2939 |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |          -0.0139 |          19.6440 |           0.2938 |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |          -0.0142 |          19.5289 |           0.2940 |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |          -0.0165 |          19.4780 |           0.2944 |
[32m[20221213 15:21:07 @agent_ppo2.py:185][0m |          -0.0064 |          20.8683 |           0.2944 |
[32m[20221213 15:21:08 @agent_ppo2.py:185][0m |          -0.0128 |          19.3752 |           0.2942 |
[32m[20221213 15:21:08 @agent_ppo2.py:185][0m |          -0.0183 |          19.2275 |           0.2944 |
[32m[20221213 15:21:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:21:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 227.02
[32m[20221213 15:21:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.99
[32m[20221213 15:21:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.37
[32m[20221213 15:21:08 @agent_ppo2.py:143][0m Total time:      28.21 min
[32m[20221213 15:21:08 @agent_ppo2.py:145][0m 2545664 total steps have happened
[32m[20221213 15:21:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1243 --------------------------#
[32m[20221213 15:21:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:08 @agent_ppo2.py:185][0m |          -0.0026 |          20.9947 |           0.3052 |
[32m[20221213 15:21:08 @agent_ppo2.py:185][0m |          -0.0097 |          20.7094 |           0.3047 |
[32m[20221213 15:21:08 @agent_ppo2.py:185][0m |          -0.0133 |          20.5864 |           0.3043 |
[32m[20221213 15:21:09 @agent_ppo2.py:185][0m |          -0.0106 |          20.8373 |           0.3041 |
[32m[20221213 15:21:09 @agent_ppo2.py:185][0m |           0.0238 |          25.9257 |           0.3040 |
[32m[20221213 15:21:09 @agent_ppo2.py:185][0m |          -0.0133 |          20.4360 |           0.3040 |
[32m[20221213 15:21:09 @agent_ppo2.py:185][0m |          -0.0140 |          20.3107 |           0.3038 |
[32m[20221213 15:21:09 @agent_ppo2.py:185][0m |          -0.0185 |          20.2486 |           0.3037 |
[32m[20221213 15:21:09 @agent_ppo2.py:185][0m |          -0.0187 |          20.1751 |           0.3039 |
[32m[20221213 15:21:09 @agent_ppo2.py:185][0m |          -0.0167 |          20.1288 |           0.3037 |
[32m[20221213 15:21:09 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:21:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 217.20
[32m[20221213 15:21:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.79
[32m[20221213 15:21:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.11
[32m[20221213 15:21:09 @agent_ppo2.py:143][0m Total time:      28.24 min
[32m[20221213 15:21:09 @agent_ppo2.py:145][0m 2547712 total steps have happened
[32m[20221213 15:21:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1244 --------------------------#
[32m[20221213 15:21:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0015 |          19.9029 |           0.3019 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0094 |          19.5581 |           0.3014 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0099 |          19.3384 |           0.3010 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0113 |          19.1658 |           0.3009 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0140 |          19.0058 |           0.3008 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0140 |          18.9022 |           0.3013 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0049 |          19.8326 |           0.3011 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0160 |          18.6205 |           0.3010 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0103 |          18.7836 |           0.3011 |
[32m[20221213 15:21:10 @agent_ppo2.py:185][0m |          -0.0156 |          18.3414 |           0.3007 |
[32m[20221213 15:21:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:21:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.81
[32m[20221213 15:21:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.95
[32m[20221213 15:21:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.37
[32m[20221213 15:21:11 @agent_ppo2.py:143][0m Total time:      28.26 min
[32m[20221213 15:21:11 @agent_ppo2.py:145][0m 2549760 total steps have happened
[32m[20221213 15:21:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1245 --------------------------#
[32m[20221213 15:21:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:11 @agent_ppo2.py:185][0m |           0.0063 |          20.7177 |           0.2979 |
[32m[20221213 15:21:11 @agent_ppo2.py:185][0m |          -0.0057 |          19.6743 |           0.2973 |
[32m[20221213 15:21:11 @agent_ppo2.py:185][0m |          -0.0056 |          19.5441 |           0.2973 |
[32m[20221213 15:21:11 @agent_ppo2.py:185][0m |          -0.0118 |          19.3087 |           0.2969 |
[32m[20221213 15:21:11 @agent_ppo2.py:185][0m |          -0.0130 |          19.2194 |           0.2971 |
[32m[20221213 15:21:11 @agent_ppo2.py:185][0m |          -0.0078 |          19.3917 |           0.2970 |
[32m[20221213 15:21:12 @agent_ppo2.py:185][0m |          -0.0118 |          19.0747 |           0.2964 |
[32m[20221213 15:21:12 @agent_ppo2.py:185][0m |          -0.0174 |          18.9329 |           0.2968 |
[32m[20221213 15:21:12 @agent_ppo2.py:185][0m |          -0.0146 |          18.8653 |           0.2968 |
[32m[20221213 15:21:12 @agent_ppo2.py:185][0m |          -0.0164 |          18.8194 |           0.2967 |
[32m[20221213 15:21:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:21:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.13
[32m[20221213 15:21:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.58
[32m[20221213 15:21:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.85
[32m[20221213 15:21:12 @agent_ppo2.py:143][0m Total time:      28.28 min
[32m[20221213 15:21:12 @agent_ppo2.py:145][0m 2551808 total steps have happened
[32m[20221213 15:21:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1246 --------------------------#
[32m[20221213 15:21:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:12 @agent_ppo2.py:185][0m |          -0.0011 |          19.2271 |           0.2980 |
[32m[20221213 15:21:12 @agent_ppo2.py:185][0m |          -0.0054 |          19.1119 |           0.2974 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0118 |          18.7676 |           0.2976 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0151 |          18.6479 |           0.2975 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0096 |          19.1670 |           0.2971 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0080 |          19.4312 |           0.2974 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0169 |          18.3960 |           0.2975 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0184 |          18.2960 |           0.2977 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0174 |          18.2715 |           0.2978 |
[32m[20221213 15:21:13 @agent_ppo2.py:185][0m |          -0.0177 |          18.1695 |           0.2977 |
[32m[20221213 15:21:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:21:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.32
[32m[20221213 15:21:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.58
[32m[20221213 15:21:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.07
[32m[20221213 15:21:13 @agent_ppo2.py:143][0m Total time:      28.30 min
[32m[20221213 15:21:13 @agent_ppo2.py:145][0m 2553856 total steps have happened
[32m[20221213 15:21:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1247 --------------------------#
[32m[20221213 15:21:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |           0.0022 |          20.7901 |           0.3038 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0061 |          20.4037 |           0.3027 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0094 |          20.2234 |           0.3024 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0090 |          20.1413 |           0.3027 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0116 |          20.0608 |           0.3025 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0127 |          20.0181 |           0.3027 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0135 |          19.9627 |           0.3030 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0150 |          19.9279 |           0.3028 |
[32m[20221213 15:21:14 @agent_ppo2.py:185][0m |          -0.0132 |          19.8948 |           0.3032 |
[32m[20221213 15:21:15 @agent_ppo2.py:185][0m |          -0.0104 |          20.2501 |           0.3031 |
[32m[20221213 15:21:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.46
[32m[20221213 15:21:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.94
[32m[20221213 15:21:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.93
[32m[20221213 15:21:15 @agent_ppo2.py:143][0m Total time:      28.33 min
[32m[20221213 15:21:15 @agent_ppo2.py:145][0m 2555904 total steps have happened
[32m[20221213 15:21:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1248 --------------------------#
[32m[20221213 15:21:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:15 @agent_ppo2.py:185][0m |           0.0018 |          19.6553 |           0.3032 |
[32m[20221213 15:21:15 @agent_ppo2.py:185][0m |          -0.0051 |          19.0772 |           0.3027 |
[32m[20221213 15:21:15 @agent_ppo2.py:185][0m |          -0.0085 |          18.7631 |           0.3031 |
[32m[20221213 15:21:15 @agent_ppo2.py:185][0m |          -0.0107 |          18.5467 |           0.3032 |
[32m[20221213 15:21:15 @agent_ppo2.py:185][0m |          -0.0109 |          18.3443 |           0.3031 |
[32m[20221213 15:21:16 @agent_ppo2.py:185][0m |          -0.0102 |          18.1974 |           0.3030 |
[32m[20221213 15:21:16 @agent_ppo2.py:185][0m |          -0.0137 |          18.0729 |           0.3028 |
[32m[20221213 15:21:16 @agent_ppo2.py:185][0m |          -0.0115 |          18.0168 |           0.3032 |
[32m[20221213 15:21:16 @agent_ppo2.py:185][0m |          -0.0128 |          17.8844 |           0.3031 |
[32m[20221213 15:21:16 @agent_ppo2.py:185][0m |          -0.0157 |          17.8338 |           0.3034 |
[32m[20221213 15:21:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.45
[32m[20221213 15:21:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.93
[32m[20221213 15:21:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.50
[32m[20221213 15:21:16 @agent_ppo2.py:143][0m Total time:      28.35 min
[32m[20221213 15:21:16 @agent_ppo2.py:145][0m 2557952 total steps have happened
[32m[20221213 15:21:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1249 --------------------------#
[32m[20221213 15:21:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:16 @agent_ppo2.py:185][0m |          -0.0020 |          20.5704 |           0.2982 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0091 |          20.2590 |           0.2979 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0114 |          20.1571 |           0.2984 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0130 |          19.9773 |           0.2983 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0143 |          19.9165 |           0.2986 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0149 |          19.8361 |           0.2986 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0175 |          19.7473 |           0.2980 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0167 |          19.7355 |           0.2985 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0187 |          19.6592 |           0.2988 |
[32m[20221213 15:21:17 @agent_ppo2.py:185][0m |          -0.0186 |          19.6044 |           0.2988 |
[32m[20221213 15:21:17 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.28
[32m[20221213 15:21:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.59
[32m[20221213 15:21:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.08
[32m[20221213 15:21:17 @agent_ppo2.py:143][0m Total time:      28.37 min
[32m[20221213 15:21:17 @agent_ppo2.py:145][0m 2560000 total steps have happened
[32m[20221213 15:21:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1250 --------------------------#
[32m[20221213 15:21:18 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |           0.0003 |          20.7740 |           0.3031 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0080 |          20.3921 |           0.3022 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0102 |          20.1859 |           0.3017 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0099 |          20.0815 |           0.3015 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0116 |          19.9903 |           0.3013 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0126 |          19.9770 |           0.3010 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0088 |          20.0364 |           0.3006 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0132 |          19.7915 |           0.3005 |
[32m[20221213 15:21:18 @agent_ppo2.py:185][0m |          -0.0141 |          19.7664 |           0.3002 |
[32m[20221213 15:21:19 @agent_ppo2.py:185][0m |          -0.0146 |          19.6971 |           0.3001 |
[32m[20221213 15:21:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:21:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.66
[32m[20221213 15:21:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.66
[32m[20221213 15:21:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.53
[32m[20221213 15:21:19 @agent_ppo2.py:143][0m Total time:      28.39 min
[32m[20221213 15:21:19 @agent_ppo2.py:145][0m 2562048 total steps have happened
[32m[20221213 15:21:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1251 --------------------------#
[32m[20221213 15:21:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:19 @agent_ppo2.py:185][0m |           0.0002 |          18.0882 |           0.3046 |
[32m[20221213 15:21:19 @agent_ppo2.py:185][0m |          -0.0073 |          16.8591 |           0.3043 |
[32m[20221213 15:21:19 @agent_ppo2.py:185][0m |           0.0002 |          17.3523 |           0.3041 |
[32m[20221213 15:21:19 @agent_ppo2.py:185][0m |          -0.0102 |          16.2394 |           0.3041 |
[32m[20221213 15:21:19 @agent_ppo2.py:185][0m |          -0.0108 |          16.0894 |           0.3041 |
[32m[20221213 15:21:20 @agent_ppo2.py:185][0m |          -0.0127 |          15.8856 |           0.3046 |
[32m[20221213 15:21:20 @agent_ppo2.py:185][0m |          -0.0120 |          15.7838 |           0.3045 |
[32m[20221213 15:21:20 @agent_ppo2.py:185][0m |          -0.0153 |          15.5954 |           0.3044 |
[32m[20221213 15:21:20 @agent_ppo2.py:185][0m |          -0.0036 |          15.9647 |           0.3049 |
[32m[20221213 15:21:20 @agent_ppo2.py:185][0m |          -0.0146 |          15.4321 |           0.3046 |
[32m[20221213 15:21:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.61
[32m[20221213 15:21:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.67
[32m[20221213 15:21:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.25
[32m[20221213 15:21:20 @agent_ppo2.py:143][0m Total time:      28.42 min
[32m[20221213 15:21:20 @agent_ppo2.py:145][0m 2564096 total steps have happened
[32m[20221213 15:21:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1252 --------------------------#
[32m[20221213 15:21:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:20 @agent_ppo2.py:185][0m |          -0.0010 |          21.4298 |           0.3065 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0056 |          20.8297 |           0.3059 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0099 |          20.6666 |           0.3058 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0041 |          20.7963 |           0.3056 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0126 |          20.3840 |           0.3052 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0051 |          21.4419 |           0.3056 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0173 |          20.2344 |           0.3053 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0166 |          20.1516 |           0.3051 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0092 |          20.7706 |           0.3050 |
[32m[20221213 15:21:21 @agent_ppo2.py:185][0m |          -0.0172 |          20.0079 |           0.3046 |
[32m[20221213 15:21:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.50
[32m[20221213 15:21:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.35
[32m[20221213 15:21:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.83
[32m[20221213 15:21:21 @agent_ppo2.py:143][0m Total time:      28.44 min
[32m[20221213 15:21:21 @agent_ppo2.py:145][0m 2566144 total steps have happened
[32m[20221213 15:21:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1253 --------------------------#
[32m[20221213 15:21:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0023 |          21.4735 |           0.2981 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |           0.0071 |          24.3162 |           0.2977 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0030 |          21.2261 |           0.2969 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0107 |          20.9292 |           0.2967 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0039 |          22.1391 |           0.2966 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0141 |          20.5987 |           0.2959 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0153 |          20.4909 |           0.2963 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0176 |          20.4375 |           0.2963 |
[32m[20221213 15:21:22 @agent_ppo2.py:185][0m |          -0.0065 |          21.9814 |           0.2963 |
[32m[20221213 15:21:23 @agent_ppo2.py:185][0m |          -0.0068 |          21.2090 |           0.2961 |
[32m[20221213 15:21:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.88
[32m[20221213 15:21:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.68
[32m[20221213 15:21:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.80
[32m[20221213 15:21:23 @agent_ppo2.py:143][0m Total time:      28.46 min
[32m[20221213 15:21:23 @agent_ppo2.py:145][0m 2568192 total steps have happened
[32m[20221213 15:21:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1254 --------------------------#
[32m[20221213 15:21:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:23 @agent_ppo2.py:185][0m |          -0.0004 |          19.5978 |           0.3079 |
[32m[20221213 15:21:23 @agent_ppo2.py:185][0m |           0.0006 |          19.2431 |           0.3077 |
[32m[20221213 15:21:23 @agent_ppo2.py:185][0m |          -0.0078 |          18.5277 |           0.3071 |
[32m[20221213 15:21:23 @agent_ppo2.py:185][0m |          -0.0091 |          18.3125 |           0.3070 |
[32m[20221213 15:21:23 @agent_ppo2.py:185][0m |          -0.0140 |          18.0352 |           0.3066 |
[32m[20221213 15:21:24 @agent_ppo2.py:185][0m |          -0.0122 |          17.8437 |           0.3069 |
[32m[20221213 15:21:24 @agent_ppo2.py:185][0m |          -0.0165 |          17.6849 |           0.3067 |
[32m[20221213 15:21:24 @agent_ppo2.py:185][0m |          -0.0068 |          18.8489 |           0.3061 |
[32m[20221213 15:21:24 @agent_ppo2.py:185][0m |          -0.0177 |          17.3811 |           0.3060 |
[32m[20221213 15:21:24 @agent_ppo2.py:185][0m |          -0.0168 |          17.2581 |           0.3061 |
[32m[20221213 15:21:24 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.39
[32m[20221213 15:21:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.13
[32m[20221213 15:21:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.59
[32m[20221213 15:21:24 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 306.59
[32m[20221213 15:21:24 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 306.59
[32m[20221213 15:21:24 @agent_ppo2.py:143][0m Total time:      28.48 min
[32m[20221213 15:21:24 @agent_ppo2.py:145][0m 2570240 total steps have happened
[32m[20221213 15:21:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1255 --------------------------#
[32m[20221213 15:21:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:24 @agent_ppo2.py:185][0m |           0.0020 |          20.4135 |           0.2988 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0065 |          19.6553 |           0.2979 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |           0.0089 |          22.5000 |           0.2982 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0045 |          20.3391 |           0.2980 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0150 |          19.2137 |           0.2979 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0153 |          19.1469 |           0.2977 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0173 |          19.0383 |           0.2978 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0122 |          19.7351 |           0.2976 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0168 |          18.8722 |           0.2974 |
[32m[20221213 15:21:25 @agent_ppo2.py:185][0m |          -0.0168 |          18.8424 |           0.2975 |
[32m[20221213 15:21:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.65
[32m[20221213 15:21:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.77
[32m[20221213 15:21:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.45
[32m[20221213 15:21:25 @agent_ppo2.py:143][0m Total time:      28.50 min
[32m[20221213 15:21:25 @agent_ppo2.py:145][0m 2572288 total steps have happened
[32m[20221213 15:21:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1256 --------------------------#
[32m[20221213 15:21:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |           0.0079 |          21.4595 |           0.2963 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0080 |          19.2196 |           0.2953 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0087 |          18.9190 |           0.2960 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0050 |          19.1425 |           0.2954 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0108 |          18.6224 |           0.2952 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0066 |          18.9911 |           0.2954 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0138 |          18.4274 |           0.2950 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0044 |          19.3300 |           0.2949 |
[32m[20221213 15:21:26 @agent_ppo2.py:185][0m |          -0.0136 |          18.3786 |           0.2947 |
[32m[20221213 15:21:27 @agent_ppo2.py:185][0m |          -0.0159 |          18.2292 |           0.2948 |
[32m[20221213 15:21:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.15
[32m[20221213 15:21:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.58
[32m[20221213 15:21:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.45
[32m[20221213 15:21:27 @agent_ppo2.py:143][0m Total time:      28.53 min
[32m[20221213 15:21:27 @agent_ppo2.py:145][0m 2574336 total steps have happened
[32m[20221213 15:21:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1257 --------------------------#
[32m[20221213 15:21:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:27 @agent_ppo2.py:185][0m |          -0.0030 |          19.9796 |           0.3066 |
[32m[20221213 15:21:27 @agent_ppo2.py:185][0m |          -0.0075 |          19.6660 |           0.3060 |
[32m[20221213 15:21:27 @agent_ppo2.py:185][0m |          -0.0140 |          19.4892 |           0.3054 |
[32m[20221213 15:21:27 @agent_ppo2.py:185][0m |          -0.0130 |          19.3794 |           0.3052 |
[32m[20221213 15:21:27 @agent_ppo2.py:185][0m |          -0.0047 |          20.5339 |           0.3050 |
[32m[20221213 15:21:28 @agent_ppo2.py:185][0m |          -0.0131 |          19.1998 |           0.3042 |
[32m[20221213 15:21:28 @agent_ppo2.py:185][0m |          -0.0153 |          19.1001 |           0.3041 |
[32m[20221213 15:21:28 @agent_ppo2.py:185][0m |          -0.0123 |          19.1150 |           0.3039 |
[32m[20221213 15:21:28 @agent_ppo2.py:185][0m |          -0.0161 |          18.9660 |           0.3038 |
[32m[20221213 15:21:28 @agent_ppo2.py:185][0m |          -0.0004 |          21.5892 |           0.3037 |
[32m[20221213 15:21:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.55
[32m[20221213 15:21:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.65
[32m[20221213 15:21:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.29
[32m[20221213 15:21:28 @agent_ppo2.py:143][0m Total time:      28.55 min
[32m[20221213 15:21:28 @agent_ppo2.py:145][0m 2576384 total steps have happened
[32m[20221213 15:21:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1258 --------------------------#
[32m[20221213 15:21:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:28 @agent_ppo2.py:185][0m |          -0.0009 |          20.5861 |           0.2945 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0095 |          19.8339 |           0.2943 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0103 |          19.4705 |           0.2936 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0098 |          19.2227 |           0.2939 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0155 |          18.9789 |           0.2938 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0166 |          18.7999 |           0.2940 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0069 |          19.3529 |           0.2936 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0153 |          18.5446 |           0.2940 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0161 |          18.3097 |           0.2942 |
[32m[20221213 15:21:29 @agent_ppo2.py:185][0m |          -0.0189 |          18.1933 |           0.2943 |
[32m[20221213 15:21:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.52
[32m[20221213 15:21:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 252.31
[32m[20221213 15:21:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.30
[32m[20221213 15:21:29 @agent_ppo2.py:143][0m Total time:      28.57 min
[32m[20221213 15:21:29 @agent_ppo2.py:145][0m 2578432 total steps have happened
[32m[20221213 15:21:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1259 --------------------------#
[32m[20221213 15:21:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |           0.0044 |          20.0040 |           0.2933 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0064 |          19.4949 |           0.2917 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0101 |          19.3471 |           0.2923 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0122 |          19.1676 |           0.2925 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0140 |          19.0688 |           0.2924 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0140 |          18.9699 |           0.2926 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0060 |          20.1237 |           0.2922 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0123 |          19.0959 |           0.2924 |
[32m[20221213 15:21:30 @agent_ppo2.py:185][0m |          -0.0190 |          18.8073 |           0.2926 |
[32m[20221213 15:21:31 @agent_ppo2.py:185][0m |          -0.0215 |          18.7663 |           0.2926 |
[32m[20221213 15:21:31 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.00
[32m[20221213 15:21:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 232.12
[32m[20221213 15:21:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.60
[32m[20221213 15:21:31 @agent_ppo2.py:143][0m Total time:      28.59 min
[32m[20221213 15:21:31 @agent_ppo2.py:145][0m 2580480 total steps have happened
[32m[20221213 15:21:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1260 --------------------------#
[32m[20221213 15:21:31 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:21:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:31 @agent_ppo2.py:185][0m |           0.0042 |          19.5322 |           0.3018 |
[32m[20221213 15:21:31 @agent_ppo2.py:185][0m |          -0.0093 |          18.9624 |           0.3008 |
[32m[20221213 15:21:31 @agent_ppo2.py:185][0m |          -0.0066 |          18.8992 |           0.3008 |
[32m[20221213 15:21:31 @agent_ppo2.py:185][0m |          -0.0003 |          21.3859 |           0.2999 |
[32m[20221213 15:21:31 @agent_ppo2.py:185][0m |          -0.0112 |          18.4938 |           0.2999 |
[32m[20221213 15:21:32 @agent_ppo2.py:185][0m |          -0.0132 |          18.3616 |           0.2995 |
[32m[20221213 15:21:32 @agent_ppo2.py:185][0m |          -0.0073 |          18.7241 |           0.2993 |
[32m[20221213 15:21:32 @agent_ppo2.py:185][0m |          -0.0145 |          18.2332 |           0.2994 |
[32m[20221213 15:21:32 @agent_ppo2.py:185][0m |          -0.0082 |          19.1959 |           0.2991 |
[32m[20221213 15:21:32 @agent_ppo2.py:185][0m |          -0.0165 |          18.1278 |           0.2992 |
[32m[20221213 15:21:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.27
[32m[20221213 15:21:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.43
[32m[20221213 15:21:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.75
[32m[20221213 15:21:32 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 309.75
[32m[20221213 15:21:32 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 309.75
[32m[20221213 15:21:32 @agent_ppo2.py:143][0m Total time:      28.62 min
[32m[20221213 15:21:32 @agent_ppo2.py:145][0m 2582528 total steps have happened
[32m[20221213 15:21:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1261 --------------------------#
[32m[20221213 15:21:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:32 @agent_ppo2.py:185][0m |           0.0082 |          20.3196 |           0.2865 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0022 |          19.8585 |           0.2856 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0069 |          19.3722 |           0.2853 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0085 |          19.3398 |           0.2852 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0122 |          19.0756 |           0.2842 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0128 |          18.9992 |           0.2845 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0113 |          18.9061 |           0.2843 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0134 |          18.7704 |           0.2843 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0166 |          18.7350 |           0.2840 |
[32m[20221213 15:21:33 @agent_ppo2.py:185][0m |          -0.0165 |          18.6240 |           0.2840 |
[32m[20221213 15:21:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.10
[32m[20221213 15:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.91
[32m[20221213 15:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.02
[32m[20221213 15:21:33 @agent_ppo2.py:143][0m Total time:      28.64 min
[32m[20221213 15:21:33 @agent_ppo2.py:145][0m 2584576 total steps have happened
[32m[20221213 15:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1262 --------------------------#
[32m[20221213 15:21:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0034 |          20.1243 |           0.2921 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0081 |          19.7546 |           0.2917 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0095 |          19.5660 |           0.2911 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0086 |          19.5028 |           0.2912 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0108 |          19.4449 |           0.2907 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0131 |          19.4281 |           0.2908 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0162 |          19.3399 |           0.2906 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0132 |          19.2974 |           0.2904 |
[32m[20221213 15:21:34 @agent_ppo2.py:185][0m |          -0.0160 |          19.2782 |           0.2902 |
[32m[20221213 15:21:35 @agent_ppo2.py:185][0m |          -0.0157 |          19.2194 |           0.2901 |
[32m[20221213 15:21:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.49
[32m[20221213 15:21:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.53
[32m[20221213 15:21:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.67
[32m[20221213 15:21:35 @agent_ppo2.py:143][0m Total time:      28.66 min
[32m[20221213 15:21:35 @agent_ppo2.py:145][0m 2586624 total steps have happened
[32m[20221213 15:21:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1263 --------------------------#
[32m[20221213 15:21:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:35 @agent_ppo2.py:185][0m |          -0.0019 |          18.4713 |           0.2944 |
[32m[20221213 15:21:35 @agent_ppo2.py:185][0m |          -0.0056 |          18.1676 |           0.2934 |
[32m[20221213 15:21:35 @agent_ppo2.py:185][0m |          -0.0116 |          17.9535 |           0.2939 |
[32m[20221213 15:21:35 @agent_ppo2.py:185][0m |          -0.0111 |          17.8784 |           0.2939 |
[32m[20221213 15:21:35 @agent_ppo2.py:185][0m |          -0.0160 |          17.7277 |           0.2941 |
[32m[20221213 15:21:36 @agent_ppo2.py:185][0m |          -0.0151 |          17.7139 |           0.2941 |
[32m[20221213 15:21:36 @agent_ppo2.py:185][0m |          -0.0161 |          17.6304 |           0.2940 |
[32m[20221213 15:21:36 @agent_ppo2.py:185][0m |          -0.0204 |          17.5148 |           0.2939 |
[32m[20221213 15:21:36 @agent_ppo2.py:185][0m |          -0.0164 |          17.4894 |           0.2941 |
[32m[20221213 15:21:36 @agent_ppo2.py:185][0m |          -0.0201 |          17.4295 |           0.2941 |
[32m[20221213 15:21:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.59
[32m[20221213 15:21:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.21
[32m[20221213 15:21:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.76
[32m[20221213 15:21:36 @agent_ppo2.py:143][0m Total time:      28.68 min
[32m[20221213 15:21:36 @agent_ppo2.py:145][0m 2588672 total steps have happened
[32m[20221213 15:21:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1264 --------------------------#
[32m[20221213 15:21:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0029 |          18.9337 |           0.2891 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0099 |          18.5856 |           0.2884 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0125 |          18.3440 |           0.2884 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0140 |          18.1920 |           0.2883 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0024 |          20.0625 |           0.2883 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0129 |          18.0234 |           0.2871 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0171 |          17.8612 |           0.2876 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0174 |          17.7690 |           0.2874 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0183 |          17.7040 |           0.2872 |
[32m[20221213 15:21:37 @agent_ppo2.py:185][0m |          -0.0183 |          17.6045 |           0.2873 |
[32m[20221213 15:21:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.39
[32m[20221213 15:21:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.14
[32m[20221213 15:21:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.38
[32m[20221213 15:21:37 @agent_ppo2.py:143][0m Total time:      28.70 min
[32m[20221213 15:21:37 @agent_ppo2.py:145][0m 2590720 total steps have happened
[32m[20221213 15:21:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1265 --------------------------#
[32m[20221213 15:21:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |           0.0085 |          21.0602 |           0.2978 |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |          -0.0073 |          18.9789 |           0.2963 |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |           0.0030 |          21.5070 |           0.2970 |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |          -0.0113 |          18.6541 |           0.2956 |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |          -0.0139 |          18.3092 |           0.2958 |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |           0.0076 |          22.1089 |           0.2960 |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |          -0.0118 |          18.2226 |           0.2958 |
[32m[20221213 15:21:38 @agent_ppo2.py:185][0m |          -0.0168 |          17.9254 |           0.2957 |
[32m[20221213 15:21:39 @agent_ppo2.py:185][0m |          -0.0182 |          17.7923 |           0.2962 |
[32m[20221213 15:21:39 @agent_ppo2.py:185][0m |          -0.0192 |          17.7227 |           0.2959 |
[32m[20221213 15:21:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.10
[32m[20221213 15:21:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.81
[32m[20221213 15:21:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.74
[32m[20221213 15:21:39 @agent_ppo2.py:143][0m Total time:      28.73 min
[32m[20221213 15:21:39 @agent_ppo2.py:145][0m 2592768 total steps have happened
[32m[20221213 15:21:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1266 --------------------------#
[32m[20221213 15:21:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:39 @agent_ppo2.py:185][0m |           0.0006 |          19.4517 |           0.2935 |
[32m[20221213 15:21:39 @agent_ppo2.py:185][0m |          -0.0061 |          19.0258 |           0.2932 |
[32m[20221213 15:21:39 @agent_ppo2.py:185][0m |          -0.0085 |          18.8713 |           0.2931 |
[32m[20221213 15:21:39 @agent_ppo2.py:185][0m |          -0.0027 |          19.1925 |           0.2933 |
[32m[20221213 15:21:39 @agent_ppo2.py:185][0m |          -0.0127 |          18.6756 |           0.2930 |
[32m[20221213 15:21:40 @agent_ppo2.py:185][0m |          -0.0027 |          19.8992 |           0.2932 |
[32m[20221213 15:21:40 @agent_ppo2.py:185][0m |          -0.0110 |          18.5447 |           0.2930 |
[32m[20221213 15:21:40 @agent_ppo2.py:185][0m |          -0.0148 |          18.4917 |           0.2933 |
[32m[20221213 15:21:40 @agent_ppo2.py:185][0m |          -0.0005 |          20.2903 |           0.2935 |
[32m[20221213 15:21:40 @agent_ppo2.py:185][0m |          -0.0134 |          18.4641 |           0.2927 |
[32m[20221213 15:21:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.08
[32m[20221213 15:21:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.61
[32m[20221213 15:21:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.88
[32m[20221213 15:21:40 @agent_ppo2.py:143][0m Total time:      28.75 min
[32m[20221213 15:21:40 @agent_ppo2.py:145][0m 2594816 total steps have happened
[32m[20221213 15:21:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1267 --------------------------#
[32m[20221213 15:21:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0011 |          19.4911 |           0.2911 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0067 |          19.1448 |           0.2896 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |           0.0074 |          21.0247 |           0.2884 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0044 |          19.0940 |           0.2890 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0121 |          18.8340 |           0.2886 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0132 |          18.7200 |           0.2881 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0119 |          18.6510 |           0.2876 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0131 |          18.6499 |           0.2876 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0144 |          18.5796 |           0.2877 |
[32m[20221213 15:21:41 @agent_ppo2.py:185][0m |          -0.0142 |          18.5274 |           0.2876 |
[32m[20221213 15:21:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.88
[32m[20221213 15:21:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.18
[32m[20221213 15:21:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.82
[32m[20221213 15:21:41 @agent_ppo2.py:143][0m Total time:      28.77 min
[32m[20221213 15:21:41 @agent_ppo2.py:145][0m 2596864 total steps have happened
[32m[20221213 15:21:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1268 --------------------------#
[32m[20221213 15:21:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0025 |          19.9851 |           0.2802 |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0066 |          19.6830 |           0.2795 |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0096 |          19.4745 |           0.2800 |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0111 |          19.3407 |           0.2798 |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0123 |          19.2710 |           0.2795 |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0131 |          19.1735 |           0.2794 |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0122 |          19.0660 |           0.2797 |
[32m[20221213 15:21:42 @agent_ppo2.py:185][0m |          -0.0143 |          19.0181 |           0.2795 |
[32m[20221213 15:21:43 @agent_ppo2.py:185][0m |          -0.0150 |          18.9808 |           0.2798 |
[32m[20221213 15:21:43 @agent_ppo2.py:185][0m |          -0.0142 |          18.9163 |           0.2800 |
[32m[20221213 15:21:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.87
[32m[20221213 15:21:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.32
[32m[20221213 15:21:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.94
[32m[20221213 15:21:43 @agent_ppo2.py:143][0m Total time:      28.79 min
[32m[20221213 15:21:43 @agent_ppo2.py:145][0m 2598912 total steps have happened
[32m[20221213 15:21:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1269 --------------------------#
[32m[20221213 15:21:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:43 @agent_ppo2.py:185][0m |          -0.0007 |          19.1717 |           0.2921 |
[32m[20221213 15:21:43 @agent_ppo2.py:185][0m |          -0.0068 |          18.9537 |           0.2920 |
[32m[20221213 15:21:43 @agent_ppo2.py:185][0m |          -0.0117 |          18.8218 |           0.2918 |
[32m[20221213 15:21:43 @agent_ppo2.py:185][0m |          -0.0123 |          18.7122 |           0.2921 |
[32m[20221213 15:21:43 @agent_ppo2.py:185][0m |          -0.0117 |          18.6478 |           0.2923 |
[32m[20221213 15:21:44 @agent_ppo2.py:185][0m |          -0.0125 |          18.6979 |           0.2924 |
[32m[20221213 15:21:44 @agent_ppo2.py:185][0m |          -0.0092 |          18.9552 |           0.2925 |
[32m[20221213 15:21:44 @agent_ppo2.py:185][0m |          -0.0100 |          18.8395 |           0.2926 |
[32m[20221213 15:21:44 @agent_ppo2.py:185][0m |          -0.0179 |          18.3456 |           0.2926 |
[32m[20221213 15:21:44 @agent_ppo2.py:185][0m |          -0.0167 |          18.2568 |           0.2925 |
[32m[20221213 15:21:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.76
[32m[20221213 15:21:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.90
[32m[20221213 15:21:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.70
[32m[20221213 15:21:44 @agent_ppo2.py:143][0m Total time:      28.82 min
[32m[20221213 15:21:44 @agent_ppo2.py:145][0m 2600960 total steps have happened
[32m[20221213 15:21:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1270 --------------------------#
[32m[20221213 15:21:44 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:21:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0010 |          18.8084 |           0.2912 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0083 |          17.7882 |           0.2904 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0096 |          17.3150 |           0.2901 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0144 |          17.0438 |           0.2899 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0166 |          16.8332 |           0.2899 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0072 |          17.6943 |           0.2896 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |           0.0033 |          20.1217 |           0.2897 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0135 |          16.6171 |           0.2888 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0169 |          16.2383 |           0.2892 |
[32m[20221213 15:21:45 @agent_ppo2.py:185][0m |          -0.0198 |          16.0441 |           0.2892 |
[32m[20221213 15:21:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.68
[32m[20221213 15:21:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.51
[32m[20221213 15:21:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.27
[32m[20221213 15:21:45 @agent_ppo2.py:143][0m Total time:      28.84 min
[32m[20221213 15:21:45 @agent_ppo2.py:145][0m 2603008 total steps have happened
[32m[20221213 15:21:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1271 --------------------------#
[32m[20221213 15:21:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0015 |          19.9414 |           0.3041 |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0069 |          19.6897 |           0.3035 |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0116 |          19.4638 |           0.3032 |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0057 |          20.4482 |           0.3028 |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0157 |          19.2974 |           0.3030 |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0180 |          19.2488 |           0.3027 |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0172 |          19.1521 |           0.3027 |
[32m[20221213 15:21:46 @agent_ppo2.py:185][0m |          -0.0176 |          19.1003 |           0.3024 |
[32m[20221213 15:21:47 @agent_ppo2.py:185][0m |          -0.0183 |          19.0257 |           0.3022 |
[32m[20221213 15:21:47 @agent_ppo2.py:185][0m |          -0.0183 |          18.9825 |           0.3024 |
[32m[20221213 15:21:47 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.71
[32m[20221213 15:21:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.38
[32m[20221213 15:21:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.80
[32m[20221213 15:21:47 @agent_ppo2.py:143][0m Total time:      28.86 min
[32m[20221213 15:21:47 @agent_ppo2.py:145][0m 2605056 total steps have happened
[32m[20221213 15:21:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1272 --------------------------#
[32m[20221213 15:21:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:47 @agent_ppo2.py:185][0m |          -0.0028 |          20.3180 |           0.2843 |
[32m[20221213 15:21:47 @agent_ppo2.py:185][0m |          -0.0082 |          19.8608 |           0.2833 |
[32m[20221213 15:21:47 @agent_ppo2.py:185][0m |          -0.0107 |          19.7032 |           0.2832 |
[32m[20221213 15:21:47 @agent_ppo2.py:185][0m |          -0.0118 |          19.5951 |           0.2826 |
[32m[20221213 15:21:48 @agent_ppo2.py:185][0m |          -0.0149 |          19.5546 |           0.2824 |
[32m[20221213 15:21:48 @agent_ppo2.py:185][0m |          -0.0149 |          19.4256 |           0.2821 |
[32m[20221213 15:21:48 @agent_ppo2.py:185][0m |          -0.0140 |          19.3621 |           0.2817 |
[32m[20221213 15:21:48 @agent_ppo2.py:185][0m |          -0.0156 |          19.3035 |           0.2815 |
[32m[20221213 15:21:48 @agent_ppo2.py:185][0m |          -0.0184 |          19.2679 |           0.2813 |
[32m[20221213 15:21:48 @agent_ppo2.py:185][0m |          -0.0188 |          19.2191 |           0.2813 |
[32m[20221213 15:21:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.14
[32m[20221213 15:21:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.59
[32m[20221213 15:21:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.23
[32m[20221213 15:21:48 @agent_ppo2.py:143][0m Total time:      28.88 min
[32m[20221213 15:21:48 @agent_ppo2.py:145][0m 2607104 total steps have happened
[32m[20221213 15:21:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1273 --------------------------#
[32m[20221213 15:21:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |           0.0009 |          19.8922 |           0.2918 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0051 |          19.4197 |           0.2909 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0088 |          19.2370 |           0.2911 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0145 |          19.1486 |           0.2908 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0109 |          19.0212 |           0.2907 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0054 |          19.8456 |           0.2904 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0136 |          18.8540 |           0.2904 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0149 |          18.7678 |           0.2902 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0149 |          18.6991 |           0.2906 |
[32m[20221213 15:21:49 @agent_ppo2.py:185][0m |          -0.0116 |          18.7986 |           0.2902 |
[32m[20221213 15:21:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.51
[32m[20221213 15:21:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.83
[32m[20221213 15:21:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.60
[32m[20221213 15:21:50 @agent_ppo2.py:143][0m Total time:      28.91 min
[32m[20221213 15:21:50 @agent_ppo2.py:145][0m 2609152 total steps have happened
[32m[20221213 15:21:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1274 --------------------------#
[32m[20221213 15:21:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0035 |          19.5906 |           0.2824 |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0141 |          18.9548 |           0.2821 |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0077 |          18.9414 |           0.2819 |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0172 |          18.2423 |           0.2826 |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0177 |          17.9332 |           0.2825 |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0167 |          17.6774 |           0.2825 |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0196 |          17.3902 |           0.2825 |
[32m[20221213 15:21:50 @agent_ppo2.py:185][0m |          -0.0195 |          17.1082 |           0.2828 |
[32m[20221213 15:21:51 @agent_ppo2.py:185][0m |          -0.0205 |          16.9212 |           0.2826 |
[32m[20221213 15:21:51 @agent_ppo2.py:185][0m |          -0.0197 |          16.7776 |           0.2827 |
[32m[20221213 15:21:51 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.52
[32m[20221213 15:21:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.60
[32m[20221213 15:21:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.13
[32m[20221213 15:21:51 @agent_ppo2.py:143][0m Total time:      28.93 min
[32m[20221213 15:21:51 @agent_ppo2.py:145][0m 2611200 total steps have happened
[32m[20221213 15:21:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1275 --------------------------#
[32m[20221213 15:21:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:51 @agent_ppo2.py:185][0m |           0.0024 |          21.2125 |           0.2943 |
[32m[20221213 15:21:51 @agent_ppo2.py:185][0m |          -0.0074 |          20.4250 |           0.2942 |
[32m[20221213 15:21:51 @agent_ppo2.py:185][0m |          -0.0116 |          20.1021 |           0.2936 |
[32m[20221213 15:21:51 @agent_ppo2.py:185][0m |          -0.0115 |          19.8126 |           0.2936 |
[32m[20221213 15:21:52 @agent_ppo2.py:185][0m |          -0.0139 |          19.5903 |           0.2942 |
[32m[20221213 15:21:52 @agent_ppo2.py:185][0m |          -0.0132 |          19.4127 |           0.2940 |
[32m[20221213 15:21:52 @agent_ppo2.py:185][0m |          -0.0161 |          19.2793 |           0.2942 |
[32m[20221213 15:21:52 @agent_ppo2.py:185][0m |          -0.0182 |          19.1706 |           0.2941 |
[32m[20221213 15:21:52 @agent_ppo2.py:185][0m |          -0.0183 |          19.0089 |           0.2939 |
[32m[20221213 15:21:52 @agent_ppo2.py:185][0m |          -0.0178 |          18.8496 |           0.2944 |
[32m[20221213 15:21:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.68
[32m[20221213 15:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.87
[32m[20221213 15:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.76
[32m[20221213 15:21:52 @agent_ppo2.py:143][0m Total time:      28.95 min
[32m[20221213 15:21:52 @agent_ppo2.py:145][0m 2613248 total steps have happened
[32m[20221213 15:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1276 --------------------------#
[32m[20221213 15:21:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |           0.0017 |          18.8678 |           0.2887 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0042 |          18.3354 |           0.2881 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0084 |          18.0526 |           0.2880 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0122 |          17.6801 |           0.2878 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0107 |          17.5997 |           0.2880 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0143 |          17.3816 |           0.2878 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0096 |          17.4746 |           0.2879 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0172 |          17.0861 |           0.2878 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0071 |          18.3581 |           0.2876 |
[32m[20221213 15:21:53 @agent_ppo2.py:185][0m |          -0.0182 |          16.9349 |           0.2878 |
[32m[20221213 15:21:53 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.12
[32m[20221213 15:21:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.62
[32m[20221213 15:21:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.14
[32m[20221213 15:21:54 @agent_ppo2.py:143][0m Total time:      28.97 min
[32m[20221213 15:21:54 @agent_ppo2.py:145][0m 2615296 total steps have happened
[32m[20221213 15:21:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1277 --------------------------#
[32m[20221213 15:21:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:21:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |           0.0166 |          20.9031 |           0.2840 |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |          -0.0074 |          18.9889 |           0.2841 |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |          -0.0086 |          18.7305 |           0.2836 |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |           0.0171 |          22.0853 |           0.2835 |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |          -0.0116 |          18.4192 |           0.2837 |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |          -0.0137 |          18.1539 |           0.2834 |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |          -0.0140 |          18.0588 |           0.2833 |
[32m[20221213 15:21:54 @agent_ppo2.py:185][0m |          -0.0154 |          17.9785 |           0.2833 |
[32m[20221213 15:21:55 @agent_ppo2.py:185][0m |          -0.0171 |          17.8213 |           0.2832 |
[32m[20221213 15:21:55 @agent_ppo2.py:185][0m |          -0.0173 |          17.7842 |           0.2836 |
[32m[20221213 15:21:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.73
[32m[20221213 15:21:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.42
[32m[20221213 15:21:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.38
[32m[20221213 15:21:55 @agent_ppo2.py:143][0m Total time:      28.99 min
[32m[20221213 15:21:55 @agent_ppo2.py:145][0m 2617344 total steps have happened
[32m[20221213 15:21:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1278 --------------------------#
[32m[20221213 15:21:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:55 @agent_ppo2.py:185][0m |           0.0008 |          20.9274 |           0.2872 |
[32m[20221213 15:21:55 @agent_ppo2.py:185][0m |          -0.0013 |          21.1930 |           0.2864 |
[32m[20221213 15:21:55 @agent_ppo2.py:185][0m |           0.0027 |          22.8284 |           0.2862 |
[32m[20221213 15:21:55 @agent_ppo2.py:185][0m |          -0.0128 |          20.1976 |           0.2856 |
[32m[20221213 15:21:56 @agent_ppo2.py:185][0m |          -0.0137 |          20.0865 |           0.2854 |
[32m[20221213 15:21:56 @agent_ppo2.py:185][0m |          -0.0159 |          20.0250 |           0.2855 |
[32m[20221213 15:21:56 @agent_ppo2.py:185][0m |          -0.0002 |          22.1398 |           0.2853 |
[32m[20221213 15:21:56 @agent_ppo2.py:185][0m |          -0.0173 |          19.9632 |           0.2846 |
[32m[20221213 15:21:56 @agent_ppo2.py:185][0m |          -0.0188 |          19.8607 |           0.2851 |
[32m[20221213 15:21:56 @agent_ppo2.py:185][0m |          -0.0189 |          19.8230 |           0.2849 |
[32m[20221213 15:21:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:21:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.07
[32m[20221213 15:21:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.61
[32m[20221213 15:21:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.46
[32m[20221213 15:21:56 @agent_ppo2.py:143][0m Total time:      29.02 min
[32m[20221213 15:21:56 @agent_ppo2.py:145][0m 2619392 total steps have happened
[32m[20221213 15:21:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1279 --------------------------#
[32m[20221213 15:21:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |           0.0007 |          20.6791 |           0.2968 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0001 |          20.9641 |           0.2964 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0103 |          20.0187 |           0.2960 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0087 |          20.2402 |           0.2960 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0004 |          22.0915 |           0.2959 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0089 |          19.8318 |           0.2959 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0156 |          19.6887 |           0.2959 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0056 |          22.8099 |           0.2956 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0174 |          19.7175 |           0.2957 |
[32m[20221213 15:21:57 @agent_ppo2.py:185][0m |          -0.0201 |          19.5914 |           0.2956 |
[32m[20221213 15:21:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:21:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.59
[32m[20221213 15:21:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.17
[32m[20221213 15:21:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.43
[32m[20221213 15:21:58 @agent_ppo2.py:143][0m Total time:      29.04 min
[32m[20221213 15:21:58 @agent_ppo2.py:145][0m 2621440 total steps have happened
[32m[20221213 15:21:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1280 --------------------------#
[32m[20221213 15:21:58 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:21:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:58 @agent_ppo2.py:185][0m |          -0.0019 |          21.0539 |           0.2942 |
[32m[20221213 15:21:58 @agent_ppo2.py:185][0m |          -0.0030 |          20.5336 |           0.2941 |
[32m[20221213 15:21:58 @agent_ppo2.py:185][0m |          -0.0069 |          20.2768 |           0.2937 |
[32m[20221213 15:21:58 @agent_ppo2.py:185][0m |           0.0055 |          21.8783 |           0.2936 |
[32m[20221213 15:21:58 @agent_ppo2.py:185][0m |          -0.0065 |          20.0682 |           0.2934 |
[32m[20221213 15:21:58 @agent_ppo2.py:185][0m |          -0.0108 |          19.7968 |           0.2935 |
[32m[20221213 15:21:58 @agent_ppo2.py:185][0m |          -0.0123 |          19.7336 |           0.2933 |
[32m[20221213 15:21:59 @agent_ppo2.py:185][0m |          -0.0011 |          20.4749 |           0.2933 |
[32m[20221213 15:21:59 @agent_ppo2.py:185][0m |          -0.0108 |          19.5457 |           0.2931 |
[32m[20221213 15:21:59 @agent_ppo2.py:185][0m |          -0.0162 |          19.3853 |           0.2931 |
[32m[20221213 15:21:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:21:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.99
[32m[20221213 15:21:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.23
[32m[20221213 15:21:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.08
[32m[20221213 15:21:59 @agent_ppo2.py:143][0m Total time:      29.06 min
[32m[20221213 15:21:59 @agent_ppo2.py:145][0m 2623488 total steps have happened
[32m[20221213 15:21:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1281 --------------------------#
[32m[20221213 15:21:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:21:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:21:59 @agent_ppo2.py:185][0m |          -0.0011 |          17.6620 |           0.2926 |
[32m[20221213 15:21:59 @agent_ppo2.py:185][0m |          -0.0067 |          16.2011 |           0.2921 |
[32m[20221213 15:21:59 @agent_ppo2.py:185][0m |          -0.0114 |          15.6534 |           0.2922 |
[32m[20221213 15:22:00 @agent_ppo2.py:185][0m |          -0.0120 |          15.3303 |           0.2919 |
[32m[20221213 15:22:00 @agent_ppo2.py:185][0m |          -0.0123 |          15.1029 |           0.2915 |
[32m[20221213 15:22:00 @agent_ppo2.py:185][0m |          -0.0101 |          15.3482 |           0.2913 |
[32m[20221213 15:22:00 @agent_ppo2.py:185][0m |          -0.0168 |          14.8152 |           0.2914 |
[32m[20221213 15:22:00 @agent_ppo2.py:185][0m |          -0.0186 |          14.6686 |           0.2912 |
[32m[20221213 15:22:00 @agent_ppo2.py:185][0m |          -0.0169 |          14.5732 |           0.2907 |
[32m[20221213 15:22:00 @agent_ppo2.py:185][0m |          -0.0180 |          14.4806 |           0.2907 |
[32m[20221213 15:22:00 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:22:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.48
[32m[20221213 15:22:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.91
[32m[20221213 15:22:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.25
[32m[20221213 15:22:00 @agent_ppo2.py:143][0m Total time:      29.08 min
[32m[20221213 15:22:00 @agent_ppo2.py:145][0m 2625536 total steps have happened
[32m[20221213 15:22:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1282 --------------------------#
[32m[20221213 15:22:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0042 |          22.0367 |           0.2864 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0097 |          20.7900 |           0.2864 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0098 |          20.4224 |           0.2862 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0122 |          20.2488 |           0.2861 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0155 |          20.0110 |           0.2860 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0174 |          19.9328 |           0.2859 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0149 |          19.7917 |           0.2858 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0157 |          19.7619 |           0.2857 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0196 |          19.5563 |           0.2860 |
[32m[20221213 15:22:01 @agent_ppo2.py:185][0m |          -0.0189 |          19.4338 |           0.2862 |
[32m[20221213 15:22:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.58
[32m[20221213 15:22:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 249.91
[32m[20221213 15:22:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.85
[32m[20221213 15:22:02 @agent_ppo2.py:143][0m Total time:      29.11 min
[32m[20221213 15:22:02 @agent_ppo2.py:145][0m 2627584 total steps have happened
[32m[20221213 15:22:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1283 --------------------------#
[32m[20221213 15:22:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:02 @agent_ppo2.py:185][0m |          -0.0016 |          21.3755 |           0.2840 |
[32m[20221213 15:22:02 @agent_ppo2.py:185][0m |          -0.0113 |          20.8305 |           0.2835 |
[32m[20221213 15:22:02 @agent_ppo2.py:185][0m |          -0.0133 |          20.5427 |           0.2833 |
[32m[20221213 15:22:02 @agent_ppo2.py:185][0m |          -0.0054 |          21.6725 |           0.2832 |
[32m[20221213 15:22:02 @agent_ppo2.py:185][0m |          -0.0134 |          20.2171 |           0.2830 |
[32m[20221213 15:22:02 @agent_ppo2.py:185][0m |          -0.0143 |          20.0850 |           0.2828 |
[32m[20221213 15:22:02 @agent_ppo2.py:185][0m |          -0.0163 |          19.9945 |           0.2826 |
[32m[20221213 15:22:03 @agent_ppo2.py:185][0m |          -0.0155 |          19.8312 |           0.2826 |
[32m[20221213 15:22:03 @agent_ppo2.py:185][0m |          -0.0174 |          19.7381 |           0.2824 |
[32m[20221213 15:22:03 @agent_ppo2.py:185][0m |          -0.0175 |          19.7115 |           0.2824 |
[32m[20221213 15:22:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.61
[32m[20221213 15:22:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.95
[32m[20221213 15:22:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.63
[32m[20221213 15:22:03 @agent_ppo2.py:143][0m Total time:      29.13 min
[32m[20221213 15:22:03 @agent_ppo2.py:145][0m 2629632 total steps have happened
[32m[20221213 15:22:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1284 --------------------------#
[32m[20221213 15:22:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:03 @agent_ppo2.py:185][0m |           0.0013 |          20.8718 |           0.2853 |
[32m[20221213 15:22:03 @agent_ppo2.py:185][0m |          -0.0090 |          20.3412 |           0.2842 |
[32m[20221213 15:22:03 @agent_ppo2.py:185][0m |          -0.0105 |          20.0976 |           0.2839 |
[32m[20221213 15:22:04 @agent_ppo2.py:185][0m |          -0.0076 |          20.4120 |           0.2841 |
[32m[20221213 15:22:04 @agent_ppo2.py:185][0m |          -0.0126 |          19.8067 |           0.2835 |
[32m[20221213 15:22:04 @agent_ppo2.py:185][0m |          -0.0151 |          19.6550 |           0.2839 |
[32m[20221213 15:22:04 @agent_ppo2.py:185][0m |          -0.0177 |          19.5639 |           0.2839 |
[32m[20221213 15:22:04 @agent_ppo2.py:185][0m |          -0.0168 |          19.5205 |           0.2838 |
[32m[20221213 15:22:04 @agent_ppo2.py:185][0m |          -0.0172 |          19.4759 |           0.2837 |
[32m[20221213 15:22:04 @agent_ppo2.py:185][0m |          -0.0184 |          19.3854 |           0.2836 |
[32m[20221213 15:22:04 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221213 15:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.30
[32m[20221213 15:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.83
[32m[20221213 15:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.10
[32m[20221213 15:22:05 @agent_ppo2.py:143][0m Total time:      29.16 min
[32m[20221213 15:22:05 @agent_ppo2.py:145][0m 2631680 total steps have happened
[32m[20221213 15:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1285 --------------------------#
[32m[20221213 15:22:05 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:22:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:05 @agent_ppo2.py:185][0m |           0.0072 |          21.6470 |           0.2861 |
[32m[20221213 15:22:05 @agent_ppo2.py:185][0m |          -0.0016 |          20.5278 |           0.2855 |
[32m[20221213 15:22:05 @agent_ppo2.py:185][0m |          -0.0080 |          20.3311 |           0.2852 |
[32m[20221213 15:22:05 @agent_ppo2.py:185][0m |           0.0020 |          23.1521 |           0.2850 |
[32m[20221213 15:22:05 @agent_ppo2.py:185][0m |          -0.0034 |          21.3880 |           0.2853 |
[32m[20221213 15:22:05 @agent_ppo2.py:185][0m |          -0.0107 |          19.9789 |           0.2848 |
[32m[20221213 15:22:06 @agent_ppo2.py:185][0m |          -0.0145 |          19.5840 |           0.2850 |
[32m[20221213 15:22:06 @agent_ppo2.py:185][0m |          -0.0168 |          19.5025 |           0.2850 |
[32m[20221213 15:22:06 @agent_ppo2.py:185][0m |          -0.0179 |          19.3759 |           0.2850 |
[32m[20221213 15:22:06 @agent_ppo2.py:185][0m |          -0.0183 |          19.3475 |           0.2850 |
[32m[20221213 15:22:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.57
[32m[20221213 15:22:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.47
[32m[20221213 15:22:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.35
[32m[20221213 15:22:06 @agent_ppo2.py:143][0m Total time:      29.18 min
[32m[20221213 15:22:06 @agent_ppo2.py:145][0m 2633728 total steps have happened
[32m[20221213 15:22:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1286 --------------------------#
[32m[20221213 15:22:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:06 @agent_ppo2.py:185][0m |           0.0002 |          19.2632 |           0.2875 |
[32m[20221213 15:22:06 @agent_ppo2.py:185][0m |          -0.0072 |          18.8228 |           0.2877 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0099 |          18.6388 |           0.2874 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0077 |          18.5449 |           0.2873 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0110 |          18.3490 |           0.2873 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0096 |          18.2506 |           0.2871 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0148 |          18.1888 |           0.2870 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0146 |          18.1253 |           0.2871 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0141 |          18.0857 |           0.2866 |
[32m[20221213 15:22:07 @agent_ppo2.py:185][0m |          -0.0144 |          18.0189 |           0.2869 |
[32m[20221213 15:22:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.01
[32m[20221213 15:22:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.87
[32m[20221213 15:22:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.41
[32m[20221213 15:22:07 @agent_ppo2.py:143][0m Total time:      29.20 min
[32m[20221213 15:22:07 @agent_ppo2.py:145][0m 2635776 total steps have happened
[32m[20221213 15:22:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1287 --------------------------#
[32m[20221213 15:22:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |           0.0010 |          19.1757 |           0.2847 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0071 |          18.8089 |           0.2849 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0093 |          18.5742 |           0.2848 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0122 |          18.3205 |           0.2846 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0128 |          18.1832 |           0.2844 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |           0.0022 |          20.4817 |           0.2846 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0136 |          17.9560 |           0.2841 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0127 |          17.8695 |           0.2841 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0061 |          20.4482 |           0.2840 |
[32m[20221213 15:22:08 @agent_ppo2.py:185][0m |          -0.0171 |          17.5747 |           0.2837 |
[32m[20221213 15:22:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.75
[32m[20221213 15:22:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.37
[32m[20221213 15:22:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.63
[32m[20221213 15:22:09 @agent_ppo2.py:143][0m Total time:      29.23 min
[32m[20221213 15:22:09 @agent_ppo2.py:145][0m 2637824 total steps have happened
[32m[20221213 15:22:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1288 --------------------------#
[32m[20221213 15:22:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:09 @agent_ppo2.py:185][0m |          -0.0029 |          19.3370 |           0.2877 |
[32m[20221213 15:22:09 @agent_ppo2.py:185][0m |          -0.0080 |          18.6878 |           0.2876 |
[32m[20221213 15:22:09 @agent_ppo2.py:185][0m |          -0.0044 |          18.5502 |           0.2871 |
[32m[20221213 15:22:09 @agent_ppo2.py:185][0m |          -0.0128 |          18.0511 |           0.2873 |
[32m[20221213 15:22:09 @agent_ppo2.py:185][0m |          -0.0090 |          18.4987 |           0.2873 |
[32m[20221213 15:22:09 @agent_ppo2.py:185][0m |          -0.0103 |          18.0703 |           0.2872 |
[32m[20221213 15:22:10 @agent_ppo2.py:185][0m |          -0.0150 |          17.5350 |           0.2872 |
[32m[20221213 15:22:10 @agent_ppo2.py:185][0m |          -0.0129 |          18.1085 |           0.2871 |
[32m[20221213 15:22:10 @agent_ppo2.py:185][0m |          -0.0184 |          17.2957 |           0.2872 |
[32m[20221213 15:22:10 @agent_ppo2.py:185][0m |          -0.0188 |          17.1802 |           0.2871 |
[32m[20221213 15:22:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.82
[32m[20221213 15:22:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.67
[32m[20221213 15:22:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 9.96
[32m[20221213 15:22:10 @agent_ppo2.py:143][0m Total time:      29.25 min
[32m[20221213 15:22:10 @agent_ppo2.py:145][0m 2639872 total steps have happened
[32m[20221213 15:22:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1289 --------------------------#
[32m[20221213 15:22:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:10 @agent_ppo2.py:185][0m |           0.0028 |          20.3212 |           0.2853 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0017 |          19.6459 |           0.2845 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |           0.0030 |          21.8814 |           0.2841 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0099 |          19.2021 |           0.2841 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0119 |          19.0582 |           0.2834 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0102 |          19.1004 |           0.2837 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0147 |          18.8709 |           0.2835 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0114 |          18.8791 |           0.2834 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0162 |          18.7789 |           0.2832 |
[32m[20221213 15:22:11 @agent_ppo2.py:185][0m |          -0.0151 |          18.8190 |           0.2831 |
[32m[20221213 15:22:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.70
[32m[20221213 15:22:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.04
[32m[20221213 15:22:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.09
[32m[20221213 15:22:11 @agent_ppo2.py:143][0m Total time:      29.27 min
[32m[20221213 15:22:11 @agent_ppo2.py:145][0m 2641920 total steps have happened
[32m[20221213 15:22:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1290 --------------------------#
[32m[20221213 15:22:12 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:22:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0039 |          20.3428 |           0.2881 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0072 |          20.0844 |           0.2874 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0087 |          19.7714 |           0.2871 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0147 |          19.7207 |           0.2872 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0136 |          19.5322 |           0.2872 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0081 |          20.2594 |           0.2872 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0156 |          19.3579 |           0.2868 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0146 |          19.2324 |           0.2870 |
[32m[20221213 15:22:12 @agent_ppo2.py:185][0m |          -0.0159 |          19.1270 |           0.2867 |
[32m[20221213 15:22:13 @agent_ppo2.py:185][0m |          -0.0040 |          20.7270 |           0.2866 |
[32m[20221213 15:22:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.61
[32m[20221213 15:22:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.66
[32m[20221213 15:22:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.15
[32m[20221213 15:22:13 @agent_ppo2.py:143][0m Total time:      29.29 min
[32m[20221213 15:22:13 @agent_ppo2.py:145][0m 2643968 total steps have happened
[32m[20221213 15:22:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1291 --------------------------#
[32m[20221213 15:22:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:13 @agent_ppo2.py:185][0m |          -0.0026 |          20.8918 |           0.2805 |
[32m[20221213 15:22:13 @agent_ppo2.py:185][0m |           0.0124 |          22.5368 |           0.2803 |
[32m[20221213 15:22:13 @agent_ppo2.py:185][0m |           0.0014 |          21.7611 |           0.2797 |
[32m[20221213 15:22:13 @agent_ppo2.py:185][0m |          -0.0105 |          19.8910 |           0.2796 |
[32m[20221213 15:22:13 @agent_ppo2.py:185][0m |          -0.0011 |          21.4275 |           0.2796 |
[32m[20221213 15:22:14 @agent_ppo2.py:185][0m |          -0.0155 |          19.6136 |           0.2795 |
[32m[20221213 15:22:14 @agent_ppo2.py:185][0m |          -0.0158 |          19.5251 |           0.2794 |
[32m[20221213 15:22:14 @agent_ppo2.py:185][0m |          -0.0164 |          19.5217 |           0.2792 |
[32m[20221213 15:22:14 @agent_ppo2.py:185][0m |          -0.0053 |          20.6293 |           0.2793 |
[32m[20221213 15:22:14 @agent_ppo2.py:185][0m |          -0.0149 |          19.3235 |           0.2793 |
[32m[20221213 15:22:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:22:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.95
[32m[20221213 15:22:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.04
[32m[20221213 15:22:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.88
[32m[20221213 15:22:14 @agent_ppo2.py:143][0m Total time:      29.32 min
[32m[20221213 15:22:14 @agent_ppo2.py:145][0m 2646016 total steps have happened
[32m[20221213 15:22:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1292 --------------------------#
[32m[20221213 15:22:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |           0.0100 |          20.6754 |           0.2822 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0074 |          18.1630 |           0.2815 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0068 |          17.8406 |           0.2813 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0109 |          17.5775 |           0.2811 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0112 |          17.3880 |           0.2808 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0137 |          17.3125 |           0.2809 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0121 |          17.1636 |           0.2804 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0138 |          17.0632 |           0.2804 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0163 |          16.9723 |           0.2802 |
[32m[20221213 15:22:15 @agent_ppo2.py:185][0m |          -0.0172 |          16.8887 |           0.2801 |
[32m[20221213 15:22:15 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.82
[32m[20221213 15:22:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.51
[32m[20221213 15:22:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.75
[32m[20221213 15:22:15 @agent_ppo2.py:143][0m Total time:      29.34 min
[32m[20221213 15:22:15 @agent_ppo2.py:145][0m 2648064 total steps have happened
[32m[20221213 15:22:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1293 --------------------------#
[32m[20221213 15:22:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |          -0.0022 |          20.1016 |           0.2790 |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |          -0.0071 |          19.5438 |           0.2783 |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |          -0.0127 |          19.2117 |           0.2779 |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |          -0.0028 |          20.8413 |           0.2780 |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |           0.0028 |          20.4748 |           0.2780 |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |          -0.0141 |          18.7288 |           0.2775 |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |          -0.0146 |          18.5429 |           0.2773 |
[32m[20221213 15:22:16 @agent_ppo2.py:185][0m |          -0.0153 |          18.4081 |           0.2773 |
[32m[20221213 15:22:17 @agent_ppo2.py:185][0m |          -0.0201 |          18.2883 |           0.2772 |
[32m[20221213 15:22:17 @agent_ppo2.py:185][0m |          -0.0187 |          18.1891 |           0.2771 |
[32m[20221213 15:22:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.94
[32m[20221213 15:22:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.70
[32m[20221213 15:22:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.44
[32m[20221213 15:22:17 @agent_ppo2.py:143][0m Total time:      29.36 min
[32m[20221213 15:22:17 @agent_ppo2.py:145][0m 2650112 total steps have happened
[32m[20221213 15:22:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1294 --------------------------#
[32m[20221213 15:22:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:17 @agent_ppo2.py:185][0m |          -0.0004 |          20.3619 |           0.2751 |
[32m[20221213 15:22:17 @agent_ppo2.py:185][0m |          -0.0064 |          20.0030 |           0.2738 |
[32m[20221213 15:22:17 @agent_ppo2.py:185][0m |          -0.0080 |          19.8634 |           0.2730 |
[32m[20221213 15:22:17 @agent_ppo2.py:185][0m |          -0.0101 |          19.8062 |           0.2728 |
[32m[20221213 15:22:18 @agent_ppo2.py:185][0m |          -0.0148 |          19.7383 |           0.2724 |
[32m[20221213 15:22:18 @agent_ppo2.py:185][0m |          -0.0099 |          19.7171 |           0.2726 |
[32m[20221213 15:22:18 @agent_ppo2.py:185][0m |          -0.0162 |          19.6430 |           0.2720 |
[32m[20221213 15:22:18 @agent_ppo2.py:185][0m |          -0.0119 |          19.8819 |           0.2720 |
[32m[20221213 15:22:18 @agent_ppo2.py:185][0m |          -0.0154 |          19.5566 |           0.2714 |
[32m[20221213 15:22:18 @agent_ppo2.py:185][0m |          -0.0120 |          19.7148 |           0.2715 |
[32m[20221213 15:22:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.16
[32m[20221213 15:22:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.31
[32m[20221213 15:22:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 194.64
[32m[20221213 15:22:18 @agent_ppo2.py:143][0m Total time:      29.38 min
[32m[20221213 15:22:18 @agent_ppo2.py:145][0m 2652160 total steps have happened
[32m[20221213 15:22:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1295 --------------------------#
[32m[20221213 15:22:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0016 |          19.6480 |           0.2742 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0065 |          19.2546 |           0.2737 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0094 |          19.0290 |           0.2736 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |           0.0002 |          21.0496 |           0.2736 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0056 |          18.9549 |           0.2733 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0125 |          18.5936 |           0.2730 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0137 |          18.4720 |           0.2732 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0147 |          18.3604 |           0.2733 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0135 |          18.2814 |           0.2732 |
[32m[20221213 15:22:19 @agent_ppo2.py:185][0m |          -0.0143 |          18.2333 |           0.2731 |
[32m[20221213 15:22:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.38
[32m[20221213 15:22:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.40
[32m[20221213 15:22:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.00
[32m[20221213 15:22:20 @agent_ppo2.py:143][0m Total time:      29.41 min
[32m[20221213 15:22:20 @agent_ppo2.py:145][0m 2654208 total steps have happened
[32m[20221213 15:22:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1296 --------------------------#
[32m[20221213 15:22:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0033 |          20.0178 |           0.2801 |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0074 |          19.4348 |           0.2799 |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0107 |          19.2639 |           0.2796 |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0105 |          19.1038 |           0.2795 |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0139 |          19.0302 |           0.2794 |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0146 |          18.9828 |           0.2793 |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0135 |          18.9229 |           0.2790 |
[32m[20221213 15:22:20 @agent_ppo2.py:185][0m |          -0.0048 |          20.0351 |           0.2787 |
[32m[20221213 15:22:21 @agent_ppo2.py:185][0m |          -0.0143 |          18.8994 |           0.2788 |
[32m[20221213 15:22:21 @agent_ppo2.py:185][0m |          -0.0172 |          18.7839 |           0.2786 |
[32m[20221213 15:22:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.68
[32m[20221213 15:22:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.75
[32m[20221213 15:22:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.72
[32m[20221213 15:22:21 @agent_ppo2.py:143][0m Total time:      29.43 min
[32m[20221213 15:22:21 @agent_ppo2.py:145][0m 2656256 total steps have happened
[32m[20221213 15:22:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1297 --------------------------#
[32m[20221213 15:22:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:21 @agent_ppo2.py:185][0m |           0.0006 |          20.2919 |           0.2731 |
[32m[20221213 15:22:21 @agent_ppo2.py:185][0m |          -0.0077 |          19.9212 |           0.2730 |
[32m[20221213 15:22:21 @agent_ppo2.py:185][0m |          -0.0081 |          19.8024 |           0.2726 |
[32m[20221213 15:22:22 @agent_ppo2.py:185][0m |          -0.0092 |          19.7275 |           0.2726 |
[32m[20221213 15:22:22 @agent_ppo2.py:185][0m |          -0.0104 |          19.7089 |           0.2724 |
[32m[20221213 15:22:22 @agent_ppo2.py:185][0m |          -0.0141 |          19.6362 |           0.2724 |
[32m[20221213 15:22:22 @agent_ppo2.py:185][0m |          -0.0131 |          19.6151 |           0.2723 |
[32m[20221213 15:22:22 @agent_ppo2.py:185][0m |          -0.0137 |          19.6112 |           0.2721 |
[32m[20221213 15:22:22 @agent_ppo2.py:185][0m |          -0.0144 |          19.6140 |           0.2721 |
[32m[20221213 15:22:22 @agent_ppo2.py:185][0m |          -0.0173 |          19.5198 |           0.2721 |
[32m[20221213 15:22:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.19
[32m[20221213 15:22:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.76
[32m[20221213 15:22:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.49
[32m[20221213 15:22:22 @agent_ppo2.py:143][0m Total time:      29.45 min
[32m[20221213 15:22:22 @agent_ppo2.py:145][0m 2658304 total steps have happened
[32m[20221213 15:22:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1298 --------------------------#
[32m[20221213 15:22:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0017 |          20.2255 |           0.2739 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0062 |          19.8182 |           0.2735 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0080 |          19.5821 |           0.2732 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0114 |          19.4031 |           0.2732 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0120 |          19.2088 |           0.2731 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0090 |          19.4256 |           0.2731 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0165 |          18.9189 |           0.2727 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0156 |          18.8087 |           0.2727 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0109 |          19.2995 |           0.2723 |
[32m[20221213 15:22:23 @agent_ppo2.py:185][0m |          -0.0132 |          18.7086 |           0.2721 |
[32m[20221213 15:22:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.00
[32m[20221213 15:22:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.52
[32m[20221213 15:22:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.32
[32m[20221213 15:22:24 @agent_ppo2.py:143][0m Total time:      29.47 min
[32m[20221213 15:22:24 @agent_ppo2.py:145][0m 2660352 total steps have happened
[32m[20221213 15:22:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1299 --------------------------#
[32m[20221213 15:22:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:24 @agent_ppo2.py:185][0m |           0.0015 |          20.8826 |           0.2734 |
[32m[20221213 15:22:24 @agent_ppo2.py:185][0m |          -0.0075 |          20.1550 |           0.2730 |
[32m[20221213 15:22:24 @agent_ppo2.py:185][0m |          -0.0104 |          19.8887 |           0.2727 |
[32m[20221213 15:22:24 @agent_ppo2.py:185][0m |          -0.0145 |          19.7444 |           0.2725 |
[32m[20221213 15:22:24 @agent_ppo2.py:185][0m |          -0.0165 |          19.5885 |           0.2725 |
[32m[20221213 15:22:24 @agent_ppo2.py:185][0m |          -0.0136 |          19.5236 |           0.2723 |
[32m[20221213 15:22:24 @agent_ppo2.py:185][0m |          -0.0160 |          19.3451 |           0.2722 |
[32m[20221213 15:22:25 @agent_ppo2.py:185][0m |          -0.0153 |          19.3134 |           0.2720 |
[32m[20221213 15:22:25 @agent_ppo2.py:185][0m |          -0.0167 |          19.2629 |           0.2721 |
[32m[20221213 15:22:25 @agent_ppo2.py:185][0m |          -0.0183 |          19.2162 |           0.2723 |
[32m[20221213 15:22:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.01
[32m[20221213 15:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.14
[32m[20221213 15:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.16
[32m[20221213 15:22:25 @agent_ppo2.py:143][0m Total time:      29.50 min
[32m[20221213 15:22:25 @agent_ppo2.py:145][0m 2662400 total steps have happened
[32m[20221213 15:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1300 --------------------------#
[32m[20221213 15:22:25 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:22:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:25 @agent_ppo2.py:185][0m |          -0.0001 |          19.5593 |           0.2753 |
[32m[20221213 15:22:25 @agent_ppo2.py:185][0m |          -0.0082 |          19.2648 |           0.2752 |
[32m[20221213 15:22:25 @agent_ppo2.py:185][0m |          -0.0098 |          19.1514 |           0.2749 |
[32m[20221213 15:22:26 @agent_ppo2.py:185][0m |          -0.0056 |          19.3966 |           0.2750 |
[32m[20221213 15:22:26 @agent_ppo2.py:185][0m |           0.0032 |          20.4347 |           0.2752 |
[32m[20221213 15:22:26 @agent_ppo2.py:185][0m |          -0.0151 |          18.9190 |           0.2749 |
[32m[20221213 15:22:26 @agent_ppo2.py:185][0m |          -0.0153 |          18.8537 |           0.2752 |
[32m[20221213 15:22:26 @agent_ppo2.py:185][0m |          -0.0190 |          18.7948 |           0.2752 |
[32m[20221213 15:22:26 @agent_ppo2.py:185][0m |          -0.0165 |          18.7214 |           0.2755 |
[32m[20221213 15:22:26 @agent_ppo2.py:185][0m |          -0.0166 |          18.7446 |           0.2753 |
[32m[20221213 15:22:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.84
[32m[20221213 15:22:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.34
[32m[20221213 15:22:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.57
[32m[20221213 15:22:26 @agent_ppo2.py:143][0m Total time:      29.52 min
[32m[20221213 15:22:26 @agent_ppo2.py:145][0m 2664448 total steps have happened
[32m[20221213 15:22:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1301 --------------------------#
[32m[20221213 15:22:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |           0.0002 |          19.9015 |           0.2747 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0085 |          19.3997 |           0.2737 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0075 |          19.2567 |           0.2743 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0124 |          19.0769 |           0.2735 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0016 |          21.6880 |           0.2740 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0107 |          19.0255 |           0.2732 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0134 |          18.8707 |           0.2736 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0157 |          18.8315 |           0.2735 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0155 |          18.7769 |           0.2733 |
[32m[20221213 15:22:27 @agent_ppo2.py:185][0m |          -0.0186 |          18.7429 |           0.2732 |
[32m[20221213 15:22:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.01
[32m[20221213 15:22:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.32
[32m[20221213 15:22:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.35
[32m[20221213 15:22:28 @agent_ppo2.py:143][0m Total time:      29.54 min
[32m[20221213 15:22:28 @agent_ppo2.py:145][0m 2666496 total steps have happened
[32m[20221213 15:22:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1302 --------------------------#
[32m[20221213 15:22:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:28 @agent_ppo2.py:185][0m |          -0.0015 |          20.6056 |           0.2710 |
[32m[20221213 15:22:28 @agent_ppo2.py:185][0m |          -0.0089 |          20.2106 |           0.2703 |
[32m[20221213 15:22:28 @agent_ppo2.py:185][0m |          -0.0096 |          20.0674 |           0.2702 |
[32m[20221213 15:22:28 @agent_ppo2.py:185][0m |          -0.0112 |          19.9481 |           0.2700 |
[32m[20221213 15:22:28 @agent_ppo2.py:185][0m |          -0.0135 |          19.8949 |           0.2699 |
[32m[20221213 15:22:28 @agent_ppo2.py:185][0m |          -0.0001 |          21.0318 |           0.2698 |
[32m[20221213 15:22:28 @agent_ppo2.py:185][0m |          -0.0086 |          19.9816 |           0.2693 |
[32m[20221213 15:22:29 @agent_ppo2.py:185][0m |          -0.0134 |          19.6797 |           0.2693 |
[32m[20221213 15:22:29 @agent_ppo2.py:185][0m |          -0.0168 |          19.6383 |           0.2689 |
[32m[20221213 15:22:29 @agent_ppo2.py:185][0m |          -0.0163 |          19.5224 |           0.2691 |
[32m[20221213 15:22:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.13
[32m[20221213 15:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.48
[32m[20221213 15:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.22
[32m[20221213 15:22:29 @agent_ppo2.py:143][0m Total time:      29.56 min
[32m[20221213 15:22:29 @agent_ppo2.py:145][0m 2668544 total steps have happened
[32m[20221213 15:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1303 --------------------------#
[32m[20221213 15:22:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:29 @agent_ppo2.py:185][0m |          -0.0003 |          19.0151 |           0.2666 |
[32m[20221213 15:22:29 @agent_ppo2.py:185][0m |          -0.0059 |          17.9676 |           0.2665 |
[32m[20221213 15:22:29 @agent_ppo2.py:185][0m |          -0.0090 |          17.4569 |           0.2663 |
[32m[20221213 15:22:30 @agent_ppo2.py:185][0m |          -0.0088 |          17.2086 |           0.2662 |
[32m[20221213 15:22:30 @agent_ppo2.py:185][0m |          -0.0060 |          16.9359 |           0.2660 |
[32m[20221213 15:22:30 @agent_ppo2.py:185][0m |          -0.0144 |          16.5819 |           0.2661 |
[32m[20221213 15:22:30 @agent_ppo2.py:185][0m |          -0.0150 |          16.3343 |           0.2659 |
[32m[20221213 15:22:30 @agent_ppo2.py:185][0m |          -0.0118 |          16.3253 |           0.2660 |
[32m[20221213 15:22:30 @agent_ppo2.py:185][0m |          -0.0178 |          15.9487 |           0.2661 |
[32m[20221213 15:22:30 @agent_ppo2.py:185][0m |          -0.0161 |          15.8785 |           0.2661 |
[32m[20221213 15:22:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 223.37
[32m[20221213 15:22:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.70
[32m[20221213 15:22:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.07
[32m[20221213 15:22:30 @agent_ppo2.py:143][0m Total time:      29.59 min
[32m[20221213 15:22:30 @agent_ppo2.py:145][0m 2670592 total steps have happened
[32m[20221213 15:22:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1304 --------------------------#
[32m[20221213 15:22:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |           0.0031 |          21.7677 |           0.2725 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0053 |          20.9237 |           0.2721 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0095 |          20.5250 |           0.2714 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0045 |          20.6194 |           0.2711 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0081 |          20.1077 |           0.2710 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0072 |          20.4391 |           0.2708 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0086 |          19.8497 |           0.2710 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0139 |          19.6890 |           0.2706 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0032 |          20.5590 |           0.2703 |
[32m[20221213 15:22:31 @agent_ppo2.py:185][0m |          -0.0106 |          19.4703 |           0.2703 |
[32m[20221213 15:22:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.00
[32m[20221213 15:22:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.36
[32m[20221213 15:22:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.82
[32m[20221213 15:22:32 @agent_ppo2.py:143][0m Total time:      29.61 min
[32m[20221213 15:22:32 @agent_ppo2.py:145][0m 2672640 total steps have happened
[32m[20221213 15:22:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1305 --------------------------#
[32m[20221213 15:22:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:32 @agent_ppo2.py:185][0m |           0.0018 |          21.5343 |           0.2648 |
[32m[20221213 15:22:32 @agent_ppo2.py:185][0m |          -0.0097 |          20.6676 |           0.2640 |
[32m[20221213 15:22:32 @agent_ppo2.py:185][0m |          -0.0061 |          20.9656 |           0.2638 |
[32m[20221213 15:22:32 @agent_ppo2.py:185][0m |          -0.0144 |          20.3586 |           0.2636 |
[32m[20221213 15:22:32 @agent_ppo2.py:185][0m |          -0.0138 |          20.1964 |           0.2634 |
[32m[20221213 15:22:32 @agent_ppo2.py:185][0m |          -0.0138 |          20.1168 |           0.2635 |
[32m[20221213 15:22:33 @agent_ppo2.py:185][0m |          -0.0154 |          20.0403 |           0.2631 |
[32m[20221213 15:22:33 @agent_ppo2.py:185][0m |          -0.0167 |          19.9302 |           0.2631 |
[32m[20221213 15:22:33 @agent_ppo2.py:185][0m |          -0.0136 |          20.0118 |           0.2631 |
[32m[20221213 15:22:33 @agent_ppo2.py:185][0m |          -0.0196 |          19.8211 |           0.2630 |
[32m[20221213 15:22:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.53
[32m[20221213 15:22:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.08
[32m[20221213 15:22:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.84
[32m[20221213 15:22:33 @agent_ppo2.py:143][0m Total time:      29.63 min
[32m[20221213 15:22:33 @agent_ppo2.py:145][0m 2674688 total steps have happened
[32m[20221213 15:22:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1306 --------------------------#
[32m[20221213 15:22:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:33 @agent_ppo2.py:185][0m |          -0.0042 |          21.0977 |           0.2707 |
[32m[20221213 15:22:33 @agent_ppo2.py:185][0m |          -0.0076 |          20.3174 |           0.2704 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0139 |          19.9175 |           0.2702 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0140 |          19.7121 |           0.2701 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0161 |          19.5410 |           0.2698 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0148 |          19.4820 |           0.2697 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0171 |          19.2535 |           0.2696 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0129 |          19.6172 |           0.2693 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0159 |          19.3336 |           0.2693 |
[32m[20221213 15:22:34 @agent_ppo2.py:185][0m |          -0.0194 |          18.8774 |           0.2689 |
[32m[20221213 15:22:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.29
[32m[20221213 15:22:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.09
[32m[20221213 15:22:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 231.77
[32m[20221213 15:22:34 @agent_ppo2.py:143][0m Total time:      29.65 min
[32m[20221213 15:22:34 @agent_ppo2.py:145][0m 2676736 total steps have happened
[32m[20221213 15:22:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1307 --------------------------#
[32m[20221213 15:22:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0021 |          20.5866 |           0.2744 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0044 |          20.1049 |           0.2741 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0123 |          19.7927 |           0.2739 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0090 |          19.8215 |           0.2738 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0114 |          19.5516 |           0.2733 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0134 |          19.4683 |           0.2736 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0091 |          19.9237 |           0.2737 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0005 |          21.3169 |           0.2733 |
[32m[20221213 15:22:35 @agent_ppo2.py:185][0m |          -0.0165 |          19.2280 |           0.2729 |
[32m[20221213 15:22:36 @agent_ppo2.py:185][0m |          -0.0173 |          19.1975 |           0.2730 |
[32m[20221213 15:22:36 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.84
[32m[20221213 15:22:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.15
[32m[20221213 15:22:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.88
[32m[20221213 15:22:36 @agent_ppo2.py:143][0m Total time:      29.68 min
[32m[20221213 15:22:36 @agent_ppo2.py:145][0m 2678784 total steps have happened
[32m[20221213 15:22:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1308 --------------------------#
[32m[20221213 15:22:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:36 @agent_ppo2.py:185][0m |           0.0017 |          19.9132 |           0.2709 |
[32m[20221213 15:22:36 @agent_ppo2.py:185][0m |          -0.0057 |          19.5891 |           0.2699 |
[32m[20221213 15:22:36 @agent_ppo2.py:185][0m |           0.0025 |          20.6420 |           0.2694 |
[32m[20221213 15:22:36 @agent_ppo2.py:185][0m |          -0.0107 |          19.1722 |           0.2684 |
[32m[20221213 15:22:36 @agent_ppo2.py:185][0m |          -0.0097 |          18.9863 |           0.2684 |
[32m[20221213 15:22:37 @agent_ppo2.py:185][0m |          -0.0162 |          18.9064 |           0.2682 |
[32m[20221213 15:22:37 @agent_ppo2.py:185][0m |          -0.0129 |          18.8204 |           0.2682 |
[32m[20221213 15:22:37 @agent_ppo2.py:185][0m |          -0.0050 |          19.2260 |           0.2679 |
[32m[20221213 15:22:37 @agent_ppo2.py:185][0m |          -0.0145 |          18.5005 |           0.2668 |
[32m[20221213 15:22:37 @agent_ppo2.py:185][0m |          -0.0095 |          19.3791 |           0.2673 |
[32m[20221213 15:22:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.48
[32m[20221213 15:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.46
[32m[20221213 15:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.44
[32m[20221213 15:22:37 @agent_ppo2.py:143][0m Total time:      29.70 min
[32m[20221213 15:22:37 @agent_ppo2.py:145][0m 2680832 total steps have happened
[32m[20221213 15:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1309 --------------------------#
[32m[20221213 15:22:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:37 @agent_ppo2.py:185][0m |          -0.0000 |          19.9606 |           0.2579 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |           0.0088 |          21.8639 |           0.2579 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |          -0.0068 |          19.2931 |           0.2572 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |          -0.0062 |          19.1383 |           0.2569 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |          -0.0097 |          19.0578 |           0.2572 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |          -0.0120 |          18.8411 |           0.2568 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |           0.0067 |          22.2256 |           0.2566 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |          -0.0040 |          19.8930 |           0.2564 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |          -0.0072 |          19.4529 |           0.2563 |
[32m[20221213 15:22:38 @agent_ppo2.py:185][0m |          -0.0123 |          18.6121 |           0.2560 |
[32m[20221213 15:22:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.32
[32m[20221213 15:22:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.97
[32m[20221213 15:22:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.56
[32m[20221213 15:22:38 @agent_ppo2.py:143][0m Total time:      29.72 min
[32m[20221213 15:22:38 @agent_ppo2.py:145][0m 2682880 total steps have happened
[32m[20221213 15:22:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1310 --------------------------#
[32m[20221213 15:22:39 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:22:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0034 |          20.1898 |           0.2603 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0084 |          19.7842 |           0.2597 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0011 |          21.2918 |           0.2599 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0128 |          19.4070 |           0.2593 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0125 |          19.2847 |           0.2595 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0048 |          20.7565 |           0.2598 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0157 |          19.1030 |           0.2596 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0146 |          18.9737 |           0.2597 |
[32m[20221213 15:22:39 @agent_ppo2.py:185][0m |          -0.0109 |          19.1423 |           0.2596 |
[32m[20221213 15:22:40 @agent_ppo2.py:185][0m |          -0.0176 |          18.8915 |           0.2597 |
[32m[20221213 15:22:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:22:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.00
[32m[20221213 15:22:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.69
[32m[20221213 15:22:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.89
[32m[20221213 15:22:40 @agent_ppo2.py:143][0m Total time:      29.74 min
[32m[20221213 15:22:40 @agent_ppo2.py:145][0m 2684928 total steps have happened
[32m[20221213 15:22:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1311 --------------------------#
[32m[20221213 15:22:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:40 @agent_ppo2.py:185][0m |          -0.0029 |          20.3328 |           0.2674 |
[32m[20221213 15:22:40 @agent_ppo2.py:185][0m |          -0.0067 |          19.9835 |           0.2669 |
[32m[20221213 15:22:40 @agent_ppo2.py:185][0m |          -0.0045 |          20.1197 |           0.2673 |
[32m[20221213 15:22:40 @agent_ppo2.py:185][0m |          -0.0117 |          19.6461 |           0.2672 |
[32m[20221213 15:22:41 @agent_ppo2.py:185][0m |          -0.0130 |          19.5705 |           0.2670 |
[32m[20221213 15:22:41 @agent_ppo2.py:185][0m |          -0.0137 |          19.4540 |           0.2670 |
[32m[20221213 15:22:41 @agent_ppo2.py:185][0m |          -0.0046 |          22.1037 |           0.2667 |
[32m[20221213 15:22:41 @agent_ppo2.py:185][0m |          -0.0137 |          19.3428 |           0.2668 |
[32m[20221213 15:22:41 @agent_ppo2.py:185][0m |          -0.0174 |          19.1979 |           0.2667 |
[32m[20221213 15:22:41 @agent_ppo2.py:185][0m |          -0.0166 |          19.1201 |           0.2665 |
[32m[20221213 15:22:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:22:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.07
[32m[20221213 15:22:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.68
[32m[20221213 15:22:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.65
[32m[20221213 15:22:41 @agent_ppo2.py:143][0m Total time:      29.77 min
[32m[20221213 15:22:41 @agent_ppo2.py:145][0m 2686976 total steps have happened
[32m[20221213 15:22:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1312 --------------------------#
[32m[20221213 15:22:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0032 |          19.0160 |           0.2604 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0070 |          18.6487 |           0.2602 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0120 |          18.4053 |           0.2601 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0136 |          18.1761 |           0.2602 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0172 |          18.0429 |           0.2601 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0082 |          18.7174 |           0.2601 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0171 |          17.8294 |           0.2600 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0151 |          17.7588 |           0.2602 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0175 |          17.6661 |           0.2602 |
[32m[20221213 15:22:42 @agent_ppo2.py:185][0m |          -0.0168 |          17.7034 |           0.2600 |
[32m[20221213 15:22:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:22:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.31
[32m[20221213 15:22:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.67
[32m[20221213 15:22:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.05
[32m[20221213 15:22:43 @agent_ppo2.py:143][0m Total time:      29.79 min
[32m[20221213 15:22:43 @agent_ppo2.py:145][0m 2689024 total steps have happened
[32m[20221213 15:22:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1313 --------------------------#
[32m[20221213 15:22:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:43 @agent_ppo2.py:185][0m |           0.0064 |          21.5025 |           0.2706 |
[32m[20221213 15:22:43 @agent_ppo2.py:185][0m |          -0.0060 |          19.9425 |           0.2700 |
[32m[20221213 15:22:43 @agent_ppo2.py:185][0m |          -0.0104 |          19.6740 |           0.2700 |
[32m[20221213 15:22:43 @agent_ppo2.py:185][0m |          -0.0097 |          19.5856 |           0.2700 |
[32m[20221213 15:22:43 @agent_ppo2.py:185][0m |          -0.0121 |          19.1353 |           0.2696 |
[32m[20221213 15:22:43 @agent_ppo2.py:185][0m |          -0.0127 |          19.2328 |           0.2700 |
[32m[20221213 15:22:43 @agent_ppo2.py:185][0m |          -0.0173 |          18.8259 |           0.2700 |
[32m[20221213 15:22:44 @agent_ppo2.py:185][0m |          -0.0177 |          18.7205 |           0.2700 |
[32m[20221213 15:22:44 @agent_ppo2.py:185][0m |          -0.0197 |          18.6113 |           0.2698 |
[32m[20221213 15:22:44 @agent_ppo2.py:185][0m |          -0.0199 |          18.5333 |           0.2698 |
[32m[20221213 15:22:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.44
[32m[20221213 15:22:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.40
[32m[20221213 15:22:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 207.17
[32m[20221213 15:22:44 @agent_ppo2.py:143][0m Total time:      29.81 min
[32m[20221213 15:22:44 @agent_ppo2.py:145][0m 2691072 total steps have happened
[32m[20221213 15:22:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1314 --------------------------#
[32m[20221213 15:22:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:44 @agent_ppo2.py:185][0m |           0.0136 |          21.5859 |           0.2579 |
[32m[20221213 15:22:44 @agent_ppo2.py:185][0m |          -0.0038 |          19.9128 |           0.2571 |
[32m[20221213 15:22:44 @agent_ppo2.py:185][0m |          -0.0068 |          19.7084 |           0.2573 |
[32m[20221213 15:22:45 @agent_ppo2.py:185][0m |          -0.0084 |          19.5152 |           0.2569 |
[32m[20221213 15:22:45 @agent_ppo2.py:185][0m |          -0.0073 |          19.6343 |           0.2572 |
[32m[20221213 15:22:45 @agent_ppo2.py:185][0m |          -0.0124 |          19.2912 |           0.2571 |
[32m[20221213 15:22:45 @agent_ppo2.py:185][0m |          -0.0090 |          19.3985 |           0.2570 |
[32m[20221213 15:22:45 @agent_ppo2.py:185][0m |          -0.0129 |          19.1032 |           0.2571 |
[32m[20221213 15:22:45 @agent_ppo2.py:185][0m |          -0.0146 |          19.0028 |           0.2568 |
[32m[20221213 15:22:45 @agent_ppo2.py:185][0m |          -0.0160 |          18.9145 |           0.2567 |
[32m[20221213 15:22:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.40
[32m[20221213 15:22:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.41
[32m[20221213 15:22:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.45
[32m[20221213 15:22:45 @agent_ppo2.py:143][0m Total time:      29.84 min
[32m[20221213 15:22:45 @agent_ppo2.py:145][0m 2693120 total steps have happened
[32m[20221213 15:22:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1315 --------------------------#
[32m[20221213 15:22:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0029 |          19.8486 |           0.2656 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0117 |          19.5463 |           0.2653 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0133 |          19.3575 |           0.2654 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0091 |          19.4156 |           0.2653 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0149 |          19.1536 |           0.2652 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0146 |          19.0488 |           0.2653 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0052 |          20.3304 |           0.2655 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0152 |          18.9400 |           0.2652 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0169 |          18.8460 |           0.2655 |
[32m[20221213 15:22:46 @agent_ppo2.py:185][0m |          -0.0176 |          18.8201 |           0.2655 |
[32m[20221213 15:22:46 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:22:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.43
[32m[20221213 15:22:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.47
[32m[20221213 15:22:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.62
[32m[20221213 15:22:47 @agent_ppo2.py:143][0m Total time:      29.86 min
[32m[20221213 15:22:47 @agent_ppo2.py:145][0m 2695168 total steps have happened
[32m[20221213 15:22:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1316 --------------------------#
[32m[20221213 15:22:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:47 @agent_ppo2.py:185][0m |           0.0000 |          19.8970 |           0.2587 |
[32m[20221213 15:22:47 @agent_ppo2.py:185][0m |          -0.0082 |          19.4027 |           0.2583 |
[32m[20221213 15:22:47 @agent_ppo2.py:185][0m |          -0.0068 |          19.2436 |           0.2583 |
[32m[20221213 15:22:47 @agent_ppo2.py:185][0m |          -0.0099 |          18.7275 |           0.2581 |
[32m[20221213 15:22:47 @agent_ppo2.py:185][0m |          -0.0108 |          18.5728 |           0.2581 |
[32m[20221213 15:22:47 @agent_ppo2.py:185][0m |          -0.0139 |          18.4050 |           0.2577 |
[32m[20221213 15:22:48 @agent_ppo2.py:185][0m |          -0.0148 |          18.2496 |           0.2580 |
[32m[20221213 15:22:48 @agent_ppo2.py:185][0m |          -0.0121 |          18.1401 |           0.2578 |
[32m[20221213 15:22:48 @agent_ppo2.py:185][0m |          -0.0152 |          18.0395 |           0.2579 |
[32m[20221213 15:22:48 @agent_ppo2.py:185][0m |          -0.0153 |          17.9496 |           0.2578 |
[32m[20221213 15:22:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.43
[32m[20221213 15:22:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 251.20
[32m[20221213 15:22:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.82
[32m[20221213 15:22:48 @agent_ppo2.py:143][0m Total time:      29.88 min
[32m[20221213 15:22:48 @agent_ppo2.py:145][0m 2697216 total steps have happened
[32m[20221213 15:22:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1317 --------------------------#
[32m[20221213 15:22:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:48 @agent_ppo2.py:185][0m |          -0.0020 |          20.6932 |           0.2646 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0080 |          20.1312 |           0.2643 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0114 |          19.8208 |           0.2643 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0112 |          19.6427 |           0.2641 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0154 |          19.4354 |           0.2638 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0151 |          19.3399 |           0.2639 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0166 |          19.2283 |           0.2641 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0164 |          19.1496 |           0.2643 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0168 |          19.1891 |           0.2639 |
[32m[20221213 15:22:49 @agent_ppo2.py:185][0m |          -0.0190 |          18.9241 |           0.2640 |
[32m[20221213 15:22:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.31
[32m[20221213 15:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.55
[32m[20221213 15:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.16
[32m[20221213 15:22:49 @agent_ppo2.py:143][0m Total time:      29.90 min
[32m[20221213 15:22:49 @agent_ppo2.py:145][0m 2699264 total steps have happened
[32m[20221213 15:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1318 --------------------------#
[32m[20221213 15:22:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |           0.0092 |          21.8713 |           0.2662 |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |          -0.0053 |          20.2123 |           0.2652 |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |          -0.0089 |          19.9824 |           0.2655 |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |          -0.0065 |          19.9498 |           0.2655 |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |          -0.0109 |          19.7879 |           0.2653 |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |          -0.0106 |          19.7690 |           0.2654 |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |          -0.0100 |          19.6905 |           0.2652 |
[32m[20221213 15:22:50 @agent_ppo2.py:185][0m |          -0.0130 |          19.6518 |           0.2655 |
[32m[20221213 15:22:51 @agent_ppo2.py:185][0m |          -0.0125 |          19.6232 |           0.2653 |
[32m[20221213 15:22:51 @agent_ppo2.py:185][0m |          -0.0129 |          19.5870 |           0.2653 |
[32m[20221213 15:22:51 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:22:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.60
[32m[20221213 15:22:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.42
[32m[20221213 15:22:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.02
[32m[20221213 15:22:51 @agent_ppo2.py:143][0m Total time:      29.93 min
[32m[20221213 15:22:51 @agent_ppo2.py:145][0m 2701312 total steps have happened
[32m[20221213 15:22:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1319 --------------------------#
[32m[20221213 15:22:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:51 @agent_ppo2.py:185][0m |          -0.0026 |          20.6101 |           0.2692 |
[32m[20221213 15:22:51 @agent_ppo2.py:185][0m |          -0.0064 |          20.2799 |           0.2688 |
[32m[20221213 15:22:51 @agent_ppo2.py:185][0m |          -0.0110 |          20.0978 |           0.2690 |
[32m[20221213 15:22:51 @agent_ppo2.py:185][0m |          -0.0113 |          19.9623 |           0.2689 |
[32m[20221213 15:22:52 @agent_ppo2.py:185][0m |          -0.0092 |          20.2761 |           0.2691 |
[32m[20221213 15:22:52 @agent_ppo2.py:185][0m |          -0.0110 |          19.7608 |           0.2693 |
[32m[20221213 15:22:52 @agent_ppo2.py:185][0m |          -0.0101 |          19.9915 |           0.2697 |
[32m[20221213 15:22:52 @agent_ppo2.py:185][0m |          -0.0129 |          19.6791 |           0.2698 |
[32m[20221213 15:22:52 @agent_ppo2.py:185][0m |          -0.0126 |          19.6076 |           0.2700 |
[32m[20221213 15:22:52 @agent_ppo2.py:185][0m |          -0.0149 |          19.4834 |           0.2700 |
[32m[20221213 15:22:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.48
[32m[20221213 15:22:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.56
[32m[20221213 15:22:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.23
[32m[20221213 15:22:52 @agent_ppo2.py:143][0m Total time:      29.95 min
[32m[20221213 15:22:52 @agent_ppo2.py:145][0m 2703360 total steps have happened
[32m[20221213 15:22:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1320 --------------------------#
[32m[20221213 15:22:52 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:22:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |           0.0008 |          21.4661 |           0.2692 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0099 |          20.6217 |           0.2688 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0125 |          20.2331 |           0.2688 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0134 |          19.8408 |           0.2692 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0066 |          20.2507 |           0.2686 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0143 |          19.5415 |           0.2690 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0172 |          19.3940 |           0.2687 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0184 |          19.2331 |           0.2687 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0149 |          19.1678 |           0.2686 |
[32m[20221213 15:22:53 @agent_ppo2.py:185][0m |          -0.0185 |          19.0249 |           0.2686 |
[32m[20221213 15:22:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.70
[32m[20221213 15:22:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.03
[32m[20221213 15:22:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.19
[32m[20221213 15:22:54 @agent_ppo2.py:143][0m Total time:      29.97 min
[32m[20221213 15:22:54 @agent_ppo2.py:145][0m 2705408 total steps have happened
[32m[20221213 15:22:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1321 --------------------------#
[32m[20221213 15:22:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:54 @agent_ppo2.py:185][0m |          -0.0040 |          22.0678 |           0.2716 |
[32m[20221213 15:22:54 @agent_ppo2.py:185][0m |          -0.0074 |          21.4686 |           0.2714 |
[32m[20221213 15:22:54 @agent_ppo2.py:185][0m |          -0.0117 |          21.2483 |           0.2717 |
[32m[20221213 15:22:54 @agent_ppo2.py:185][0m |          -0.0142 |          21.0262 |           0.2716 |
[32m[20221213 15:22:54 @agent_ppo2.py:185][0m |          -0.0166 |          20.9031 |           0.2720 |
[32m[20221213 15:22:54 @agent_ppo2.py:185][0m |          -0.0166 |          20.8167 |           0.2722 |
[32m[20221213 15:22:54 @agent_ppo2.py:185][0m |          -0.0153 |          20.7080 |           0.2722 |
[32m[20221213 15:22:55 @agent_ppo2.py:185][0m |          -0.0199 |          20.6276 |           0.2722 |
[32m[20221213 15:22:55 @agent_ppo2.py:185][0m |          -0.0202 |          20.6133 |           0.2723 |
[32m[20221213 15:22:55 @agent_ppo2.py:185][0m |          -0.0185 |          20.5304 |           0.2725 |
[32m[20221213 15:22:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.61
[32m[20221213 15:22:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.85
[32m[20221213 15:22:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.41
[32m[20221213 15:22:55 @agent_ppo2.py:143][0m Total time:      29.99 min
[32m[20221213 15:22:55 @agent_ppo2.py:145][0m 2707456 total steps have happened
[32m[20221213 15:22:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1322 --------------------------#
[32m[20221213 15:22:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:55 @agent_ppo2.py:185][0m |          -0.0017 |          20.3221 |           0.2764 |
[32m[20221213 15:22:55 @agent_ppo2.py:185][0m |          -0.0084 |          20.1211 |           0.2761 |
[32m[20221213 15:22:55 @agent_ppo2.py:185][0m |          -0.0105 |          19.9239 |           0.2762 |
[32m[20221213 15:22:56 @agent_ppo2.py:185][0m |          -0.0114 |          19.8204 |           0.2758 |
[32m[20221213 15:22:56 @agent_ppo2.py:185][0m |          -0.0134 |          19.7442 |           0.2757 |
[32m[20221213 15:22:56 @agent_ppo2.py:185][0m |          -0.0145 |          19.6457 |           0.2756 |
[32m[20221213 15:22:56 @agent_ppo2.py:185][0m |          -0.0161 |          19.5546 |           0.2756 |
[32m[20221213 15:22:56 @agent_ppo2.py:185][0m |          -0.0186 |          19.4810 |           0.2752 |
[32m[20221213 15:22:56 @agent_ppo2.py:185][0m |          -0.0105 |          20.0897 |           0.2753 |
[32m[20221213 15:22:56 @agent_ppo2.py:185][0m |          -0.0159 |          19.3550 |           0.2751 |
[32m[20221213 15:22:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:22:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.13
[32m[20221213 15:22:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.61
[32m[20221213 15:22:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.73
[32m[20221213 15:22:56 @agent_ppo2.py:143][0m Total time:      30.02 min
[32m[20221213 15:22:56 @agent_ppo2.py:145][0m 2709504 total steps have happened
[32m[20221213 15:22:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1323 --------------------------#
[32m[20221213 15:22:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |           0.0034 |          20.2600 |           0.2742 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0012 |          19.6774 |           0.2741 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0003 |          20.6077 |           0.2736 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0098 |          19.3391 |           0.2741 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0099 |          19.5196 |           0.2740 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0123 |          19.2173 |           0.2738 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0029 |          21.0058 |           0.2741 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0025 |          21.5185 |           0.2739 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0143 |          19.0735 |           0.2738 |
[32m[20221213 15:22:57 @agent_ppo2.py:185][0m |          -0.0195 |          19.0234 |           0.2738 |
[32m[20221213 15:22:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:22:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.95
[32m[20221213 15:22:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.32
[32m[20221213 15:22:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.40
[32m[20221213 15:22:58 @agent_ppo2.py:143][0m Total time:      30.04 min
[32m[20221213 15:22:58 @agent_ppo2.py:145][0m 2711552 total steps have happened
[32m[20221213 15:22:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1324 --------------------------#
[32m[20221213 15:22:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:22:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:58 @agent_ppo2.py:185][0m |          -0.0036 |          19.9602 |           0.2766 |
[32m[20221213 15:22:58 @agent_ppo2.py:185][0m |          -0.0074 |          19.6831 |           0.2764 |
[32m[20221213 15:22:58 @agent_ppo2.py:185][0m |          -0.0101 |          19.5315 |           0.2760 |
[32m[20221213 15:22:58 @agent_ppo2.py:185][0m |          -0.0128 |          19.4086 |           0.2755 |
[32m[20221213 15:22:58 @agent_ppo2.py:185][0m |          -0.0121 |          19.2791 |           0.2755 |
[32m[20221213 15:22:58 @agent_ppo2.py:185][0m |          -0.0147 |          19.1474 |           0.2754 |
[32m[20221213 15:22:59 @agent_ppo2.py:185][0m |          -0.0169 |          19.0590 |           0.2752 |
[32m[20221213 15:22:59 @agent_ppo2.py:185][0m |          -0.0133 |          19.1415 |           0.2751 |
[32m[20221213 15:22:59 @agent_ppo2.py:185][0m |          -0.0170 |          18.8468 |           0.2751 |
[32m[20221213 15:22:59 @agent_ppo2.py:185][0m |          -0.0188 |          18.6990 |           0.2749 |
[32m[20221213 15:22:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:22:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.49
[32m[20221213 15:22:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.41
[32m[20221213 15:22:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.74
[32m[20221213 15:22:59 @agent_ppo2.py:143][0m Total time:      30.06 min
[32m[20221213 15:22:59 @agent_ppo2.py:145][0m 2713600 total steps have happened
[32m[20221213 15:22:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1325 --------------------------#
[32m[20221213 15:22:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:22:59 @agent_ppo2.py:185][0m |          -0.0008 |          20.5173 |           0.2743 |
[32m[20221213 15:22:59 @agent_ppo2.py:185][0m |          -0.0025 |          20.2719 |           0.2737 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0079 |          20.0135 |           0.2732 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0063 |          19.8900 |           0.2733 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0109 |          19.8326 |           0.2728 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0029 |          21.0287 |           0.2730 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0139 |          19.6964 |           0.2726 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0157 |          19.6993 |           0.2723 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0163 |          19.6010 |           0.2727 |
[32m[20221213 15:23:00 @agent_ppo2.py:185][0m |          -0.0105 |          19.8527 |           0.2723 |
[32m[20221213 15:23:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.23
[32m[20221213 15:23:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.96
[32m[20221213 15:23:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.74
[32m[20221213 15:23:00 @agent_ppo2.py:143][0m Total time:      30.09 min
[32m[20221213 15:23:00 @agent_ppo2.py:145][0m 2715648 total steps have happened
[32m[20221213 15:23:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1326 --------------------------#
[32m[20221213 15:23:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |           0.0005 |          17.7403 |           0.2769 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0058 |          17.3629 |           0.2763 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0087 |          17.1319 |           0.2764 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0125 |          16.9333 |           0.2758 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0119 |          16.7782 |           0.2755 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0128 |          16.7256 |           0.2757 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0161 |          16.5969 |           0.2754 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0148 |          16.4949 |           0.2751 |
[32m[20221213 15:23:01 @agent_ppo2.py:185][0m |          -0.0033 |          18.6153 |           0.2754 |
[32m[20221213 15:23:02 @agent_ppo2.py:185][0m |          -0.0151 |          16.3738 |           0.2747 |
[32m[20221213 15:23:02 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:23:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.85
[32m[20221213 15:23:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.95
[32m[20221213 15:23:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.29
[32m[20221213 15:23:02 @agent_ppo2.py:143][0m Total time:      30.11 min
[32m[20221213 15:23:02 @agent_ppo2.py:145][0m 2717696 total steps have happened
[32m[20221213 15:23:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1327 --------------------------#
[32m[20221213 15:23:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:02 @agent_ppo2.py:185][0m |           0.0040 |          20.7323 |           0.2726 |
[32m[20221213 15:23:02 @agent_ppo2.py:185][0m |          -0.0074 |          19.7519 |           0.2726 |
[32m[20221213 15:23:02 @agent_ppo2.py:185][0m |          -0.0112 |          19.6185 |           0.2726 |
[32m[20221213 15:23:02 @agent_ppo2.py:185][0m |          -0.0057 |          20.6919 |           0.2725 |
[32m[20221213 15:23:02 @agent_ppo2.py:185][0m |          -0.0137 |          19.4834 |           0.2728 |
[32m[20221213 15:23:03 @agent_ppo2.py:185][0m |          -0.0158 |          19.4321 |           0.2729 |
[32m[20221213 15:23:03 @agent_ppo2.py:185][0m |          -0.0136 |          19.4060 |           0.2732 |
[32m[20221213 15:23:03 @agent_ppo2.py:185][0m |          -0.0161 |          19.3382 |           0.2729 |
[32m[20221213 15:23:03 @agent_ppo2.py:185][0m |          -0.0156 |          19.4681 |           0.2728 |
[32m[20221213 15:23:03 @agent_ppo2.py:185][0m |          -0.0220 |          19.2820 |           0.2729 |
[32m[20221213 15:23:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:23:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.07
[32m[20221213 15:23:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.02
[32m[20221213 15:23:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.56
[32m[20221213 15:23:03 @agent_ppo2.py:143][0m Total time:      30.13 min
[32m[20221213 15:23:03 @agent_ppo2.py:145][0m 2719744 total steps have happened
[32m[20221213 15:23:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1328 --------------------------#
[32m[20221213 15:23:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:03 @agent_ppo2.py:185][0m |          -0.0040 |          19.7705 |           0.2726 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0088 |          19.3621 |           0.2714 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0109 |          19.1926 |           0.2715 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0140 |          19.0562 |           0.2713 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0127 |          19.1539 |           0.2710 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0175 |          18.8820 |           0.2711 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0200 |          18.7955 |           0.2706 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0098 |          20.0550 |           0.2707 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0155 |          18.7170 |           0.2705 |
[32m[20221213 15:23:04 @agent_ppo2.py:185][0m |          -0.0142 |          18.7653 |           0.2702 |
[32m[20221213 15:23:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:23:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.39
[32m[20221213 15:23:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.57
[32m[20221213 15:23:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.63
[32m[20221213 15:23:05 @agent_ppo2.py:143][0m Total time:      30.16 min
[32m[20221213 15:23:05 @agent_ppo2.py:145][0m 2721792 total steps have happened
[32m[20221213 15:23:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1329 --------------------------#
[32m[20221213 15:23:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0010 |          20.3988 |           0.2703 |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0070 |          19.9205 |           0.2705 |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0080 |          19.7746 |           0.2706 |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0070 |          19.6589 |           0.2705 |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0064 |          19.8413 |           0.2704 |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0129 |          19.5092 |           0.2707 |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0116 |          19.4119 |           0.2707 |
[32m[20221213 15:23:05 @agent_ppo2.py:185][0m |          -0.0140 |          19.3789 |           0.2707 |
[32m[20221213 15:23:06 @agent_ppo2.py:185][0m |          -0.0047 |          21.7807 |           0.2712 |
[32m[20221213 15:23:06 @agent_ppo2.py:185][0m |          -0.0118 |          19.4543 |           0.2708 |
[32m[20221213 15:23:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.63
[32m[20221213 15:23:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.52
[32m[20221213 15:23:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.02
[32m[20221213 15:23:06 @agent_ppo2.py:143][0m Total time:      30.18 min
[32m[20221213 15:23:06 @agent_ppo2.py:145][0m 2723840 total steps have happened
[32m[20221213 15:23:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1330 --------------------------#
[32m[20221213 15:23:06 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:23:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:06 @agent_ppo2.py:185][0m |          -0.0028 |          20.0536 |           0.2762 |
[32m[20221213 15:23:06 @agent_ppo2.py:185][0m |          -0.0084 |          19.7684 |           0.2756 |
[32m[20221213 15:23:06 @agent_ppo2.py:185][0m |          -0.0035 |          19.9212 |           0.2753 |
[32m[20221213 15:23:07 @agent_ppo2.py:185][0m |          -0.0117 |          19.3191 |           0.2752 |
[32m[20221213 15:23:07 @agent_ppo2.py:185][0m |          -0.0134 |          19.2074 |           0.2755 |
[32m[20221213 15:23:07 @agent_ppo2.py:185][0m |          -0.0163 |          19.1368 |           0.2753 |
[32m[20221213 15:23:07 @agent_ppo2.py:185][0m |          -0.0164 |          19.1076 |           0.2753 |
[32m[20221213 15:23:07 @agent_ppo2.py:185][0m |          -0.0184 |          19.0420 |           0.2750 |
[32m[20221213 15:23:07 @agent_ppo2.py:185][0m |          -0.0114 |          19.6270 |           0.2750 |
[32m[20221213 15:23:07 @agent_ppo2.py:185][0m |          -0.0184 |          18.9216 |           0.2750 |
[32m[20221213 15:23:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.03
[32m[20221213 15:23:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.89
[32m[20221213 15:23:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.96
[32m[20221213 15:23:07 @agent_ppo2.py:143][0m Total time:      30.20 min
[32m[20221213 15:23:07 @agent_ppo2.py:145][0m 2725888 total steps have happened
[32m[20221213 15:23:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1331 --------------------------#
[32m[20221213 15:23:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0006 |          19.3178 |           0.2760 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |           0.0038 |          19.8670 |           0.2755 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0056 |          19.0942 |           0.2751 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0133 |          18.7555 |           0.2751 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0132 |          18.7373 |           0.2750 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0137 |          18.6466 |           0.2748 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0090 |          18.8845 |           0.2749 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0154 |          18.5483 |           0.2747 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0158 |          18.4999 |           0.2746 |
[32m[20221213 15:23:08 @agent_ppo2.py:185][0m |          -0.0143 |          18.4814 |           0.2745 |
[32m[20221213 15:23:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.92
[32m[20221213 15:23:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.52
[32m[20221213 15:23:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.90
[32m[20221213 15:23:09 @agent_ppo2.py:143][0m Total time:      30.22 min
[32m[20221213 15:23:09 @agent_ppo2.py:145][0m 2727936 total steps have happened
[32m[20221213 15:23:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1332 --------------------------#
[32m[20221213 15:23:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:09 @agent_ppo2.py:185][0m |           0.0013 |          19.9230 |           0.2701 |
[32m[20221213 15:23:09 @agent_ppo2.py:185][0m |          -0.0093 |          19.3560 |           0.2698 |
[32m[20221213 15:23:09 @agent_ppo2.py:185][0m |          -0.0112 |          19.0478 |           0.2693 |
[32m[20221213 15:23:09 @agent_ppo2.py:185][0m |          -0.0014 |          19.9141 |           0.2694 |
[32m[20221213 15:23:09 @agent_ppo2.py:185][0m |          -0.0162 |          18.6945 |           0.2692 |
[32m[20221213 15:23:09 @agent_ppo2.py:185][0m |          -0.0147 |          18.5363 |           0.2691 |
[32m[20221213 15:23:09 @agent_ppo2.py:185][0m |          -0.0165 |          18.4051 |           0.2690 |
[32m[20221213 15:23:10 @agent_ppo2.py:185][0m |          -0.0155 |          18.3311 |           0.2689 |
[32m[20221213 15:23:10 @agent_ppo2.py:185][0m |          -0.0185 |          18.2576 |           0.2690 |
[32m[20221213 15:23:10 @agent_ppo2.py:185][0m |          -0.0192 |          18.1779 |           0.2692 |
[32m[20221213 15:23:10 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:23:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.18
[32m[20221213 15:23:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 253.91
[32m[20221213 15:23:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.03
[32m[20221213 15:23:10 @agent_ppo2.py:143][0m Total time:      30.25 min
[32m[20221213 15:23:10 @agent_ppo2.py:145][0m 2729984 total steps have happened
[32m[20221213 15:23:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1333 --------------------------#
[32m[20221213 15:23:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:10 @agent_ppo2.py:185][0m |          -0.0021 |          19.7104 |           0.2768 |
[32m[20221213 15:23:10 @agent_ppo2.py:185][0m |          -0.0058 |          19.0056 |           0.2767 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0095 |          18.4747 |           0.2758 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0003 |          18.6746 |           0.2760 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0117 |          17.9756 |           0.2759 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0057 |          18.7220 |           0.2753 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0056 |          18.4246 |           0.2755 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0177 |          17.2977 |           0.2753 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0154 |          16.9911 |           0.2751 |
[32m[20221213 15:23:11 @agent_ppo2.py:185][0m |          -0.0146 |          16.8091 |           0.2751 |
[32m[20221213 15:23:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:23:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.58
[32m[20221213 15:23:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 254.17
[32m[20221213 15:23:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.65
[32m[20221213 15:23:11 @agent_ppo2.py:143][0m Total time:      30.27 min
[32m[20221213 15:23:11 @agent_ppo2.py:145][0m 2732032 total steps have happened
[32m[20221213 15:23:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1334 --------------------------#
[32m[20221213 15:23:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0044 |          20.2329 |           0.2816 |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0083 |          19.5851 |           0.2809 |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0114 |          19.2630 |           0.2805 |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0137 |          19.0443 |           0.2804 |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0149 |          18.8346 |           0.2803 |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0133 |          18.8628 |           0.2799 |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0074 |          19.6554 |           0.2798 |
[32m[20221213 15:23:12 @agent_ppo2.py:185][0m |          -0.0185 |          18.3364 |           0.2793 |
[32m[20221213 15:23:13 @agent_ppo2.py:185][0m |          -0.0180 |          18.2447 |           0.2791 |
[32m[20221213 15:23:13 @agent_ppo2.py:185][0m |          -0.0172 |          18.1071 |           0.2790 |
[32m[20221213 15:23:13 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:23:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.73
[32m[20221213 15:23:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.94
[32m[20221213 15:23:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.96
[32m[20221213 15:23:13 @agent_ppo2.py:143][0m Total time:      30.29 min
[32m[20221213 15:23:13 @agent_ppo2.py:145][0m 2734080 total steps have happened
[32m[20221213 15:23:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1335 --------------------------#
[32m[20221213 15:23:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:13 @agent_ppo2.py:185][0m |          -0.0036 |          19.4670 |           0.2740 |
[32m[20221213 15:23:13 @agent_ppo2.py:185][0m |          -0.0122 |          18.6414 |           0.2733 |
[32m[20221213 15:23:13 @agent_ppo2.py:185][0m |          -0.0073 |          18.3224 |           0.2730 |
[32m[20221213 15:23:13 @agent_ppo2.py:185][0m |          -0.0152 |          17.8276 |           0.2726 |
[32m[20221213 15:23:14 @agent_ppo2.py:185][0m |          -0.0095 |          17.9386 |           0.2728 |
[32m[20221213 15:23:14 @agent_ppo2.py:185][0m |          -0.0165 |          17.3564 |           0.2724 |
[32m[20221213 15:23:14 @agent_ppo2.py:185][0m |          -0.0171 |          17.1471 |           0.2725 |
[32m[20221213 15:23:14 @agent_ppo2.py:185][0m |          -0.0197 |          16.9890 |           0.2721 |
[32m[20221213 15:23:14 @agent_ppo2.py:185][0m |          -0.0153 |          16.9016 |           0.2722 |
[32m[20221213 15:23:14 @agent_ppo2.py:185][0m |          -0.0179 |          16.7774 |           0.2718 |
[32m[20221213 15:23:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:23:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.33
[32m[20221213 15:23:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.22
[32m[20221213 15:23:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.63
[32m[20221213 15:23:14 @agent_ppo2.py:143][0m Total time:      30.32 min
[32m[20221213 15:23:14 @agent_ppo2.py:145][0m 2736128 total steps have happened
[32m[20221213 15:23:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1336 --------------------------#
[32m[20221213 15:23:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |           0.0072 |          22.4051 |           0.2693 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0060 |          20.7593 |           0.2687 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0154 |          20.1371 |           0.2688 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0161 |          19.8634 |           0.2689 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0078 |          20.7825 |           0.2689 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0145 |          19.5492 |           0.2690 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0172 |          19.5436 |           0.2689 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0094 |          20.3818 |           0.2691 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0158 |          19.3737 |           0.2689 |
[32m[20221213 15:23:15 @agent_ppo2.py:185][0m |          -0.0189 |          19.2667 |           0.2688 |
[32m[20221213 15:23:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:23:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.19
[32m[20221213 15:23:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.30
[32m[20221213 15:23:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.39
[32m[20221213 15:23:16 @agent_ppo2.py:143][0m Total time:      30.34 min
[32m[20221213 15:23:16 @agent_ppo2.py:145][0m 2738176 total steps have happened
[32m[20221213 15:23:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1337 --------------------------#
[32m[20221213 15:23:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:16 @agent_ppo2.py:185][0m |          -0.0019 |          20.4583 |           0.2714 |
[32m[20221213 15:23:16 @agent_ppo2.py:185][0m |          -0.0076 |          19.7021 |           0.2704 |
[32m[20221213 15:23:16 @agent_ppo2.py:185][0m |          -0.0105 |          19.3205 |           0.2706 |
[32m[20221213 15:23:16 @agent_ppo2.py:185][0m |          -0.0111 |          19.0314 |           0.2707 |
[32m[20221213 15:23:16 @agent_ppo2.py:185][0m |          -0.0143 |          18.8379 |           0.2703 |
[32m[20221213 15:23:16 @agent_ppo2.py:185][0m |          -0.0140 |          18.6431 |           0.2702 |
[32m[20221213 15:23:17 @agent_ppo2.py:185][0m |          -0.0090 |          18.6834 |           0.2701 |
[32m[20221213 15:23:17 @agent_ppo2.py:185][0m |          -0.0183 |          18.3043 |           0.2699 |
[32m[20221213 15:23:17 @agent_ppo2.py:185][0m |          -0.0180 |          18.1776 |           0.2697 |
[32m[20221213 15:23:17 @agent_ppo2.py:185][0m |          -0.0183 |          18.0538 |           0.2699 |
[32m[20221213 15:23:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.92
[32m[20221213 15:23:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 244.16
[32m[20221213 15:23:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.54
[32m[20221213 15:23:17 @agent_ppo2.py:143][0m Total time:      30.36 min
[32m[20221213 15:23:17 @agent_ppo2.py:145][0m 2740224 total steps have happened
[32m[20221213 15:23:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1338 --------------------------#
[32m[20221213 15:23:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:17 @agent_ppo2.py:185][0m |          -0.0024 |          20.1823 |           0.2701 |
[32m[20221213 15:23:17 @agent_ppo2.py:185][0m |          -0.0090 |          19.6514 |           0.2687 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0123 |          19.3750 |           0.2688 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0123 |          19.1150 |           0.2686 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0149 |          18.9058 |           0.2684 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0173 |          18.6675 |           0.2682 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0177 |          18.5047 |           0.2681 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0190 |          18.3816 |           0.2677 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0223 |          18.1972 |           0.2675 |
[32m[20221213 15:23:18 @agent_ppo2.py:185][0m |          -0.0187 |          18.0309 |           0.2671 |
[32m[20221213 15:23:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.44
[32m[20221213 15:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.62
[32m[20221213 15:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.34
[32m[20221213 15:23:18 @agent_ppo2.py:143][0m Total time:      30.39 min
[32m[20221213 15:23:18 @agent_ppo2.py:145][0m 2742272 total steps have happened
[32m[20221213 15:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1339 --------------------------#
[32m[20221213 15:23:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |           0.0012 |          18.9692 |           0.2661 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0072 |          18.4091 |           0.2655 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0084 |          18.2025 |           0.2655 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0127 |          18.0097 |           0.2651 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0145 |          17.8521 |           0.2652 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0133 |          17.7503 |           0.2654 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0166 |          17.6639 |           0.2650 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0124 |          17.9592 |           0.2650 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0182 |          17.4465 |           0.2649 |
[32m[20221213 15:23:19 @agent_ppo2.py:185][0m |          -0.0192 |          17.3751 |           0.2649 |
[32m[20221213 15:23:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.46
[32m[20221213 15:23:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.95
[32m[20221213 15:23:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.76
[32m[20221213 15:23:20 @agent_ppo2.py:143][0m Total time:      30.41 min
[32m[20221213 15:23:20 @agent_ppo2.py:145][0m 2744320 total steps have happened
[32m[20221213 15:23:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1340 --------------------------#
[32m[20221213 15:23:20 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:23:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:20 @agent_ppo2.py:185][0m |          -0.0005 |          21.8789 |           0.2607 |
[32m[20221213 15:23:20 @agent_ppo2.py:185][0m |          -0.0044 |          21.1186 |           0.2605 |
[32m[20221213 15:23:20 @agent_ppo2.py:185][0m |          -0.0053 |          20.8402 |           0.2600 |
[32m[20221213 15:23:20 @agent_ppo2.py:185][0m |          -0.0100 |          20.6762 |           0.2604 |
[32m[20221213 15:23:20 @agent_ppo2.py:185][0m |          -0.0096 |          20.5853 |           0.2603 |
[32m[20221213 15:23:21 @agent_ppo2.py:185][0m |          -0.0044 |          20.9034 |           0.2601 |
[32m[20221213 15:23:21 @agent_ppo2.py:185][0m |          -0.0134 |          20.3720 |           0.2602 |
[32m[20221213 15:23:21 @agent_ppo2.py:185][0m |          -0.0136 |          20.3196 |           0.2604 |
[32m[20221213 15:23:21 @agent_ppo2.py:185][0m |          -0.0160 |          20.2561 |           0.2603 |
[32m[20221213 15:23:21 @agent_ppo2.py:185][0m |          -0.0165 |          20.2335 |           0.2607 |
[32m[20221213 15:23:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.96
[32m[20221213 15:23:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.65
[32m[20221213 15:23:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.68
[32m[20221213 15:23:21 @agent_ppo2.py:143][0m Total time:      30.43 min
[32m[20221213 15:23:21 @agent_ppo2.py:145][0m 2746368 total steps have happened
[32m[20221213 15:23:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1341 --------------------------#
[32m[20221213 15:23:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:21 @agent_ppo2.py:185][0m |          -0.0016 |          20.9289 |           0.2680 |
[32m[20221213 15:23:21 @agent_ppo2.py:185][0m |          -0.0095 |          20.5895 |           0.2671 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0091 |          20.4562 |           0.2673 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0102 |          20.3874 |           0.2674 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0121 |          20.3447 |           0.2671 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0144 |          20.2401 |           0.2672 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0134 |          20.2009 |           0.2671 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0150 |          20.1348 |           0.2674 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0062 |          21.0263 |           0.2670 |
[32m[20221213 15:23:22 @agent_ppo2.py:185][0m |          -0.0118 |          20.5260 |           0.2675 |
[32m[20221213 15:23:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.16
[32m[20221213 15:23:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.45
[32m[20221213 15:23:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.82
[32m[20221213 15:23:22 @agent_ppo2.py:143][0m Total time:      30.45 min
[32m[20221213 15:23:22 @agent_ppo2.py:145][0m 2748416 total steps have happened
[32m[20221213 15:23:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1342 --------------------------#
[32m[20221213 15:23:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0011 |          18.8633 |           0.2613 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0076 |          18.2378 |           0.2613 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0109 |          17.9586 |           0.2613 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0077 |          17.8635 |           0.2617 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0122 |          17.4088 |           0.2619 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0144 |          17.2078 |           0.2619 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0098 |          17.4265 |           0.2620 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0152 |          16.9146 |           0.2622 |
[32m[20221213 15:23:23 @agent_ppo2.py:185][0m |          -0.0175 |          16.7477 |           0.2624 |
[32m[20221213 15:23:24 @agent_ppo2.py:185][0m |          -0.0186 |          16.6799 |           0.2626 |
[32m[20221213 15:23:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.33
[32m[20221213 15:23:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.91
[32m[20221213 15:23:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.89
[32m[20221213 15:23:24 @agent_ppo2.py:143][0m Total time:      30.48 min
[32m[20221213 15:23:24 @agent_ppo2.py:145][0m 2750464 total steps have happened
[32m[20221213 15:23:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1343 --------------------------#
[32m[20221213 15:23:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:24 @agent_ppo2.py:185][0m |           0.0003 |          20.4471 |           0.2638 |
[32m[20221213 15:23:24 @agent_ppo2.py:185][0m |          -0.0092 |          19.7277 |           0.2636 |
[32m[20221213 15:23:24 @agent_ppo2.py:185][0m |          -0.0119 |          19.4289 |           0.2634 |
[32m[20221213 15:23:24 @agent_ppo2.py:185][0m |          -0.0101 |          19.3355 |           0.2636 |
[32m[20221213 15:23:24 @agent_ppo2.py:185][0m |          -0.0118 |          19.0587 |           0.2641 |
[32m[20221213 15:23:25 @agent_ppo2.py:185][0m |          -0.0134 |          18.9337 |           0.2636 |
[32m[20221213 15:23:25 @agent_ppo2.py:185][0m |          -0.0090 |          19.2541 |           0.2636 |
[32m[20221213 15:23:25 @agent_ppo2.py:185][0m |          -0.0146 |          18.5628 |           0.2640 |
[32m[20221213 15:23:25 @agent_ppo2.py:185][0m |          -0.0188 |          18.3713 |           0.2641 |
[32m[20221213 15:23:25 @agent_ppo2.py:185][0m |          -0.0201 |          18.3191 |           0.2638 |
[32m[20221213 15:23:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.69
[32m[20221213 15:23:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.28
[32m[20221213 15:23:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.16
[32m[20221213 15:23:25 @agent_ppo2.py:143][0m Total time:      30.50 min
[32m[20221213 15:23:25 @agent_ppo2.py:145][0m 2752512 total steps have happened
[32m[20221213 15:23:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1344 --------------------------#
[32m[20221213 15:23:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:25 @agent_ppo2.py:185][0m |          -0.0041 |          20.5270 |           0.2764 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0072 |          19.8903 |           0.2756 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0105 |          19.5571 |           0.2754 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0152 |          19.3760 |           0.2754 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0104 |          19.2100 |           0.2751 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0135 |          19.0600 |           0.2751 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0171 |          18.8674 |           0.2747 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0133 |          18.8301 |           0.2751 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0183 |          18.5911 |           0.2743 |
[32m[20221213 15:23:26 @agent_ppo2.py:185][0m |          -0.0192 |          18.4809 |           0.2744 |
[32m[20221213 15:23:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.87
[32m[20221213 15:23:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.68
[32m[20221213 15:23:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.92
[32m[20221213 15:23:26 @agent_ppo2.py:143][0m Total time:      30.52 min
[32m[20221213 15:23:26 @agent_ppo2.py:145][0m 2754560 total steps have happened
[32m[20221213 15:23:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1345 --------------------------#
[32m[20221213 15:23:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0005 |          20.1226 |           0.2695 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0052 |          19.6987 |           0.2690 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0095 |          19.5507 |           0.2689 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0101 |          19.2704 |           0.2690 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0105 |          19.1468 |           0.2689 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0114 |          19.0047 |           0.2687 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0127 |          18.8664 |           0.2687 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0137 |          18.7022 |           0.2687 |
[32m[20221213 15:23:27 @agent_ppo2.py:185][0m |          -0.0160 |          18.6352 |           0.2688 |
[32m[20221213 15:23:28 @agent_ppo2.py:185][0m |          -0.0160 |          18.5056 |           0.2687 |
[32m[20221213 15:23:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:23:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.24
[32m[20221213 15:23:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.77
[32m[20221213 15:23:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.11
[32m[20221213 15:23:28 @agent_ppo2.py:143][0m Total time:      30.54 min
[32m[20221213 15:23:28 @agent_ppo2.py:145][0m 2756608 total steps have happened
[32m[20221213 15:23:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1346 --------------------------#
[32m[20221213 15:23:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:28 @agent_ppo2.py:185][0m |          -0.0013 |          20.8562 |           0.2684 |
[32m[20221213 15:23:28 @agent_ppo2.py:185][0m |          -0.0073 |          20.4044 |           0.2685 |
[32m[20221213 15:23:28 @agent_ppo2.py:185][0m |          -0.0104 |          20.1044 |           0.2683 |
[32m[20221213 15:23:28 @agent_ppo2.py:185][0m |          -0.0118 |          19.9423 |           0.2681 |
[32m[20221213 15:23:28 @agent_ppo2.py:185][0m |          -0.0144 |          19.7475 |           0.2682 |
[32m[20221213 15:23:29 @agent_ppo2.py:185][0m |          -0.0142 |          19.6283 |           0.2679 |
[32m[20221213 15:23:29 @agent_ppo2.py:185][0m |          -0.0142 |          19.5840 |           0.2678 |
[32m[20221213 15:23:29 @agent_ppo2.py:185][0m |          -0.0182 |          19.4533 |           0.2677 |
[32m[20221213 15:23:29 @agent_ppo2.py:185][0m |          -0.0187 |          19.3369 |           0.2677 |
[32m[20221213 15:23:29 @agent_ppo2.py:185][0m |          -0.0201 |          19.2619 |           0.2676 |
[32m[20221213 15:23:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.16
[32m[20221213 15:23:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.08
[32m[20221213 15:23:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.36
[32m[20221213 15:23:29 @agent_ppo2.py:143][0m Total time:      30.57 min
[32m[20221213 15:23:29 @agent_ppo2.py:145][0m 2758656 total steps have happened
[32m[20221213 15:23:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1347 --------------------------#
[32m[20221213 15:23:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |           0.0004 |          21.1252 |           0.2667 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0052 |          20.6217 |           0.2665 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0083 |          20.3730 |           0.2663 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0092 |          20.0607 |           0.2662 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0122 |          19.8069 |           0.2662 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0142 |          19.6327 |           0.2661 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0162 |          19.4906 |           0.2659 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0079 |          19.4612 |           0.2657 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0165 |          19.2407 |           0.2656 |
[32m[20221213 15:23:30 @agent_ppo2.py:185][0m |          -0.0150 |          19.1546 |           0.2657 |
[32m[20221213 15:23:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.28
[32m[20221213 15:23:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.13
[32m[20221213 15:23:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.11
[32m[20221213 15:23:30 @agent_ppo2.py:143][0m Total time:      30.59 min
[32m[20221213 15:23:30 @agent_ppo2.py:145][0m 2760704 total steps have happened
[32m[20221213 15:23:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1348 --------------------------#
[32m[20221213 15:23:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |           0.0016 |          21.5195 |           0.2724 |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |          -0.0071 |          20.8502 |           0.2723 |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |          -0.0041 |          21.2703 |           0.2724 |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |          -0.0088 |          20.4370 |           0.2723 |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |          -0.0054 |          20.7854 |           0.2725 |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |          -0.0130 |          20.1247 |           0.2726 |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |          -0.0168 |          19.9750 |           0.2722 |
[32m[20221213 15:23:31 @agent_ppo2.py:185][0m |          -0.0164 |          19.8200 |           0.2724 |
[32m[20221213 15:23:32 @agent_ppo2.py:185][0m |          -0.0155 |          19.8382 |           0.2724 |
[32m[20221213 15:23:32 @agent_ppo2.py:185][0m |          -0.0153 |          19.8821 |           0.2723 |
[32m[20221213 15:23:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.50
[32m[20221213 15:23:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.64
[32m[20221213 15:23:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.74
[32m[20221213 15:23:32 @agent_ppo2.py:143][0m Total time:      30.61 min
[32m[20221213 15:23:32 @agent_ppo2.py:145][0m 2762752 total steps have happened
[32m[20221213 15:23:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1349 --------------------------#
[32m[20221213 15:23:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:32 @agent_ppo2.py:185][0m |           0.0036 |          20.7629 |           0.2716 |
[32m[20221213 15:23:32 @agent_ppo2.py:185][0m |          -0.0082 |          20.2199 |           0.2720 |
[32m[20221213 15:23:32 @agent_ppo2.py:185][0m |          -0.0101 |          20.0145 |           0.2719 |
[32m[20221213 15:23:32 @agent_ppo2.py:185][0m |          -0.0138 |          19.8171 |           0.2721 |
[32m[20221213 15:23:33 @agent_ppo2.py:185][0m |          -0.0067 |          20.7928 |           0.2724 |
[32m[20221213 15:23:33 @agent_ppo2.py:185][0m |          -0.0155 |          19.5748 |           0.2723 |
[32m[20221213 15:23:33 @agent_ppo2.py:185][0m |          -0.0150 |          19.3780 |           0.2723 |
[32m[20221213 15:23:33 @agent_ppo2.py:185][0m |          -0.0136 |          19.4950 |           0.2727 |
[32m[20221213 15:23:33 @agent_ppo2.py:185][0m |          -0.0189 |          19.2063 |           0.2726 |
[32m[20221213 15:23:33 @agent_ppo2.py:185][0m |          -0.0153 |          19.1009 |           0.2728 |
[32m[20221213 15:23:33 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.84
[32m[20221213 15:23:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.33
[32m[20221213 15:23:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.72
[32m[20221213 15:23:33 @agent_ppo2.py:143][0m Total time:      30.63 min
[32m[20221213 15:23:33 @agent_ppo2.py:145][0m 2764800 total steps have happened
[32m[20221213 15:23:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1350 --------------------------#
[32m[20221213 15:23:33 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:23:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0030 |          21.0948 |           0.2720 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0081 |          20.7159 |           0.2718 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0114 |          20.5346 |           0.2717 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0111 |          20.4002 |           0.2715 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0140 |          20.3364 |           0.2716 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0145 |          20.2347 |           0.2715 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0152 |          20.2155 |           0.2714 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0172 |          20.1217 |           0.2716 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0169 |          20.1014 |           0.2714 |
[32m[20221213 15:23:34 @agent_ppo2.py:185][0m |          -0.0183 |          20.0720 |           0.2715 |
[32m[20221213 15:23:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.02
[32m[20221213 15:23:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.30
[32m[20221213 15:23:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.79
[32m[20221213 15:23:35 @agent_ppo2.py:143][0m Total time:      30.66 min
[32m[20221213 15:23:35 @agent_ppo2.py:145][0m 2766848 total steps have happened
[32m[20221213 15:23:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1351 --------------------------#
[32m[20221213 15:23:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0008 |          20.6827 |           0.2690 |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0093 |          20.3495 |           0.2687 |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0131 |          20.1344 |           0.2688 |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0148 |          19.9427 |           0.2691 |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0040 |          22.2780 |           0.2691 |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0070 |          20.9462 |           0.2688 |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0173 |          19.4352 |           0.2691 |
[32m[20221213 15:23:35 @agent_ppo2.py:185][0m |          -0.0144 |          19.2779 |           0.2692 |
[32m[20221213 15:23:36 @agent_ppo2.py:185][0m |          -0.0163 |          19.1339 |           0.2695 |
[32m[20221213 15:23:36 @agent_ppo2.py:185][0m |          -0.0216 |          18.9500 |           0.2697 |
[32m[20221213 15:23:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.72
[32m[20221213 15:23:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.62
[32m[20221213 15:23:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.27
[32m[20221213 15:23:36 @agent_ppo2.py:143][0m Total time:      30.68 min
[32m[20221213 15:23:36 @agent_ppo2.py:145][0m 2768896 total steps have happened
[32m[20221213 15:23:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1352 --------------------------#
[32m[20221213 15:23:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:36 @agent_ppo2.py:185][0m |          -0.0048 |          19.6926 |           0.2821 |
[32m[20221213 15:23:36 @agent_ppo2.py:185][0m |          -0.0066 |          19.2625 |           0.2819 |
[32m[20221213 15:23:36 @agent_ppo2.py:185][0m |          -0.0120 |          18.7669 |           0.2817 |
[32m[20221213 15:23:36 @agent_ppo2.py:185][0m |          -0.0124 |          18.5517 |           0.2811 |
[32m[20221213 15:23:37 @agent_ppo2.py:185][0m |          -0.0158 |          18.3704 |           0.2814 |
[32m[20221213 15:23:37 @agent_ppo2.py:185][0m |          -0.0162 |          18.2009 |           0.2813 |
[32m[20221213 15:23:37 @agent_ppo2.py:185][0m |          -0.0159 |          18.1140 |           0.2810 |
[32m[20221213 15:23:37 @agent_ppo2.py:185][0m |          -0.0050 |          18.4793 |           0.2814 |
[32m[20221213 15:23:37 @agent_ppo2.py:185][0m |          -0.0168 |          17.8984 |           0.2812 |
[32m[20221213 15:23:37 @agent_ppo2.py:185][0m |          -0.0175 |          17.7682 |           0.2815 |
[32m[20221213 15:23:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.83
[32m[20221213 15:23:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.39
[32m[20221213 15:23:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.84
[32m[20221213 15:23:37 @agent_ppo2.py:143][0m Total time:      30.70 min
[32m[20221213 15:23:37 @agent_ppo2.py:145][0m 2770944 total steps have happened
[32m[20221213 15:23:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1353 --------------------------#
[32m[20221213 15:23:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0047 |          20.8063 |           0.2885 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0081 |          20.3513 |           0.2875 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0104 |          20.2527 |           0.2876 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0148 |          20.0733 |           0.2880 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0149 |          20.0124 |           0.2875 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0141 |          19.9646 |           0.2875 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0170 |          19.8291 |           0.2875 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0161 |          19.9097 |           0.2873 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0194 |          19.6832 |           0.2874 |
[32m[20221213 15:23:38 @agent_ppo2.py:185][0m |          -0.0184 |          19.6932 |           0.2870 |
[32m[20221213 15:23:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.21
[32m[20221213 15:23:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.36
[32m[20221213 15:23:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.05
[32m[20221213 15:23:39 @agent_ppo2.py:143][0m Total time:      30.72 min
[32m[20221213 15:23:39 @agent_ppo2.py:145][0m 2772992 total steps have happened
[32m[20221213 15:23:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1354 --------------------------#
[32m[20221213 15:23:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:39 @agent_ppo2.py:185][0m |          -0.0012 |          20.0276 |           0.2782 |
[32m[20221213 15:23:39 @agent_ppo2.py:185][0m |          -0.0054 |          19.7689 |           0.2780 |
[32m[20221213 15:23:39 @agent_ppo2.py:185][0m |          -0.0075 |          19.5725 |           0.2780 |
[32m[20221213 15:23:39 @agent_ppo2.py:185][0m |          -0.0068 |          19.5189 |           0.2779 |
[32m[20221213 15:23:39 @agent_ppo2.py:185][0m |          -0.0112 |          19.3748 |           0.2783 |
[32m[20221213 15:23:39 @agent_ppo2.py:185][0m |          -0.0092 |          19.2554 |           0.2782 |
[32m[20221213 15:23:39 @agent_ppo2.py:185][0m |          -0.0053 |          19.4609 |           0.2780 |
[32m[20221213 15:23:40 @agent_ppo2.py:185][0m |          -0.0127 |          19.1121 |           0.2780 |
[32m[20221213 15:23:40 @agent_ppo2.py:185][0m |          -0.0128 |          19.2832 |           0.2784 |
[32m[20221213 15:23:40 @agent_ppo2.py:185][0m |          -0.0154 |          18.9512 |           0.2778 |
[32m[20221213 15:23:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.82
[32m[20221213 15:23:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.06
[32m[20221213 15:23:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.84
[32m[20221213 15:23:40 @agent_ppo2.py:143][0m Total time:      30.74 min
[32m[20221213 15:23:40 @agent_ppo2.py:145][0m 2775040 total steps have happened
[32m[20221213 15:23:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1355 --------------------------#
[32m[20221213 15:23:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:40 @agent_ppo2.py:185][0m |          -0.0012 |          19.5213 |           0.2872 |
[32m[20221213 15:23:40 @agent_ppo2.py:185][0m |          -0.0073 |          19.1634 |           0.2864 |
[32m[20221213 15:23:40 @agent_ppo2.py:185][0m |          -0.0095 |          19.0012 |           0.2866 |
[32m[20221213 15:23:41 @agent_ppo2.py:185][0m |          -0.0103 |          18.9090 |           0.2865 |
[32m[20221213 15:23:41 @agent_ppo2.py:185][0m |          -0.0118 |          18.8278 |           0.2863 |
[32m[20221213 15:23:41 @agent_ppo2.py:185][0m |          -0.0122 |          18.8104 |           0.2864 |
[32m[20221213 15:23:41 @agent_ppo2.py:185][0m |          -0.0118 |          18.8555 |           0.2863 |
[32m[20221213 15:23:41 @agent_ppo2.py:185][0m |          -0.0134 |          18.7211 |           0.2864 |
[32m[20221213 15:23:41 @agent_ppo2.py:185][0m |          -0.0090 |          19.1068 |           0.2865 |
[32m[20221213 15:23:41 @agent_ppo2.py:185][0m |          -0.0146 |          18.6343 |           0.2863 |
[32m[20221213 15:23:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.33
[32m[20221213 15:23:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.55
[32m[20221213 15:23:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.00
[32m[20221213 15:23:41 @agent_ppo2.py:143][0m Total time:      30.77 min
[32m[20221213 15:23:41 @agent_ppo2.py:145][0m 2777088 total steps have happened
[32m[20221213 15:23:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1356 --------------------------#
[32m[20221213 15:23:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |           0.0124 |          22.7510 |           0.2811 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0065 |          19.8688 |           0.2797 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0064 |          19.7775 |           0.2799 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0101 |          19.6101 |           0.2798 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0113 |          19.5426 |           0.2793 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0127 |          19.5309 |           0.2793 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0119 |          19.4664 |           0.2791 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0077 |          19.8244 |           0.2791 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0091 |          19.5556 |           0.2789 |
[32m[20221213 15:23:42 @agent_ppo2.py:185][0m |          -0.0150 |          19.2841 |           0.2785 |
[32m[20221213 15:23:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.81
[32m[20221213 15:23:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.99
[32m[20221213 15:23:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.15
[32m[20221213 15:23:43 @agent_ppo2.py:143][0m Total time:      30.79 min
[32m[20221213 15:23:43 @agent_ppo2.py:145][0m 2779136 total steps have happened
[32m[20221213 15:23:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1357 --------------------------#
[32m[20221213 15:23:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:43 @agent_ppo2.py:185][0m |          -0.0013 |          21.1039 |           0.2832 |
[32m[20221213 15:23:43 @agent_ppo2.py:185][0m |          -0.0075 |          20.7682 |           0.2826 |
[32m[20221213 15:23:43 @agent_ppo2.py:185][0m |          -0.0093 |          20.5602 |           0.2828 |
[32m[20221213 15:23:43 @agent_ppo2.py:185][0m |          -0.0133 |          20.4125 |           0.2825 |
[32m[20221213 15:23:43 @agent_ppo2.py:185][0m |          -0.0122 |          20.3446 |           0.2826 |
[32m[20221213 15:23:43 @agent_ppo2.py:185][0m |          -0.0141 |          20.2232 |           0.2826 |
[32m[20221213 15:23:43 @agent_ppo2.py:185][0m |          -0.0155 |          20.1548 |           0.2825 |
[32m[20221213 15:23:44 @agent_ppo2.py:185][0m |          -0.0159 |          20.1143 |           0.2825 |
[32m[20221213 15:23:44 @agent_ppo2.py:185][0m |          -0.0203 |          20.0358 |           0.2827 |
[32m[20221213 15:23:44 @agent_ppo2.py:185][0m |          -0.0189 |          19.9980 |           0.2828 |
[32m[20221213 15:23:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.93
[32m[20221213 15:23:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.63
[32m[20221213 15:23:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.97
[32m[20221213 15:23:44 @agent_ppo2.py:143][0m Total time:      30.81 min
[32m[20221213 15:23:44 @agent_ppo2.py:145][0m 2781184 total steps have happened
[32m[20221213 15:23:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1358 --------------------------#
[32m[20221213 15:23:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:44 @agent_ppo2.py:185][0m |          -0.0005 |          20.0145 |           0.2784 |
[32m[20221213 15:23:44 @agent_ppo2.py:185][0m |          -0.0060 |          19.7870 |           0.2776 |
[32m[20221213 15:23:44 @agent_ppo2.py:185][0m |          -0.0106 |          19.7185 |           0.2772 |
[32m[20221213 15:23:45 @agent_ppo2.py:185][0m |          -0.0075 |          19.6050 |           0.2767 |
[32m[20221213 15:23:45 @agent_ppo2.py:185][0m |          -0.0116 |          19.6636 |           0.2767 |
[32m[20221213 15:23:45 @agent_ppo2.py:185][0m |          -0.0135 |          19.5094 |           0.2763 |
[32m[20221213 15:23:45 @agent_ppo2.py:185][0m |          -0.0147 |          19.4323 |           0.2762 |
[32m[20221213 15:23:45 @agent_ppo2.py:185][0m |          -0.0156 |          19.3919 |           0.2761 |
[32m[20221213 15:23:45 @agent_ppo2.py:185][0m |          -0.0107 |          19.5469 |           0.2757 |
[32m[20221213 15:23:45 @agent_ppo2.py:185][0m |          -0.0142 |          19.2726 |           0.2754 |
[32m[20221213 15:23:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.52
[32m[20221213 15:23:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.50
[32m[20221213 15:23:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.67
[32m[20221213 15:23:45 @agent_ppo2.py:143][0m Total time:      30.83 min
[32m[20221213 15:23:45 @agent_ppo2.py:145][0m 2783232 total steps have happened
[32m[20221213 15:23:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1359 --------------------------#
[32m[20221213 15:23:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0028 |          19.0872 |           0.2789 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0040 |          18.9548 |           0.2781 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0111 |          18.0507 |           0.2778 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0085 |          18.1306 |           0.2781 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0080 |          18.3494 |           0.2778 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0105 |          17.8568 |           0.2777 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0091 |          17.5513 |           0.2776 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0179 |          17.2528 |           0.2777 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0121 |          17.6442 |           0.2779 |
[32m[20221213 15:23:46 @agent_ppo2.py:185][0m |          -0.0073 |          18.8741 |           0.2774 |
[32m[20221213 15:23:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.42
[32m[20221213 15:23:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.59
[32m[20221213 15:23:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.13
[32m[20221213 15:23:47 @agent_ppo2.py:143][0m Total time:      30.86 min
[32m[20221213 15:23:47 @agent_ppo2.py:145][0m 2785280 total steps have happened
[32m[20221213 15:23:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1360 --------------------------#
[32m[20221213 15:23:47 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:23:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:47 @agent_ppo2.py:185][0m |          -0.0029 |          20.1672 |           0.2742 |
[32m[20221213 15:23:47 @agent_ppo2.py:185][0m |          -0.0081 |          19.6340 |           0.2738 |
[32m[20221213 15:23:47 @agent_ppo2.py:185][0m |          -0.0041 |          19.9139 |           0.2732 |
[32m[20221213 15:23:47 @agent_ppo2.py:185][0m |          -0.0007 |          20.3268 |           0.2731 |
[32m[20221213 15:23:47 @agent_ppo2.py:185][0m |          -0.0146 |          18.9633 |           0.2726 |
[32m[20221213 15:23:47 @agent_ppo2.py:185][0m |          -0.0143 |          18.7733 |           0.2726 |
[32m[20221213 15:23:47 @agent_ppo2.py:185][0m |          -0.0127 |          18.8408 |           0.2721 |
[32m[20221213 15:23:48 @agent_ppo2.py:185][0m |          -0.0151 |          18.5158 |           0.2718 |
[32m[20221213 15:23:48 @agent_ppo2.py:185][0m |          -0.0185 |          18.3348 |           0.2718 |
[32m[20221213 15:23:48 @agent_ppo2.py:185][0m |          -0.0187 |          18.2821 |           0.2714 |
[32m[20221213 15:23:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.80
[32m[20221213 15:23:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.48
[32m[20221213 15:23:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.44
[32m[20221213 15:23:48 @agent_ppo2.py:143][0m Total time:      30.88 min
[32m[20221213 15:23:48 @agent_ppo2.py:145][0m 2787328 total steps have happened
[32m[20221213 15:23:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1361 --------------------------#
[32m[20221213 15:23:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:48 @agent_ppo2.py:185][0m |          -0.0020 |          20.6804 |           0.2822 |
[32m[20221213 15:23:48 @agent_ppo2.py:185][0m |          -0.0035 |          20.2338 |           0.2822 |
[32m[20221213 15:23:48 @agent_ppo2.py:185][0m |          -0.0083 |          20.1062 |           0.2823 |
[32m[20221213 15:23:49 @agent_ppo2.py:185][0m |          -0.0090 |          20.0234 |           0.2824 |
[32m[20221213 15:23:49 @agent_ppo2.py:185][0m |          -0.0131 |          19.9447 |           0.2827 |
[32m[20221213 15:23:49 @agent_ppo2.py:185][0m |          -0.0137 |          19.9383 |           0.2826 |
[32m[20221213 15:23:49 @agent_ppo2.py:185][0m |          -0.0049 |          21.8460 |           0.2825 |
[32m[20221213 15:23:49 @agent_ppo2.py:185][0m |          -0.0093 |          20.2725 |           0.2826 |
[32m[20221213 15:23:49 @agent_ppo2.py:185][0m |          -0.0166 |          19.7734 |           0.2829 |
[32m[20221213 15:23:49 @agent_ppo2.py:185][0m |          -0.0161 |          19.7346 |           0.2827 |
[32m[20221213 15:23:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.75
[32m[20221213 15:23:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.71
[32m[20221213 15:23:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.60
[32m[20221213 15:23:49 @agent_ppo2.py:143][0m Total time:      30.90 min
[32m[20221213 15:23:49 @agent_ppo2.py:145][0m 2789376 total steps have happened
[32m[20221213 15:23:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1362 --------------------------#
[32m[20221213 15:23:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0024 |          19.9748 |           0.2807 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0096 |          19.7203 |           0.2801 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0119 |          19.5952 |           0.2798 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0128 |          19.4628 |           0.2795 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0065 |          19.8757 |           0.2793 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0159 |          19.2576 |           0.2793 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0081 |          19.9952 |           0.2790 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0145 |          19.1197 |           0.2783 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0188 |          18.9520 |           0.2784 |
[32m[20221213 15:23:50 @agent_ppo2.py:185][0m |          -0.0156 |          19.0309 |           0.2784 |
[32m[20221213 15:23:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.63
[32m[20221213 15:23:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.08
[32m[20221213 15:23:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.15
[32m[20221213 15:23:51 @agent_ppo2.py:143][0m Total time:      30.92 min
[32m[20221213 15:23:51 @agent_ppo2.py:145][0m 2791424 total steps have happened
[32m[20221213 15:23:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1363 --------------------------#
[32m[20221213 15:23:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:51 @agent_ppo2.py:185][0m |          -0.0010 |          21.2113 |           0.2798 |
[32m[20221213 15:23:51 @agent_ppo2.py:185][0m |          -0.0010 |          21.6816 |           0.2797 |
[32m[20221213 15:23:51 @agent_ppo2.py:185][0m |          -0.0119 |          20.4989 |           0.2795 |
[32m[20221213 15:23:51 @agent_ppo2.py:185][0m |          -0.0126 |          20.3173 |           0.2797 |
[32m[20221213 15:23:51 @agent_ppo2.py:185][0m |          -0.0093 |          20.3327 |           0.2797 |
[32m[20221213 15:23:51 @agent_ppo2.py:185][0m |          -0.0139 |          20.0691 |           0.2799 |
[32m[20221213 15:23:52 @agent_ppo2.py:185][0m |          -0.0149 |          19.9542 |           0.2797 |
[32m[20221213 15:23:52 @agent_ppo2.py:185][0m |          -0.0176 |          19.8910 |           0.2798 |
[32m[20221213 15:23:52 @agent_ppo2.py:185][0m |          -0.0179 |          19.8299 |           0.2799 |
[32m[20221213 15:23:52 @agent_ppo2.py:185][0m |          -0.0083 |          21.1038 |           0.2800 |
[32m[20221213 15:23:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.68
[32m[20221213 15:23:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.02
[32m[20221213 15:23:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.97
[32m[20221213 15:23:52 @agent_ppo2.py:143][0m Total time:      30.95 min
[32m[20221213 15:23:52 @agent_ppo2.py:145][0m 2793472 total steps have happened
[32m[20221213 15:23:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1364 --------------------------#
[32m[20221213 15:23:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:52 @agent_ppo2.py:185][0m |           0.0009 |          18.8314 |           0.2746 |
[32m[20221213 15:23:52 @agent_ppo2.py:185][0m |          -0.0056 |          18.2216 |           0.2747 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |          -0.0065 |          17.8875 |           0.2745 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |          -0.0090 |          17.5124 |           0.2743 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |          -0.0107 |          17.2615 |           0.2743 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |           0.0023 |          18.3113 |           0.2741 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |          -0.0128 |          17.1051 |           0.2743 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |          -0.0153 |          16.7507 |           0.2742 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |          -0.0154 |          16.6256 |           0.2742 |
[32m[20221213 15:23:53 @agent_ppo2.py:185][0m |          -0.0103 |          16.9474 |           0.2743 |
[32m[20221213 15:23:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.49
[32m[20221213 15:23:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.97
[32m[20221213 15:23:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.34
[32m[20221213 15:23:53 @agent_ppo2.py:143][0m Total time:      30.97 min
[32m[20221213 15:23:53 @agent_ppo2.py:145][0m 2795520 total steps have happened
[32m[20221213 15:23:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1365 --------------------------#
[32m[20221213 15:23:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0004 |          21.5188 |           0.2764 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |           0.0042 |          22.0520 |           0.2759 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0103 |          20.7452 |           0.2754 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0122 |          20.5728 |           0.2760 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0141 |          20.4671 |           0.2760 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0101 |          20.7573 |           0.2762 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0171 |          20.2570 |           0.2759 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0161 |          20.1686 |           0.2761 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0162 |          20.0928 |           0.2761 |
[32m[20221213 15:23:54 @agent_ppo2.py:185][0m |          -0.0180 |          20.0159 |           0.2758 |
[32m[20221213 15:23:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.40
[32m[20221213 15:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.76
[32m[20221213 15:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.12
[32m[20221213 15:23:55 @agent_ppo2.py:143][0m Total time:      30.99 min
[32m[20221213 15:23:55 @agent_ppo2.py:145][0m 2797568 total steps have happened
[32m[20221213 15:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1366 --------------------------#
[32m[20221213 15:23:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:55 @agent_ppo2.py:185][0m |          -0.0012 |          19.8022 |           0.2809 |
[32m[20221213 15:23:55 @agent_ppo2.py:185][0m |          -0.0043 |          19.6206 |           0.2798 |
[32m[20221213 15:23:55 @agent_ppo2.py:185][0m |          -0.0065 |          19.5342 |           0.2797 |
[32m[20221213 15:23:55 @agent_ppo2.py:185][0m |          -0.0060 |          19.6119 |           0.2797 |
[32m[20221213 15:23:55 @agent_ppo2.py:185][0m |          -0.0034 |          20.0459 |           0.2796 |
[32m[20221213 15:23:55 @agent_ppo2.py:185][0m |          -0.0133 |          19.3476 |           0.2797 |
[32m[20221213 15:23:56 @agent_ppo2.py:185][0m |          -0.0103 |          19.3282 |           0.2800 |
[32m[20221213 15:23:56 @agent_ppo2.py:185][0m |          -0.0136 |          19.2827 |           0.2801 |
[32m[20221213 15:23:56 @agent_ppo2.py:185][0m |          -0.0115 |          19.2755 |           0.2796 |
[32m[20221213 15:23:56 @agent_ppo2.py:185][0m |          -0.0077 |          19.7986 |           0.2800 |
[32m[20221213 15:23:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:23:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.64
[32m[20221213 15:23:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.11
[32m[20221213 15:23:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.18
[32m[20221213 15:23:56 @agent_ppo2.py:143][0m Total time:      31.01 min
[32m[20221213 15:23:56 @agent_ppo2.py:145][0m 2799616 total steps have happened
[32m[20221213 15:23:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1367 --------------------------#
[32m[20221213 15:23:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:23:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:56 @agent_ppo2.py:185][0m |          -0.0021 |          19.9243 |           0.2759 |
[32m[20221213 15:23:56 @agent_ppo2.py:185][0m |          -0.0044 |          19.9541 |           0.2760 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0136 |          19.4232 |           0.2758 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0123 |          19.2778 |           0.2755 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0140 |          19.1708 |           0.2755 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0112 |          19.1368 |           0.2756 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0155 |          18.9759 |           0.2754 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0162 |          18.8486 |           0.2755 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0158 |          18.7838 |           0.2754 |
[32m[20221213 15:23:57 @agent_ppo2.py:185][0m |          -0.0107 |          18.9652 |           0.2756 |
[32m[20221213 15:23:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:23:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.96
[32m[20221213 15:23:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.31
[32m[20221213 15:23:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.50
[32m[20221213 15:23:57 @agent_ppo2.py:143][0m Total time:      31.04 min
[32m[20221213 15:23:57 @agent_ppo2.py:145][0m 2801664 total steps have happened
[32m[20221213 15:23:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1368 --------------------------#
[32m[20221213 15:23:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |           0.0006 |          19.3291 |           0.2842 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |          -0.0055 |          18.7442 |           0.2842 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |          -0.0072 |          18.5598 |           0.2839 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |           0.0001 |          19.0735 |           0.2840 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |          -0.0105 |          18.3905 |           0.2842 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |          -0.0111 |          18.2963 |           0.2844 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |           0.0011 |          20.9642 |           0.2841 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |          -0.0148 |          18.2758 |           0.2845 |
[32m[20221213 15:23:58 @agent_ppo2.py:185][0m |          -0.0135 |          18.1020 |           0.2842 |
[32m[20221213 15:23:59 @agent_ppo2.py:185][0m |          -0.0043 |          19.1473 |           0.2845 |
[32m[20221213 15:23:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.03
[32m[20221213 15:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.32
[32m[20221213 15:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.00
[32m[20221213 15:23:59 @agent_ppo2.py:143][0m Total time:      31.06 min
[32m[20221213 15:23:59 @agent_ppo2.py:145][0m 2803712 total steps have happened
[32m[20221213 15:23:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1369 --------------------------#
[32m[20221213 15:23:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:23:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:23:59 @agent_ppo2.py:185][0m |          -0.0008 |          19.7519 |           0.2873 |
[32m[20221213 15:23:59 @agent_ppo2.py:185][0m |          -0.0084 |          19.2316 |           0.2875 |
[32m[20221213 15:23:59 @agent_ppo2.py:185][0m |          -0.0095 |          18.9887 |           0.2876 |
[32m[20221213 15:23:59 @agent_ppo2.py:185][0m |          -0.0135 |          18.7754 |           0.2878 |
[32m[20221213 15:23:59 @agent_ppo2.py:185][0m |          -0.0123 |          18.7068 |           0.2877 |
[32m[20221213 15:24:00 @agent_ppo2.py:185][0m |           0.0040 |          20.7205 |           0.2877 |
[32m[20221213 15:24:00 @agent_ppo2.py:185][0m |          -0.0112 |          18.5096 |           0.2873 |
[32m[20221213 15:24:00 @agent_ppo2.py:185][0m |          -0.0164 |          18.4006 |           0.2878 |
[32m[20221213 15:24:00 @agent_ppo2.py:185][0m |          -0.0158 |          18.3816 |           0.2879 |
[32m[20221213 15:24:00 @agent_ppo2.py:185][0m |          -0.0175 |          18.2924 |           0.2880 |
[32m[20221213 15:24:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.51
[32m[20221213 15:24:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.12
[32m[20221213 15:24:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.47
[32m[20221213 15:24:00 @agent_ppo2.py:143][0m Total time:      31.08 min
[32m[20221213 15:24:00 @agent_ppo2.py:145][0m 2805760 total steps have happened
[32m[20221213 15:24:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1370 --------------------------#
[32m[20221213 15:24:00 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:24:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:00 @agent_ppo2.py:185][0m |          -0.0060 |          20.1015 |           0.2845 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0130 |          19.7999 |           0.2836 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0129 |          19.5625 |           0.2834 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0152 |          19.4059 |           0.2833 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0149 |          19.3598 |           0.2831 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0182 |          19.1286 |           0.2834 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0182 |          19.0871 |           0.2832 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0185 |          18.9807 |           0.2831 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0172 |          18.9148 |           0.2831 |
[32m[20221213 15:24:01 @agent_ppo2.py:185][0m |          -0.0210 |          18.8390 |           0.2828 |
[32m[20221213 15:24:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.35
[32m[20221213 15:24:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.80
[32m[20221213 15:24:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.15
[32m[20221213 15:24:01 @agent_ppo2.py:143][0m Total time:      31.10 min
[32m[20221213 15:24:01 @agent_ppo2.py:145][0m 2807808 total steps have happened
[32m[20221213 15:24:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1371 --------------------------#
[32m[20221213 15:24:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0017 |          21.9495 |           0.2795 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0106 |          21.5930 |           0.2786 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0145 |          21.4368 |           0.2783 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0061 |          22.0696 |           0.2783 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0149 |          21.2465 |           0.2782 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0156 |          21.2057 |           0.2780 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0209 |          21.1140 |           0.2779 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0192 |          21.0192 |           0.2777 |
[32m[20221213 15:24:02 @agent_ppo2.py:185][0m |          -0.0101 |          22.0789 |           0.2779 |
[32m[20221213 15:24:03 @agent_ppo2.py:185][0m |          -0.0162 |          21.0758 |           0.2776 |
[32m[20221213 15:24:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.23
[32m[20221213 15:24:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.87
[32m[20221213 15:24:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.17
[32m[20221213 15:24:03 @agent_ppo2.py:143][0m Total time:      31.13 min
[32m[20221213 15:24:03 @agent_ppo2.py:145][0m 2809856 total steps have happened
[32m[20221213 15:24:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1372 --------------------------#
[32m[20221213 15:24:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:03 @agent_ppo2.py:185][0m |           0.0059 |          22.0572 |           0.2855 |
[32m[20221213 15:24:03 @agent_ppo2.py:185][0m |          -0.0076 |          20.5157 |           0.2854 |
[32m[20221213 15:24:03 @agent_ppo2.py:185][0m |          -0.0116 |          20.1505 |           0.2854 |
[32m[20221213 15:24:03 @agent_ppo2.py:185][0m |          -0.0132 |          19.9017 |           0.2851 |
[32m[20221213 15:24:03 @agent_ppo2.py:185][0m |          -0.0099 |          19.9406 |           0.2854 |
[32m[20221213 15:24:04 @agent_ppo2.py:185][0m |          -0.0148 |          19.5198 |           0.2852 |
[32m[20221213 15:24:04 @agent_ppo2.py:185][0m |          -0.0182 |          19.4536 |           0.2851 |
[32m[20221213 15:24:04 @agent_ppo2.py:185][0m |          -0.0170 |          19.2523 |           0.2850 |
[32m[20221213 15:24:04 @agent_ppo2.py:185][0m |          -0.0205 |          19.1036 |           0.2852 |
[32m[20221213 15:24:04 @agent_ppo2.py:185][0m |          -0.0187 |          19.0396 |           0.2850 |
[32m[20221213 15:24:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:24:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.11
[32m[20221213 15:24:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.89
[32m[20221213 15:24:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.34
[32m[20221213 15:24:04 @agent_ppo2.py:143][0m Total time:      31.15 min
[32m[20221213 15:24:04 @agent_ppo2.py:145][0m 2811904 total steps have happened
[32m[20221213 15:24:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1373 --------------------------#
[32m[20221213 15:24:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:04 @agent_ppo2.py:185][0m |          -0.0022 |          22.0281 |           0.2823 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0052 |          21.5654 |           0.2815 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0109 |          21.1538 |           0.2816 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0109 |          20.9880 |           0.2816 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0145 |          20.8874 |           0.2815 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0144 |          20.7648 |           0.2815 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0156 |          20.7131 |           0.2816 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0160 |          20.5922 |           0.2816 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0171 |          20.5138 |           0.2818 |
[32m[20221213 15:24:05 @agent_ppo2.py:185][0m |          -0.0174 |          20.4645 |           0.2818 |
[32m[20221213 15:24:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:24:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.84
[32m[20221213 15:24:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.32
[32m[20221213 15:24:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.36
[32m[20221213 15:24:06 @agent_ppo2.py:143][0m Total time:      31.17 min
[32m[20221213 15:24:06 @agent_ppo2.py:145][0m 2813952 total steps have happened
[32m[20221213 15:24:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1374 --------------------------#
[32m[20221213 15:24:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:06 @agent_ppo2.py:185][0m |          -0.0027 |          20.3805 |           0.2953 |
[32m[20221213 15:24:06 @agent_ppo2.py:185][0m |          -0.0077 |          19.8160 |           0.2945 |
[32m[20221213 15:24:06 @agent_ppo2.py:185][0m |          -0.0090 |          19.6367 |           0.2945 |
[32m[20221213 15:24:06 @agent_ppo2.py:185][0m |          -0.0074 |          19.9146 |           0.2945 |
[32m[20221213 15:24:06 @agent_ppo2.py:185][0m |          -0.0147 |          19.4565 |           0.2943 |
[32m[20221213 15:24:06 @agent_ppo2.py:185][0m |          -0.0126 |          19.2856 |           0.2937 |
[32m[20221213 15:24:06 @agent_ppo2.py:185][0m |          -0.0102 |          19.3549 |           0.2938 |
[32m[20221213 15:24:07 @agent_ppo2.py:185][0m |          -0.0001 |          21.3697 |           0.2939 |
[32m[20221213 15:24:07 @agent_ppo2.py:185][0m |          -0.0162 |          19.2498 |           0.2934 |
[32m[20221213 15:24:07 @agent_ppo2.py:185][0m |          -0.0164 |          19.0919 |           0.2934 |
[32m[20221213 15:24:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:24:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.40
[32m[20221213 15:24:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.55
[32m[20221213 15:24:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.99
[32m[20221213 15:24:07 @agent_ppo2.py:143][0m Total time:      31.20 min
[32m[20221213 15:24:07 @agent_ppo2.py:145][0m 2816000 total steps have happened
[32m[20221213 15:24:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1375 --------------------------#
[32m[20221213 15:24:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:07 @agent_ppo2.py:185][0m |          -0.0001 |          21.5186 |           0.2812 |
[32m[20221213 15:24:07 @agent_ppo2.py:185][0m |          -0.0057 |          21.1724 |           0.2807 |
[32m[20221213 15:24:07 @agent_ppo2.py:185][0m |          -0.0075 |          21.0430 |           0.2801 |
[32m[20221213 15:24:08 @agent_ppo2.py:185][0m |          -0.0090 |          20.9137 |           0.2801 |
[32m[20221213 15:24:08 @agent_ppo2.py:185][0m |          -0.0116 |          20.8865 |           0.2800 |
[32m[20221213 15:24:08 @agent_ppo2.py:185][0m |          -0.0002 |          21.7695 |           0.2799 |
[32m[20221213 15:24:08 @agent_ppo2.py:185][0m |          -0.0103 |          20.7356 |           0.2800 |
[32m[20221213 15:24:08 @agent_ppo2.py:185][0m |          -0.0097 |          20.7755 |           0.2800 |
[32m[20221213 15:24:08 @agent_ppo2.py:185][0m |          -0.0084 |          20.8535 |           0.2799 |
[32m[20221213 15:24:08 @agent_ppo2.py:185][0m |          -0.0092 |          20.7188 |           0.2801 |
[32m[20221213 15:24:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.04
[32m[20221213 15:24:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.43
[32m[20221213 15:24:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.76
[32m[20221213 15:24:08 @agent_ppo2.py:143][0m Total time:      31.22 min
[32m[20221213 15:24:08 @agent_ppo2.py:145][0m 2818048 total steps have happened
[32m[20221213 15:24:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1376 --------------------------#
[32m[20221213 15:24:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0027 |          21.1672 |           0.2831 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |           0.0174 |          24.3266 |           0.2825 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0052 |          20.8803 |           0.2822 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0132 |          20.4958 |           0.2823 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0121 |          20.4543 |           0.2826 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0136 |          20.3602 |           0.2824 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0126 |          20.3147 |           0.2823 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0140 |          20.2747 |           0.2824 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0155 |          20.2843 |           0.2825 |
[32m[20221213 15:24:09 @agent_ppo2.py:185][0m |          -0.0156 |          20.2455 |           0.2825 |
[32m[20221213 15:24:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 239.97
[32m[20221213 15:24:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.19
[32m[20221213 15:24:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.06
[32m[20221213 15:24:10 @agent_ppo2.py:143][0m Total time:      31.24 min
[32m[20221213 15:24:10 @agent_ppo2.py:145][0m 2820096 total steps have happened
[32m[20221213 15:24:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1377 --------------------------#
[32m[20221213 15:24:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:10 @agent_ppo2.py:185][0m |           0.0042 |          20.3200 |           0.2868 |
[32m[20221213 15:24:10 @agent_ppo2.py:185][0m |          -0.0070 |          19.8228 |           0.2859 |
[32m[20221213 15:24:10 @agent_ppo2.py:185][0m |          -0.0081 |          19.7288 |           0.2856 |
[32m[20221213 15:24:10 @agent_ppo2.py:185][0m |          -0.0105 |          19.6551 |           0.2849 |
[32m[20221213 15:24:10 @agent_ppo2.py:185][0m |          -0.0096 |          19.5231 |           0.2845 |
[32m[20221213 15:24:10 @agent_ppo2.py:185][0m |          -0.0130 |          19.4753 |           0.2844 |
[32m[20221213 15:24:11 @agent_ppo2.py:185][0m |          -0.0074 |          19.9582 |           0.2843 |
[32m[20221213 15:24:11 @agent_ppo2.py:185][0m |          -0.0166 |          19.3234 |           0.2839 |
[32m[20221213 15:24:11 @agent_ppo2.py:185][0m |           0.0200 |          24.7162 |           0.2839 |
[32m[20221213 15:24:11 @agent_ppo2.py:185][0m |          -0.0019 |          21.6156 |           0.2832 |
[32m[20221213 15:24:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.15
[32m[20221213 15:24:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.93
[32m[20221213 15:24:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.36
[32m[20221213 15:24:11 @agent_ppo2.py:143][0m Total time:      31.26 min
[32m[20221213 15:24:11 @agent_ppo2.py:145][0m 2822144 total steps have happened
[32m[20221213 15:24:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1378 --------------------------#
[32m[20221213 15:24:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:11 @agent_ppo2.py:185][0m |          -0.0048 |          20.1869 |           0.2771 |
[32m[20221213 15:24:11 @agent_ppo2.py:185][0m |          -0.0089 |          19.7669 |           0.2768 |
[32m[20221213 15:24:11 @agent_ppo2.py:185][0m |          -0.0109 |          19.5392 |           0.2767 |
[32m[20221213 15:24:12 @agent_ppo2.py:185][0m |          -0.0090 |          19.4339 |           0.2767 |
[32m[20221213 15:24:12 @agent_ppo2.py:185][0m |          -0.0140 |          19.2705 |           0.2770 |
[32m[20221213 15:24:12 @agent_ppo2.py:185][0m |          -0.0138 |          19.1197 |           0.2769 |
[32m[20221213 15:24:12 @agent_ppo2.py:185][0m |          -0.0177 |          18.9998 |           0.2770 |
[32m[20221213 15:24:12 @agent_ppo2.py:185][0m |          -0.0161 |          18.9388 |           0.2770 |
[32m[20221213 15:24:12 @agent_ppo2.py:185][0m |          -0.0189 |          18.8019 |           0.2769 |
[32m[20221213 15:24:12 @agent_ppo2.py:185][0m |          -0.0200 |          18.7113 |           0.2772 |
[32m[20221213 15:24:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.52
[32m[20221213 15:24:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.35
[32m[20221213 15:24:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.18
[32m[20221213 15:24:12 @agent_ppo2.py:143][0m Total time:      31.28 min
[32m[20221213 15:24:12 @agent_ppo2.py:145][0m 2824192 total steps have happened
[32m[20221213 15:24:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1379 --------------------------#
[32m[20221213 15:24:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |           0.0015 |          20.7100 |           0.2757 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0020 |          20.5789 |           0.2752 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0054 |          20.2274 |           0.2748 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0059 |          20.4720 |           0.2753 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0125 |          19.9090 |           0.2751 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0137 |          19.8617 |           0.2749 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0094 |          20.1138 |           0.2749 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0157 |          19.7336 |           0.2749 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |           0.0040 |          23.5494 |           0.2750 |
[32m[20221213 15:24:13 @agent_ppo2.py:185][0m |          -0.0199 |          19.7693 |           0.2748 |
[32m[20221213 15:24:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:24:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.42
[32m[20221213 15:24:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.74
[32m[20221213 15:24:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.99
[32m[20221213 15:24:14 @agent_ppo2.py:143][0m Total time:      31.31 min
[32m[20221213 15:24:14 @agent_ppo2.py:145][0m 2826240 total steps have happened
[32m[20221213 15:24:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1380 --------------------------#
[32m[20221213 15:24:14 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:24:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:14 @agent_ppo2.py:185][0m |           0.0042 |          20.7115 |           0.2843 |
[32m[20221213 15:24:14 @agent_ppo2.py:185][0m |          -0.0066 |          20.2855 |           0.2830 |
[32m[20221213 15:24:14 @agent_ppo2.py:185][0m |          -0.0109 |          20.1207 |           0.2826 |
[32m[20221213 15:24:14 @agent_ppo2.py:185][0m |          -0.0107 |          19.9508 |           0.2825 |
[32m[20221213 15:24:14 @agent_ppo2.py:185][0m |          -0.0119 |          19.8926 |           0.2824 |
[32m[20221213 15:24:14 @agent_ppo2.py:185][0m |          -0.0140 |          19.7504 |           0.2824 |
[32m[20221213 15:24:15 @agent_ppo2.py:185][0m |          -0.0012 |          21.7404 |           0.2826 |
[32m[20221213 15:24:15 @agent_ppo2.py:185][0m |           0.0047 |          21.8240 |           0.2817 |
[32m[20221213 15:24:15 @agent_ppo2.py:185][0m |          -0.0142 |          19.5860 |           0.2824 |
[32m[20221213 15:24:15 @agent_ppo2.py:185][0m |          -0.0110 |          19.8617 |           0.2822 |
[32m[20221213 15:24:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:24:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.50
[32m[20221213 15:24:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.66
[32m[20221213 15:24:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.78
[32m[20221213 15:24:15 @agent_ppo2.py:143][0m Total time:      31.33 min
[32m[20221213 15:24:15 @agent_ppo2.py:145][0m 2828288 total steps have happened
[32m[20221213 15:24:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1381 --------------------------#
[32m[20221213 15:24:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:15 @agent_ppo2.py:185][0m |          -0.0021 |          20.7018 |           0.2828 |
[32m[20221213 15:24:15 @agent_ppo2.py:185][0m |          -0.0074 |          20.2867 |           0.2825 |
[32m[20221213 15:24:15 @agent_ppo2.py:185][0m |          -0.0110 |          20.0125 |           0.2823 |
[32m[20221213 15:24:16 @agent_ppo2.py:185][0m |          -0.0056 |          21.5064 |           0.2824 |
[32m[20221213 15:24:16 @agent_ppo2.py:185][0m |          -0.0099 |          19.8274 |           0.2822 |
[32m[20221213 15:24:16 @agent_ppo2.py:185][0m |          -0.0143 |          19.3791 |           0.2820 |
[32m[20221213 15:24:16 @agent_ppo2.py:185][0m |          -0.0153 |          19.2590 |           0.2819 |
[32m[20221213 15:24:16 @agent_ppo2.py:185][0m |          -0.0182 |          19.1278 |           0.2821 |
[32m[20221213 15:24:16 @agent_ppo2.py:185][0m |          -0.0190 |          19.0372 |           0.2819 |
[32m[20221213 15:24:16 @agent_ppo2.py:185][0m |          -0.0181 |          18.8970 |           0.2822 |
[32m[20221213 15:24:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.54
[32m[20221213 15:24:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.52
[32m[20221213 15:24:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.04
[32m[20221213 15:24:16 @agent_ppo2.py:143][0m Total time:      31.35 min
[32m[20221213 15:24:16 @agent_ppo2.py:145][0m 2830336 total steps have happened
[32m[20221213 15:24:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1382 --------------------------#
[32m[20221213 15:24:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |           0.0004 |          21.7546 |           0.2888 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0029 |          21.2724 |           0.2886 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |           0.0014 |          21.7533 |           0.2885 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0095 |          21.0456 |           0.2883 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0124 |          20.8054 |           0.2887 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0151 |          20.7149 |           0.2889 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0094 |          21.0738 |           0.2889 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0156 |          20.5037 |           0.2888 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0153 |          20.3656 |           0.2892 |
[32m[20221213 15:24:17 @agent_ppo2.py:185][0m |          -0.0142 |          20.3300 |           0.2890 |
[32m[20221213 15:24:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.19
[32m[20221213 15:24:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.02
[32m[20221213 15:24:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.45
[32m[20221213 15:24:18 @agent_ppo2.py:143][0m Total time:      31.37 min
[32m[20221213 15:24:18 @agent_ppo2.py:145][0m 2832384 total steps have happened
[32m[20221213 15:24:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1383 --------------------------#
[32m[20221213 15:24:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:18 @agent_ppo2.py:185][0m |          -0.0004 |          21.7543 |           0.2920 |
[32m[20221213 15:24:18 @agent_ppo2.py:185][0m |          -0.0079 |          20.9895 |           0.2911 |
[32m[20221213 15:24:18 @agent_ppo2.py:185][0m |          -0.0096 |          20.8758 |           0.2910 |
[32m[20221213 15:24:18 @agent_ppo2.py:185][0m |          -0.0095 |          20.7166 |           0.2908 |
[32m[20221213 15:24:18 @agent_ppo2.py:185][0m |          -0.0132 |          20.6461 |           0.2909 |
[32m[20221213 15:24:18 @agent_ppo2.py:185][0m |          -0.0123 |          20.5528 |           0.2904 |
[32m[20221213 15:24:19 @agent_ppo2.py:185][0m |          -0.0164 |          20.4076 |           0.2906 |
[32m[20221213 15:24:19 @agent_ppo2.py:185][0m |          -0.0099 |          21.1072 |           0.2906 |
[32m[20221213 15:24:19 @agent_ppo2.py:185][0m |          -0.0139 |          20.3217 |           0.2905 |
[32m[20221213 15:24:19 @agent_ppo2.py:185][0m |          -0.0164 |          20.2112 |           0.2906 |
[32m[20221213 15:24:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.53
[32m[20221213 15:24:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.03
[32m[20221213 15:24:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.46
[32m[20221213 15:24:19 @agent_ppo2.py:143][0m Total time:      31.40 min
[32m[20221213 15:24:19 @agent_ppo2.py:145][0m 2834432 total steps have happened
[32m[20221213 15:24:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1384 --------------------------#
[32m[20221213 15:24:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:19 @agent_ppo2.py:185][0m |          -0.0002 |          20.9030 |           0.2848 |
[32m[20221213 15:24:19 @agent_ppo2.py:185][0m |          -0.0038 |          20.5771 |           0.2843 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0085 |          20.2864 |           0.2843 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0112 |          20.0715 |           0.2843 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0089 |          19.9946 |           0.2838 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0146 |          19.7513 |           0.2843 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0151 |          19.5769 |           0.2841 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0170 |          19.4268 |           0.2843 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0078 |          20.5410 |           0.2844 |
[32m[20221213 15:24:20 @agent_ppo2.py:185][0m |          -0.0099 |          20.3446 |           0.2842 |
[32m[20221213 15:24:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.80
[32m[20221213 15:24:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.74
[32m[20221213 15:24:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.18
[32m[20221213 15:24:20 @agent_ppo2.py:143][0m Total time:      31.42 min
[32m[20221213 15:24:20 @agent_ppo2.py:145][0m 2836480 total steps have happened
[32m[20221213 15:24:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1385 --------------------------#
[32m[20221213 15:24:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |           0.0078 |          22.1629 |           0.2864 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0081 |          20.9445 |           0.2862 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0074 |          20.8214 |           0.2862 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0110 |          20.6265 |           0.2864 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0095 |          20.5490 |           0.2863 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0133 |          20.4415 |           0.2860 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0139 |          20.3524 |           0.2861 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0035 |          22.0329 |           0.2861 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0175 |          20.3115 |           0.2857 |
[32m[20221213 15:24:21 @agent_ppo2.py:185][0m |          -0.0180 |          20.1880 |           0.2858 |
[32m[20221213 15:24:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.92
[32m[20221213 15:24:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.81
[32m[20221213 15:24:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.64
[32m[20221213 15:24:22 @agent_ppo2.py:143][0m Total time:      31.44 min
[32m[20221213 15:24:22 @agent_ppo2.py:145][0m 2838528 total steps have happened
[32m[20221213 15:24:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1386 --------------------------#
[32m[20221213 15:24:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:22 @agent_ppo2.py:185][0m |           0.0071 |          21.1343 |           0.2879 |
[32m[20221213 15:24:22 @agent_ppo2.py:185][0m |          -0.0052 |          20.5821 |           0.2875 |
[32m[20221213 15:24:22 @agent_ppo2.py:185][0m |          -0.0086 |          20.3847 |           0.2873 |
[32m[20221213 15:24:22 @agent_ppo2.py:185][0m |          -0.0108 |          20.2694 |           0.2874 |
[32m[20221213 15:24:22 @agent_ppo2.py:185][0m |          -0.0107 |          20.1528 |           0.2874 |
[32m[20221213 15:24:22 @agent_ppo2.py:185][0m |          -0.0132 |          20.0651 |           0.2879 |
[32m[20221213 15:24:23 @agent_ppo2.py:185][0m |          -0.0147 |          19.9852 |           0.2876 |
[32m[20221213 15:24:23 @agent_ppo2.py:185][0m |          -0.0130 |          19.9611 |           0.2877 |
[32m[20221213 15:24:23 @agent_ppo2.py:185][0m |          -0.0113 |          20.2325 |           0.2876 |
[32m[20221213 15:24:23 @agent_ppo2.py:185][0m |          -0.0186 |          19.8150 |           0.2879 |
[32m[20221213 15:24:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.35
[32m[20221213 15:24:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.05
[32m[20221213 15:24:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.75
[32m[20221213 15:24:23 @agent_ppo2.py:143][0m Total time:      31.46 min
[32m[20221213 15:24:23 @agent_ppo2.py:145][0m 2840576 total steps have happened
[32m[20221213 15:24:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1387 --------------------------#
[32m[20221213 15:24:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:23 @agent_ppo2.py:185][0m |          -0.0023 |          21.9300 |           0.2978 |
[32m[20221213 15:24:23 @agent_ppo2.py:185][0m |          -0.0096 |          21.6292 |           0.2977 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0116 |          21.4386 |           0.2975 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0095 |          21.4952 |           0.2973 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0159 |          21.2918 |           0.2974 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0082 |          21.3037 |           0.2974 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0182 |          21.1788 |           0.2972 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0050 |          22.4031 |           0.2974 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0148 |          21.1060 |           0.2974 |
[32m[20221213 15:24:24 @agent_ppo2.py:185][0m |          -0.0049 |          21.6017 |           0.2975 |
[32m[20221213 15:24:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.52
[32m[20221213 15:24:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.45
[32m[20221213 15:24:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.19
[32m[20221213 15:24:24 @agent_ppo2.py:143][0m Total time:      31.49 min
[32m[20221213 15:24:24 @agent_ppo2.py:145][0m 2842624 total steps have happened
[32m[20221213 15:24:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1388 --------------------------#
[32m[20221213 15:24:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0025 |          21.1608 |           0.2955 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0073 |          20.8465 |           0.2956 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0080 |          20.6605 |           0.2955 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0109 |          20.4782 |           0.2956 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0124 |          20.4004 |           0.2954 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0128 |          20.3332 |           0.2955 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0154 |          20.2764 |           0.2955 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0167 |          20.1840 |           0.2955 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0161 |          20.1446 |           0.2954 |
[32m[20221213 15:24:25 @agent_ppo2.py:185][0m |          -0.0053 |          23.1083 |           0.2954 |
[32m[20221213 15:24:25 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:24:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.23
[32m[20221213 15:24:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.87
[32m[20221213 15:24:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 223.52
[32m[20221213 15:24:26 @agent_ppo2.py:143][0m Total time:      31.51 min
[32m[20221213 15:24:26 @agent_ppo2.py:145][0m 2844672 total steps have happened
[32m[20221213 15:24:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1389 --------------------------#
[32m[20221213 15:24:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:26 @agent_ppo2.py:185][0m |           0.0000 |          20.2672 |           0.2916 |
[32m[20221213 15:24:26 @agent_ppo2.py:185][0m |          -0.0090 |          19.5611 |           0.2911 |
[32m[20221213 15:24:26 @agent_ppo2.py:185][0m |          -0.0018 |          20.3344 |           0.2907 |
[32m[20221213 15:24:26 @agent_ppo2.py:185][0m |          -0.0107 |          18.7631 |           0.2898 |
[32m[20221213 15:24:26 @agent_ppo2.py:185][0m |          -0.0119 |          18.4705 |           0.2897 |
[32m[20221213 15:24:26 @agent_ppo2.py:185][0m |          -0.0140 |          18.1752 |           0.2895 |
[32m[20221213 15:24:27 @agent_ppo2.py:185][0m |          -0.0142 |          17.9641 |           0.2894 |
[32m[20221213 15:24:27 @agent_ppo2.py:185][0m |          -0.0059 |          19.6348 |           0.2893 |
[32m[20221213 15:24:27 @agent_ppo2.py:185][0m |          -0.0147 |          17.5077 |           0.2887 |
[32m[20221213 15:24:27 @agent_ppo2.py:185][0m |          -0.0001 |          20.2392 |           0.2887 |
[32m[20221213 15:24:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.89
[32m[20221213 15:24:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.89
[32m[20221213 15:24:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.81
[32m[20221213 15:24:27 @agent_ppo2.py:143][0m Total time:      31.53 min
[32m[20221213 15:24:27 @agent_ppo2.py:145][0m 2846720 total steps have happened
[32m[20221213 15:24:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1390 --------------------------#
[32m[20221213 15:24:27 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:24:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:27 @agent_ppo2.py:185][0m |           0.0004 |          22.5296 |           0.2906 |
[32m[20221213 15:24:27 @agent_ppo2.py:185][0m |           0.0036 |          22.6018 |           0.2903 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0024 |          21.9299 |           0.2902 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0109 |          21.4007 |           0.2900 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0123 |          21.2917 |           0.2907 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0108 |          21.2703 |           0.2901 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0135 |          21.2053 |           0.2901 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0156 |          21.1244 |           0.2899 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0150 |          21.0792 |           0.2895 |
[32m[20221213 15:24:28 @agent_ppo2.py:185][0m |          -0.0212 |          21.0370 |           0.2896 |
[32m[20221213 15:24:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.84
[32m[20221213 15:24:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.70
[32m[20221213 15:24:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.59
[32m[20221213 15:24:28 @agent_ppo2.py:143][0m Total time:      31.55 min
[32m[20221213 15:24:28 @agent_ppo2.py:145][0m 2848768 total steps have happened
[32m[20221213 15:24:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1391 --------------------------#
[32m[20221213 15:24:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |           0.0017 |          20.2267 |           0.2903 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0057 |          19.7935 |           0.2899 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0106 |          19.6329 |           0.2897 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0093 |          19.4866 |           0.2898 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0097 |          19.5797 |           0.2895 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0138 |          19.3549 |           0.2892 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0145 |          19.2747 |           0.2896 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0002 |          20.9563 |           0.2895 |
[32m[20221213 15:24:29 @agent_ppo2.py:185][0m |          -0.0086 |          19.7259 |           0.2894 |
[32m[20221213 15:24:30 @agent_ppo2.py:185][0m |          -0.0166 |          19.0807 |           0.2890 |
[32m[20221213 15:24:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.59
[32m[20221213 15:24:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.26
[32m[20221213 15:24:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.76
[32m[20221213 15:24:30 @agent_ppo2.py:143][0m Total time:      31.58 min
[32m[20221213 15:24:30 @agent_ppo2.py:145][0m 2850816 total steps have happened
[32m[20221213 15:24:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1392 --------------------------#
[32m[20221213 15:24:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:30 @agent_ppo2.py:185][0m |          -0.0028 |          19.9111 |           0.2872 |
[32m[20221213 15:24:30 @agent_ppo2.py:185][0m |          -0.0098 |          19.7364 |           0.2863 |
[32m[20221213 15:24:30 @agent_ppo2.py:185][0m |          -0.0078 |          20.2028 |           0.2862 |
[32m[20221213 15:24:30 @agent_ppo2.py:185][0m |          -0.0141 |          19.4869 |           0.2863 |
[32m[20221213 15:24:30 @agent_ppo2.py:185][0m |          -0.0150 |          19.3657 |           0.2865 |
[32m[20221213 15:24:31 @agent_ppo2.py:185][0m |          -0.0152 |          19.2834 |           0.2866 |
[32m[20221213 15:24:31 @agent_ppo2.py:185][0m |          -0.0163 |          19.2042 |           0.2865 |
[32m[20221213 15:24:31 @agent_ppo2.py:185][0m |          -0.0169 |          19.1636 |           0.2865 |
[32m[20221213 15:24:31 @agent_ppo2.py:185][0m |          -0.0183 |          19.1027 |           0.2865 |
[32m[20221213 15:24:31 @agent_ppo2.py:185][0m |          -0.0189 |          19.0551 |           0.2862 |
[32m[20221213 15:24:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:24:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.35
[32m[20221213 15:24:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.40
[32m[20221213 15:24:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.09
[32m[20221213 15:24:31 @agent_ppo2.py:143][0m Total time:      31.60 min
[32m[20221213 15:24:31 @agent_ppo2.py:145][0m 2852864 total steps have happened
[32m[20221213 15:24:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1393 --------------------------#
[32m[20221213 15:24:31 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:24:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:31 @agent_ppo2.py:185][0m |          -0.0015 |          19.9090 |           0.2830 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0092 |          19.3424 |           0.2825 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0112 |          19.0034 |           0.2825 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0132 |          18.7216 |           0.2825 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0117 |          18.5886 |           0.2820 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0165 |          18.2972 |           0.2822 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0158 |          18.1443 |           0.2823 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0187 |          17.9596 |           0.2823 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0174 |          17.8654 |           0.2821 |
[32m[20221213 15:24:32 @agent_ppo2.py:185][0m |          -0.0149 |          18.0810 |           0.2818 |
[32m[20221213 15:24:32 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 15:24:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.14
[32m[20221213 15:24:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.80
[32m[20221213 15:24:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.50
[32m[20221213 15:24:33 @agent_ppo2.py:143][0m Total time:      31.62 min
[32m[20221213 15:24:33 @agent_ppo2.py:145][0m 2854912 total steps have happened
[32m[20221213 15:24:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1394 --------------------------#
[32m[20221213 15:24:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:33 @agent_ppo2.py:185][0m |           0.0024 |          21.5872 |           0.2872 |
[32m[20221213 15:24:33 @agent_ppo2.py:185][0m |          -0.0053 |          21.0012 |           0.2870 |
[32m[20221213 15:24:33 @agent_ppo2.py:185][0m |          -0.0072 |          20.8253 |           0.2871 |
[32m[20221213 15:24:33 @agent_ppo2.py:185][0m |          -0.0040 |          21.2451 |           0.2868 |
[32m[20221213 15:24:33 @agent_ppo2.py:185][0m |          -0.0130 |          20.6777 |           0.2869 |
[32m[20221213 15:24:33 @agent_ppo2.py:185][0m |          -0.0150 |          20.5801 |           0.2864 |
[32m[20221213 15:24:34 @agent_ppo2.py:185][0m |          -0.0147 |          20.5063 |           0.2860 |
[32m[20221213 15:24:34 @agent_ppo2.py:185][0m |          -0.0147 |          20.4722 |           0.2859 |
[32m[20221213 15:24:34 @agent_ppo2.py:185][0m |          -0.0163 |          20.4158 |           0.2860 |
[32m[20221213 15:24:34 @agent_ppo2.py:185][0m |          -0.0123 |          20.6052 |           0.2858 |
[32m[20221213 15:24:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:24:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.06
[32m[20221213 15:24:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.89
[32m[20221213 15:24:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.09
[32m[20221213 15:24:34 @agent_ppo2.py:143][0m Total time:      31.65 min
[32m[20221213 15:24:34 @agent_ppo2.py:145][0m 2856960 total steps have happened
[32m[20221213 15:24:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1395 --------------------------#
[32m[20221213 15:24:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:34 @agent_ppo2.py:185][0m |          -0.0017 |          21.3386 |           0.3012 |
[32m[20221213 15:24:34 @agent_ppo2.py:185][0m |          -0.0100 |          21.1007 |           0.3002 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0058 |          21.4430 |           0.2999 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0114 |          20.8742 |           0.2991 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0135 |          20.7921 |           0.2990 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0118 |          20.9437 |           0.2987 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0155 |          20.6900 |           0.2986 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0148 |          20.6699 |           0.2984 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0057 |          23.0064 |           0.2984 |
[32m[20221213 15:24:35 @agent_ppo2.py:185][0m |          -0.0108 |          21.1096 |           0.2982 |
[32m[20221213 15:24:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:24:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.01
[32m[20221213 15:24:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.51
[32m[20221213 15:24:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.70
[32m[20221213 15:24:35 @agent_ppo2.py:143][0m Total time:      31.67 min
[32m[20221213 15:24:35 @agent_ppo2.py:145][0m 2859008 total steps have happened
[32m[20221213 15:24:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1396 --------------------------#
[32m[20221213 15:24:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |           0.0043 |          22.6068 |           0.2936 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0084 |          21.6662 |           0.2931 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0073 |          21.5627 |           0.2927 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0140 |          21.2041 |           0.2923 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0127 |          21.0236 |           0.2917 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0145 |          20.9019 |           0.2914 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0154 |          20.7706 |           0.2909 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0175 |          20.7049 |           0.2906 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0169 |          20.5878 |           0.2902 |
[32m[20221213 15:24:36 @agent_ppo2.py:185][0m |          -0.0171 |          20.5588 |           0.2899 |
[32m[20221213 15:24:36 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:24:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.34
[32m[20221213 15:24:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.92
[32m[20221213 15:24:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.74
[32m[20221213 15:24:37 @agent_ppo2.py:143][0m Total time:      31.69 min
[32m[20221213 15:24:37 @agent_ppo2.py:145][0m 2861056 total steps have happened
[32m[20221213 15:24:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1397 --------------------------#
[32m[20221213 15:24:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:37 @agent_ppo2.py:185][0m |           0.0038 |          21.1328 |           0.2894 |
[32m[20221213 15:24:37 @agent_ppo2.py:185][0m |           0.0001 |          20.7922 |           0.2886 |
[32m[20221213 15:24:37 @agent_ppo2.py:185][0m |          -0.0093 |          20.4642 |           0.2885 |
[32m[20221213 15:24:37 @agent_ppo2.py:185][0m |          -0.0107 |          20.3306 |           0.2881 |
[32m[20221213 15:24:37 @agent_ppo2.py:185][0m |          -0.0010 |          22.4172 |           0.2882 |
[32m[20221213 15:24:38 @agent_ppo2.py:185][0m |          -0.0029 |          20.9228 |           0.2865 |
[32m[20221213 15:24:38 @agent_ppo2.py:185][0m |          -0.0122 |          20.0821 |           0.2873 |
[32m[20221213 15:24:38 @agent_ppo2.py:185][0m |          -0.0135 |          19.9736 |           0.2874 |
[32m[20221213 15:24:38 @agent_ppo2.py:185][0m |          -0.0144 |          19.9226 |           0.2869 |
[32m[20221213 15:24:38 @agent_ppo2.py:185][0m |          -0.0165 |          19.8860 |           0.2869 |
[32m[20221213 15:24:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:24:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.93
[32m[20221213 15:24:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.32
[32m[20221213 15:24:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.18
[32m[20221213 15:24:38 @agent_ppo2.py:143][0m Total time:      31.71 min
[32m[20221213 15:24:38 @agent_ppo2.py:145][0m 2863104 total steps have happened
[32m[20221213 15:24:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1398 --------------------------#
[32m[20221213 15:24:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:38 @agent_ppo2.py:185][0m |           0.0010 |          21.1957 |           0.2840 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0047 |          20.8750 |           0.2838 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0015 |          21.4749 |           0.2832 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0128 |          20.6931 |           0.2831 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0112 |          20.5817 |           0.2830 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0096 |          21.1019 |           0.2834 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0166 |          20.3977 |           0.2833 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0183 |          20.3456 |           0.2831 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0196 |          20.2816 |           0.2838 |
[32m[20221213 15:24:39 @agent_ppo2.py:185][0m |          -0.0160 |          20.2415 |           0.2834 |
[32m[20221213 15:24:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:24:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.89
[32m[20221213 15:24:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.74
[32m[20221213 15:24:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.57
[32m[20221213 15:24:39 @agent_ppo2.py:143][0m Total time:      31.74 min
[32m[20221213 15:24:39 @agent_ppo2.py:145][0m 2865152 total steps have happened
[32m[20221213 15:24:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1399 --------------------------#
[32m[20221213 15:24:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |           0.0011 |          20.7885 |           0.2805 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0069 |          20.3710 |           0.2801 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0115 |          20.1317 |           0.2800 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0092 |          19.9734 |           0.2796 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0129 |          19.7701 |           0.2796 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0122 |          19.7901 |           0.2794 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0154 |          19.5828 |           0.2793 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0139 |          19.4477 |           0.2793 |
[32m[20221213 15:24:40 @agent_ppo2.py:185][0m |          -0.0187 |          19.3363 |           0.2792 |
[32m[20221213 15:24:41 @agent_ppo2.py:185][0m |          -0.0160 |          19.2675 |           0.2792 |
[32m[20221213 15:24:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.23
[32m[20221213 15:24:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.17
[32m[20221213 15:24:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.45
[32m[20221213 15:24:41 @agent_ppo2.py:143][0m Total time:      31.76 min
[32m[20221213 15:24:41 @agent_ppo2.py:145][0m 2867200 total steps have happened
[32m[20221213 15:24:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1400 --------------------------#
[32m[20221213 15:24:41 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:24:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:41 @agent_ppo2.py:185][0m |          -0.0013 |          20.7127 |           0.2829 |
[32m[20221213 15:24:41 @agent_ppo2.py:185][0m |          -0.0062 |          20.0697 |           0.2822 |
[32m[20221213 15:24:41 @agent_ppo2.py:185][0m |          -0.0099 |          19.7176 |           0.2817 |
[32m[20221213 15:24:41 @agent_ppo2.py:185][0m |          -0.0123 |          19.4478 |           0.2820 |
[32m[20221213 15:24:41 @agent_ppo2.py:185][0m |          -0.0131 |          19.2911 |           0.2817 |
[32m[20221213 15:24:42 @agent_ppo2.py:185][0m |          -0.0128 |          19.1252 |           0.2815 |
[32m[20221213 15:24:42 @agent_ppo2.py:185][0m |          -0.0054 |          19.4904 |           0.2814 |
[32m[20221213 15:24:42 @agent_ppo2.py:185][0m |          -0.0048 |          19.3184 |           0.2811 |
[32m[20221213 15:24:42 @agent_ppo2.py:185][0m |          -0.0147 |          18.8766 |           0.2807 |
[32m[20221213 15:24:42 @agent_ppo2.py:185][0m |          -0.0154 |          18.7745 |           0.2808 |
[32m[20221213 15:24:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.32
[32m[20221213 15:24:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.78
[32m[20221213 15:24:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.51
[32m[20221213 15:24:42 @agent_ppo2.py:143][0m Total time:      31.78 min
[32m[20221213 15:24:42 @agent_ppo2.py:145][0m 2869248 total steps have happened
[32m[20221213 15:24:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1401 --------------------------#
[32m[20221213 15:24:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:42 @agent_ppo2.py:185][0m |           0.0029 |          21.7728 |           0.2839 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0065 |          21.1991 |           0.2828 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0097 |          21.0202 |           0.2828 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0108 |          20.9030 |           0.2823 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0111 |          20.8642 |           0.2823 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0102 |          20.9110 |           0.2816 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |           0.0023 |          22.8780 |           0.2820 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0097 |          20.9317 |           0.2811 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0144 |          20.6512 |           0.2814 |
[32m[20221213 15:24:43 @agent_ppo2.py:185][0m |          -0.0147 |          20.5950 |           0.2813 |
[32m[20221213 15:24:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.22
[32m[20221213 15:24:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.92
[32m[20221213 15:24:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.62
[32m[20221213 15:24:43 @agent_ppo2.py:143][0m Total time:      31.80 min
[32m[20221213 15:24:43 @agent_ppo2.py:145][0m 2871296 total steps have happened
[32m[20221213 15:24:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1402 --------------------------#
[32m[20221213 15:24:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0012 |          20.7575 |           0.2788 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0052 |          20.4356 |           0.2779 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0119 |          19.9777 |           0.2775 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0109 |          19.9095 |           0.2775 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0072 |          20.7128 |           0.2775 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0122 |          19.6892 |           0.2773 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0150 |          19.4266 |           0.2776 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0171 |          19.3802 |           0.2774 |
[32m[20221213 15:24:44 @agent_ppo2.py:185][0m |          -0.0182 |          19.2680 |           0.2772 |
[32m[20221213 15:24:45 @agent_ppo2.py:185][0m |          -0.0066 |          19.7125 |           0.2775 |
[32m[20221213 15:24:45 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.28
[32m[20221213 15:24:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.98
[32m[20221213 15:24:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 276.98
[32m[20221213 15:24:45 @agent_ppo2.py:143][0m Total time:      31.83 min
[32m[20221213 15:24:45 @agent_ppo2.py:145][0m 2873344 total steps have happened
[32m[20221213 15:24:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1403 --------------------------#
[32m[20221213 15:24:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:45 @agent_ppo2.py:185][0m |          -0.0018 |          22.4450 |           0.2753 |
[32m[20221213 15:24:45 @agent_ppo2.py:185][0m |          -0.0086 |          21.5850 |           0.2750 |
[32m[20221213 15:24:45 @agent_ppo2.py:185][0m |          -0.0137 |          21.2387 |           0.2747 |
[32m[20221213 15:24:45 @agent_ppo2.py:185][0m |          -0.0128 |          21.0779 |           0.2749 |
[32m[20221213 15:24:45 @agent_ppo2.py:185][0m |          -0.0129 |          20.9160 |           0.2743 |
[32m[20221213 15:24:46 @agent_ppo2.py:185][0m |          -0.0123 |          20.9613 |           0.2744 |
[32m[20221213 15:24:46 @agent_ppo2.py:185][0m |          -0.0056 |          23.0224 |           0.2744 |
[32m[20221213 15:24:46 @agent_ppo2.py:185][0m |          -0.0173 |          20.6272 |           0.2739 |
[32m[20221213 15:24:46 @agent_ppo2.py:185][0m |          -0.0183 |          20.5148 |           0.2741 |
[32m[20221213 15:24:46 @agent_ppo2.py:185][0m |          -0.0191 |          20.4125 |           0.2740 |
[32m[20221213 15:24:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.48
[32m[20221213 15:24:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.77
[32m[20221213 15:24:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.97
[32m[20221213 15:24:46 @agent_ppo2.py:143][0m Total time:      31.85 min
[32m[20221213 15:24:46 @agent_ppo2.py:145][0m 2875392 total steps have happened
[32m[20221213 15:24:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1404 --------------------------#
[32m[20221213 15:24:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |           0.0018 |          20.6352 |           0.2801 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0084 |          20.0165 |           0.2797 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0025 |          21.1842 |           0.2794 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0150 |          19.2898 |           0.2789 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0151 |          19.1298 |           0.2790 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0168 |          18.9906 |           0.2787 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0179 |          18.8347 |           0.2789 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0180 |          18.7646 |           0.2787 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0194 |          18.6506 |           0.2786 |
[32m[20221213 15:24:47 @agent_ppo2.py:185][0m |          -0.0201 |          18.5650 |           0.2785 |
[32m[20221213 15:24:47 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:24:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.27
[32m[20221213 15:24:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.41
[32m[20221213 15:24:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.51
[32m[20221213 15:24:48 @agent_ppo2.py:143][0m Total time:      31.87 min
[32m[20221213 15:24:48 @agent_ppo2.py:145][0m 2877440 total steps have happened
[32m[20221213 15:24:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1405 --------------------------#
[32m[20221213 15:24:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:48 @agent_ppo2.py:185][0m |          -0.0018 |          21.9451 |           0.2767 |
[32m[20221213 15:24:48 @agent_ppo2.py:185][0m |          -0.0078 |          21.3165 |           0.2759 |
[32m[20221213 15:24:48 @agent_ppo2.py:185][0m |          -0.0087 |          21.1023 |           0.2752 |
[32m[20221213 15:24:48 @agent_ppo2.py:185][0m |          -0.0041 |          21.9258 |           0.2752 |
[32m[20221213 15:24:48 @agent_ppo2.py:185][0m |          -0.0141 |          20.8546 |           0.2750 |
[32m[20221213 15:24:48 @agent_ppo2.py:185][0m |          -0.0053 |          22.4662 |           0.2750 |
[32m[20221213 15:24:49 @agent_ppo2.py:185][0m |          -0.0140 |          20.7627 |           0.2749 |
[32m[20221213 15:24:49 @agent_ppo2.py:185][0m |          -0.0148 |          20.6706 |           0.2750 |
[32m[20221213 15:24:49 @agent_ppo2.py:185][0m |          -0.0175 |          20.6450 |           0.2749 |
[32m[20221213 15:24:49 @agent_ppo2.py:185][0m |          -0.0151 |          20.6789 |           0.2748 |
[32m[20221213 15:24:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:24:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.63
[32m[20221213 15:24:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.50
[32m[20221213 15:24:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.76
[32m[20221213 15:24:49 @agent_ppo2.py:143][0m Total time:      31.90 min
[32m[20221213 15:24:49 @agent_ppo2.py:145][0m 2879488 total steps have happened
[32m[20221213 15:24:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1406 --------------------------#
[32m[20221213 15:24:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:49 @agent_ppo2.py:185][0m |          -0.0011 |          21.6010 |           0.2769 |
[32m[20221213 15:24:49 @agent_ppo2.py:185][0m |          -0.0088 |          21.3351 |           0.2759 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0125 |          21.0960 |           0.2757 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0126 |          20.9653 |           0.2757 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0169 |          20.8534 |           0.2754 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0165 |          20.7057 |           0.2753 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0159 |          20.6354 |           0.2755 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0124 |          21.1855 |           0.2753 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0173 |          20.5269 |           0.2752 |
[32m[20221213 15:24:50 @agent_ppo2.py:185][0m |          -0.0079 |          22.0022 |           0.2753 |
[32m[20221213 15:24:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.79
[32m[20221213 15:24:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.18
[32m[20221213 15:24:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.37
[32m[20221213 15:24:50 @agent_ppo2.py:143][0m Total time:      31.92 min
[32m[20221213 15:24:50 @agent_ppo2.py:145][0m 2881536 total steps have happened
[32m[20221213 15:24:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1407 --------------------------#
[32m[20221213 15:24:51 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0014 |          21.1576 |           0.2770 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0066 |          20.6494 |           0.2767 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0107 |          20.4388 |           0.2768 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0113 |          20.3589 |           0.2765 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0122 |          20.1689 |           0.2768 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0114 |          20.1109 |           0.2767 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0121 |          19.9660 |           0.2766 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0152 |          19.8979 |           0.2764 |
[32m[20221213 15:24:51 @agent_ppo2.py:185][0m |          -0.0114 |          20.0839 |           0.2766 |
[32m[20221213 15:24:52 @agent_ppo2.py:185][0m |          -0.0104 |          19.9210 |           0.2764 |
[32m[20221213 15:24:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.18
[32m[20221213 15:24:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.04
[32m[20221213 15:24:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.19
[32m[20221213 15:24:52 @agent_ppo2.py:143][0m Total time:      31.94 min
[32m[20221213 15:24:52 @agent_ppo2.py:145][0m 2883584 total steps have happened
[32m[20221213 15:24:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1408 --------------------------#
[32m[20221213 15:24:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:52 @agent_ppo2.py:185][0m |          -0.0024 |          21.1646 |           0.2817 |
[32m[20221213 15:24:52 @agent_ppo2.py:185][0m |          -0.0091 |          20.8097 |           0.2816 |
[32m[20221213 15:24:52 @agent_ppo2.py:185][0m |          -0.0084 |          20.6240 |           0.2818 |
[32m[20221213 15:24:52 @agent_ppo2.py:185][0m |          -0.0124 |          20.3550 |           0.2811 |
[32m[20221213 15:24:52 @agent_ppo2.py:185][0m |          -0.0044 |          20.9019 |           0.2813 |
[32m[20221213 15:24:52 @agent_ppo2.py:185][0m |          -0.0072 |          20.6546 |           0.2811 |
[32m[20221213 15:24:53 @agent_ppo2.py:185][0m |          -0.0147 |          19.9722 |           0.2811 |
[32m[20221213 15:24:53 @agent_ppo2.py:185][0m |          -0.0155 |          19.9760 |           0.2813 |
[32m[20221213 15:24:53 @agent_ppo2.py:185][0m |          -0.0167 |          19.8352 |           0.2810 |
[32m[20221213 15:24:53 @agent_ppo2.py:185][0m |          -0.0149 |          19.8217 |           0.2808 |
[32m[20221213 15:24:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.36
[32m[20221213 15:24:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.80
[32m[20221213 15:24:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.70
[32m[20221213 15:24:53 @agent_ppo2.py:143][0m Total time:      31.96 min
[32m[20221213 15:24:53 @agent_ppo2.py:145][0m 2885632 total steps have happened
[32m[20221213 15:24:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1409 --------------------------#
[32m[20221213 15:24:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:53 @agent_ppo2.py:185][0m |           0.0019 |          21.7119 |           0.2782 |
[32m[20221213 15:24:53 @agent_ppo2.py:185][0m |          -0.0074 |          21.2216 |           0.2780 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0099 |          20.9104 |           0.2775 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0100 |          20.7036 |           0.2775 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0127 |          20.5055 |           0.2777 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0113 |          20.3500 |           0.2772 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0125 |          20.1667 |           0.2773 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0163 |          20.0435 |           0.2768 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0148 |          19.9686 |           0.2770 |
[32m[20221213 15:24:54 @agent_ppo2.py:185][0m |          -0.0096 |          20.5935 |           0.2766 |
[32m[20221213 15:24:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:24:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.86
[32m[20221213 15:24:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.01
[32m[20221213 15:24:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.45
[32m[20221213 15:24:54 @agent_ppo2.py:143][0m Total time:      31.99 min
[32m[20221213 15:24:54 @agent_ppo2.py:145][0m 2887680 total steps have happened
[32m[20221213 15:24:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1410 --------------------------#
[32m[20221213 15:24:55 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:24:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |           0.0010 |          20.3321 |           0.2862 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0092 |          19.5819 |           0.2854 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0132 |          19.4317 |           0.2852 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0120 |          19.2607 |           0.2855 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0131 |          19.1788 |           0.2852 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0159 |          19.0445 |           0.2853 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0180 |          18.8994 |           0.2851 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0148 |          18.8400 |           0.2850 |
[32m[20221213 15:24:55 @agent_ppo2.py:185][0m |          -0.0174 |          18.7713 |           0.2848 |
[32m[20221213 15:24:56 @agent_ppo2.py:185][0m |          -0.0176 |          18.7565 |           0.2845 |
[32m[20221213 15:24:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.25
[32m[20221213 15:24:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.50
[32m[20221213 15:24:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.33
[32m[20221213 15:24:56 @agent_ppo2.py:143][0m Total time:      32.01 min
[32m[20221213 15:24:56 @agent_ppo2.py:145][0m 2889728 total steps have happened
[32m[20221213 15:24:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1411 --------------------------#
[32m[20221213 15:24:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:24:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:56 @agent_ppo2.py:185][0m |          -0.0021 |          21.6359 |           0.2763 |
[32m[20221213 15:24:56 @agent_ppo2.py:185][0m |          -0.0072 |          21.2893 |           0.2761 |
[32m[20221213 15:24:56 @agent_ppo2.py:185][0m |           0.0089 |          24.0071 |           0.2763 |
[32m[20221213 15:24:56 @agent_ppo2.py:185][0m |          -0.0098 |          21.1567 |           0.2760 |
[32m[20221213 15:24:56 @agent_ppo2.py:185][0m |          -0.0122 |          20.9951 |           0.2764 |
[32m[20221213 15:24:57 @agent_ppo2.py:185][0m |          -0.0153 |          20.9548 |           0.2765 |
[32m[20221213 15:24:57 @agent_ppo2.py:185][0m |          -0.0143 |          20.8768 |           0.2765 |
[32m[20221213 15:24:57 @agent_ppo2.py:185][0m |          -0.0149 |          20.8637 |           0.2768 |
[32m[20221213 15:24:57 @agent_ppo2.py:185][0m |          -0.0102 |          21.9648 |           0.2767 |
[32m[20221213 15:24:57 @agent_ppo2.py:185][0m |          -0.0163 |          20.7875 |           0.2765 |
[32m[20221213 15:24:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.12
[32m[20221213 15:24:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.12
[32m[20221213 15:24:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.28
[32m[20221213 15:24:57 @agent_ppo2.py:143][0m Total time:      32.03 min
[32m[20221213 15:24:57 @agent_ppo2.py:145][0m 2891776 total steps have happened
[32m[20221213 15:24:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1412 --------------------------#
[32m[20221213 15:24:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:57 @agent_ppo2.py:185][0m |          -0.0017 |          20.9445 |           0.2803 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |           0.0060 |          23.4817 |           0.2803 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0087 |          20.5748 |           0.2794 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0095 |          20.4492 |           0.2797 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0097 |          20.4386 |           0.2802 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0134 |          20.3503 |           0.2799 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0159 |          20.2847 |           0.2797 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0148 |          20.2566 |           0.2802 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0167 |          20.1850 |           0.2801 |
[32m[20221213 15:24:58 @agent_ppo2.py:185][0m |          -0.0182 |          20.1671 |           0.2804 |
[32m[20221213 15:24:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:24:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.10
[32m[20221213 15:24:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.16
[32m[20221213 15:24:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.29
[32m[20221213 15:24:58 @agent_ppo2.py:143][0m Total time:      32.05 min
[32m[20221213 15:24:58 @agent_ppo2.py:145][0m 2893824 total steps have happened
[32m[20221213 15:24:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1413 --------------------------#
[32m[20221213 15:24:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:24:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0008 |          20.0894 |           0.2824 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0035 |          19.7175 |           0.2821 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0056 |          19.4560 |           0.2820 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0091 |          19.2322 |           0.2821 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0101 |          19.2183 |           0.2818 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0105 |          19.2586 |           0.2822 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0123 |          18.8840 |           0.2818 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0116 |          18.8775 |           0.2820 |
[32m[20221213 15:24:59 @agent_ppo2.py:185][0m |          -0.0040 |          21.1445 |           0.2823 |
[32m[20221213 15:25:00 @agent_ppo2.py:185][0m |          -0.0103 |          18.6982 |           0.2822 |
[32m[20221213 15:25:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:25:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.14
[32m[20221213 15:25:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.30
[32m[20221213 15:25:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 261.21
[32m[20221213 15:25:00 @agent_ppo2.py:143][0m Total time:      32.08 min
[32m[20221213 15:25:00 @agent_ppo2.py:145][0m 2895872 total steps have happened
[32m[20221213 15:25:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1414 --------------------------#
[32m[20221213 15:25:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:00 @agent_ppo2.py:185][0m |           0.0045 |          21.8692 |           0.2777 |
[32m[20221213 15:25:00 @agent_ppo2.py:185][0m |          -0.0049 |          20.8024 |           0.2775 |
[32m[20221213 15:25:00 @agent_ppo2.py:185][0m |          -0.0069 |          20.6800 |           0.2772 |
[32m[20221213 15:25:00 @agent_ppo2.py:185][0m |          -0.0119 |          20.5410 |           0.2772 |
[32m[20221213 15:25:00 @agent_ppo2.py:185][0m |          -0.0157 |          20.4490 |           0.2775 |
[32m[20221213 15:25:01 @agent_ppo2.py:185][0m |          -0.0129 |          20.3476 |           0.2774 |
[32m[20221213 15:25:01 @agent_ppo2.py:185][0m |          -0.0062 |          21.9613 |           0.2775 |
[32m[20221213 15:25:01 @agent_ppo2.py:185][0m |          -0.0160 |          20.2097 |           0.2772 |
[32m[20221213 15:25:01 @agent_ppo2.py:185][0m |          -0.0174 |          20.1065 |           0.2774 |
[32m[20221213 15:25:01 @agent_ppo2.py:185][0m |          -0.0159 |          20.0314 |           0.2774 |
[32m[20221213 15:25:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 237.85
[32m[20221213 15:25:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.55
[32m[20221213 15:25:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.02
[32m[20221213 15:25:01 @agent_ppo2.py:143][0m Total time:      32.10 min
[32m[20221213 15:25:01 @agent_ppo2.py:145][0m 2897920 total steps have happened
[32m[20221213 15:25:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1415 --------------------------#
[32m[20221213 15:25:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:01 @agent_ppo2.py:185][0m |          -0.0009 |          20.2980 |           0.2790 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0086 |          19.3437 |           0.2790 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0123 |          18.8692 |           0.2784 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0121 |          18.5125 |           0.2784 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0140 |          18.2500 |           0.2781 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0141 |          18.0649 |           0.2784 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0182 |          17.8926 |           0.2782 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0170 |          17.8546 |           0.2784 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0171 |          17.6209 |           0.2784 |
[32m[20221213 15:25:02 @agent_ppo2.py:185][0m |          -0.0172 |          17.5204 |           0.2787 |
[32m[20221213 15:25:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.37
[32m[20221213 15:25:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.79
[32m[20221213 15:25:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.94
[32m[20221213 15:25:02 @agent_ppo2.py:143][0m Total time:      32.12 min
[32m[20221213 15:25:02 @agent_ppo2.py:145][0m 2899968 total steps have happened
[32m[20221213 15:25:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1416 --------------------------#
[32m[20221213 15:25:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0031 |          21.7510 |           0.2864 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0040 |          21.4661 |           0.2864 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0114 |          20.9323 |           0.2865 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0039 |          22.1920 |           0.2862 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0025 |          22.2518 |           0.2861 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0140 |          20.5797 |           0.2858 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0092 |          21.2245 |           0.2859 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0168 |          20.3906 |           0.2856 |
[32m[20221213 15:25:03 @agent_ppo2.py:185][0m |          -0.0176 |          20.3427 |           0.2859 |
[32m[20221213 15:25:04 @agent_ppo2.py:185][0m |          -0.0160 |          20.2244 |           0.2858 |
[32m[20221213 15:25:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.06
[32m[20221213 15:25:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.60
[32m[20221213 15:25:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.27
[32m[20221213 15:25:04 @agent_ppo2.py:143][0m Total time:      32.14 min
[32m[20221213 15:25:04 @agent_ppo2.py:145][0m 2902016 total steps have happened
[32m[20221213 15:25:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1417 --------------------------#
[32m[20221213 15:25:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:04 @agent_ppo2.py:185][0m |           0.0060 |          21.8684 |           0.2775 |
[32m[20221213 15:25:04 @agent_ppo2.py:185][0m |          -0.0033 |          21.1196 |           0.2766 |
[32m[20221213 15:25:04 @agent_ppo2.py:185][0m |          -0.0052 |          21.2718 |           0.2763 |
[32m[20221213 15:25:04 @agent_ppo2.py:185][0m |          -0.0136 |          20.7024 |           0.2761 |
[32m[20221213 15:25:05 @agent_ppo2.py:185][0m |          -0.0144 |          20.5496 |           0.2767 |
[32m[20221213 15:25:05 @agent_ppo2.py:185][0m |          -0.0154 |          20.4539 |           0.2762 |
[32m[20221213 15:25:05 @agent_ppo2.py:185][0m |          -0.0149 |          20.3674 |           0.2765 |
[32m[20221213 15:25:05 @agent_ppo2.py:185][0m |          -0.0178 |          20.3449 |           0.2767 |
[32m[20221213 15:25:05 @agent_ppo2.py:185][0m |          -0.0175 |          20.2887 |           0.2767 |
[32m[20221213 15:25:05 @agent_ppo2.py:185][0m |          -0.0182 |          20.1859 |           0.2769 |
[32m[20221213 15:25:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.79
[32m[20221213 15:25:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.30
[32m[20221213 15:25:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.63
[32m[20221213 15:25:05 @agent_ppo2.py:143][0m Total time:      32.17 min
[32m[20221213 15:25:05 @agent_ppo2.py:145][0m 2904064 total steps have happened
[32m[20221213 15:25:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1418 --------------------------#
[32m[20221213 15:25:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:05 @agent_ppo2.py:185][0m |           0.0032 |          22.6875 |           0.2927 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0076 |          22.0208 |           0.2919 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0014 |          23.4097 |           0.2915 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |           0.0003 |          22.3962 |           0.2909 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0117 |          21.6922 |           0.2908 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0035 |          22.6192 |           0.2908 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0033 |          23.3202 |           0.2902 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0136 |          21.5182 |           0.2895 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0144 |          21.3946 |           0.2898 |
[32m[20221213 15:25:06 @agent_ppo2.py:185][0m |          -0.0145 |          21.3024 |           0.2897 |
[32m[20221213 15:25:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.24
[32m[20221213 15:25:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.05
[32m[20221213 15:25:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.68
[32m[20221213 15:25:06 @agent_ppo2.py:143][0m Total time:      32.19 min
[32m[20221213 15:25:06 @agent_ppo2.py:145][0m 2906112 total steps have happened
[32m[20221213 15:25:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1419 --------------------------#
[32m[20221213 15:25:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |           0.0011 |          21.1629 |           0.2927 |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |          -0.0044 |          21.2701 |           0.2922 |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |          -0.0076 |          20.6149 |           0.2917 |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |          -0.0125 |          20.3223 |           0.2916 |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |          -0.0125 |          20.1911 |           0.2913 |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |          -0.0104 |          20.1431 |           0.2912 |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |          -0.0135 |          20.0039 |           0.2908 |
[32m[20221213 15:25:07 @agent_ppo2.py:185][0m |          -0.0159 |          19.8886 |           0.2905 |
[32m[20221213 15:25:08 @agent_ppo2.py:185][0m |          -0.0169 |          19.7922 |           0.2905 |
[32m[20221213 15:25:08 @agent_ppo2.py:185][0m |          -0.0174 |          19.7175 |           0.2904 |
[32m[20221213 15:25:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:25:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.56
[32m[20221213 15:25:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.47
[32m[20221213 15:25:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.37
[32m[20221213 15:25:08 @agent_ppo2.py:143][0m Total time:      32.21 min
[32m[20221213 15:25:08 @agent_ppo2.py:145][0m 2908160 total steps have happened
[32m[20221213 15:25:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1420 --------------------------#
[32m[20221213 15:25:08 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:25:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:08 @agent_ppo2.py:185][0m |           0.0012 |          20.6988 |           0.2839 |
[32m[20221213 15:25:08 @agent_ppo2.py:185][0m |          -0.0069 |          20.1772 |           0.2839 |
[32m[20221213 15:25:08 @agent_ppo2.py:185][0m |          -0.0080 |          19.8908 |           0.2836 |
[32m[20221213 15:25:08 @agent_ppo2.py:185][0m |          -0.0109 |          19.6889 |           0.2833 |
[32m[20221213 15:25:09 @agent_ppo2.py:185][0m |          -0.0103 |          19.5350 |           0.2831 |
[32m[20221213 15:25:09 @agent_ppo2.py:185][0m |          -0.0139 |          19.3886 |           0.2832 |
[32m[20221213 15:25:09 @agent_ppo2.py:185][0m |          -0.0147 |          19.2869 |           0.2831 |
[32m[20221213 15:25:09 @agent_ppo2.py:185][0m |          -0.0050 |          20.5729 |           0.2830 |
[32m[20221213 15:25:09 @agent_ppo2.py:185][0m |          -0.0135 |          19.0909 |           0.2831 |
[32m[20221213 15:25:09 @agent_ppo2.py:185][0m |          -0.0152 |          19.0148 |           0.2829 |
[32m[20221213 15:25:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:25:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.71
[32m[20221213 15:25:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.97
[32m[20221213 15:25:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.37
[32m[20221213 15:25:09 @agent_ppo2.py:143][0m Total time:      32.23 min
[32m[20221213 15:25:09 @agent_ppo2.py:145][0m 2910208 total steps have happened
[32m[20221213 15:25:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1421 --------------------------#
[32m[20221213 15:25:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |           0.0011 |          22.9629 |           0.2780 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0064 |          22.3983 |           0.2777 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |           0.0020 |          23.0131 |           0.2776 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0011 |          22.5312 |           0.2779 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0073 |          22.0205 |           0.2779 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0117 |          21.8247 |           0.2780 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0098 |          22.0418 |           0.2779 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0126 |          21.7361 |           0.2777 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0147 |          21.6739 |           0.2779 |
[32m[20221213 15:25:10 @agent_ppo2.py:185][0m |          -0.0174 |          21.6588 |           0.2778 |
[32m[20221213 15:25:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.28
[32m[20221213 15:25:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.35
[32m[20221213 15:25:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.29
[32m[20221213 15:25:11 @agent_ppo2.py:143][0m Total time:      32.26 min
[32m[20221213 15:25:11 @agent_ppo2.py:145][0m 2912256 total steps have happened
[32m[20221213 15:25:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1422 --------------------------#
[32m[20221213 15:25:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:11 @agent_ppo2.py:185][0m |          -0.0001 |          20.6928 |           0.2814 |
[32m[20221213 15:25:11 @agent_ppo2.py:185][0m |          -0.0052 |          20.0182 |           0.2816 |
[32m[20221213 15:25:11 @agent_ppo2.py:185][0m |          -0.0059 |          19.5960 |           0.2806 |
[32m[20221213 15:25:11 @agent_ppo2.py:185][0m |          -0.0099 |          19.3172 |           0.2810 |
[32m[20221213 15:25:11 @agent_ppo2.py:185][0m |          -0.0111 |          19.0295 |           0.2809 |
[32m[20221213 15:25:11 @agent_ppo2.py:185][0m |          -0.0143 |          18.8279 |           0.2803 |
[32m[20221213 15:25:11 @agent_ppo2.py:185][0m |          -0.0140 |          18.6105 |           0.2801 |
[32m[20221213 15:25:12 @agent_ppo2.py:185][0m |          -0.0152 |          18.4513 |           0.2804 |
[32m[20221213 15:25:12 @agent_ppo2.py:185][0m |          -0.0146 |          18.2981 |           0.2803 |
[32m[20221213 15:25:12 @agent_ppo2.py:185][0m |          -0.0117 |          18.2089 |           0.2803 |
[32m[20221213 15:25:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:25:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.71
[32m[20221213 15:25:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.50
[32m[20221213 15:25:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.01
[32m[20221213 15:25:12 @agent_ppo2.py:143][0m Total time:      32.28 min
[32m[20221213 15:25:12 @agent_ppo2.py:145][0m 2914304 total steps have happened
[32m[20221213 15:25:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1423 --------------------------#
[32m[20221213 15:25:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:12 @agent_ppo2.py:185][0m |          -0.0004 |          21.2163 |           0.2802 |
[32m[20221213 15:25:12 @agent_ppo2.py:185][0m |          -0.0065 |          20.7910 |           0.2798 |
[32m[20221213 15:25:12 @agent_ppo2.py:185][0m |          -0.0061 |          20.8367 |           0.2799 |
[32m[20221213 15:25:13 @agent_ppo2.py:185][0m |          -0.0104 |          20.4933 |           0.2799 |
[32m[20221213 15:25:13 @agent_ppo2.py:185][0m |          -0.0126 |          20.3618 |           0.2802 |
[32m[20221213 15:25:13 @agent_ppo2.py:185][0m |          -0.0153 |          20.2075 |           0.2803 |
[32m[20221213 15:25:13 @agent_ppo2.py:185][0m |          -0.0159 |          20.1698 |           0.2805 |
[32m[20221213 15:25:13 @agent_ppo2.py:185][0m |          -0.0067 |          21.8654 |           0.2807 |
[32m[20221213 15:25:13 @agent_ppo2.py:185][0m |          -0.0172 |          20.0156 |           0.2806 |
[32m[20221213 15:25:13 @agent_ppo2.py:185][0m |          -0.0173 |          19.8899 |           0.2805 |
[32m[20221213 15:25:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:25:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.59
[32m[20221213 15:25:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.35
[32m[20221213 15:25:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.21
[32m[20221213 15:25:13 @agent_ppo2.py:143][0m Total time:      32.30 min
[32m[20221213 15:25:13 @agent_ppo2.py:145][0m 2916352 total steps have happened
[32m[20221213 15:25:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1424 --------------------------#
[32m[20221213 15:25:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0012 |          22.2922 |           0.2874 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0079 |          21.8496 |           0.2863 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0119 |          21.6242 |           0.2859 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0119 |          21.4786 |           0.2861 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0032 |          22.8191 |           0.2858 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0143 |          21.3483 |           0.2858 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0151 |          21.1455 |           0.2856 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0160 |          21.1009 |           0.2855 |
[32m[20221213 15:25:14 @agent_ppo2.py:185][0m |          -0.0152 |          21.0265 |           0.2854 |
[32m[20221213 15:25:15 @agent_ppo2.py:185][0m |          -0.0160 |          20.9632 |           0.2854 |
[32m[20221213 15:25:15 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:25:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.04
[32m[20221213 15:25:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.78
[32m[20221213 15:25:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.76
[32m[20221213 15:25:15 @agent_ppo2.py:143][0m Total time:      32.33 min
[32m[20221213 15:25:15 @agent_ppo2.py:145][0m 2918400 total steps have happened
[32m[20221213 15:25:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1425 --------------------------#
[32m[20221213 15:25:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:15 @agent_ppo2.py:185][0m |          -0.0009 |          22.1866 |           0.2842 |
[32m[20221213 15:25:15 @agent_ppo2.py:185][0m |          -0.0060 |          21.5309 |           0.2839 |
[32m[20221213 15:25:15 @agent_ppo2.py:185][0m |          -0.0041 |          21.3061 |           0.2839 |
[32m[20221213 15:25:15 @agent_ppo2.py:185][0m |          -0.0093 |          21.1224 |           0.2838 |
[32m[20221213 15:25:15 @agent_ppo2.py:185][0m |          -0.0070 |          21.1556 |           0.2834 |
[32m[20221213 15:25:16 @agent_ppo2.py:185][0m |          -0.0114 |          20.8071 |           0.2833 |
[32m[20221213 15:25:16 @agent_ppo2.py:185][0m |          -0.0127 |          20.6583 |           0.2833 |
[32m[20221213 15:25:16 @agent_ppo2.py:185][0m |          -0.0123 |          20.6402 |           0.2835 |
[32m[20221213 15:25:16 @agent_ppo2.py:185][0m |          -0.0144 |          20.5186 |           0.2831 |
[32m[20221213 15:25:16 @agent_ppo2.py:185][0m |          -0.0156 |          20.4761 |           0.2834 |
[32m[20221213 15:25:16 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:25:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.16
[32m[20221213 15:25:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.88
[32m[20221213 15:25:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.61
[32m[20221213 15:25:16 @agent_ppo2.py:143][0m Total time:      32.35 min
[32m[20221213 15:25:16 @agent_ppo2.py:145][0m 2920448 total steps have happened
[32m[20221213 15:25:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1426 --------------------------#
[32m[20221213 15:25:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:16 @agent_ppo2.py:185][0m |          -0.0003 |          21.8414 |           0.2827 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |           0.0015 |          22.8160 |           0.2827 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0103 |          21.5108 |           0.2818 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0119 |          21.4142 |           0.2821 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0128 |          21.3606 |           0.2815 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0117 |          21.2967 |           0.2818 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0143 |          21.2459 |           0.2818 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0146 |          21.2225 |           0.2815 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0190 |          21.1750 |           0.2815 |
[32m[20221213 15:25:17 @agent_ppo2.py:185][0m |          -0.0190 |          21.1305 |           0.2815 |
[32m[20221213 15:25:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:25:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.91
[32m[20221213 15:25:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.52
[32m[20221213 15:25:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.02
[32m[20221213 15:25:17 @agent_ppo2.py:143][0m Total time:      32.37 min
[32m[20221213 15:25:17 @agent_ppo2.py:145][0m 2922496 total steps have happened
[32m[20221213 15:25:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1427 --------------------------#
[32m[20221213 15:25:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0009 |          21.1622 |           0.2877 |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0073 |          20.3385 |           0.2868 |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0096 |          19.8024 |           0.2867 |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0120 |          19.4814 |           0.2868 |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0129 |          19.2146 |           0.2868 |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0124 |          18.9472 |           0.2861 |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0156 |          18.7652 |           0.2868 |
[32m[20221213 15:25:18 @agent_ppo2.py:185][0m |          -0.0166 |          18.5961 |           0.2867 |
[32m[20221213 15:25:19 @agent_ppo2.py:185][0m |          -0.0101 |          18.9638 |           0.2864 |
[32m[20221213 15:25:19 @agent_ppo2.py:185][0m |          -0.0167 |          18.3244 |           0.2865 |
[32m[20221213 15:25:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.53
[32m[20221213 15:25:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.77
[32m[20221213 15:25:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.15
[32m[20221213 15:25:19 @agent_ppo2.py:143][0m Total time:      32.39 min
[32m[20221213 15:25:19 @agent_ppo2.py:145][0m 2924544 total steps have happened
[32m[20221213 15:25:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1428 --------------------------#
[32m[20221213 15:25:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:19 @agent_ppo2.py:185][0m |           0.0060 |          24.9650 |           0.2843 |
[32m[20221213 15:25:19 @agent_ppo2.py:185][0m |           0.0007 |          23.6379 |           0.2837 |
[32m[20221213 15:25:19 @agent_ppo2.py:185][0m |          -0.0054 |          22.8930 |           0.2837 |
[32m[20221213 15:25:19 @agent_ppo2.py:185][0m |          -0.0066 |          22.6363 |           0.2838 |
[32m[20221213 15:25:20 @agent_ppo2.py:185][0m |          -0.0108 |          22.3963 |           0.2833 |
[32m[20221213 15:25:20 @agent_ppo2.py:185][0m |          -0.0131 |          22.2468 |           0.2833 |
[32m[20221213 15:25:20 @agent_ppo2.py:185][0m |          -0.0146 |          22.1093 |           0.2833 |
[32m[20221213 15:25:20 @agent_ppo2.py:185][0m |          -0.0165 |          22.1663 |           0.2830 |
[32m[20221213 15:25:20 @agent_ppo2.py:185][0m |          -0.0205 |          21.9588 |           0.2831 |
[32m[20221213 15:25:20 @agent_ppo2.py:185][0m |          -0.0178 |          21.8166 |           0.2832 |
[32m[20221213 15:25:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.66
[32m[20221213 15:25:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.71
[32m[20221213 15:25:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.03
[32m[20221213 15:25:20 @agent_ppo2.py:143][0m Total time:      32.42 min
[32m[20221213 15:25:20 @agent_ppo2.py:145][0m 2926592 total steps have happened
[32m[20221213 15:25:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1429 --------------------------#
[32m[20221213 15:25:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0037 |          21.7786 |           0.2845 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0098 |          21.2857 |           0.2838 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0087 |          21.0901 |           0.2835 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0117 |          20.9382 |           0.2831 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0136 |          20.8869 |           0.2832 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0095 |          21.0802 |           0.2827 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0138 |          20.7628 |           0.2822 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0099 |          21.2913 |           0.2822 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0150 |          20.7379 |           0.2822 |
[32m[20221213 15:25:21 @agent_ppo2.py:185][0m |          -0.0185 |          20.6822 |           0.2818 |
[32m[20221213 15:25:21 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.84
[32m[20221213 15:25:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.91
[32m[20221213 15:25:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.53
[32m[20221213 15:25:21 @agent_ppo2.py:143][0m Total time:      32.44 min
[32m[20221213 15:25:21 @agent_ppo2.py:145][0m 2928640 total steps have happened
[32m[20221213 15:25:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1430 --------------------------#
[32m[20221213 15:25:22 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:25:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0028 |          21.9861 |           0.2754 |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0101 |          21.6844 |           0.2754 |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0107 |          21.4963 |           0.2755 |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0002 |          22.7182 |           0.2755 |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0131 |          21.2849 |           0.2750 |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0133 |          21.2028 |           0.2753 |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0145 |          21.0966 |           0.2754 |
[32m[20221213 15:25:22 @agent_ppo2.py:185][0m |          -0.0142 |          21.0628 |           0.2754 |
[32m[20221213 15:25:23 @agent_ppo2.py:185][0m |          -0.0164 |          20.9800 |           0.2752 |
[32m[20221213 15:25:23 @agent_ppo2.py:185][0m |          -0.0181 |          20.9385 |           0.2754 |
[32m[20221213 15:25:23 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:25:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.46
[32m[20221213 15:25:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.43
[32m[20221213 15:25:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.97
[32m[20221213 15:25:23 @agent_ppo2.py:143][0m Total time:      32.46 min
[32m[20221213 15:25:23 @agent_ppo2.py:145][0m 2930688 total steps have happened
[32m[20221213 15:25:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1431 --------------------------#
[32m[20221213 15:25:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:23 @agent_ppo2.py:185][0m |          -0.0031 |          20.7892 |           0.2841 |
[32m[20221213 15:25:23 @agent_ppo2.py:185][0m |          -0.0065 |          20.4576 |           0.2833 |
[32m[20221213 15:25:23 @agent_ppo2.py:185][0m |          -0.0089 |          20.3186 |           0.2831 |
[32m[20221213 15:25:23 @agent_ppo2.py:185][0m |          -0.0119 |          20.2108 |           0.2833 |
[32m[20221213 15:25:24 @agent_ppo2.py:185][0m |          -0.0063 |          21.3332 |           0.2831 |
[32m[20221213 15:25:24 @agent_ppo2.py:185][0m |          -0.0150 |          20.0591 |           0.2830 |
[32m[20221213 15:25:24 @agent_ppo2.py:185][0m |          -0.0150 |          20.0052 |           0.2830 |
[32m[20221213 15:25:24 @agent_ppo2.py:185][0m |          -0.0159 |          19.9644 |           0.2831 |
[32m[20221213 15:25:24 @agent_ppo2.py:185][0m |          -0.0142 |          20.3773 |           0.2835 |
[32m[20221213 15:25:24 @agent_ppo2.py:185][0m |          -0.0144 |          19.8737 |           0.2831 |
[32m[20221213 15:25:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.51
[32m[20221213 15:25:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.79
[32m[20221213 15:25:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.38
[32m[20221213 15:25:24 @agent_ppo2.py:143][0m Total time:      32.48 min
[32m[20221213 15:25:24 @agent_ppo2.py:145][0m 2932736 total steps have happened
[32m[20221213 15:25:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1432 --------------------------#
[32m[20221213 15:25:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0000 |          21.2404 |           0.2838 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |           0.0034 |          23.8520 |           0.2833 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0094 |          20.9831 |           0.2825 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0142 |          20.8472 |           0.2826 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0140 |          20.7652 |           0.2828 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0101 |          20.7680 |           0.2824 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0162 |          20.7034 |           0.2821 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0156 |          20.6698 |           0.2824 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0135 |          20.6363 |           0.2823 |
[32m[20221213 15:25:25 @agent_ppo2.py:185][0m |          -0.0131 |          20.6019 |           0.2820 |
[32m[20221213 15:25:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.38
[32m[20221213 15:25:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.73
[32m[20221213 15:25:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.82
[32m[20221213 15:25:26 @agent_ppo2.py:143][0m Total time:      32.51 min
[32m[20221213 15:25:26 @agent_ppo2.py:145][0m 2934784 total steps have happened
[32m[20221213 15:25:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1433 --------------------------#
[32m[20221213 15:25:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |           0.0030 |          21.2610 |           0.2773 |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |          -0.0087 |          20.7330 |           0.2761 |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |          -0.0100 |          20.4802 |           0.2756 |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |          -0.0125 |          20.3257 |           0.2753 |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |          -0.0136 |          20.2032 |           0.2750 |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |          -0.0148 |          20.1183 |           0.2750 |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |          -0.0142 |          20.0316 |           0.2748 |
[32m[20221213 15:25:26 @agent_ppo2.py:185][0m |          -0.0143 |          19.9631 |           0.2746 |
[32m[20221213 15:25:27 @agent_ppo2.py:185][0m |          -0.0168 |          19.9177 |           0.2746 |
[32m[20221213 15:25:27 @agent_ppo2.py:185][0m |          -0.0154 |          19.8889 |           0.2744 |
[32m[20221213 15:25:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:25:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.93
[32m[20221213 15:25:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.38
[32m[20221213 15:25:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.67
[32m[20221213 15:25:27 @agent_ppo2.py:143][0m Total time:      32.53 min
[32m[20221213 15:25:27 @agent_ppo2.py:145][0m 2936832 total steps have happened
[32m[20221213 15:25:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1434 --------------------------#
[32m[20221213 15:25:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:27 @agent_ppo2.py:185][0m |           0.0033 |          22.0816 |           0.2823 |
[32m[20221213 15:25:27 @agent_ppo2.py:185][0m |          -0.0030 |          22.1620 |           0.2821 |
[32m[20221213 15:25:27 @agent_ppo2.py:185][0m |          -0.0098 |          21.5072 |           0.2816 |
[32m[20221213 15:25:27 @agent_ppo2.py:185][0m |          -0.0050 |          21.8995 |           0.2816 |
[32m[20221213 15:25:28 @agent_ppo2.py:185][0m |          -0.0086 |          21.3630 |           0.2819 |
[32m[20221213 15:25:28 @agent_ppo2.py:185][0m |          -0.0144 |          21.3442 |           0.2813 |
[32m[20221213 15:25:28 @agent_ppo2.py:185][0m |          -0.0159 |          21.2140 |           0.2813 |
[32m[20221213 15:25:28 @agent_ppo2.py:185][0m |          -0.0150 |          21.1959 |           0.2814 |
[32m[20221213 15:25:28 @agent_ppo2.py:185][0m |          -0.0051 |          22.5478 |           0.2812 |
[32m[20221213 15:25:28 @agent_ppo2.py:185][0m |          -0.0165 |          21.1096 |           0.2808 |
[32m[20221213 15:25:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.86
[32m[20221213 15:25:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.69
[32m[20221213 15:25:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.37
[32m[20221213 15:25:28 @agent_ppo2.py:143][0m Total time:      32.55 min
[32m[20221213 15:25:28 @agent_ppo2.py:145][0m 2938880 total steps have happened
[32m[20221213 15:25:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1435 --------------------------#
[32m[20221213 15:25:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0020 |          21.7093 |           0.2840 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0013 |          21.7590 |           0.2833 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0046 |          21.5984 |           0.2826 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0122 |          21.2806 |           0.2824 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0134 |          21.1937 |           0.2823 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0109 |          21.2298 |           0.2821 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0138 |          21.1116 |           0.2819 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0167 |          21.0936 |           0.2821 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0158 |          21.0247 |           0.2815 |
[32m[20221213 15:25:29 @agent_ppo2.py:185][0m |          -0.0173 |          21.0072 |           0.2817 |
[32m[20221213 15:25:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.75
[32m[20221213 15:25:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.28
[32m[20221213 15:25:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.00
[32m[20221213 15:25:30 @agent_ppo2.py:143][0m Total time:      32.57 min
[32m[20221213 15:25:30 @agent_ppo2.py:145][0m 2940928 total steps have happened
[32m[20221213 15:25:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1436 --------------------------#
[32m[20221213 15:25:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:30 @agent_ppo2.py:185][0m |           0.0002 |          20.6134 |           0.2717 |
[32m[20221213 15:25:30 @agent_ppo2.py:185][0m |          -0.0052 |          20.2864 |           0.2708 |
[32m[20221213 15:25:30 @agent_ppo2.py:185][0m |          -0.0071 |          20.2157 |           0.2707 |
[32m[20221213 15:25:30 @agent_ppo2.py:185][0m |          -0.0040 |          20.8729 |           0.2703 |
[32m[20221213 15:25:30 @agent_ppo2.py:185][0m |           0.0035 |          22.6041 |           0.2697 |
[32m[20221213 15:25:30 @agent_ppo2.py:185][0m |           0.0025 |          22.4537 |           0.2704 |
[32m[20221213 15:25:30 @agent_ppo2.py:185][0m |          -0.0117 |          19.8824 |           0.2696 |
[32m[20221213 15:25:31 @agent_ppo2.py:185][0m |          -0.0156 |          19.8478 |           0.2698 |
[32m[20221213 15:25:31 @agent_ppo2.py:185][0m |          -0.0152 |          19.7854 |           0.2698 |
[32m[20221213 15:25:31 @agent_ppo2.py:185][0m |          -0.0158 |          19.7235 |           0.2696 |
[32m[20221213 15:25:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.20
[32m[20221213 15:25:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.34
[32m[20221213 15:25:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.02
[32m[20221213 15:25:31 @agent_ppo2.py:143][0m Total time:      32.60 min
[32m[20221213 15:25:31 @agent_ppo2.py:145][0m 2942976 total steps have happened
[32m[20221213 15:25:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1437 --------------------------#
[32m[20221213 15:25:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:31 @agent_ppo2.py:185][0m |          -0.0034 |          21.9617 |           0.2798 |
[32m[20221213 15:25:31 @agent_ppo2.py:185][0m |          -0.0101 |          21.6948 |           0.2791 |
[32m[20221213 15:25:31 @agent_ppo2.py:185][0m |          -0.0103 |          21.5751 |           0.2794 |
[32m[20221213 15:25:32 @agent_ppo2.py:185][0m |          -0.0134 |          21.4846 |           0.2797 |
[32m[20221213 15:25:32 @agent_ppo2.py:185][0m |          -0.0142 |          21.4264 |           0.2797 |
[32m[20221213 15:25:32 @agent_ppo2.py:185][0m |          -0.0146 |          21.4089 |           0.2799 |
[32m[20221213 15:25:32 @agent_ppo2.py:185][0m |          -0.0158 |          21.3323 |           0.2800 |
[32m[20221213 15:25:32 @agent_ppo2.py:185][0m |          -0.0171 |          21.3149 |           0.2803 |
[32m[20221213 15:25:32 @agent_ppo2.py:185][0m |          -0.0151 |          21.2864 |           0.2804 |
[32m[20221213 15:25:32 @agent_ppo2.py:185][0m |          -0.0122 |          21.6052 |           0.2806 |
[32m[20221213 15:25:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.16
[32m[20221213 15:25:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.13
[32m[20221213 15:25:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.70
[32m[20221213 15:25:32 @agent_ppo2.py:143][0m Total time:      32.62 min
[32m[20221213 15:25:32 @agent_ppo2.py:145][0m 2945024 total steps have happened
[32m[20221213 15:25:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1438 --------------------------#
[32m[20221213 15:25:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0008 |          21.7234 |           0.2768 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0021 |          21.8150 |           0.2769 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0086 |          21.2179 |           0.2764 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0112 |          21.0934 |           0.2764 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0102 |          20.9766 |           0.2760 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0110 |          20.9231 |           0.2757 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0155 |          20.8295 |           0.2759 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0104 |          20.8330 |           0.2757 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0148 |          20.7555 |           0.2756 |
[32m[20221213 15:25:33 @agent_ppo2.py:185][0m |          -0.0134 |          20.6911 |           0.2754 |
[32m[20221213 15:25:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.82
[32m[20221213 15:25:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.78
[32m[20221213 15:25:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.27
[32m[20221213 15:25:34 @agent_ppo2.py:143][0m Total time:      32.64 min
[32m[20221213 15:25:34 @agent_ppo2.py:145][0m 2947072 total steps have happened
[32m[20221213 15:25:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1439 --------------------------#
[32m[20221213 15:25:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:34 @agent_ppo2.py:185][0m |           0.0003 |          22.0016 |           0.2759 |
[32m[20221213 15:25:34 @agent_ppo2.py:185][0m |          -0.0042 |          21.7543 |           0.2759 |
[32m[20221213 15:25:34 @agent_ppo2.py:185][0m |          -0.0017 |          23.1309 |           0.2753 |
[32m[20221213 15:25:34 @agent_ppo2.py:185][0m |          -0.0103 |          21.5464 |           0.2754 |
[32m[20221213 15:25:34 @agent_ppo2.py:185][0m |          -0.0121 |          21.4415 |           0.2755 |
[32m[20221213 15:25:34 @agent_ppo2.py:185][0m |          -0.0103 |          21.5695 |           0.2754 |
[32m[20221213 15:25:34 @agent_ppo2.py:185][0m |          -0.0146 |          21.3524 |           0.2752 |
[32m[20221213 15:25:35 @agent_ppo2.py:185][0m |          -0.0137 |          21.2475 |           0.2756 |
[32m[20221213 15:25:35 @agent_ppo2.py:185][0m |          -0.0075 |          21.8249 |           0.2753 |
[32m[20221213 15:25:35 @agent_ppo2.py:185][0m |          -0.0171 |          21.1322 |           0.2754 |
[32m[20221213 15:25:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.25
[32m[20221213 15:25:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.55
[32m[20221213 15:25:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.34
[32m[20221213 15:25:35 @agent_ppo2.py:143][0m Total time:      32.66 min
[32m[20221213 15:25:35 @agent_ppo2.py:145][0m 2949120 total steps have happened
[32m[20221213 15:25:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1440 --------------------------#
[32m[20221213 15:25:35 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:25:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:35 @agent_ppo2.py:185][0m |           0.0014 |          21.0855 |           0.2820 |
[32m[20221213 15:25:35 @agent_ppo2.py:185][0m |          -0.0068 |          20.5468 |           0.2813 |
[32m[20221213 15:25:35 @agent_ppo2.py:185][0m |          -0.0098 |          20.3198 |           0.2811 |
[32m[20221213 15:25:36 @agent_ppo2.py:185][0m |          -0.0125 |          20.1783 |           0.2809 |
[32m[20221213 15:25:36 @agent_ppo2.py:185][0m |          -0.0137 |          20.0817 |           0.2809 |
[32m[20221213 15:25:36 @agent_ppo2.py:185][0m |          -0.0150 |          20.0156 |           0.2807 |
[32m[20221213 15:25:36 @agent_ppo2.py:185][0m |          -0.0155 |          19.9678 |           0.2806 |
[32m[20221213 15:25:36 @agent_ppo2.py:185][0m |          -0.0186 |          19.9320 |           0.2806 |
[32m[20221213 15:25:36 @agent_ppo2.py:185][0m |          -0.0180 |          19.8779 |           0.2807 |
[32m[20221213 15:25:36 @agent_ppo2.py:185][0m |          -0.0181 |          19.8508 |           0.2804 |
[32m[20221213 15:25:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.66
[32m[20221213 15:25:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.29
[32m[20221213 15:25:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.05
[32m[20221213 15:25:36 @agent_ppo2.py:143][0m Total time:      32.68 min
[32m[20221213 15:25:36 @agent_ppo2.py:145][0m 2951168 total steps have happened
[32m[20221213 15:25:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1441 --------------------------#
[32m[20221213 15:25:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |           0.0017 |          21.0923 |           0.2836 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0049 |          21.0148 |           0.2834 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0094 |          20.6480 |           0.2835 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0122 |          20.5404 |           0.2833 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0076 |          20.8677 |           0.2830 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0139 |          20.3465 |           0.2830 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0154 |          20.3177 |           0.2831 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0155 |          20.2285 |           0.2828 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0138 |          20.2027 |           0.2826 |
[32m[20221213 15:25:37 @agent_ppo2.py:185][0m |          -0.0155 |          20.0992 |           0.2825 |
[32m[20221213 15:25:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.07
[32m[20221213 15:25:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.64
[32m[20221213 15:25:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.10
[32m[20221213 15:25:38 @agent_ppo2.py:143][0m Total time:      32.71 min
[32m[20221213 15:25:38 @agent_ppo2.py:145][0m 2953216 total steps have happened
[32m[20221213 15:25:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1442 --------------------------#
[32m[20221213 15:25:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:38 @agent_ppo2.py:185][0m |          -0.0025 |          20.7967 |           0.2824 |
[32m[20221213 15:25:38 @agent_ppo2.py:185][0m |          -0.0063 |          20.3155 |           0.2815 |
[32m[20221213 15:25:38 @agent_ppo2.py:185][0m |          -0.0099 |          20.0509 |           0.2816 |
[32m[20221213 15:25:38 @agent_ppo2.py:185][0m |          -0.0070 |          19.9592 |           0.2814 |
[32m[20221213 15:25:38 @agent_ppo2.py:185][0m |          -0.0132 |          19.7606 |           0.2815 |
[32m[20221213 15:25:38 @agent_ppo2.py:185][0m |          -0.0114 |          19.6425 |           0.2814 |
[32m[20221213 15:25:38 @agent_ppo2.py:185][0m |          -0.0106 |          19.5851 |           0.2814 |
[32m[20221213 15:25:39 @agent_ppo2.py:185][0m |          -0.0140 |          19.4704 |           0.2813 |
[32m[20221213 15:25:39 @agent_ppo2.py:185][0m |          -0.0059 |          21.2451 |           0.2813 |
[32m[20221213 15:25:39 @agent_ppo2.py:185][0m |          -0.0125 |          19.3282 |           0.2810 |
[32m[20221213 15:25:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.99
[32m[20221213 15:25:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.45
[32m[20221213 15:25:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.17
[32m[20221213 15:25:39 @agent_ppo2.py:143][0m Total time:      32.73 min
[32m[20221213 15:25:39 @agent_ppo2.py:145][0m 2955264 total steps have happened
[32m[20221213 15:25:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1443 --------------------------#
[32m[20221213 15:25:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:39 @agent_ppo2.py:185][0m |          -0.0022 |          21.8284 |           0.2801 |
[32m[20221213 15:25:39 @agent_ppo2.py:185][0m |          -0.0090 |          21.4688 |           0.2798 |
[32m[20221213 15:25:39 @agent_ppo2.py:185][0m |          -0.0095 |          21.2650 |           0.2797 |
[32m[20221213 15:25:40 @agent_ppo2.py:185][0m |          -0.0019 |          22.1512 |           0.2798 |
[32m[20221213 15:25:40 @agent_ppo2.py:185][0m |          -0.0142 |          20.9720 |           0.2795 |
[32m[20221213 15:25:40 @agent_ppo2.py:185][0m |          -0.0062 |          21.9454 |           0.2798 |
[32m[20221213 15:25:40 @agent_ppo2.py:185][0m |          -0.0104 |          20.8596 |           0.2794 |
[32m[20221213 15:25:40 @agent_ppo2.py:185][0m |          -0.0164 |          20.6018 |           0.2796 |
[32m[20221213 15:25:40 @agent_ppo2.py:185][0m |          -0.0176 |          20.5470 |           0.2796 |
[32m[20221213 15:25:40 @agent_ppo2.py:185][0m |          -0.0179 |          20.5357 |           0.2797 |
[32m[20221213 15:25:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.50
[32m[20221213 15:25:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.01
[32m[20221213 15:25:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.48
[32m[20221213 15:25:40 @agent_ppo2.py:143][0m Total time:      32.75 min
[32m[20221213 15:25:40 @agent_ppo2.py:145][0m 2957312 total steps have happened
[32m[20221213 15:25:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1444 --------------------------#
[32m[20221213 15:25:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0005 |          21.8679 |           0.2757 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0003 |          21.8195 |           0.2749 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0075 |          21.1431 |           0.2744 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0101 |          20.8898 |           0.2743 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0102 |          20.8633 |           0.2739 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0134 |          20.5340 |           0.2736 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0140 |          20.3914 |           0.2736 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0170 |          20.2847 |           0.2734 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0186 |          20.1538 |           0.2734 |
[32m[20221213 15:25:41 @agent_ppo2.py:185][0m |          -0.0174 |          20.0498 |           0.2731 |
[32m[20221213 15:25:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.32
[32m[20221213 15:25:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.47
[32m[20221213 15:25:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.76
[32m[20221213 15:25:42 @agent_ppo2.py:143][0m Total time:      32.77 min
[32m[20221213 15:25:42 @agent_ppo2.py:145][0m 2959360 total steps have happened
[32m[20221213 15:25:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1445 --------------------------#
[32m[20221213 15:25:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:42 @agent_ppo2.py:185][0m |          -0.0003 |          21.8939 |           0.2816 |
[32m[20221213 15:25:42 @agent_ppo2.py:185][0m |          -0.0031 |          21.8812 |           0.2815 |
[32m[20221213 15:25:42 @agent_ppo2.py:185][0m |          -0.0096 |          21.2468 |           0.2814 |
[32m[20221213 15:25:42 @agent_ppo2.py:185][0m |          -0.0129 |          21.1293 |           0.2816 |
[32m[20221213 15:25:42 @agent_ppo2.py:185][0m |          -0.0011 |          22.4092 |           0.2812 |
[32m[20221213 15:25:42 @agent_ppo2.py:185][0m |          -0.0148 |          20.9657 |           0.2815 |
[32m[20221213 15:25:43 @agent_ppo2.py:185][0m |          -0.0159 |          20.8234 |           0.2816 |
[32m[20221213 15:25:43 @agent_ppo2.py:185][0m |          -0.0152 |          20.7940 |           0.2815 |
[32m[20221213 15:25:43 @agent_ppo2.py:185][0m |          -0.0158 |          20.6983 |           0.2815 |
[32m[20221213 15:25:43 @agent_ppo2.py:185][0m |          -0.0112 |          20.8677 |           0.2813 |
[32m[20221213 15:25:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.53
[32m[20221213 15:25:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.04
[32m[20221213 15:25:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.65
[32m[20221213 15:25:43 @agent_ppo2.py:143][0m Total time:      32.80 min
[32m[20221213 15:25:43 @agent_ppo2.py:145][0m 2961408 total steps have happened
[32m[20221213 15:25:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1446 --------------------------#
[32m[20221213 15:25:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:43 @agent_ppo2.py:185][0m |          -0.0034 |          21.1267 |           0.2847 |
[32m[20221213 15:25:43 @agent_ppo2.py:185][0m |          -0.0096 |          20.8491 |           0.2846 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0101 |          20.7248 |           0.2845 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0049 |          21.3070 |           0.2842 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0086 |          20.7564 |           0.2840 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0119 |          20.5423 |           0.2841 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0156 |          20.4785 |           0.2839 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0137 |          20.4510 |           0.2840 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0166 |          20.4167 |           0.2838 |
[32m[20221213 15:25:44 @agent_ppo2.py:185][0m |          -0.0179 |          20.3665 |           0.2830 |
[32m[20221213 15:25:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.04
[32m[20221213 15:25:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.84
[32m[20221213 15:25:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.27
[32m[20221213 15:25:44 @agent_ppo2.py:143][0m Total time:      32.82 min
[32m[20221213 15:25:44 @agent_ppo2.py:145][0m 2963456 total steps have happened
[32m[20221213 15:25:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1447 --------------------------#
[32m[20221213 15:25:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |           0.0045 |          22.4153 |           0.2773 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0053 |          21.6127 |           0.2765 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0086 |          21.4460 |           0.2755 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0098 |          21.4554 |           0.2757 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0137 |          21.3755 |           0.2759 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0043 |          22.3054 |           0.2752 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0076 |          21.6333 |           0.2750 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0138 |          21.2569 |           0.2748 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0142 |          21.2393 |           0.2748 |
[32m[20221213 15:25:45 @agent_ppo2.py:185][0m |          -0.0096 |          21.9900 |           0.2750 |
[32m[20221213 15:25:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:25:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.28
[32m[20221213 15:25:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.26
[32m[20221213 15:25:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.91
[32m[20221213 15:25:46 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 315.91
[32m[20221213 15:25:46 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 315.91
[32m[20221213 15:25:46 @agent_ppo2.py:143][0m Total time:      32.84 min
[32m[20221213 15:25:46 @agent_ppo2.py:145][0m 2965504 total steps have happened
[32m[20221213 15:25:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1448 --------------------------#
[32m[20221213 15:25:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:46 @agent_ppo2.py:185][0m |           0.0044 |          22.4286 |           0.2708 |
[32m[20221213 15:25:46 @agent_ppo2.py:185][0m |          -0.0049 |          21.8730 |           0.2707 |
[32m[20221213 15:25:46 @agent_ppo2.py:185][0m |          -0.0090 |          21.7322 |           0.2706 |
[32m[20221213 15:25:46 @agent_ppo2.py:185][0m |          -0.0115 |          21.6264 |           0.2706 |
[32m[20221213 15:25:46 @agent_ppo2.py:185][0m |          -0.0139 |          21.5307 |           0.2708 |
[32m[20221213 15:25:46 @agent_ppo2.py:185][0m |          -0.0133 |          21.4886 |           0.2711 |
[32m[20221213 15:25:47 @agent_ppo2.py:185][0m |          -0.0150 |          21.4221 |           0.2708 |
[32m[20221213 15:25:47 @agent_ppo2.py:185][0m |          -0.0150 |          21.3526 |           0.2713 |
[32m[20221213 15:25:47 @agent_ppo2.py:185][0m |          -0.0163 |          21.2867 |           0.2713 |
[32m[20221213 15:25:47 @agent_ppo2.py:185][0m |          -0.0159 |          21.2647 |           0.2712 |
[32m[20221213 15:25:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.45
[32m[20221213 15:25:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.48
[32m[20221213 15:25:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.14
[32m[20221213 15:25:47 @agent_ppo2.py:143][0m Total time:      32.86 min
[32m[20221213 15:25:47 @agent_ppo2.py:145][0m 2967552 total steps have happened
[32m[20221213 15:25:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1449 --------------------------#
[32m[20221213 15:25:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:47 @agent_ppo2.py:185][0m |          -0.0023 |          22.1768 |           0.2836 |
[32m[20221213 15:25:47 @agent_ppo2.py:185][0m |          -0.0084 |          22.0422 |           0.2831 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0098 |          21.9225 |           0.2828 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0144 |          21.8314 |           0.2826 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0112 |          21.8568 |           0.2825 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0154 |          21.6928 |           0.2823 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0137 |          21.6657 |           0.2824 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0187 |          21.5636 |           0.2823 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0191 |          21.5120 |           0.2821 |
[32m[20221213 15:25:48 @agent_ppo2.py:185][0m |          -0.0194 |          21.4826 |           0.2823 |
[32m[20221213 15:25:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.77
[32m[20221213 15:25:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.53
[32m[20221213 15:25:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.14
[32m[20221213 15:25:48 @agent_ppo2.py:143][0m Total time:      32.89 min
[32m[20221213 15:25:48 @agent_ppo2.py:145][0m 2969600 total steps have happened
[32m[20221213 15:25:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1450 --------------------------#
[32m[20221213 15:25:49 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:25:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0020 |          21.4324 |           0.2762 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |           0.0000 |          21.6959 |           0.2750 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0075 |          21.1936 |           0.2753 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0099 |          21.1144 |           0.2751 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0108 |          21.0855 |           0.2748 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0114 |          20.9865 |           0.2748 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0121 |          20.9110 |           0.2745 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0134 |          20.9141 |           0.2747 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0137 |          20.8668 |           0.2746 |
[32m[20221213 15:25:49 @agent_ppo2.py:185][0m |          -0.0089 |          21.0572 |           0.2748 |
[32m[20221213 15:25:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.85
[32m[20221213 15:25:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.71
[32m[20221213 15:25:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.24
[32m[20221213 15:25:50 @agent_ppo2.py:143][0m Total time:      32.91 min
[32m[20221213 15:25:50 @agent_ppo2.py:145][0m 2971648 total steps have happened
[32m[20221213 15:25:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1451 --------------------------#
[32m[20221213 15:25:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:50 @agent_ppo2.py:185][0m |          -0.0005 |          21.1728 |           0.2809 |
[32m[20221213 15:25:50 @agent_ppo2.py:185][0m |          -0.0042 |          21.1479 |           0.2800 |
[32m[20221213 15:25:50 @agent_ppo2.py:185][0m |          -0.0088 |          21.0718 |           0.2801 |
[32m[20221213 15:25:50 @agent_ppo2.py:185][0m |          -0.0092 |          21.0037 |           0.2797 |
[32m[20221213 15:25:50 @agent_ppo2.py:185][0m |          -0.0114 |          20.9399 |           0.2801 |
[32m[20221213 15:25:50 @agent_ppo2.py:185][0m |          -0.0122 |          20.9098 |           0.2803 |
[32m[20221213 15:25:51 @agent_ppo2.py:185][0m |          -0.0134 |          20.8550 |           0.2802 |
[32m[20221213 15:25:51 @agent_ppo2.py:185][0m |          -0.0122 |          20.8864 |           0.2802 |
[32m[20221213 15:25:51 @agent_ppo2.py:185][0m |          -0.0069 |          21.6930 |           0.2803 |
[32m[20221213 15:25:51 @agent_ppo2.py:185][0m |          -0.0088 |          21.6281 |           0.2802 |
[32m[20221213 15:25:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.42
[32m[20221213 15:25:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.08
[32m[20221213 15:25:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.52
[32m[20221213 15:25:51 @agent_ppo2.py:143][0m Total time:      32.93 min
[32m[20221213 15:25:51 @agent_ppo2.py:145][0m 2973696 total steps have happened
[32m[20221213 15:25:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1452 --------------------------#
[32m[20221213 15:25:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:51 @agent_ppo2.py:185][0m |           0.0008 |          21.5243 |           0.2795 |
[32m[20221213 15:25:51 @agent_ppo2.py:185][0m |           0.0063 |          22.5501 |           0.2786 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0108 |          20.2083 |           0.2777 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0131 |          19.9504 |           0.2771 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0136 |          19.7941 |           0.2764 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0163 |          19.7176 |           0.2761 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0129 |          19.9169 |           0.2763 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0188 |          19.5581 |           0.2761 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0192 |          19.5500 |           0.2757 |
[32m[20221213 15:25:52 @agent_ppo2.py:185][0m |          -0.0189 |          19.3696 |           0.2757 |
[32m[20221213 15:25:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.61
[32m[20221213 15:25:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.85
[32m[20221213 15:25:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.71
[32m[20221213 15:25:52 @agent_ppo2.py:143][0m Total time:      32.95 min
[32m[20221213 15:25:52 @agent_ppo2.py:145][0m 2975744 total steps have happened
[32m[20221213 15:25:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1453 --------------------------#
[32m[20221213 15:25:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |           0.0001 |          21.9734 |           0.2813 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0077 |          21.6692 |           0.2803 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |           0.0019 |          22.6262 |           0.2800 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0073 |          21.5267 |           0.2788 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0108 |          21.4609 |           0.2796 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0118 |          21.4301 |           0.2794 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0130 |          21.3507 |           0.2793 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0147 |          21.3737 |           0.2793 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0139 |          21.2950 |           0.2792 |
[32m[20221213 15:25:53 @agent_ppo2.py:185][0m |          -0.0139 |          21.2578 |           0.2791 |
[32m[20221213 15:25:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.54
[32m[20221213 15:25:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.35
[32m[20221213 15:25:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.84
[32m[20221213 15:25:54 @agent_ppo2.py:143][0m Total time:      32.98 min
[32m[20221213 15:25:54 @agent_ppo2.py:145][0m 2977792 total steps have happened
[32m[20221213 15:25:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1454 --------------------------#
[32m[20221213 15:25:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:54 @agent_ppo2.py:185][0m |          -0.0027 |          22.4134 |           0.2808 |
[32m[20221213 15:25:54 @agent_ppo2.py:185][0m |          -0.0056 |          22.0548 |           0.2802 |
[32m[20221213 15:25:54 @agent_ppo2.py:185][0m |          -0.0096 |          21.8877 |           0.2804 |
[32m[20221213 15:25:54 @agent_ppo2.py:185][0m |          -0.0142 |          21.7704 |           0.2803 |
[32m[20221213 15:25:54 @agent_ppo2.py:185][0m |          -0.0121 |          21.6516 |           0.2804 |
[32m[20221213 15:25:55 @agent_ppo2.py:185][0m |          -0.0114 |          21.6243 |           0.2804 |
[32m[20221213 15:25:55 @agent_ppo2.py:185][0m |          -0.0140 |          21.4728 |           0.2807 |
[32m[20221213 15:25:55 @agent_ppo2.py:185][0m |          -0.0145 |          21.4175 |           0.2806 |
[32m[20221213 15:25:55 @agent_ppo2.py:185][0m |          -0.0152 |          21.2817 |           0.2812 |
[32m[20221213 15:25:55 @agent_ppo2.py:185][0m |          -0.0143 |          21.2733 |           0.2809 |
[32m[20221213 15:25:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.70
[32m[20221213 15:25:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.47
[32m[20221213 15:25:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.41
[32m[20221213 15:25:55 @agent_ppo2.py:143][0m Total time:      33.00 min
[32m[20221213 15:25:55 @agent_ppo2.py:145][0m 2979840 total steps have happened
[32m[20221213 15:25:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1455 --------------------------#
[32m[20221213 15:25:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:55 @agent_ppo2.py:185][0m |          -0.0015 |          20.5682 |           0.2815 |
[32m[20221213 15:25:55 @agent_ppo2.py:185][0m |          -0.0092 |          18.8116 |           0.2810 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0114 |          17.8118 |           0.2802 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0063 |          17.9895 |           0.2802 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0141 |          17.3172 |           0.2796 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0163 |          17.2063 |           0.2795 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0150 |          17.1033 |           0.2791 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0160 |          17.0252 |           0.2789 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0165 |          16.9448 |           0.2786 |
[32m[20221213 15:25:56 @agent_ppo2.py:185][0m |          -0.0177 |          16.9001 |           0.2781 |
[32m[20221213 15:25:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.61
[32m[20221213 15:25:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.48
[32m[20221213 15:25:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.12
[32m[20221213 15:25:56 @agent_ppo2.py:143][0m Total time:      33.02 min
[32m[20221213 15:25:56 @agent_ppo2.py:145][0m 2981888 total steps have happened
[32m[20221213 15:25:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1456 --------------------------#
[32m[20221213 15:25:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |          -0.0019 |          23.9586 |           0.2736 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |           0.0020 |          25.0474 |           0.2731 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |           0.0003 |          25.6477 |           0.2728 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |          -0.0114 |          22.4498 |           0.2726 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |          -0.0132 |          22.2488 |           0.2724 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |          -0.0083 |          22.6464 |           0.2722 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |          -0.0124 |          22.0072 |           0.2720 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |          -0.0126 |          21.8726 |           0.2718 |
[32m[20221213 15:25:57 @agent_ppo2.py:185][0m |          -0.0167 |          21.8212 |           0.2720 |
[32m[20221213 15:25:58 @agent_ppo2.py:185][0m |          -0.0178 |          21.7060 |           0.2721 |
[32m[20221213 15:25:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:25:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.07
[32m[20221213 15:25:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.09
[32m[20221213 15:25:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.46
[32m[20221213 15:25:58 @agent_ppo2.py:143][0m Total time:      33.04 min
[32m[20221213 15:25:58 @agent_ppo2.py:145][0m 2983936 total steps have happened
[32m[20221213 15:25:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1457 --------------------------#
[32m[20221213 15:25:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:25:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:58 @agent_ppo2.py:185][0m |          -0.0017 |          21.0351 |           0.2679 |
[32m[20221213 15:25:58 @agent_ppo2.py:185][0m |          -0.0070 |          20.1115 |           0.2667 |
[32m[20221213 15:25:58 @agent_ppo2.py:185][0m |          -0.0105 |          19.5597 |           0.2665 |
[32m[20221213 15:25:58 @agent_ppo2.py:185][0m |          -0.0120 |          19.1601 |           0.2664 |
[32m[20221213 15:25:58 @agent_ppo2.py:185][0m |          -0.0134 |          18.8087 |           0.2661 |
[32m[20221213 15:25:59 @agent_ppo2.py:185][0m |          -0.0146 |          18.5523 |           0.2657 |
[32m[20221213 15:25:59 @agent_ppo2.py:185][0m |          -0.0144 |          18.3323 |           0.2660 |
[32m[20221213 15:25:59 @agent_ppo2.py:185][0m |          -0.0115 |          18.3166 |           0.2659 |
[32m[20221213 15:25:59 @agent_ppo2.py:185][0m |          -0.0166 |          17.9658 |           0.2658 |
[32m[20221213 15:25:59 @agent_ppo2.py:185][0m |          -0.0184 |          17.7691 |           0.2656 |
[32m[20221213 15:25:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:25:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.06
[32m[20221213 15:25:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.50
[32m[20221213 15:25:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.54
[32m[20221213 15:25:59 @agent_ppo2.py:143][0m Total time:      33.06 min
[32m[20221213 15:25:59 @agent_ppo2.py:145][0m 2985984 total steps have happened
[32m[20221213 15:25:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1458 --------------------------#
[32m[20221213 15:25:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:25:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:25:59 @agent_ppo2.py:185][0m |           0.0020 |          24.3664 |           0.2763 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0075 |          23.1530 |           0.2762 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0103 |          22.9681 |           0.2762 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0112 |          22.8673 |           0.2762 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0169 |          22.7772 |           0.2763 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0143 |          22.7479 |           0.2766 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0202 |          22.6772 |           0.2767 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0166 |          22.6076 |           0.2769 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0176 |          22.5479 |           0.2771 |
[32m[20221213 15:26:00 @agent_ppo2.py:185][0m |          -0.0163 |          22.4958 |           0.2772 |
[32m[20221213 15:26:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.12
[32m[20221213 15:26:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.15
[32m[20221213 15:26:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.87
[32m[20221213 15:26:00 @agent_ppo2.py:143][0m Total time:      33.09 min
[32m[20221213 15:26:00 @agent_ppo2.py:145][0m 2988032 total steps have happened
[32m[20221213 15:26:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1459 --------------------------#
[32m[20221213 15:26:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0015 |          21.7727 |           0.2699 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0072 |          21.5324 |           0.2691 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0088 |          21.3975 |           0.2693 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0029 |          21.8178 |           0.2691 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0106 |          21.2285 |           0.2693 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0107 |          21.1155 |           0.2697 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0128 |          21.0577 |           0.2698 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0051 |          22.0037 |           0.2694 |
[32m[20221213 15:26:01 @agent_ppo2.py:185][0m |          -0.0138 |          20.9405 |           0.2697 |
[32m[20221213 15:26:02 @agent_ppo2.py:185][0m |          -0.0084 |          21.3792 |           0.2699 |
[32m[20221213 15:26:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.01
[32m[20221213 15:26:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.32
[32m[20221213 15:26:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.12
[32m[20221213 15:26:02 @agent_ppo2.py:143][0m Total time:      33.11 min
[32m[20221213 15:26:02 @agent_ppo2.py:145][0m 2990080 total steps have happened
[32m[20221213 15:26:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1460 --------------------------#
[32m[20221213 15:26:02 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:26:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:02 @agent_ppo2.py:185][0m |          -0.0011 |          21.4629 |           0.2817 |
[32m[20221213 15:26:02 @agent_ppo2.py:185][0m |          -0.0067 |          20.6180 |           0.2820 |
[32m[20221213 15:26:02 @agent_ppo2.py:185][0m |          -0.0076 |          20.1043 |           0.2823 |
[32m[20221213 15:26:02 @agent_ppo2.py:185][0m |          -0.0107 |          19.7837 |           0.2824 |
[32m[20221213 15:26:02 @agent_ppo2.py:185][0m |          -0.0137 |          19.5659 |           0.2824 |
[32m[20221213 15:26:03 @agent_ppo2.py:185][0m |          -0.0099 |          19.9341 |           0.2825 |
[32m[20221213 15:26:03 @agent_ppo2.py:185][0m |          -0.0149 |          19.2892 |           0.2822 |
[32m[20221213 15:26:03 @agent_ppo2.py:185][0m |          -0.0155 |          19.1069 |           0.2826 |
[32m[20221213 15:26:03 @agent_ppo2.py:185][0m |          -0.0169 |          19.0142 |           0.2825 |
[32m[20221213 15:26:03 @agent_ppo2.py:185][0m |          -0.0172 |          18.8377 |           0.2823 |
[32m[20221213 15:26:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.26
[32m[20221213 15:26:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.14
[32m[20221213 15:26:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.06
[32m[20221213 15:26:03 @agent_ppo2.py:143][0m Total time:      33.13 min
[32m[20221213 15:26:03 @agent_ppo2.py:145][0m 2992128 total steps have happened
[32m[20221213 15:26:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1461 --------------------------#
[32m[20221213 15:26:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:03 @agent_ppo2.py:185][0m |           0.0228 |          26.0002 |           0.2804 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |           0.0050 |          23.8832 |           0.2789 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0002 |          23.7399 |           0.2793 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0111 |          22.3485 |           0.2784 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0118 |          22.2180 |           0.2786 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0148 |          22.1509 |           0.2784 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0173 |          22.0416 |           0.2782 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0150 |          22.0061 |           0.2779 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0088 |          23.5499 |           0.2779 |
[32m[20221213 15:26:04 @agent_ppo2.py:185][0m |          -0.0177 |          21.9310 |           0.2772 |
[32m[20221213 15:26:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.52
[32m[20221213 15:26:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.35
[32m[20221213 15:26:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.71
[32m[20221213 15:26:04 @agent_ppo2.py:143][0m Total time:      33.15 min
[32m[20221213 15:26:04 @agent_ppo2.py:145][0m 2994176 total steps have happened
[32m[20221213 15:26:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1462 --------------------------#
[32m[20221213 15:26:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0001 |          23.3026 |           0.2730 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0083 |          23.1008 |           0.2723 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0102 |          22.9287 |           0.2716 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0077 |          23.0927 |           0.2714 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0121 |          22.7664 |           0.2709 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0139 |          22.7143 |           0.2708 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |           0.0008 |          25.9480 |           0.2704 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0156 |          22.7501 |           0.2695 |
[32m[20221213 15:26:05 @agent_ppo2.py:185][0m |          -0.0126 |          22.8465 |           0.2696 |
[32m[20221213 15:26:06 @agent_ppo2.py:185][0m |          -0.0177 |          22.5280 |           0.2694 |
[32m[20221213 15:26:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.05
[32m[20221213 15:26:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.29
[32m[20221213 15:26:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.05
[32m[20221213 15:26:06 @agent_ppo2.py:143][0m Total time:      33.18 min
[32m[20221213 15:26:06 @agent_ppo2.py:145][0m 2996224 total steps have happened
[32m[20221213 15:26:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1463 --------------------------#
[32m[20221213 15:26:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:06 @agent_ppo2.py:185][0m |          -0.0022 |          22.1069 |           0.2709 |
[32m[20221213 15:26:06 @agent_ppo2.py:185][0m |          -0.0104 |          21.8408 |           0.2704 |
[32m[20221213 15:26:06 @agent_ppo2.py:185][0m |          -0.0133 |          21.6849 |           0.2702 |
[32m[20221213 15:26:06 @agent_ppo2.py:185][0m |          -0.0147 |          21.5680 |           0.2702 |
[32m[20221213 15:26:06 @agent_ppo2.py:185][0m |          -0.0118 |          21.7496 |           0.2697 |
[32m[20221213 15:26:07 @agent_ppo2.py:185][0m |          -0.0174 |          21.2731 |           0.2696 |
[32m[20221213 15:26:07 @agent_ppo2.py:185][0m |          -0.0142 |          21.2342 |           0.2696 |
[32m[20221213 15:26:07 @agent_ppo2.py:185][0m |          -0.0181 |          21.0631 |           0.2694 |
[32m[20221213 15:26:07 @agent_ppo2.py:185][0m |          -0.0179 |          20.9781 |           0.2694 |
[32m[20221213 15:26:07 @agent_ppo2.py:185][0m |          -0.0194 |          20.8824 |           0.2690 |
[32m[20221213 15:26:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.64
[32m[20221213 15:26:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.75
[32m[20221213 15:26:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.95
[32m[20221213 15:26:07 @agent_ppo2.py:143][0m Total time:      33.20 min
[32m[20221213 15:26:07 @agent_ppo2.py:145][0m 2998272 total steps have happened
[32m[20221213 15:26:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1464 --------------------------#
[32m[20221213 15:26:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0023 |          20.5244 |           0.2670 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0045 |          20.2702 |           0.2664 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0116 |          19.2336 |           0.2660 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0150 |          18.8844 |           0.2663 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0167 |          18.5774 |           0.2662 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0196 |          18.4053 |           0.2658 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0189 |          18.1564 |           0.2655 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0198 |          17.8935 |           0.2654 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0225 |          17.7670 |           0.2653 |
[32m[20221213 15:26:08 @agent_ppo2.py:185][0m |          -0.0211 |          17.6455 |           0.2653 |
[32m[20221213 15:26:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:26:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.11
[32m[20221213 15:26:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.76
[32m[20221213 15:26:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.02
[32m[20221213 15:26:08 @agent_ppo2.py:143][0m Total time:      33.22 min
[32m[20221213 15:26:08 @agent_ppo2.py:145][0m 3000320 total steps have happened
[32m[20221213 15:26:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1465 --------------------------#
[32m[20221213 15:26:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0013 |          22.7020 |           0.2700 |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0023 |          22.5658 |           0.2692 |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0096 |          21.8537 |           0.2686 |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0117 |          21.6973 |           0.2688 |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0157 |          21.5913 |           0.2684 |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0163 |          21.5456 |           0.2682 |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0162 |          21.5031 |           0.2681 |
[32m[20221213 15:26:09 @agent_ppo2.py:185][0m |          -0.0171 |          21.4376 |           0.2679 |
[32m[20221213 15:26:10 @agent_ppo2.py:185][0m |          -0.0032 |          24.7933 |           0.2678 |
[32m[20221213 15:26:10 @agent_ppo2.py:185][0m |          -0.0175 |          21.3845 |           0.2669 |
[32m[20221213 15:26:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.96
[32m[20221213 15:26:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.39
[32m[20221213 15:26:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.70
[32m[20221213 15:26:10 @agent_ppo2.py:143][0m Total time:      33.24 min
[32m[20221213 15:26:10 @agent_ppo2.py:145][0m 3002368 total steps have happened
[32m[20221213 15:26:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1466 --------------------------#
[32m[20221213 15:26:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:10 @agent_ppo2.py:185][0m |           0.0100 |          22.3069 |           0.2540 |
[32m[20221213 15:26:10 @agent_ppo2.py:185][0m |           0.0019 |          21.9878 |           0.2538 |
[32m[20221213 15:26:10 @agent_ppo2.py:185][0m |          -0.0081 |          20.8379 |           0.2535 |
[32m[20221213 15:26:10 @agent_ppo2.py:185][0m |          -0.0103 |          20.7383 |           0.2534 |
[32m[20221213 15:26:11 @agent_ppo2.py:185][0m |          -0.0106 |          20.6520 |           0.2531 |
[32m[20221213 15:26:11 @agent_ppo2.py:185][0m |          -0.0113 |          20.5814 |           0.2532 |
[32m[20221213 15:26:11 @agent_ppo2.py:185][0m |          -0.0099 |          20.5586 |           0.2532 |
[32m[20221213 15:26:11 @agent_ppo2.py:185][0m |          -0.0122 |          20.4446 |           0.2531 |
[32m[20221213 15:26:11 @agent_ppo2.py:185][0m |          -0.0138 |          20.3965 |           0.2528 |
[32m[20221213 15:26:11 @agent_ppo2.py:185][0m |          -0.0129 |          20.3465 |           0.2527 |
[32m[20221213 15:26:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.61
[32m[20221213 15:26:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.72
[32m[20221213 15:26:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.47
[32m[20221213 15:26:11 @agent_ppo2.py:143][0m Total time:      33.27 min
[32m[20221213 15:26:11 @agent_ppo2.py:145][0m 3004416 total steps have happened
[32m[20221213 15:26:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1467 --------------------------#
[32m[20221213 15:26:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |           0.0071 |          23.2606 |           0.2640 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |           0.0031 |          23.4471 |           0.2630 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0071 |          21.6905 |           0.2626 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0037 |          22.4473 |           0.2626 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0110 |          21.3951 |           0.2624 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0130 |          21.1674 |           0.2623 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0142 |          21.0379 |           0.2621 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0120 |          20.9114 |           0.2620 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0162 |          20.8198 |           0.2619 |
[32m[20221213 15:26:12 @agent_ppo2.py:185][0m |          -0.0138 |          20.6994 |           0.2618 |
[32m[20221213 15:26:12 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.18
[32m[20221213 15:26:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.72
[32m[20221213 15:26:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.90
[32m[20221213 15:26:13 @agent_ppo2.py:143][0m Total time:      33.29 min
[32m[20221213 15:26:13 @agent_ppo2.py:145][0m 3006464 total steps have happened
[32m[20221213 15:26:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1468 --------------------------#
[32m[20221213 15:26:13 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:26:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:13 @agent_ppo2.py:185][0m |           0.0006 |          22.9306 |           0.2638 |
[32m[20221213 15:26:13 @agent_ppo2.py:185][0m |          -0.0039 |          22.4016 |           0.2634 |
[32m[20221213 15:26:13 @agent_ppo2.py:185][0m |          -0.0051 |          22.1856 |           0.2634 |
[32m[20221213 15:26:13 @agent_ppo2.py:185][0m |          -0.0086 |          22.0997 |           0.2635 |
[32m[20221213 15:26:13 @agent_ppo2.py:185][0m |          -0.0022 |          23.5825 |           0.2633 |
[32m[20221213 15:26:13 @agent_ppo2.py:185][0m |          -0.0108 |          21.9340 |           0.2631 |
[32m[20221213 15:26:13 @agent_ppo2.py:185][0m |          -0.0128 |          21.8964 |           0.2635 |
[32m[20221213 15:26:14 @agent_ppo2.py:185][0m |          -0.0144 |          21.8540 |           0.2628 |
[32m[20221213 15:26:14 @agent_ppo2.py:185][0m |          -0.0144 |          21.8097 |           0.2636 |
[32m[20221213 15:26:14 @agent_ppo2.py:185][0m |          -0.0143 |          21.7832 |           0.2634 |
[32m[20221213 15:26:14 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.41
[32m[20221213 15:26:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.12
[32m[20221213 15:26:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.57
[32m[20221213 15:26:14 @agent_ppo2.py:143][0m Total time:      33.31 min
[32m[20221213 15:26:14 @agent_ppo2.py:145][0m 3008512 total steps have happened
[32m[20221213 15:26:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1469 --------------------------#
[32m[20221213 15:26:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:14 @agent_ppo2.py:185][0m |           0.0008 |          21.2914 |           0.2583 |
[32m[20221213 15:26:14 @agent_ppo2.py:185][0m |          -0.0039 |          21.0532 |           0.2579 |
[32m[20221213 15:26:14 @agent_ppo2.py:185][0m |          -0.0092 |          20.8819 |           0.2579 |
[32m[20221213 15:26:15 @agent_ppo2.py:185][0m |          -0.0109 |          20.7944 |           0.2580 |
[32m[20221213 15:26:15 @agent_ppo2.py:185][0m |           0.0049 |          23.1712 |           0.2582 |
[32m[20221213 15:26:15 @agent_ppo2.py:185][0m |          -0.0123 |          20.6056 |           0.2579 |
[32m[20221213 15:26:15 @agent_ppo2.py:185][0m |          -0.0132 |          20.4728 |           0.2578 |
[32m[20221213 15:26:15 @agent_ppo2.py:185][0m |          -0.0146 |          20.4356 |           0.2578 |
[32m[20221213 15:26:15 @agent_ppo2.py:185][0m |          -0.0136 |          20.2926 |           0.2579 |
[32m[20221213 15:26:15 @agent_ppo2.py:185][0m |          -0.0168 |          20.2245 |           0.2576 |
[32m[20221213 15:26:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:26:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.75
[32m[20221213 15:26:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.71
[32m[20221213 15:26:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.73
[32m[20221213 15:26:15 @agent_ppo2.py:143][0m Total time:      33.33 min
[32m[20221213 15:26:15 @agent_ppo2.py:145][0m 3010560 total steps have happened
[32m[20221213 15:26:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1470 --------------------------#
[32m[20221213 15:26:15 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:26:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |           0.0035 |          22.4944 |           0.2648 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0093 |          21.7051 |           0.2647 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0101 |          21.5660 |           0.2648 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0114 |          21.4068 |           0.2647 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0134 |          21.3295 |           0.2648 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0155 |          21.2430 |           0.2650 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0124 |          21.2543 |           0.2651 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0023 |          23.7491 |           0.2651 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0152 |          21.0668 |           0.2649 |
[32m[20221213 15:26:16 @agent_ppo2.py:185][0m |          -0.0165 |          21.0421 |           0.2652 |
[32m[20221213 15:26:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.68
[32m[20221213 15:26:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.22
[32m[20221213 15:26:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.42
[32m[20221213 15:26:17 @agent_ppo2.py:143][0m Total time:      33.36 min
[32m[20221213 15:26:17 @agent_ppo2.py:145][0m 3012608 total steps have happened
[32m[20221213 15:26:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1471 --------------------------#
[32m[20221213 15:26:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:17 @agent_ppo2.py:185][0m |           0.0004 |          21.7352 |           0.2669 |
[32m[20221213 15:26:17 @agent_ppo2.py:185][0m |          -0.0050 |          21.4345 |           0.2664 |
[32m[20221213 15:26:17 @agent_ppo2.py:185][0m |          -0.0016 |          21.6688 |           0.2663 |
[32m[20221213 15:26:17 @agent_ppo2.py:185][0m |          -0.0106 |          21.2011 |           0.2659 |
[32m[20221213 15:26:17 @agent_ppo2.py:185][0m |           0.0025 |          23.2500 |           0.2657 |
[32m[20221213 15:26:17 @agent_ppo2.py:185][0m |          -0.0104 |          21.0310 |           0.2649 |
[32m[20221213 15:26:17 @agent_ppo2.py:185][0m |          -0.0149 |          20.9094 |           0.2655 |
[32m[20221213 15:26:18 @agent_ppo2.py:185][0m |          -0.0162 |          20.8447 |           0.2655 |
[32m[20221213 15:26:18 @agent_ppo2.py:185][0m |          -0.0111 |          21.0231 |           0.2650 |
[32m[20221213 15:26:18 @agent_ppo2.py:185][0m |          -0.0180 |          20.7257 |           0.2653 |
[32m[20221213 15:26:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:26:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.38
[32m[20221213 15:26:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.97
[32m[20221213 15:26:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.12
[32m[20221213 15:26:18 @agent_ppo2.py:143][0m Total time:      33.38 min
[32m[20221213 15:26:18 @agent_ppo2.py:145][0m 3014656 total steps have happened
[32m[20221213 15:26:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1472 --------------------------#
[32m[20221213 15:26:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:18 @agent_ppo2.py:185][0m |           0.0012 |          21.3684 |           0.2640 |
[32m[20221213 15:26:18 @agent_ppo2.py:185][0m |          -0.0049 |          20.9371 |           0.2635 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0125 |          20.3671 |           0.2641 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0152 |          20.1482 |           0.2640 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0150 |          19.9493 |           0.2641 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0184 |          19.7994 |           0.2642 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0170 |          19.6592 |           0.2646 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0197 |          19.5092 |           0.2648 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0069 |          20.6720 |           0.2644 |
[32m[20221213 15:26:19 @agent_ppo2.py:185][0m |          -0.0181 |          19.3894 |           0.2646 |
[32m[20221213 15:26:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:26:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.83
[32m[20221213 15:26:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.91
[32m[20221213 15:26:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.76
[32m[20221213 15:26:19 @agent_ppo2.py:143][0m Total time:      33.40 min
[32m[20221213 15:26:19 @agent_ppo2.py:145][0m 3016704 total steps have happened
[32m[20221213 15:26:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1473 --------------------------#
[32m[20221213 15:26:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |           0.0042 |          22.5401 |           0.2682 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0015 |          22.0893 |           0.2679 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0078 |          21.7434 |           0.2677 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0099 |          21.6342 |           0.2673 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0093 |          21.5421 |           0.2671 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0137 |          21.4514 |           0.2671 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0135 |          21.3615 |           0.2671 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0122 |          21.3315 |           0.2669 |
[32m[20221213 15:26:20 @agent_ppo2.py:185][0m |          -0.0089 |          21.3127 |           0.2665 |
[32m[20221213 15:26:21 @agent_ppo2.py:185][0m |          -0.0130 |          21.2411 |           0.2665 |
[32m[20221213 15:26:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.70
[32m[20221213 15:26:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.06
[32m[20221213 15:26:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.91
[32m[20221213 15:26:21 @agent_ppo2.py:143][0m Total time:      33.43 min
[32m[20221213 15:26:21 @agent_ppo2.py:145][0m 3018752 total steps have happened
[32m[20221213 15:26:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1474 --------------------------#
[32m[20221213 15:26:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:21 @agent_ppo2.py:185][0m |          -0.0042 |          21.6529 |           0.2609 |
[32m[20221213 15:26:21 @agent_ppo2.py:185][0m |          -0.0011 |          22.2195 |           0.2605 |
[32m[20221213 15:26:21 @agent_ppo2.py:185][0m |          -0.0144 |          21.4238 |           0.2602 |
[32m[20221213 15:26:21 @agent_ppo2.py:185][0m |          -0.0147 |          21.1478 |           0.2602 |
[32m[20221213 15:26:21 @agent_ppo2.py:185][0m |          -0.0167 |          21.0516 |           0.2603 |
[32m[20221213 15:26:22 @agent_ppo2.py:185][0m |          -0.0178 |          20.9319 |           0.2606 |
[32m[20221213 15:26:22 @agent_ppo2.py:185][0m |          -0.0179 |          20.8060 |           0.2604 |
[32m[20221213 15:26:22 @agent_ppo2.py:185][0m |          -0.0184 |          20.7306 |           0.2603 |
[32m[20221213 15:26:22 @agent_ppo2.py:185][0m |          -0.0121 |          21.0181 |           0.2602 |
[32m[20221213 15:26:22 @agent_ppo2.py:185][0m |          -0.0195 |          20.4981 |           0.2603 |
[32m[20221213 15:26:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.47
[32m[20221213 15:26:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.28
[32m[20221213 15:26:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.50
[32m[20221213 15:26:22 @agent_ppo2.py:143][0m Total time:      33.45 min
[32m[20221213 15:26:22 @agent_ppo2.py:145][0m 3020800 total steps have happened
[32m[20221213 15:26:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1475 --------------------------#
[32m[20221213 15:26:22 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:26:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:22 @agent_ppo2.py:185][0m |          -0.0016 |          21.8365 |           0.2588 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0034 |          22.0334 |           0.2587 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0008 |          22.2275 |           0.2582 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0110 |          21.1793 |           0.2582 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0124 |          21.0260 |           0.2584 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0144 |          20.9427 |           0.2579 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0162 |          20.8953 |           0.2581 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0157 |          20.8391 |           0.2579 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0172 |          20.7838 |           0.2579 |
[32m[20221213 15:26:23 @agent_ppo2.py:185][0m |          -0.0184 |          20.7401 |           0.2576 |
[32m[20221213 15:26:23 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:26:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.25
[32m[20221213 15:26:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.94
[32m[20221213 15:26:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.57
[32m[20221213 15:26:23 @agent_ppo2.py:143][0m Total time:      33.47 min
[32m[20221213 15:26:23 @agent_ppo2.py:145][0m 3022848 total steps have happened
[32m[20221213 15:26:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1476 --------------------------#
[32m[20221213 15:26:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0014 |          21.7264 |           0.2648 |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0024 |          21.9019 |           0.2642 |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0127 |          21.2187 |           0.2640 |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0134 |          21.1148 |           0.2634 |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0148 |          21.0375 |           0.2633 |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0163 |          20.9458 |           0.2630 |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0116 |          21.3123 |           0.2630 |
[32m[20221213 15:26:24 @agent_ppo2.py:185][0m |          -0.0161 |          20.8012 |           0.2626 |
[32m[20221213 15:26:25 @agent_ppo2.py:185][0m |          -0.0176 |          20.7391 |           0.2626 |
[32m[20221213 15:26:25 @agent_ppo2.py:185][0m |          -0.0172 |          20.6728 |           0.2623 |
[32m[20221213 15:26:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.11
[32m[20221213 15:26:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.43
[32m[20221213 15:26:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.38
[32m[20221213 15:26:25 @agent_ppo2.py:143][0m Total time:      33.49 min
[32m[20221213 15:26:25 @agent_ppo2.py:145][0m 3024896 total steps have happened
[32m[20221213 15:26:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1477 --------------------------#
[32m[20221213 15:26:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:25 @agent_ppo2.py:185][0m |          -0.0034 |          20.9662 |           0.2629 |
[32m[20221213 15:26:25 @agent_ppo2.py:185][0m |          -0.0060 |          20.5631 |           0.2630 |
[32m[20221213 15:26:25 @agent_ppo2.py:185][0m |          -0.0085 |          20.3985 |           0.2623 |
[32m[20221213 15:26:25 @agent_ppo2.py:185][0m |          -0.0125 |          20.2864 |           0.2622 |
[32m[20221213 15:26:26 @agent_ppo2.py:185][0m |          -0.0123 |          20.1692 |           0.2618 |
[32m[20221213 15:26:26 @agent_ppo2.py:185][0m |          -0.0131 |          20.0930 |           0.2613 |
[32m[20221213 15:26:26 @agent_ppo2.py:185][0m |          -0.0139 |          20.0533 |           0.2615 |
[32m[20221213 15:26:26 @agent_ppo2.py:185][0m |          -0.0163 |          19.9885 |           0.2610 |
[32m[20221213 15:26:26 @agent_ppo2.py:185][0m |          -0.0164 |          19.9210 |           0.2609 |
[32m[20221213 15:26:26 @agent_ppo2.py:185][0m |          -0.0089 |          21.4346 |           0.2607 |
[32m[20221213 15:26:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.22
[32m[20221213 15:26:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.46
[32m[20221213 15:26:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.14
[32m[20221213 15:26:26 @agent_ppo2.py:143][0m Total time:      33.52 min
[32m[20221213 15:26:26 @agent_ppo2.py:145][0m 3026944 total steps have happened
[32m[20221213 15:26:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1478 --------------------------#
[32m[20221213 15:26:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |           0.0053 |          21.4884 |           0.2546 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0072 |          20.4703 |           0.2547 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0096 |          20.3627 |           0.2551 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0083 |          20.3657 |           0.2553 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0101 |          20.1698 |           0.2551 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0077 |          20.7478 |           0.2553 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0058 |          21.2557 |           0.2549 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0154 |          20.0182 |           0.2554 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0138 |          19.9735 |           0.2555 |
[32m[20221213 15:26:27 @agent_ppo2.py:185][0m |          -0.0165 |          19.8705 |           0.2559 |
[32m[20221213 15:26:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.85
[32m[20221213 15:26:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.61
[32m[20221213 15:26:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.02
[32m[20221213 15:26:28 @agent_ppo2.py:143][0m Total time:      33.54 min
[32m[20221213 15:26:28 @agent_ppo2.py:145][0m 3028992 total steps have happened
[32m[20221213 15:26:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1479 --------------------------#
[32m[20221213 15:26:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:28 @agent_ppo2.py:185][0m |           0.0014 |          21.6738 |           0.2534 |
[32m[20221213 15:26:28 @agent_ppo2.py:185][0m |          -0.0099 |          21.1559 |           0.2537 |
[32m[20221213 15:26:28 @agent_ppo2.py:185][0m |          -0.0104 |          20.8590 |           0.2537 |
[32m[20221213 15:26:28 @agent_ppo2.py:185][0m |          -0.0119 |          20.6057 |           0.2535 |
[32m[20221213 15:26:28 @agent_ppo2.py:185][0m |          -0.0118 |          20.4569 |           0.2532 |
[32m[20221213 15:26:28 @agent_ppo2.py:185][0m |          -0.0133 |          20.2021 |           0.2532 |
[32m[20221213 15:26:28 @agent_ppo2.py:185][0m |          -0.0145 |          20.0463 |           0.2533 |
[32m[20221213 15:26:29 @agent_ppo2.py:185][0m |          -0.0160 |          19.9235 |           0.2532 |
[32m[20221213 15:26:29 @agent_ppo2.py:185][0m |          -0.0152 |          19.7495 |           0.2529 |
[32m[20221213 15:26:29 @agent_ppo2.py:185][0m |          -0.0103 |          20.4370 |           0.2531 |
[32m[20221213 15:26:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.86
[32m[20221213 15:26:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.07
[32m[20221213 15:26:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.14
[32m[20221213 15:26:29 @agent_ppo2.py:143][0m Total time:      33.56 min
[32m[20221213 15:26:29 @agent_ppo2.py:145][0m 3031040 total steps have happened
[32m[20221213 15:26:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1480 --------------------------#
[32m[20221213 15:26:29 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:26:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:29 @agent_ppo2.py:185][0m |           0.0142 |          23.3090 |           0.2605 |
[32m[20221213 15:26:29 @agent_ppo2.py:185][0m |          -0.0063 |          22.1229 |           0.2583 |
[32m[20221213 15:26:29 @agent_ppo2.py:185][0m |          -0.0100 |          21.7830 |           0.2587 |
[32m[20221213 15:26:30 @agent_ppo2.py:185][0m |          -0.0157 |          21.6312 |           0.2585 |
[32m[20221213 15:26:30 @agent_ppo2.py:185][0m |          -0.0095 |          21.7090 |           0.2584 |
[32m[20221213 15:26:30 @agent_ppo2.py:185][0m |          -0.0165 |          21.4277 |           0.2582 |
[32m[20221213 15:26:30 @agent_ppo2.py:185][0m |          -0.0064 |          23.5520 |           0.2584 |
[32m[20221213 15:26:30 @agent_ppo2.py:185][0m |          -0.0150 |          21.3131 |           0.2576 |
[32m[20221213 15:26:30 @agent_ppo2.py:185][0m |          -0.0176 |          21.2106 |           0.2579 |
[32m[20221213 15:26:30 @agent_ppo2.py:185][0m |          -0.0197 |          21.0833 |           0.2579 |
[32m[20221213 15:26:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.27
[32m[20221213 15:26:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.53
[32m[20221213 15:26:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.45
[32m[20221213 15:26:30 @agent_ppo2.py:143][0m Total time:      33.58 min
[32m[20221213 15:26:30 @agent_ppo2.py:145][0m 3033088 total steps have happened
[32m[20221213 15:26:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1481 --------------------------#
[32m[20221213 15:26:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0035 |          22.4715 |           0.2608 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0094 |          22.1832 |           0.2602 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0120 |          21.9814 |           0.2601 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0118 |          21.9160 |           0.2601 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0136 |          21.7637 |           0.2598 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0162 |          21.6585 |           0.2598 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0167 |          21.5382 |           0.2599 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0160 |          21.4828 |           0.2600 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0185 |          21.4278 |           0.2599 |
[32m[20221213 15:26:31 @agent_ppo2.py:185][0m |          -0.0175 |          21.3999 |           0.2597 |
[32m[20221213 15:26:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.77
[32m[20221213 15:26:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.68
[32m[20221213 15:26:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.91
[32m[20221213 15:26:32 @agent_ppo2.py:143][0m Total time:      33.61 min
[32m[20221213 15:26:32 @agent_ppo2.py:145][0m 3035136 total steps have happened
[32m[20221213 15:26:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1482 --------------------------#
[32m[20221213 15:26:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:32 @agent_ppo2.py:185][0m |          -0.0000 |          21.4391 |           0.2612 |
[32m[20221213 15:26:32 @agent_ppo2.py:185][0m |          -0.0046 |          20.8675 |           0.2610 |
[32m[20221213 15:26:32 @agent_ppo2.py:185][0m |          -0.0046 |          20.8211 |           0.2611 |
[32m[20221213 15:26:32 @agent_ppo2.py:185][0m |          -0.0082 |          20.3707 |           0.2607 |
[32m[20221213 15:26:32 @agent_ppo2.py:185][0m |          -0.0110 |          20.1365 |           0.2608 |
[32m[20221213 15:26:32 @agent_ppo2.py:185][0m |          -0.0016 |          22.0485 |           0.2610 |
[32m[20221213 15:26:32 @agent_ppo2.py:185][0m |          -0.0122 |          19.7335 |           0.2607 |
[32m[20221213 15:26:33 @agent_ppo2.py:185][0m |          -0.0124 |          19.6411 |           0.2608 |
[32m[20221213 15:26:33 @agent_ppo2.py:185][0m |          -0.0041 |          21.0225 |           0.2610 |
[32m[20221213 15:26:33 @agent_ppo2.py:185][0m |          -0.0149 |          19.3868 |           0.2612 |
[32m[20221213 15:26:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.02
[32m[20221213 15:26:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.13
[32m[20221213 15:26:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.92
[32m[20221213 15:26:33 @agent_ppo2.py:143][0m Total time:      33.63 min
[32m[20221213 15:26:33 @agent_ppo2.py:145][0m 3037184 total steps have happened
[32m[20221213 15:26:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1483 --------------------------#
[32m[20221213 15:26:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:33 @agent_ppo2.py:185][0m |           0.0025 |          23.7239 |           0.2630 |
[32m[20221213 15:26:33 @agent_ppo2.py:185][0m |          -0.0069 |          23.0979 |           0.2625 |
[32m[20221213 15:26:33 @agent_ppo2.py:185][0m |          -0.0087 |          22.8555 |           0.2621 |
[32m[20221213 15:26:34 @agent_ppo2.py:185][0m |          -0.0098 |          22.7245 |           0.2620 |
[32m[20221213 15:26:34 @agent_ppo2.py:185][0m |          -0.0118 |          22.6372 |           0.2623 |
[32m[20221213 15:26:34 @agent_ppo2.py:185][0m |          -0.0131 |          22.5620 |           0.2625 |
[32m[20221213 15:26:34 @agent_ppo2.py:185][0m |          -0.0124 |          22.5446 |           0.2624 |
[32m[20221213 15:26:34 @agent_ppo2.py:185][0m |          -0.0124 |          22.4837 |           0.2623 |
[32m[20221213 15:26:34 @agent_ppo2.py:185][0m |          -0.0163 |          22.3905 |           0.2622 |
[32m[20221213 15:26:34 @agent_ppo2.py:185][0m |          -0.0169 |          22.3392 |           0.2620 |
[32m[20221213 15:26:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.34
[32m[20221213 15:26:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.56
[32m[20221213 15:26:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.90
[32m[20221213 15:26:34 @agent_ppo2.py:143][0m Total time:      33.65 min
[32m[20221213 15:26:34 @agent_ppo2.py:145][0m 3039232 total steps have happened
[32m[20221213 15:26:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1484 --------------------------#
[32m[20221213 15:26:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |           0.0046 |          22.1261 |           0.2630 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0028 |          21.2345 |           0.2620 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0078 |          20.8507 |           0.2614 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0001 |          22.6911 |           0.2613 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |           0.0026 |          23.1854 |           0.2609 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0112 |          20.5932 |           0.2603 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0123 |          20.4930 |           0.2601 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0149 |          20.4420 |           0.2603 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0151 |          20.4303 |           0.2600 |
[32m[20221213 15:26:35 @agent_ppo2.py:185][0m |          -0.0067 |          22.0680 |           0.2600 |
[32m[20221213 15:26:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:26:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.43
[32m[20221213 15:26:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.86
[32m[20221213 15:26:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.90
[32m[20221213 15:26:36 @agent_ppo2.py:143][0m Total time:      33.67 min
[32m[20221213 15:26:36 @agent_ppo2.py:145][0m 3041280 total steps have happened
[32m[20221213 15:26:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1485 --------------------------#
[32m[20221213 15:26:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:36 @agent_ppo2.py:185][0m |          -0.0020 |          21.6281 |           0.2555 |
[32m[20221213 15:26:36 @agent_ppo2.py:185][0m |          -0.0088 |          21.2887 |           0.2544 |
[32m[20221213 15:26:36 @agent_ppo2.py:185][0m |          -0.0023 |          22.0048 |           0.2541 |
[32m[20221213 15:26:36 @agent_ppo2.py:185][0m |          -0.0117 |          20.9858 |           0.2536 |
[32m[20221213 15:26:36 @agent_ppo2.py:185][0m |          -0.0150 |          20.8631 |           0.2536 |
[32m[20221213 15:26:36 @agent_ppo2.py:185][0m |          -0.0126 |          20.9609 |           0.2538 |
[32m[20221213 15:26:36 @agent_ppo2.py:185][0m |          -0.0160 |          20.6746 |           0.2537 |
[32m[20221213 15:26:37 @agent_ppo2.py:185][0m |          -0.0122 |          20.6406 |           0.2537 |
[32m[20221213 15:26:37 @agent_ppo2.py:185][0m |          -0.0094 |          21.4236 |           0.2540 |
[32m[20221213 15:26:37 @agent_ppo2.py:185][0m |          -0.0176 |          20.5343 |           0.2538 |
[32m[20221213 15:26:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.44
[32m[20221213 15:26:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.02
[32m[20221213 15:26:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.30
[32m[20221213 15:26:37 @agent_ppo2.py:143][0m Total time:      33.70 min
[32m[20221213 15:26:37 @agent_ppo2.py:145][0m 3043328 total steps have happened
[32m[20221213 15:26:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1486 --------------------------#
[32m[20221213 15:26:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:37 @agent_ppo2.py:185][0m |           0.0019 |          21.7507 |           0.2577 |
[32m[20221213 15:26:37 @agent_ppo2.py:185][0m |          -0.0070 |          21.4502 |           0.2574 |
[32m[20221213 15:26:37 @agent_ppo2.py:185][0m |          -0.0079 |          21.3885 |           0.2573 |
[32m[20221213 15:26:38 @agent_ppo2.py:185][0m |          -0.0095 |          21.2890 |           0.2574 |
[32m[20221213 15:26:38 @agent_ppo2.py:185][0m |          -0.0008 |          21.7741 |           0.2574 |
[32m[20221213 15:26:38 @agent_ppo2.py:185][0m |          -0.0110 |          21.1265 |           0.2575 |
[32m[20221213 15:26:38 @agent_ppo2.py:185][0m |          -0.0138 |          21.0843 |           0.2577 |
[32m[20221213 15:26:38 @agent_ppo2.py:185][0m |          -0.0141 |          21.0093 |           0.2576 |
[32m[20221213 15:26:38 @agent_ppo2.py:185][0m |          -0.0143 |          20.9698 |           0.2578 |
[32m[20221213 15:26:38 @agent_ppo2.py:185][0m |          -0.0120 |          21.1978 |           0.2581 |
[32m[20221213 15:26:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.91
[32m[20221213 15:26:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.67
[32m[20221213 15:26:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.81
[32m[20221213 15:26:38 @agent_ppo2.py:143][0m Total time:      33.72 min
[32m[20221213 15:26:38 @agent_ppo2.py:145][0m 3045376 total steps have happened
[32m[20221213 15:26:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1487 --------------------------#
[32m[20221213 15:26:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |           0.0016 |          21.8462 |           0.2669 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0024 |          21.6867 |           0.2673 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0087 |          21.4767 |           0.2669 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0076 |          21.4991 |           0.2671 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0090 |          21.5909 |           0.2669 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0095 |          21.3900 |           0.2668 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0137 |          21.2900 |           0.2670 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0147 |          21.2696 |           0.2672 |
[32m[20221213 15:26:39 @agent_ppo2.py:185][0m |          -0.0139 |          21.2370 |           0.2674 |
[32m[20221213 15:26:40 @agent_ppo2.py:185][0m |          -0.0179 |          21.1941 |           0.2672 |
[32m[20221213 15:26:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:26:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.78
[32m[20221213 15:26:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.58
[32m[20221213 15:26:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.90
[32m[20221213 15:26:40 @agent_ppo2.py:143][0m Total time:      33.74 min
[32m[20221213 15:26:40 @agent_ppo2.py:145][0m 3047424 total steps have happened
[32m[20221213 15:26:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1488 --------------------------#
[32m[20221213 15:26:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:40 @agent_ppo2.py:185][0m |           0.0010 |          21.7285 |           0.2654 |
[32m[20221213 15:26:40 @agent_ppo2.py:185][0m |          -0.0067 |          21.4405 |           0.2646 |
[32m[20221213 15:26:40 @agent_ppo2.py:185][0m |          -0.0058 |          21.4158 |           0.2643 |
[32m[20221213 15:26:40 @agent_ppo2.py:185][0m |          -0.0062 |          21.3979 |           0.2643 |
[32m[20221213 15:26:40 @agent_ppo2.py:185][0m |          -0.0134 |          21.1496 |           0.2645 |
[32m[20221213 15:26:41 @agent_ppo2.py:185][0m |          -0.0122 |          21.1364 |           0.2639 |
[32m[20221213 15:26:41 @agent_ppo2.py:185][0m |          -0.0023 |          22.5108 |           0.2639 |
[32m[20221213 15:26:41 @agent_ppo2.py:185][0m |          -0.0142 |          20.9975 |           0.2638 |
[32m[20221213 15:26:41 @agent_ppo2.py:185][0m |           0.0022 |          23.8262 |           0.2637 |
[32m[20221213 15:26:41 @agent_ppo2.py:185][0m |          -0.0144 |          21.0077 |           0.2629 |
[32m[20221213 15:26:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:26:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.53
[32m[20221213 15:26:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.13
[32m[20221213 15:26:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.04
[32m[20221213 15:26:41 @agent_ppo2.py:143][0m Total time:      33.77 min
[32m[20221213 15:26:41 @agent_ppo2.py:145][0m 3049472 total steps have happened
[32m[20221213 15:26:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1489 --------------------------#
[32m[20221213 15:26:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:41 @agent_ppo2.py:185][0m |           0.0039 |          22.1329 |           0.2630 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0072 |          21.2820 |           0.2620 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0098 |          21.1672 |           0.2620 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0119 |          21.0388 |           0.2618 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0102 |          20.9420 |           0.2619 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0142 |          20.8835 |           0.2617 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0116 |          20.9016 |           0.2615 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0113 |          20.7740 |           0.2614 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0146 |          20.6873 |           0.2612 |
[32m[20221213 15:26:42 @agent_ppo2.py:185][0m |          -0.0065 |          21.5437 |           0.2611 |
[32m[20221213 15:26:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.57
[32m[20221213 15:26:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.05
[32m[20221213 15:26:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.58
[32m[20221213 15:26:42 @agent_ppo2.py:143][0m Total time:      33.79 min
[32m[20221213 15:26:42 @agent_ppo2.py:145][0m 3051520 total steps have happened
[32m[20221213 15:26:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1490 --------------------------#
[32m[20221213 15:26:43 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:26:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |           0.0009 |          20.6779 |           0.2571 |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |          -0.0029 |          20.4130 |           0.2568 |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |          -0.0068 |          20.2852 |           0.2567 |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |          -0.0069 |          20.1435 |           0.2566 |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |          -0.0017 |          20.9903 |           0.2561 |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |          -0.0097 |          19.9512 |           0.2558 |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |          -0.0056 |          20.2911 |           0.2560 |
[32m[20221213 15:26:43 @agent_ppo2.py:185][0m |          -0.0114 |          19.8782 |           0.2557 |
[32m[20221213 15:26:44 @agent_ppo2.py:185][0m |          -0.0134 |          19.8327 |           0.2557 |
[32m[20221213 15:26:44 @agent_ppo2.py:185][0m |          -0.0127 |          19.7666 |           0.2554 |
[32m[20221213 15:26:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.99
[32m[20221213 15:26:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.61
[32m[20221213 15:26:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.11
[32m[20221213 15:26:44 @agent_ppo2.py:143][0m Total time:      33.81 min
[32m[20221213 15:26:44 @agent_ppo2.py:145][0m 3053568 total steps have happened
[32m[20221213 15:26:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1491 --------------------------#
[32m[20221213 15:26:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:26:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:44 @agent_ppo2.py:185][0m |           0.0075 |          21.8065 |           0.2531 |
[32m[20221213 15:26:44 @agent_ppo2.py:185][0m |          -0.0023 |          20.7161 |           0.2529 |
[32m[20221213 15:26:44 @agent_ppo2.py:185][0m |          -0.0105 |          20.2338 |           0.2527 |
[32m[20221213 15:26:44 @agent_ppo2.py:185][0m |          -0.0111 |          20.1230 |           0.2530 |
[32m[20221213 15:26:45 @agent_ppo2.py:185][0m |          -0.0142 |          19.8627 |           0.2530 |
[32m[20221213 15:26:45 @agent_ppo2.py:185][0m |          -0.0125 |          19.7310 |           0.2529 |
[32m[20221213 15:26:45 @agent_ppo2.py:185][0m |          -0.0184 |          19.5388 |           0.2530 |
[32m[20221213 15:26:45 @agent_ppo2.py:185][0m |          -0.0190 |          19.4676 |           0.2529 |
[32m[20221213 15:26:45 @agent_ppo2.py:185][0m |          -0.0167 |          19.3779 |           0.2529 |
[32m[20221213 15:26:45 @agent_ppo2.py:185][0m |          -0.0204 |          19.2851 |           0.2531 |
[32m[20221213 15:26:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.75
[32m[20221213 15:26:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.46
[32m[20221213 15:26:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.15
[32m[20221213 15:26:45 @agent_ppo2.py:143][0m Total time:      33.83 min
[32m[20221213 15:26:45 @agent_ppo2.py:145][0m 3055616 total steps have happened
[32m[20221213 15:26:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1492 --------------------------#
[32m[20221213 15:26:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |           0.0083 |          22.8843 |           0.2516 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0086 |          21.1580 |           0.2514 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0114 |          20.8511 |           0.2511 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0116 |          20.6498 |           0.2513 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0149 |          20.3848 |           0.2508 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0159 |          20.1553 |           0.2511 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0171 |          19.9058 |           0.2509 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0178 |          19.5510 |           0.2510 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0057 |          21.7984 |           0.2508 |
[32m[20221213 15:26:46 @agent_ppo2.py:185][0m |          -0.0170 |          19.3553 |           0.2504 |
[32m[20221213 15:26:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:26:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.04
[32m[20221213 15:26:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.04
[32m[20221213 15:26:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.41
[32m[20221213 15:26:47 @agent_ppo2.py:143][0m Total time:      33.86 min
[32m[20221213 15:26:47 @agent_ppo2.py:145][0m 3057664 total steps have happened
[32m[20221213 15:26:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1493 --------------------------#
[32m[20221213 15:26:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:47 @agent_ppo2.py:185][0m |          -0.0035 |          21.3332 |           0.2537 |
[32m[20221213 15:26:47 @agent_ppo2.py:185][0m |           0.0105 |          23.4260 |           0.2533 |
[32m[20221213 15:26:47 @agent_ppo2.py:185][0m |          -0.0082 |          20.0978 |           0.2517 |
[32m[20221213 15:26:47 @agent_ppo2.py:185][0m |          -0.0082 |          20.2535 |           0.2533 |
[32m[20221213 15:26:47 @agent_ppo2.py:185][0m |          -0.0158 |          19.6845 |           0.2532 |
[32m[20221213 15:26:47 @agent_ppo2.py:185][0m |          -0.0108 |          20.0839 |           0.2530 |
[32m[20221213 15:26:47 @agent_ppo2.py:185][0m |          -0.0219 |          19.4000 |           0.2530 |
[32m[20221213 15:26:48 @agent_ppo2.py:185][0m |          -0.0140 |          19.3565 |           0.2530 |
[32m[20221213 15:26:48 @agent_ppo2.py:185][0m |          -0.0153 |          19.1633 |           0.2527 |
[32m[20221213 15:26:48 @agent_ppo2.py:185][0m |          -0.0183 |          18.9715 |           0.2529 |
[32m[20221213 15:26:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:26:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.94
[32m[20221213 15:26:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.83
[32m[20221213 15:26:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.15
[32m[20221213 15:26:48 @agent_ppo2.py:143][0m Total time:      33.88 min
[32m[20221213 15:26:48 @agent_ppo2.py:145][0m 3059712 total steps have happened
[32m[20221213 15:26:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1494 --------------------------#
[32m[20221213 15:26:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:48 @agent_ppo2.py:185][0m |          -0.0013 |          22.2932 |           0.2573 |
[32m[20221213 15:26:48 @agent_ppo2.py:185][0m |          -0.0090 |          21.7533 |           0.2565 |
[32m[20221213 15:26:48 @agent_ppo2.py:185][0m |          -0.0105 |          21.4845 |           0.2560 |
[32m[20221213 15:26:49 @agent_ppo2.py:185][0m |          -0.0085 |          21.4448 |           0.2560 |
[32m[20221213 15:26:49 @agent_ppo2.py:185][0m |          -0.0142 |          21.2451 |           0.2556 |
[32m[20221213 15:26:49 @agent_ppo2.py:185][0m |          -0.0140 |          21.1140 |           0.2558 |
[32m[20221213 15:26:49 @agent_ppo2.py:185][0m |          -0.0143 |          21.0535 |           0.2555 |
[32m[20221213 15:26:49 @agent_ppo2.py:185][0m |          -0.0176 |          20.9708 |           0.2554 |
[32m[20221213 15:26:49 @agent_ppo2.py:185][0m |          -0.0115 |          21.3529 |           0.2554 |
[32m[20221213 15:26:49 @agent_ppo2.py:185][0m |          -0.0032 |          22.4139 |           0.2551 |
[32m[20221213 15:26:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:26:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.91
[32m[20221213 15:26:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.83
[32m[20221213 15:26:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.78
[32m[20221213 15:26:49 @agent_ppo2.py:143][0m Total time:      33.90 min
[32m[20221213 15:26:49 @agent_ppo2.py:145][0m 3061760 total steps have happened
[32m[20221213 15:26:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1495 --------------------------#
[32m[20221213 15:26:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |           0.0028 |          21.3037 |           0.2548 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |           0.0085 |          22.6948 |           0.2546 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0030 |          20.3582 |           0.2537 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0117 |          20.1020 |           0.2538 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0149 |          19.9610 |           0.2537 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0146 |          19.8817 |           0.2536 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0118 |          20.3878 |           0.2531 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0091 |          20.8172 |           0.2529 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0159 |          19.5156 |           0.2529 |
[32m[20221213 15:26:50 @agent_ppo2.py:185][0m |          -0.0133 |          20.0760 |           0.2528 |
[32m[20221213 15:26:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:26:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.54
[32m[20221213 15:26:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.87
[32m[20221213 15:26:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.14
[32m[20221213 15:26:51 @agent_ppo2.py:143][0m Total time:      33.92 min
[32m[20221213 15:26:51 @agent_ppo2.py:145][0m 3063808 total steps have happened
[32m[20221213 15:26:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1496 --------------------------#
[32m[20221213 15:26:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:51 @agent_ppo2.py:185][0m |          -0.0016 |          22.2219 |           0.2466 |
[32m[20221213 15:26:51 @agent_ppo2.py:185][0m |          -0.0068 |          21.8162 |           0.2459 |
[32m[20221213 15:26:51 @agent_ppo2.py:185][0m |           0.0006 |          24.2758 |           0.2456 |
[32m[20221213 15:26:51 @agent_ppo2.py:185][0m |          -0.0082 |          21.7915 |           0.2450 |
[32m[20221213 15:26:51 @agent_ppo2.py:185][0m |          -0.0113 |          21.3535 |           0.2452 |
[32m[20221213 15:26:51 @agent_ppo2.py:185][0m |          -0.0146 |          21.2469 |           0.2450 |
[32m[20221213 15:26:52 @agent_ppo2.py:185][0m |          -0.0160 |          21.1786 |           0.2449 |
[32m[20221213 15:26:52 @agent_ppo2.py:185][0m |          -0.0160 |          21.1380 |           0.2451 |
[32m[20221213 15:26:52 @agent_ppo2.py:185][0m |          -0.0170 |          21.0950 |           0.2449 |
[32m[20221213 15:26:52 @agent_ppo2.py:185][0m |          -0.0169 |          21.0393 |           0.2448 |
[32m[20221213 15:26:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:26:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.19
[32m[20221213 15:26:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.38
[32m[20221213 15:26:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.34
[32m[20221213 15:26:52 @agent_ppo2.py:143][0m Total time:      33.95 min
[32m[20221213 15:26:52 @agent_ppo2.py:145][0m 3065856 total steps have happened
[32m[20221213 15:26:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1497 --------------------------#
[32m[20221213 15:26:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:52 @agent_ppo2.py:185][0m |          -0.0033 |          22.7051 |           0.2622 |
[32m[20221213 15:26:52 @agent_ppo2.py:185][0m |          -0.0090 |          22.3401 |           0.2615 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0114 |          22.1825 |           0.2612 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0147 |          22.1243 |           0.2609 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0041 |          23.7594 |           0.2610 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0033 |          22.6439 |           0.2606 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0104 |          22.0041 |           0.2609 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0133 |          21.8559 |           0.2610 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0142 |          21.7644 |           0.2609 |
[32m[20221213 15:26:53 @agent_ppo2.py:185][0m |          -0.0095 |          22.1613 |           0.2611 |
[32m[20221213 15:26:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.01
[32m[20221213 15:26:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.97
[32m[20221213 15:26:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.55
[32m[20221213 15:26:53 @agent_ppo2.py:143][0m Total time:      33.97 min
[32m[20221213 15:26:53 @agent_ppo2.py:145][0m 3067904 total steps have happened
[32m[20221213 15:26:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1498 --------------------------#
[32m[20221213 15:26:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |          -0.0021 |          20.6852 |           0.2552 |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |          -0.0094 |          20.3139 |           0.2546 |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |          -0.0124 |          20.1392 |           0.2541 |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |          -0.0118 |          19.9831 |           0.2543 |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |          -0.0152 |          19.8884 |           0.2542 |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |          -0.0148 |          19.7963 |           0.2540 |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |           0.0090 |          24.1957 |           0.2541 |
[32m[20221213 15:26:54 @agent_ppo2.py:185][0m |          -0.0143 |          19.7096 |           0.2528 |
[32m[20221213 15:26:55 @agent_ppo2.py:185][0m |          -0.0185 |          19.6212 |           0.2539 |
[32m[20221213 15:26:55 @agent_ppo2.py:185][0m |          -0.0172 |          19.5766 |           0.2540 |
[32m[20221213 15:26:55 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:26:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.65
[32m[20221213 15:26:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.16
[32m[20221213 15:26:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.17
[32m[20221213 15:26:55 @agent_ppo2.py:143][0m Total time:      33.99 min
[32m[20221213 15:26:55 @agent_ppo2.py:145][0m 3069952 total steps have happened
[32m[20221213 15:26:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1499 --------------------------#
[32m[20221213 15:26:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:55 @agent_ppo2.py:185][0m |          -0.0014 |          20.8960 |           0.2510 |
[32m[20221213 15:26:55 @agent_ppo2.py:185][0m |          -0.0067 |          20.6901 |           0.2509 |
[32m[20221213 15:26:55 @agent_ppo2.py:185][0m |          -0.0037 |          21.1479 |           0.2510 |
[32m[20221213 15:26:55 @agent_ppo2.py:185][0m |          -0.0131 |          20.4419 |           0.2509 |
[32m[20221213 15:26:56 @agent_ppo2.py:185][0m |          -0.0104 |          20.5731 |           0.2506 |
[32m[20221213 15:26:56 @agent_ppo2.py:185][0m |           0.0015 |          22.6552 |           0.2502 |
[32m[20221213 15:26:56 @agent_ppo2.py:185][0m |          -0.0095 |          20.5138 |           0.2506 |
[32m[20221213 15:26:56 @agent_ppo2.py:185][0m |          -0.0169 |          20.1698 |           0.2504 |
[32m[20221213 15:26:56 @agent_ppo2.py:185][0m |          -0.0167 |          20.0914 |           0.2506 |
[32m[20221213 15:26:56 @agent_ppo2.py:185][0m |          -0.0180 |          20.0625 |           0.2504 |
[32m[20221213 15:26:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:26:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.79
[32m[20221213 15:26:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.39
[32m[20221213 15:26:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.15
[32m[20221213 15:26:56 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 315.91
[32m[20221213 15:26:56 @agent_ppo2.py:143][0m Total time:      34.02 min
[32m[20221213 15:26:56 @agent_ppo2.py:145][0m 3072000 total steps have happened
[32m[20221213 15:26:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1500 --------------------------#
[32m[20221213 15:26:56 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:26:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0022 |          21.2120 |           0.2562 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0061 |          20.9373 |           0.2555 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0096 |          20.8481 |           0.2552 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0110 |          20.8085 |           0.2551 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0074 |          20.8793 |           0.2550 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0126 |          20.6438 |           0.2549 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0116 |          20.5730 |           0.2550 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0144 |          20.5360 |           0.2548 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0132 |          20.5181 |           0.2546 |
[32m[20221213 15:26:57 @agent_ppo2.py:185][0m |          -0.0144 |          20.4528 |           0.2549 |
[32m[20221213 15:26:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:26:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.14
[32m[20221213 15:26:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.18
[32m[20221213 15:26:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.16
[32m[20221213 15:26:57 @agent_ppo2.py:143][0m Total time:      34.04 min
[32m[20221213 15:26:57 @agent_ppo2.py:145][0m 3074048 total steps have happened
[32m[20221213 15:26:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1501 --------------------------#
[32m[20221213 15:26:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |           0.0004 |          20.3615 |           0.2547 |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |          -0.0039 |          19.7294 |           0.2545 |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |          -0.0074 |          19.3984 |           0.2540 |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |           0.0052 |          21.5486 |           0.2539 |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |          -0.0124 |          19.0604 |           0.2535 |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |          -0.0058 |          19.4427 |           0.2538 |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |          -0.0099 |          18.9168 |           0.2534 |
[32m[20221213 15:26:58 @agent_ppo2.py:185][0m |          -0.0162 |          18.5501 |           0.2532 |
[32m[20221213 15:26:59 @agent_ppo2.py:185][0m |          -0.0167 |          18.4932 |           0.2535 |
[32m[20221213 15:26:59 @agent_ppo2.py:185][0m |          -0.0169 |          18.3684 |           0.2533 |
[32m[20221213 15:26:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:26:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.84
[32m[20221213 15:26:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.21
[32m[20221213 15:26:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.46
[32m[20221213 15:26:59 @agent_ppo2.py:143][0m Total time:      34.06 min
[32m[20221213 15:26:59 @agent_ppo2.py:145][0m 3076096 total steps have happened
[32m[20221213 15:26:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1502 --------------------------#
[32m[20221213 15:26:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:26:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:26:59 @agent_ppo2.py:185][0m |          -0.0001 |          21.2853 |           0.2559 |
[32m[20221213 15:26:59 @agent_ppo2.py:185][0m |          -0.0066 |          20.8142 |           0.2559 |
[32m[20221213 15:26:59 @agent_ppo2.py:185][0m |          -0.0096 |          20.6951 |           0.2554 |
[32m[20221213 15:26:59 @agent_ppo2.py:185][0m |          -0.0097 |          20.5044 |           0.2553 |
[32m[20221213 15:27:00 @agent_ppo2.py:185][0m |          -0.0120 |          20.3957 |           0.2554 |
[32m[20221213 15:27:00 @agent_ppo2.py:185][0m |          -0.0150 |          20.3102 |           0.2556 |
[32m[20221213 15:27:00 @agent_ppo2.py:185][0m |          -0.0116 |          20.7588 |           0.2559 |
[32m[20221213 15:27:00 @agent_ppo2.py:185][0m |          -0.0158 |          20.1944 |           0.2555 |
[32m[20221213 15:27:00 @agent_ppo2.py:185][0m |          -0.0166 |          20.1098 |           0.2556 |
[32m[20221213 15:27:00 @agent_ppo2.py:185][0m |          -0.0165 |          20.0497 |           0.2558 |
[32m[20221213 15:27:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.44
[32m[20221213 15:27:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.32
[32m[20221213 15:27:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.06
[32m[20221213 15:27:00 @agent_ppo2.py:143][0m Total time:      34.08 min
[32m[20221213 15:27:00 @agent_ppo2.py:145][0m 3078144 total steps have happened
[32m[20221213 15:27:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1503 --------------------------#
[32m[20221213 15:27:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |           0.0022 |          22.1887 |           0.2438 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0042 |          22.2764 |           0.2434 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0084 |          21.6593 |           0.2429 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0103 |          21.5569 |           0.2431 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0108 |          21.5430 |           0.2430 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |           0.0098 |          25.2620 |           0.2430 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0117 |          21.5259 |           0.2427 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0143 |          21.3038 |           0.2429 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0136 |          21.2907 |           0.2427 |
[32m[20221213 15:27:01 @agent_ppo2.py:185][0m |          -0.0139 |          21.2913 |           0.2428 |
[32m[20221213 15:27:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:27:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.95
[32m[20221213 15:27:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.90
[32m[20221213 15:27:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.21
[32m[20221213 15:27:02 @agent_ppo2.py:143][0m Total time:      34.11 min
[32m[20221213 15:27:02 @agent_ppo2.py:145][0m 3080192 total steps have happened
[32m[20221213 15:27:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1504 --------------------------#
[32m[20221213 15:27:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:02 @agent_ppo2.py:185][0m |          -0.0024 |          20.5286 |           0.2507 |
[32m[20221213 15:27:02 @agent_ppo2.py:185][0m |          -0.0089 |          20.1106 |           0.2503 |
[32m[20221213 15:27:02 @agent_ppo2.py:185][0m |          -0.0007 |          21.3285 |           0.2502 |
[32m[20221213 15:27:02 @agent_ppo2.py:185][0m |          -0.0116 |          19.8955 |           0.2500 |
[32m[20221213 15:27:02 @agent_ppo2.py:185][0m |          -0.0104 |          19.9368 |           0.2496 |
[32m[20221213 15:27:02 @agent_ppo2.py:185][0m |          -0.0146 |          19.7468 |           0.2498 |
[32m[20221213 15:27:02 @agent_ppo2.py:185][0m |          -0.0130 |          19.6841 |           0.2501 |
[32m[20221213 15:27:03 @agent_ppo2.py:185][0m |          -0.0157 |          19.6592 |           0.2498 |
[32m[20221213 15:27:03 @agent_ppo2.py:185][0m |          -0.0174 |          19.5790 |           0.2501 |
[32m[20221213 15:27:03 @agent_ppo2.py:185][0m |          -0.0186 |          19.5722 |           0.2500 |
[32m[20221213 15:27:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:27:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.09
[32m[20221213 15:27:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.08
[32m[20221213 15:27:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.74
[32m[20221213 15:27:03 @agent_ppo2.py:143][0m Total time:      34.13 min
[32m[20221213 15:27:03 @agent_ppo2.py:145][0m 3082240 total steps have happened
[32m[20221213 15:27:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1505 --------------------------#
[32m[20221213 15:27:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:03 @agent_ppo2.py:185][0m |          -0.0043 |          21.0530 |           0.2614 |
[32m[20221213 15:27:03 @agent_ppo2.py:185][0m |          -0.0046 |          20.8351 |           0.2611 |
[32m[20221213 15:27:03 @agent_ppo2.py:185][0m |          -0.0107 |          20.6100 |           0.2611 |
[32m[20221213 15:27:04 @agent_ppo2.py:185][0m |          -0.0109 |          20.5322 |           0.2612 |
[32m[20221213 15:27:04 @agent_ppo2.py:185][0m |          -0.0128 |          20.3940 |           0.2610 |
[32m[20221213 15:27:04 @agent_ppo2.py:185][0m |           0.0017 |          22.7123 |           0.2611 |
[32m[20221213 15:27:04 @agent_ppo2.py:185][0m |          -0.0126 |          20.2301 |           0.2605 |
[32m[20221213 15:27:04 @agent_ppo2.py:185][0m |          -0.0109 |          20.7665 |           0.2610 |
[32m[20221213 15:27:04 @agent_ppo2.py:185][0m |          -0.0146 |          20.0740 |           0.2610 |
[32m[20221213 15:27:04 @agent_ppo2.py:185][0m |          -0.0123 |          20.1634 |           0.2612 |
[32m[20221213 15:27:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.35
[32m[20221213 15:27:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.32
[32m[20221213 15:27:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.02
[32m[20221213 15:27:04 @agent_ppo2.py:143][0m Total time:      34.15 min
[32m[20221213 15:27:04 @agent_ppo2.py:145][0m 3084288 total steps have happened
[32m[20221213 15:27:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1506 --------------------------#
[32m[20221213 15:27:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0005 |          21.8556 |           0.2613 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0026 |          21.6056 |           0.2612 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0085 |          21.2241 |           0.2612 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0101 |          21.0826 |           0.2613 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0116 |          20.9696 |           0.2613 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0016 |          21.6159 |           0.2612 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0128 |          20.8125 |           0.2609 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0117 |          21.0305 |           0.2609 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0166 |          20.6713 |           0.2607 |
[32m[20221213 15:27:05 @agent_ppo2.py:185][0m |          -0.0041 |          23.7831 |           0.2607 |
[32m[20221213 15:27:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.08
[32m[20221213 15:27:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.86
[32m[20221213 15:27:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.06
[32m[20221213 15:27:06 @agent_ppo2.py:143][0m Total time:      34.17 min
[32m[20221213 15:27:06 @agent_ppo2.py:145][0m 3086336 total steps have happened
[32m[20221213 15:27:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1507 --------------------------#
[32m[20221213 15:27:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:06 @agent_ppo2.py:185][0m |           0.0012 |          20.9940 |           0.2558 |
[32m[20221213 15:27:06 @agent_ppo2.py:185][0m |          -0.0042 |          20.7807 |           0.2561 |
[32m[20221213 15:27:06 @agent_ppo2.py:185][0m |          -0.0102 |          20.5676 |           0.2556 |
[32m[20221213 15:27:06 @agent_ppo2.py:185][0m |          -0.0118 |          20.5004 |           0.2557 |
[32m[20221213 15:27:06 @agent_ppo2.py:185][0m |          -0.0143 |          20.3859 |           0.2556 |
[32m[20221213 15:27:06 @agent_ppo2.py:185][0m |          -0.0111 |          20.3943 |           0.2555 |
[32m[20221213 15:27:06 @agent_ppo2.py:185][0m |          -0.0166 |          20.2029 |           0.2555 |
[32m[20221213 15:27:07 @agent_ppo2.py:185][0m |          -0.0011 |          22.7645 |           0.2555 |
[32m[20221213 15:27:07 @agent_ppo2.py:185][0m |          -0.0139 |          20.0594 |           0.2552 |
[32m[20221213 15:27:07 @agent_ppo2.py:185][0m |          -0.0158 |          20.0104 |           0.2554 |
[32m[20221213 15:27:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.04
[32m[20221213 15:27:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.35
[32m[20221213 15:27:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.16
[32m[20221213 15:27:07 @agent_ppo2.py:143][0m Total time:      34.20 min
[32m[20221213 15:27:07 @agent_ppo2.py:145][0m 3088384 total steps have happened
[32m[20221213 15:27:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1508 --------------------------#
[32m[20221213 15:27:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:07 @agent_ppo2.py:185][0m |          -0.0011 |          20.0139 |           0.2538 |
[32m[20221213 15:27:07 @agent_ppo2.py:185][0m |          -0.0078 |          18.9159 |           0.2537 |
[32m[20221213 15:27:07 @agent_ppo2.py:185][0m |          -0.0073 |          18.2044 |           0.2536 |
[32m[20221213 15:27:08 @agent_ppo2.py:185][0m |          -0.0099 |          17.7663 |           0.2539 |
[32m[20221213 15:27:08 @agent_ppo2.py:185][0m |          -0.0112 |          17.4403 |           0.2541 |
[32m[20221213 15:27:08 @agent_ppo2.py:185][0m |          -0.0131 |          17.2038 |           0.2541 |
[32m[20221213 15:27:08 @agent_ppo2.py:185][0m |          -0.0144 |          17.0012 |           0.2537 |
[32m[20221213 15:27:08 @agent_ppo2.py:185][0m |          -0.0133 |          16.8885 |           0.2539 |
[32m[20221213 15:27:08 @agent_ppo2.py:185][0m |          -0.0179 |          16.7468 |           0.2541 |
[32m[20221213 15:27:08 @agent_ppo2.py:185][0m |          -0.0170 |          16.6892 |           0.2540 |
[32m[20221213 15:27:08 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.50
[32m[20221213 15:27:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.72
[32m[20221213 15:27:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.04
[32m[20221213 15:27:08 @agent_ppo2.py:143][0m Total time:      34.22 min
[32m[20221213 15:27:08 @agent_ppo2.py:145][0m 3090432 total steps have happened
[32m[20221213 15:27:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1509 --------------------------#
[32m[20221213 15:27:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0007 |          23.4894 |           0.2659 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0062 |          22.4454 |           0.2659 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0042 |          22.4932 |           0.2663 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0112 |          21.6754 |           0.2661 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0117 |          21.5369 |           0.2661 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0068 |          22.3961 |           0.2662 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0085 |          21.6174 |           0.2660 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0120 |          21.3667 |           0.2662 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0153 |          21.0962 |           0.2659 |
[32m[20221213 15:27:09 @agent_ppo2.py:185][0m |          -0.0145 |          21.0940 |           0.2660 |
[32m[20221213 15:27:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.22
[32m[20221213 15:27:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.93
[32m[20221213 15:27:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.63
[32m[20221213 15:27:10 @agent_ppo2.py:143][0m Total time:      34.24 min
[32m[20221213 15:27:10 @agent_ppo2.py:145][0m 3092480 total steps have happened
[32m[20221213 15:27:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1510 --------------------------#
[32m[20221213 15:27:10 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:27:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:10 @agent_ppo2.py:185][0m |          -0.0025 |          22.5136 |           0.2638 |
[32m[20221213 15:27:10 @agent_ppo2.py:185][0m |          -0.0042 |          22.3732 |           0.2633 |
[32m[20221213 15:27:10 @agent_ppo2.py:185][0m |          -0.0102 |          21.8706 |           0.2630 |
[32m[20221213 15:27:10 @agent_ppo2.py:185][0m |          -0.0105 |          21.7376 |           0.2634 |
[32m[20221213 15:27:10 @agent_ppo2.py:185][0m |          -0.0115 |          21.6350 |           0.2634 |
[32m[20221213 15:27:10 @agent_ppo2.py:185][0m |          -0.0022 |          23.1530 |           0.2630 |
[32m[20221213 15:27:11 @agent_ppo2.py:185][0m |          -0.0141 |          21.4535 |           0.2631 |
[32m[20221213 15:27:11 @agent_ppo2.py:185][0m |          -0.0145 |          21.3437 |           0.2629 |
[32m[20221213 15:27:11 @agent_ppo2.py:185][0m |          -0.0185 |          21.2304 |           0.2628 |
[32m[20221213 15:27:11 @agent_ppo2.py:185][0m |          -0.0110 |          21.4681 |           0.2632 |
[32m[20221213 15:27:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.43
[32m[20221213 15:27:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.16
[32m[20221213 15:27:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.71
[32m[20221213 15:27:11 @agent_ppo2.py:143][0m Total time:      34.26 min
[32m[20221213 15:27:11 @agent_ppo2.py:145][0m 3094528 total steps have happened
[32m[20221213 15:27:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1511 --------------------------#
[32m[20221213 15:27:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:11 @agent_ppo2.py:185][0m |           0.0196 |          26.3003 |           0.2657 |
[32m[20221213 15:27:11 @agent_ppo2.py:185][0m |          -0.0078 |          22.1973 |           0.2656 |
[32m[20221213 15:27:11 @agent_ppo2.py:185][0m |          -0.0091 |          21.7039 |           0.2652 |
[32m[20221213 15:27:12 @agent_ppo2.py:185][0m |          -0.0142 |          21.4566 |           0.2648 |
[32m[20221213 15:27:12 @agent_ppo2.py:185][0m |          -0.0130 |          21.3112 |           0.2649 |
[32m[20221213 15:27:12 @agent_ppo2.py:185][0m |          -0.0131 |          21.2030 |           0.2651 |
[32m[20221213 15:27:12 @agent_ppo2.py:185][0m |          -0.0156 |          21.1419 |           0.2649 |
[32m[20221213 15:27:12 @agent_ppo2.py:185][0m |          -0.0155 |          21.0895 |           0.2651 |
[32m[20221213 15:27:12 @agent_ppo2.py:185][0m |          -0.0157 |          21.1089 |           0.2646 |
[32m[20221213 15:27:12 @agent_ppo2.py:185][0m |          -0.0074 |          22.3412 |           0.2645 |
[32m[20221213 15:27:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.13
[32m[20221213 15:27:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.81
[32m[20221213 15:27:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.97
[32m[20221213 15:27:12 @agent_ppo2.py:143][0m Total time:      34.28 min
[32m[20221213 15:27:12 @agent_ppo2.py:145][0m 3096576 total steps have happened
[32m[20221213 15:27:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1512 --------------------------#
[32m[20221213 15:27:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0044 |          22.0158 |           0.2641 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0103 |          21.7220 |           0.2633 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0098 |          21.6144 |           0.2633 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0129 |          21.5453 |           0.2628 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0117 |          21.5367 |           0.2629 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0148 |          21.5001 |           0.2627 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0149 |          21.4468 |           0.2628 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0149 |          21.3741 |           0.2624 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0170 |          21.3239 |           0.2626 |
[32m[20221213 15:27:13 @agent_ppo2.py:185][0m |          -0.0170 |          21.3093 |           0.2625 |
[32m[20221213 15:27:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:27:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.79
[32m[20221213 15:27:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.74
[32m[20221213 15:27:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.88
[32m[20221213 15:27:14 @agent_ppo2.py:143][0m Total time:      34.31 min
[32m[20221213 15:27:14 @agent_ppo2.py:145][0m 3098624 total steps have happened
[32m[20221213 15:27:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1513 --------------------------#
[32m[20221213 15:27:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:14 @agent_ppo2.py:185][0m |           0.0008 |          22.1696 |           0.2630 |
[32m[20221213 15:27:14 @agent_ppo2.py:185][0m |          -0.0026 |          22.0597 |           0.2623 |
[32m[20221213 15:27:14 @agent_ppo2.py:185][0m |          -0.0114 |          21.6890 |           0.2617 |
[32m[20221213 15:27:14 @agent_ppo2.py:185][0m |          -0.0119 |          21.5734 |           0.2616 |
[32m[20221213 15:27:14 @agent_ppo2.py:185][0m |          -0.0098 |          21.9143 |           0.2617 |
[32m[20221213 15:27:14 @agent_ppo2.py:185][0m |          -0.0178 |          21.4463 |           0.2613 |
[32m[20221213 15:27:15 @agent_ppo2.py:185][0m |          -0.0154 |          21.4062 |           0.2613 |
[32m[20221213 15:27:15 @agent_ppo2.py:185][0m |          -0.0132 |          21.6431 |           0.2615 |
[32m[20221213 15:27:15 @agent_ppo2.py:185][0m |          -0.0177 |          21.2666 |           0.2611 |
[32m[20221213 15:27:15 @agent_ppo2.py:185][0m |          -0.0106 |          22.9286 |           0.2614 |
[32m[20221213 15:27:15 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:27:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.57
[32m[20221213 15:27:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.12
[32m[20221213 15:27:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.98
[32m[20221213 15:27:15 @agent_ppo2.py:143][0m Total time:      34.33 min
[32m[20221213 15:27:15 @agent_ppo2.py:145][0m 3100672 total steps have happened
[32m[20221213 15:27:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1514 --------------------------#
[32m[20221213 15:27:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:15 @agent_ppo2.py:185][0m |           0.0010 |          19.8823 |           0.2515 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0072 |          18.9385 |           0.2512 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0077 |          18.6317 |           0.2511 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0104 |          18.2682 |           0.2510 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0103 |          18.1034 |           0.2506 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0041 |          18.4182 |           0.2507 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0138 |          17.7971 |           0.2506 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0133 |          17.4978 |           0.2500 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0137 |          17.3957 |           0.2502 |
[32m[20221213 15:27:16 @agent_ppo2.py:185][0m |          -0.0081 |          17.9317 |           0.2503 |
[32m[20221213 15:27:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:27:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.98
[32m[20221213 15:27:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.49
[32m[20221213 15:27:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.69
[32m[20221213 15:27:16 @agent_ppo2.py:143][0m Total time:      34.35 min
[32m[20221213 15:27:16 @agent_ppo2.py:145][0m 3102720 total steps have happened
[32m[20221213 15:27:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1515 --------------------------#
[32m[20221213 15:27:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |           0.0004 |          22.7577 |           0.2595 |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |          -0.0067 |          21.9253 |           0.2596 |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |          -0.0076 |          21.6762 |           0.2593 |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |          -0.0096 |          21.5289 |           0.2596 |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |          -0.0126 |          21.4156 |           0.2598 |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |          -0.0159 |          21.4003 |           0.2601 |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |          -0.0155 |          21.2540 |           0.2597 |
[32m[20221213 15:27:17 @agent_ppo2.py:185][0m |          -0.0164 |          21.1367 |           0.2599 |
[32m[20221213 15:27:18 @agent_ppo2.py:185][0m |          -0.0174 |          21.1691 |           0.2601 |
[32m[20221213 15:27:18 @agent_ppo2.py:185][0m |          -0.0196 |          21.0498 |           0.2601 |
[32m[20221213 15:27:18 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:27:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.98
[32m[20221213 15:27:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.22
[32m[20221213 15:27:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.53
[32m[20221213 15:27:18 @agent_ppo2.py:143][0m Total time:      34.38 min
[32m[20221213 15:27:18 @agent_ppo2.py:145][0m 3104768 total steps have happened
[32m[20221213 15:27:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1516 --------------------------#
[32m[20221213 15:27:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:18 @agent_ppo2.py:185][0m |          -0.0020 |          22.0159 |           0.2656 |
[32m[20221213 15:27:18 @agent_ppo2.py:185][0m |          -0.0097 |          21.7245 |           0.2652 |
[32m[20221213 15:27:18 @agent_ppo2.py:185][0m |          -0.0062 |          22.1049 |           0.2651 |
[32m[20221213 15:27:19 @agent_ppo2.py:185][0m |          -0.0145 |          21.5383 |           0.2648 |
[32m[20221213 15:27:19 @agent_ppo2.py:185][0m |          -0.0145 |          21.5388 |           0.2649 |
[32m[20221213 15:27:19 @agent_ppo2.py:185][0m |          -0.0158 |          21.4310 |           0.2646 |
[32m[20221213 15:27:19 @agent_ppo2.py:185][0m |          -0.0175 |          21.4145 |           0.2647 |
[32m[20221213 15:27:19 @agent_ppo2.py:185][0m |          -0.0143 |          21.4281 |           0.2646 |
[32m[20221213 15:27:19 @agent_ppo2.py:185][0m |          -0.0067 |          22.7258 |           0.2645 |
[32m[20221213 15:27:19 @agent_ppo2.py:185][0m |           0.0017 |          24.0164 |           0.2640 |
[32m[20221213 15:27:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:27:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.34
[32m[20221213 15:27:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.61
[32m[20221213 15:27:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.59
[32m[20221213 15:27:19 @agent_ppo2.py:143][0m Total time:      34.40 min
[32m[20221213 15:27:19 @agent_ppo2.py:145][0m 3106816 total steps have happened
[32m[20221213 15:27:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1517 --------------------------#
[32m[20221213 15:27:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0020 |          21.6476 |           0.2510 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0088 |          21.3792 |           0.2516 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0005 |          23.6935 |           0.2515 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0138 |          21.2066 |           0.2516 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0151 |          21.0910 |           0.2514 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0154 |          21.0026 |           0.2517 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0161 |          20.9425 |           0.2515 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0169 |          20.8808 |           0.2517 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0176 |          20.8464 |           0.2516 |
[32m[20221213 15:27:20 @agent_ppo2.py:185][0m |          -0.0208 |          20.8041 |           0.2518 |
[32m[20221213 15:27:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.68
[32m[20221213 15:27:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.55
[32m[20221213 15:27:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.06
[32m[20221213 15:27:21 @agent_ppo2.py:143][0m Total time:      34.42 min
[32m[20221213 15:27:21 @agent_ppo2.py:145][0m 3108864 total steps have happened
[32m[20221213 15:27:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1518 --------------------------#
[32m[20221213 15:27:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:21 @agent_ppo2.py:185][0m |           0.0085 |          21.2083 |           0.2579 |
[32m[20221213 15:27:21 @agent_ppo2.py:185][0m |          -0.0005 |          20.6355 |           0.2565 |
[32m[20221213 15:27:21 @agent_ppo2.py:185][0m |           0.0076 |          22.6815 |           0.2569 |
[32m[20221213 15:27:21 @agent_ppo2.py:185][0m |          -0.0083 |          20.4799 |           0.2574 |
[32m[20221213 15:27:21 @agent_ppo2.py:185][0m |          -0.0067 |          20.4748 |           0.2572 |
[32m[20221213 15:27:21 @agent_ppo2.py:185][0m |          -0.0005 |          21.3221 |           0.2572 |
[32m[20221213 15:27:21 @agent_ppo2.py:185][0m |          -0.0122 |          20.2352 |           0.2572 |
[32m[20221213 15:27:22 @agent_ppo2.py:185][0m |          -0.0137 |          20.2089 |           0.2567 |
[32m[20221213 15:27:22 @agent_ppo2.py:185][0m |          -0.0130 |          20.1468 |           0.2568 |
[32m[20221213 15:27:22 @agent_ppo2.py:185][0m |          -0.0115 |          20.1595 |           0.2568 |
[32m[20221213 15:27:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.23
[32m[20221213 15:27:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.96
[32m[20221213 15:27:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.33
[32m[20221213 15:27:22 @agent_ppo2.py:143][0m Total time:      34.45 min
[32m[20221213 15:27:22 @agent_ppo2.py:145][0m 3110912 total steps have happened
[32m[20221213 15:27:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1519 --------------------------#
[32m[20221213 15:27:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:22 @agent_ppo2.py:185][0m |          -0.0030 |          21.3126 |           0.2526 |
[32m[20221213 15:27:22 @agent_ppo2.py:185][0m |          -0.0070 |          21.0056 |           0.2519 |
[32m[20221213 15:27:22 @agent_ppo2.py:185][0m |          -0.0106 |          20.8539 |           0.2514 |
[32m[20221213 15:27:23 @agent_ppo2.py:185][0m |          -0.0130 |          20.7266 |           0.2511 |
[32m[20221213 15:27:23 @agent_ppo2.py:185][0m |          -0.0126 |          20.6068 |           0.2512 |
[32m[20221213 15:27:23 @agent_ppo2.py:185][0m |          -0.0141 |          20.5248 |           0.2508 |
[32m[20221213 15:27:23 @agent_ppo2.py:185][0m |          -0.0150 |          20.4359 |           0.2508 |
[32m[20221213 15:27:23 @agent_ppo2.py:185][0m |          -0.0036 |          21.8576 |           0.2509 |
[32m[20221213 15:27:23 @agent_ppo2.py:185][0m |          -0.0155 |          20.3047 |           0.2506 |
[32m[20221213 15:27:23 @agent_ppo2.py:185][0m |          -0.0155 |          20.2527 |           0.2512 |
[32m[20221213 15:27:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.23
[32m[20221213 15:27:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.23
[32m[20221213 15:27:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.43
[32m[20221213 15:27:23 @agent_ppo2.py:143][0m Total time:      34.47 min
[32m[20221213 15:27:23 @agent_ppo2.py:145][0m 3112960 total steps have happened
[32m[20221213 15:27:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1520 --------------------------#
[32m[20221213 15:27:23 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:27:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |           0.0023 |          20.6564 |           0.2515 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0076 |          20.2208 |           0.2510 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0093 |          19.9437 |           0.2507 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0113 |          19.8396 |           0.2503 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0130 |          19.7777 |           0.2504 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0122 |          19.6645 |           0.2499 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0105 |          19.8461 |           0.2501 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0141 |          19.5489 |           0.2497 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0130 |          19.4910 |           0.2494 |
[32m[20221213 15:27:24 @agent_ppo2.py:185][0m |          -0.0153 |          19.4271 |           0.2496 |
[32m[20221213 15:27:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.21
[32m[20221213 15:27:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.17
[32m[20221213 15:27:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.55
[32m[20221213 15:27:25 @agent_ppo2.py:143][0m Total time:      34.49 min
[32m[20221213 15:27:25 @agent_ppo2.py:145][0m 3115008 total steps have happened
[32m[20221213 15:27:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1521 --------------------------#
[32m[20221213 15:27:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:25 @agent_ppo2.py:185][0m |          -0.0013 |          21.1331 |           0.2601 |
[32m[20221213 15:27:25 @agent_ppo2.py:185][0m |          -0.0059 |          20.8310 |           0.2601 |
[32m[20221213 15:27:25 @agent_ppo2.py:185][0m |          -0.0077 |          20.6564 |           0.2600 |
[32m[20221213 15:27:25 @agent_ppo2.py:185][0m |          -0.0112 |          20.5389 |           0.2599 |
[32m[20221213 15:27:25 @agent_ppo2.py:185][0m |           0.0000 |          22.1353 |           0.2598 |
[32m[20221213 15:27:25 @agent_ppo2.py:185][0m |          -0.0110 |          20.4131 |           0.2595 |
[32m[20221213 15:27:25 @agent_ppo2.py:185][0m |          -0.0012 |          22.0070 |           0.2601 |
[32m[20221213 15:27:26 @agent_ppo2.py:185][0m |          -0.0024 |          22.3090 |           0.2599 |
[32m[20221213 15:27:26 @agent_ppo2.py:185][0m |          -0.0143 |          20.2414 |           0.2600 |
[32m[20221213 15:27:26 @agent_ppo2.py:185][0m |          -0.0157 |          20.1517 |           0.2604 |
[32m[20221213 15:27:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.67
[32m[20221213 15:27:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.88
[32m[20221213 15:27:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.87
[32m[20221213 15:27:26 @agent_ppo2.py:143][0m Total time:      34.51 min
[32m[20221213 15:27:26 @agent_ppo2.py:145][0m 3117056 total steps have happened
[32m[20221213 15:27:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1522 --------------------------#
[32m[20221213 15:27:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:26 @agent_ppo2.py:185][0m |           0.0126 |          23.3709 |           0.2616 |
[32m[20221213 15:27:26 @agent_ppo2.py:185][0m |           0.0001 |          21.2739 |           0.2611 |
[32m[20221213 15:27:26 @agent_ppo2.py:185][0m |          -0.0097 |          20.4841 |           0.2607 |
[32m[20221213 15:27:27 @agent_ppo2.py:185][0m |          -0.0104 |          20.3270 |           0.2608 |
[32m[20221213 15:27:27 @agent_ppo2.py:185][0m |          -0.0119 |          20.2214 |           0.2606 |
[32m[20221213 15:27:27 @agent_ppo2.py:185][0m |          -0.0021 |          20.8545 |           0.2605 |
[32m[20221213 15:27:27 @agent_ppo2.py:185][0m |          -0.0130 |          20.1594 |           0.2603 |
[32m[20221213 15:27:27 @agent_ppo2.py:185][0m |          -0.0125 |          20.0165 |           0.2602 |
[32m[20221213 15:27:27 @agent_ppo2.py:185][0m |          -0.0163 |          19.9744 |           0.2603 |
[32m[20221213 15:27:27 @agent_ppo2.py:185][0m |          -0.0158 |          19.9085 |           0.2601 |
[32m[20221213 15:27:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:27:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.44
[32m[20221213 15:27:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.28
[32m[20221213 15:27:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.28
[32m[20221213 15:27:27 @agent_ppo2.py:143][0m Total time:      34.53 min
[32m[20221213 15:27:27 @agent_ppo2.py:145][0m 3119104 total steps have happened
[32m[20221213 15:27:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1523 --------------------------#
[32m[20221213 15:27:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |           0.0002 |          20.8579 |           0.2528 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0065 |          20.6165 |           0.2525 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0090 |          20.5238 |           0.2525 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0098 |          20.4518 |           0.2523 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0129 |          20.4299 |           0.2520 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0128 |          20.3504 |           0.2519 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0154 |          20.3088 |           0.2521 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0159 |          20.2706 |           0.2520 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0152 |          20.2429 |           0.2520 |
[32m[20221213 15:27:28 @agent_ppo2.py:185][0m |          -0.0133 |          20.3393 |           0.2518 |
[32m[20221213 15:27:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.33
[32m[20221213 15:27:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.20
[32m[20221213 15:27:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.68
[32m[20221213 15:27:29 @agent_ppo2.py:143][0m Total time:      34.56 min
[32m[20221213 15:27:29 @agent_ppo2.py:145][0m 3121152 total steps have happened
[32m[20221213 15:27:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1524 --------------------------#
[32m[20221213 15:27:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:29 @agent_ppo2.py:185][0m |          -0.0014 |          21.7645 |           0.2473 |
[32m[20221213 15:27:29 @agent_ppo2.py:185][0m |          -0.0090 |          21.3264 |           0.2469 |
[32m[20221213 15:27:29 @agent_ppo2.py:185][0m |          -0.0092 |          21.1299 |           0.2464 |
[32m[20221213 15:27:29 @agent_ppo2.py:185][0m |          -0.0132 |          20.9369 |           0.2463 |
[32m[20221213 15:27:29 @agent_ppo2.py:185][0m |          -0.0123 |          20.8647 |           0.2463 |
[32m[20221213 15:27:29 @agent_ppo2.py:185][0m |          -0.0093 |          21.0851 |           0.2459 |
[32m[20221213 15:27:30 @agent_ppo2.py:185][0m |          -0.0142 |          20.6961 |           0.2458 |
[32m[20221213 15:27:30 @agent_ppo2.py:185][0m |          -0.0114 |          20.8774 |           0.2455 |
[32m[20221213 15:27:30 @agent_ppo2.py:185][0m |          -0.0155 |          20.4613 |           0.2453 |
[32m[20221213 15:27:30 @agent_ppo2.py:185][0m |          -0.0183 |          20.4436 |           0.2455 |
[32m[20221213 15:27:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:27:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 240.67
[32m[20221213 15:27:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.39
[32m[20221213 15:27:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.99
[32m[20221213 15:27:30 @agent_ppo2.py:143][0m Total time:      34.58 min
[32m[20221213 15:27:30 @agent_ppo2.py:145][0m 3123200 total steps have happened
[32m[20221213 15:27:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1525 --------------------------#
[32m[20221213 15:27:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:30 @agent_ppo2.py:185][0m |           0.0016 |          21.0861 |           0.2569 |
[32m[20221213 15:27:30 @agent_ppo2.py:185][0m |           0.0052 |          22.0440 |           0.2565 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0091 |          20.1612 |           0.2564 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0059 |          20.8313 |           0.2562 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0146 |          19.9760 |           0.2564 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0153 |          19.8648 |           0.2566 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0133 |          19.9256 |           0.2564 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0078 |          21.5924 |           0.2564 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0148 |          19.7844 |           0.2564 |
[32m[20221213 15:27:31 @agent_ppo2.py:185][0m |          -0.0179 |          19.4776 |           0.2563 |
[32m[20221213 15:27:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.99
[32m[20221213 15:27:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.12
[32m[20221213 15:27:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.38
[32m[20221213 15:27:31 @agent_ppo2.py:143][0m Total time:      34.60 min
[32m[20221213 15:27:31 @agent_ppo2.py:145][0m 3125248 total steps have happened
[32m[20221213 15:27:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1526 --------------------------#
[32m[20221213 15:27:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0014 |          21.0578 |           0.2532 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |           0.0038 |          22.6508 |           0.2527 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0077 |          20.5370 |           0.2526 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0145 |          20.4221 |           0.2522 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0125 |          20.3434 |           0.2521 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0117 |          20.3259 |           0.2517 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0164 |          20.2215 |           0.2513 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0161 |          20.1820 |           0.2512 |
[32m[20221213 15:27:32 @agent_ppo2.py:185][0m |          -0.0031 |          22.6951 |           0.2508 |
[32m[20221213 15:27:33 @agent_ppo2.py:185][0m |          -0.0162 |          20.1772 |           0.2507 |
[32m[20221213 15:27:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:27:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.79
[32m[20221213 15:27:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.45
[32m[20221213 15:27:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.22
[32m[20221213 15:27:33 @agent_ppo2.py:143][0m Total time:      34.63 min
[32m[20221213 15:27:33 @agent_ppo2.py:145][0m 3127296 total steps have happened
[32m[20221213 15:27:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1527 --------------------------#
[32m[20221213 15:27:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:33 @agent_ppo2.py:185][0m |          -0.0006 |          20.5528 |           0.2554 |
[32m[20221213 15:27:33 @agent_ppo2.py:185][0m |          -0.0067 |          20.3386 |           0.2548 |
[32m[20221213 15:27:33 @agent_ppo2.py:185][0m |          -0.0091 |          20.2600 |           0.2542 |
[32m[20221213 15:27:33 @agent_ppo2.py:185][0m |          -0.0045 |          20.7565 |           0.2542 |
[32m[20221213 15:27:33 @agent_ppo2.py:185][0m |          -0.0113 |          20.0783 |           0.2540 |
[32m[20221213 15:27:34 @agent_ppo2.py:185][0m |          -0.0152 |          20.0044 |           0.2539 |
[32m[20221213 15:27:34 @agent_ppo2.py:185][0m |          -0.0152 |          19.9824 |           0.2539 |
[32m[20221213 15:27:34 @agent_ppo2.py:185][0m |          -0.0110 |          20.0011 |           0.2536 |
[32m[20221213 15:27:34 @agent_ppo2.py:185][0m |          -0.0174 |          19.8556 |           0.2535 |
[32m[20221213 15:27:34 @agent_ppo2.py:185][0m |          -0.0119 |          20.2304 |           0.2535 |
[32m[20221213 15:27:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:27:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.77
[32m[20221213 15:27:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.80
[32m[20221213 15:27:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.87
[32m[20221213 15:27:34 @agent_ppo2.py:143][0m Total time:      34.65 min
[32m[20221213 15:27:34 @agent_ppo2.py:145][0m 3129344 total steps have happened
[32m[20221213 15:27:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1528 --------------------------#
[32m[20221213 15:27:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:34 @agent_ppo2.py:185][0m |          -0.0023 |          20.9552 |           0.2550 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0071 |          20.6441 |           0.2541 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0095 |          20.5979 |           0.2535 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0057 |          20.7996 |           0.2532 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0040 |          21.0167 |           0.2531 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0127 |          20.2936 |           0.2528 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0110 |          20.3346 |           0.2526 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0134 |          20.1790 |           0.2524 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0031 |          21.4266 |           0.2521 |
[32m[20221213 15:27:35 @agent_ppo2.py:185][0m |          -0.0142 |          20.0723 |           0.2516 |
[32m[20221213 15:27:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:27:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.26
[32m[20221213 15:27:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.68
[32m[20221213 15:27:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.30
[32m[20221213 15:27:35 @agent_ppo2.py:143][0m Total time:      34.67 min
[32m[20221213 15:27:35 @agent_ppo2.py:145][0m 3131392 total steps have happened
[32m[20221213 15:27:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1529 --------------------------#
[32m[20221213 15:27:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0015 |          21.5773 |           0.2415 |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0080 |          21.1751 |           0.2412 |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0083 |          20.9871 |           0.2411 |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0059 |          21.0879 |           0.2411 |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0107 |          20.7413 |           0.2411 |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0121 |          20.6328 |           0.2411 |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0159 |          20.6624 |           0.2409 |
[32m[20221213 15:27:36 @agent_ppo2.py:185][0m |          -0.0152 |          20.5220 |           0.2407 |
[32m[20221213 15:27:37 @agent_ppo2.py:185][0m |          -0.0025 |          22.2345 |           0.2409 |
[32m[20221213 15:27:37 @agent_ppo2.py:185][0m |          -0.0103 |          20.4706 |           0.2406 |
[32m[20221213 15:27:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.49
[32m[20221213 15:27:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.97
[32m[20221213 15:27:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.84
[32m[20221213 15:27:37 @agent_ppo2.py:143][0m Total time:      34.69 min
[32m[20221213 15:27:37 @agent_ppo2.py:145][0m 3133440 total steps have happened
[32m[20221213 15:27:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1530 --------------------------#
[32m[20221213 15:27:37 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:27:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:37 @agent_ppo2.py:185][0m |          -0.0029 |          21.0756 |           0.2437 |
[32m[20221213 15:27:37 @agent_ppo2.py:185][0m |           0.0148 |          22.7986 |           0.2430 |
[32m[20221213 15:27:37 @agent_ppo2.py:185][0m |          -0.0111 |          20.6439 |           0.2427 |
[32m[20221213 15:27:37 @agent_ppo2.py:185][0m |          -0.0051 |          20.8759 |           0.2426 |
[32m[20221213 15:27:38 @agent_ppo2.py:185][0m |          -0.0011 |          21.6183 |           0.2422 |
[32m[20221213 15:27:38 @agent_ppo2.py:185][0m |          -0.0070 |          21.2133 |           0.2423 |
[32m[20221213 15:27:38 @agent_ppo2.py:185][0m |          -0.0159 |          20.3335 |           0.2419 |
[32m[20221213 15:27:38 @agent_ppo2.py:185][0m |          -0.0173 |          20.2638 |           0.2422 |
[32m[20221213 15:27:38 @agent_ppo2.py:185][0m |          -0.0170 |          20.2085 |           0.2419 |
[32m[20221213 15:27:38 @agent_ppo2.py:185][0m |          -0.0109 |          22.0165 |           0.2420 |
[32m[20221213 15:27:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.82
[32m[20221213 15:27:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.84
[32m[20221213 15:27:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.56
[32m[20221213 15:27:38 @agent_ppo2.py:143][0m Total time:      34.72 min
[32m[20221213 15:27:38 @agent_ppo2.py:145][0m 3135488 total steps have happened
[32m[20221213 15:27:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1531 --------------------------#
[32m[20221213 15:27:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |           0.0001 |          19.6276 |           0.2383 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0041 |          19.2531 |           0.2373 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0088 |          19.1461 |           0.2375 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0096 |          19.0731 |           0.2374 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0130 |          18.9861 |           0.2371 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0152 |          18.9111 |           0.2368 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0145 |          18.8648 |           0.2368 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0189 |          18.7986 |           0.2368 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0175 |          18.7802 |           0.2368 |
[32m[20221213 15:27:39 @agent_ppo2.py:185][0m |          -0.0167 |          18.7323 |           0.2365 |
[32m[20221213 15:27:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.55
[32m[20221213 15:27:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.12
[32m[20221213 15:27:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.29
[32m[20221213 15:27:39 @agent_ppo2.py:143][0m Total time:      34.74 min
[32m[20221213 15:27:39 @agent_ppo2.py:145][0m 3137536 total steps have happened
[32m[20221213 15:27:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1532 --------------------------#
[32m[20221213 15:27:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0018 |          20.3813 |           0.2437 |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0080 |          19.8618 |           0.2432 |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0112 |          19.6013 |           0.2429 |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0147 |          19.4090 |           0.2429 |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0026 |          21.2824 |           0.2424 |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0131 |          19.2333 |           0.2420 |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0147 |          19.0437 |           0.2420 |
[32m[20221213 15:27:40 @agent_ppo2.py:185][0m |          -0.0054 |          20.8969 |           0.2421 |
[32m[20221213 15:27:41 @agent_ppo2.py:185][0m |          -0.0145 |          18.8861 |           0.2419 |
[32m[20221213 15:27:41 @agent_ppo2.py:185][0m |          -0.0130 |          18.9158 |           0.2418 |
[32m[20221213 15:27:41 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:27:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.63
[32m[20221213 15:27:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.86
[32m[20221213 15:27:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.55
[32m[20221213 15:27:41 @agent_ppo2.py:143][0m Total time:      34.76 min
[32m[20221213 15:27:41 @agent_ppo2.py:145][0m 3139584 total steps have happened
[32m[20221213 15:27:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1533 --------------------------#
[32m[20221213 15:27:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:41 @agent_ppo2.py:185][0m |          -0.0010 |          21.0751 |           0.2365 |
[32m[20221213 15:27:41 @agent_ppo2.py:185][0m |          -0.0095 |          20.8396 |           0.2362 |
[32m[20221213 15:27:41 @agent_ppo2.py:185][0m |          -0.0098 |          20.7199 |           0.2358 |
[32m[20221213 15:27:41 @agent_ppo2.py:185][0m |          -0.0087 |          20.6046 |           0.2356 |
[32m[20221213 15:27:42 @agent_ppo2.py:185][0m |          -0.0114 |          20.6265 |           0.2358 |
[32m[20221213 15:27:42 @agent_ppo2.py:185][0m |          -0.0076 |          20.6608 |           0.2353 |
[32m[20221213 15:27:42 @agent_ppo2.py:185][0m |          -0.0082 |          20.9193 |           0.2350 |
[32m[20221213 15:27:42 @agent_ppo2.py:185][0m |          -0.0129 |          20.4069 |           0.2350 |
[32m[20221213 15:27:42 @agent_ppo2.py:185][0m |          -0.0149 |          20.4276 |           0.2354 |
[32m[20221213 15:27:42 @agent_ppo2.py:185][0m |          -0.0153 |          20.3809 |           0.2352 |
[32m[20221213 15:27:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:27:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.17
[32m[20221213 15:27:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.45
[32m[20221213 15:27:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.41
[32m[20221213 15:27:42 @agent_ppo2.py:143][0m Total time:      34.78 min
[32m[20221213 15:27:42 @agent_ppo2.py:145][0m 3141632 total steps have happened
[32m[20221213 15:27:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1534 --------------------------#
[32m[20221213 15:27:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0003 |          18.7067 |           0.2353 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0114 |          17.8461 |           0.2349 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0100 |          17.5419 |           0.2346 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0078 |          17.7167 |           0.2344 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0149 |          17.1254 |           0.2341 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0154 |          16.9885 |           0.2341 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0047 |          17.8936 |           0.2341 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0122 |          16.8881 |           0.2338 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0109 |          17.0383 |           0.2338 |
[32m[20221213 15:27:43 @agent_ppo2.py:185][0m |          -0.0159 |          16.7254 |           0.2339 |
[32m[20221213 15:27:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 233.88
[32m[20221213 15:27:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.78
[32m[20221213 15:27:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.94
[32m[20221213 15:27:44 @agent_ppo2.py:143][0m Total time:      34.81 min
[32m[20221213 15:27:44 @agent_ppo2.py:145][0m 3143680 total steps have happened
[32m[20221213 15:27:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1535 --------------------------#
[32m[20221213 15:27:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:44 @agent_ppo2.py:185][0m |           0.0018 |          20.6327 |           0.2298 |
[32m[20221213 15:27:44 @agent_ppo2.py:185][0m |          -0.0041 |          19.9302 |           0.2297 |
[32m[20221213 15:27:44 @agent_ppo2.py:185][0m |           0.0027 |          21.5676 |           0.2296 |
[32m[20221213 15:27:44 @agent_ppo2.py:185][0m |          -0.0116 |          19.5460 |           0.2296 |
[32m[20221213 15:27:44 @agent_ppo2.py:185][0m |          -0.0125 |          19.3183 |           0.2294 |
[32m[20221213 15:27:44 @agent_ppo2.py:185][0m |          -0.0144 |          19.1586 |           0.2293 |
[32m[20221213 15:27:44 @agent_ppo2.py:185][0m |          -0.0168 |          19.1271 |           0.2293 |
[32m[20221213 15:27:45 @agent_ppo2.py:185][0m |          -0.0171 |          18.9829 |           0.2293 |
[32m[20221213 15:27:45 @agent_ppo2.py:185][0m |          -0.0183 |          18.8713 |           0.2294 |
[32m[20221213 15:27:45 @agent_ppo2.py:185][0m |          -0.0130 |          19.0337 |           0.2290 |
[32m[20221213 15:27:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:27:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.50
[32m[20221213 15:27:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.47
[32m[20221213 15:27:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.22
[32m[20221213 15:27:45 @agent_ppo2.py:143][0m Total time:      34.83 min
[32m[20221213 15:27:45 @agent_ppo2.py:145][0m 3145728 total steps have happened
[32m[20221213 15:27:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1536 --------------------------#
[32m[20221213 15:27:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:45 @agent_ppo2.py:185][0m |          -0.0018 |          20.7613 |           0.2269 |
[32m[20221213 15:27:45 @agent_ppo2.py:185][0m |          -0.0082 |          20.1043 |           0.2266 |
[32m[20221213 15:27:45 @agent_ppo2.py:185][0m |          -0.0111 |          19.7161 |           0.2266 |
[32m[20221213 15:27:45 @agent_ppo2.py:185][0m |          -0.0148 |          19.3954 |           0.2265 |
[32m[20221213 15:27:46 @agent_ppo2.py:185][0m |          -0.0045 |          20.5605 |           0.2264 |
[32m[20221213 15:27:46 @agent_ppo2.py:185][0m |          -0.0099 |          19.0706 |           0.2263 |
[32m[20221213 15:27:46 @agent_ppo2.py:185][0m |          -0.0162 |          18.9267 |           0.2263 |
[32m[20221213 15:27:46 @agent_ppo2.py:185][0m |          -0.0161 |          18.7854 |           0.2261 |
[32m[20221213 15:27:46 @agent_ppo2.py:185][0m |          -0.0176 |          18.7060 |           0.2263 |
[32m[20221213 15:27:46 @agent_ppo2.py:185][0m |          -0.0162 |          18.6846 |           0.2262 |
[32m[20221213 15:27:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.30
[32m[20221213 15:27:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.48
[32m[20221213 15:27:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.85
[32m[20221213 15:27:46 @agent_ppo2.py:143][0m Total time:      34.85 min
[32m[20221213 15:27:46 @agent_ppo2.py:145][0m 3147776 total steps have happened
[32m[20221213 15:27:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1537 --------------------------#
[32m[20221213 15:27:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0011 |          21.1416 |           0.2396 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0055 |          20.6188 |           0.2390 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0081 |          20.3749 |           0.2388 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0091 |          20.2264 |           0.2391 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0055 |          20.2956 |           0.2391 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0094 |          20.0819 |           0.2395 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0115 |          19.9696 |           0.2396 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0129 |          19.9401 |           0.2397 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0129 |          19.9340 |           0.2396 |
[32m[20221213 15:27:47 @agent_ppo2.py:185][0m |          -0.0150 |          19.8518 |           0.2398 |
[32m[20221213 15:27:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.17
[32m[20221213 15:27:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.35
[32m[20221213 15:27:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.44
[32m[20221213 15:27:48 @agent_ppo2.py:143][0m Total time:      34.87 min
[32m[20221213 15:27:48 @agent_ppo2.py:145][0m 3149824 total steps have happened
[32m[20221213 15:27:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1538 --------------------------#
[32m[20221213 15:27:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:48 @agent_ppo2.py:185][0m |           0.0006 |          19.5695 |           0.2375 |
[32m[20221213 15:27:48 @agent_ppo2.py:185][0m |          -0.0079 |          19.1718 |           0.2373 |
[32m[20221213 15:27:48 @agent_ppo2.py:185][0m |          -0.0077 |          18.9272 |           0.2373 |
[32m[20221213 15:27:48 @agent_ppo2.py:185][0m |          -0.0016 |          19.3080 |           0.2373 |
[32m[20221213 15:27:48 @agent_ppo2.py:185][0m |          -0.0078 |          18.6844 |           0.2377 |
[32m[20221213 15:27:48 @agent_ppo2.py:185][0m |          -0.0119 |          18.5691 |           0.2377 |
[32m[20221213 15:27:48 @agent_ppo2.py:185][0m |          -0.0032 |          19.8394 |           0.2377 |
[32m[20221213 15:27:49 @agent_ppo2.py:185][0m |          -0.0111 |          18.4442 |           0.2375 |
[32m[20221213 15:27:49 @agent_ppo2.py:185][0m |          -0.0127 |          18.3399 |           0.2375 |
[32m[20221213 15:27:49 @agent_ppo2.py:185][0m |          -0.0092 |          18.4491 |           0.2380 |
[32m[20221213 15:27:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.57
[32m[20221213 15:27:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.02
[32m[20221213 15:27:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.92
[32m[20221213 15:27:49 @agent_ppo2.py:143][0m Total time:      34.90 min
[32m[20221213 15:27:49 @agent_ppo2.py:145][0m 3151872 total steps have happened
[32m[20221213 15:27:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1539 --------------------------#
[32m[20221213 15:27:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:49 @agent_ppo2.py:185][0m |          -0.0018 |          19.6040 |           0.2398 |
[32m[20221213 15:27:49 @agent_ppo2.py:185][0m |          -0.0074 |          19.0928 |           0.2389 |
[32m[20221213 15:27:49 @agent_ppo2.py:185][0m |          -0.0097 |          18.8952 |           0.2391 |
[32m[20221213 15:27:50 @agent_ppo2.py:185][0m |          -0.0113 |          18.7662 |           0.2386 |
[32m[20221213 15:27:50 @agent_ppo2.py:185][0m |          -0.0149 |          18.5433 |           0.2385 |
[32m[20221213 15:27:50 @agent_ppo2.py:185][0m |          -0.0154 |          18.4195 |           0.2383 |
[32m[20221213 15:27:50 @agent_ppo2.py:185][0m |          -0.0080 |          19.6246 |           0.2383 |
[32m[20221213 15:27:50 @agent_ppo2.py:185][0m |          -0.0119 |          18.3078 |           0.2381 |
[32m[20221213 15:27:50 @agent_ppo2.py:185][0m |          -0.0161 |          18.2275 |           0.2381 |
[32m[20221213 15:27:50 @agent_ppo2.py:185][0m |          -0.0198 |          18.0409 |           0.2377 |
[32m[20221213 15:27:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:27:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.45
[32m[20221213 15:27:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.42
[32m[20221213 15:27:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.57
[32m[20221213 15:27:50 @agent_ppo2.py:143][0m Total time:      34.92 min
[32m[20221213 15:27:50 @agent_ppo2.py:145][0m 3153920 total steps have happened
[32m[20221213 15:27:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1540 --------------------------#
[32m[20221213 15:27:50 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:27:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0001 |          19.6491 |           0.2334 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0082 |          19.3789 |           0.2328 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0067 |          19.1461 |           0.2331 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0032 |          19.8566 |           0.2329 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0117 |          18.8832 |           0.2329 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0121 |          18.7716 |           0.2326 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0129 |          18.6357 |           0.2329 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0143 |          18.6504 |           0.2330 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0140 |          18.4904 |           0.2329 |
[32m[20221213 15:27:51 @agent_ppo2.py:185][0m |          -0.0158 |          18.4483 |           0.2328 |
[32m[20221213 15:27:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.83
[32m[20221213 15:27:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.06
[32m[20221213 15:27:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.64
[32m[20221213 15:27:52 @agent_ppo2.py:143][0m Total time:      34.94 min
[32m[20221213 15:27:52 @agent_ppo2.py:145][0m 3155968 total steps have happened
[32m[20221213 15:27:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1541 --------------------------#
[32m[20221213 15:27:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:52 @agent_ppo2.py:185][0m |           0.0103 |          21.0493 |           0.2425 |
[32m[20221213 15:27:52 @agent_ppo2.py:185][0m |           0.0084 |          20.5863 |           0.2422 |
[32m[20221213 15:27:52 @agent_ppo2.py:185][0m |          -0.0093 |          18.8881 |           0.2423 |
[32m[20221213 15:27:52 @agent_ppo2.py:185][0m |          -0.0125 |          18.7279 |           0.2423 |
[32m[20221213 15:27:52 @agent_ppo2.py:185][0m |          -0.0108 |          18.5998 |           0.2423 |
[32m[20221213 15:27:52 @agent_ppo2.py:185][0m |          -0.0125 |          18.5025 |           0.2424 |
[32m[20221213 15:27:53 @agent_ppo2.py:185][0m |          -0.0135 |          18.4046 |           0.2424 |
[32m[20221213 15:27:53 @agent_ppo2.py:185][0m |          -0.0157 |          18.3068 |           0.2425 |
[32m[20221213 15:27:53 @agent_ppo2.py:185][0m |          -0.0154 |          18.2361 |           0.2425 |
[32m[20221213 15:27:53 @agent_ppo2.py:185][0m |          -0.0172 |          18.1890 |           0.2427 |
[32m[20221213 15:27:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.93
[32m[20221213 15:27:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.45
[32m[20221213 15:27:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.31
[32m[20221213 15:27:53 @agent_ppo2.py:143][0m Total time:      34.96 min
[32m[20221213 15:27:53 @agent_ppo2.py:145][0m 3158016 total steps have happened
[32m[20221213 15:27:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1542 --------------------------#
[32m[20221213 15:27:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:27:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:53 @agent_ppo2.py:185][0m |          -0.0022 |          20.5992 |           0.2448 |
[32m[20221213 15:27:53 @agent_ppo2.py:185][0m |          -0.0075 |          20.2789 |           0.2447 |
[32m[20221213 15:27:53 @agent_ppo2.py:185][0m |          -0.0016 |          20.3366 |           0.2442 |
[32m[20221213 15:27:54 @agent_ppo2.py:185][0m |          -0.0052 |          20.3220 |           0.2442 |
[32m[20221213 15:27:54 @agent_ppo2.py:185][0m |          -0.0043 |          21.2976 |           0.2439 |
[32m[20221213 15:27:54 @agent_ppo2.py:185][0m |          -0.0123 |          19.9616 |           0.2439 |
[32m[20221213 15:27:54 @agent_ppo2.py:185][0m |          -0.0142 |          19.8240 |           0.2439 |
[32m[20221213 15:27:54 @agent_ppo2.py:185][0m |          -0.0145 |          19.7582 |           0.2438 |
[32m[20221213 15:27:54 @agent_ppo2.py:185][0m |          -0.0156 |          19.6735 |           0.2435 |
[32m[20221213 15:27:54 @agent_ppo2.py:185][0m |          -0.0188 |          19.6395 |           0.2435 |
[32m[20221213 15:27:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.98
[32m[20221213 15:27:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.78
[32m[20221213 15:27:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.49
[32m[20221213 15:27:54 @agent_ppo2.py:143][0m Total time:      34.99 min
[32m[20221213 15:27:54 @agent_ppo2.py:145][0m 3160064 total steps have happened
[32m[20221213 15:27:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1543 --------------------------#
[32m[20221213 15:27:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |           0.0007 |          20.0601 |           0.2404 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |           0.0089 |          21.5135 |           0.2404 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0077 |          19.4365 |           0.2402 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0084 |          19.3659 |           0.2403 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0105 |          19.1280 |           0.2402 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0148 |          18.9683 |           0.2401 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0143 |          18.9243 |           0.2402 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0156 |          18.8673 |           0.2402 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0148 |          18.8272 |           0.2399 |
[32m[20221213 15:27:55 @agent_ppo2.py:185][0m |          -0.0156 |          18.7131 |           0.2400 |
[32m[20221213 15:27:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 218.22
[32m[20221213 15:27:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 245.34
[32m[20221213 15:27:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.00
[32m[20221213 15:27:56 @agent_ppo2.py:143][0m Total time:      35.01 min
[32m[20221213 15:27:56 @agent_ppo2.py:145][0m 3162112 total steps have happened
[32m[20221213 15:27:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1544 --------------------------#
[32m[20221213 15:27:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:56 @agent_ppo2.py:185][0m |           0.0098 |          22.1129 |           0.2402 |
[32m[20221213 15:27:56 @agent_ppo2.py:185][0m |          -0.0068 |          19.9467 |           0.2392 |
[32m[20221213 15:27:56 @agent_ppo2.py:185][0m |          -0.0123 |          19.6292 |           0.2392 |
[32m[20221213 15:27:56 @agent_ppo2.py:185][0m |          -0.0104 |          19.5813 |           0.2391 |
[32m[20221213 15:27:56 @agent_ppo2.py:185][0m |          -0.0132 |          19.2988 |           0.2390 |
[32m[20221213 15:27:56 @agent_ppo2.py:185][0m |          -0.0144 |          19.2038 |           0.2391 |
[32m[20221213 15:27:57 @agent_ppo2.py:185][0m |          -0.0156 |          19.1245 |           0.2387 |
[32m[20221213 15:27:57 @agent_ppo2.py:185][0m |          -0.0163 |          19.0594 |           0.2389 |
[32m[20221213 15:27:57 @agent_ppo2.py:185][0m |          -0.0185 |          18.9825 |           0.2387 |
[32m[20221213 15:27:57 @agent_ppo2.py:185][0m |          -0.0164 |          18.9257 |           0.2388 |
[32m[20221213 15:27:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:27:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.81
[32m[20221213 15:27:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.73
[32m[20221213 15:27:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.16
[32m[20221213 15:27:57 @agent_ppo2.py:143][0m Total time:      35.03 min
[32m[20221213 15:27:57 @agent_ppo2.py:145][0m 3164160 total steps have happened
[32m[20221213 15:27:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1545 --------------------------#
[32m[20221213 15:27:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:57 @agent_ppo2.py:185][0m |          -0.0046 |          20.0120 |           0.2440 |
[32m[20221213 15:27:57 @agent_ppo2.py:185][0m |           0.0018 |          22.1609 |           0.2432 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |          -0.0039 |          20.6376 |           0.2424 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |          -0.0163 |          19.0384 |           0.2427 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |          -0.0170 |          18.9059 |           0.2426 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |          -0.0179 |          18.8007 |           0.2427 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |          -0.0147 |          18.7406 |           0.2424 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |           0.0024 |          21.7440 |           0.2423 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |          -0.0170 |          18.6649 |           0.2418 |
[32m[20221213 15:27:58 @agent_ppo2.py:185][0m |          -0.0193 |          18.5509 |           0.2421 |
[32m[20221213 15:27:58 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:27:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.41
[32m[20221213 15:27:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.03
[32m[20221213 15:27:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.46
[32m[20221213 15:27:58 @agent_ppo2.py:143][0m Total time:      35.05 min
[32m[20221213 15:27:58 @agent_ppo2.py:145][0m 3166208 total steps have happened
[32m[20221213 15:27:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1546 --------------------------#
[32m[20221213 15:27:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:27:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0030 |          18.7294 |           0.2385 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0069 |          18.1969 |           0.2382 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0092 |          18.0668 |           0.2379 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0119 |          17.9138 |           0.2377 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0130 |          17.8567 |           0.2377 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0153 |          17.8117 |           0.2377 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0148 |          17.7266 |           0.2377 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0155 |          17.6951 |           0.2376 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0140 |          17.6895 |           0.2376 |
[32m[20221213 15:27:59 @agent_ppo2.py:185][0m |          -0.0163 |          17.5922 |           0.2374 |
[32m[20221213 15:27:59 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.59
[32m[20221213 15:28:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.60
[32m[20221213 15:28:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.10
[32m[20221213 15:28:00 @agent_ppo2.py:143][0m Total time:      35.08 min
[32m[20221213 15:28:00 @agent_ppo2.py:145][0m 3168256 total steps have happened
[32m[20221213 15:28:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1547 --------------------------#
[32m[20221213 15:28:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:00 @agent_ppo2.py:185][0m |           0.0037 |          19.3999 |           0.2312 |
[32m[20221213 15:28:00 @agent_ppo2.py:185][0m |          -0.0088 |          18.7946 |           0.2309 |
[32m[20221213 15:28:00 @agent_ppo2.py:185][0m |          -0.0016 |          19.7648 |           0.2313 |
[32m[20221213 15:28:00 @agent_ppo2.py:185][0m |          -0.0118 |          18.4196 |           0.2313 |
[32m[20221213 15:28:00 @agent_ppo2.py:185][0m |          -0.0051 |          18.8769 |           0.2316 |
[32m[20221213 15:28:01 @agent_ppo2.py:185][0m |          -0.0105 |          18.5359 |           0.2314 |
[32m[20221213 15:28:01 @agent_ppo2.py:185][0m |          -0.0158 |          17.9776 |           0.2315 |
[32m[20221213 15:28:01 @agent_ppo2.py:185][0m |          -0.0165 |          17.8155 |           0.2315 |
[32m[20221213 15:28:01 @agent_ppo2.py:185][0m |          -0.0142 |          17.8013 |           0.2314 |
[32m[20221213 15:28:01 @agent_ppo2.py:185][0m |          -0.0122 |          18.2200 |           0.2316 |
[32m[20221213 15:28:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.52
[32m[20221213 15:28:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.50
[32m[20221213 15:28:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.48
[32m[20221213 15:28:01 @agent_ppo2.py:143][0m Total time:      35.10 min
[32m[20221213 15:28:01 @agent_ppo2.py:145][0m 3170304 total steps have happened
[32m[20221213 15:28:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1548 --------------------------#
[32m[20221213 15:28:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:01 @agent_ppo2.py:185][0m |           0.0017 |          19.7160 |           0.2443 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0072 |          18.9759 |           0.2442 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0105 |          18.7502 |           0.2437 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0130 |          18.5753 |           0.2435 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0139 |          18.4533 |           0.2435 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0060 |          19.1369 |           0.2434 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0089 |          18.4872 |           0.2437 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0157 |          18.2416 |           0.2435 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0168 |          18.2063 |           0.2436 |
[32m[20221213 15:28:02 @agent_ppo2.py:185][0m |          -0.0176 |          18.1582 |           0.2435 |
[32m[20221213 15:28:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.15
[32m[20221213 15:28:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.44
[32m[20221213 15:28:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.94
[32m[20221213 15:28:02 @agent_ppo2.py:143][0m Total time:      35.12 min
[32m[20221213 15:28:02 @agent_ppo2.py:145][0m 3172352 total steps have happened
[32m[20221213 15:28:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1549 --------------------------#
[32m[20221213 15:28:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:03 @agent_ppo2.py:185][0m |           0.0006 |          19.3320 |           0.2417 |
[32m[20221213 15:28:03 @agent_ppo2.py:185][0m |          -0.0045 |          19.0589 |           0.2415 |
[32m[20221213 15:28:03 @agent_ppo2.py:185][0m |          -0.0040 |          18.9905 |           0.2415 |
[32m[20221213 15:28:03 @agent_ppo2.py:185][0m |          -0.0100 |          18.8030 |           0.2413 |
[32m[20221213 15:28:03 @agent_ppo2.py:185][0m |          -0.0117 |          18.7288 |           0.2415 |
[32m[20221213 15:28:03 @agent_ppo2.py:185][0m |          -0.0128 |          18.6178 |           0.2415 |
[32m[20221213 15:28:03 @agent_ppo2.py:185][0m |          -0.0123 |          18.5895 |           0.2416 |
[32m[20221213 15:28:04 @agent_ppo2.py:185][0m |          -0.0130 |          18.5386 |           0.2413 |
[32m[20221213 15:28:04 @agent_ppo2.py:185][0m |          -0.0148 |          18.4399 |           0.2414 |
[32m[20221213 15:28:04 @agent_ppo2.py:185][0m |          -0.0062 |          19.4364 |           0.2415 |
[32m[20221213 15:28:04 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 15:28:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.15
[32m[20221213 15:28:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.35
[32m[20221213 15:28:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.29
[32m[20221213 15:28:04 @agent_ppo2.py:143][0m Total time:      35.15 min
[32m[20221213 15:28:04 @agent_ppo2.py:145][0m 3174400 total steps have happened
[32m[20221213 15:28:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1550 --------------------------#
[32m[20221213 15:28:04 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:28:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |           0.0065 |          21.4993 |           0.2379 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0076 |          19.8991 |           0.2373 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0049 |          20.1320 |           0.2378 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0121 |          19.5667 |           0.2380 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0126 |          19.5434 |           0.2380 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0138 |          19.3558 |           0.2379 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0081 |          20.3810 |           0.2380 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0161 |          19.2879 |           0.2380 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0162 |          19.2556 |           0.2382 |
[32m[20221213 15:28:05 @agent_ppo2.py:185][0m |          -0.0110 |          19.4826 |           0.2383 |
[32m[20221213 15:28:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 234.76
[32m[20221213 15:28:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 255.85
[32m[20221213 15:28:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.11
[32m[20221213 15:28:06 @agent_ppo2.py:143][0m Total time:      35.17 min
[32m[20221213 15:28:06 @agent_ppo2.py:145][0m 3176448 total steps have happened
[32m[20221213 15:28:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1551 --------------------------#
[32m[20221213 15:28:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:06 @agent_ppo2.py:185][0m |          -0.0016 |          19.7194 |           0.2429 |
[32m[20221213 15:28:06 @agent_ppo2.py:185][0m |          -0.0058 |          19.2535 |           0.2425 |
[32m[20221213 15:28:06 @agent_ppo2.py:185][0m |          -0.0104 |          19.0897 |           0.2421 |
[32m[20221213 15:28:06 @agent_ppo2.py:185][0m |          -0.0113 |          18.9456 |           0.2420 |
[32m[20221213 15:28:06 @agent_ppo2.py:185][0m |          -0.0137 |          18.8550 |           0.2416 |
[32m[20221213 15:28:06 @agent_ppo2.py:185][0m |          -0.0079 |          19.6046 |           0.2416 |
[32m[20221213 15:28:07 @agent_ppo2.py:185][0m |          -0.0110 |          19.1437 |           0.2414 |
[32m[20221213 15:28:07 @agent_ppo2.py:185][0m |          -0.0162 |          18.6729 |           0.2409 |
[32m[20221213 15:28:07 @agent_ppo2.py:185][0m |          -0.0165 |          18.6175 |           0.2412 |
[32m[20221213 15:28:07 @agent_ppo2.py:185][0m |          -0.0146 |          18.5639 |           0.2412 |
[32m[20221213 15:28:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.22
[32m[20221213 15:28:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.97
[32m[20221213 15:28:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.60
[32m[20221213 15:28:07 @agent_ppo2.py:143][0m Total time:      35.20 min
[32m[20221213 15:28:07 @agent_ppo2.py:145][0m 3178496 total steps have happened
[32m[20221213 15:28:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1552 --------------------------#
[32m[20221213 15:28:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:07 @agent_ppo2.py:185][0m |           0.0122 |          20.0027 |           0.2457 |
[32m[20221213 15:28:07 @agent_ppo2.py:185][0m |          -0.0009 |          19.1727 |           0.2450 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |          -0.0028 |          18.9979 |           0.2449 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |           0.0090 |          21.5676 |           0.2447 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |          -0.0150 |          18.3916 |           0.2446 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |          -0.0138 |          18.1531 |           0.2450 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |          -0.0137 |          18.0669 |           0.2449 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |          -0.0149 |          17.9949 |           0.2450 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |          -0.0145 |          17.9686 |           0.2450 |
[32m[20221213 15:28:08 @agent_ppo2.py:185][0m |          -0.0188 |          17.8900 |           0.2452 |
[32m[20221213 15:28:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.54
[32m[20221213 15:28:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.63
[32m[20221213 15:28:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.32
[32m[20221213 15:28:08 @agent_ppo2.py:143][0m Total time:      35.22 min
[32m[20221213 15:28:08 @agent_ppo2.py:145][0m 3180544 total steps have happened
[32m[20221213 15:28:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1553 --------------------------#
[32m[20221213 15:28:09 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:28:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |           0.0068 |          20.7622 |           0.2460 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0016 |          20.3396 |           0.2451 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0102 |          19.5923 |           0.2449 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0104 |          19.5166 |           0.2448 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0120 |          19.3592 |           0.2444 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0135 |          19.2731 |           0.2445 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0153 |          19.2382 |           0.2447 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0090 |          20.0629 |           0.2443 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0162 |          19.1098 |           0.2443 |
[32m[20221213 15:28:09 @agent_ppo2.py:185][0m |          -0.0094 |          19.5184 |           0.2445 |
[32m[20221213 15:28:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.17
[32m[20221213 15:28:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.44
[32m[20221213 15:28:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.71
[32m[20221213 15:28:10 @agent_ppo2.py:143][0m Total time:      35.24 min
[32m[20221213 15:28:10 @agent_ppo2.py:145][0m 3182592 total steps have happened
[32m[20221213 15:28:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1554 --------------------------#
[32m[20221213 15:28:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:10 @agent_ppo2.py:185][0m |           0.0023 |          19.1774 |           0.2472 |
[32m[20221213 15:28:10 @agent_ppo2.py:185][0m |          -0.0055 |          18.6325 |           0.2466 |
[32m[20221213 15:28:10 @agent_ppo2.py:185][0m |          -0.0087 |          18.4808 |           0.2467 |
[32m[20221213 15:28:10 @agent_ppo2.py:185][0m |          -0.0101 |          18.3447 |           0.2463 |
[32m[20221213 15:28:10 @agent_ppo2.py:185][0m |          -0.0034 |          19.2021 |           0.2464 |
[32m[20221213 15:28:11 @agent_ppo2.py:185][0m |          -0.0070 |          18.3986 |           0.2461 |
[32m[20221213 15:28:11 @agent_ppo2.py:185][0m |          -0.0121 |          18.1915 |           0.2463 |
[32m[20221213 15:28:11 @agent_ppo2.py:185][0m |          -0.0135 |          18.1339 |           0.2463 |
[32m[20221213 15:28:11 @agent_ppo2.py:185][0m |          -0.0126 |          18.0915 |           0.2462 |
[32m[20221213 15:28:11 @agent_ppo2.py:185][0m |          -0.0120 |          18.2555 |           0.2462 |
[32m[20221213 15:28:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.39
[32m[20221213 15:28:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.11
[32m[20221213 15:28:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.40
[32m[20221213 15:28:11 @agent_ppo2.py:143][0m Total time:      35.26 min
[32m[20221213 15:28:11 @agent_ppo2.py:145][0m 3184640 total steps have happened
[32m[20221213 15:28:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1555 --------------------------#
[32m[20221213 15:28:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:11 @agent_ppo2.py:185][0m |           0.0015 |          20.1226 |           0.2425 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0048 |          19.6361 |           0.2421 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |           0.0016 |          20.5830 |           0.2417 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0051 |          19.4158 |           0.2418 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0084 |          19.1256 |           0.2419 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0125 |          19.0233 |           0.2421 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0131 |          18.9329 |           0.2420 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0134 |          18.8257 |           0.2419 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0114 |          18.8131 |           0.2419 |
[32m[20221213 15:28:12 @agent_ppo2.py:185][0m |          -0.0146 |          18.7461 |           0.2421 |
[32m[20221213 15:28:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.07
[32m[20221213 15:28:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.86
[32m[20221213 15:28:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.34
[32m[20221213 15:28:12 @agent_ppo2.py:143][0m Total time:      35.29 min
[32m[20221213 15:28:12 @agent_ppo2.py:145][0m 3186688 total steps have happened
[32m[20221213 15:28:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1556 --------------------------#
[32m[20221213 15:28:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0013 |          20.3964 |           0.2475 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0037 |          20.2459 |           0.2477 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0078 |          19.8707 |           0.2477 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0107 |          19.6948 |           0.2478 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0105 |          19.5973 |           0.2478 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0141 |          19.4964 |           0.2482 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0152 |          19.3735 |           0.2477 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0150 |          19.3357 |           0.2482 |
[32m[20221213 15:28:13 @agent_ppo2.py:185][0m |          -0.0044 |          21.7744 |           0.2483 |
[32m[20221213 15:28:14 @agent_ppo2.py:185][0m |          -0.0161 |          19.2170 |           0.2482 |
[32m[20221213 15:28:14 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.67
[32m[20221213 15:28:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.83
[32m[20221213 15:28:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.93
[32m[20221213 15:28:14 @agent_ppo2.py:143][0m Total time:      35.31 min
[32m[20221213 15:28:14 @agent_ppo2.py:145][0m 3188736 total steps have happened
[32m[20221213 15:28:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1557 --------------------------#
[32m[20221213 15:28:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:14 @agent_ppo2.py:185][0m |           0.0016 |          19.9295 |           0.2477 |
[32m[20221213 15:28:14 @agent_ppo2.py:185][0m |          -0.0025 |          19.5937 |           0.2476 |
[32m[20221213 15:28:14 @agent_ppo2.py:185][0m |          -0.0031 |          19.5397 |           0.2480 |
[32m[20221213 15:28:14 @agent_ppo2.py:185][0m |           0.0013 |          20.1384 |           0.2481 |
[32m[20221213 15:28:14 @agent_ppo2.py:185][0m |           0.0155 |          22.7466 |           0.2479 |
[32m[20221213 15:28:15 @agent_ppo2.py:185][0m |          -0.0106 |          19.0710 |           0.2482 |
[32m[20221213 15:28:15 @agent_ppo2.py:185][0m |          -0.0089 |          19.0226 |           0.2482 |
[32m[20221213 15:28:15 @agent_ppo2.py:185][0m |          -0.0004 |          20.8843 |           0.2482 |
[32m[20221213 15:28:15 @agent_ppo2.py:185][0m |           0.0032 |          21.2515 |           0.2478 |
[32m[20221213 15:28:15 @agent_ppo2.py:185][0m |          -0.0055 |          19.4796 |           0.2479 |
[32m[20221213 15:28:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.97
[32m[20221213 15:28:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.99
[32m[20221213 15:28:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.91
[32m[20221213 15:28:15 @agent_ppo2.py:143][0m Total time:      35.33 min
[32m[20221213 15:28:15 @agent_ppo2.py:145][0m 3190784 total steps have happened
[32m[20221213 15:28:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1558 --------------------------#
[32m[20221213 15:28:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:15 @agent_ppo2.py:185][0m |           0.0218 |          23.6946 |           0.2521 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0047 |          20.0537 |           0.2520 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0095 |          19.8629 |           0.2514 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0131 |          19.7738 |           0.2512 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0141 |          19.7279 |           0.2511 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0113 |          19.7378 |           0.2510 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0153 |          19.5343 |           0.2510 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0155 |          19.4987 |           0.2510 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0140 |          19.4411 |           0.2508 |
[32m[20221213 15:28:16 @agent_ppo2.py:185][0m |          -0.0151 |          19.4086 |           0.2510 |
[32m[20221213 15:28:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.74
[32m[20221213 15:28:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.53
[32m[20221213 15:28:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.26
[32m[20221213 15:28:16 @agent_ppo2.py:143][0m Total time:      35.35 min
[32m[20221213 15:28:16 @agent_ppo2.py:145][0m 3192832 total steps have happened
[32m[20221213 15:28:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1559 --------------------------#
[32m[20221213 15:28:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |           0.0142 |          23.1349 |           0.2545 |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |          -0.0035 |          20.3861 |           0.2540 |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |          -0.0080 |          20.0660 |           0.2542 |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |          -0.0101 |          19.9107 |           0.2540 |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |          -0.0110 |          19.7472 |           0.2542 |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |          -0.0120 |          19.6411 |           0.2541 |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |          -0.0142 |          19.5762 |           0.2542 |
[32m[20221213 15:28:17 @agent_ppo2.py:185][0m |          -0.0140 |          19.4679 |           0.2543 |
[32m[20221213 15:28:18 @agent_ppo2.py:185][0m |          -0.0136 |          19.4201 |           0.2544 |
[32m[20221213 15:28:18 @agent_ppo2.py:185][0m |          -0.0166 |          19.3234 |           0.2544 |
[32m[20221213 15:28:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 228.71
[32m[20221213 15:28:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 248.35
[32m[20221213 15:28:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.18
[32m[20221213 15:28:18 @agent_ppo2.py:143][0m Total time:      35.38 min
[32m[20221213 15:28:18 @agent_ppo2.py:145][0m 3194880 total steps have happened
[32m[20221213 15:28:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1560 --------------------------#
[32m[20221213 15:28:18 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:28:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:18 @agent_ppo2.py:185][0m |           0.0008 |          19.6324 |           0.2502 |
[32m[20221213 15:28:18 @agent_ppo2.py:185][0m |          -0.0107 |          19.0671 |           0.2498 |
[32m[20221213 15:28:18 @agent_ppo2.py:185][0m |          -0.0127 |          18.7894 |           0.2494 |
[32m[20221213 15:28:18 @agent_ppo2.py:185][0m |          -0.0143 |          18.6080 |           0.2491 |
[32m[20221213 15:28:19 @agent_ppo2.py:185][0m |          -0.0175 |          18.4748 |           0.2490 |
[32m[20221213 15:28:19 @agent_ppo2.py:185][0m |          -0.0111 |          18.7043 |           0.2489 |
[32m[20221213 15:28:19 @agent_ppo2.py:185][0m |          -0.0133 |          18.4154 |           0.2490 |
[32m[20221213 15:28:19 @agent_ppo2.py:185][0m |          -0.0185 |          18.1017 |           0.2487 |
[32m[20221213 15:28:19 @agent_ppo2.py:185][0m |          -0.0204 |          18.0353 |           0.2489 |
[32m[20221213 15:28:19 @agent_ppo2.py:185][0m |          -0.0204 |          17.9389 |           0.2489 |
[32m[20221213 15:28:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.30
[32m[20221213 15:28:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.63
[32m[20221213 15:28:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.73
[32m[20221213 15:28:19 @agent_ppo2.py:143][0m Total time:      35.40 min
[32m[20221213 15:28:19 @agent_ppo2.py:145][0m 3196928 total steps have happened
[32m[20221213 15:28:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1561 --------------------------#
[32m[20221213 15:28:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |           0.0053 |          20.2001 |           0.2490 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0088 |          18.7614 |           0.2487 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0078 |          18.3260 |           0.2488 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0047 |          19.1133 |           0.2487 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0126 |          17.6636 |           0.2488 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0152 |          17.2723 |           0.2488 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0178 |          17.0657 |           0.2486 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0183 |          16.9526 |           0.2483 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0138 |          16.7982 |           0.2483 |
[32m[20221213 15:28:20 @agent_ppo2.py:185][0m |          -0.0187 |          16.7865 |           0.2483 |
[32m[20221213 15:28:20 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:28:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.61
[32m[20221213 15:28:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.16
[32m[20221213 15:28:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.49
[32m[20221213 15:28:21 @agent_ppo2.py:143][0m Total time:      35.42 min
[32m[20221213 15:28:21 @agent_ppo2.py:145][0m 3198976 total steps have happened
[32m[20221213 15:28:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1562 --------------------------#
[32m[20221213 15:28:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:21 @agent_ppo2.py:185][0m |          -0.0007 |          20.0562 |           0.2571 |
[32m[20221213 15:28:21 @agent_ppo2.py:185][0m |          -0.0073 |          19.5312 |           0.2574 |
[32m[20221213 15:28:21 @agent_ppo2.py:185][0m |          -0.0091 |          19.1829 |           0.2573 |
[32m[20221213 15:28:21 @agent_ppo2.py:185][0m |          -0.0099 |          18.8612 |           0.2575 |
[32m[20221213 15:28:21 @agent_ppo2.py:185][0m |          -0.0107 |          18.6003 |           0.2575 |
[32m[20221213 15:28:21 @agent_ppo2.py:185][0m |          -0.0130 |          18.3850 |           0.2575 |
[32m[20221213 15:28:21 @agent_ppo2.py:185][0m |          -0.0129 |          18.2153 |           0.2578 |
[32m[20221213 15:28:22 @agent_ppo2.py:185][0m |          -0.0145 |          18.0123 |           0.2579 |
[32m[20221213 15:28:22 @agent_ppo2.py:185][0m |          -0.0171 |          17.8547 |           0.2579 |
[32m[20221213 15:28:22 @agent_ppo2.py:185][0m |          -0.0173 |          17.7700 |           0.2580 |
[32m[20221213 15:28:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.53
[32m[20221213 15:28:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.59
[32m[20221213 15:28:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.64
[32m[20221213 15:28:22 @agent_ppo2.py:143][0m Total time:      35.45 min
[32m[20221213 15:28:22 @agent_ppo2.py:145][0m 3201024 total steps have happened
[32m[20221213 15:28:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1563 --------------------------#
[32m[20221213 15:28:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:22 @agent_ppo2.py:185][0m |          -0.0002 |          21.2560 |           0.2583 |
[32m[20221213 15:28:22 @agent_ppo2.py:185][0m |          -0.0071 |          20.5759 |           0.2581 |
[32m[20221213 15:28:22 @agent_ppo2.py:185][0m |          -0.0101 |          20.3121 |           0.2580 |
[32m[20221213 15:28:23 @agent_ppo2.py:185][0m |          -0.0096 |          20.1100 |           0.2581 |
[32m[20221213 15:28:23 @agent_ppo2.py:185][0m |          -0.0099 |          19.9536 |           0.2580 |
[32m[20221213 15:28:23 @agent_ppo2.py:185][0m |          -0.0141 |          19.8180 |           0.2583 |
[32m[20221213 15:28:23 @agent_ppo2.py:185][0m |          -0.0134 |          19.8034 |           0.2582 |
[32m[20221213 15:28:23 @agent_ppo2.py:185][0m |          -0.0163 |          19.6934 |           0.2580 |
[32m[20221213 15:28:23 @agent_ppo2.py:185][0m |          -0.0149 |          19.6713 |           0.2582 |
[32m[20221213 15:28:23 @agent_ppo2.py:185][0m |          -0.0186 |          19.5247 |           0.2583 |
[32m[20221213 15:28:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.45
[32m[20221213 15:28:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.28
[32m[20221213 15:28:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.02
[32m[20221213 15:28:23 @agent_ppo2.py:143][0m Total time:      35.47 min
[32m[20221213 15:28:23 @agent_ppo2.py:145][0m 3203072 total steps have happened
[32m[20221213 15:28:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1564 --------------------------#
[32m[20221213 15:28:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0040 |          21.5838 |           0.2640 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0035 |          21.4808 |           0.2642 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0097 |          20.8564 |           0.2639 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0116 |          20.6877 |           0.2638 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0102 |          20.7084 |           0.2637 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0143 |          20.4787 |           0.2637 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0124 |          20.3836 |           0.2636 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0135 |          20.3879 |           0.2637 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0058 |          21.1929 |           0.2635 |
[32m[20221213 15:28:24 @agent_ppo2.py:185][0m |          -0.0127 |          20.3093 |           0.2635 |
[32m[20221213 15:28:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.14
[32m[20221213 15:28:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.97
[32m[20221213 15:28:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.59
[32m[20221213 15:28:25 @agent_ppo2.py:143][0m Total time:      35.49 min
[32m[20221213 15:28:25 @agent_ppo2.py:145][0m 3205120 total steps have happened
[32m[20221213 15:28:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1565 --------------------------#
[32m[20221213 15:28:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:25 @agent_ppo2.py:185][0m |           0.0058 |          22.0429 |           0.2526 |
[32m[20221213 15:28:25 @agent_ppo2.py:185][0m |          -0.0061 |          20.9992 |           0.2523 |
[32m[20221213 15:28:25 @agent_ppo2.py:185][0m |          -0.0075 |          20.8214 |           0.2524 |
[32m[20221213 15:28:25 @agent_ppo2.py:185][0m |          -0.0060 |          21.1983 |           0.2521 |
[32m[20221213 15:28:25 @agent_ppo2.py:185][0m |          -0.0107 |          20.5873 |           0.2521 |
[32m[20221213 15:28:26 @agent_ppo2.py:185][0m |          -0.0116 |          20.4381 |           0.2525 |
[32m[20221213 15:28:26 @agent_ppo2.py:185][0m |          -0.0131 |          20.3927 |           0.2518 |
[32m[20221213 15:28:26 @agent_ppo2.py:185][0m |          -0.0152 |          20.2920 |           0.2517 |
[32m[20221213 15:28:26 @agent_ppo2.py:185][0m |          -0.0143 |          20.2408 |           0.2518 |
[32m[20221213 15:28:26 @agent_ppo2.py:185][0m |          -0.0161 |          20.1776 |           0.2517 |
[32m[20221213 15:28:26 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:28:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.31
[32m[20221213 15:28:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.13
[32m[20221213 15:28:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.67
[32m[20221213 15:28:26 @agent_ppo2.py:143][0m Total time:      35.51 min
[32m[20221213 15:28:26 @agent_ppo2.py:145][0m 3207168 total steps have happened
[32m[20221213 15:28:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1566 --------------------------#
[32m[20221213 15:28:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:26 @agent_ppo2.py:185][0m |           0.0022 |          21.1269 |           0.2558 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |          -0.0042 |          20.7403 |           0.2552 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |           0.0052 |          21.6789 |           0.2550 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |          -0.0061 |          20.5116 |           0.2552 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |           0.0010 |          22.4989 |           0.2545 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |          -0.0091 |          20.3603 |           0.2543 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |          -0.0110 |          20.2623 |           0.2540 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |          -0.0091 |          20.1974 |           0.2541 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |          -0.0109 |          20.1785 |           0.2539 |
[32m[20221213 15:28:27 @agent_ppo2.py:185][0m |          -0.0002 |          21.7943 |           0.2540 |
[32m[20221213 15:28:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.44
[32m[20221213 15:28:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.99
[32m[20221213 15:28:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.99
[32m[20221213 15:28:27 @agent_ppo2.py:143][0m Total time:      35.54 min
[32m[20221213 15:28:27 @agent_ppo2.py:145][0m 3209216 total steps have happened
[32m[20221213 15:28:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1567 --------------------------#
[32m[20221213 15:28:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0015 |          19.7932 |           0.2544 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0084 |          19.1189 |           0.2546 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0075 |          18.9104 |           0.2545 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |           0.0042 |          20.4141 |           0.2547 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0112 |          18.6950 |           0.2542 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0133 |          18.5197 |           0.2544 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0153 |          18.5461 |           0.2546 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0160 |          18.4661 |           0.2541 |
[32m[20221213 15:28:28 @agent_ppo2.py:185][0m |          -0.0173 |          18.3193 |           0.2544 |
[32m[20221213 15:28:29 @agent_ppo2.py:185][0m |          -0.0167 |          18.2796 |           0.2540 |
[32m[20221213 15:28:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.36
[32m[20221213 15:28:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.56
[32m[20221213 15:28:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.66
[32m[20221213 15:28:29 @agent_ppo2.py:143][0m Total time:      35.56 min
[32m[20221213 15:28:29 @agent_ppo2.py:145][0m 3211264 total steps have happened
[32m[20221213 15:28:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1568 --------------------------#
[32m[20221213 15:28:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:29 @agent_ppo2.py:185][0m |          -0.0036 |          21.3309 |           0.2533 |
[32m[20221213 15:28:29 @agent_ppo2.py:185][0m |          -0.0079 |          20.6143 |           0.2529 |
[32m[20221213 15:28:29 @agent_ppo2.py:185][0m |          -0.0121 |          20.3074 |           0.2526 |
[32m[20221213 15:28:29 @agent_ppo2.py:185][0m |          -0.0111 |          20.0419 |           0.2526 |
[32m[20221213 15:28:29 @agent_ppo2.py:185][0m |          -0.0146 |          19.8383 |           0.2524 |
[32m[20221213 15:28:30 @agent_ppo2.py:185][0m |          -0.0168 |          19.7052 |           0.2523 |
[32m[20221213 15:28:30 @agent_ppo2.py:185][0m |          -0.0168 |          19.5796 |           0.2522 |
[32m[20221213 15:28:30 @agent_ppo2.py:185][0m |          -0.0166 |          19.4861 |           0.2525 |
[32m[20221213 15:28:30 @agent_ppo2.py:185][0m |          -0.0160 |          19.3371 |           0.2522 |
[32m[20221213 15:28:30 @agent_ppo2.py:185][0m |          -0.0145 |          19.7041 |           0.2522 |
[32m[20221213 15:28:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.44
[32m[20221213 15:28:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.75
[32m[20221213 15:28:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.74
[32m[20221213 15:28:30 @agent_ppo2.py:143][0m Total time:      35.58 min
[32m[20221213 15:28:30 @agent_ppo2.py:145][0m 3213312 total steps have happened
[32m[20221213 15:28:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1569 --------------------------#
[32m[20221213 15:28:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0001 |          20.1175 |           0.2542 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0060 |          19.3341 |           0.2542 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0097 |          19.0221 |           0.2533 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0126 |          18.7310 |           0.2537 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0124 |          18.5897 |           0.2536 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0118 |          18.4556 |           0.2539 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0165 |          18.2824 |           0.2536 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0074 |          18.5169 |           0.2536 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0180 |          18.1780 |           0.2535 |
[32m[20221213 15:28:31 @agent_ppo2.py:185][0m |          -0.0165 |          18.0295 |           0.2537 |
[32m[20221213 15:28:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.83
[32m[20221213 15:28:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.12
[32m[20221213 15:28:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.22
[32m[20221213 15:28:31 @agent_ppo2.py:143][0m Total time:      35.60 min
[32m[20221213 15:28:31 @agent_ppo2.py:145][0m 3215360 total steps have happened
[32m[20221213 15:28:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1570 --------------------------#
[32m[20221213 15:28:32 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:28:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0006 |          21.5500 |           0.2580 |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0063 |          20.8503 |           0.2571 |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0024 |          21.4205 |           0.2569 |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0124 |          20.3051 |           0.2565 |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0080 |          20.5142 |           0.2563 |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0147 |          19.9344 |           0.2562 |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0172 |          19.8165 |           0.2560 |
[32m[20221213 15:28:32 @agent_ppo2.py:185][0m |          -0.0158 |          19.6705 |           0.2559 |
[32m[20221213 15:28:33 @agent_ppo2.py:185][0m |          -0.0187 |          19.5994 |           0.2559 |
[32m[20221213 15:28:33 @agent_ppo2.py:185][0m |          -0.0148 |          19.7620 |           0.2556 |
[32m[20221213 15:28:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.23
[32m[20221213 15:28:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.23
[32m[20221213 15:28:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.01
[32m[20221213 15:28:33 @agent_ppo2.py:143][0m Total time:      35.63 min
[32m[20221213 15:28:33 @agent_ppo2.py:145][0m 3217408 total steps have happened
[32m[20221213 15:28:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1571 --------------------------#
[32m[20221213 15:28:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:33 @agent_ppo2.py:185][0m |          -0.0024 |          22.7842 |           0.2572 |
[32m[20221213 15:28:33 @agent_ppo2.py:185][0m |          -0.0095 |          22.3601 |           0.2567 |
[32m[20221213 15:28:33 @agent_ppo2.py:185][0m |          -0.0103 |          22.1443 |           0.2568 |
[32m[20221213 15:28:33 @agent_ppo2.py:185][0m |          -0.0116 |          21.9439 |           0.2564 |
[32m[20221213 15:28:34 @agent_ppo2.py:185][0m |          -0.0156 |          21.8347 |           0.2563 |
[32m[20221213 15:28:34 @agent_ppo2.py:185][0m |          -0.0069 |          22.2379 |           0.2563 |
[32m[20221213 15:28:34 @agent_ppo2.py:185][0m |          -0.0103 |          21.8213 |           0.2564 |
[32m[20221213 15:28:34 @agent_ppo2.py:185][0m |          -0.0163 |          21.5285 |           0.2561 |
[32m[20221213 15:28:34 @agent_ppo2.py:185][0m |          -0.0172 |          21.4638 |           0.2562 |
[32m[20221213 15:28:34 @agent_ppo2.py:185][0m |          -0.0096 |          22.7298 |           0.2561 |
[32m[20221213 15:28:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.14
[32m[20221213 15:28:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.70
[32m[20221213 15:28:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.45
[32m[20221213 15:28:34 @agent_ppo2.py:143][0m Total time:      35.65 min
[32m[20221213 15:28:34 @agent_ppo2.py:145][0m 3219456 total steps have happened
[32m[20221213 15:28:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1572 --------------------------#
[32m[20221213 15:28:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0013 |          20.8521 |           0.2584 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0131 |          20.3218 |           0.2576 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0142 |          20.1128 |           0.2575 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0172 |          19.8871 |           0.2573 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0178 |          19.6849 |           0.2574 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0151 |          19.7519 |           0.2572 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0080 |          20.7217 |           0.2572 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0102 |          20.6108 |           0.2566 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0188 |          19.1889 |           0.2571 |
[32m[20221213 15:28:35 @agent_ppo2.py:185][0m |          -0.0195 |          19.0833 |           0.2567 |
[32m[20221213 15:28:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.15
[32m[20221213 15:28:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.97
[32m[20221213 15:28:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.05
[32m[20221213 15:28:36 @agent_ppo2.py:143][0m Total time:      35.67 min
[32m[20221213 15:28:36 @agent_ppo2.py:145][0m 3221504 total steps have happened
[32m[20221213 15:28:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1573 --------------------------#
[32m[20221213 15:28:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |           0.0005 |          19.9738 |           0.2554 |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |          -0.0079 |          18.4286 |           0.2545 |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |          -0.0115 |          17.5071 |           0.2541 |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |          -0.0122 |          16.9069 |           0.2536 |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |          -0.0054 |          16.8882 |           0.2535 |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |          -0.0156 |          16.0409 |           0.2531 |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |          -0.0164 |          15.7295 |           0.2529 |
[32m[20221213 15:28:36 @agent_ppo2.py:185][0m |          -0.0166 |          15.4901 |           0.2526 |
[32m[20221213 15:28:37 @agent_ppo2.py:185][0m |          -0.0173 |          15.2324 |           0.2525 |
[32m[20221213 15:28:37 @agent_ppo2.py:185][0m |          -0.0160 |          15.2028 |           0.2525 |
[32m[20221213 15:28:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.49
[32m[20221213 15:28:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.23
[32m[20221213 15:28:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.07
[32m[20221213 15:28:37 @agent_ppo2.py:143][0m Total time:      35.69 min
[32m[20221213 15:28:37 @agent_ppo2.py:145][0m 3223552 total steps have happened
[32m[20221213 15:28:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1574 --------------------------#
[32m[20221213 15:28:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:37 @agent_ppo2.py:185][0m |           0.0060 |          22.9190 |           0.2480 |
[32m[20221213 15:28:37 @agent_ppo2.py:185][0m |          -0.0075 |          21.6872 |           0.2476 |
[32m[20221213 15:28:37 @agent_ppo2.py:185][0m |          -0.0015 |          22.2806 |           0.2471 |
[32m[20221213 15:28:38 @agent_ppo2.py:185][0m |          -0.0113 |          21.1811 |           0.2468 |
[32m[20221213 15:28:38 @agent_ppo2.py:185][0m |          -0.0150 |          20.9722 |           0.2468 |
[32m[20221213 15:28:38 @agent_ppo2.py:185][0m |          -0.0154 |          20.7996 |           0.2468 |
[32m[20221213 15:28:38 @agent_ppo2.py:185][0m |           0.0001 |          23.3116 |           0.2469 |
[32m[20221213 15:28:38 @agent_ppo2.py:185][0m |          -0.0062 |          21.5219 |           0.2466 |
[32m[20221213 15:28:38 @agent_ppo2.py:185][0m |          -0.0178 |          20.4556 |           0.2465 |
[32m[20221213 15:28:38 @agent_ppo2.py:185][0m |          -0.0179 |          20.3791 |           0.2464 |
[32m[20221213 15:28:38 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:28:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.88
[32m[20221213 15:28:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.01
[32m[20221213 15:28:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.65
[32m[20221213 15:28:38 @agent_ppo2.py:143][0m Total time:      35.72 min
[32m[20221213 15:28:38 @agent_ppo2.py:145][0m 3225600 total steps have happened
[32m[20221213 15:28:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1575 --------------------------#
[32m[20221213 15:28:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0011 |          22.2649 |           0.2458 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0045 |          20.9481 |           0.2455 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0088 |          20.3204 |           0.2458 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0054 |          19.8898 |           0.2454 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0105 |          19.5707 |           0.2456 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0106 |          19.3678 |           0.2457 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0185 |          19.1015 |           0.2455 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0144 |          18.9820 |           0.2457 |
[32m[20221213 15:28:39 @agent_ppo2.py:185][0m |          -0.0139 |          19.0102 |           0.2458 |
[32m[20221213 15:28:40 @agent_ppo2.py:185][0m |          -0.0153 |          18.6683 |           0.2456 |
[32m[20221213 15:28:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:28:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.50
[32m[20221213 15:28:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.48
[32m[20221213 15:28:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.22
[32m[20221213 15:28:40 @agent_ppo2.py:143][0m Total time:      35.74 min
[32m[20221213 15:28:40 @agent_ppo2.py:145][0m 3227648 total steps have happened
[32m[20221213 15:28:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1576 --------------------------#
[32m[20221213 15:28:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:40 @agent_ppo2.py:185][0m |          -0.0037 |          24.2705 |           0.2488 |
[32m[20221213 15:28:40 @agent_ppo2.py:185][0m |          -0.0014 |          24.3307 |           0.2484 |
[32m[20221213 15:28:40 @agent_ppo2.py:185][0m |          -0.0111 |          22.9088 |           0.2481 |
[32m[20221213 15:28:40 @agent_ppo2.py:185][0m |          -0.0153 |          22.5859 |           0.2485 |
[32m[20221213 15:28:40 @agent_ppo2.py:185][0m |          -0.0173 |          22.4035 |           0.2486 |
[32m[20221213 15:28:41 @agent_ppo2.py:185][0m |          -0.0092 |          22.9388 |           0.2486 |
[32m[20221213 15:28:41 @agent_ppo2.py:185][0m |          -0.0186 |          22.2115 |           0.2481 |
[32m[20221213 15:28:41 @agent_ppo2.py:185][0m |          -0.0150 |          22.2257 |           0.2485 |
[32m[20221213 15:28:41 @agent_ppo2.py:185][0m |          -0.0111 |          22.4699 |           0.2488 |
[32m[20221213 15:28:41 @agent_ppo2.py:185][0m |          -0.0195 |          21.8199 |           0.2488 |
[32m[20221213 15:28:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:28:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.85
[32m[20221213 15:28:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.95
[32m[20221213 15:28:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.60
[32m[20221213 15:28:41 @agent_ppo2.py:143][0m Total time:      35.77 min
[32m[20221213 15:28:41 @agent_ppo2.py:145][0m 3229696 total steps have happened
[32m[20221213 15:28:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1577 --------------------------#
[32m[20221213 15:28:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0000 |          23.5171 |           0.2619 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0059 |          22.7426 |           0.2616 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0107 |          22.2523 |           0.2613 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0027 |          23.0413 |           0.2616 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0144 |          21.7379 |           0.2613 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0169 |          21.4342 |           0.2617 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0153 |          21.2140 |           0.2617 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0161 |          21.0957 |           0.2616 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0162 |          20.9766 |           0.2617 |
[32m[20221213 15:28:42 @agent_ppo2.py:185][0m |          -0.0157 |          20.7682 |           0.2616 |
[32m[20221213 15:28:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.31
[32m[20221213 15:28:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.11
[32m[20221213 15:28:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.51
[32m[20221213 15:28:42 @agent_ppo2.py:143][0m Total time:      35.79 min
[32m[20221213 15:28:42 @agent_ppo2.py:145][0m 3231744 total steps have happened
[32m[20221213 15:28:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1578 --------------------------#
[32m[20221213 15:28:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0018 |          23.1100 |           0.2551 |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0051 |          22.4033 |           0.2549 |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0002 |          24.0069 |           0.2544 |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0113 |          21.8085 |           0.2539 |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0144 |          21.5717 |           0.2542 |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0148 |          21.4569 |           0.2538 |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0073 |          21.5667 |           0.2537 |
[32m[20221213 15:28:43 @agent_ppo2.py:185][0m |          -0.0187 |          21.2846 |           0.2538 |
[32m[20221213 15:28:44 @agent_ppo2.py:185][0m |          -0.0181 |          21.1159 |           0.2532 |
[32m[20221213 15:28:44 @agent_ppo2.py:185][0m |          -0.0180 |          21.0803 |           0.2535 |
[32m[20221213 15:28:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.04
[32m[20221213 15:28:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.58
[32m[20221213 15:28:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.01
[32m[20221213 15:28:44 @agent_ppo2.py:143][0m Total time:      35.81 min
[32m[20221213 15:28:44 @agent_ppo2.py:145][0m 3233792 total steps have happened
[32m[20221213 15:28:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1579 --------------------------#
[32m[20221213 15:28:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:44 @agent_ppo2.py:185][0m |           0.0033 |          22.2536 |           0.2551 |
[32m[20221213 15:28:44 @agent_ppo2.py:185][0m |          -0.0042 |          21.2016 |           0.2540 |
[32m[20221213 15:28:44 @agent_ppo2.py:185][0m |          -0.0117 |          20.8887 |           0.2534 |
[32m[20221213 15:28:44 @agent_ppo2.py:185][0m |          -0.0133 |          20.7592 |           0.2536 |
[32m[20221213 15:28:45 @agent_ppo2.py:185][0m |          -0.0017 |          22.4725 |           0.2536 |
[32m[20221213 15:28:45 @agent_ppo2.py:185][0m |          -0.0068 |          21.9034 |           0.2527 |
[32m[20221213 15:28:45 @agent_ppo2.py:185][0m |          -0.0161 |          20.5419 |           0.2530 |
[32m[20221213 15:28:45 @agent_ppo2.py:185][0m |          -0.0181 |          20.4569 |           0.2528 |
[32m[20221213 15:28:45 @agent_ppo2.py:185][0m |          -0.0185 |          20.4209 |           0.2527 |
[32m[20221213 15:28:45 @agent_ppo2.py:185][0m |          -0.0168 |          20.3720 |           0.2524 |
[32m[20221213 15:28:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.63
[32m[20221213 15:28:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.59
[32m[20221213 15:28:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.14
[32m[20221213 15:28:45 @agent_ppo2.py:143][0m Total time:      35.83 min
[32m[20221213 15:28:45 @agent_ppo2.py:145][0m 3235840 total steps have happened
[32m[20221213 15:28:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1580 --------------------------#
[32m[20221213 15:28:45 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:28:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0018 |          23.2486 |           0.2588 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0079 |          22.6217 |           0.2583 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0113 |          22.3087 |           0.2583 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0140 |          22.0266 |           0.2579 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0116 |          21.8785 |           0.2582 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0143 |          21.6499 |           0.2579 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0094 |          22.1960 |           0.2580 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0157 |          21.3870 |           0.2578 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0171 |          21.3112 |           0.2580 |
[32m[20221213 15:28:46 @agent_ppo2.py:185][0m |          -0.0029 |          24.4646 |           0.2581 |
[32m[20221213 15:28:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.15
[32m[20221213 15:28:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.12
[32m[20221213 15:28:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.14
[32m[20221213 15:28:47 @agent_ppo2.py:143][0m Total time:      35.86 min
[32m[20221213 15:28:47 @agent_ppo2.py:145][0m 3237888 total steps have happened
[32m[20221213 15:28:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1581 --------------------------#
[32m[20221213 15:28:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:47 @agent_ppo2.py:185][0m |          -0.0025 |          22.6398 |           0.2462 |
[32m[20221213 15:28:47 @agent_ppo2.py:185][0m |          -0.0094 |          22.2665 |           0.2456 |
[32m[20221213 15:28:47 @agent_ppo2.py:185][0m |          -0.0100 |          22.1083 |           0.2455 |
[32m[20221213 15:28:47 @agent_ppo2.py:185][0m |          -0.0093 |          22.1026 |           0.2457 |
[32m[20221213 15:28:47 @agent_ppo2.py:185][0m |          -0.0042 |          22.9814 |           0.2453 |
[32m[20221213 15:28:47 @agent_ppo2.py:185][0m |          -0.0130 |          21.9491 |           0.2452 |
[32m[20221213 15:28:47 @agent_ppo2.py:185][0m |          -0.0041 |          24.0566 |           0.2453 |
[32m[20221213 15:28:48 @agent_ppo2.py:185][0m |          -0.0081 |          22.7070 |           0.2450 |
[32m[20221213 15:28:48 @agent_ppo2.py:185][0m |          -0.0135 |          22.0468 |           0.2451 |
[32m[20221213 15:28:48 @agent_ppo2.py:185][0m |          -0.0079 |          23.1799 |           0.2449 |
[32m[20221213 15:28:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:28:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.70
[32m[20221213 15:28:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.32
[32m[20221213 15:28:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.13
[32m[20221213 15:28:48 @agent_ppo2.py:143][0m Total time:      35.88 min
[32m[20221213 15:28:48 @agent_ppo2.py:145][0m 3239936 total steps have happened
[32m[20221213 15:28:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1582 --------------------------#
[32m[20221213 15:28:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:48 @agent_ppo2.py:185][0m |           0.0089 |          22.8731 |           0.2533 |
[32m[20221213 15:28:48 @agent_ppo2.py:185][0m |          -0.0089 |          20.8550 |           0.2532 |
[32m[20221213 15:28:48 @agent_ppo2.py:185][0m |          -0.0090 |          20.5388 |           0.2527 |
[32m[20221213 15:28:49 @agent_ppo2.py:185][0m |          -0.0000 |          22.9620 |           0.2525 |
[32m[20221213 15:28:49 @agent_ppo2.py:185][0m |          -0.0131 |          20.2059 |           0.2519 |
[32m[20221213 15:28:49 @agent_ppo2.py:185][0m |          -0.0158 |          20.1410 |           0.2518 |
[32m[20221213 15:28:49 @agent_ppo2.py:185][0m |          -0.0156 |          20.0233 |           0.2519 |
[32m[20221213 15:28:49 @agent_ppo2.py:185][0m |          -0.0141 |          19.9839 |           0.2517 |
[32m[20221213 15:28:49 @agent_ppo2.py:185][0m |          -0.0155 |          19.8340 |           0.2516 |
[32m[20221213 15:28:49 @agent_ppo2.py:185][0m |          -0.0172 |          19.7073 |           0.2512 |
[32m[20221213 15:28:49 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:28:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.21
[32m[20221213 15:28:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.61
[32m[20221213 15:28:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.90
[32m[20221213 15:28:49 @agent_ppo2.py:143][0m Total time:      35.90 min
[32m[20221213 15:28:49 @agent_ppo2.py:145][0m 3241984 total steps have happened
[32m[20221213 15:28:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1583 --------------------------#
[32m[20221213 15:28:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |           0.0028 |          23.0974 |           0.2419 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0054 |          22.3913 |           0.2413 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0054 |          22.1004 |           0.2415 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0082 |          21.8726 |           0.2418 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0097 |          21.6919 |           0.2417 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0062 |          22.1915 |           0.2419 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0134 |          21.4821 |           0.2419 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0122 |          21.4100 |           0.2421 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0138 |          21.3303 |           0.2421 |
[32m[20221213 15:28:50 @agent_ppo2.py:185][0m |          -0.0163 |          21.2011 |           0.2421 |
[32m[20221213 15:28:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.06
[32m[20221213 15:28:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.13
[32m[20221213 15:28:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.49
[32m[20221213 15:28:51 @agent_ppo2.py:143][0m Total time:      35.92 min
[32m[20221213 15:28:51 @agent_ppo2.py:145][0m 3244032 total steps have happened
[32m[20221213 15:28:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1584 --------------------------#
[32m[20221213 15:28:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:51 @agent_ppo2.py:185][0m |           0.0002 |          22.6050 |           0.2492 |
[32m[20221213 15:28:51 @agent_ppo2.py:185][0m |          -0.0031 |          22.1537 |           0.2488 |
[32m[20221213 15:28:51 @agent_ppo2.py:185][0m |          -0.0099 |          21.8381 |           0.2488 |
[32m[20221213 15:28:51 @agent_ppo2.py:185][0m |          -0.0008 |          22.5190 |           0.2486 |
[32m[20221213 15:28:51 @agent_ppo2.py:185][0m |          -0.0066 |          22.2176 |           0.2477 |
[32m[20221213 15:28:51 @agent_ppo2.py:185][0m |          -0.0135 |          21.5122 |           0.2477 |
[32m[20221213 15:28:52 @agent_ppo2.py:185][0m |          -0.0158 |          21.4216 |           0.2476 |
[32m[20221213 15:28:52 @agent_ppo2.py:185][0m |          -0.0171 |          21.3672 |           0.2475 |
[32m[20221213 15:28:52 @agent_ppo2.py:185][0m |          -0.0165 |          21.3195 |           0.2474 |
[32m[20221213 15:28:52 @agent_ppo2.py:185][0m |          -0.0158 |          21.2600 |           0.2471 |
[32m[20221213 15:28:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:28:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.96
[32m[20221213 15:28:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.67
[32m[20221213 15:28:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.63
[32m[20221213 15:28:52 @agent_ppo2.py:143][0m Total time:      35.95 min
[32m[20221213 15:28:52 @agent_ppo2.py:145][0m 3246080 total steps have happened
[32m[20221213 15:28:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1585 --------------------------#
[32m[20221213 15:28:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:52 @agent_ppo2.py:185][0m |          -0.0026 |          21.0398 |           0.2446 |
[32m[20221213 15:28:52 @agent_ppo2.py:185][0m |          -0.0073 |          19.9150 |           0.2441 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0068 |          19.2770 |           0.2440 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0117 |          18.4194 |           0.2443 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0132 |          18.1700 |           0.2441 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0154 |          17.9665 |           0.2443 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0163 |          17.6819 |           0.2442 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0084 |          18.2089 |           0.2443 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0186 |          17.3935 |           0.2444 |
[32m[20221213 15:28:53 @agent_ppo2.py:185][0m |          -0.0188 |          17.3485 |           0.2441 |
[32m[20221213 15:28:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.79
[32m[20221213 15:28:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.28
[32m[20221213 15:28:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.31
[32m[20221213 15:28:53 @agent_ppo2.py:143][0m Total time:      35.97 min
[32m[20221213 15:28:53 @agent_ppo2.py:145][0m 3248128 total steps have happened
[32m[20221213 15:28:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1586 --------------------------#
[32m[20221213 15:28:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |           0.0004 |          22.7272 |           0.2478 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |           0.0133 |          25.4477 |           0.2482 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |          -0.0067 |          22.1760 |           0.2480 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |          -0.0121 |          21.8732 |           0.2481 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |          -0.0134 |          21.8119 |           0.2479 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |          -0.0131 |          21.7422 |           0.2478 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |          -0.0074 |          22.9766 |           0.2479 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |          -0.0114 |          21.7170 |           0.2486 |
[32m[20221213 15:28:54 @agent_ppo2.py:185][0m |          -0.0168 |          21.5339 |           0.2479 |
[32m[20221213 15:28:55 @agent_ppo2.py:185][0m |          -0.0073 |          22.5517 |           0.2483 |
[32m[20221213 15:28:55 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:28:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.59
[32m[20221213 15:28:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.08
[32m[20221213 15:28:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.32
[32m[20221213 15:28:55 @agent_ppo2.py:143][0m Total time:      35.99 min
[32m[20221213 15:28:55 @agent_ppo2.py:145][0m 3250176 total steps have happened
[32m[20221213 15:28:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1587 --------------------------#
[32m[20221213 15:28:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:55 @agent_ppo2.py:185][0m |           0.0010 |          22.1489 |           0.2521 |
[32m[20221213 15:28:55 @agent_ppo2.py:185][0m |          -0.0078 |          21.2738 |           0.2520 |
[32m[20221213 15:28:55 @agent_ppo2.py:185][0m |          -0.0100 |          20.8942 |           0.2519 |
[32m[20221213 15:28:55 @agent_ppo2.py:185][0m |          -0.0101 |          20.4914 |           0.2520 |
[32m[20221213 15:28:55 @agent_ppo2.py:185][0m |          -0.0118 |          20.1268 |           0.2518 |
[32m[20221213 15:28:56 @agent_ppo2.py:185][0m |          -0.0091 |          19.7681 |           0.2517 |
[32m[20221213 15:28:56 @agent_ppo2.py:185][0m |          -0.0153 |          19.4560 |           0.2517 |
[32m[20221213 15:28:56 @agent_ppo2.py:185][0m |          -0.0147 |          19.2057 |           0.2517 |
[32m[20221213 15:28:56 @agent_ppo2.py:185][0m |          -0.0153 |          19.0004 |           0.2515 |
[32m[20221213 15:28:56 @agent_ppo2.py:185][0m |          -0.0153 |          18.8219 |           0.2514 |
[32m[20221213 15:28:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.27
[32m[20221213 15:28:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.42
[32m[20221213 15:28:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.10
[32m[20221213 15:28:56 @agent_ppo2.py:143][0m Total time:      36.01 min
[32m[20221213 15:28:56 @agent_ppo2.py:145][0m 3252224 total steps have happened
[32m[20221213 15:28:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1588 --------------------------#
[32m[20221213 15:28:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:28:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:56 @agent_ppo2.py:185][0m |           0.0052 |          23.3648 |           0.2482 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0076 |          22.3087 |           0.2477 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0111 |          21.8714 |           0.2476 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0132 |          21.5828 |           0.2475 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0156 |          21.2545 |           0.2474 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0172 |          20.9852 |           0.2472 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0147 |          20.7939 |           0.2473 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0153 |          20.5869 |           0.2471 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0162 |          20.3776 |           0.2472 |
[32m[20221213 15:28:57 @agent_ppo2.py:185][0m |          -0.0161 |          20.2191 |           0.2472 |
[32m[20221213 15:28:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:28:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.03
[32m[20221213 15:28:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.82
[32m[20221213 15:28:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 324.22
[32m[20221213 15:28:57 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 324.22
[32m[20221213 15:28:57 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 324.22
[32m[20221213 15:28:57 @agent_ppo2.py:143][0m Total time:      36.04 min
[32m[20221213 15:28:57 @agent_ppo2.py:145][0m 3254272 total steps have happened
[32m[20221213 15:28:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1589 --------------------------#
[32m[20221213 15:28:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:28:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0024 |          22.4705 |           0.2426 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0089 |          21.5931 |           0.2420 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |           0.0003 |          23.0571 |           0.2418 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0105 |          21.1272 |           0.2415 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0117 |          20.9289 |           0.2416 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0150 |          20.8051 |           0.2415 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0144 |          20.7341 |           0.2412 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0163 |          20.6183 |           0.2411 |
[32m[20221213 15:28:58 @agent_ppo2.py:185][0m |          -0.0154 |          20.5430 |           0.2410 |
[32m[20221213 15:28:59 @agent_ppo2.py:185][0m |          -0.0167 |          20.4932 |           0.2410 |
[32m[20221213 15:28:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:28:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.48
[32m[20221213 15:28:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.61
[32m[20221213 15:28:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.14
[32m[20221213 15:28:59 @agent_ppo2.py:143][0m Total time:      36.06 min
[32m[20221213 15:28:59 @agent_ppo2.py:145][0m 3256320 total steps have happened
[32m[20221213 15:28:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1590 --------------------------#
[32m[20221213 15:28:59 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:28:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:28:59 @agent_ppo2.py:185][0m |           0.0062 |          25.1691 |           0.2404 |
[32m[20221213 15:28:59 @agent_ppo2.py:185][0m |          -0.0068 |          22.8032 |           0.2402 |
[32m[20221213 15:28:59 @agent_ppo2.py:185][0m |          -0.0100 |          22.5449 |           0.2402 |
[32m[20221213 15:28:59 @agent_ppo2.py:185][0m |          -0.0101 |          22.4529 |           0.2400 |
[32m[20221213 15:28:59 @agent_ppo2.py:185][0m |          -0.0141 |          22.3443 |           0.2401 |
[32m[20221213 15:29:00 @agent_ppo2.py:185][0m |          -0.0135 |          22.2119 |           0.2398 |
[32m[20221213 15:29:00 @agent_ppo2.py:185][0m |          -0.0142 |          22.1706 |           0.2398 |
[32m[20221213 15:29:00 @agent_ppo2.py:185][0m |          -0.0173 |          22.1062 |           0.2397 |
[32m[20221213 15:29:00 @agent_ppo2.py:185][0m |          -0.0162 |          21.9752 |           0.2397 |
[32m[20221213 15:29:00 @agent_ppo2.py:185][0m |          -0.0159 |          21.9533 |           0.2398 |
[32m[20221213 15:29:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:29:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.17
[32m[20221213 15:29:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.27
[32m[20221213 15:29:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.95
[32m[20221213 15:29:00 @agent_ppo2.py:143][0m Total time:      36.08 min
[32m[20221213 15:29:00 @agent_ppo2.py:145][0m 3258368 total steps have happened
[32m[20221213 15:29:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1591 --------------------------#
[32m[20221213 15:29:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:00 @agent_ppo2.py:185][0m |          -0.0016 |          22.1270 |           0.2390 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0086 |          21.8984 |           0.2389 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0102 |          21.6917 |           0.2381 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |           0.0024 |          24.5298 |           0.2379 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0071 |          21.6957 |           0.2370 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0138 |          21.4208 |           0.2378 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0164 |          21.3411 |           0.2377 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0175 |          21.2824 |           0.2376 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0155 |          21.2100 |           0.2375 |
[32m[20221213 15:29:01 @agent_ppo2.py:185][0m |          -0.0069 |          22.2842 |           0.2373 |
[32m[20221213 15:29:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.44
[32m[20221213 15:29:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.29
[32m[20221213 15:29:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.92
[32m[20221213 15:29:01 @agent_ppo2.py:143][0m Total time:      36.10 min
[32m[20221213 15:29:01 @agent_ppo2.py:145][0m 3260416 total steps have happened
[32m[20221213 15:29:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1592 --------------------------#
[32m[20221213 15:29:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |           0.0010 |          21.6595 |           0.2467 |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |          -0.0048 |          21.3677 |           0.2469 |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |          -0.0077 |          21.2188 |           0.2470 |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |          -0.0047 |          21.6601 |           0.2469 |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |          -0.0114 |          21.0440 |           0.2470 |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |          -0.0130 |          20.9324 |           0.2472 |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |          -0.0089 |          21.0829 |           0.2471 |
[32m[20221213 15:29:02 @agent_ppo2.py:185][0m |          -0.0150 |          20.7943 |           0.2473 |
[32m[20221213 15:29:03 @agent_ppo2.py:185][0m |          -0.0162 |          20.6823 |           0.2474 |
[32m[20221213 15:29:03 @agent_ppo2.py:185][0m |          -0.0043 |          21.7848 |           0.2476 |
[32m[20221213 15:29:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.68
[32m[20221213 15:29:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.75
[32m[20221213 15:29:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.01
[32m[20221213 15:29:03 @agent_ppo2.py:143][0m Total time:      36.13 min
[32m[20221213 15:29:03 @agent_ppo2.py:145][0m 3262464 total steps have happened
[32m[20221213 15:29:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1593 --------------------------#
[32m[20221213 15:29:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:03 @agent_ppo2.py:185][0m |           0.0064 |          22.6468 |           0.2410 |
[32m[20221213 15:29:03 @agent_ppo2.py:185][0m |          -0.0052 |          21.5398 |           0.2408 |
[32m[20221213 15:29:03 @agent_ppo2.py:185][0m |          -0.0100 |          21.2987 |           0.2406 |
[32m[20221213 15:29:03 @agent_ppo2.py:185][0m |          -0.0114 |          21.1652 |           0.2404 |
[32m[20221213 15:29:04 @agent_ppo2.py:185][0m |          -0.0136 |          21.0531 |           0.2405 |
[32m[20221213 15:29:04 @agent_ppo2.py:185][0m |          -0.0126 |          21.0633 |           0.2405 |
[32m[20221213 15:29:04 @agent_ppo2.py:185][0m |          -0.0159 |          20.8727 |           0.2404 |
[32m[20221213 15:29:04 @agent_ppo2.py:185][0m |          -0.0157 |          20.8568 |           0.2402 |
[32m[20221213 15:29:04 @agent_ppo2.py:185][0m |          -0.0179 |          20.7579 |           0.2401 |
[32m[20221213 15:29:04 @agent_ppo2.py:185][0m |          -0.0184 |          20.7301 |           0.2404 |
[32m[20221213 15:29:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:29:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.14
[32m[20221213 15:29:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.39
[32m[20221213 15:29:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.81
[32m[20221213 15:29:04 @agent_ppo2.py:143][0m Total time:      36.15 min
[32m[20221213 15:29:04 @agent_ppo2.py:145][0m 3264512 total steps have happened
[32m[20221213 15:29:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1594 --------------------------#
[32m[20221213 15:29:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0039 |          23.3852 |           0.2432 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0095 |          22.7629 |           0.2426 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0129 |          22.5163 |           0.2423 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0035 |          24.8666 |           0.2424 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0104 |          22.3239 |           0.2414 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0144 |          22.0838 |           0.2419 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |           0.0036 |          26.0677 |           0.2419 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0071 |          22.8428 |           0.2416 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0157 |          21.8582 |           0.2415 |
[32m[20221213 15:29:05 @agent_ppo2.py:185][0m |          -0.0097 |          22.5566 |           0.2415 |
[32m[20221213 15:29:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:29:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.29
[32m[20221213 15:29:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.63
[32m[20221213 15:29:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.66
[32m[20221213 15:29:06 @agent_ppo2.py:143][0m Total time:      36.17 min
[32m[20221213 15:29:06 @agent_ppo2.py:145][0m 3266560 total steps have happened
[32m[20221213 15:29:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1595 --------------------------#
[32m[20221213 15:29:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0029 |          22.0532 |           0.2498 |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0074 |          21.4598 |           0.2491 |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0034 |          21.9616 |           0.2490 |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0124 |          20.9567 |           0.2492 |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0072 |          21.5366 |           0.2492 |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0133 |          20.6958 |           0.2490 |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0156 |          20.5694 |           0.2490 |
[32m[20221213 15:29:06 @agent_ppo2.py:185][0m |          -0.0144 |          20.5152 |           0.2487 |
[32m[20221213 15:29:07 @agent_ppo2.py:185][0m |          -0.0024 |          21.9957 |           0.2487 |
[32m[20221213 15:29:07 @agent_ppo2.py:185][0m |          -0.0147 |          20.3633 |           0.2482 |
[32m[20221213 15:29:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.71
[32m[20221213 15:29:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.23
[32m[20221213 15:29:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.43
[32m[20221213 15:29:07 @agent_ppo2.py:143][0m Total time:      36.19 min
[32m[20221213 15:29:07 @agent_ppo2.py:145][0m 3268608 total steps have happened
[32m[20221213 15:29:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1596 --------------------------#
[32m[20221213 15:29:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:07 @agent_ppo2.py:185][0m |           0.0005 |          22.6132 |           0.2485 |
[32m[20221213 15:29:07 @agent_ppo2.py:185][0m |          -0.0104 |          22.1277 |           0.2485 |
[32m[20221213 15:29:07 @agent_ppo2.py:185][0m |          -0.0138 |          21.8966 |           0.2481 |
[32m[20221213 15:29:07 @agent_ppo2.py:185][0m |          -0.0144 |          21.5829 |           0.2484 |
[32m[20221213 15:29:08 @agent_ppo2.py:185][0m |          -0.0152 |          21.3305 |           0.2482 |
[32m[20221213 15:29:08 @agent_ppo2.py:185][0m |          -0.0019 |          22.5797 |           0.2483 |
[32m[20221213 15:29:08 @agent_ppo2.py:185][0m |          -0.0135 |          20.9902 |           0.2483 |
[32m[20221213 15:29:08 @agent_ppo2.py:185][0m |          -0.0185 |          20.7148 |           0.2481 |
[32m[20221213 15:29:08 @agent_ppo2.py:185][0m |          -0.0195 |          20.5043 |           0.2483 |
[32m[20221213 15:29:08 @agent_ppo2.py:185][0m |          -0.0205 |          20.4150 |           0.2482 |
[32m[20221213 15:29:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:29:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.86
[32m[20221213 15:29:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.12
[32m[20221213 15:29:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.56
[32m[20221213 15:29:08 @agent_ppo2.py:143][0m Total time:      36.22 min
[32m[20221213 15:29:08 @agent_ppo2.py:145][0m 3270656 total steps have happened
[32m[20221213 15:29:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1597 --------------------------#
[32m[20221213 15:29:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |           0.0009 |          22.4411 |           0.2471 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0057 |          21.8295 |           0.2467 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0027 |          21.6513 |           0.2462 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0101 |          21.3931 |           0.2463 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0102 |          21.3811 |           0.2459 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0144 |          21.2001 |           0.2461 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0112 |          21.1435 |           0.2458 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0117 |          21.1936 |           0.2458 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0152 |          20.9849 |           0.2456 |
[32m[20221213 15:29:09 @agent_ppo2.py:185][0m |          -0.0205 |          20.9615 |           0.2455 |
[32m[20221213 15:29:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.73
[32m[20221213 15:29:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.25
[32m[20221213 15:29:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.23
[32m[20221213 15:29:10 @agent_ppo2.py:143][0m Total time:      36.24 min
[32m[20221213 15:29:10 @agent_ppo2.py:145][0m 3272704 total steps have happened
[32m[20221213 15:29:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1598 --------------------------#
[32m[20221213 15:29:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:10 @agent_ppo2.py:185][0m |          -0.0007 |          23.1146 |           0.2398 |
[32m[20221213 15:29:10 @agent_ppo2.py:185][0m |          -0.0059 |          22.6177 |           0.2396 |
[32m[20221213 15:29:10 @agent_ppo2.py:185][0m |           0.0038 |          24.4854 |           0.2394 |
[32m[20221213 15:29:10 @agent_ppo2.py:185][0m |          -0.0035 |          22.3594 |           0.2391 |
[32m[20221213 15:29:10 @agent_ppo2.py:185][0m |          -0.0122 |          21.9492 |           0.2389 |
[32m[20221213 15:29:10 @agent_ppo2.py:185][0m |          -0.0149 |          21.8405 |           0.2389 |
[32m[20221213 15:29:10 @agent_ppo2.py:185][0m |          -0.0116 |          21.7404 |           0.2386 |
[32m[20221213 15:29:11 @agent_ppo2.py:185][0m |          -0.0145 |          21.6043 |           0.2388 |
[32m[20221213 15:29:11 @agent_ppo2.py:185][0m |          -0.0165 |          21.4791 |           0.2386 |
[32m[20221213 15:29:11 @agent_ppo2.py:185][0m |          -0.0162 |          21.4364 |           0.2385 |
[32m[20221213 15:29:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.90
[32m[20221213 15:29:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.01
[32m[20221213 15:29:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 172.44
[32m[20221213 15:29:11 @agent_ppo2.py:143][0m Total time:      36.26 min
[32m[20221213 15:29:11 @agent_ppo2.py:145][0m 3274752 total steps have happened
[32m[20221213 15:29:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1599 --------------------------#
[32m[20221213 15:29:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:11 @agent_ppo2.py:185][0m |           0.0038 |          23.2244 |           0.2407 |
[32m[20221213 15:29:11 @agent_ppo2.py:185][0m |          -0.0031 |          22.4890 |           0.2398 |
[32m[20221213 15:29:11 @agent_ppo2.py:185][0m |          -0.0081 |          22.1699 |           0.2402 |
[32m[20221213 15:29:12 @agent_ppo2.py:185][0m |          -0.0116 |          22.0580 |           0.2400 |
[32m[20221213 15:29:12 @agent_ppo2.py:185][0m |          -0.0134 |          21.9562 |           0.2401 |
[32m[20221213 15:29:12 @agent_ppo2.py:185][0m |          -0.0153 |          21.8934 |           0.2405 |
[32m[20221213 15:29:12 @agent_ppo2.py:185][0m |          -0.0157 |          21.7864 |           0.2402 |
[32m[20221213 15:29:12 @agent_ppo2.py:185][0m |          -0.0163 |          21.8015 |           0.2401 |
[32m[20221213 15:29:12 @agent_ppo2.py:185][0m |          -0.0171 |          21.7104 |           0.2403 |
[32m[20221213 15:29:12 @agent_ppo2.py:185][0m |          -0.0015 |          24.5181 |           0.2398 |
[32m[20221213 15:29:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.57
[32m[20221213 15:29:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.32
[32m[20221213 15:29:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.06
[32m[20221213 15:29:12 @agent_ppo2.py:143][0m Total time:      36.28 min
[32m[20221213 15:29:12 @agent_ppo2.py:145][0m 3276800 total steps have happened
[32m[20221213 15:29:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1600 --------------------------#
[32m[20221213 15:29:12 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:29:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0032 |          21.3774 |           0.2386 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0059 |          21.2926 |           0.2381 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0045 |          21.4187 |           0.2382 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0139 |          20.7040 |           0.2380 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0059 |          22.0041 |           0.2379 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0062 |          21.0238 |           0.2378 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0153 |          20.4960 |           0.2380 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0082 |          22.0041 |           0.2379 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0147 |          20.4156 |           0.2379 |
[32m[20221213 15:29:13 @agent_ppo2.py:185][0m |          -0.0173 |          20.3420 |           0.2376 |
[32m[20221213 15:29:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.75
[32m[20221213 15:29:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.11
[32m[20221213 15:29:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 94.76
[32m[20221213 15:29:14 @agent_ppo2.py:143][0m Total time:      36.31 min
[32m[20221213 15:29:14 @agent_ppo2.py:145][0m 3278848 total steps have happened
[32m[20221213 15:29:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1601 --------------------------#
[32m[20221213 15:29:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:14 @agent_ppo2.py:185][0m |           0.0002 |          22.6764 |           0.2418 |
[32m[20221213 15:29:14 @agent_ppo2.py:185][0m |          -0.0057 |          22.4725 |           0.2413 |
[32m[20221213 15:29:14 @agent_ppo2.py:185][0m |          -0.0106 |          22.0222 |           0.2410 |
[32m[20221213 15:29:14 @agent_ppo2.py:185][0m |          -0.0012 |          24.0038 |           0.2408 |
[32m[20221213 15:29:14 @agent_ppo2.py:185][0m |          -0.0092 |          21.7625 |           0.2396 |
[32m[20221213 15:29:14 @agent_ppo2.py:185][0m |          -0.0140 |          21.5241 |           0.2402 |
[32m[20221213 15:29:14 @agent_ppo2.py:185][0m |          -0.0142 |          21.4557 |           0.2398 |
[32m[20221213 15:29:15 @agent_ppo2.py:185][0m |          -0.0161 |          21.3055 |           0.2396 |
[32m[20221213 15:29:15 @agent_ppo2.py:185][0m |          -0.0164 |          21.2444 |           0.2395 |
[32m[20221213 15:29:15 @agent_ppo2.py:185][0m |          -0.0174 |          21.1508 |           0.2394 |
[32m[20221213 15:29:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.83
[32m[20221213 15:29:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.24
[32m[20221213 15:29:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.55
[32m[20221213 15:29:15 @agent_ppo2.py:143][0m Total time:      36.33 min
[32m[20221213 15:29:15 @agent_ppo2.py:145][0m 3280896 total steps have happened
[32m[20221213 15:29:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1602 --------------------------#
[32m[20221213 15:29:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:15 @agent_ppo2.py:185][0m |           0.0051 |          22.3474 |           0.2349 |
[32m[20221213 15:29:15 @agent_ppo2.py:185][0m |          -0.0046 |          22.0108 |           0.2343 |
[32m[20221213 15:29:15 @agent_ppo2.py:185][0m |          -0.0084 |          21.7775 |           0.2346 |
[32m[20221213 15:29:16 @agent_ppo2.py:185][0m |          -0.0022 |          22.2265 |           0.2348 |
[32m[20221213 15:29:16 @agent_ppo2.py:185][0m |          -0.0011 |          22.8264 |           0.2347 |
[32m[20221213 15:29:16 @agent_ppo2.py:185][0m |           0.0073 |          24.0866 |           0.2348 |
[32m[20221213 15:29:16 @agent_ppo2.py:185][0m |          -0.0121 |          21.4190 |           0.2347 |
[32m[20221213 15:29:16 @agent_ppo2.py:185][0m |          -0.0076 |          21.6074 |           0.2349 |
[32m[20221213 15:29:16 @agent_ppo2.py:185][0m |          -0.0133 |          21.2156 |           0.2351 |
[32m[20221213 15:29:16 @agent_ppo2.py:185][0m |          -0.0153 |          21.1373 |           0.2349 |
[32m[20221213 15:29:16 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:29:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.79
[32m[20221213 15:29:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.81
[32m[20221213 15:29:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.00
[32m[20221213 15:29:16 @agent_ppo2.py:143][0m Total time:      36.35 min
[32m[20221213 15:29:16 @agent_ppo2.py:145][0m 3282944 total steps have happened
[32m[20221213 15:29:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1603 --------------------------#
[32m[20221213 15:29:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |           0.0069 |          21.9077 |           0.2404 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0082 |          20.4100 |           0.2397 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0098 |          20.0115 |           0.2395 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0110 |          19.7939 |           0.2395 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0098 |          19.5922 |           0.2395 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0079 |          19.5245 |           0.2395 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0077 |          19.6481 |           0.2390 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0148 |          19.1358 |           0.2392 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0166 |          19.0743 |           0.2392 |
[32m[20221213 15:29:17 @agent_ppo2.py:185][0m |          -0.0061 |          20.8127 |           0.2391 |
[32m[20221213 15:29:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:29:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.12
[32m[20221213 15:29:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.42
[32m[20221213 15:29:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.77
[32m[20221213 15:29:18 @agent_ppo2.py:143][0m Total time:      36.38 min
[32m[20221213 15:29:18 @agent_ppo2.py:145][0m 3284992 total steps have happened
[32m[20221213 15:29:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1604 --------------------------#
[32m[20221213 15:29:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:18 @agent_ppo2.py:185][0m |          -0.0048 |          23.5124 |           0.2382 |
[32m[20221213 15:29:18 @agent_ppo2.py:185][0m |          -0.0109 |          22.9260 |           0.2380 |
[32m[20221213 15:29:18 @agent_ppo2.py:185][0m |          -0.0029 |          25.0760 |           0.2382 |
[32m[20221213 15:29:18 @agent_ppo2.py:185][0m |          -0.0139 |          22.6160 |           0.2379 |
[32m[20221213 15:29:18 @agent_ppo2.py:185][0m |          -0.0153 |          22.4700 |           0.2380 |
[32m[20221213 15:29:19 @agent_ppo2.py:185][0m |          -0.0154 |          22.4112 |           0.2378 |
[32m[20221213 15:29:19 @agent_ppo2.py:185][0m |          -0.0102 |          22.9386 |           0.2379 |
[32m[20221213 15:29:19 @agent_ppo2.py:185][0m |          -0.0164 |          22.2935 |           0.2381 |
[32m[20221213 15:29:19 @agent_ppo2.py:185][0m |          -0.0200 |          22.2487 |           0.2381 |
[32m[20221213 15:29:19 @agent_ppo2.py:185][0m |          -0.0202 |          22.2153 |           0.2382 |
[32m[20221213 15:29:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:29:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.61
[32m[20221213 15:29:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.34
[32m[20221213 15:29:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.40
[32m[20221213 15:29:19 @agent_ppo2.py:143][0m Total time:      36.40 min
[32m[20221213 15:29:19 @agent_ppo2.py:145][0m 3287040 total steps have happened
[32m[20221213 15:29:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1605 --------------------------#
[32m[20221213 15:29:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:19 @agent_ppo2.py:185][0m |          -0.0008 |          23.1666 |           0.2438 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0067 |          22.7656 |           0.2431 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0117 |          22.5487 |           0.2432 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0156 |          22.4075 |           0.2430 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0103 |          22.5843 |           0.2431 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0158 |          22.1877 |           0.2431 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0110 |          22.8109 |           0.2430 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0172 |          21.9596 |           0.2432 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0183 |          21.8642 |           0.2429 |
[32m[20221213 15:29:20 @agent_ppo2.py:185][0m |          -0.0195 |          21.7873 |           0.2429 |
[32m[20221213 15:29:20 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:29:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.82
[32m[20221213 15:29:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.84
[32m[20221213 15:29:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.65
[32m[20221213 15:29:20 @agent_ppo2.py:143][0m Total time:      36.42 min
[32m[20221213 15:29:20 @agent_ppo2.py:145][0m 3289088 total steps have happened
[32m[20221213 15:29:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1606 --------------------------#
[32m[20221213 15:29:21 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0030 |          22.9583 |           0.2462 |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0100 |          22.4522 |           0.2456 |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0118 |          22.2397 |           0.2461 |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0118 |          22.0671 |           0.2459 |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0134 |          21.9618 |           0.2457 |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0137 |          21.9035 |           0.2460 |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0146 |          21.7761 |           0.2461 |
[32m[20221213 15:29:21 @agent_ppo2.py:185][0m |          -0.0183 |          21.6225 |           0.2459 |
[32m[20221213 15:29:22 @agent_ppo2.py:185][0m |          -0.0188 |          21.5293 |           0.2462 |
[32m[20221213 15:29:22 @agent_ppo2.py:185][0m |          -0.0072 |          22.2188 |           0.2460 |
[32m[20221213 15:29:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:29:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 245.54
[32m[20221213 15:29:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 270.03
[32m[20221213 15:29:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.36
[32m[20221213 15:29:22 @agent_ppo2.py:143][0m Total time:      36.44 min
[32m[20221213 15:29:22 @agent_ppo2.py:145][0m 3291136 total steps have happened
[32m[20221213 15:29:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1607 --------------------------#
[32m[20221213 15:29:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:22 @agent_ppo2.py:185][0m |          -0.0020 |          22.0783 |           0.2425 |
[32m[20221213 15:29:22 @agent_ppo2.py:185][0m |          -0.0087 |          21.5873 |           0.2419 |
[32m[20221213 15:29:22 @agent_ppo2.py:185][0m |          -0.0038 |          21.8103 |           0.2418 |
[32m[20221213 15:29:22 @agent_ppo2.py:185][0m |          -0.0112 |          21.2800 |           0.2415 |
[32m[20221213 15:29:23 @agent_ppo2.py:185][0m |          -0.0069 |          21.4811 |           0.2414 |
[32m[20221213 15:29:23 @agent_ppo2.py:185][0m |          -0.0133 |          21.0712 |           0.2413 |
[32m[20221213 15:29:23 @agent_ppo2.py:185][0m |          -0.0153 |          21.0032 |           0.2411 |
[32m[20221213 15:29:23 @agent_ppo2.py:185][0m |          -0.0144 |          20.8698 |           0.2410 |
[32m[20221213 15:29:23 @agent_ppo2.py:185][0m |          -0.0164 |          20.8357 |           0.2412 |
[32m[20221213 15:29:23 @agent_ppo2.py:185][0m |          -0.0167 |          20.7785 |           0.2410 |
[32m[20221213 15:29:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.45
[32m[20221213 15:29:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.83
[32m[20221213 15:29:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.83
[32m[20221213 15:29:23 @agent_ppo2.py:143][0m Total time:      36.47 min
[32m[20221213 15:29:23 @agent_ppo2.py:145][0m 3293184 total steps have happened
[32m[20221213 15:29:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1608 --------------------------#
[32m[20221213 15:29:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0036 |          21.7565 |           0.2512 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0059 |          20.8482 |           0.2509 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0105 |          20.2007 |           0.2507 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0059 |          20.9249 |           0.2503 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0154 |          19.6040 |           0.2501 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0151 |          19.3848 |           0.2500 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0181 |          19.1703 |           0.2498 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0062 |          20.6643 |           0.2497 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0187 |          18.9064 |           0.2496 |
[32m[20221213 15:29:24 @agent_ppo2.py:185][0m |          -0.0172 |          18.8176 |           0.2497 |
[32m[20221213 15:29:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.26
[32m[20221213 15:29:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.32
[32m[20221213 15:29:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.64
[32m[20221213 15:29:25 @agent_ppo2.py:143][0m Total time:      36.49 min
[32m[20221213 15:29:25 @agent_ppo2.py:145][0m 3295232 total steps have happened
[32m[20221213 15:29:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1609 --------------------------#
[32m[20221213 15:29:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0010 |          23.0489 |           0.2509 |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0027 |          22.5987 |           0.2505 |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0096 |          22.2891 |           0.2503 |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0089 |          22.1049 |           0.2502 |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0116 |          22.0305 |           0.2502 |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0151 |          21.8933 |           0.2502 |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0119 |          21.8527 |           0.2503 |
[32m[20221213 15:29:25 @agent_ppo2.py:185][0m |          -0.0130 |          21.7816 |           0.2501 |
[32m[20221213 15:29:26 @agent_ppo2.py:185][0m |          -0.0151 |          21.6221 |           0.2502 |
[32m[20221213 15:29:26 @agent_ppo2.py:185][0m |          -0.0169 |          21.6105 |           0.2500 |
[32m[20221213 15:29:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.13
[32m[20221213 15:29:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.50
[32m[20221213 15:29:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.19
[32m[20221213 15:29:26 @agent_ppo2.py:143][0m Total time:      36.51 min
[32m[20221213 15:29:26 @agent_ppo2.py:145][0m 3297280 total steps have happened
[32m[20221213 15:29:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1610 --------------------------#
[32m[20221213 15:29:26 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:29:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:26 @agent_ppo2.py:185][0m |           0.0022 |          22.2391 |           0.2494 |
[32m[20221213 15:29:26 @agent_ppo2.py:185][0m |          -0.0048 |          21.9266 |           0.2489 |
[32m[20221213 15:29:26 @agent_ppo2.py:185][0m |          -0.0082 |          21.7407 |           0.2488 |
[32m[20221213 15:29:26 @agent_ppo2.py:185][0m |          -0.0098 |          21.6007 |           0.2488 |
[32m[20221213 15:29:27 @agent_ppo2.py:185][0m |          -0.0107 |          21.4554 |           0.2489 |
[32m[20221213 15:29:27 @agent_ppo2.py:185][0m |          -0.0058 |          21.9691 |           0.2490 |
[32m[20221213 15:29:27 @agent_ppo2.py:185][0m |          -0.0093 |          21.3250 |           0.2486 |
[32m[20221213 15:29:27 @agent_ppo2.py:185][0m |          -0.0129 |          21.0305 |           0.2489 |
[32m[20221213 15:29:27 @agent_ppo2.py:185][0m |          -0.0149 |          20.8689 |           0.2491 |
[32m[20221213 15:29:27 @agent_ppo2.py:185][0m |          -0.0127 |          20.8094 |           0.2490 |
[32m[20221213 15:29:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.07
[32m[20221213 15:29:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.90
[32m[20221213 15:29:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.41
[32m[20221213 15:29:27 @agent_ppo2.py:143][0m Total time:      36.53 min
[32m[20221213 15:29:27 @agent_ppo2.py:145][0m 3299328 total steps have happened
[32m[20221213 15:29:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1611 --------------------------#
[32m[20221213 15:29:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |           0.0028 |          22.8793 |           0.2406 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0081 |          21.9907 |           0.2400 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0023 |          22.6080 |           0.2399 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0062 |          21.9202 |           0.2395 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0141 |          21.3752 |           0.2398 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0135 |          21.2997 |           0.2395 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0066 |          21.8815 |           0.2396 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0160 |          21.0639 |           0.2396 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0157 |          21.0047 |           0.2396 |
[32m[20221213 15:29:28 @agent_ppo2.py:185][0m |          -0.0179 |          20.9515 |           0.2392 |
[32m[20221213 15:29:28 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.05
[32m[20221213 15:29:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.43
[32m[20221213 15:29:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 238.23
[32m[20221213 15:29:29 @agent_ppo2.py:143][0m Total time:      36.56 min
[32m[20221213 15:29:29 @agent_ppo2.py:145][0m 3301376 total steps have happened
[32m[20221213 15:29:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1612 --------------------------#
[32m[20221213 15:29:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:29 @agent_ppo2.py:185][0m |          -0.0011 |          23.3031 |           0.2470 |
[32m[20221213 15:29:29 @agent_ppo2.py:185][0m |          -0.0034 |          23.5374 |           0.2459 |
[32m[20221213 15:29:29 @agent_ppo2.py:185][0m |           0.0008 |          25.7171 |           0.2460 |
[32m[20221213 15:29:29 @agent_ppo2.py:185][0m |           0.0007 |          25.4202 |           0.2460 |
[32m[20221213 15:29:29 @agent_ppo2.py:185][0m |          -0.0134 |          22.5612 |           0.2462 |
[32m[20221213 15:29:29 @agent_ppo2.py:185][0m |          -0.0114 |          22.4670 |           0.2466 |
[32m[20221213 15:29:29 @agent_ppo2.py:185][0m |          -0.0127 |          22.4486 |           0.2464 |
[32m[20221213 15:29:30 @agent_ppo2.py:185][0m |          -0.0131 |          22.3995 |           0.2465 |
[32m[20221213 15:29:30 @agent_ppo2.py:185][0m |          -0.0072 |          23.9387 |           0.2462 |
[32m[20221213 15:29:30 @agent_ppo2.py:185][0m |          -0.0164 |          22.2631 |           0.2459 |
[32m[20221213 15:29:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:29:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.60
[32m[20221213 15:29:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.36
[32m[20221213 15:29:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.63
[32m[20221213 15:29:30 @agent_ppo2.py:143][0m Total time:      36.58 min
[32m[20221213 15:29:30 @agent_ppo2.py:145][0m 3303424 total steps have happened
[32m[20221213 15:29:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1613 --------------------------#
[32m[20221213 15:29:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:30 @agent_ppo2.py:185][0m |          -0.0027 |          21.7552 |           0.2456 |
[32m[20221213 15:29:30 @agent_ppo2.py:185][0m |          -0.0078 |          21.4570 |           0.2451 |
[32m[20221213 15:29:30 @agent_ppo2.py:185][0m |          -0.0068 |          21.3143 |           0.2450 |
[32m[20221213 15:29:30 @agent_ppo2.py:185][0m |          -0.0010 |          22.0121 |           0.2451 |
[32m[20221213 15:29:31 @agent_ppo2.py:185][0m |          -0.0099 |          21.1108 |           0.2444 |
[32m[20221213 15:29:31 @agent_ppo2.py:185][0m |           0.0008 |          23.0652 |           0.2444 |
[32m[20221213 15:29:31 @agent_ppo2.py:185][0m |          -0.0093 |          21.0042 |           0.2432 |
[32m[20221213 15:29:31 @agent_ppo2.py:185][0m |          -0.0137 |          20.9058 |           0.2443 |
[32m[20221213 15:29:31 @agent_ppo2.py:185][0m |          -0.0136 |          20.8784 |           0.2440 |
[32m[20221213 15:29:31 @agent_ppo2.py:185][0m |          -0.0112 |          20.7975 |           0.2441 |
[32m[20221213 15:29:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.73
[32m[20221213 15:29:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.74
[32m[20221213 15:29:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.75
[32m[20221213 15:29:31 @agent_ppo2.py:143][0m Total time:      36.60 min
[32m[20221213 15:29:31 @agent_ppo2.py:145][0m 3305472 total steps have happened
[32m[20221213 15:29:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1614 --------------------------#
[32m[20221213 15:29:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0000 |          22.0632 |           0.2434 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0043 |          21.8283 |           0.2430 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0067 |          21.4633 |           0.2430 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0124 |          21.3722 |           0.2428 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0151 |          21.2861 |           0.2427 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0133 |          21.1887 |           0.2426 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0162 |          21.1519 |           0.2427 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0131 |          21.0622 |           0.2426 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0147 |          21.0618 |           0.2425 |
[32m[20221213 15:29:32 @agent_ppo2.py:185][0m |          -0.0069 |          22.2227 |           0.2424 |
[32m[20221213 15:29:32 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.76
[32m[20221213 15:29:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.45
[32m[20221213 15:29:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.17
[32m[20221213 15:29:33 @agent_ppo2.py:143][0m Total time:      36.62 min
[32m[20221213 15:29:33 @agent_ppo2.py:145][0m 3307520 total steps have happened
[32m[20221213 15:29:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1615 --------------------------#
[32m[20221213 15:29:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:33 @agent_ppo2.py:185][0m |          -0.0037 |          23.1856 |           0.2463 |
[32m[20221213 15:29:33 @agent_ppo2.py:185][0m |           0.0027 |          23.3163 |           0.2458 |
[32m[20221213 15:29:33 @agent_ppo2.py:185][0m |          -0.0094 |          22.4489 |           0.2454 |
[32m[20221213 15:29:33 @agent_ppo2.py:185][0m |          -0.0124 |          22.2503 |           0.2453 |
[32m[20221213 15:29:33 @agent_ppo2.py:185][0m |          -0.0035 |          23.1513 |           0.2454 |
[32m[20221213 15:29:33 @agent_ppo2.py:185][0m |          -0.0046 |          23.7457 |           0.2449 |
[32m[20221213 15:29:33 @agent_ppo2.py:185][0m |          -0.0044 |          23.2466 |           0.2446 |
[32m[20221213 15:29:34 @agent_ppo2.py:185][0m |          -0.0159 |          22.0567 |           0.2441 |
[32m[20221213 15:29:34 @agent_ppo2.py:185][0m |          -0.0168 |          21.7760 |           0.2443 |
[32m[20221213 15:29:34 @agent_ppo2.py:185][0m |          -0.0175 |          21.7556 |           0.2443 |
[32m[20221213 15:29:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:29:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.06
[32m[20221213 15:29:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.92
[32m[20221213 15:29:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.63
[32m[20221213 15:29:34 @agent_ppo2.py:143][0m Total time:      36.65 min
[32m[20221213 15:29:34 @agent_ppo2.py:145][0m 3309568 total steps have happened
[32m[20221213 15:29:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1616 --------------------------#
[32m[20221213 15:29:34 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:34 @agent_ppo2.py:185][0m |          -0.0027 |          22.0932 |           0.2356 |
[32m[20221213 15:29:34 @agent_ppo2.py:185][0m |          -0.0088 |          21.5176 |           0.2352 |
[32m[20221213 15:29:34 @agent_ppo2.py:185][0m |          -0.0107 |          21.2235 |           0.2351 |
[32m[20221213 15:29:35 @agent_ppo2.py:185][0m |          -0.0134 |          21.0622 |           0.2350 |
[32m[20221213 15:29:35 @agent_ppo2.py:185][0m |          -0.0154 |          20.8723 |           0.2350 |
[32m[20221213 15:29:35 @agent_ppo2.py:185][0m |          -0.0119 |          21.1355 |           0.2349 |
[32m[20221213 15:29:35 @agent_ppo2.py:185][0m |          -0.0166 |          20.6776 |           0.2349 |
[32m[20221213 15:29:35 @agent_ppo2.py:185][0m |          -0.0171 |          20.5839 |           0.2348 |
[32m[20221213 15:29:35 @agent_ppo2.py:185][0m |          -0.0182 |          20.5335 |           0.2350 |
[32m[20221213 15:29:35 @agent_ppo2.py:185][0m |          -0.0102 |          21.7808 |           0.2347 |
[32m[20221213 15:29:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.10
[32m[20221213 15:29:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.35
[32m[20221213 15:29:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 179.15
[32m[20221213 15:29:35 @agent_ppo2.py:143][0m Total time:      36.67 min
[32m[20221213 15:29:35 @agent_ppo2.py:145][0m 3311616 total steps have happened
[32m[20221213 15:29:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1617 --------------------------#
[32m[20221213 15:29:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0002 |          22.2876 |           0.2441 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0068 |          21.9397 |           0.2435 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0079 |          21.7426 |           0.2431 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0062 |          21.9029 |           0.2422 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0115 |          21.3833 |           0.2423 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0031 |          22.5954 |           0.2424 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0157 |          21.1877 |           0.2418 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0112 |          21.3924 |           0.2418 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0150 |          21.0569 |           0.2419 |
[32m[20221213 15:29:36 @agent_ppo2.py:185][0m |          -0.0159 |          20.8727 |           0.2416 |
[32m[20221213 15:29:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.24
[32m[20221213 15:29:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.95
[32m[20221213 15:29:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.92
[32m[20221213 15:29:37 @agent_ppo2.py:143][0m Total time:      36.69 min
[32m[20221213 15:29:37 @agent_ppo2.py:145][0m 3313664 total steps have happened
[32m[20221213 15:29:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1618 --------------------------#
[32m[20221213 15:29:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:37 @agent_ppo2.py:185][0m |           0.0006 |          23.7586 |           0.2409 |
[32m[20221213 15:29:37 @agent_ppo2.py:185][0m |          -0.0106 |          23.0275 |           0.2408 |
[32m[20221213 15:29:37 @agent_ppo2.py:185][0m |          -0.0035 |          23.0323 |           0.2405 |
[32m[20221213 15:29:37 @agent_ppo2.py:185][0m |          -0.0121 |          22.2424 |           0.2402 |
[32m[20221213 15:29:37 @agent_ppo2.py:185][0m |          -0.0127 |          22.0350 |           0.2404 |
[32m[20221213 15:29:37 @agent_ppo2.py:185][0m |          -0.0143 |          21.8483 |           0.2401 |
[32m[20221213 15:29:37 @agent_ppo2.py:185][0m |          -0.0163 |          21.6606 |           0.2402 |
[32m[20221213 15:29:38 @agent_ppo2.py:185][0m |          -0.0134 |          21.6088 |           0.2401 |
[32m[20221213 15:29:38 @agent_ppo2.py:185][0m |          -0.0166 |          21.4917 |           0.2401 |
[32m[20221213 15:29:38 @agent_ppo2.py:185][0m |          -0.0201 |          21.3582 |           0.2398 |
[32m[20221213 15:29:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.18
[32m[20221213 15:29:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.02
[32m[20221213 15:29:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.38
[32m[20221213 15:29:38 @agent_ppo2.py:143][0m Total time:      36.71 min
[32m[20221213 15:29:38 @agent_ppo2.py:145][0m 3315712 total steps have happened
[32m[20221213 15:29:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1619 --------------------------#
[32m[20221213 15:29:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:38 @agent_ppo2.py:185][0m |           0.0129 |          24.6966 |           0.2413 |
[32m[20221213 15:29:38 @agent_ppo2.py:185][0m |          -0.0085 |          21.9335 |           0.2403 |
[32m[20221213 15:29:38 @agent_ppo2.py:185][0m |          -0.0093 |          21.6832 |           0.2407 |
[32m[20221213 15:29:39 @agent_ppo2.py:185][0m |          -0.0132 |          21.5765 |           0.2410 |
[32m[20221213 15:29:39 @agent_ppo2.py:185][0m |          -0.0135 |          21.4760 |           0.2410 |
[32m[20221213 15:29:39 @agent_ppo2.py:185][0m |          -0.0140 |          21.4055 |           0.2406 |
[32m[20221213 15:29:39 @agent_ppo2.py:185][0m |          -0.0095 |          21.7205 |           0.2409 |
[32m[20221213 15:29:39 @agent_ppo2.py:185][0m |          -0.0139 |          21.2221 |           0.2410 |
[32m[20221213 15:29:39 @agent_ppo2.py:185][0m |          -0.0077 |          22.2903 |           0.2411 |
[32m[20221213 15:29:39 @agent_ppo2.py:185][0m |          -0.0149 |          21.1180 |           0.2409 |
[32m[20221213 15:29:39 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.87
[32m[20221213 15:29:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.48
[32m[20221213 15:29:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.63
[32m[20221213 15:29:39 @agent_ppo2.py:143][0m Total time:      36.73 min
[32m[20221213 15:29:39 @agent_ppo2.py:145][0m 3317760 total steps have happened
[32m[20221213 15:29:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1620 --------------------------#
[32m[20221213 15:29:39 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:29:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0032 |          22.1191 |           0.2331 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0104 |          21.1711 |           0.2325 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0117 |          20.7045 |           0.2324 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0159 |          20.2517 |           0.2323 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0142 |          19.8576 |           0.2321 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0038 |          21.2240 |           0.2319 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0180 |          19.3895 |           0.2312 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0191 |          19.1214 |           0.2314 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0203 |          18.9110 |           0.2310 |
[32m[20221213 15:29:40 @agent_ppo2.py:185][0m |          -0.0214 |          18.7274 |           0.2310 |
[32m[20221213 15:29:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.46
[32m[20221213 15:29:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.78
[32m[20221213 15:29:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.59
[32m[20221213 15:29:41 @agent_ppo2.py:143][0m Total time:      36.76 min
[32m[20221213 15:29:41 @agent_ppo2.py:145][0m 3319808 total steps have happened
[32m[20221213 15:29:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1621 --------------------------#
[32m[20221213 15:29:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:41 @agent_ppo2.py:185][0m |          -0.0025 |          23.7154 |           0.2416 |
[32m[20221213 15:29:41 @agent_ppo2.py:185][0m |          -0.0046 |          23.0767 |           0.2421 |
[32m[20221213 15:29:41 @agent_ppo2.py:185][0m |          -0.0102 |          22.6660 |           0.2422 |
[32m[20221213 15:29:41 @agent_ppo2.py:185][0m |          -0.0094 |          22.5757 |           0.2423 |
[32m[20221213 15:29:41 @agent_ppo2.py:185][0m |          -0.0147 |          22.2080 |           0.2422 |
[32m[20221213 15:29:41 @agent_ppo2.py:185][0m |          -0.0100 |          22.3630 |           0.2423 |
[32m[20221213 15:29:42 @agent_ppo2.py:185][0m |          -0.0149 |          21.8968 |           0.2426 |
[32m[20221213 15:29:42 @agent_ppo2.py:185][0m |          -0.0176 |          21.7637 |           0.2427 |
[32m[20221213 15:29:42 @agent_ppo2.py:185][0m |          -0.0176 |          21.6087 |           0.2428 |
[32m[20221213 15:29:42 @agent_ppo2.py:185][0m |          -0.0184 |          21.5753 |           0.2431 |
[32m[20221213 15:29:42 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.18
[32m[20221213 15:29:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.06
[32m[20221213 15:29:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.28
[32m[20221213 15:29:42 @agent_ppo2.py:143][0m Total time:      36.78 min
[32m[20221213 15:29:42 @agent_ppo2.py:145][0m 3321856 total steps have happened
[32m[20221213 15:29:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1622 --------------------------#
[32m[20221213 15:29:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:42 @agent_ppo2.py:185][0m |          -0.0005 |          25.0188 |           0.2407 |
[32m[20221213 15:29:42 @agent_ppo2.py:185][0m |          -0.0098 |          23.9990 |           0.2406 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0036 |          25.0581 |           0.2403 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0137 |          23.5310 |           0.2402 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0070 |          23.7266 |           0.2400 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0161 |          22.8006 |           0.2397 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0170 |          22.6676 |           0.2399 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0185 |          22.6627 |           0.2399 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0155 |          22.5170 |           0.2397 |
[32m[20221213 15:29:43 @agent_ppo2.py:185][0m |          -0.0177 |          22.2333 |           0.2398 |
[32m[20221213 15:29:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.50
[32m[20221213 15:29:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.44
[32m[20221213 15:29:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.09
[32m[20221213 15:29:43 @agent_ppo2.py:143][0m Total time:      36.80 min
[32m[20221213 15:29:43 @agent_ppo2.py:145][0m 3323904 total steps have happened
[32m[20221213 15:29:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1623 --------------------------#
[32m[20221213 15:29:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |           0.0002 |          23.8872 |           0.2442 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0052 |          23.0917 |           0.2441 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |           0.0026 |          26.0070 |           0.2438 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0038 |          23.1140 |           0.2434 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0150 |          22.4356 |           0.2435 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0057 |          23.9196 |           0.2434 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0144 |          22.3332 |           0.2430 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0163 |          22.1211 |           0.2432 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0136 |          22.2586 |           0.2433 |
[32m[20221213 15:29:44 @agent_ppo2.py:185][0m |          -0.0167 |          21.9625 |           0.2430 |
[32m[20221213 15:29:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.04
[32m[20221213 15:29:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.07
[32m[20221213 15:29:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.33
[32m[20221213 15:29:45 @agent_ppo2.py:143][0m Total time:      36.82 min
[32m[20221213 15:29:45 @agent_ppo2.py:145][0m 3325952 total steps have happened
[32m[20221213 15:29:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1624 --------------------------#
[32m[20221213 15:29:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:45 @agent_ppo2.py:185][0m |           0.0016 |          23.7797 |           0.2360 |
[32m[20221213 15:29:45 @agent_ppo2.py:185][0m |          -0.0016 |          23.0352 |           0.2358 |
[32m[20221213 15:29:45 @agent_ppo2.py:185][0m |          -0.0069 |          22.7068 |           0.2358 |
[32m[20221213 15:29:45 @agent_ppo2.py:185][0m |          -0.0094 |          22.5082 |           0.2358 |
[32m[20221213 15:29:45 @agent_ppo2.py:185][0m |          -0.0105 |          22.3892 |           0.2357 |
[32m[20221213 15:29:45 @agent_ppo2.py:185][0m |          -0.0131 |          22.2808 |           0.2356 |
[32m[20221213 15:29:46 @agent_ppo2.py:185][0m |          -0.0062 |          23.2179 |           0.2357 |
[32m[20221213 15:29:46 @agent_ppo2.py:185][0m |          -0.0133 |          22.1511 |           0.2353 |
[32m[20221213 15:29:46 @agent_ppo2.py:185][0m |          -0.0128 |          22.0705 |           0.2355 |
[32m[20221213 15:29:46 @agent_ppo2.py:185][0m |          -0.0142 |          21.9882 |           0.2356 |
[32m[20221213 15:29:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.18
[32m[20221213 15:29:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.11
[32m[20221213 15:29:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.50
[32m[20221213 15:29:46 @agent_ppo2.py:143][0m Total time:      36.85 min
[32m[20221213 15:29:46 @agent_ppo2.py:145][0m 3328000 total steps have happened
[32m[20221213 15:29:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1625 --------------------------#
[32m[20221213 15:29:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:46 @agent_ppo2.py:185][0m |          -0.0021 |          23.4269 |           0.2421 |
[32m[20221213 15:29:46 @agent_ppo2.py:185][0m |          -0.0026 |          23.1817 |           0.2420 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0023 |          23.6503 |           0.2420 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0118 |          22.6508 |           0.2415 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0050 |          23.5264 |           0.2419 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0109 |          22.6138 |           0.2410 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0144 |          22.4092 |           0.2419 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0163 |          22.3489 |           0.2418 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0159 |          22.2708 |           0.2419 |
[32m[20221213 15:29:47 @agent_ppo2.py:185][0m |          -0.0105 |          22.8638 |           0.2419 |
[32m[20221213 15:29:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.48
[32m[20221213 15:29:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.95
[32m[20221213 15:29:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.33
[32m[20221213 15:29:47 @agent_ppo2.py:143][0m Total time:      36.87 min
[32m[20221213 15:29:47 @agent_ppo2.py:145][0m 3330048 total steps have happened
[32m[20221213 15:29:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1626 --------------------------#
[32m[20221213 15:29:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0033 |          22.3205 |           0.2409 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0087 |          21.6582 |           0.2405 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0041 |          21.7167 |           0.2400 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0061 |          21.4341 |           0.2399 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0029 |          22.0038 |           0.2399 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0165 |          20.7796 |           0.2398 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0139 |          20.7748 |           0.2399 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0189 |          20.5073 |           0.2401 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0175 |          20.4202 |           0.2400 |
[32m[20221213 15:29:48 @agent_ppo2.py:185][0m |          -0.0181 |          20.3284 |           0.2399 |
[32m[20221213 15:29:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.77
[32m[20221213 15:29:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 260.55
[32m[20221213 15:29:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.29
[32m[20221213 15:29:49 @agent_ppo2.py:143][0m Total time:      36.89 min
[32m[20221213 15:29:49 @agent_ppo2.py:145][0m 3332096 total steps have happened
[32m[20221213 15:29:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1627 --------------------------#
[32m[20221213 15:29:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:49 @agent_ppo2.py:185][0m |          -0.0021 |          21.7351 |           0.2428 |
[32m[20221213 15:29:49 @agent_ppo2.py:185][0m |          -0.0077 |          21.1699 |           0.2426 |
[32m[20221213 15:29:49 @agent_ppo2.py:185][0m |          -0.0098 |          20.8654 |           0.2428 |
[32m[20221213 15:29:49 @agent_ppo2.py:185][0m |           0.0007 |          22.4699 |           0.2425 |
[32m[20221213 15:29:49 @agent_ppo2.py:185][0m |          -0.0083 |          20.6337 |           0.2424 |
[32m[20221213 15:29:50 @agent_ppo2.py:185][0m |          -0.0149 |          20.1995 |           0.2421 |
[32m[20221213 15:29:50 @agent_ppo2.py:185][0m |          -0.0143 |          20.0006 |           0.2421 |
[32m[20221213 15:29:50 @agent_ppo2.py:185][0m |          -0.0172 |          19.8614 |           0.2418 |
[32m[20221213 15:29:50 @agent_ppo2.py:185][0m |          -0.0164 |          19.7729 |           0.2418 |
[32m[20221213 15:29:50 @agent_ppo2.py:185][0m |          -0.0174 |          19.6201 |           0.2416 |
[32m[20221213 15:29:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:29:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.55
[32m[20221213 15:29:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.45
[32m[20221213 15:29:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.23
[32m[20221213 15:29:50 @agent_ppo2.py:143][0m Total time:      36.91 min
[32m[20221213 15:29:50 @agent_ppo2.py:145][0m 3334144 total steps have happened
[32m[20221213 15:29:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1628 --------------------------#
[32m[20221213 15:29:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:50 @agent_ppo2.py:185][0m |          -0.0024 |          22.0433 |           0.2472 |
[32m[20221213 15:29:50 @agent_ppo2.py:185][0m |          -0.0046 |          21.2319 |           0.2466 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0113 |          20.5912 |           0.2459 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0137 |          20.1768 |           0.2462 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0136 |          19.9981 |           0.2463 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0162 |          19.7530 |           0.2460 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0169 |          19.6678 |           0.2464 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0190 |          19.5316 |           0.2466 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0192 |          19.3754 |           0.2465 |
[32m[20221213 15:29:51 @agent_ppo2.py:185][0m |          -0.0199 |          19.3699 |           0.2464 |
[32m[20221213 15:29:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.07
[32m[20221213 15:29:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 257.89
[32m[20221213 15:29:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.83
[32m[20221213 15:29:51 @agent_ppo2.py:143][0m Total time:      36.94 min
[32m[20221213 15:29:51 @agent_ppo2.py:145][0m 3336192 total steps have happened
[32m[20221213 15:29:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1629 --------------------------#
[32m[20221213 15:29:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:29:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0015 |          21.4167 |           0.2418 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0083 |          20.6643 |           0.2412 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0142 |          20.2658 |           0.2407 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0137 |          19.8613 |           0.2402 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0160 |          19.5242 |           0.2399 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0169 |          19.2406 |           0.2396 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0173 |          18.9985 |           0.2392 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0182 |          18.8131 |           0.2390 |
[32m[20221213 15:29:52 @agent_ppo2.py:185][0m |          -0.0195 |          18.6622 |           0.2388 |
[32m[20221213 15:29:53 @agent_ppo2.py:185][0m |          -0.0207 |          18.4643 |           0.2384 |
[32m[20221213 15:29:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.08
[32m[20221213 15:29:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.98
[32m[20221213 15:29:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.09
[32m[20221213 15:29:53 @agent_ppo2.py:143][0m Total time:      36.96 min
[32m[20221213 15:29:53 @agent_ppo2.py:145][0m 3338240 total steps have happened
[32m[20221213 15:29:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1630 --------------------------#
[32m[20221213 15:29:53 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:29:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:53 @agent_ppo2.py:185][0m |           0.0016 |          23.1728 |           0.2371 |
[32m[20221213 15:29:53 @agent_ppo2.py:185][0m |          -0.0103 |          22.2304 |           0.2363 |
[32m[20221213 15:29:53 @agent_ppo2.py:185][0m |           0.0015 |          24.4777 |           0.2365 |
[32m[20221213 15:29:53 @agent_ppo2.py:185][0m |          -0.0133 |          21.8524 |           0.2362 |
[32m[20221213 15:29:53 @agent_ppo2.py:185][0m |          -0.0036 |          24.3690 |           0.2365 |
[32m[20221213 15:29:54 @agent_ppo2.py:185][0m |          -0.0112 |          21.5790 |           0.2359 |
[32m[20221213 15:29:54 @agent_ppo2.py:185][0m |          -0.0144 |          21.4552 |           0.2360 |
[32m[20221213 15:29:54 @agent_ppo2.py:185][0m |          -0.0032 |          24.0577 |           0.2360 |
[32m[20221213 15:29:54 @agent_ppo2.py:185][0m |          -0.0180 |          21.3137 |           0.2357 |
[32m[20221213 15:29:54 @agent_ppo2.py:185][0m |          -0.0172 |          21.2115 |           0.2360 |
[32m[20221213 15:29:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:29:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.59
[32m[20221213 15:29:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.85
[32m[20221213 15:29:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.08
[32m[20221213 15:29:54 @agent_ppo2.py:143][0m Total time:      36.98 min
[32m[20221213 15:29:54 @agent_ppo2.py:145][0m 3340288 total steps have happened
[32m[20221213 15:29:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1631 --------------------------#
[32m[20221213 15:29:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:54 @agent_ppo2.py:185][0m |          -0.0003 |          21.6483 |           0.2359 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0078 |          21.2325 |           0.2354 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0104 |          20.9969 |           0.2352 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0114 |          20.7664 |           0.2352 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0029 |          22.4359 |           0.2352 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0135 |          20.4826 |           0.2351 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0127 |          20.4311 |           0.2348 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0157 |          20.2360 |           0.2350 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0126 |          20.4148 |           0.2350 |
[32m[20221213 15:29:55 @agent_ppo2.py:185][0m |          -0.0153 |          20.0772 |           0.2347 |
[32m[20221213 15:29:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:29:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.70
[32m[20221213 15:29:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.47
[32m[20221213 15:29:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.21
[32m[20221213 15:29:55 @agent_ppo2.py:143][0m Total time:      37.00 min
[32m[20221213 15:29:55 @agent_ppo2.py:145][0m 3342336 total steps have happened
[32m[20221213 15:29:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1632 --------------------------#
[32m[20221213 15:29:56 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:29:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:56 @agent_ppo2.py:185][0m |          -0.0006 |          21.7750 |           0.2427 |
[32m[20221213 15:29:56 @agent_ppo2.py:185][0m |          -0.0087 |          21.2021 |           0.2422 |
[32m[20221213 15:29:56 @agent_ppo2.py:185][0m |          -0.0011 |          22.0409 |           0.2420 |
[32m[20221213 15:29:56 @agent_ppo2.py:185][0m |          -0.0110 |          20.7847 |           0.2417 |
[32m[20221213 15:29:56 @agent_ppo2.py:185][0m |          -0.0138 |          20.5544 |           0.2416 |
[32m[20221213 15:29:57 @agent_ppo2.py:185][0m |          -0.0133 |          20.4865 |           0.2417 |
[32m[20221213 15:29:57 @agent_ppo2.py:185][0m |          -0.0125 |          20.3477 |           0.2414 |
[32m[20221213 15:29:57 @agent_ppo2.py:185][0m |          -0.0153 |          20.1721 |           0.2414 |
[32m[20221213 15:29:57 @agent_ppo2.py:185][0m |          -0.0123 |          20.6524 |           0.2411 |
[32m[20221213 15:29:57 @agent_ppo2.py:185][0m |          -0.0125 |          20.1608 |           0.2411 |
[32m[20221213 15:29:57 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221213 15:29:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.61
[32m[20221213 15:29:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.30
[32m[20221213 15:29:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.94
[32m[20221213 15:29:57 @agent_ppo2.py:143][0m Total time:      37.03 min
[32m[20221213 15:29:57 @agent_ppo2.py:145][0m 3344384 total steps have happened
[32m[20221213 15:29:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1633 --------------------------#
[32m[20221213 15:29:57 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:29:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0001 |          21.9900 |           0.2394 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0068 |          21.3033 |           0.2393 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0095 |          20.9576 |           0.2391 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0103 |          20.6940 |           0.2390 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0137 |          20.4856 |           0.2388 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0164 |          20.2570 |           0.2390 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0148 |          20.1038 |           0.2390 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0157 |          19.9777 |           0.2387 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0135 |          19.8971 |           0.2388 |
[32m[20221213 15:29:58 @agent_ppo2.py:185][0m |          -0.0193 |          19.7173 |           0.2386 |
[32m[20221213 15:29:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:29:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.49
[32m[20221213 15:29:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.10
[32m[20221213 15:29:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.66
[32m[20221213 15:29:59 @agent_ppo2.py:143][0m Total time:      37.06 min
[32m[20221213 15:29:59 @agent_ppo2.py:145][0m 3346432 total steps have happened
[32m[20221213 15:29:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1634 --------------------------#
[32m[20221213 15:29:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:29:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:29:59 @agent_ppo2.py:185][0m |          -0.0018 |          22.1903 |           0.2411 |
[32m[20221213 15:29:59 @agent_ppo2.py:185][0m |           0.0028 |          23.4309 |           0.2404 |
[32m[20221213 15:29:59 @agent_ppo2.py:185][0m |          -0.0065 |          21.6833 |           0.2394 |
[32m[20221213 15:29:59 @agent_ppo2.py:185][0m |          -0.0076 |          21.4239 |           0.2395 |
[32m[20221213 15:29:59 @agent_ppo2.py:185][0m |          -0.0078 |          21.3861 |           0.2393 |
[32m[20221213 15:29:59 @agent_ppo2.py:185][0m |          -0.0127 |          21.2434 |           0.2393 |
[32m[20221213 15:30:00 @agent_ppo2.py:185][0m |          -0.0135 |          21.1985 |           0.2390 |
[32m[20221213 15:30:00 @agent_ppo2.py:185][0m |          -0.0105 |          21.1388 |           0.2390 |
[32m[20221213 15:30:00 @agent_ppo2.py:185][0m |          -0.0150 |          21.1001 |           0.2388 |
[32m[20221213 15:30:00 @agent_ppo2.py:185][0m |          -0.0151 |          21.1493 |           0.2386 |
[32m[20221213 15:30:00 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:30:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.98
[32m[20221213 15:30:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.13
[32m[20221213 15:30:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.34
[32m[20221213 15:30:00 @agent_ppo2.py:143][0m Total time:      37.08 min
[32m[20221213 15:30:00 @agent_ppo2.py:145][0m 3348480 total steps have happened
[32m[20221213 15:30:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1635 --------------------------#
[32m[20221213 15:30:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:00 @agent_ppo2.py:185][0m |          -0.0026 |          22.1541 |           0.2333 |
[32m[20221213 15:30:00 @agent_ppo2.py:185][0m |          -0.0091 |          21.8468 |           0.2327 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0094 |          21.6377 |           0.2326 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0116 |          21.5112 |           0.2326 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0099 |          21.4734 |           0.2326 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0073 |          21.5908 |           0.2329 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0140 |          21.2021 |           0.2327 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0148 |          21.1257 |           0.2328 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0067 |          22.1624 |           0.2326 |
[32m[20221213 15:30:01 @agent_ppo2.py:185][0m |          -0.0158 |          20.9910 |           0.2321 |
[32m[20221213 15:30:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:30:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.75
[32m[20221213 15:30:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.90
[32m[20221213 15:30:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.76
[32m[20221213 15:30:01 @agent_ppo2.py:143][0m Total time:      37.10 min
[32m[20221213 15:30:01 @agent_ppo2.py:145][0m 3350528 total steps have happened
[32m[20221213 15:30:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1636 --------------------------#
[32m[20221213 15:30:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |           0.0111 |          24.0725 |           0.2319 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |           0.0031 |          23.4825 |           0.2317 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |          -0.0096 |          20.7048 |           0.2318 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |          -0.0120 |          20.3728 |           0.2322 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |          -0.0008 |          23.0639 |           0.2319 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |          -0.0042 |          21.1597 |           0.2319 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |          -0.0129 |          19.7649 |           0.2317 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |          -0.0161 |          19.6205 |           0.2321 |
[32m[20221213 15:30:02 @agent_ppo2.py:185][0m |          -0.0159 |          19.4496 |           0.2323 |
[32m[20221213 15:30:03 @agent_ppo2.py:185][0m |          -0.0180 |          19.3257 |           0.2325 |
[32m[20221213 15:30:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:30:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.07
[32m[20221213 15:30:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.24
[32m[20221213 15:30:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.63
[32m[20221213 15:30:03 @agent_ppo2.py:143][0m Total time:      37.13 min
[32m[20221213 15:30:03 @agent_ppo2.py:145][0m 3352576 total steps have happened
[32m[20221213 15:30:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1637 --------------------------#
[32m[20221213 15:30:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:30:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:03 @agent_ppo2.py:185][0m |          -0.0006 |          22.9108 |           0.2420 |
[32m[20221213 15:30:03 @agent_ppo2.py:185][0m |           0.0069 |          23.2512 |           0.2413 |
[32m[20221213 15:30:03 @agent_ppo2.py:185][0m |          -0.0055 |          22.0569 |           0.2408 |
[32m[20221213 15:30:03 @agent_ppo2.py:185][0m |          -0.0055 |          22.1760 |           0.2405 |
[32m[20221213 15:30:04 @agent_ppo2.py:185][0m |          -0.0096 |          21.6120 |           0.2406 |
[32m[20221213 15:30:04 @agent_ppo2.py:185][0m |          -0.0048 |          22.5944 |           0.2403 |
[32m[20221213 15:30:04 @agent_ppo2.py:185][0m |          -0.0128 |          21.2795 |           0.2400 |
[32m[20221213 15:30:04 @agent_ppo2.py:185][0m |          -0.0087 |          21.4821 |           0.2398 |
[32m[20221213 15:30:04 @agent_ppo2.py:185][0m |          -0.0153 |          21.0075 |           0.2398 |
[32m[20221213 15:30:04 @agent_ppo2.py:185][0m |          -0.0144 |          20.8400 |           0.2398 |
[32m[20221213 15:30:04 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:30:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.61
[32m[20221213 15:30:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.11
[32m[20221213 15:30:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.61
[32m[20221213 15:30:04 @agent_ppo2.py:143][0m Total time:      37.15 min
[32m[20221213 15:30:04 @agent_ppo2.py:145][0m 3354624 total steps have happened
[32m[20221213 15:30:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1638 --------------------------#
[32m[20221213 15:30:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0016 |          22.3474 |           0.2415 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0018 |          22.3688 |           0.2407 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0044 |          22.3138 |           0.2406 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0117 |          21.8874 |           0.2402 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0123 |          21.7884 |           0.2399 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0072 |          22.2580 |           0.2400 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0086 |          21.7892 |           0.2398 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0124 |          21.6915 |           0.2393 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0139 |          21.5130 |           0.2392 |
[32m[20221213 15:30:05 @agent_ppo2.py:185][0m |          -0.0151 |          21.4496 |           0.2392 |
[32m[20221213 15:30:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:30:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.55
[32m[20221213 15:30:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.04
[32m[20221213 15:30:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.18
[32m[20221213 15:30:06 @agent_ppo2.py:143][0m Total time:      37.17 min
[32m[20221213 15:30:06 @agent_ppo2.py:145][0m 3356672 total steps have happened
[32m[20221213 15:30:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1639 --------------------------#
[32m[20221213 15:30:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:06 @agent_ppo2.py:185][0m |          -0.0011 |          22.2032 |           0.2412 |
[32m[20221213 15:30:06 @agent_ppo2.py:185][0m |          -0.0063 |          21.8377 |           0.2403 |
[32m[20221213 15:30:06 @agent_ppo2.py:185][0m |          -0.0129 |          21.5677 |           0.2400 |
[32m[20221213 15:30:06 @agent_ppo2.py:185][0m |          -0.0136 |          21.3916 |           0.2399 |
[32m[20221213 15:30:06 @agent_ppo2.py:185][0m |          -0.0118 |          21.2282 |           0.2399 |
[32m[20221213 15:30:06 @agent_ppo2.py:185][0m |          -0.0086 |          22.0642 |           0.2396 |
[32m[20221213 15:30:06 @agent_ppo2.py:185][0m |          -0.0173 |          21.0033 |           0.2396 |
[32m[20221213 15:30:07 @agent_ppo2.py:185][0m |          -0.0067 |          21.4900 |           0.2396 |
[32m[20221213 15:30:07 @agent_ppo2.py:185][0m |          -0.0199 |          20.5561 |           0.2395 |
[32m[20221213 15:30:07 @agent_ppo2.py:185][0m |          -0.0206 |          20.4139 |           0.2394 |
[32m[20221213 15:30:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:30:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 238.55
[32m[20221213 15:30:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.65
[32m[20221213 15:30:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.35
[32m[20221213 15:30:07 @agent_ppo2.py:143][0m Total time:      37.20 min
[32m[20221213 15:30:07 @agent_ppo2.py:145][0m 3358720 total steps have happened
[32m[20221213 15:30:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1640 --------------------------#
[32m[20221213 15:30:07 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:30:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:07 @agent_ppo2.py:185][0m |          -0.0015 |          21.3966 |           0.2413 |
[32m[20221213 15:30:07 @agent_ppo2.py:185][0m |          -0.0042 |          20.5898 |           0.2412 |
[32m[20221213 15:30:07 @agent_ppo2.py:185][0m |          -0.0083 |          20.1004 |           0.2407 |
[32m[20221213 15:30:08 @agent_ppo2.py:185][0m |          -0.0094 |          19.7708 |           0.2406 |
[32m[20221213 15:30:08 @agent_ppo2.py:185][0m |          -0.0139 |          19.4672 |           0.2406 |
[32m[20221213 15:30:08 @agent_ppo2.py:185][0m |          -0.0128 |          19.2385 |           0.2405 |
[32m[20221213 15:30:08 @agent_ppo2.py:185][0m |          -0.0164 |          19.0587 |           0.2404 |
[32m[20221213 15:30:08 @agent_ppo2.py:185][0m |          -0.0168 |          18.8597 |           0.2405 |
[32m[20221213 15:30:08 @agent_ppo2.py:185][0m |          -0.0120 |          19.1139 |           0.2405 |
[32m[20221213 15:30:08 @agent_ppo2.py:185][0m |          -0.0194 |          18.5460 |           0.2405 |
[32m[20221213 15:30:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:30:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.96
[32m[20221213 15:30:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 261.05
[32m[20221213 15:30:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.94
[32m[20221213 15:30:08 @agent_ppo2.py:143][0m Total time:      37.22 min
[32m[20221213 15:30:08 @agent_ppo2.py:145][0m 3360768 total steps have happened
[32m[20221213 15:30:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1641 --------------------------#
[32m[20221213 15:30:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0015 |          22.6782 |           0.2356 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0094 |          21.8496 |           0.2347 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |           0.0035 |          23.0991 |           0.2342 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0157 |          21.1996 |           0.2341 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0166 |          20.9020 |           0.2339 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0181 |          20.6492 |           0.2340 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0188 |          20.4989 |           0.2339 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0173 |          20.3095 |           0.2338 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0053 |          21.6149 |           0.2337 |
[32m[20221213 15:30:09 @agent_ppo2.py:185][0m |          -0.0205 |          20.0346 |           0.2334 |
[32m[20221213 15:30:09 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:30:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.48
[32m[20221213 15:30:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.55
[32m[20221213 15:30:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 122.76
[32m[20221213 15:30:10 @agent_ppo2.py:143][0m Total time:      37.24 min
[32m[20221213 15:30:10 @agent_ppo2.py:145][0m 3362816 total steps have happened
[32m[20221213 15:30:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1642 --------------------------#
[32m[20221213 15:30:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:30:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:10 @agent_ppo2.py:185][0m |          -0.0015 |          23.9572 |           0.2336 |
[32m[20221213 15:30:10 @agent_ppo2.py:185][0m |          -0.0081 |          23.2954 |           0.2328 |
[32m[20221213 15:30:10 @agent_ppo2.py:185][0m |          -0.0099 |          23.0372 |           0.2330 |
[32m[20221213 15:30:10 @agent_ppo2.py:185][0m |          -0.0018 |          25.0993 |           0.2328 |
[32m[20221213 15:30:10 @agent_ppo2.py:185][0m |          -0.0119 |          22.7367 |           0.2328 |
[32m[20221213 15:30:10 @agent_ppo2.py:185][0m |          -0.0148 |          22.5977 |           0.2327 |
[32m[20221213 15:30:11 @agent_ppo2.py:185][0m |          -0.0170 |          22.5020 |           0.2331 |
[32m[20221213 15:30:11 @agent_ppo2.py:185][0m |          -0.0177 |          22.4168 |           0.2327 |
[32m[20221213 15:30:11 @agent_ppo2.py:185][0m |          -0.0177 |          22.3446 |           0.2328 |
[32m[20221213 15:30:11 @agent_ppo2.py:185][0m |          -0.0177 |          22.2438 |           0.2330 |
[32m[20221213 15:30:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:30:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.22
[32m[20221213 15:30:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.47
[32m[20221213 15:30:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.35
[32m[20221213 15:30:11 @agent_ppo2.py:143][0m Total time:      37.26 min
[32m[20221213 15:30:11 @agent_ppo2.py:145][0m 3364864 total steps have happened
[32m[20221213 15:30:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1643 --------------------------#
[32m[20221213 15:30:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:30:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:11 @agent_ppo2.py:185][0m |          -0.0030 |          22.1977 |           0.2348 |
[32m[20221213 15:30:11 @agent_ppo2.py:185][0m |          -0.0079 |          21.7783 |           0.2342 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |           0.0019 |          23.8725 |           0.2339 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |          -0.0079 |          21.4957 |           0.2332 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |          -0.0144 |          21.2560 |           0.2336 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |          -0.0141 |          21.1598 |           0.2338 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |          -0.0146 |          21.0439 |           0.2336 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |          -0.0164 |          20.9129 |           0.2338 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |          -0.0143 |          20.8686 |           0.2337 |
[32m[20221213 15:30:12 @agent_ppo2.py:185][0m |          -0.0164 |          20.7728 |           0.2337 |
[32m[20221213 15:30:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:30:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.19
[32m[20221213 15:30:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.21
[32m[20221213 15:30:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.56
[32m[20221213 15:30:12 @agent_ppo2.py:143][0m Total time:      37.29 min
[32m[20221213 15:30:12 @agent_ppo2.py:145][0m 3366912 total steps have happened
[32m[20221213 15:30:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1644 --------------------------#
[32m[20221213 15:30:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0002 |          22.8176 |           0.2333 |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0063 |          22.2508 |           0.2329 |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0037 |          22.4670 |           0.2330 |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0116 |          21.7191 |           0.2330 |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0114 |          21.6495 |           0.2329 |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0142 |          21.5623 |           0.2333 |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0142 |          21.4476 |           0.2337 |
[32m[20221213 15:30:13 @agent_ppo2.py:185][0m |          -0.0149 |          21.4438 |           0.2336 |
[32m[20221213 15:30:14 @agent_ppo2.py:185][0m |          -0.0153 |          21.3258 |           0.2338 |
[32m[20221213 15:30:14 @agent_ppo2.py:185][0m |          -0.0193 |          21.2787 |           0.2338 |
[32m[20221213 15:30:14 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:30:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.03
[32m[20221213 15:30:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.98
[32m[20221213 15:30:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.16
[32m[20221213 15:30:14 @agent_ppo2.py:143][0m Total time:      37.31 min
[32m[20221213 15:30:14 @agent_ppo2.py:145][0m 3368960 total steps have happened
[32m[20221213 15:30:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1645 --------------------------#
[32m[20221213 15:30:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:14 @agent_ppo2.py:185][0m |          -0.0000 |          22.6430 |           0.2423 |
[32m[20221213 15:30:14 @agent_ppo2.py:185][0m |          -0.0053 |          21.9701 |           0.2420 |
[32m[20221213 15:30:14 @agent_ppo2.py:185][0m |          -0.0085 |          21.4231 |           0.2416 |
[32m[20221213 15:30:14 @agent_ppo2.py:185][0m |          -0.0081 |          21.0806 |           0.2414 |
[32m[20221213 15:30:15 @agent_ppo2.py:185][0m |          -0.0108 |          20.8949 |           0.2408 |
[32m[20221213 15:30:15 @agent_ppo2.py:185][0m |          -0.0129 |          20.7001 |           0.2410 |
[32m[20221213 15:30:15 @agent_ppo2.py:185][0m |          -0.0146 |          20.4480 |           0.2409 |
[32m[20221213 15:30:15 @agent_ppo2.py:185][0m |          -0.0085 |          20.7798 |           0.2407 |
[32m[20221213 15:30:15 @agent_ppo2.py:185][0m |          -0.0136 |          20.2876 |           0.2402 |
[32m[20221213 15:30:15 @agent_ppo2.py:185][0m |          -0.0087 |          20.1717 |           0.2404 |
[32m[20221213 15:30:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:30:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.62
[32m[20221213 15:30:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.30
[32m[20221213 15:30:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.39
[32m[20221213 15:30:15 @agent_ppo2.py:143][0m Total time:      37.33 min
[32m[20221213 15:30:15 @agent_ppo2.py:145][0m 3371008 total steps have happened
[32m[20221213 15:30:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1646 --------------------------#
[32m[20221213 15:30:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |           0.0131 |          25.9006 |           0.2347 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0067 |          22.4573 |           0.2334 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0089 |          22.1245 |           0.2336 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0126 |          21.9565 |           0.2332 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0150 |          21.8781 |           0.2332 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0149 |          21.7714 |           0.2328 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0143 |          21.7247 |           0.2329 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0159 |          21.6527 |           0.2328 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0139 |          21.6971 |           0.2326 |
[32m[20221213 15:30:16 @agent_ppo2.py:185][0m |          -0.0160 |          21.5336 |           0.2324 |
[32m[20221213 15:30:16 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:30:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.95
[32m[20221213 15:30:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.91
[32m[20221213 15:30:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.88
[32m[20221213 15:30:17 @agent_ppo2.py:143][0m Total time:      37.36 min
[32m[20221213 15:30:17 @agent_ppo2.py:145][0m 3373056 total steps have happened
[32m[20221213 15:30:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1647 --------------------------#
[32m[20221213 15:30:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:17 @agent_ppo2.py:185][0m |           0.0055 |          22.6401 |           0.2320 |
[32m[20221213 15:30:17 @agent_ppo2.py:185][0m |          -0.0039 |          21.5140 |           0.2326 |
[32m[20221213 15:30:17 @agent_ppo2.py:185][0m |          -0.0110 |          21.1373 |           0.2325 |
[32m[20221213 15:30:17 @agent_ppo2.py:185][0m |          -0.0131 |          20.9368 |           0.2321 |
[32m[20221213 15:30:17 @agent_ppo2.py:185][0m |          -0.0098 |          21.0703 |           0.2324 |
[32m[20221213 15:30:17 @agent_ppo2.py:185][0m |          -0.0139 |          20.5890 |           0.2319 |
[32m[20221213 15:30:18 @agent_ppo2.py:185][0m |          -0.0169 |          20.4650 |           0.2323 |
[32m[20221213 15:30:18 @agent_ppo2.py:185][0m |          -0.0165 |          20.3339 |           0.2324 |
[32m[20221213 15:30:18 @agent_ppo2.py:185][0m |          -0.0159 |          20.1966 |           0.2319 |
[32m[20221213 15:30:18 @agent_ppo2.py:185][0m |          -0.0191 |          20.1498 |           0.2320 |
[32m[20221213 15:30:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:30:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.65
[32m[20221213 15:30:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.70
[32m[20221213 15:30:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.00
[32m[20221213 15:30:18 @agent_ppo2.py:143][0m Total time:      37.38 min
[32m[20221213 15:30:18 @agent_ppo2.py:145][0m 3375104 total steps have happened
[32m[20221213 15:30:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1648 --------------------------#
[32m[20221213 15:30:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:30:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:18 @agent_ppo2.py:185][0m |          -0.0026 |          22.6934 |           0.2353 |
[32m[20221213 15:30:18 @agent_ppo2.py:185][0m |          -0.0010 |          23.1466 |           0.2352 |
[32m[20221213 15:30:18 @agent_ppo2.py:185][0m |          -0.0050 |          22.0767 |           0.2345 |
[32m[20221213 15:30:19 @agent_ppo2.py:185][0m |          -0.0006 |          23.8516 |           0.2346 |
[32m[20221213 15:30:19 @agent_ppo2.py:185][0m |          -0.0151 |          21.5517 |           0.2346 |
[32m[20221213 15:30:19 @agent_ppo2.py:185][0m |          -0.0150 |          21.3572 |           0.2346 |
[32m[20221213 15:30:19 @agent_ppo2.py:185][0m |          -0.0062 |          22.2413 |           0.2345 |
[32m[20221213 15:30:19 @agent_ppo2.py:185][0m |          -0.0158 |          21.2085 |           0.2347 |
[32m[20221213 15:30:19 @agent_ppo2.py:185][0m |          -0.0153 |          21.1082 |           0.2343 |
[32m[20221213 15:30:19 @agent_ppo2.py:185][0m |          -0.0136 |          21.2335 |           0.2344 |
[32m[20221213 15:30:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:30:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.66
[32m[20221213 15:30:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.33
[32m[20221213 15:30:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.23
[32m[20221213 15:30:19 @agent_ppo2.py:143][0m Total time:      37.40 min
[32m[20221213 15:30:19 @agent_ppo2.py:145][0m 3377152 total steps have happened
[32m[20221213 15:30:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1649 --------------------------#
[32m[20221213 15:30:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:30:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0026 |          22.2982 |           0.2404 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0014 |          21.9791 |           0.2401 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0032 |          21.8256 |           0.2402 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0042 |          21.7271 |           0.2399 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0124 |          21.0088 |           0.2402 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0156 |          20.8837 |           0.2397 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0044 |          21.9231 |           0.2397 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0103 |          20.6791 |           0.2402 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0127 |          20.5354 |           0.2397 |
[32m[20221213 15:30:20 @agent_ppo2.py:185][0m |          -0.0162 |          20.5385 |           0.2399 |
[32m[20221213 15:30:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:30:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.60
[32m[20221213 15:30:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.29
[32m[20221213 15:30:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.28
[32m[20221213 15:30:21 @agent_ppo2.py:143][0m Total time:      37.42 min
[32m[20221213 15:30:21 @agent_ppo2.py:145][0m 3379200 total steps have happened
[32m[20221213 15:30:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1650 --------------------------#
[32m[20221213 15:30:21 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:30:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:21 @agent_ppo2.py:185][0m |          -0.0017 |          20.8914 |           0.2358 |
[32m[20221213 15:30:21 @agent_ppo2.py:185][0m |          -0.0039 |          20.1255 |           0.2351 |
[32m[20221213 15:30:21 @agent_ppo2.py:185][0m |          -0.0062 |          19.8798 |           0.2352 |
[32m[20221213 15:30:21 @agent_ppo2.py:185][0m |          -0.0096 |          19.5948 |           0.2348 |
[32m[20221213 15:30:21 @agent_ppo2.py:185][0m |          -0.0113 |          19.4802 |           0.2348 |
[32m[20221213 15:30:22 @agent_ppo2.py:185][0m |          -0.0127 |          19.3804 |           0.2346 |
[32m[20221213 15:30:22 @agent_ppo2.py:185][0m |          -0.0011 |          22.2823 |           0.2345 |
[32m[20221213 15:30:22 @agent_ppo2.py:185][0m |          -0.0074 |          19.6957 |           0.2337 |
[32m[20221213 15:30:22 @agent_ppo2.py:185][0m |          -0.0147 |          19.1464 |           0.2342 |
[32m[20221213 15:30:22 @agent_ppo2.py:185][0m |          -0.0176 |          19.1140 |           0.2342 |
[32m[20221213 15:30:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:30:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.60
[32m[20221213 15:30:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.61
[32m[20221213 15:30:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 148.52
[32m[20221213 15:30:22 @agent_ppo2.py:143][0m Total time:      37.45 min
[32m[20221213 15:30:22 @agent_ppo2.py:145][0m 3381248 total steps have happened
[32m[20221213 15:30:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1651 --------------------------#
[32m[20221213 15:30:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:30:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:22 @agent_ppo2.py:185][0m |          -0.0008 |          21.8227 |           0.2390 |
[32m[20221213 15:30:22 @agent_ppo2.py:185][0m |          -0.0089 |          20.9998 |           0.2390 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0133 |          20.6028 |           0.2388 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0127 |          20.3627 |           0.2387 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0137 |          20.2201 |           0.2389 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0168 |          20.0495 |           0.2388 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0165 |          19.9128 |           0.2392 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0045 |          22.2686 |           0.2392 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0159 |          19.7562 |           0.2386 |
[32m[20221213 15:30:23 @agent_ppo2.py:185][0m |          -0.0188 |          19.6474 |           0.2390 |
[32m[20221213 15:30:23 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:30:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.36
[32m[20221213 15:30:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.36
[32m[20221213 15:30:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.85
[32m[20221213 15:30:23 @agent_ppo2.py:143][0m Total time:      37.47 min
[32m[20221213 15:30:23 @agent_ppo2.py:145][0m 3383296 total steps have happened
[32m[20221213 15:30:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1652 --------------------------#
[32m[20221213 15:30:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:30:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0000 |          22.5616 |           0.2396 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0064 |          22.2270 |           0.2392 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0094 |          22.0572 |           0.2399 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0114 |          21.9256 |           0.2393 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0124 |          21.8759 |           0.2398 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0136 |          21.8426 |           0.2397 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0161 |          21.8280 |           0.2398 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0088 |          22.5728 |           0.2398 |
[32m[20221213 15:30:24 @agent_ppo2.py:185][0m |          -0.0173 |          21.6909 |           0.2399 |
[32m[20221213 15:30:25 @agent_ppo2.py:185][0m |          -0.0185 |          21.6324 |           0.2398 |
[32m[20221213 15:30:25 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:30:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.81
[32m[20221213 15:30:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.07
[32m[20221213 15:30:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.54
[32m[20221213 15:30:25 @agent_ppo2.py:143][0m Total time:      37.49 min
[32m[20221213 15:30:25 @agent_ppo2.py:145][0m 3385344 total steps have happened
[32m[20221213 15:30:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1653 --------------------------#
[32m[20221213 15:30:25 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:30:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:25 @agent_ppo2.py:185][0m |          -0.0004 |          22.5476 |           0.2338 |
[32m[20221213 15:30:25 @agent_ppo2.py:185][0m |          -0.0079 |          22.2202 |           0.2335 |
[32m[20221213 15:30:25 @agent_ppo2.py:185][0m |          -0.0023 |          23.0456 |           0.2337 |
[32m[20221213 15:30:25 @agent_ppo2.py:185][0m |          -0.0098 |          21.9616 |           0.2332 |
[32m[20221213 15:30:26 @agent_ppo2.py:185][0m |          -0.0118 |          21.8562 |           0.2335 |
[32m[20221213 15:30:26 @agent_ppo2.py:185][0m |          -0.0121 |          21.8145 |           0.2333 |
[32m[20221213 15:30:26 @agent_ppo2.py:185][0m |          -0.0114 |          21.7853 |           0.2331 |
[32m[20221213 15:30:26 @agent_ppo2.py:185][0m |          -0.0118 |          21.7606 |           0.2334 |
[32m[20221213 15:30:26 @agent_ppo2.py:185][0m |          -0.0162 |          21.6333 |           0.2329 |
[32m[20221213 15:30:26 @agent_ppo2.py:185][0m |          -0.0057 |          23.6726 |           0.2330 |
[32m[20221213 15:30:26 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:30:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.33
[32m[20221213 15:30:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.41
[32m[20221213 15:30:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.75
[32m[20221213 15:30:27 @agent_ppo2.py:143][0m Total time:      37.52 min
[32m[20221213 15:30:27 @agent_ppo2.py:145][0m 3387392 total steps have happened
[32m[20221213 15:30:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1654 --------------------------#
[32m[20221213 15:30:27 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:30:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:27 @agent_ppo2.py:185][0m |           0.0056 |          23.1867 |           0.2374 |
[32m[20221213 15:30:27 @agent_ppo2.py:185][0m |          -0.0066 |          22.6749 |           0.2370 |
[32m[20221213 15:30:27 @agent_ppo2.py:185][0m |          -0.0024 |          22.8231 |           0.2367 |
[32m[20221213 15:30:27 @agent_ppo2.py:185][0m |          -0.0080 |          22.3643 |           0.2366 |
[32m[20221213 15:30:27 @agent_ppo2.py:185][0m |          -0.0107 |          22.1101 |           0.2368 |
[32m[20221213 15:30:28 @agent_ppo2.py:185][0m |           0.0060 |          25.3990 |           0.2368 |
[32m[20221213 15:30:28 @agent_ppo2.py:185][0m |          -0.0128 |          21.9452 |           0.2366 |
[32m[20221213 15:30:28 @agent_ppo2.py:185][0m |          -0.0155 |          21.8066 |           0.2366 |
[32m[20221213 15:30:28 @agent_ppo2.py:185][0m |          -0.0142 |          21.7542 |           0.2368 |
[32m[20221213 15:30:28 @agent_ppo2.py:185][0m |          -0.0143 |          21.6911 |           0.2367 |
[32m[20221213 15:30:28 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 15:30:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.67
[32m[20221213 15:30:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.40
[32m[20221213 15:30:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.49
[32m[20221213 15:30:28 @agent_ppo2.py:143][0m Total time:      37.55 min
[32m[20221213 15:30:28 @agent_ppo2.py:145][0m 3389440 total steps have happened
[32m[20221213 15:30:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1655 --------------------------#
[32m[20221213 15:30:28 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |           0.0086 |          25.7138 |           0.2414 |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |          -0.0041 |          22.4908 |           0.2399 |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |          -0.0108 |          22.3599 |           0.2406 |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |          -0.0096 |          22.2088 |           0.2407 |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |          -0.0094 |          22.2229 |           0.2405 |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |          -0.0119 |          22.1357 |           0.2406 |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |          -0.0149 |          22.0340 |           0.2403 |
[32m[20221213 15:30:29 @agent_ppo2.py:185][0m |          -0.0153 |          22.0129 |           0.2405 |
[32m[20221213 15:30:30 @agent_ppo2.py:185][0m |          -0.0156 |          22.0063 |           0.2405 |
[32m[20221213 15:30:30 @agent_ppo2.py:185][0m |          -0.0159 |          21.9795 |           0.2404 |
[32m[20221213 15:30:30 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221213 15:30:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.37
[32m[20221213 15:30:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.32
[32m[20221213 15:30:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.60
[32m[20221213 15:30:30 @agent_ppo2.py:143][0m Total time:      37.58 min
[32m[20221213 15:30:30 @agent_ppo2.py:145][0m 3391488 total steps have happened
[32m[20221213 15:30:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1656 --------------------------#
[32m[20221213 15:30:30 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:30 @agent_ppo2.py:185][0m |          -0.0020 |          22.2932 |           0.2385 |
[32m[20221213 15:30:30 @agent_ppo2.py:185][0m |          -0.0077 |          22.0557 |           0.2376 |
[32m[20221213 15:30:30 @agent_ppo2.py:185][0m |          -0.0105 |          21.9087 |           0.2373 |
[32m[20221213 15:30:31 @agent_ppo2.py:185][0m |          -0.0110 |          21.7576 |           0.2369 |
[32m[20221213 15:30:31 @agent_ppo2.py:185][0m |          -0.0042 |          24.1832 |           0.2370 |
[32m[20221213 15:30:31 @agent_ppo2.py:185][0m |          -0.0144 |          21.7152 |           0.2367 |
[32m[20221213 15:30:31 @agent_ppo2.py:185][0m |          -0.0132 |          21.5933 |           0.2369 |
[32m[20221213 15:30:31 @agent_ppo2.py:185][0m |          -0.0163 |          21.5184 |           0.2365 |
[32m[20221213 15:30:31 @agent_ppo2.py:185][0m |          -0.0160 |          21.4498 |           0.2366 |
[32m[20221213 15:30:31 @agent_ppo2.py:185][0m |          -0.0174 |          21.3798 |           0.2360 |
[32m[20221213 15:30:31 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 15:30:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.20
[32m[20221213 15:30:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.67
[32m[20221213 15:30:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.85
[32m[20221213 15:30:31 @agent_ppo2.py:143][0m Total time:      37.60 min
[32m[20221213 15:30:31 @agent_ppo2.py:145][0m 3393536 total steps have happened
[32m[20221213 15:30:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1657 --------------------------#
[32m[20221213 15:30:32 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:32 @agent_ppo2.py:185][0m |          -0.0011 |          22.5740 |           0.2358 |
[32m[20221213 15:30:32 @agent_ppo2.py:185][0m |          -0.0059 |          22.2292 |           0.2357 |
[32m[20221213 15:30:32 @agent_ppo2.py:185][0m |          -0.0064 |          22.0209 |           0.2355 |
[32m[20221213 15:30:32 @agent_ppo2.py:185][0m |          -0.0095 |          21.7697 |           0.2355 |
[32m[20221213 15:30:32 @agent_ppo2.py:185][0m |          -0.0117 |          21.6298 |           0.2355 |
[32m[20221213 15:30:32 @agent_ppo2.py:185][0m |          -0.0113 |          21.4690 |           0.2355 |
[32m[20221213 15:30:33 @agent_ppo2.py:185][0m |          -0.0079 |          21.6953 |           0.2356 |
[32m[20221213 15:30:33 @agent_ppo2.py:185][0m |          -0.0141 |          21.2558 |           0.2356 |
[32m[20221213 15:30:33 @agent_ppo2.py:185][0m |          -0.0114 |          21.2841 |           0.2358 |
[32m[20221213 15:30:33 @agent_ppo2.py:185][0m |          -0.0140 |          21.1060 |           0.2356 |
[32m[20221213 15:30:33 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221213 15:30:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.19
[32m[20221213 15:30:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.71
[32m[20221213 15:30:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.85
[32m[20221213 15:30:33 @agent_ppo2.py:143][0m Total time:      37.63 min
[32m[20221213 15:30:33 @agent_ppo2.py:145][0m 3395584 total steps have happened
[32m[20221213 15:30:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1658 --------------------------#
[32m[20221213 15:30:33 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:33 @agent_ppo2.py:185][0m |          -0.0009 |          23.4093 |           0.2375 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0043 |          22.9930 |           0.2367 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0076 |          22.7754 |           0.2366 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0117 |          22.4730 |           0.2361 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0118 |          22.2064 |           0.2359 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0122 |          22.0813 |           0.2356 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0120 |          21.9261 |           0.2357 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0076 |          22.1329 |           0.2361 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0067 |          22.6009 |           0.2352 |
[32m[20221213 15:30:34 @agent_ppo2.py:185][0m |          -0.0161 |          21.5835 |           0.2359 |
[32m[20221213 15:30:34 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221213 15:30:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.23
[32m[20221213 15:30:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.83
[32m[20221213 15:30:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.06
[32m[20221213 15:30:35 @agent_ppo2.py:143][0m Total time:      37.66 min
[32m[20221213 15:30:35 @agent_ppo2.py:145][0m 3397632 total steps have happened
[32m[20221213 15:30:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1659 --------------------------#
[32m[20221213 15:30:35 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:35 @agent_ppo2.py:185][0m |          -0.0005 |          22.7467 |           0.2352 |
[32m[20221213 15:30:35 @agent_ppo2.py:185][0m |          -0.0054 |          22.3816 |           0.2346 |
[32m[20221213 15:30:35 @agent_ppo2.py:185][0m |          -0.0114 |          22.1476 |           0.2341 |
[32m[20221213 15:30:35 @agent_ppo2.py:185][0m |          -0.0074 |          21.9898 |           0.2341 |
[32m[20221213 15:30:36 @agent_ppo2.py:185][0m |          -0.0128 |          21.8658 |           0.2336 |
[32m[20221213 15:30:36 @agent_ppo2.py:185][0m |          -0.0140 |          21.8070 |           0.2339 |
[32m[20221213 15:30:36 @agent_ppo2.py:185][0m |          -0.0131 |          21.8300 |           0.2337 |
[32m[20221213 15:30:36 @agent_ppo2.py:185][0m |          -0.0049 |          24.0234 |           0.2336 |
[32m[20221213 15:30:36 @agent_ppo2.py:185][0m |          -0.0149 |          21.5889 |           0.2332 |
[32m[20221213 15:30:36 @agent_ppo2.py:185][0m |          -0.0171 |          21.5123 |           0.2335 |
[32m[20221213 15:30:36 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.98
[32m[20221213 15:30:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.42
[32m[20221213 15:30:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 139.50
[32m[20221213 15:30:36 @agent_ppo2.py:143][0m Total time:      37.68 min
[32m[20221213 15:30:36 @agent_ppo2.py:145][0m 3399680 total steps have happened
[32m[20221213 15:30:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1660 --------------------------#
[32m[20221213 15:30:36 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 15:30:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0016 |          22.8134 |           0.2354 |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0092 |          22.4020 |           0.2346 |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0111 |          22.1997 |           0.2341 |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0105 |          22.0755 |           0.2343 |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0112 |          22.0325 |           0.2337 |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0155 |          21.7415 |           0.2335 |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0172 |          21.6363 |           0.2333 |
[32m[20221213 15:30:37 @agent_ppo2.py:185][0m |          -0.0198 |          21.5274 |           0.2331 |
[32m[20221213 15:30:38 @agent_ppo2.py:185][0m |          -0.0163 |          21.4645 |           0.2330 |
[32m[20221213 15:30:38 @agent_ppo2.py:185][0m |          -0.0166 |          21.4201 |           0.2329 |
[32m[20221213 15:30:38 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 15:30:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 231.82
[32m[20221213 15:30:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 250.56
[32m[20221213 15:30:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.52
[32m[20221213 15:30:38 @agent_ppo2.py:143][0m Total time:      37.71 min
[32m[20221213 15:30:38 @agent_ppo2.py:145][0m 3401728 total steps have happened
[32m[20221213 15:30:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1661 --------------------------#
[32m[20221213 15:30:38 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:38 @agent_ppo2.py:185][0m |          -0.0027 |          22.8576 |           0.2341 |
[32m[20221213 15:30:38 @agent_ppo2.py:185][0m |          -0.0069 |          22.1056 |           0.2334 |
[32m[20221213 15:30:38 @agent_ppo2.py:185][0m |          -0.0080 |          21.7417 |           0.2334 |
[32m[20221213 15:30:39 @agent_ppo2.py:185][0m |          -0.0133 |          21.4331 |           0.2335 |
[32m[20221213 15:30:39 @agent_ppo2.py:185][0m |          -0.0156 |          21.2070 |           0.2334 |
[32m[20221213 15:30:39 @agent_ppo2.py:185][0m |          -0.0151 |          21.0377 |           0.2331 |
[32m[20221213 15:30:39 @agent_ppo2.py:185][0m |          -0.0156 |          20.9229 |           0.2335 |
[32m[20221213 15:30:39 @agent_ppo2.py:185][0m |          -0.0178 |          20.7903 |           0.2331 |
[32m[20221213 15:30:39 @agent_ppo2.py:185][0m |          -0.0114 |          21.4080 |           0.2330 |
[32m[20221213 15:30:39 @agent_ppo2.py:185][0m |          -0.0202 |          20.6016 |           0.2327 |
[32m[20221213 15:30:39 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.74
[32m[20221213 15:30:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.98
[32m[20221213 15:30:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.71
[32m[20221213 15:30:39 @agent_ppo2.py:143][0m Total time:      37.74 min
[32m[20221213 15:30:39 @agent_ppo2.py:145][0m 3403776 total steps have happened
[32m[20221213 15:30:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1662 --------------------------#
[32m[20221213 15:30:40 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:30:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:40 @agent_ppo2.py:185][0m |           0.0137 |          24.3156 |           0.2290 |
[32m[20221213 15:30:40 @agent_ppo2.py:185][0m |          -0.0055 |          22.3191 |           0.2282 |
[32m[20221213 15:30:40 @agent_ppo2.py:185][0m |          -0.0094 |          22.0295 |           0.2286 |
[32m[20221213 15:30:40 @agent_ppo2.py:185][0m |          -0.0121 |          21.8968 |           0.2286 |
[32m[20221213 15:30:40 @agent_ppo2.py:185][0m |          -0.0099 |          22.0435 |           0.2286 |
[32m[20221213 15:30:40 @agent_ppo2.py:185][0m |          -0.0155 |          21.6644 |           0.2287 |
[32m[20221213 15:30:40 @agent_ppo2.py:185][0m |          -0.0158 |          21.5864 |           0.2285 |
[32m[20221213 15:30:41 @agent_ppo2.py:185][0m |          -0.0178 |          21.4581 |           0.2285 |
[32m[20221213 15:30:41 @agent_ppo2.py:185][0m |          -0.0139 |          21.6245 |           0.2285 |
[32m[20221213 15:30:41 @agent_ppo2.py:185][0m |          -0.0190 |          21.3886 |           0.2283 |
[32m[20221213 15:30:41 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.81
[32m[20221213 15:30:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.49
[32m[20221213 15:30:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.52
[32m[20221213 15:30:41 @agent_ppo2.py:143][0m Total time:      37.76 min
[32m[20221213 15:30:41 @agent_ppo2.py:145][0m 3405824 total steps have happened
[32m[20221213 15:30:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1663 --------------------------#
[32m[20221213 15:30:41 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:41 @agent_ppo2.py:185][0m |          -0.0020 |          22.0067 |           0.2347 |
[32m[20221213 15:30:41 @agent_ppo2.py:185][0m |           0.0097 |          23.8183 |           0.2342 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0096 |          21.3802 |           0.2339 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0111 |          21.2503 |           0.2338 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0063 |          21.5049 |           0.2332 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0136 |          21.1125 |           0.2330 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0084 |          21.3237 |           0.2326 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0147 |          20.9979 |           0.2323 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0131 |          21.0558 |           0.2324 |
[32m[20221213 15:30:42 @agent_ppo2.py:185][0m |          -0.0116 |          21.3550 |           0.2320 |
[32m[20221213 15:30:42 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.10
[32m[20221213 15:30:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.53
[32m[20221213 15:30:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.80
[32m[20221213 15:30:43 @agent_ppo2.py:143][0m Total time:      37.79 min
[32m[20221213 15:30:43 @agent_ppo2.py:145][0m 3407872 total steps have happened
[32m[20221213 15:30:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1664 --------------------------#
[32m[20221213 15:30:43 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:30:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:43 @agent_ppo2.py:185][0m |          -0.0012 |          22.3964 |           0.2259 |
[32m[20221213 15:30:43 @agent_ppo2.py:185][0m |          -0.0099 |          22.1309 |           0.2254 |
[32m[20221213 15:30:43 @agent_ppo2.py:185][0m |          -0.0116 |          21.9279 |           0.2250 |
[32m[20221213 15:30:43 @agent_ppo2.py:185][0m |          -0.0136 |          21.7795 |           0.2248 |
[32m[20221213 15:30:43 @agent_ppo2.py:185][0m |          -0.0094 |          22.6511 |           0.2247 |
[32m[20221213 15:30:44 @agent_ppo2.py:185][0m |          -0.0159 |          21.6018 |           0.2249 |
[32m[20221213 15:30:44 @agent_ppo2.py:185][0m |          -0.0185 |          21.5354 |           0.2246 |
[32m[20221213 15:30:44 @agent_ppo2.py:185][0m |          -0.0140 |          21.9073 |           0.2247 |
[32m[20221213 15:30:44 @agent_ppo2.py:185][0m |          -0.0199 |          21.3687 |           0.2243 |
[32m[20221213 15:30:44 @agent_ppo2.py:185][0m |          -0.0210 |          21.3018 |           0.2241 |
[32m[20221213 15:30:44 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 15:30:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.65
[32m[20221213 15:30:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.93
[32m[20221213 15:30:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.74
[32m[20221213 15:30:44 @agent_ppo2.py:143][0m Total time:      37.82 min
[32m[20221213 15:30:44 @agent_ppo2.py:145][0m 3409920 total steps have happened
[32m[20221213 15:30:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1665 --------------------------#
[32m[20221213 15:30:44 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0016 |          22.3760 |           0.2304 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0079 |          21.9611 |           0.2292 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0011 |          23.3723 |           0.2295 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0058 |          22.0867 |           0.2289 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0012 |          22.9912 |           0.2287 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0118 |          21.2639 |           0.2284 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0152 |          21.1105 |           0.2288 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0091 |          21.2996 |           0.2286 |
[32m[20221213 15:30:45 @agent_ppo2.py:185][0m |          -0.0139 |          20.8586 |           0.2284 |
[32m[20221213 15:30:46 @agent_ppo2.py:185][0m |          -0.0012 |          22.9076 |           0.2285 |
[32m[20221213 15:30:46 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.84
[32m[20221213 15:30:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.96
[32m[20221213 15:30:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.89
[32m[20221213 15:30:46 @agent_ppo2.py:143][0m Total time:      37.84 min
[32m[20221213 15:30:46 @agent_ppo2.py:145][0m 3411968 total steps have happened
[32m[20221213 15:30:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1666 --------------------------#
[32m[20221213 15:30:46 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:46 @agent_ppo2.py:185][0m |          -0.0010 |          23.6694 |           0.2299 |
[32m[20221213 15:30:46 @agent_ppo2.py:185][0m |          -0.0084 |          23.2290 |           0.2302 |
[32m[20221213 15:30:46 @agent_ppo2.py:185][0m |          -0.0085 |          23.0335 |           0.2297 |
[32m[20221213 15:30:46 @agent_ppo2.py:185][0m |          -0.0136 |          22.8891 |           0.2297 |
[32m[20221213 15:30:47 @agent_ppo2.py:185][0m |          -0.0027 |          24.6956 |           0.2297 |
[32m[20221213 15:30:47 @agent_ppo2.py:185][0m |          -0.0074 |          23.6556 |           0.2293 |
[32m[20221213 15:30:47 @agent_ppo2.py:185][0m |          -0.0173 |          22.6145 |           0.2293 |
[32m[20221213 15:30:47 @agent_ppo2.py:185][0m |          -0.0190 |          22.5772 |           0.2295 |
[32m[20221213 15:30:47 @agent_ppo2.py:185][0m |          -0.0156 |          22.6109 |           0.2291 |
[32m[20221213 15:30:47 @agent_ppo2.py:185][0m |          -0.0129 |          22.9783 |           0.2292 |
[32m[20221213 15:30:47 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 15:30:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.77
[32m[20221213 15:30:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.91
[32m[20221213 15:30:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.23
[32m[20221213 15:30:47 @agent_ppo2.py:143][0m Total time:      37.87 min
[32m[20221213 15:30:47 @agent_ppo2.py:145][0m 3414016 total steps have happened
[32m[20221213 15:30:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1667 --------------------------#
[32m[20221213 15:30:48 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |           0.0029 |          21.8204 |           0.2326 |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |          -0.0105 |          21.0425 |           0.2321 |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |          -0.0079 |          20.6524 |           0.2321 |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |          -0.0041 |          21.0157 |           0.2320 |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |          -0.0134 |          20.0517 |           0.2320 |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |          -0.0012 |          21.9453 |           0.2317 |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |          -0.0143 |          19.7117 |           0.2311 |
[32m[20221213 15:30:48 @agent_ppo2.py:185][0m |          -0.0167 |          19.6031 |           0.2315 |
[32m[20221213 15:30:49 @agent_ppo2.py:185][0m |          -0.0177 |          19.4937 |           0.2315 |
[32m[20221213 15:30:49 @agent_ppo2.py:185][0m |          -0.0168 |          19.3863 |           0.2316 |
[32m[20221213 15:30:49 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.31
[32m[20221213 15:30:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.38
[32m[20221213 15:30:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 170.03
[32m[20221213 15:30:49 @agent_ppo2.py:143][0m Total time:      37.90 min
[32m[20221213 15:30:49 @agent_ppo2.py:145][0m 3416064 total steps have happened
[32m[20221213 15:30:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1668 --------------------------#
[32m[20221213 15:30:49 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:49 @agent_ppo2.py:185][0m |           0.0035 |          23.1318 |           0.2172 |
[32m[20221213 15:30:49 @agent_ppo2.py:185][0m |          -0.0095 |          22.2598 |           0.2173 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0125 |          22.1442 |           0.2172 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0105 |          22.4132 |           0.2173 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0088 |          22.2922 |           0.2170 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0068 |          23.7599 |           0.2172 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0163 |          21.9171 |           0.2170 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0183 |          21.8304 |           0.2171 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0196 |          21.8886 |           0.2173 |
[32m[20221213 15:30:50 @agent_ppo2.py:185][0m |          -0.0062 |          25.0027 |           0.2174 |
[32m[20221213 15:30:50 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 15:30:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.11
[32m[20221213 15:30:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.91
[32m[20221213 15:30:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.18
[32m[20221213 15:30:50 @agent_ppo2.py:143][0m Total time:      37.92 min
[32m[20221213 15:30:50 @agent_ppo2.py:145][0m 3418112 total steps have happened
[32m[20221213 15:30:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1669 --------------------------#
[32m[20221213 15:30:51 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:51 @agent_ppo2.py:185][0m |           0.0000 |          21.3849 |           0.2246 |
[32m[20221213 15:30:51 @agent_ppo2.py:185][0m |          -0.0048 |          21.1197 |           0.2247 |
[32m[20221213 15:30:51 @agent_ppo2.py:185][0m |          -0.0077 |          20.9789 |           0.2242 |
[32m[20221213 15:30:51 @agent_ppo2.py:185][0m |          -0.0105 |          20.8931 |           0.2245 |
[32m[20221213 15:30:51 @agent_ppo2.py:185][0m |          -0.0083 |          20.9543 |           0.2243 |
[32m[20221213 15:30:51 @agent_ppo2.py:185][0m |          -0.0055 |          21.4055 |           0.2242 |
[32m[20221213 15:30:52 @agent_ppo2.py:185][0m |          -0.0130 |          20.6928 |           0.2242 |
[32m[20221213 15:30:52 @agent_ppo2.py:185][0m |          -0.0140 |          20.6752 |           0.2244 |
[32m[20221213 15:30:52 @agent_ppo2.py:185][0m |          -0.0152 |          20.5972 |           0.2243 |
[32m[20221213 15:30:52 @agent_ppo2.py:185][0m |          -0.0155 |          20.5822 |           0.2244 |
[32m[20221213 15:30:52 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 15:30:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.10
[32m[20221213 15:30:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.05
[32m[20221213 15:30:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.90
[32m[20221213 15:30:52 @agent_ppo2.py:143][0m Total time:      37.95 min
[32m[20221213 15:30:52 @agent_ppo2.py:145][0m 3420160 total steps have happened
[32m[20221213 15:30:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1670 --------------------------#
[32m[20221213 15:30:52 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 15:30:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:52 @agent_ppo2.py:185][0m |          -0.0009 |          22.5419 |           0.2309 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0074 |          22.1581 |           0.2303 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0104 |          21.9806 |           0.2303 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0124 |          21.8561 |           0.2303 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0142 |          21.7408 |           0.2299 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0154 |          21.6964 |           0.2298 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0171 |          21.6441 |           0.2299 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0100 |          22.1422 |           0.2297 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0179 |          21.5635 |           0.2298 |
[32m[20221213 15:30:53 @agent_ppo2.py:185][0m |          -0.0117 |          22.4334 |           0.2299 |
[32m[20221213 15:30:53 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.54
[32m[20221213 15:30:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.77
[32m[20221213 15:30:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.61
[32m[20221213 15:30:54 @agent_ppo2.py:143][0m Total time:      37.97 min
[32m[20221213 15:30:54 @agent_ppo2.py:145][0m 3422208 total steps have happened
[32m[20221213 15:30:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1671 --------------------------#
[32m[20221213 15:30:54 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:54 @agent_ppo2.py:185][0m |          -0.0024 |          22.2979 |           0.2271 |
[32m[20221213 15:30:54 @agent_ppo2.py:185][0m |          -0.0079 |          21.7159 |           0.2266 |
[32m[20221213 15:30:54 @agent_ppo2.py:185][0m |          -0.0108 |          21.5760 |           0.2266 |
[32m[20221213 15:30:54 @agent_ppo2.py:185][0m |          -0.0141 |          21.3432 |           0.2266 |
[32m[20221213 15:30:54 @agent_ppo2.py:185][0m |          -0.0136 |          21.2696 |           0.2265 |
[32m[20221213 15:30:55 @agent_ppo2.py:185][0m |          -0.0161 |          21.2010 |           0.2266 |
[32m[20221213 15:30:55 @agent_ppo2.py:185][0m |          -0.0096 |          21.8945 |           0.2265 |
[32m[20221213 15:30:55 @agent_ppo2.py:185][0m |          -0.0160 |          21.0351 |           0.2265 |
[32m[20221213 15:30:55 @agent_ppo2.py:185][0m |          -0.0178 |          20.9278 |           0.2265 |
[32m[20221213 15:30:55 @agent_ppo2.py:185][0m |          -0.0205 |          20.8709 |           0.2264 |
[32m[20221213 15:30:55 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 15:30:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.60
[32m[20221213 15:30:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.05
[32m[20221213 15:30:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.92
[32m[20221213 15:30:55 @agent_ppo2.py:143][0m Total time:      38.00 min
[32m[20221213 15:30:55 @agent_ppo2.py:145][0m 3424256 total steps have happened
[32m[20221213 15:30:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1672 --------------------------#
[32m[20221213 15:30:55 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:30:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |           0.0006 |          21.3376 |           0.2293 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0083 |          20.8176 |           0.2292 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0078 |          20.5120 |           0.2287 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0109 |          20.4082 |           0.2290 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0123 |          20.2368 |           0.2290 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0137 |          20.1244 |           0.2289 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0149 |          20.0441 |           0.2290 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0156 |          20.0095 |           0.2291 |
[32m[20221213 15:30:56 @agent_ppo2.py:185][0m |          -0.0138 |          19.9352 |           0.2290 |
[32m[20221213 15:30:57 @agent_ppo2.py:185][0m |          -0.0163 |          19.8932 |           0.2290 |
[32m[20221213 15:30:57 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 15:30:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.84
[32m[20221213 15:30:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.55
[32m[20221213 15:30:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.93
[32m[20221213 15:30:57 @agent_ppo2.py:143][0m Total time:      38.03 min
[32m[20221213 15:30:57 @agent_ppo2.py:145][0m 3426304 total steps have happened
[32m[20221213 15:30:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1673 --------------------------#
[32m[20221213 15:30:57 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:30:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:57 @agent_ppo2.py:185][0m |          -0.0040 |          22.3180 |           0.2327 |
[32m[20221213 15:30:57 @agent_ppo2.py:185][0m |          -0.0077 |          21.8500 |           0.2321 |
[32m[20221213 15:30:57 @agent_ppo2.py:185][0m |          -0.0101 |          21.6446 |           0.2321 |
[32m[20221213 15:30:58 @agent_ppo2.py:185][0m |          -0.0118 |          21.4348 |           0.2316 |
[32m[20221213 15:30:58 @agent_ppo2.py:185][0m |          -0.0042 |          22.7342 |           0.2316 |
[32m[20221213 15:30:58 @agent_ppo2.py:185][0m |          -0.0138 |          21.2520 |           0.2311 |
[32m[20221213 15:30:58 @agent_ppo2.py:185][0m |          -0.0132 |          21.2100 |           0.2311 |
[32m[20221213 15:30:58 @agent_ppo2.py:185][0m |          -0.0170 |          21.0702 |           0.2308 |
[32m[20221213 15:30:58 @agent_ppo2.py:185][0m |          -0.0180 |          21.0248 |           0.2310 |
[32m[20221213 15:30:58 @agent_ppo2.py:185][0m |          -0.0182 |          20.8925 |           0.2306 |
[32m[20221213 15:30:58 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 15:30:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.78
[32m[20221213 15:30:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.31
[32m[20221213 15:30:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.65
[32m[20221213 15:30:59 @agent_ppo2.py:143][0m Total time:      38.06 min
[32m[20221213 15:30:59 @agent_ppo2.py:145][0m 3428352 total steps have happened
[32m[20221213 15:30:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1674 --------------------------#
[32m[20221213 15:30:59 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:30:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:30:59 @agent_ppo2.py:185][0m |           0.0058 |          23.0497 |           0.2278 |
[32m[20221213 15:30:59 @agent_ppo2.py:185][0m |          -0.0087 |          22.0575 |           0.2279 |
[32m[20221213 15:30:59 @agent_ppo2.py:185][0m |          -0.0116 |          21.8338 |           0.2278 |
[32m[20221213 15:30:59 @agent_ppo2.py:185][0m |          -0.0133 |          21.6954 |           0.2276 |
[32m[20221213 15:30:59 @agent_ppo2.py:185][0m |          -0.0134 |          21.6032 |           0.2274 |
[32m[20221213 15:30:59 @agent_ppo2.py:185][0m |          -0.0147 |          21.5333 |           0.2279 |
[32m[20221213 15:31:00 @agent_ppo2.py:185][0m |          -0.0172 |          21.4793 |           0.2277 |
[32m[20221213 15:31:00 @agent_ppo2.py:185][0m |          -0.0185 |          21.4152 |           0.2279 |
[32m[20221213 15:31:00 @agent_ppo2.py:185][0m |          -0.0166 |          21.3574 |           0.2278 |
[32m[20221213 15:31:00 @agent_ppo2.py:185][0m |          -0.0179 |          21.3040 |           0.2278 |
[32m[20221213 15:31:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:31:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.12
[32m[20221213 15:31:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.54
[32m[20221213 15:31:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.99
[32m[20221213 15:31:00 @agent_ppo2.py:143][0m Total time:      38.08 min
[32m[20221213 15:31:00 @agent_ppo2.py:145][0m 3430400 total steps have happened
[32m[20221213 15:31:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1675 --------------------------#
[32m[20221213 15:31:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:00 @agent_ppo2.py:185][0m |           0.0015 |          22.7601 |           0.2321 |
[32m[20221213 15:31:00 @agent_ppo2.py:185][0m |          -0.0114 |          21.8928 |           0.2312 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0104 |          21.6107 |           0.2315 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0132 |          21.3960 |           0.2310 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0117 |          21.2036 |           0.2309 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0147 |          21.1583 |           0.2308 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0158 |          20.9654 |           0.2308 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0116 |          21.6531 |           0.2307 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0185 |          20.7715 |           0.2306 |
[32m[20221213 15:31:01 @agent_ppo2.py:185][0m |          -0.0205 |          20.7453 |           0.2305 |
[32m[20221213 15:31:01 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.25
[32m[20221213 15:31:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.92
[32m[20221213 15:31:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.52
[32m[20221213 15:31:01 @agent_ppo2.py:143][0m Total time:      38.10 min
[32m[20221213 15:31:01 @agent_ppo2.py:145][0m 3432448 total steps have happened
[32m[20221213 15:31:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1676 --------------------------#
[32m[20221213 15:31:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |           0.0007 |          23.8487 |           0.2334 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0080 |          22.6983 |           0.2324 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0140 |          22.3894 |           0.2325 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0161 |          22.2189 |           0.2325 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0154 |          22.0662 |           0.2322 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0181 |          21.9723 |           0.2322 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0185 |          21.8674 |           0.2320 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0225 |          21.7463 |           0.2320 |
[32m[20221213 15:31:02 @agent_ppo2.py:185][0m |          -0.0195 |          21.7047 |           0.2317 |
[32m[20221213 15:31:03 @agent_ppo2.py:185][0m |          -0.0198 |          21.5516 |           0.2319 |
[32m[20221213 15:31:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.67
[32m[20221213 15:31:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.97
[32m[20221213 15:31:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.27
[32m[20221213 15:31:03 @agent_ppo2.py:143][0m Total time:      38.13 min
[32m[20221213 15:31:03 @agent_ppo2.py:145][0m 3434496 total steps have happened
[32m[20221213 15:31:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1677 --------------------------#
[32m[20221213 15:31:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:03 @agent_ppo2.py:185][0m |          -0.0034 |          22.3128 |           0.2305 |
[32m[20221213 15:31:03 @agent_ppo2.py:185][0m |          -0.0090 |          21.6861 |           0.2299 |
[32m[20221213 15:31:03 @agent_ppo2.py:185][0m |          -0.0085 |          21.5667 |           0.2299 |
[32m[20221213 15:31:03 @agent_ppo2.py:185][0m |          -0.0115 |          21.4429 |           0.2297 |
[32m[20221213 15:31:03 @agent_ppo2.py:185][0m |          -0.0121 |          21.3682 |           0.2297 |
[32m[20221213 15:31:04 @agent_ppo2.py:185][0m |          -0.0140 |          21.2738 |           0.2298 |
[32m[20221213 15:31:04 @agent_ppo2.py:185][0m |          -0.0149 |          21.2617 |           0.2296 |
[32m[20221213 15:31:04 @agent_ppo2.py:185][0m |          -0.0149 |          21.1663 |           0.2294 |
[32m[20221213 15:31:04 @agent_ppo2.py:185][0m |          -0.0158 |          21.0877 |           0.2293 |
[32m[20221213 15:31:04 @agent_ppo2.py:185][0m |          -0.0069 |          22.6713 |           0.2295 |
[32m[20221213 15:31:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:31:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.67
[32m[20221213 15:31:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.87
[32m[20221213 15:31:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.38
[32m[20221213 15:31:04 @agent_ppo2.py:143][0m Total time:      38.15 min
[32m[20221213 15:31:04 @agent_ppo2.py:145][0m 3436544 total steps have happened
[32m[20221213 15:31:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1678 --------------------------#
[32m[20221213 15:31:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:04 @agent_ppo2.py:185][0m |          -0.0006 |          22.6413 |           0.2250 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0069 |          22.3018 |           0.2251 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0006 |          24.6019 |           0.2252 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0117 |          22.0573 |           0.2252 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |           0.0009 |          24.0511 |           0.2250 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0133 |          21.8935 |           0.2250 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0142 |          21.8384 |           0.2252 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0147 |          21.7935 |           0.2252 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0082 |          22.2108 |           0.2249 |
[32m[20221213 15:31:05 @agent_ppo2.py:185][0m |          -0.0158 |          21.7112 |           0.2251 |
[32m[20221213 15:31:05 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:31:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.13
[32m[20221213 15:31:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.31
[32m[20221213 15:31:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.08
[32m[20221213 15:31:05 @agent_ppo2.py:143][0m Total time:      38.17 min
[32m[20221213 15:31:05 @agent_ppo2.py:145][0m 3438592 total steps have happened
[32m[20221213 15:31:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1679 --------------------------#
[32m[20221213 15:31:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:31:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0003 |          21.6379 |           0.2302 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0037 |          21.5651 |           0.2304 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0130 |          21.1090 |           0.2306 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0112 |          21.0370 |           0.2306 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0073 |          21.8236 |           0.2304 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0141 |          20.7712 |           0.2300 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0171 |          20.6679 |           0.2303 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0064 |          23.2679 |           0.2304 |
[32m[20221213 15:31:06 @agent_ppo2.py:185][0m |          -0.0022 |          23.5945 |           0.2301 |
[32m[20221213 15:31:07 @agent_ppo2.py:185][0m |          -0.0186 |          20.5217 |           0.2301 |
[32m[20221213 15:31:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:31:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.69
[32m[20221213 15:31:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.50
[32m[20221213 15:31:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.09
[32m[20221213 15:31:07 @agent_ppo2.py:143][0m Total time:      38.19 min
[32m[20221213 15:31:07 @agent_ppo2.py:145][0m 3440640 total steps have happened
[32m[20221213 15:31:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1680 --------------------------#
[32m[20221213 15:31:07 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:31:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:07 @agent_ppo2.py:185][0m |          -0.0006 |          21.9793 |           0.2290 |
[32m[20221213 15:31:07 @agent_ppo2.py:185][0m |          -0.0016 |          21.9876 |           0.2285 |
[32m[20221213 15:31:07 @agent_ppo2.py:185][0m |          -0.0095 |          21.5672 |           0.2283 |
[32m[20221213 15:31:07 @agent_ppo2.py:185][0m |          -0.0096 |          21.4802 |           0.2284 |
[32m[20221213 15:31:07 @agent_ppo2.py:185][0m |          -0.0074 |          21.5519 |           0.2282 |
[32m[20221213 15:31:08 @agent_ppo2.py:185][0m |          -0.0075 |          21.7766 |           0.2287 |
[32m[20221213 15:31:08 @agent_ppo2.py:185][0m |          -0.0132 |          21.3106 |           0.2283 |
[32m[20221213 15:31:08 @agent_ppo2.py:185][0m |          -0.0130 |          21.2626 |           0.2283 |
[32m[20221213 15:31:08 @agent_ppo2.py:185][0m |          -0.0150 |          21.2104 |           0.2283 |
[32m[20221213 15:31:08 @agent_ppo2.py:185][0m |          -0.0170 |          21.2026 |           0.2283 |
[32m[20221213 15:31:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:31:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.43
[32m[20221213 15:31:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.96
[32m[20221213 15:31:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.60
[32m[20221213 15:31:08 @agent_ppo2.py:143][0m Total time:      38.22 min
[32m[20221213 15:31:08 @agent_ppo2.py:145][0m 3442688 total steps have happened
[32m[20221213 15:31:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1681 --------------------------#
[32m[20221213 15:31:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:31:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:08 @agent_ppo2.py:185][0m |           0.0031 |          21.5974 |           0.2356 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0068 |          20.7889 |           0.2348 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |           0.0040 |          21.9293 |           0.2344 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0102 |          20.4811 |           0.2343 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0131 |          20.3541 |           0.2344 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0039 |          21.3566 |           0.2343 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0151 |          20.1243 |           0.2337 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0072 |          21.3898 |           0.2339 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0024 |          21.8884 |           0.2337 |
[32m[20221213 15:31:09 @agent_ppo2.py:185][0m |          -0.0163 |          19.9950 |           0.2332 |
[32m[20221213 15:31:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:31:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.56
[32m[20221213 15:31:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.21
[32m[20221213 15:31:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.46
[32m[20221213 15:31:09 @agent_ppo2.py:143][0m Total time:      38.24 min
[32m[20221213 15:31:09 @agent_ppo2.py:145][0m 3444736 total steps have happened
[32m[20221213 15:31:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1682 --------------------------#
[32m[20221213 15:31:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:31:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0014 |          23.0187 |           0.2318 |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0057 |          22.5401 |           0.2317 |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0099 |          22.2863 |           0.2314 |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0104 |          22.1108 |           0.2313 |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0090 |          22.1048 |           0.2312 |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0079 |          23.1368 |           0.2309 |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0138 |          21.9490 |           0.2307 |
[32m[20221213 15:31:10 @agent_ppo2.py:185][0m |          -0.0134 |          21.7606 |           0.2309 |
[32m[20221213 15:31:11 @agent_ppo2.py:185][0m |          -0.0145 |          21.7368 |           0.2307 |
[32m[20221213 15:31:11 @agent_ppo2.py:185][0m |          -0.0157 |          21.6900 |           0.2308 |
[32m[20221213 15:31:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:31:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.25
[32m[20221213 15:31:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.65
[32m[20221213 15:31:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.53
[32m[20221213 15:31:11 @agent_ppo2.py:143][0m Total time:      38.26 min
[32m[20221213 15:31:11 @agent_ppo2.py:145][0m 3446784 total steps have happened
[32m[20221213 15:31:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1683 --------------------------#
[32m[20221213 15:31:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:11 @agent_ppo2.py:185][0m |           0.0013 |          22.1980 |           0.2268 |
[32m[20221213 15:31:11 @agent_ppo2.py:185][0m |          -0.0081 |          20.8630 |           0.2271 |
[32m[20221213 15:31:11 @agent_ppo2.py:185][0m |          -0.0055 |          21.1783 |           0.2271 |
[32m[20221213 15:31:12 @agent_ppo2.py:185][0m |          -0.0104 |          19.9362 |           0.2268 |
[32m[20221213 15:31:12 @agent_ppo2.py:185][0m |          -0.0119 |          19.5483 |           0.2271 |
[32m[20221213 15:31:12 @agent_ppo2.py:185][0m |          -0.0133 |          19.2903 |           0.2268 |
[32m[20221213 15:31:12 @agent_ppo2.py:185][0m |          -0.0072 |          19.1455 |           0.2269 |
[32m[20221213 15:31:12 @agent_ppo2.py:185][0m |          -0.0177 |          18.8533 |           0.2267 |
[32m[20221213 15:31:12 @agent_ppo2.py:185][0m |          -0.0166 |          18.4954 |           0.2267 |
[32m[20221213 15:31:12 @agent_ppo2.py:185][0m |          -0.0160 |          18.2407 |           0.2266 |
[32m[20221213 15:31:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:31:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.26
[32m[20221213 15:31:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.06
[32m[20221213 15:31:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.32
[32m[20221213 15:31:12 @agent_ppo2.py:143][0m Total time:      38.29 min
[32m[20221213 15:31:12 @agent_ppo2.py:145][0m 3448832 total steps have happened
[32m[20221213 15:31:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1684 --------------------------#
[32m[20221213 15:31:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0019 |          23.1798 |           0.2266 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0094 |          21.9014 |           0.2260 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0054 |          22.2661 |           0.2260 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0147 |          21.0953 |           0.2255 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0180 |          20.8262 |           0.2254 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0164 |          20.5577 |           0.2256 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0176 |          20.3591 |           0.2253 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0191 |          20.1676 |           0.2252 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0181 |          20.2435 |           0.2249 |
[32m[20221213 15:31:13 @agent_ppo2.py:185][0m |          -0.0206 |          19.8916 |           0.2250 |
[32m[20221213 15:31:13 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:31:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.64
[32m[20221213 15:31:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.41
[32m[20221213 15:31:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.27
[32m[20221213 15:31:14 @agent_ppo2.py:143][0m Total time:      38.31 min
[32m[20221213 15:31:14 @agent_ppo2.py:145][0m 3450880 total steps have happened
[32m[20221213 15:31:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1685 --------------------------#
[32m[20221213 15:31:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:14 @agent_ppo2.py:185][0m |           0.0104 |          23.8217 |           0.2226 |
[32m[20221213 15:31:14 @agent_ppo2.py:185][0m |          -0.0065 |          22.5077 |           0.2229 |
[32m[20221213 15:31:14 @agent_ppo2.py:185][0m |          -0.0100 |          22.3575 |           0.2226 |
[32m[20221213 15:31:14 @agent_ppo2.py:185][0m |          -0.0117 |          22.2129 |           0.2226 |
[32m[20221213 15:31:14 @agent_ppo2.py:185][0m |          -0.0134 |          22.1550 |           0.2224 |
[32m[20221213 15:31:14 @agent_ppo2.py:185][0m |          -0.0119 |          22.0978 |           0.2223 |
[32m[20221213 15:31:15 @agent_ppo2.py:185][0m |          -0.0067 |          23.5038 |           0.2224 |
[32m[20221213 15:31:15 @agent_ppo2.py:185][0m |          -0.0138 |          22.0129 |           0.2218 |
[32m[20221213 15:31:15 @agent_ppo2.py:185][0m |          -0.0153 |          21.9916 |           0.2220 |
[32m[20221213 15:31:15 @agent_ppo2.py:185][0m |          -0.0172 |          21.9506 |           0.2223 |
[32m[20221213 15:31:15 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:31:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.07
[32m[20221213 15:31:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.79
[32m[20221213 15:31:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.82
[32m[20221213 15:31:15 @agent_ppo2.py:143][0m Total time:      38.33 min
[32m[20221213 15:31:15 @agent_ppo2.py:145][0m 3452928 total steps have happened
[32m[20221213 15:31:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1686 --------------------------#
[32m[20221213 15:31:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:15 @agent_ppo2.py:185][0m |           0.0004 |          22.0517 |           0.2275 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0077 |          21.4808 |           0.2270 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0112 |          20.9399 |           0.2272 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0069 |          21.1236 |           0.2268 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0150 |          20.4235 |           0.2268 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0087 |          20.8221 |           0.2266 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0200 |          20.0964 |           0.2263 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0124 |          20.3687 |           0.2262 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0197 |          19.8304 |           0.2262 |
[32m[20221213 15:31:16 @agent_ppo2.py:185][0m |          -0.0201 |          19.7055 |           0.2259 |
[32m[20221213 15:31:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:31:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.98
[32m[20221213 15:31:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.85
[32m[20221213 15:31:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.46
[32m[20221213 15:31:16 @agent_ppo2.py:143][0m Total time:      38.35 min
[32m[20221213 15:31:16 @agent_ppo2.py:145][0m 3454976 total steps have happened
[32m[20221213 15:31:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1687 --------------------------#
[32m[20221213 15:31:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |           0.0022 |          22.6952 |           0.2252 |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |          -0.0046 |          22.1633 |           0.2250 |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |          -0.0116 |          21.7854 |           0.2248 |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |          -0.0125 |          21.5907 |           0.2246 |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |          -0.0148 |          21.3570 |           0.2243 |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |          -0.0171 |          21.1912 |           0.2243 |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |          -0.0093 |          21.4764 |           0.2242 |
[32m[20221213 15:31:17 @agent_ppo2.py:185][0m |          -0.0084 |          22.3799 |           0.2241 |
[32m[20221213 15:31:18 @agent_ppo2.py:185][0m |          -0.0203 |          20.8926 |           0.2239 |
[32m[20221213 15:31:18 @agent_ppo2.py:185][0m |          -0.0202 |          20.7489 |           0.2240 |
[32m[20221213 15:31:18 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:31:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.46
[32m[20221213 15:31:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.34
[32m[20221213 15:31:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.48
[32m[20221213 15:31:18 @agent_ppo2.py:143][0m Total time:      38.38 min
[32m[20221213 15:31:18 @agent_ppo2.py:145][0m 3457024 total steps have happened
[32m[20221213 15:31:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1688 --------------------------#
[32m[20221213 15:31:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:31:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:18 @agent_ppo2.py:185][0m |           0.0086 |          23.3357 |           0.2231 |
[32m[20221213 15:31:18 @agent_ppo2.py:185][0m |          -0.0027 |          21.5228 |           0.2227 |
[32m[20221213 15:31:18 @agent_ppo2.py:185][0m |          -0.0062 |          21.4005 |           0.2226 |
[32m[20221213 15:31:18 @agent_ppo2.py:185][0m |          -0.0113 |          20.7893 |           0.2225 |
[32m[20221213 15:31:19 @agent_ppo2.py:185][0m |          -0.0173 |          20.5828 |           0.2224 |
[32m[20221213 15:31:19 @agent_ppo2.py:185][0m |          -0.0125 |          20.3904 |           0.2223 |
[32m[20221213 15:31:19 @agent_ppo2.py:185][0m |          -0.0170 |          20.1651 |           0.2225 |
[32m[20221213 15:31:19 @agent_ppo2.py:185][0m |          -0.0108 |          20.3793 |           0.2224 |
[32m[20221213 15:31:19 @agent_ppo2.py:185][0m |          -0.0166 |          19.9462 |           0.2223 |
[32m[20221213 15:31:19 @agent_ppo2.py:185][0m |          -0.0189 |          19.8021 |           0.2226 |
[32m[20221213 15:31:19 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:31:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.33
[32m[20221213 15:31:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.06
[32m[20221213 15:31:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.92
[32m[20221213 15:31:19 @agent_ppo2.py:143][0m Total time:      38.40 min
[32m[20221213 15:31:19 @agent_ppo2.py:145][0m 3459072 total steps have happened
[32m[20221213 15:31:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1689 --------------------------#
[32m[20221213 15:31:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |           0.0153 |          25.7363 |           0.2258 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |          -0.0035 |          22.8030 |           0.2255 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |          -0.0110 |          22.5692 |           0.2255 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |           0.0027 |          24.7635 |           0.2254 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |          -0.0126 |          22.4122 |           0.2253 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |          -0.0159 |          22.2303 |           0.2254 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |          -0.0144 |          22.1948 |           0.2254 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |          -0.0051 |          23.5989 |           0.2255 |
[32m[20221213 15:31:20 @agent_ppo2.py:185][0m |          -0.0069 |          23.2561 |           0.2252 |
[32m[20221213 15:31:21 @agent_ppo2.py:185][0m |          -0.0160 |          21.9902 |           0.2246 |
[32m[20221213 15:31:21 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 15:31:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.94
[32m[20221213 15:31:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.35
[32m[20221213 15:31:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.18
[32m[20221213 15:31:21 @agent_ppo2.py:143][0m Total time:      38.43 min
[32m[20221213 15:31:21 @agent_ppo2.py:145][0m 3461120 total steps have happened
[32m[20221213 15:31:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1690 --------------------------#
[32m[20221213 15:31:21 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:31:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:21 @agent_ppo2.py:185][0m |          -0.0013 |          23.1599 |           0.2236 |
[32m[20221213 15:31:21 @agent_ppo2.py:185][0m |          -0.0105 |          22.6108 |           0.2231 |
[32m[20221213 15:31:21 @agent_ppo2.py:185][0m |          -0.0111 |          22.4188 |           0.2227 |
[32m[20221213 15:31:21 @agent_ppo2.py:185][0m |          -0.0005 |          24.2332 |           0.2225 |
[32m[20221213 15:31:22 @agent_ppo2.py:185][0m |          -0.0146 |          22.2257 |           0.2221 |
[32m[20221213 15:31:22 @agent_ppo2.py:185][0m |          -0.0147 |          22.1202 |           0.2221 |
[32m[20221213 15:31:22 @agent_ppo2.py:185][0m |          -0.0173 |          22.0021 |           0.2222 |
[32m[20221213 15:31:22 @agent_ppo2.py:185][0m |          -0.0182 |          22.0142 |           0.2220 |
[32m[20221213 15:31:22 @agent_ppo2.py:185][0m |          -0.0181 |          21.8841 |           0.2220 |
[32m[20221213 15:31:22 @agent_ppo2.py:185][0m |          -0.0198 |          21.8619 |           0.2218 |
[32m[20221213 15:31:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:31:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.49
[32m[20221213 15:31:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.30
[32m[20221213 15:31:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.54
[32m[20221213 15:31:22 @agent_ppo2.py:143][0m Total time:      38.45 min
[32m[20221213 15:31:22 @agent_ppo2.py:145][0m 3463168 total steps have happened
[32m[20221213 15:31:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1691 --------------------------#
[32m[20221213 15:31:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0025 |          22.2243 |           0.2290 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0103 |          21.9296 |           0.2283 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0125 |          21.7802 |           0.2282 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0091 |          21.7883 |           0.2282 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0144 |          21.5743 |           0.2278 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0158 |          21.5395 |           0.2279 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0176 |          21.4536 |           0.2281 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0188 |          21.4049 |           0.2280 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0151 |          21.4267 |           0.2278 |
[32m[20221213 15:31:23 @agent_ppo2.py:185][0m |          -0.0180 |          21.3710 |           0.2279 |
[32m[20221213 15:31:23 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:31:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.63
[32m[20221213 15:31:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.06
[32m[20221213 15:31:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.85
[32m[20221213 15:31:24 @agent_ppo2.py:143][0m Total time:      38.47 min
[32m[20221213 15:31:24 @agent_ppo2.py:145][0m 3465216 total steps have happened
[32m[20221213 15:31:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1692 --------------------------#
[32m[20221213 15:31:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:24 @agent_ppo2.py:185][0m |           0.0031 |          23.1143 |           0.2228 |
[32m[20221213 15:31:24 @agent_ppo2.py:185][0m |           0.0062 |          24.2938 |           0.2226 |
[32m[20221213 15:31:24 @agent_ppo2.py:185][0m |          -0.0084 |          22.6876 |           0.2225 |
[32m[20221213 15:31:24 @agent_ppo2.py:185][0m |          -0.0075 |          22.7554 |           0.2225 |
[32m[20221213 15:31:24 @agent_ppo2.py:185][0m |          -0.0107 |          22.4641 |           0.2222 |
[32m[20221213 15:31:24 @agent_ppo2.py:185][0m |          -0.0153 |          22.4568 |           0.2221 |
[32m[20221213 15:31:24 @agent_ppo2.py:185][0m |          -0.0171 |          22.3260 |           0.2220 |
[32m[20221213 15:31:25 @agent_ppo2.py:185][0m |          -0.0161 |          22.2419 |           0.2217 |
[32m[20221213 15:31:25 @agent_ppo2.py:185][0m |          -0.0167 |          22.3167 |           0.2218 |
[32m[20221213 15:31:25 @agent_ppo2.py:185][0m |          -0.0197 |          22.1805 |           0.2215 |
[32m[20221213 15:31:25 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:31:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.94
[32m[20221213 15:31:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.21
[32m[20221213 15:31:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.50
[32m[20221213 15:31:25 @agent_ppo2.py:143][0m Total time:      38.50 min
[32m[20221213 15:31:25 @agent_ppo2.py:145][0m 3467264 total steps have happened
[32m[20221213 15:31:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1693 --------------------------#
[32m[20221213 15:31:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:25 @agent_ppo2.py:185][0m |           0.0146 |          25.4200 |           0.2251 |
[32m[20221213 15:31:25 @agent_ppo2.py:185][0m |          -0.0054 |          22.2185 |           0.2249 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0104 |          21.9382 |           0.2247 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0082 |          22.0190 |           0.2248 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0139 |          21.6516 |           0.2247 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0157 |          21.5318 |           0.2248 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0131 |          21.4539 |           0.2246 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0080 |          22.3194 |           0.2247 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0159 |          21.2944 |           0.2245 |
[32m[20221213 15:31:26 @agent_ppo2.py:185][0m |          -0.0174 |          21.1873 |           0.2248 |
[32m[20221213 15:31:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 251.69
[32m[20221213 15:31:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.51
[32m[20221213 15:31:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.70
[32m[20221213 15:31:26 @agent_ppo2.py:143][0m Total time:      38.52 min
[32m[20221213 15:31:26 @agent_ppo2.py:145][0m 3469312 total steps have happened
[32m[20221213 15:31:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1694 --------------------------#
[32m[20221213 15:31:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |           0.0092 |          24.0885 |           0.2162 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0069 |          21.0902 |           0.2156 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0097 |          20.6131 |           0.2157 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0122 |          20.2762 |           0.2158 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0157 |          19.9508 |           0.2157 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0094 |          20.4422 |           0.2159 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0158 |          19.4677 |           0.2158 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0191 |          19.2658 |           0.2159 |
[32m[20221213 15:31:27 @agent_ppo2.py:185][0m |          -0.0169 |          19.4096 |           0.2163 |
[32m[20221213 15:31:28 @agent_ppo2.py:185][0m |          -0.0191 |          18.8873 |           0.2161 |
[32m[20221213 15:31:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.45
[32m[20221213 15:31:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.66
[32m[20221213 15:31:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.46
[32m[20221213 15:31:28 @agent_ppo2.py:143][0m Total time:      38.54 min
[32m[20221213 15:31:28 @agent_ppo2.py:145][0m 3471360 total steps have happened
[32m[20221213 15:31:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1695 --------------------------#
[32m[20221213 15:31:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:31:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:28 @agent_ppo2.py:185][0m |           0.0021 |          23.2522 |           0.2178 |
[32m[20221213 15:31:28 @agent_ppo2.py:185][0m |          -0.0090 |          22.2038 |           0.2176 |
[32m[20221213 15:31:28 @agent_ppo2.py:185][0m |          -0.0123 |          21.9269 |           0.2178 |
[32m[20221213 15:31:28 @agent_ppo2.py:185][0m |          -0.0114 |          21.6742 |           0.2178 |
[32m[20221213 15:31:28 @agent_ppo2.py:185][0m |          -0.0144 |          21.5102 |           0.2178 |
[32m[20221213 15:31:29 @agent_ppo2.py:185][0m |          -0.0169 |          21.3856 |           0.2176 |
[32m[20221213 15:31:29 @agent_ppo2.py:185][0m |          -0.0149 |          21.2802 |           0.2176 |
[32m[20221213 15:31:29 @agent_ppo2.py:185][0m |          -0.0161 |          21.1802 |           0.2175 |
[32m[20221213 15:31:29 @agent_ppo2.py:185][0m |          -0.0085 |          22.5579 |           0.2176 |
[32m[20221213 15:31:29 @agent_ppo2.py:185][0m |          -0.0211 |          21.1348 |           0.2176 |
[32m[20221213 15:31:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:31:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.49
[32m[20221213 15:31:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.77
[32m[20221213 15:31:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.26
[32m[20221213 15:31:29 @agent_ppo2.py:143][0m Total time:      38.57 min
[32m[20221213 15:31:29 @agent_ppo2.py:145][0m 3473408 total steps have happened
[32m[20221213 15:31:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1696 --------------------------#
[32m[20221213 15:31:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:29 @agent_ppo2.py:185][0m |          -0.0017 |          22.6606 |           0.2282 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0093 |          22.1006 |           0.2278 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |           0.0014 |          23.8590 |           0.2279 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0135 |          21.5683 |           0.2273 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0153 |          21.3368 |           0.2276 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0142 |          21.2101 |           0.2276 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0124 |          21.6492 |           0.2275 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0193 |          20.8929 |           0.2274 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0200 |          20.8398 |           0.2275 |
[32m[20221213 15:31:30 @agent_ppo2.py:185][0m |          -0.0180 |          20.7102 |           0.2276 |
[32m[20221213 15:31:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:31:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.59
[32m[20221213 15:31:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.16
[32m[20221213 15:31:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.61
[32m[20221213 15:31:31 @agent_ppo2.py:143][0m Total time:      38.59 min
[32m[20221213 15:31:31 @agent_ppo2.py:145][0m 3475456 total steps have happened
[32m[20221213 15:31:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1697 --------------------------#
[32m[20221213 15:31:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:31 @agent_ppo2.py:185][0m |          -0.0028 |          22.9999 |           0.2229 |
[32m[20221213 15:31:31 @agent_ppo2.py:185][0m |          -0.0100 |          22.5039 |           0.2228 |
[32m[20221213 15:31:31 @agent_ppo2.py:185][0m |          -0.0071 |          22.6247 |           0.2228 |
[32m[20221213 15:31:31 @agent_ppo2.py:185][0m |          -0.0103 |          22.1349 |           0.2227 |
[32m[20221213 15:31:31 @agent_ppo2.py:185][0m |          -0.0141 |          21.9695 |           0.2228 |
[32m[20221213 15:31:31 @agent_ppo2.py:185][0m |          -0.0177 |          21.8848 |           0.2227 |
[32m[20221213 15:31:31 @agent_ppo2.py:185][0m |          -0.0179 |          21.8415 |           0.2224 |
[32m[20221213 15:31:32 @agent_ppo2.py:185][0m |          -0.0183 |          21.7125 |           0.2226 |
[32m[20221213 15:31:32 @agent_ppo2.py:185][0m |          -0.0162 |          21.6486 |           0.2226 |
[32m[20221213 15:31:32 @agent_ppo2.py:185][0m |          -0.0145 |          21.6107 |           0.2225 |
[32m[20221213 15:31:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.16
[32m[20221213 15:31:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.24
[32m[20221213 15:31:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.06
[32m[20221213 15:31:32 @agent_ppo2.py:143][0m Total time:      38.61 min
[32m[20221213 15:31:32 @agent_ppo2.py:145][0m 3477504 total steps have happened
[32m[20221213 15:31:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1698 --------------------------#
[32m[20221213 15:31:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:31:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:32 @agent_ppo2.py:185][0m |          -0.0030 |          22.3687 |           0.2216 |
[32m[20221213 15:31:32 @agent_ppo2.py:185][0m |          -0.0080 |          21.9511 |           0.2212 |
[32m[20221213 15:31:32 @agent_ppo2.py:185][0m |           0.0014 |          24.8718 |           0.2211 |
[32m[20221213 15:31:33 @agent_ppo2.py:185][0m |          -0.0114 |          21.6313 |           0.2209 |
[32m[20221213 15:31:33 @agent_ppo2.py:185][0m |          -0.0137 |          21.4765 |           0.2210 |
[32m[20221213 15:31:33 @agent_ppo2.py:185][0m |          -0.0112 |          21.5580 |           0.2212 |
[32m[20221213 15:31:33 @agent_ppo2.py:185][0m |          -0.0159 |          21.3470 |           0.2209 |
[32m[20221213 15:31:33 @agent_ppo2.py:185][0m |          -0.0149 |          21.2631 |           0.2209 |
[32m[20221213 15:31:33 @agent_ppo2.py:185][0m |          -0.0168 |          21.2035 |           0.2209 |
[32m[20221213 15:31:33 @agent_ppo2.py:185][0m |          -0.0144 |          21.2747 |           0.2207 |
[32m[20221213 15:31:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.06
[32m[20221213 15:31:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.92
[32m[20221213 15:31:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.52
[32m[20221213 15:31:33 @agent_ppo2.py:143][0m Total time:      38.63 min
[32m[20221213 15:31:33 @agent_ppo2.py:145][0m 3479552 total steps have happened
[32m[20221213 15:31:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1699 --------------------------#
[32m[20221213 15:31:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |           0.0004 |          19.8066 |           0.2271 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0080 |          19.4011 |           0.2271 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0117 |          19.1897 |           0.2266 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0032 |          20.7326 |           0.2267 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0144 |          18.9827 |           0.2265 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0168 |          18.7638 |           0.2263 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0173 |          18.6750 |           0.2261 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0154 |          18.5480 |           0.2260 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0099 |          18.7061 |           0.2260 |
[32m[20221213 15:31:34 @agent_ppo2.py:185][0m |          -0.0167 |          18.3781 |           0.2257 |
[32m[20221213 15:31:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:31:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.46
[32m[20221213 15:31:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.25
[32m[20221213 15:31:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.20
[32m[20221213 15:31:35 @agent_ppo2.py:143][0m Total time:      38.66 min
[32m[20221213 15:31:35 @agent_ppo2.py:145][0m 3481600 total steps have happened
[32m[20221213 15:31:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1700 --------------------------#
[32m[20221213 15:31:35 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:31:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:35 @agent_ppo2.py:185][0m |          -0.0033 |          21.1756 |           0.2235 |
[32m[20221213 15:31:35 @agent_ppo2.py:185][0m |          -0.0096 |          20.8212 |           0.2234 |
[32m[20221213 15:31:35 @agent_ppo2.py:185][0m |          -0.0007 |          21.9019 |           0.2233 |
[32m[20221213 15:31:35 @agent_ppo2.py:185][0m |          -0.0110 |          20.5182 |           0.2234 |
[32m[20221213 15:31:35 @agent_ppo2.py:185][0m |          -0.0160 |          20.4323 |           0.2235 |
[32m[20221213 15:31:35 @agent_ppo2.py:185][0m |          -0.0171 |          20.3484 |           0.2236 |
[32m[20221213 15:31:36 @agent_ppo2.py:185][0m |          -0.0172 |          20.2311 |           0.2235 |
[32m[20221213 15:31:36 @agent_ppo2.py:185][0m |          -0.0181 |          20.2324 |           0.2234 |
[32m[20221213 15:31:36 @agent_ppo2.py:185][0m |          -0.0192 |          20.1645 |           0.2236 |
[32m[20221213 15:31:36 @agent_ppo2.py:185][0m |          -0.0201 |          20.0865 |           0.2236 |
[32m[20221213 15:31:36 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:31:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.64
[32m[20221213 15:31:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.07
[32m[20221213 15:31:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.80
[32m[20221213 15:31:36 @agent_ppo2.py:143][0m Total time:      38.68 min
[32m[20221213 15:31:36 @agent_ppo2.py:145][0m 3483648 total steps have happened
[32m[20221213 15:31:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1701 --------------------------#
[32m[20221213 15:31:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:36 @agent_ppo2.py:185][0m |          -0.0026 |          21.9047 |           0.2237 |
[32m[20221213 15:31:36 @agent_ppo2.py:185][0m |          -0.0062 |          21.7082 |           0.2235 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |           0.0046 |          23.3134 |           0.2236 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |          -0.0063 |          22.1425 |           0.2236 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |          -0.0139 |          21.1641 |           0.2235 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |          -0.0138 |          20.9769 |           0.2236 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |          -0.0161 |          20.8493 |           0.2237 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |          -0.0185 |          20.7341 |           0.2237 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |          -0.0160 |          20.7862 |           0.2237 |
[32m[20221213 15:31:37 @agent_ppo2.py:185][0m |          -0.0189 |          20.5628 |           0.2237 |
[32m[20221213 15:31:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:31:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.97
[32m[20221213 15:31:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.25
[32m[20221213 15:31:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.91
[32m[20221213 15:31:37 @agent_ppo2.py:143][0m Total time:      38.70 min
[32m[20221213 15:31:37 @agent_ppo2.py:145][0m 3485696 total steps have happened
[32m[20221213 15:31:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1702 --------------------------#
[32m[20221213 15:31:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0021 |          17.1119 |           0.2264 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0100 |          16.1042 |           0.2256 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0134 |          15.6773 |           0.2251 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0136 |          15.4592 |           0.2248 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0166 |          15.3009 |           0.2247 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0101 |          16.1274 |           0.2243 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0152 |          15.0078 |           0.2239 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0168 |          14.9524 |           0.2240 |
[32m[20221213 15:31:38 @agent_ppo2.py:185][0m |          -0.0207 |          14.8187 |           0.2238 |
[32m[20221213 15:31:39 @agent_ppo2.py:185][0m |          -0.0199 |          14.7763 |           0.2239 |
[32m[20221213 15:31:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:31:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.56
[32m[20221213 15:31:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.87
[32m[20221213 15:31:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.18
[32m[20221213 15:31:39 @agent_ppo2.py:143][0m Total time:      38.73 min
[32m[20221213 15:31:39 @agent_ppo2.py:145][0m 3487744 total steps have happened
[32m[20221213 15:31:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1703 --------------------------#
[32m[20221213 15:31:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:39 @agent_ppo2.py:185][0m |           0.0029 |          22.6525 |           0.2236 |
[32m[20221213 15:31:39 @agent_ppo2.py:185][0m |          -0.0070 |          22.0021 |           0.2233 |
[32m[20221213 15:31:39 @agent_ppo2.py:185][0m |          -0.0095 |          21.8176 |           0.2229 |
[32m[20221213 15:31:39 @agent_ppo2.py:185][0m |          -0.0068 |          21.7157 |           0.2226 |
[32m[20221213 15:31:39 @agent_ppo2.py:185][0m |          -0.0110 |          21.4564 |           0.2225 |
[32m[20221213 15:31:40 @agent_ppo2.py:185][0m |          -0.0106 |          21.3717 |           0.2224 |
[32m[20221213 15:31:40 @agent_ppo2.py:185][0m |          -0.0127 |          21.2675 |           0.2222 |
[32m[20221213 15:31:40 @agent_ppo2.py:185][0m |          -0.0163 |          21.0751 |           0.2224 |
[32m[20221213 15:31:40 @agent_ppo2.py:185][0m |          -0.0114 |          21.1559 |           0.2225 |
[32m[20221213 15:31:40 @agent_ppo2.py:185][0m |          -0.0078 |          21.9519 |           0.2223 |
[32m[20221213 15:31:40 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:31:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.52
[32m[20221213 15:31:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.21
[32m[20221213 15:31:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.20
[32m[20221213 15:31:40 @agent_ppo2.py:143][0m Total time:      38.75 min
[32m[20221213 15:31:40 @agent_ppo2.py:145][0m 3489792 total steps have happened
[32m[20221213 15:31:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1704 --------------------------#
[32m[20221213 15:31:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:40 @agent_ppo2.py:185][0m |           0.0036 |          22.4817 |           0.2236 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0022 |          21.7599 |           0.2235 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0100 |          21.3399 |           0.2235 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0101 |          21.1387 |           0.2238 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0128 |          20.9639 |           0.2240 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0144 |          20.9121 |           0.2241 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0158 |          20.7073 |           0.2240 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0162 |          20.6039 |           0.2239 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0175 |          20.4796 |           0.2240 |
[32m[20221213 15:31:41 @agent_ppo2.py:185][0m |          -0.0174 |          20.3640 |           0.2241 |
[32m[20221213 15:31:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.09
[32m[20221213 15:31:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.70
[32m[20221213 15:31:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.69
[32m[20221213 15:31:41 @agent_ppo2.py:143][0m Total time:      38.77 min
[32m[20221213 15:31:41 @agent_ppo2.py:145][0m 3491840 total steps have happened
[32m[20221213 15:31:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1705 --------------------------#
[32m[20221213 15:31:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |           0.0036 |          22.7311 |           0.2217 |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |          -0.0096 |          22.0021 |           0.2211 |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |          -0.0123 |          21.9683 |           0.2213 |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |          -0.0154 |          21.6078 |           0.2215 |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |          -0.0196 |          21.5258 |           0.2212 |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |          -0.0178 |          21.2985 |           0.2215 |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |          -0.0206 |          21.2373 |           0.2214 |
[32m[20221213 15:31:42 @agent_ppo2.py:185][0m |          -0.0210 |          21.0886 |           0.2215 |
[32m[20221213 15:31:43 @agent_ppo2.py:185][0m |          -0.0218 |          20.9933 |           0.2217 |
[32m[20221213 15:31:43 @agent_ppo2.py:185][0m |          -0.0213 |          20.8794 |           0.2215 |
[32m[20221213 15:31:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.89
[32m[20221213 15:31:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.03
[32m[20221213 15:31:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.23
[32m[20221213 15:31:43 @agent_ppo2.py:143][0m Total time:      38.79 min
[32m[20221213 15:31:43 @agent_ppo2.py:145][0m 3493888 total steps have happened
[32m[20221213 15:31:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1706 --------------------------#
[32m[20221213 15:31:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:43 @agent_ppo2.py:185][0m |          -0.0011 |          21.2238 |           0.2298 |
[32m[20221213 15:31:43 @agent_ppo2.py:185][0m |          -0.0086 |          20.5842 |           0.2295 |
[32m[20221213 15:31:43 @agent_ppo2.py:185][0m |          -0.0006 |          22.9164 |           0.2294 |
[32m[20221213 15:31:43 @agent_ppo2.py:185][0m |          -0.0044 |          20.1604 |           0.2290 |
[32m[20221213 15:31:44 @agent_ppo2.py:185][0m |          -0.0143 |          19.5531 |           0.2293 |
[32m[20221213 15:31:44 @agent_ppo2.py:185][0m |          -0.0188 |          19.3030 |           0.2291 |
[32m[20221213 15:31:44 @agent_ppo2.py:185][0m |          -0.0158 |          19.0922 |           0.2291 |
[32m[20221213 15:31:44 @agent_ppo2.py:185][0m |          -0.0117 |          19.5084 |           0.2290 |
[32m[20221213 15:31:44 @agent_ppo2.py:185][0m |          -0.0207 |          18.6455 |           0.2290 |
[32m[20221213 15:31:44 @agent_ppo2.py:185][0m |          -0.0213 |          18.4138 |           0.2289 |
[32m[20221213 15:31:44 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:31:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.43
[32m[20221213 15:31:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 256.24
[32m[20221213 15:31:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.30
[32m[20221213 15:31:44 @agent_ppo2.py:143][0m Total time:      38.82 min
[32m[20221213 15:31:44 @agent_ppo2.py:145][0m 3495936 total steps have happened
[32m[20221213 15:31:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1707 --------------------------#
[32m[20221213 15:31:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:31:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |           0.0059 |          22.9819 |           0.2304 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0044 |          22.0040 |           0.2306 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0123 |          21.5604 |           0.2305 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0135 |          21.3954 |           0.2307 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0135 |          21.2458 |           0.2305 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0160 |          21.1133 |           0.2306 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0156 |          21.1007 |           0.2306 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0180 |          20.9222 |           0.2306 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0098 |          21.9842 |           0.2307 |
[32m[20221213 15:31:45 @agent_ppo2.py:185][0m |          -0.0047 |          22.7920 |           0.2307 |
[32m[20221213 15:31:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.92
[32m[20221213 15:31:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 263.76
[32m[20221213 15:31:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.90
[32m[20221213 15:31:46 @agent_ppo2.py:143][0m Total time:      38.84 min
[32m[20221213 15:31:46 @agent_ppo2.py:145][0m 3497984 total steps have happened
[32m[20221213 15:31:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1708 --------------------------#
[32m[20221213 15:31:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:46 @agent_ppo2.py:185][0m |           0.0067 |          21.9692 |           0.2304 |
[32m[20221213 15:31:46 @agent_ppo2.py:185][0m |           0.0015 |          21.0899 |           0.2301 |
[32m[20221213 15:31:46 @agent_ppo2.py:185][0m |          -0.0076 |          19.8283 |           0.2300 |
[32m[20221213 15:31:46 @agent_ppo2.py:185][0m |          -0.0094 |          19.4632 |           0.2299 |
[32m[20221213 15:31:46 @agent_ppo2.py:185][0m |          -0.0087 |          19.2565 |           0.2302 |
[32m[20221213 15:31:46 @agent_ppo2.py:185][0m |          -0.0106 |          19.0062 |           0.2298 |
[32m[20221213 15:31:46 @agent_ppo2.py:185][0m |          -0.0130 |          18.8503 |           0.2300 |
[32m[20221213 15:31:47 @agent_ppo2.py:185][0m |          -0.0107 |          18.7346 |           0.2302 |
[32m[20221213 15:31:47 @agent_ppo2.py:185][0m |          -0.0141 |          18.6256 |           0.2300 |
[32m[20221213 15:31:47 @agent_ppo2.py:185][0m |          -0.0144 |          18.5257 |           0.2302 |
[32m[20221213 15:31:47 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:31:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.39
[32m[20221213 15:31:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.30
[32m[20221213 15:31:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.04
[32m[20221213 15:31:47 @agent_ppo2.py:143][0m Total time:      38.86 min
[32m[20221213 15:31:47 @agent_ppo2.py:145][0m 3500032 total steps have happened
[32m[20221213 15:31:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1709 --------------------------#
[32m[20221213 15:31:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:47 @agent_ppo2.py:185][0m |          -0.0020 |          21.7723 |           0.2289 |
[32m[20221213 15:31:47 @agent_ppo2.py:185][0m |          -0.0069 |          20.9801 |           0.2285 |
[32m[20221213 15:31:47 @agent_ppo2.py:185][0m |          -0.0039 |          20.9204 |           0.2284 |
[32m[20221213 15:31:48 @agent_ppo2.py:185][0m |          -0.0009 |          21.0754 |           0.2285 |
[32m[20221213 15:31:48 @agent_ppo2.py:185][0m |          -0.0143 |          19.9497 |           0.2286 |
[32m[20221213 15:31:48 @agent_ppo2.py:185][0m |          -0.0160 |          19.7215 |           0.2283 |
[32m[20221213 15:31:48 @agent_ppo2.py:185][0m |          -0.0177 |          19.5705 |           0.2281 |
[32m[20221213 15:31:48 @agent_ppo2.py:185][0m |          -0.0177 |          19.4646 |           0.2282 |
[32m[20221213 15:31:48 @agent_ppo2.py:185][0m |          -0.0174 |          19.2494 |           0.2284 |
[32m[20221213 15:31:48 @agent_ppo2.py:185][0m |          -0.0206 |          19.2186 |           0.2284 |
[32m[20221213 15:31:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.46
[32m[20221213 15:31:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.38
[32m[20221213 15:31:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.33
[32m[20221213 15:31:48 @agent_ppo2.py:143][0m Total time:      38.88 min
[32m[20221213 15:31:48 @agent_ppo2.py:145][0m 3502080 total steps have happened
[32m[20221213 15:31:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1710 --------------------------#
[32m[20221213 15:31:48 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:31:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |           0.0003 |          22.5910 |           0.2336 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0082 |          21.7620 |           0.2339 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0100 |          21.4515 |           0.2339 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0123 |          21.2702 |           0.2339 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0134 |          21.1519 |           0.2341 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0143 |          21.0601 |           0.2343 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0087 |          22.4024 |           0.2341 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0135 |          21.0848 |           0.2343 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0177 |          20.9134 |           0.2343 |
[32m[20221213 15:31:49 @agent_ppo2.py:185][0m |          -0.0166 |          20.8720 |           0.2344 |
[32m[20221213 15:31:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:31:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.95
[32m[20221213 15:31:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.43
[32m[20221213 15:31:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.44
[32m[20221213 15:31:50 @agent_ppo2.py:143][0m Total time:      38.91 min
[32m[20221213 15:31:50 @agent_ppo2.py:145][0m 3504128 total steps have happened
[32m[20221213 15:31:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1711 --------------------------#
[32m[20221213 15:31:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:50 @agent_ppo2.py:185][0m |          -0.0006 |          21.1186 |           0.2338 |
[32m[20221213 15:31:50 @agent_ppo2.py:185][0m |          -0.0063 |          20.9503 |           0.2332 |
[32m[20221213 15:31:50 @agent_ppo2.py:185][0m |          -0.0097 |          20.8257 |           0.2332 |
[32m[20221213 15:31:50 @agent_ppo2.py:185][0m |          -0.0060 |          21.0473 |           0.2328 |
[32m[20221213 15:31:50 @agent_ppo2.py:185][0m |          -0.0122 |          20.7411 |           0.2326 |
[32m[20221213 15:31:50 @agent_ppo2.py:185][0m |          -0.0115 |          20.6716 |           0.2328 |
[32m[20221213 15:31:51 @agent_ppo2.py:185][0m |          -0.0112 |          20.6750 |           0.2321 |
[32m[20221213 15:31:51 @agent_ppo2.py:185][0m |          -0.0135 |          20.6034 |           0.2324 |
[32m[20221213 15:31:51 @agent_ppo2.py:185][0m |          -0.0125 |          20.6215 |           0.2326 |
[32m[20221213 15:31:51 @agent_ppo2.py:185][0m |          -0.0100 |          20.7920 |           0.2321 |
[32m[20221213 15:31:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:31:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.30
[32m[20221213 15:31:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.04
[32m[20221213 15:31:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.14
[32m[20221213 15:31:51 @agent_ppo2.py:143][0m Total time:      38.93 min
[32m[20221213 15:31:51 @agent_ppo2.py:145][0m 3506176 total steps have happened
[32m[20221213 15:31:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1712 --------------------------#
[32m[20221213 15:31:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:51 @agent_ppo2.py:185][0m |           0.0007 |          21.5061 |           0.2323 |
[32m[20221213 15:31:51 @agent_ppo2.py:185][0m |          -0.0032 |          21.1076 |           0.2322 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0088 |          20.9195 |           0.2321 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0127 |          20.7632 |           0.2316 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0149 |          20.6801 |           0.2316 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0157 |          20.6178 |           0.2315 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0128 |          20.8502 |           0.2312 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0150 |          20.4648 |           0.2311 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0167 |          20.3849 |           0.2307 |
[32m[20221213 15:31:52 @agent_ppo2.py:185][0m |          -0.0171 |          20.3484 |           0.2307 |
[32m[20221213 15:31:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:31:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.34
[32m[20221213 15:31:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.40
[32m[20221213 15:31:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.25
[32m[20221213 15:31:52 @agent_ppo2.py:143][0m Total time:      38.95 min
[32m[20221213 15:31:52 @agent_ppo2.py:145][0m 3508224 total steps have happened
[32m[20221213 15:31:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1713 --------------------------#
[32m[20221213 15:31:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |           0.0006 |          21.4530 |           0.2267 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |          -0.0054 |          21.2230 |           0.2263 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |          -0.0054 |          21.5246 |           0.2263 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |           0.0079 |          23.7141 |           0.2256 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |          -0.0097 |          21.0354 |           0.2254 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |          -0.0106 |          20.8858 |           0.2255 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |          -0.0114 |          20.8537 |           0.2255 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |          -0.0124 |          20.8189 |           0.2254 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |          -0.0150 |          20.7950 |           0.2257 |
[32m[20221213 15:31:53 @agent_ppo2.py:185][0m |           0.0093 |          24.5825 |           0.2254 |
[32m[20221213 15:31:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.78
[32m[20221213 15:31:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.81
[32m[20221213 15:31:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.95
[32m[20221213 15:31:54 @agent_ppo2.py:143][0m Total time:      38.98 min
[32m[20221213 15:31:54 @agent_ppo2.py:145][0m 3510272 total steps have happened
[32m[20221213 15:31:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1714 --------------------------#
[32m[20221213 15:31:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:54 @agent_ppo2.py:185][0m |          -0.0006 |          21.9520 |           0.2300 |
[32m[20221213 15:31:54 @agent_ppo2.py:185][0m |          -0.0065 |          21.5641 |           0.2299 |
[32m[20221213 15:31:54 @agent_ppo2.py:185][0m |          -0.0091 |          21.3086 |           0.2300 |
[32m[20221213 15:31:54 @agent_ppo2.py:185][0m |          -0.0135 |          21.1277 |           0.2298 |
[32m[20221213 15:31:54 @agent_ppo2.py:185][0m |          -0.0146 |          21.0255 |           0.2299 |
[32m[20221213 15:31:55 @agent_ppo2.py:185][0m |          -0.0147 |          20.9658 |           0.2298 |
[32m[20221213 15:31:55 @agent_ppo2.py:185][0m |          -0.0162 |          20.8544 |           0.2300 |
[32m[20221213 15:31:55 @agent_ppo2.py:185][0m |          -0.0162 |          20.7887 |           0.2299 |
[32m[20221213 15:31:55 @agent_ppo2.py:185][0m |          -0.0172 |          20.7339 |           0.2301 |
[32m[20221213 15:31:55 @agent_ppo2.py:185][0m |          -0.0066 |          22.7997 |           0.2300 |
[32m[20221213 15:31:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.18
[32m[20221213 15:31:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.06
[32m[20221213 15:31:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.39
[32m[20221213 15:31:55 @agent_ppo2.py:143][0m Total time:      39.00 min
[32m[20221213 15:31:55 @agent_ppo2.py:145][0m 3512320 total steps have happened
[32m[20221213 15:31:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1715 --------------------------#
[32m[20221213 15:31:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:55 @agent_ppo2.py:185][0m |           0.0071 |          23.0910 |           0.2361 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |          -0.0052 |          21.7159 |           0.2360 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |          -0.0114 |          21.4765 |           0.2358 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |           0.0073 |          23.4731 |           0.2357 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |          -0.0132 |          21.1252 |           0.2354 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |           0.0050 |          23.2892 |           0.2357 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |          -0.0165 |          20.9730 |           0.2355 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |          -0.0169 |          20.7969 |           0.2358 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |          -0.0192 |          20.7266 |           0.2359 |
[32m[20221213 15:31:56 @agent_ppo2.py:185][0m |          -0.0200 |          20.7065 |           0.2359 |
[32m[20221213 15:31:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:31:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 248.18
[32m[20221213 15:31:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.52
[32m[20221213 15:31:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.06
[32m[20221213 15:31:56 @agent_ppo2.py:143][0m Total time:      39.02 min
[32m[20221213 15:31:56 @agent_ppo2.py:145][0m 3514368 total steps have happened
[32m[20221213 15:31:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1716 --------------------------#
[32m[20221213 15:31:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |           0.0082 |          23.3314 |           0.2346 |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |          -0.0040 |          21.8523 |           0.2343 |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |          -0.0019 |          21.9180 |           0.2346 |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |          -0.0082 |          21.8638 |           0.2342 |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |          -0.0105 |          21.4765 |           0.2342 |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |          -0.0060 |          22.8817 |           0.2343 |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |          -0.0147 |          21.4366 |           0.2341 |
[32m[20221213 15:31:57 @agent_ppo2.py:185][0m |          -0.0140 |          21.3224 |           0.2342 |
[32m[20221213 15:31:58 @agent_ppo2.py:185][0m |          -0.0127 |          21.3278 |           0.2341 |
[32m[20221213 15:31:58 @agent_ppo2.py:185][0m |          -0.0100 |          21.9867 |           0.2339 |
[32m[20221213 15:31:58 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.12
[32m[20221213 15:31:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.31
[32m[20221213 15:31:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.47
[32m[20221213 15:31:58 @agent_ppo2.py:143][0m Total time:      39.04 min
[32m[20221213 15:31:58 @agent_ppo2.py:145][0m 3516416 total steps have happened
[32m[20221213 15:31:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1717 --------------------------#
[32m[20221213 15:31:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:31:58 @agent_ppo2.py:185][0m |          -0.0004 |          21.7653 |           0.2426 |
[32m[20221213 15:31:58 @agent_ppo2.py:185][0m |          -0.0079 |          21.4793 |           0.2427 |
[32m[20221213 15:31:58 @agent_ppo2.py:185][0m |          -0.0061 |          21.3672 |           0.2423 |
[32m[20221213 15:31:58 @agent_ppo2.py:185][0m |          -0.0085 |          21.1496 |           0.2423 |
[32m[20221213 15:31:59 @agent_ppo2.py:185][0m |          -0.0125 |          21.0783 |           0.2424 |
[32m[20221213 15:31:59 @agent_ppo2.py:185][0m |          -0.0097 |          21.0683 |           0.2423 |
[32m[20221213 15:31:59 @agent_ppo2.py:185][0m |          -0.0116 |          20.9583 |           0.2419 |
[32m[20221213 15:31:59 @agent_ppo2.py:185][0m |          -0.0056 |          21.7360 |           0.2421 |
[32m[20221213 15:31:59 @agent_ppo2.py:185][0m |          -0.0142 |          20.8217 |           0.2421 |
[32m[20221213 15:31:59 @agent_ppo2.py:185][0m |          -0.0138 |          20.7796 |           0.2421 |
[32m[20221213 15:31:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:31:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.24
[32m[20221213 15:31:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.75
[32m[20221213 15:31:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.91
[32m[20221213 15:31:59 @agent_ppo2.py:143][0m Total time:      39.07 min
[32m[20221213 15:31:59 @agent_ppo2.py:145][0m 3518464 total steps have happened
[32m[20221213 15:31:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1718 --------------------------#
[32m[20221213 15:31:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:31:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |           0.0029 |          21.5362 |           0.2375 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0057 |          20.3380 |           0.2367 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0029 |          21.8072 |           0.2371 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0075 |          20.0983 |           0.2369 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0133 |          19.7628 |           0.2368 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0154 |          19.6281 |           0.2372 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0157 |          19.4659 |           0.2367 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0164 |          19.4087 |           0.2369 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0181 |          19.2699 |           0.2366 |
[32m[20221213 15:32:00 @agent_ppo2.py:185][0m |          -0.0193 |          19.2201 |           0.2369 |
[32m[20221213 15:32:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.66
[32m[20221213 15:32:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.58
[32m[20221213 15:32:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.97
[32m[20221213 15:32:01 @agent_ppo2.py:143][0m Total time:      39.09 min
[32m[20221213 15:32:01 @agent_ppo2.py:145][0m 3520512 total steps have happened
[32m[20221213 15:32:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1719 --------------------------#
[32m[20221213 15:32:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:01 @agent_ppo2.py:185][0m |          -0.0017 |          21.0783 |           0.2385 |
[32m[20221213 15:32:01 @agent_ppo2.py:185][0m |          -0.0020 |          20.7885 |           0.2376 |
[32m[20221213 15:32:01 @agent_ppo2.py:185][0m |          -0.0084 |          20.5468 |           0.2375 |
[32m[20221213 15:32:01 @agent_ppo2.py:185][0m |          -0.0102 |          20.3587 |           0.2373 |
[32m[20221213 15:32:01 @agent_ppo2.py:185][0m |          -0.0113 |          20.3020 |           0.2368 |
[32m[20221213 15:32:01 @agent_ppo2.py:185][0m |          -0.0117 |          20.2261 |           0.2369 |
[32m[20221213 15:32:01 @agent_ppo2.py:185][0m |          -0.0142 |          20.1974 |           0.2368 |
[32m[20221213 15:32:02 @agent_ppo2.py:185][0m |          -0.0151 |          20.1777 |           0.2364 |
[32m[20221213 15:32:02 @agent_ppo2.py:185][0m |          -0.0127 |          20.1457 |           0.2365 |
[32m[20221213 15:32:02 @agent_ppo2.py:185][0m |          -0.0142 |          20.0520 |           0.2365 |
[32m[20221213 15:32:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:32:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.92
[32m[20221213 15:32:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.32
[32m[20221213 15:32:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.27
[32m[20221213 15:32:02 @agent_ppo2.py:143][0m Total time:      39.11 min
[32m[20221213 15:32:02 @agent_ppo2.py:145][0m 3522560 total steps have happened
[32m[20221213 15:32:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1720 --------------------------#
[32m[20221213 15:32:02 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:32:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:02 @agent_ppo2.py:185][0m |          -0.0002 |          21.4650 |           0.2329 |
[32m[20221213 15:32:02 @agent_ppo2.py:185][0m |          -0.0063 |          20.8714 |           0.2327 |
[32m[20221213 15:32:02 @agent_ppo2.py:185][0m |          -0.0104 |          20.5554 |           0.2326 |
[32m[20221213 15:32:03 @agent_ppo2.py:185][0m |          -0.0112 |          20.3373 |           0.2321 |
[32m[20221213 15:32:03 @agent_ppo2.py:185][0m |          -0.0124 |          20.2064 |           0.2324 |
[32m[20221213 15:32:03 @agent_ppo2.py:185][0m |          -0.0139 |          20.0593 |           0.2322 |
[32m[20221213 15:32:03 @agent_ppo2.py:185][0m |          -0.0095 |          20.3418 |           0.2317 |
[32m[20221213 15:32:03 @agent_ppo2.py:185][0m |          -0.0144 |          19.8159 |           0.2314 |
[32m[20221213 15:32:03 @agent_ppo2.py:185][0m |          -0.0180 |          19.7594 |           0.2316 |
[32m[20221213 15:32:03 @agent_ppo2.py:185][0m |          -0.0182 |          19.6365 |           0.2315 |
[32m[20221213 15:32:03 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:32:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.36
[32m[20221213 15:32:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.18
[32m[20221213 15:32:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.98
[32m[20221213 15:32:03 @agent_ppo2.py:143][0m Total time:      39.13 min
[32m[20221213 15:32:03 @agent_ppo2.py:145][0m 3524608 total steps have happened
[32m[20221213 15:32:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1721 --------------------------#
[32m[20221213 15:32:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |           0.0005 |          22.2553 |           0.2345 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0056 |          21.7027 |           0.2348 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0095 |          21.5009 |           0.2346 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0118 |          21.3936 |           0.2349 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0137 |          21.3227 |           0.2345 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0121 |          21.2768 |           0.2354 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0135 |          21.1559 |           0.2350 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0158 |          21.1133 |           0.2352 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0161 |          21.0981 |           0.2355 |
[32m[20221213 15:32:04 @agent_ppo2.py:185][0m |          -0.0153 |          21.0202 |           0.2353 |
[32m[20221213 15:32:04 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.35
[32m[20221213 15:32:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.41
[32m[20221213 15:32:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.33
[32m[20221213 15:32:05 @agent_ppo2.py:143][0m Total time:      39.16 min
[32m[20221213 15:32:05 @agent_ppo2.py:145][0m 3526656 total steps have happened
[32m[20221213 15:32:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1722 --------------------------#
[32m[20221213 15:32:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:05 @agent_ppo2.py:185][0m |           0.0099 |          21.9805 |           0.2363 |
[32m[20221213 15:32:05 @agent_ppo2.py:185][0m |          -0.0039 |          20.8635 |           0.2357 |
[32m[20221213 15:32:05 @agent_ppo2.py:185][0m |          -0.0095 |          20.5313 |           0.2357 |
[32m[20221213 15:32:05 @agent_ppo2.py:185][0m |          -0.0102 |          20.3661 |           0.2354 |
[32m[20221213 15:32:05 @agent_ppo2.py:185][0m |          -0.0128 |          20.2060 |           0.2355 |
[32m[20221213 15:32:05 @agent_ppo2.py:185][0m |          -0.0076 |          20.8059 |           0.2355 |
[32m[20221213 15:32:06 @agent_ppo2.py:185][0m |          -0.0133 |          19.9478 |           0.2352 |
[32m[20221213 15:32:06 @agent_ppo2.py:185][0m |          -0.0169 |          19.8829 |           0.2352 |
[32m[20221213 15:32:06 @agent_ppo2.py:185][0m |          -0.0156 |          19.8316 |           0.2352 |
[32m[20221213 15:32:06 @agent_ppo2.py:185][0m |          -0.0184 |          19.7497 |           0.2350 |
[32m[20221213 15:32:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:32:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 243.20
[32m[20221213 15:32:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.46
[32m[20221213 15:32:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.11
[32m[20221213 15:32:06 @agent_ppo2.py:143][0m Total time:      39.18 min
[32m[20221213 15:32:06 @agent_ppo2.py:145][0m 3528704 total steps have happened
[32m[20221213 15:32:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1723 --------------------------#
[32m[20221213 15:32:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:32:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:06 @agent_ppo2.py:185][0m |           0.0109 |          22.3051 |           0.2358 |
[32m[20221213 15:32:06 @agent_ppo2.py:185][0m |          -0.0075 |          20.9803 |           0.2353 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0116 |          20.8027 |           0.2354 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0135 |          20.6713 |           0.2352 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0076 |          20.7356 |           0.2351 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0125 |          20.5406 |           0.2350 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0162 |          20.4045 |           0.2349 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0163 |          20.3489 |           0.2348 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0153 |          20.3562 |           0.2349 |
[32m[20221213 15:32:07 @agent_ppo2.py:185][0m |          -0.0188 |          20.2066 |           0.2349 |
[32m[20221213 15:32:07 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:32:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.27
[32m[20221213 15:32:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.74
[32m[20221213 15:32:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.46
[32m[20221213 15:32:07 @agent_ppo2.py:143][0m Total time:      39.20 min
[32m[20221213 15:32:07 @agent_ppo2.py:145][0m 3530752 total steps have happened
[32m[20221213 15:32:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1724 --------------------------#
[32m[20221213 15:32:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0043 |          20.9130 |           0.2357 |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0102 |          20.6149 |           0.2352 |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0138 |          20.4654 |           0.2351 |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0148 |          20.3390 |           0.2351 |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0128 |          20.2790 |           0.2353 |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0109 |          20.3745 |           0.2350 |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0150 |          20.0154 |           0.2352 |
[32m[20221213 15:32:08 @agent_ppo2.py:185][0m |          -0.0075 |          21.4577 |           0.2355 |
[32m[20221213 15:32:09 @agent_ppo2.py:185][0m |          -0.0179 |          19.8826 |           0.2355 |
[32m[20221213 15:32:09 @agent_ppo2.py:185][0m |          -0.0188 |          19.7824 |           0.2353 |
[32m[20221213 15:32:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:32:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.27
[32m[20221213 15:32:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.73
[32m[20221213 15:32:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.07
[32m[20221213 15:32:09 @agent_ppo2.py:143][0m Total time:      39.23 min
[32m[20221213 15:32:09 @agent_ppo2.py:145][0m 3532800 total steps have happened
[32m[20221213 15:32:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1725 --------------------------#
[32m[20221213 15:32:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:09 @agent_ppo2.py:185][0m |          -0.0009 |          21.0816 |           0.2357 |
[32m[20221213 15:32:09 @agent_ppo2.py:185][0m |          -0.0056 |          20.5424 |           0.2351 |
[32m[20221213 15:32:09 @agent_ppo2.py:185][0m |          -0.0114 |          20.1135 |           0.2347 |
[32m[20221213 15:32:09 @agent_ppo2.py:185][0m |          -0.0138 |          19.7721 |           0.2347 |
[32m[20221213 15:32:10 @agent_ppo2.py:185][0m |          -0.0140 |          19.5890 |           0.2344 |
[32m[20221213 15:32:10 @agent_ppo2.py:185][0m |          -0.0158 |          19.2952 |           0.2344 |
[32m[20221213 15:32:10 @agent_ppo2.py:185][0m |          -0.0146 |          19.2176 |           0.2344 |
[32m[20221213 15:32:10 @agent_ppo2.py:185][0m |          -0.0179 |          18.9409 |           0.2342 |
[32m[20221213 15:32:10 @agent_ppo2.py:185][0m |          -0.0194 |          18.7459 |           0.2343 |
[32m[20221213 15:32:10 @agent_ppo2.py:185][0m |          -0.0182 |          18.6660 |           0.2342 |
[32m[20221213 15:32:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:32:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.03
[32m[20221213 15:32:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.58
[32m[20221213 15:32:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.67
[32m[20221213 15:32:10 @agent_ppo2.py:143][0m Total time:      39.25 min
[32m[20221213 15:32:10 @agent_ppo2.py:145][0m 3534848 total steps have happened
[32m[20221213 15:32:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1726 --------------------------#
[32m[20221213 15:32:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |           0.0014 |          23.5102 |           0.2403 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0043 |          22.7418 |           0.2391 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0080 |          22.3949 |           0.2403 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0112 |          21.9708 |           0.2400 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0086 |          21.7812 |           0.2398 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0095 |          21.5530 |           0.2400 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0122 |          21.4858 |           0.2398 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0159 |          21.2670 |           0.2399 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0124 |          21.2233 |           0.2401 |
[32m[20221213 15:32:11 @agent_ppo2.py:185][0m |          -0.0166 |          21.0795 |           0.2403 |
[32m[20221213 15:32:11 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:32:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.12
[32m[20221213 15:32:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.50
[32m[20221213 15:32:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.30
[32m[20221213 15:32:12 @agent_ppo2.py:143][0m Total time:      39.27 min
[32m[20221213 15:32:12 @agent_ppo2.py:145][0m 3536896 total steps have happened
[32m[20221213 15:32:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1727 --------------------------#
[32m[20221213 15:32:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:12 @agent_ppo2.py:185][0m |          -0.0024 |          22.4087 |           0.2403 |
[32m[20221213 15:32:12 @agent_ppo2.py:185][0m |          -0.0086 |          21.7247 |           0.2399 |
[32m[20221213 15:32:12 @agent_ppo2.py:185][0m |          -0.0117 |          21.5492 |           0.2398 |
[32m[20221213 15:32:12 @agent_ppo2.py:185][0m |          -0.0157 |          21.4482 |           0.2401 |
[32m[20221213 15:32:12 @agent_ppo2.py:185][0m |          -0.0160 |          21.3570 |           0.2400 |
[32m[20221213 15:32:12 @agent_ppo2.py:185][0m |          -0.0153 |          21.3241 |           0.2398 |
[32m[20221213 15:32:13 @agent_ppo2.py:185][0m |          -0.0147 |          21.3592 |           0.2398 |
[32m[20221213 15:32:13 @agent_ppo2.py:185][0m |          -0.0161 |          21.2079 |           0.2399 |
[32m[20221213 15:32:13 @agent_ppo2.py:185][0m |          -0.0167 |          21.1690 |           0.2400 |
[32m[20221213 15:32:13 @agent_ppo2.py:185][0m |          -0.0179 |          21.2284 |           0.2398 |
[32m[20221213 15:32:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.29
[32m[20221213 15:32:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.13
[32m[20221213 15:32:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.19
[32m[20221213 15:32:13 @agent_ppo2.py:143][0m Total time:      39.30 min
[32m[20221213 15:32:13 @agent_ppo2.py:145][0m 3538944 total steps have happened
[32m[20221213 15:32:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1728 --------------------------#
[32m[20221213 15:32:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:32:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:13 @agent_ppo2.py:185][0m |          -0.0008 |          21.7697 |           0.2378 |
[32m[20221213 15:32:13 @agent_ppo2.py:185][0m |          -0.0095 |          20.9153 |           0.2375 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0141 |          20.4440 |           0.2371 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0153 |          20.0923 |           0.2372 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0185 |          19.8260 |           0.2369 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0185 |          19.5160 |           0.2368 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0195 |          19.2345 |           0.2369 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0211 |          19.0349 |           0.2366 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0214 |          18.8603 |           0.2368 |
[32m[20221213 15:32:14 @agent_ppo2.py:185][0m |          -0.0212 |          18.6260 |           0.2364 |
[32m[20221213 15:32:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:32:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.12
[32m[20221213 15:32:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.71
[32m[20221213 15:32:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.24
[32m[20221213 15:32:14 @agent_ppo2.py:143][0m Total time:      39.32 min
[32m[20221213 15:32:14 @agent_ppo2.py:145][0m 3540992 total steps have happened
[32m[20221213 15:32:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1729 --------------------------#
[32m[20221213 15:32:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:32:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |           0.0001 |          22.2393 |           0.2446 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0075 |          21.4710 |           0.2445 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0094 |          21.1348 |           0.2445 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0120 |          20.9347 |           0.2443 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0130 |          20.8866 |           0.2443 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0146 |          20.6946 |           0.2443 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0157 |          20.6158 |           0.2444 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0173 |          20.4912 |           0.2446 |
[32m[20221213 15:32:15 @agent_ppo2.py:185][0m |          -0.0190 |          20.4191 |           0.2444 |
[32m[20221213 15:32:16 @agent_ppo2.py:185][0m |          -0.0104 |          21.5101 |           0.2445 |
[32m[20221213 15:32:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.72
[32m[20221213 15:32:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.05
[32m[20221213 15:32:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.01
[32m[20221213 15:32:16 @agent_ppo2.py:143][0m Total time:      39.34 min
[32m[20221213 15:32:16 @agent_ppo2.py:145][0m 3543040 total steps have happened
[32m[20221213 15:32:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1730 --------------------------#
[32m[20221213 15:32:16 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:32:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:16 @agent_ppo2.py:185][0m |           0.0011 |          23.1926 |           0.2384 |
[32m[20221213 15:32:16 @agent_ppo2.py:185][0m |           0.0080 |          25.1093 |           0.2383 |
[32m[20221213 15:32:16 @agent_ppo2.py:185][0m |          -0.0090 |          22.3576 |           0.2377 |
[32m[20221213 15:32:16 @agent_ppo2.py:185][0m |          -0.0114 |          22.2128 |           0.2375 |
[32m[20221213 15:32:16 @agent_ppo2.py:185][0m |          -0.0129 |          22.0517 |           0.2374 |
[32m[20221213 15:32:17 @agent_ppo2.py:185][0m |          -0.0134 |          21.9551 |           0.2374 |
[32m[20221213 15:32:17 @agent_ppo2.py:185][0m |          -0.0159 |          21.9330 |           0.2373 |
[32m[20221213 15:32:17 @agent_ppo2.py:185][0m |          -0.0188 |          21.8706 |           0.2372 |
[32m[20221213 15:32:17 @agent_ppo2.py:185][0m |          -0.0122 |          21.8502 |           0.2374 |
[32m[20221213 15:32:17 @agent_ppo2.py:185][0m |          -0.0173 |          21.7479 |           0.2375 |
[32m[20221213 15:32:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.85
[32m[20221213 15:32:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.76
[32m[20221213 15:32:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.09
[32m[20221213 15:32:17 @agent_ppo2.py:143][0m Total time:      39.36 min
[32m[20221213 15:32:17 @agent_ppo2.py:145][0m 3545088 total steps have happened
[32m[20221213 15:32:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1731 --------------------------#
[32m[20221213 15:32:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:17 @agent_ppo2.py:185][0m |          -0.0034 |          22.1619 |           0.2450 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0082 |          21.7589 |           0.2448 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |           0.0010 |          23.7713 |           0.2445 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0107 |          21.4613 |           0.2442 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0161 |          21.2612 |           0.2441 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0156 |          21.1416 |           0.2439 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0170 |          21.0208 |           0.2440 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0140 |          21.1768 |           0.2436 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0130 |          21.1522 |           0.2437 |
[32m[20221213 15:32:18 @agent_ppo2.py:185][0m |          -0.0191 |          20.7858 |           0.2436 |
[32m[20221213 15:32:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:32:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.77
[32m[20221213 15:32:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.92
[32m[20221213 15:32:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.88
[32m[20221213 15:32:18 @agent_ppo2.py:143][0m Total time:      39.39 min
[32m[20221213 15:32:18 @agent_ppo2.py:145][0m 3547136 total steps have happened
[32m[20221213 15:32:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1732 --------------------------#
[32m[20221213 15:32:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |           0.0029 |          22.7487 |           0.2385 |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |          -0.0077 |          21.6111 |           0.2376 |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |          -0.0133 |          21.0803 |           0.2373 |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |          -0.0157 |          20.7172 |           0.2369 |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |          -0.0153 |          20.4429 |           0.2367 |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |          -0.0190 |          20.1311 |           0.2367 |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |          -0.0158 |          20.2880 |           0.2365 |
[32m[20221213 15:32:19 @agent_ppo2.py:185][0m |          -0.0208 |          19.7371 |           0.2365 |
[32m[20221213 15:32:20 @agent_ppo2.py:185][0m |          -0.0210 |          19.5943 |           0.2364 |
[32m[20221213 15:32:20 @agent_ppo2.py:185][0m |          -0.0215 |          19.3788 |           0.2364 |
[32m[20221213 15:32:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.91
[32m[20221213 15:32:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.63
[32m[20221213 15:32:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.05
[32m[20221213 15:32:20 @agent_ppo2.py:143][0m Total time:      39.41 min
[32m[20221213 15:32:20 @agent_ppo2.py:145][0m 3549184 total steps have happened
[32m[20221213 15:32:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1733 --------------------------#
[32m[20221213 15:32:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:20 @agent_ppo2.py:185][0m |           0.0106 |          23.3036 |           0.2346 |
[32m[20221213 15:32:20 @agent_ppo2.py:185][0m |          -0.0076 |          22.1414 |           0.2341 |
[32m[20221213 15:32:20 @agent_ppo2.py:185][0m |          -0.0092 |          22.0229 |           0.2339 |
[32m[20221213 15:32:20 @agent_ppo2.py:185][0m |          -0.0133 |          21.9221 |           0.2338 |
[32m[20221213 15:32:21 @agent_ppo2.py:185][0m |          -0.0130 |          21.8423 |           0.2333 |
[32m[20221213 15:32:21 @agent_ppo2.py:185][0m |          -0.0164 |          21.7589 |           0.2332 |
[32m[20221213 15:32:21 @agent_ppo2.py:185][0m |          -0.0171 |          21.7059 |           0.2329 |
[32m[20221213 15:32:21 @agent_ppo2.py:185][0m |          -0.0178 |          21.6330 |           0.2329 |
[32m[20221213 15:32:21 @agent_ppo2.py:185][0m |          -0.0169 |          21.6254 |           0.2324 |
[32m[20221213 15:32:21 @agent_ppo2.py:185][0m |          -0.0174 |          21.5468 |           0.2323 |
[32m[20221213 15:32:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:32:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.04
[32m[20221213 15:32:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.91
[32m[20221213 15:32:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.87
[32m[20221213 15:32:21 @agent_ppo2.py:143][0m Total time:      39.43 min
[32m[20221213 15:32:21 @agent_ppo2.py:145][0m 3551232 total steps have happened
[32m[20221213 15:32:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1734 --------------------------#
[32m[20221213 15:32:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0015 |          21.7976 |           0.2281 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0053 |          21.5361 |           0.2280 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0091 |          21.3577 |           0.2278 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0070 |          21.3711 |           0.2277 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0113 |          21.0220 |           0.2277 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0139 |          20.9620 |           0.2275 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0137 |          20.8278 |           0.2274 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0111 |          20.8522 |           0.2274 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0144 |          20.6634 |           0.2274 |
[32m[20221213 15:32:22 @agent_ppo2.py:185][0m |          -0.0155 |          20.5004 |           0.2273 |
[32m[20221213 15:32:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.25
[32m[20221213 15:32:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.82
[32m[20221213 15:32:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.62
[32m[20221213 15:32:23 @agent_ppo2.py:143][0m Total time:      39.46 min
[32m[20221213 15:32:23 @agent_ppo2.py:145][0m 3553280 total steps have happened
[32m[20221213 15:32:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1735 --------------------------#
[32m[20221213 15:32:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:23 @agent_ppo2.py:185][0m |           0.0003 |          22.3679 |           0.2315 |
[32m[20221213 15:32:23 @agent_ppo2.py:185][0m |          -0.0038 |          21.3456 |           0.2312 |
[32m[20221213 15:32:23 @agent_ppo2.py:185][0m |          -0.0067 |          20.7184 |           0.2310 |
[32m[20221213 15:32:23 @agent_ppo2.py:185][0m |          -0.0032 |          20.9195 |           0.2308 |
[32m[20221213 15:32:23 @agent_ppo2.py:185][0m |          -0.0139 |          19.9193 |           0.2306 |
[32m[20221213 15:32:23 @agent_ppo2.py:185][0m |          -0.0150 |          19.4844 |           0.2306 |
[32m[20221213 15:32:23 @agent_ppo2.py:185][0m |          -0.0163 |          19.1868 |           0.2308 |
[32m[20221213 15:32:24 @agent_ppo2.py:185][0m |          -0.0150 |          18.8862 |           0.2307 |
[32m[20221213 15:32:24 @agent_ppo2.py:185][0m |          -0.0154 |          18.6740 |           0.2307 |
[32m[20221213 15:32:24 @agent_ppo2.py:185][0m |          -0.0177 |          18.4328 |           0.2307 |
[32m[20221213 15:32:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.86
[32m[20221213 15:32:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.92
[32m[20221213 15:32:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.88
[32m[20221213 15:32:24 @agent_ppo2.py:143][0m Total time:      39.48 min
[32m[20221213 15:32:24 @agent_ppo2.py:145][0m 3555328 total steps have happened
[32m[20221213 15:32:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1736 --------------------------#
[32m[20221213 15:32:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:24 @agent_ppo2.py:185][0m |          -0.0012 |          21.7543 |           0.2257 |
[32m[20221213 15:32:24 @agent_ppo2.py:185][0m |          -0.0098 |          21.1857 |           0.2254 |
[32m[20221213 15:32:24 @agent_ppo2.py:185][0m |          -0.0096 |          20.7578 |           0.2251 |
[32m[20221213 15:32:25 @agent_ppo2.py:185][0m |          -0.0063 |          20.6858 |           0.2247 |
[32m[20221213 15:32:25 @agent_ppo2.py:185][0m |          -0.0132 |          20.1852 |           0.2247 |
[32m[20221213 15:32:25 @agent_ppo2.py:185][0m |          -0.0138 |          20.0067 |           0.2242 |
[32m[20221213 15:32:25 @agent_ppo2.py:185][0m |          -0.0120 |          19.9654 |           0.2242 |
[32m[20221213 15:32:25 @agent_ppo2.py:185][0m |          -0.0155 |          19.6861 |           0.2235 |
[32m[20221213 15:32:25 @agent_ppo2.py:185][0m |          -0.0170 |          19.5224 |           0.2236 |
[32m[20221213 15:32:25 @agent_ppo2.py:185][0m |          -0.0146 |          19.3761 |           0.2234 |
[32m[20221213 15:32:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.91
[32m[20221213 15:32:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.46
[32m[20221213 15:32:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.99
[32m[20221213 15:32:25 @agent_ppo2.py:143][0m Total time:      39.50 min
[32m[20221213 15:32:25 @agent_ppo2.py:145][0m 3557376 total steps have happened
[32m[20221213 15:32:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1737 --------------------------#
[32m[20221213 15:32:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0047 |          23.9340 |           0.2282 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0103 |          23.3487 |           0.2276 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0115 |          23.0871 |           0.2273 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0152 |          22.9544 |           0.2271 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0150 |          22.7976 |           0.2269 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0171 |          22.7865 |           0.2270 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0165 |          22.6825 |           0.2268 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0178 |          22.6523 |           0.2269 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0202 |          22.6016 |           0.2270 |
[32m[20221213 15:32:26 @agent_ppo2.py:185][0m |          -0.0181 |          22.5398 |           0.2268 |
[32m[20221213 15:32:26 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:32:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.69
[32m[20221213 15:32:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.06
[32m[20221213 15:32:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.84
[32m[20221213 15:32:27 @agent_ppo2.py:143][0m Total time:      39.52 min
[32m[20221213 15:32:27 @agent_ppo2.py:145][0m 3559424 total steps have happened
[32m[20221213 15:32:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1738 --------------------------#
[32m[20221213 15:32:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:27 @agent_ppo2.py:185][0m |          -0.0001 |          22.3144 |           0.2290 |
[32m[20221213 15:32:27 @agent_ppo2.py:185][0m |          -0.0018 |          21.8846 |           0.2286 |
[32m[20221213 15:32:27 @agent_ppo2.py:185][0m |          -0.0115 |          21.3399 |           0.2281 |
[32m[20221213 15:32:27 @agent_ppo2.py:185][0m |          -0.0131 |          21.0473 |           0.2282 |
[32m[20221213 15:32:27 @agent_ppo2.py:185][0m |          -0.0150 |          20.8473 |           0.2279 |
[32m[20221213 15:32:27 @agent_ppo2.py:185][0m |          -0.0168 |          20.6623 |           0.2279 |
[32m[20221213 15:32:28 @agent_ppo2.py:185][0m |          -0.0167 |          20.5055 |           0.2280 |
[32m[20221213 15:32:28 @agent_ppo2.py:185][0m |          -0.0187 |          20.3828 |           0.2278 |
[32m[20221213 15:32:28 @agent_ppo2.py:185][0m |          -0.0196 |          20.2697 |           0.2279 |
[32m[20221213 15:32:28 @agent_ppo2.py:185][0m |          -0.0202 |          20.0867 |           0.2278 |
[32m[20221213 15:32:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.16
[32m[20221213 15:32:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.32
[32m[20221213 15:32:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.24
[32m[20221213 15:32:28 @agent_ppo2.py:143][0m Total time:      39.55 min
[32m[20221213 15:32:28 @agent_ppo2.py:145][0m 3561472 total steps have happened
[32m[20221213 15:32:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1739 --------------------------#
[32m[20221213 15:32:28 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:32:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:28 @agent_ppo2.py:185][0m |          -0.0031 |          22.6785 |           0.2208 |
[32m[20221213 15:32:28 @agent_ppo2.py:185][0m |          -0.0088 |          22.2835 |           0.2200 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0057 |          22.4862 |           0.2202 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0124 |          21.9514 |           0.2205 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0138 |          21.8537 |           0.2206 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0139 |          21.7808 |           0.2205 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0147 |          21.7062 |           0.2203 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0140 |          21.6431 |           0.2204 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0170 |          21.5958 |           0.2203 |
[32m[20221213 15:32:29 @agent_ppo2.py:185][0m |          -0.0152 |          21.4607 |           0.2205 |
[32m[20221213 15:32:29 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:32:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.98
[32m[20221213 15:32:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.62
[32m[20221213 15:32:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.30
[32m[20221213 15:32:29 @agent_ppo2.py:143][0m Total time:      39.57 min
[32m[20221213 15:32:29 @agent_ppo2.py:145][0m 3563520 total steps have happened
[32m[20221213 15:32:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1740 --------------------------#
[32m[20221213 15:32:30 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:32:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |           0.0110 |          24.8930 |           0.2249 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |          -0.0066 |          22.8281 |           0.2246 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |           0.0054 |          25.1577 |           0.2245 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |          -0.0081 |          22.5979 |           0.2239 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |          -0.0081 |          22.6412 |           0.2240 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |          -0.0068 |          23.1914 |           0.2241 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |          -0.0148 |          22.4209 |           0.2237 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |          -0.0031 |          23.6362 |           0.2238 |
[32m[20221213 15:32:30 @agent_ppo2.py:185][0m |          -0.0140 |          22.3267 |           0.2231 |
[32m[20221213 15:32:31 @agent_ppo2.py:185][0m |          -0.0145 |          22.2927 |           0.2235 |
[32m[20221213 15:32:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:32:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.52
[32m[20221213 15:32:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.81
[32m[20221213 15:32:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.25
[32m[20221213 15:32:31 @agent_ppo2.py:143][0m Total time:      39.59 min
[32m[20221213 15:32:31 @agent_ppo2.py:145][0m 3565568 total steps have happened
[32m[20221213 15:32:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1741 --------------------------#
[32m[20221213 15:32:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:31 @agent_ppo2.py:185][0m |          -0.0021 |          22.6195 |           0.2255 |
[32m[20221213 15:32:31 @agent_ppo2.py:185][0m |          -0.0082 |          22.2586 |           0.2252 |
[32m[20221213 15:32:31 @agent_ppo2.py:185][0m |          -0.0056 |          22.4733 |           0.2251 |
[32m[20221213 15:32:31 @agent_ppo2.py:185][0m |          -0.0128 |          21.9348 |           0.2248 |
[32m[20221213 15:32:31 @agent_ppo2.py:185][0m |          -0.0086 |          22.3890 |           0.2248 |
[32m[20221213 15:32:32 @agent_ppo2.py:185][0m |          -0.0104 |          21.9013 |           0.2243 |
[32m[20221213 15:32:32 @agent_ppo2.py:185][0m |          -0.0063 |          23.1092 |           0.2247 |
[32m[20221213 15:32:32 @agent_ppo2.py:185][0m |          -0.0132 |          21.9017 |           0.2245 |
[32m[20221213 15:32:32 @agent_ppo2.py:185][0m |          -0.0179 |          21.6104 |           0.2245 |
[32m[20221213 15:32:32 @agent_ppo2.py:185][0m |          -0.0179 |          21.5691 |           0.2247 |
[32m[20221213 15:32:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.32
[32m[20221213 15:32:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.44
[32m[20221213 15:32:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.21
[32m[20221213 15:32:32 @agent_ppo2.py:143][0m Total time:      39.61 min
[32m[20221213 15:32:32 @agent_ppo2.py:145][0m 3567616 total steps have happened
[32m[20221213 15:32:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1742 --------------------------#
[32m[20221213 15:32:32 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:32:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:32 @agent_ppo2.py:185][0m |          -0.0044 |          22.5313 |           0.2258 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0063 |          21.8008 |           0.2250 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0092 |          21.5559 |           0.2250 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0079 |          21.3043 |           0.2248 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0119 |          21.1683 |           0.2248 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0131 |          21.1022 |           0.2248 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0088 |          21.3229 |           0.2245 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0144 |          20.9340 |           0.2242 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0132 |          20.8086 |           0.2243 |
[32m[20221213 15:32:33 @agent_ppo2.py:185][0m |          -0.0159 |          20.7701 |           0.2242 |
[32m[20221213 15:32:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.85
[32m[20221213 15:32:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.10
[32m[20221213 15:32:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.27
[32m[20221213 15:32:33 @agent_ppo2.py:143][0m Total time:      39.64 min
[32m[20221213 15:32:33 @agent_ppo2.py:145][0m 3569664 total steps have happened
[32m[20221213 15:32:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1743 --------------------------#
[32m[20221213 15:32:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0019 |          21.7404 |           0.2195 |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0055 |          21.5092 |           0.2189 |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0072 |          21.3072 |           0.2190 |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0105 |          21.1415 |           0.2191 |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0123 |          21.0565 |           0.2191 |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0125 |          20.9746 |           0.2191 |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0158 |          20.8453 |           0.2191 |
[32m[20221213 15:32:34 @agent_ppo2.py:185][0m |          -0.0160 |          20.7490 |           0.2191 |
[32m[20221213 15:32:35 @agent_ppo2.py:185][0m |          -0.0159 |          20.6731 |           0.2191 |
[32m[20221213 15:32:35 @agent_ppo2.py:185][0m |          -0.0072 |          23.0568 |           0.2194 |
[32m[20221213 15:32:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:32:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.61
[32m[20221213 15:32:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.04
[32m[20221213 15:32:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.08
[32m[20221213 15:32:35 @agent_ppo2.py:143][0m Total time:      39.66 min
[32m[20221213 15:32:35 @agent_ppo2.py:145][0m 3571712 total steps have happened
[32m[20221213 15:32:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1744 --------------------------#
[32m[20221213 15:32:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:35 @agent_ppo2.py:185][0m |           0.0008 |          22.8300 |           0.2263 |
[32m[20221213 15:32:35 @agent_ppo2.py:185][0m |          -0.0098 |          22.1130 |           0.2261 |
[32m[20221213 15:32:35 @agent_ppo2.py:185][0m |          -0.0121 |          21.8082 |           0.2259 |
[32m[20221213 15:32:35 @agent_ppo2.py:185][0m |          -0.0112 |          21.6054 |           0.2254 |
[32m[20221213 15:32:36 @agent_ppo2.py:185][0m |          -0.0136 |          21.4519 |           0.2253 |
[32m[20221213 15:32:36 @agent_ppo2.py:185][0m |          -0.0139 |          21.3394 |           0.2251 |
[32m[20221213 15:32:36 @agent_ppo2.py:185][0m |          -0.0111 |          21.2927 |           0.2251 |
[32m[20221213 15:32:36 @agent_ppo2.py:185][0m |          -0.0167 |          21.0846 |           0.2247 |
[32m[20221213 15:32:36 @agent_ppo2.py:185][0m |          -0.0172 |          21.0380 |           0.2247 |
[32m[20221213 15:32:36 @agent_ppo2.py:185][0m |          -0.0168 |          20.9778 |           0.2248 |
[32m[20221213 15:32:36 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.15
[32m[20221213 15:32:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.85
[32m[20221213 15:32:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.35
[32m[20221213 15:32:36 @agent_ppo2.py:143][0m Total time:      39.68 min
[32m[20221213 15:32:36 @agent_ppo2.py:145][0m 3573760 total steps have happened
[32m[20221213 15:32:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1745 --------------------------#
[32m[20221213 15:32:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0018 |          22.9602 |           0.2192 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0051 |          22.4445 |           0.2186 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0115 |          22.0607 |           0.2186 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0104 |          21.9663 |           0.2183 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0122 |          21.9047 |           0.2187 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0118 |          21.8271 |           0.2184 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0155 |          21.8612 |           0.2185 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0145 |          21.7450 |           0.2187 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0148 |          21.7220 |           0.2188 |
[32m[20221213 15:32:37 @agent_ppo2.py:185][0m |          -0.0044 |          22.8882 |           0.2185 |
[32m[20221213 15:32:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:32:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.50
[32m[20221213 15:32:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.00
[32m[20221213 15:32:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.16
[32m[20221213 15:32:38 @agent_ppo2.py:143][0m Total time:      39.71 min
[32m[20221213 15:32:38 @agent_ppo2.py:145][0m 3575808 total steps have happened
[32m[20221213 15:32:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1746 --------------------------#
[32m[20221213 15:32:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:38 @agent_ppo2.py:185][0m |           0.0062 |          23.8185 |           0.2265 |
[32m[20221213 15:32:38 @agent_ppo2.py:185][0m |          -0.0016 |          23.8798 |           0.2264 |
[32m[20221213 15:32:38 @agent_ppo2.py:185][0m |          -0.0110 |          21.6956 |           0.2261 |
[32m[20221213 15:32:38 @agent_ppo2.py:185][0m |          -0.0147 |          21.4683 |           0.2260 |
[32m[20221213 15:32:38 @agent_ppo2.py:185][0m |          -0.0168 |          21.2444 |           0.2262 |
[32m[20221213 15:32:38 @agent_ppo2.py:185][0m |          -0.0162 |          21.0284 |           0.2261 |
[32m[20221213 15:32:39 @agent_ppo2.py:185][0m |          -0.0182 |          20.8896 |           0.2261 |
[32m[20221213 15:32:39 @agent_ppo2.py:185][0m |          -0.0056 |          22.4720 |           0.2262 |
[32m[20221213 15:32:39 @agent_ppo2.py:185][0m |          -0.0163 |          20.7718 |           0.2259 |
[32m[20221213 15:32:39 @agent_ppo2.py:185][0m |          -0.0211 |          20.5921 |           0.2259 |
[32m[20221213 15:32:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:32:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.06
[32m[20221213 15:32:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.65
[32m[20221213 15:32:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.98
[32m[20221213 15:32:39 @agent_ppo2.py:143][0m Total time:      39.73 min
[32m[20221213 15:32:39 @agent_ppo2.py:145][0m 3577856 total steps have happened
[32m[20221213 15:32:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1747 --------------------------#
[32m[20221213 15:32:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:39 @agent_ppo2.py:185][0m |           0.0040 |          23.1973 |           0.2264 |
[32m[20221213 15:32:39 @agent_ppo2.py:185][0m |          -0.0078 |          22.5444 |           0.2259 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0086 |          22.4419 |           0.2254 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0120 |          22.2799 |           0.2254 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0126 |          22.1920 |           0.2251 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0083 |          22.5055 |           0.2247 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0157 |          22.0895 |           0.2248 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0171 |          21.9814 |           0.2250 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0147 |          21.9371 |           0.2248 |
[32m[20221213 15:32:40 @agent_ppo2.py:185][0m |          -0.0165 |          21.8727 |           0.2248 |
[32m[20221213 15:32:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:32:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.89
[32m[20221213 15:32:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.10
[32m[20221213 15:32:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.03
[32m[20221213 15:32:40 @agent_ppo2.py:143][0m Total time:      39.75 min
[32m[20221213 15:32:40 @agent_ppo2.py:145][0m 3579904 total steps have happened
[32m[20221213 15:32:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1748 --------------------------#
[32m[20221213 15:32:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |           0.0001 |          22.3660 |           0.2220 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0077 |          21.8634 |           0.2214 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0124 |          21.5742 |           0.2214 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0145 |          21.3462 |           0.2210 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0089 |          22.3452 |           0.2209 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0124 |          21.2531 |           0.2207 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0153 |          20.9358 |           0.2204 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0118 |          21.2299 |           0.2205 |
[32m[20221213 15:32:41 @agent_ppo2.py:185][0m |          -0.0188 |          20.6922 |           0.2205 |
[32m[20221213 15:32:42 @agent_ppo2.py:185][0m |          -0.0197 |          20.6027 |           0.2203 |
[32m[20221213 15:32:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.69
[32m[20221213 15:32:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.79
[32m[20221213 15:32:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.96
[32m[20221213 15:32:42 @agent_ppo2.py:143][0m Total time:      39.78 min
[32m[20221213 15:32:42 @agent_ppo2.py:145][0m 3581952 total steps have happened
[32m[20221213 15:32:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1749 --------------------------#
[32m[20221213 15:32:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:42 @agent_ppo2.py:185][0m |          -0.0026 |          22.7268 |           0.2241 |
[32m[20221213 15:32:42 @agent_ppo2.py:185][0m |          -0.0101 |          22.3738 |           0.2238 |
[32m[20221213 15:32:42 @agent_ppo2.py:185][0m |          -0.0135 |          22.1505 |           0.2233 |
[32m[20221213 15:32:42 @agent_ppo2.py:185][0m |          -0.0149 |          22.0145 |           0.2231 |
[32m[20221213 15:32:42 @agent_ppo2.py:185][0m |          -0.0017 |          24.0570 |           0.2228 |
[32m[20221213 15:32:43 @agent_ppo2.py:185][0m |          -0.0186 |          21.7924 |           0.2227 |
[32m[20221213 15:32:43 @agent_ppo2.py:185][0m |          -0.0161 |          21.6872 |           0.2224 |
[32m[20221213 15:32:43 @agent_ppo2.py:185][0m |          -0.0149 |          21.5903 |           0.2224 |
[32m[20221213 15:32:43 @agent_ppo2.py:185][0m |          -0.0132 |          22.2637 |           0.2221 |
[32m[20221213 15:32:43 @agent_ppo2.py:185][0m |          -0.0190 |          21.5020 |           0.2219 |
[32m[20221213 15:32:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.64
[32m[20221213 15:32:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.03
[32m[20221213 15:32:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.95
[32m[20221213 15:32:43 @agent_ppo2.py:143][0m Total time:      39.80 min
[32m[20221213 15:32:43 @agent_ppo2.py:145][0m 3584000 total steps have happened
[32m[20221213 15:32:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1750 --------------------------#
[32m[20221213 15:32:43 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:32:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:43 @agent_ppo2.py:185][0m |          -0.0028 |          21.7148 |           0.2214 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0012 |          21.5161 |           0.2209 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0116 |          20.4340 |           0.2207 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0009 |          23.0453 |           0.2203 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0125 |          19.8529 |           0.2198 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0143 |          19.6459 |           0.2202 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0072 |          21.8738 |           0.2202 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0177 |          19.2742 |           0.2200 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0141 |          19.2111 |           0.2202 |
[32m[20221213 15:32:44 @agent_ppo2.py:185][0m |          -0.0163 |          19.1071 |           0.2202 |
[32m[20221213 15:32:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:32:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.51
[32m[20221213 15:32:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.88
[32m[20221213 15:32:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.07
[32m[20221213 15:32:45 @agent_ppo2.py:143][0m Total time:      39.82 min
[32m[20221213 15:32:45 @agent_ppo2.py:145][0m 3586048 total steps have happened
[32m[20221213 15:32:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1751 --------------------------#
[32m[20221213 15:32:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:32:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:45 @agent_ppo2.py:185][0m |          -0.0011 |          23.9619 |           0.2130 |
[32m[20221213 15:32:45 @agent_ppo2.py:185][0m |          -0.0071 |          23.5001 |           0.2127 |
[32m[20221213 15:32:45 @agent_ppo2.py:185][0m |          -0.0089 |          23.3126 |           0.2128 |
[32m[20221213 15:32:45 @agent_ppo2.py:185][0m |          -0.0088 |          23.2074 |           0.2129 |
[32m[20221213 15:32:45 @agent_ppo2.py:185][0m |          -0.0131 |          23.0742 |           0.2129 |
[32m[20221213 15:32:45 @agent_ppo2.py:185][0m |          -0.0135 |          22.9938 |           0.2128 |
[32m[20221213 15:32:45 @agent_ppo2.py:185][0m |          -0.0149 |          22.9049 |           0.2128 |
[32m[20221213 15:32:46 @agent_ppo2.py:185][0m |          -0.0162 |          22.8377 |           0.2128 |
[32m[20221213 15:32:46 @agent_ppo2.py:185][0m |          -0.0140 |          22.8528 |           0.2129 |
[32m[20221213 15:32:46 @agent_ppo2.py:185][0m |          -0.0180 |          22.7647 |           0.2129 |
[32m[20221213 15:32:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:32:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.69
[32m[20221213 15:32:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.58
[32m[20221213 15:32:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.72
[32m[20221213 15:32:46 @agent_ppo2.py:143][0m Total time:      39.84 min
[32m[20221213 15:32:46 @agent_ppo2.py:145][0m 3588096 total steps have happened
[32m[20221213 15:32:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1752 --------------------------#
[32m[20221213 15:32:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:46 @agent_ppo2.py:185][0m |          -0.0009 |          23.5904 |           0.2190 |
[32m[20221213 15:32:46 @agent_ppo2.py:185][0m |          -0.0057 |          23.2771 |           0.2188 |
[32m[20221213 15:32:46 @agent_ppo2.py:185][0m |          -0.0101 |          23.1473 |           0.2186 |
[32m[20221213 15:32:47 @agent_ppo2.py:185][0m |          -0.0091 |          23.0174 |           0.2183 |
[32m[20221213 15:32:47 @agent_ppo2.py:185][0m |          -0.0099 |          23.1408 |           0.2179 |
[32m[20221213 15:32:47 @agent_ppo2.py:185][0m |          -0.0040 |          24.2791 |           0.2182 |
[32m[20221213 15:32:47 @agent_ppo2.py:185][0m |          -0.0145 |          22.8761 |           0.2177 |
[32m[20221213 15:32:47 @agent_ppo2.py:185][0m |          -0.0044 |          23.6457 |           0.2177 |
[32m[20221213 15:32:47 @agent_ppo2.py:185][0m |          -0.0139 |          22.7097 |           0.2176 |
[32m[20221213 15:32:47 @agent_ppo2.py:185][0m |          -0.0148 |          22.5996 |           0.2175 |
[32m[20221213 15:32:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.11
[32m[20221213 15:32:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.19
[32m[20221213 15:32:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.02
[32m[20221213 15:32:47 @agent_ppo2.py:143][0m Total time:      39.87 min
[32m[20221213 15:32:47 @agent_ppo2.py:145][0m 3590144 total steps have happened
[32m[20221213 15:32:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1753 --------------------------#
[32m[20221213 15:32:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |           0.0127 |          24.9157 |           0.2189 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0082 |          22.8171 |           0.2184 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0101 |          22.6433 |           0.2180 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0124 |          22.4861 |           0.2181 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0142 |          22.4068 |           0.2181 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0164 |          22.3131 |           0.2184 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0133 |          22.3995 |           0.2182 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0153 |          22.1804 |           0.2183 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0173 |          22.1136 |           0.2183 |
[32m[20221213 15:32:48 @agent_ppo2.py:185][0m |          -0.0162 |          22.0132 |           0.2185 |
[32m[20221213 15:32:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.49
[32m[20221213 15:32:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.19
[32m[20221213 15:32:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.01
[32m[20221213 15:32:49 @agent_ppo2.py:143][0m Total time:      39.89 min
[32m[20221213 15:32:49 @agent_ppo2.py:145][0m 3592192 total steps have happened
[32m[20221213 15:32:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1754 --------------------------#
[32m[20221213 15:32:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:49 @agent_ppo2.py:185][0m |          -0.0016 |          22.1127 |           0.2164 |
[32m[20221213 15:32:49 @agent_ppo2.py:185][0m |          -0.0086 |          21.5422 |           0.2159 |
[32m[20221213 15:32:49 @agent_ppo2.py:185][0m |          -0.0106 |          21.2766 |           0.2163 |
[32m[20221213 15:32:49 @agent_ppo2.py:185][0m |          -0.0142 |          21.1476 |           0.2161 |
[32m[20221213 15:32:49 @agent_ppo2.py:185][0m |          -0.0135 |          21.0923 |           0.2161 |
[32m[20221213 15:32:49 @agent_ppo2.py:185][0m |          -0.0151 |          20.9772 |           0.2162 |
[32m[20221213 15:32:50 @agent_ppo2.py:185][0m |          -0.0158 |          21.0588 |           0.2160 |
[32m[20221213 15:32:50 @agent_ppo2.py:185][0m |          -0.0131 |          20.8956 |           0.2162 |
[32m[20221213 15:32:50 @agent_ppo2.py:185][0m |          -0.0179 |          20.8003 |           0.2161 |
[32m[20221213 15:32:50 @agent_ppo2.py:185][0m |          -0.0180 |          20.7939 |           0.2160 |
[32m[20221213 15:32:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:32:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.33
[32m[20221213 15:32:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.21
[32m[20221213 15:32:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.46
[32m[20221213 15:32:50 @agent_ppo2.py:143][0m Total time:      39.91 min
[32m[20221213 15:32:50 @agent_ppo2.py:145][0m 3594240 total steps have happened
[32m[20221213 15:32:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1755 --------------------------#
[32m[20221213 15:32:50 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:32:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:50 @agent_ppo2.py:185][0m |          -0.0021 |          22.6954 |           0.2256 |
[32m[20221213 15:32:50 @agent_ppo2.py:185][0m |          -0.0094 |          22.1731 |           0.2250 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0111 |          21.7865 |           0.2253 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0135 |          21.5588 |           0.2254 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0139 |          21.3197 |           0.2254 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0159 |          21.1326 |           0.2254 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0074 |          21.6992 |           0.2254 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0155 |          20.9238 |           0.2254 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0123 |          21.1221 |           0.2252 |
[32m[20221213 15:32:51 @agent_ppo2.py:185][0m |          -0.0144 |          20.9142 |           0.2252 |
[32m[20221213 15:32:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.81
[32m[20221213 15:32:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.20
[32m[20221213 15:32:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.94
[32m[20221213 15:32:51 @agent_ppo2.py:143][0m Total time:      39.94 min
[32m[20221213 15:32:51 @agent_ppo2.py:145][0m 3596288 total steps have happened
[32m[20221213 15:32:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1756 --------------------------#
[32m[20221213 15:32:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0009 |          23.2669 |           0.2192 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |           0.0014 |          24.6722 |           0.2190 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0108 |          22.3384 |           0.2190 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0132 |          22.1680 |           0.2188 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0149 |          22.0321 |           0.2187 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0176 |          21.9693 |           0.2185 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0162 |          21.9453 |           0.2186 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0170 |          21.8462 |           0.2185 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0162 |          21.8398 |           0.2185 |
[32m[20221213 15:32:52 @agent_ppo2.py:185][0m |          -0.0188 |          21.7856 |           0.2182 |
[32m[20221213 15:32:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.46
[32m[20221213 15:32:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.44
[32m[20221213 15:32:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.11
[32m[20221213 15:32:53 @agent_ppo2.py:143][0m Total time:      39.96 min
[32m[20221213 15:32:53 @agent_ppo2.py:145][0m 3598336 total steps have happened
[32m[20221213 15:32:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1757 --------------------------#
[32m[20221213 15:32:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:53 @agent_ppo2.py:185][0m |          -0.0009 |          22.9100 |           0.2223 |
[32m[20221213 15:32:53 @agent_ppo2.py:185][0m |          -0.0077 |          22.5562 |           0.2219 |
[32m[20221213 15:32:53 @agent_ppo2.py:185][0m |          -0.0031 |          24.5446 |           0.2218 |
[32m[20221213 15:32:53 @agent_ppo2.py:185][0m |          -0.0063 |          22.7794 |           0.2214 |
[32m[20221213 15:32:53 @agent_ppo2.py:185][0m |          -0.0101 |          22.4579 |           0.2213 |
[32m[20221213 15:32:54 @agent_ppo2.py:185][0m |          -0.0152 |          21.8414 |           0.2213 |
[32m[20221213 15:32:54 @agent_ppo2.py:185][0m |          -0.0158 |          21.7298 |           0.2215 |
[32m[20221213 15:32:54 @agent_ppo2.py:185][0m |          -0.0071 |          22.7695 |           0.2216 |
[32m[20221213 15:32:54 @agent_ppo2.py:185][0m |          -0.0173 |          21.4928 |           0.2216 |
[32m[20221213 15:32:54 @agent_ppo2.py:185][0m |          -0.0189 |          21.3901 |           0.2216 |
[32m[20221213 15:32:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:32:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.19
[32m[20221213 15:32:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.43
[32m[20221213 15:32:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.67
[32m[20221213 15:32:54 @agent_ppo2.py:143][0m Total time:      39.98 min
[32m[20221213 15:32:54 @agent_ppo2.py:145][0m 3600384 total steps have happened
[32m[20221213 15:32:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1758 --------------------------#
[32m[20221213 15:32:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:54 @agent_ppo2.py:185][0m |          -0.0024 |          22.3409 |           0.2249 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0009 |          22.6681 |           0.2243 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0121 |          20.3759 |           0.2238 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0131 |          19.8562 |           0.2241 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0142 |          19.4396 |           0.2240 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0037 |          20.1086 |           0.2242 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0135 |          18.8688 |           0.2246 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0166 |          18.6314 |           0.2247 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0192 |          18.4351 |           0.2248 |
[32m[20221213 15:32:55 @agent_ppo2.py:185][0m |          -0.0129 |          19.3143 |           0.2247 |
[32m[20221213 15:32:55 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:32:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.67
[32m[20221213 15:32:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.83
[32m[20221213 15:32:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.90
[32m[20221213 15:32:55 @agent_ppo2.py:143][0m Total time:      40.00 min
[32m[20221213 15:32:55 @agent_ppo2.py:145][0m 3602432 total steps have happened
[32m[20221213 15:32:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1759 --------------------------#
[32m[20221213 15:32:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |           0.0017 |          25.3742 |           0.2176 |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |          -0.0085 |          22.3701 |           0.2179 |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |          -0.0138 |          21.5885 |           0.2179 |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |          -0.0132 |          21.1276 |           0.2177 |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |          -0.0150 |          20.7780 |           0.2177 |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |          -0.0155 |          20.5001 |           0.2177 |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |          -0.0184 |          20.2765 |           0.2177 |
[32m[20221213 15:32:56 @agent_ppo2.py:185][0m |          -0.0187 |          20.2222 |           0.2177 |
[32m[20221213 15:32:57 @agent_ppo2.py:185][0m |          -0.0188 |          19.9868 |           0.2177 |
[32m[20221213 15:32:57 @agent_ppo2.py:185][0m |          -0.0196 |          19.8031 |           0.2179 |
[32m[20221213 15:32:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:32:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.37
[32m[20221213 15:32:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.95
[32m[20221213 15:32:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.22
[32m[20221213 15:32:57 @agent_ppo2.py:143][0m Total time:      40.03 min
[32m[20221213 15:32:57 @agent_ppo2.py:145][0m 3604480 total steps have happened
[32m[20221213 15:32:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1760 --------------------------#
[32m[20221213 15:32:57 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:32:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:57 @agent_ppo2.py:185][0m |          -0.0008 |          23.6030 |           0.2273 |
[32m[20221213 15:32:57 @agent_ppo2.py:185][0m |          -0.0077 |          22.9378 |           0.2272 |
[32m[20221213 15:32:57 @agent_ppo2.py:185][0m |          -0.0086 |          22.7237 |           0.2266 |
[32m[20221213 15:32:58 @agent_ppo2.py:185][0m |          -0.0124 |          22.6213 |           0.2264 |
[32m[20221213 15:32:58 @agent_ppo2.py:185][0m |          -0.0014 |          25.3151 |           0.2260 |
[32m[20221213 15:32:58 @agent_ppo2.py:185][0m |          -0.0109 |          22.5818 |           0.2254 |
[32m[20221213 15:32:58 @agent_ppo2.py:185][0m |          -0.0171 |          22.4083 |           0.2256 |
[32m[20221213 15:32:58 @agent_ppo2.py:185][0m |          -0.0130 |          22.5193 |           0.2256 |
[32m[20221213 15:32:58 @agent_ppo2.py:185][0m |          -0.0184 |          22.3250 |           0.2255 |
[32m[20221213 15:32:58 @agent_ppo2.py:185][0m |          -0.0189 |          22.2851 |           0.2254 |
[32m[20221213 15:32:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:32:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.28
[32m[20221213 15:32:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.53
[32m[20221213 15:32:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.58
[32m[20221213 15:32:58 @agent_ppo2.py:143][0m Total time:      40.05 min
[32m[20221213 15:32:58 @agent_ppo2.py:145][0m 3606528 total steps have happened
[32m[20221213 15:32:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1761 --------------------------#
[32m[20221213 15:32:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:32:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0017 |          23.1221 |           0.2169 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0054 |          22.7841 |           0.2165 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0052 |          22.7087 |           0.2161 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0062 |          23.5430 |           0.2158 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0123 |          22.1089 |           0.2154 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0152 |          21.9839 |           0.2155 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0047 |          23.9657 |           0.2154 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0145 |          21.8821 |           0.2148 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0106 |          24.9075 |           0.2150 |
[32m[20221213 15:32:59 @agent_ppo2.py:185][0m |          -0.0187 |          21.8087 |           0.2149 |
[32m[20221213 15:32:59 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:33:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.22
[32m[20221213 15:33:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.07
[32m[20221213 15:33:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.67
[32m[20221213 15:33:00 @agent_ppo2.py:143][0m Total time:      40.07 min
[32m[20221213 15:33:00 @agent_ppo2.py:145][0m 3608576 total steps have happened
[32m[20221213 15:33:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1762 --------------------------#
[32m[20221213 15:33:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:00 @agent_ppo2.py:185][0m |          -0.0033 |          21.4195 |           0.2173 |
[32m[20221213 15:33:00 @agent_ppo2.py:185][0m |          -0.0087 |          20.6445 |           0.2170 |
[32m[20221213 15:33:00 @agent_ppo2.py:185][0m |          -0.0127 |          20.2778 |           0.2168 |
[32m[20221213 15:33:00 @agent_ppo2.py:185][0m |          -0.0055 |          20.8831 |           0.2168 |
[32m[20221213 15:33:00 @agent_ppo2.py:185][0m |          -0.0149 |          19.9522 |           0.2161 |
[32m[20221213 15:33:00 @agent_ppo2.py:185][0m |          -0.0109 |          20.0195 |           0.2161 |
[32m[20221213 15:33:01 @agent_ppo2.py:185][0m |          -0.0098 |          19.9270 |           0.2158 |
[32m[20221213 15:33:01 @agent_ppo2.py:185][0m |          -0.0156 |          19.5561 |           0.2158 |
[32m[20221213 15:33:01 @agent_ppo2.py:185][0m |          -0.0187 |          19.4587 |           0.2159 |
[32m[20221213 15:33:01 @agent_ppo2.py:185][0m |          -0.0155 |          19.5955 |           0.2156 |
[32m[20221213 15:33:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:33:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.76
[32m[20221213 15:33:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.96
[32m[20221213 15:33:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.23
[32m[20221213 15:33:01 @agent_ppo2.py:143][0m Total time:      40.10 min
[32m[20221213 15:33:01 @agent_ppo2.py:145][0m 3610624 total steps have happened
[32m[20221213 15:33:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1763 --------------------------#
[32m[20221213 15:33:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:01 @agent_ppo2.py:185][0m |          -0.0007 |          24.1596 |           0.2085 |
[32m[20221213 15:33:01 @agent_ppo2.py:185][0m |          -0.0072 |          23.3655 |           0.2081 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0052 |          23.9210 |           0.2076 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0132 |          22.9989 |           0.2074 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0138 |          22.9215 |           0.2074 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0121 |          22.8836 |           0.2070 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0144 |          22.7094 |           0.2067 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0161 |          22.7127 |           0.2064 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0102 |          23.0271 |           0.2062 |
[32m[20221213 15:33:02 @agent_ppo2.py:185][0m |          -0.0151 |          22.5421 |           0.2059 |
[32m[20221213 15:33:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:33:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.38
[32m[20221213 15:33:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.92
[32m[20221213 15:33:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.52
[32m[20221213 15:33:02 @agent_ppo2.py:143][0m Total time:      40.12 min
[32m[20221213 15:33:02 @agent_ppo2.py:145][0m 3612672 total steps have happened
[32m[20221213 15:33:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1764 --------------------------#
[32m[20221213 15:33:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0017 |          22.1708 |           0.2155 |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0084 |          21.5691 |           0.2150 |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0057 |          21.5223 |           0.2148 |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0042 |          22.3457 |           0.2145 |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0128 |          20.9229 |           0.2143 |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0138 |          20.8419 |           0.2144 |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0050 |          23.2779 |           0.2145 |
[32m[20221213 15:33:03 @agent_ppo2.py:185][0m |          -0.0022 |          22.1792 |           0.2137 |
[32m[20221213 15:33:04 @agent_ppo2.py:185][0m |          -0.0167 |          20.4952 |           0.2139 |
[32m[20221213 15:33:04 @agent_ppo2.py:185][0m |          -0.0148 |          20.4398 |           0.2141 |
[32m[20221213 15:33:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:33:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.14
[32m[20221213 15:33:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.61
[32m[20221213 15:33:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.30
[32m[20221213 15:33:04 @agent_ppo2.py:143][0m Total time:      40.14 min
[32m[20221213 15:33:04 @agent_ppo2.py:145][0m 3614720 total steps have happened
[32m[20221213 15:33:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1765 --------------------------#
[32m[20221213 15:33:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:04 @agent_ppo2.py:185][0m |          -0.0032 |          22.8454 |           0.2088 |
[32m[20221213 15:33:04 @agent_ppo2.py:185][0m |          -0.0104 |          22.3343 |           0.2086 |
[32m[20221213 15:33:04 @agent_ppo2.py:185][0m |          -0.0119 |          22.0707 |           0.2085 |
[32m[20221213 15:33:04 @agent_ppo2.py:185][0m |          -0.0047 |          22.5952 |           0.2083 |
[32m[20221213 15:33:05 @agent_ppo2.py:185][0m |          -0.0104 |          22.0348 |           0.2084 |
[32m[20221213 15:33:05 @agent_ppo2.py:185][0m |          -0.0164 |          21.5170 |           0.2082 |
[32m[20221213 15:33:05 @agent_ppo2.py:185][0m |          -0.0167 |          21.4193 |           0.2082 |
[32m[20221213 15:33:05 @agent_ppo2.py:185][0m |          -0.0195 |          21.3275 |           0.2082 |
[32m[20221213 15:33:05 @agent_ppo2.py:185][0m |          -0.0088 |          22.0020 |           0.2081 |
[32m[20221213 15:33:05 @agent_ppo2.py:185][0m |          -0.0186 |          21.1276 |           0.2080 |
[32m[20221213 15:33:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:33:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.92
[32m[20221213 15:33:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.89
[32m[20221213 15:33:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.09
[32m[20221213 15:33:05 @agent_ppo2.py:143][0m Total time:      40.17 min
[32m[20221213 15:33:05 @agent_ppo2.py:145][0m 3616768 total steps have happened
[32m[20221213 15:33:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1766 --------------------------#
[32m[20221213 15:33:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0002 |          23.7272 |           0.2101 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0050 |          23.0414 |           0.2094 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0078 |          22.7738 |           0.2090 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0099 |          22.5982 |           0.2088 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0107 |          22.4949 |           0.2086 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0102 |          22.4146 |           0.2086 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0140 |          22.3971 |           0.2083 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |           0.0022 |          24.2961 |           0.2081 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0143 |          22.1982 |           0.2080 |
[32m[20221213 15:33:06 @agent_ppo2.py:185][0m |          -0.0156 |          22.1700 |           0.2080 |
[32m[20221213 15:33:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:33:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.70
[32m[20221213 15:33:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.74
[32m[20221213 15:33:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.29
[32m[20221213 15:33:07 @agent_ppo2.py:143][0m Total time:      40.19 min
[32m[20221213 15:33:07 @agent_ppo2.py:145][0m 3618816 total steps have happened
[32m[20221213 15:33:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1767 --------------------------#
[32m[20221213 15:33:07 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:07 @agent_ppo2.py:185][0m |           0.0003 |          22.3152 |           0.2075 |
[32m[20221213 15:33:07 @agent_ppo2.py:185][0m |          -0.0082 |          22.0234 |           0.2071 |
[32m[20221213 15:33:07 @agent_ppo2.py:185][0m |           0.0013 |          23.0611 |           0.2072 |
[32m[20221213 15:33:07 @agent_ppo2.py:185][0m |          -0.0092 |          21.8268 |           0.2069 |
[32m[20221213 15:33:07 @agent_ppo2.py:185][0m |          -0.0133 |          21.7406 |           0.2070 |
[32m[20221213 15:33:07 @agent_ppo2.py:185][0m |          -0.0139 |          21.6383 |           0.2070 |
[32m[20221213 15:33:08 @agent_ppo2.py:185][0m |          -0.0001 |          24.1370 |           0.2070 |
[32m[20221213 15:33:08 @agent_ppo2.py:185][0m |          -0.0134 |          21.5961 |           0.2065 |
[32m[20221213 15:33:08 @agent_ppo2.py:185][0m |          -0.0154 |          21.5256 |           0.2067 |
[32m[20221213 15:33:08 @agent_ppo2.py:185][0m |          -0.0123 |          21.5504 |           0.2068 |
[32m[20221213 15:33:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:33:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.13
[32m[20221213 15:33:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.56
[32m[20221213 15:33:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.21
[32m[20221213 15:33:08 @agent_ppo2.py:143][0m Total time:      40.21 min
[32m[20221213 15:33:08 @agent_ppo2.py:145][0m 3620864 total steps have happened
[32m[20221213 15:33:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1768 --------------------------#
[32m[20221213 15:33:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:08 @agent_ppo2.py:185][0m |           0.0008 |          21.4347 |           0.2083 |
[32m[20221213 15:33:08 @agent_ppo2.py:185][0m |          -0.0087 |          20.5552 |           0.2080 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0036 |          20.9273 |           0.2082 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0114 |          20.1167 |           0.2080 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0055 |          20.9439 |           0.2080 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0144 |          19.7996 |           0.2080 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0153 |          19.6537 |           0.2083 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0076 |          19.9524 |           0.2079 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0150 |          19.2194 |           0.2079 |
[32m[20221213 15:33:09 @agent_ppo2.py:185][0m |          -0.0149 |          19.0579 |           0.2083 |
[32m[20221213 15:33:09 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:33:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.27
[32m[20221213 15:33:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.09
[32m[20221213 15:33:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 191.21
[32m[20221213 15:33:09 @agent_ppo2.py:143][0m Total time:      40.24 min
[32m[20221213 15:33:09 @agent_ppo2.py:145][0m 3622912 total steps have happened
[32m[20221213 15:33:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1769 --------------------------#
[32m[20221213 15:33:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0018 |          23.4908 |           0.2071 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0100 |          22.7405 |           0.2067 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0099 |          22.4500 |           0.2066 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0152 |          22.3775 |           0.2065 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0043 |          24.0838 |           0.2063 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0183 |          21.9189 |           0.2062 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0171 |          21.7355 |           0.2063 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0088 |          22.6914 |           0.2065 |
[32m[20221213 15:33:10 @agent_ppo2.py:185][0m |          -0.0165 |          21.5382 |           0.2054 |
[32m[20221213 15:33:11 @agent_ppo2.py:185][0m |          -0.0212 |          21.4287 |           0.2063 |
[32m[20221213 15:33:11 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:33:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.61
[32m[20221213 15:33:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.92
[32m[20221213 15:33:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.73
[32m[20221213 15:33:11 @agent_ppo2.py:143][0m Total time:      40.26 min
[32m[20221213 15:33:11 @agent_ppo2.py:145][0m 3624960 total steps have happened
[32m[20221213 15:33:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1770 --------------------------#
[32m[20221213 15:33:11 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:33:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:11 @agent_ppo2.py:185][0m |          -0.0035 |          24.0998 |           0.2053 |
[32m[20221213 15:33:11 @agent_ppo2.py:185][0m |          -0.0112 |          23.6407 |           0.2048 |
[32m[20221213 15:33:11 @agent_ppo2.py:185][0m |          -0.0117 |          23.3705 |           0.2052 |
[32m[20221213 15:33:11 @agent_ppo2.py:185][0m |          -0.0054 |          24.5618 |           0.2052 |
[32m[20221213 15:33:11 @agent_ppo2.py:185][0m |          -0.0123 |          23.0817 |           0.2050 |
[32m[20221213 15:33:12 @agent_ppo2.py:185][0m |          -0.0166 |          22.9002 |           0.2054 |
[32m[20221213 15:33:12 @agent_ppo2.py:185][0m |          -0.0162 |          22.8168 |           0.2054 |
[32m[20221213 15:33:12 @agent_ppo2.py:185][0m |          -0.0155 |          22.7412 |           0.2054 |
[32m[20221213 15:33:12 @agent_ppo2.py:185][0m |          -0.0167 |          22.5745 |           0.2055 |
[32m[20221213 15:33:12 @agent_ppo2.py:185][0m |          -0.0062 |          25.7991 |           0.2055 |
[32m[20221213 15:33:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:33:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.75
[32m[20221213 15:33:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.55
[32m[20221213 15:33:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.71
[32m[20221213 15:33:12 @agent_ppo2.py:143][0m Total time:      40.28 min
[32m[20221213 15:33:12 @agent_ppo2.py:145][0m 3627008 total steps have happened
[32m[20221213 15:33:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1771 --------------------------#
[32m[20221213 15:33:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:12 @agent_ppo2.py:185][0m |          -0.0017 |          22.4434 |           0.2051 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0049 |          21.9941 |           0.2047 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0029 |          22.2046 |           0.2047 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0034 |          22.5402 |           0.2047 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0135 |          21.3199 |           0.2041 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0137 |          21.0824 |           0.2046 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0174 |          20.9274 |           0.2043 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0185 |          20.7479 |           0.2046 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0190 |          20.6521 |           0.2046 |
[32m[20221213 15:33:13 @agent_ppo2.py:185][0m |          -0.0190 |          20.4962 |           0.2047 |
[32m[20221213 15:33:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:33:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.29
[32m[20221213 15:33:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.65
[32m[20221213 15:33:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.27
[32m[20221213 15:33:13 @agent_ppo2.py:143][0m Total time:      40.30 min
[32m[20221213 15:33:13 @agent_ppo2.py:145][0m 3629056 total steps have happened
[32m[20221213 15:33:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1772 --------------------------#
[32m[20221213 15:33:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |           0.0041 |          20.1032 |           0.2088 |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |          -0.0097 |          18.2709 |           0.2084 |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |          -0.0122 |          17.7773 |           0.2084 |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |          -0.0142 |          17.3007 |           0.2085 |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |          -0.0175 |          17.0183 |           0.2085 |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |          -0.0171 |          16.7319 |           0.2085 |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |          -0.0147 |          16.6666 |           0.2085 |
[32m[20221213 15:33:14 @agent_ppo2.py:185][0m |          -0.0160 |          16.2948 |           0.2085 |
[32m[20221213 15:33:15 @agent_ppo2.py:185][0m |          -0.0228 |          16.0438 |           0.2085 |
[32m[20221213 15:33:15 @agent_ppo2.py:185][0m |          -0.0224 |          15.8473 |           0.2088 |
[32m[20221213 15:33:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:33:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.02
[32m[20221213 15:33:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.89
[32m[20221213 15:33:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.83
[32m[20221213 15:33:15 @agent_ppo2.py:143][0m Total time:      40.33 min
[32m[20221213 15:33:15 @agent_ppo2.py:145][0m 3631104 total steps have happened
[32m[20221213 15:33:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1773 --------------------------#
[32m[20221213 15:33:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:15 @agent_ppo2.py:185][0m |           0.0019 |          22.9711 |           0.2173 |
[32m[20221213 15:33:15 @agent_ppo2.py:185][0m |          -0.0072 |          21.8322 |           0.2171 |
[32m[20221213 15:33:15 @agent_ppo2.py:185][0m |          -0.0114 |          21.1904 |           0.2171 |
[32m[20221213 15:33:15 @agent_ppo2.py:185][0m |          -0.0103 |          20.8039 |           0.2169 |
[32m[20221213 15:33:16 @agent_ppo2.py:185][0m |          -0.0129 |          20.7881 |           0.2171 |
[32m[20221213 15:33:16 @agent_ppo2.py:185][0m |          -0.0168 |          20.2301 |           0.2173 |
[32m[20221213 15:33:16 @agent_ppo2.py:185][0m |          -0.0173 |          19.9673 |           0.2174 |
[32m[20221213 15:33:16 @agent_ppo2.py:185][0m |          -0.0229 |          19.8106 |           0.2174 |
[32m[20221213 15:33:16 @agent_ppo2.py:185][0m |          -0.0189 |          19.6260 |           0.2175 |
[32m[20221213 15:33:16 @agent_ppo2.py:185][0m |          -0.0193 |          19.5560 |           0.2177 |
[32m[20221213 15:33:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:33:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.40
[32m[20221213 15:33:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.78
[32m[20221213 15:33:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.49
[32m[20221213 15:33:16 @agent_ppo2.py:143][0m Total time:      40.35 min
[32m[20221213 15:33:16 @agent_ppo2.py:145][0m 3633152 total steps have happened
[32m[20221213 15:33:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1774 --------------------------#
[32m[20221213 15:33:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0020 |          22.3404 |           0.2151 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0079 |          21.3374 |           0.2148 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0123 |          21.0497 |           0.2144 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0165 |          20.7023 |           0.2144 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0135 |          20.4910 |           0.2141 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0150 |          20.1537 |           0.2139 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0111 |          20.7800 |           0.2141 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0098 |          21.2100 |           0.2139 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0211 |          19.6397 |           0.2136 |
[32m[20221213 15:33:17 @agent_ppo2.py:185][0m |          -0.0205 |          19.4658 |           0.2137 |
[32m[20221213 15:33:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:33:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.87
[32m[20221213 15:33:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.76
[32m[20221213 15:33:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.17
[32m[20221213 15:33:18 @agent_ppo2.py:143][0m Total time:      40.37 min
[32m[20221213 15:33:18 @agent_ppo2.py:145][0m 3635200 total steps have happened
[32m[20221213 15:33:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1775 --------------------------#
[32m[20221213 15:33:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:18 @agent_ppo2.py:185][0m |           0.0005 |          22.0133 |           0.2112 |
[32m[20221213 15:33:18 @agent_ppo2.py:185][0m |          -0.0052 |          21.0534 |           0.2109 |
[32m[20221213 15:33:18 @agent_ppo2.py:185][0m |          -0.0113 |          20.5998 |           0.2108 |
[32m[20221213 15:33:18 @agent_ppo2.py:185][0m |          -0.0102 |          20.1742 |           0.2106 |
[32m[20221213 15:33:18 @agent_ppo2.py:185][0m |          -0.0125 |          19.8664 |           0.2106 |
[32m[20221213 15:33:18 @agent_ppo2.py:185][0m |          -0.0146 |          19.7014 |           0.2107 |
[32m[20221213 15:33:18 @agent_ppo2.py:185][0m |          -0.0150 |          19.4442 |           0.2104 |
[32m[20221213 15:33:19 @agent_ppo2.py:185][0m |          -0.0040 |          20.4744 |           0.2105 |
[32m[20221213 15:33:19 @agent_ppo2.py:185][0m |          -0.0178 |          19.1938 |           0.2105 |
[32m[20221213 15:33:19 @agent_ppo2.py:185][0m |          -0.0180 |          18.9977 |           0.2103 |
[32m[20221213 15:33:19 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:33:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.69
[32m[20221213 15:33:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.69
[32m[20221213 15:33:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.75
[32m[20221213 15:33:19 @agent_ppo2.py:143][0m Total time:      40.40 min
[32m[20221213 15:33:19 @agent_ppo2.py:145][0m 3637248 total steps have happened
[32m[20221213 15:33:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1776 --------------------------#
[32m[20221213 15:33:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:19 @agent_ppo2.py:185][0m |           0.0206 |          27.5956 |           0.2122 |
[32m[20221213 15:33:19 @agent_ppo2.py:185][0m |           0.0027 |          25.0660 |           0.2122 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0071 |          23.6514 |           0.2119 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0111 |          23.3580 |           0.2118 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0124 |          23.2498 |           0.2120 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0117 |          23.0875 |           0.2120 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0178 |          23.0262 |           0.2119 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0094 |          23.6164 |           0.2120 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0161 |          22.8778 |           0.2120 |
[32m[20221213 15:33:20 @agent_ppo2.py:185][0m |          -0.0117 |          23.2056 |           0.2120 |
[32m[20221213 15:33:20 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:33:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.76
[32m[20221213 15:33:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.60
[32m[20221213 15:33:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.94
[32m[20221213 15:33:20 @agent_ppo2.py:143][0m Total time:      40.42 min
[32m[20221213 15:33:20 @agent_ppo2.py:145][0m 3639296 total steps have happened
[32m[20221213 15:33:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1777 --------------------------#
[32m[20221213 15:33:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0005 |          23.5798 |           0.2132 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0020 |          23.3901 |           0.2132 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0108 |          22.6398 |           0.2131 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |           0.0030 |          24.5243 |           0.2132 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0125 |          22.4230 |           0.2131 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0136 |          22.2380 |           0.2135 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0122 |          22.3037 |           0.2133 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0129 |          22.1006 |           0.2133 |
[32m[20221213 15:33:21 @agent_ppo2.py:185][0m |          -0.0071 |          24.9514 |           0.2135 |
[32m[20221213 15:33:22 @agent_ppo2.py:185][0m |          -0.0167 |          22.0421 |           0.2138 |
[32m[20221213 15:33:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:33:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.69
[32m[20221213 15:33:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.02
[32m[20221213 15:33:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.65
[32m[20221213 15:33:22 @agent_ppo2.py:143][0m Total time:      40.44 min
[32m[20221213 15:33:22 @agent_ppo2.py:145][0m 3641344 total steps have happened
[32m[20221213 15:33:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1778 --------------------------#
[32m[20221213 15:33:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:22 @agent_ppo2.py:185][0m |           0.0107 |          25.9065 |           0.2196 |
[32m[20221213 15:33:22 @agent_ppo2.py:185][0m |          -0.0040 |          22.7039 |           0.2188 |
[32m[20221213 15:33:22 @agent_ppo2.py:185][0m |          -0.0108 |          22.4717 |           0.2191 |
[32m[20221213 15:33:22 @agent_ppo2.py:185][0m |          -0.0015 |          23.6727 |           0.2193 |
[32m[20221213 15:33:23 @agent_ppo2.py:185][0m |          -0.0121 |          22.1790 |           0.2192 |
[32m[20221213 15:33:23 @agent_ppo2.py:185][0m |          -0.0139 |          22.0811 |           0.2193 |
[32m[20221213 15:33:23 @agent_ppo2.py:185][0m |          -0.0145 |          22.0203 |           0.2191 |
[32m[20221213 15:33:23 @agent_ppo2.py:185][0m |          -0.0141 |          21.9363 |           0.2193 |
[32m[20221213 15:33:23 @agent_ppo2.py:185][0m |          -0.0169 |          21.8866 |           0.2190 |
[32m[20221213 15:33:23 @agent_ppo2.py:185][0m |          -0.0182 |          21.8486 |           0.2190 |
[32m[20221213 15:33:23 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:33:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.78
[32m[20221213 15:33:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.59
[32m[20221213 15:33:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.62
[32m[20221213 15:33:23 @agent_ppo2.py:143][0m Total time:      40.47 min
[32m[20221213 15:33:23 @agent_ppo2.py:145][0m 3643392 total steps have happened
[32m[20221213 15:33:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1779 --------------------------#
[32m[20221213 15:33:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |           0.0017 |          22.6331 |           0.2144 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0059 |          22.3070 |           0.2145 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0102 |          22.2147 |           0.2146 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0114 |          22.0262 |           0.2147 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0154 |          21.9002 |           0.2147 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0066 |          22.7963 |           0.2147 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0140 |          21.7534 |           0.2148 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0164 |          21.6471 |           0.2148 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0164 |          21.6076 |           0.2148 |
[32m[20221213 15:33:24 @agent_ppo2.py:185][0m |          -0.0181 |          21.5224 |           0.2148 |
[32m[20221213 15:33:24 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:33:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.52
[32m[20221213 15:33:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.90
[32m[20221213 15:33:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.67
[32m[20221213 15:33:25 @agent_ppo2.py:143][0m Total time:      40.49 min
[32m[20221213 15:33:25 @agent_ppo2.py:145][0m 3645440 total steps have happened
[32m[20221213 15:33:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1780 --------------------------#
[32m[20221213 15:33:25 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:33:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:25 @agent_ppo2.py:185][0m |          -0.0007 |          21.9960 |           0.2206 |
[32m[20221213 15:33:25 @agent_ppo2.py:185][0m |          -0.0063 |          21.7878 |           0.2204 |
[32m[20221213 15:33:25 @agent_ppo2.py:185][0m |          -0.0080 |          21.7344 |           0.2203 |
[32m[20221213 15:33:25 @agent_ppo2.py:185][0m |          -0.0112 |          21.5507 |           0.2203 |
[32m[20221213 15:33:25 @agent_ppo2.py:185][0m |          -0.0112 |          21.4745 |           0.2200 |
[32m[20221213 15:33:25 @agent_ppo2.py:185][0m |          -0.0136 |          21.4179 |           0.2200 |
[32m[20221213 15:33:26 @agent_ppo2.py:185][0m |          -0.0134 |          21.4124 |           0.2198 |
[32m[20221213 15:33:26 @agent_ppo2.py:185][0m |          -0.0163 |          21.3749 |           0.2199 |
[32m[20221213 15:33:26 @agent_ppo2.py:185][0m |          -0.0125 |          21.3258 |           0.2200 |
[32m[20221213 15:33:26 @agent_ppo2.py:185][0m |          -0.0160 |          21.2749 |           0.2198 |
[32m[20221213 15:33:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:33:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.27
[32m[20221213 15:33:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.60
[32m[20221213 15:33:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.90
[32m[20221213 15:33:26 @agent_ppo2.py:143][0m Total time:      40.51 min
[32m[20221213 15:33:26 @agent_ppo2.py:145][0m 3647488 total steps have happened
[32m[20221213 15:33:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1781 --------------------------#
[32m[20221213 15:33:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:26 @agent_ppo2.py:185][0m |          -0.0020 |          22.5384 |           0.2242 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0091 |          22.0239 |           0.2236 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0110 |          21.7991 |           0.2233 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0124 |          21.5747 |           0.2232 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0116 |          21.4108 |           0.2230 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0132 |          21.2217 |           0.2229 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0144 |          21.0589 |           0.2229 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0156 |          20.9124 |           0.2229 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0150 |          20.8061 |           0.2227 |
[32m[20221213 15:33:27 @agent_ppo2.py:185][0m |          -0.0169 |          20.7131 |           0.2231 |
[32m[20221213 15:33:27 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:33:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.95
[32m[20221213 15:33:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.91
[32m[20221213 15:33:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.38
[32m[20221213 15:33:27 @agent_ppo2.py:143][0m Total time:      40.54 min
[32m[20221213 15:33:27 @agent_ppo2.py:145][0m 3649536 total steps have happened
[32m[20221213 15:33:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1782 --------------------------#
[32m[20221213 15:33:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:28 @agent_ppo2.py:185][0m |          -0.0016 |          21.8872 |           0.2132 |
[32m[20221213 15:33:28 @agent_ppo2.py:185][0m |          -0.0052 |          21.5869 |           0.2125 |
[32m[20221213 15:33:28 @agent_ppo2.py:185][0m |          -0.0111 |          21.3266 |           0.2125 |
[32m[20221213 15:33:28 @agent_ppo2.py:185][0m |          -0.0133 |          21.1852 |           0.2126 |
[32m[20221213 15:33:28 @agent_ppo2.py:185][0m |          -0.0133 |          21.0976 |           0.2121 |
[32m[20221213 15:33:28 @agent_ppo2.py:185][0m |          -0.0114 |          21.2434 |           0.2120 |
[32m[20221213 15:33:28 @agent_ppo2.py:185][0m |          -0.0059 |          22.6550 |           0.2120 |
[32m[20221213 15:33:29 @agent_ppo2.py:185][0m |          -0.0060 |          23.3143 |           0.2120 |
[32m[20221213 15:33:29 @agent_ppo2.py:185][0m |          -0.0160 |          20.8291 |           0.2116 |
[32m[20221213 15:33:29 @agent_ppo2.py:185][0m |          -0.0188 |          20.6433 |           0.2117 |
[32m[20221213 15:33:29 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:33:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.01
[32m[20221213 15:33:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.53
[32m[20221213 15:33:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.96
[32m[20221213 15:33:29 @agent_ppo2.py:143][0m Total time:      40.56 min
[32m[20221213 15:33:29 @agent_ppo2.py:145][0m 3651584 total steps have happened
[32m[20221213 15:33:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1783 --------------------------#
[32m[20221213 15:33:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:29 @agent_ppo2.py:185][0m |           0.0032 |          24.1282 |           0.2182 |
[32m[20221213 15:33:29 @agent_ppo2.py:185][0m |          -0.0090 |          22.5996 |           0.2172 |
[32m[20221213 15:33:29 @agent_ppo2.py:185][0m |          -0.0134 |          22.0302 |           0.2173 |
[32m[20221213 15:33:30 @agent_ppo2.py:185][0m |          -0.0148 |          21.6997 |           0.2173 |
[32m[20221213 15:33:30 @agent_ppo2.py:185][0m |          -0.0108 |          21.5400 |           0.2174 |
[32m[20221213 15:33:30 @agent_ppo2.py:185][0m |          -0.0148 |          21.1792 |           0.2174 |
[32m[20221213 15:33:30 @agent_ppo2.py:185][0m |          -0.0151 |          21.0514 |           0.2174 |
[32m[20221213 15:33:30 @agent_ppo2.py:185][0m |          -0.0173 |          20.8287 |           0.2174 |
[32m[20221213 15:33:30 @agent_ppo2.py:185][0m |          -0.0204 |          20.6936 |           0.2174 |
[32m[20221213 15:33:30 @agent_ppo2.py:185][0m |          -0.0188 |          20.6518 |           0.2175 |
[32m[20221213 15:33:30 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 15:33:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.34
[32m[20221213 15:33:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.42
[32m[20221213 15:33:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 204.43
[32m[20221213 15:33:30 @agent_ppo2.py:143][0m Total time:      40.59 min
[32m[20221213 15:33:30 @agent_ppo2.py:145][0m 3653632 total steps have happened
[32m[20221213 15:33:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1784 --------------------------#
[32m[20221213 15:33:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0012 |          21.8882 |           0.2211 |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0073 |          21.3192 |           0.2207 |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0067 |          21.0520 |           0.2206 |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0126 |          20.8793 |           0.2202 |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0127 |          20.6592 |           0.2203 |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0130 |          20.3914 |           0.2201 |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0181 |          20.3789 |           0.2197 |
[32m[20221213 15:33:31 @agent_ppo2.py:185][0m |          -0.0178 |          20.2591 |           0.2198 |
[32m[20221213 15:33:32 @agent_ppo2.py:185][0m |          -0.0187 |          20.0370 |           0.2198 |
[32m[20221213 15:33:32 @agent_ppo2.py:185][0m |          -0.0111 |          20.5133 |           0.2198 |
[32m[20221213 15:33:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:33:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.46
[32m[20221213 15:33:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.42
[32m[20221213 15:33:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.01
[32m[20221213 15:33:32 @agent_ppo2.py:143][0m Total time:      40.61 min
[32m[20221213 15:33:32 @agent_ppo2.py:145][0m 3655680 total steps have happened
[32m[20221213 15:33:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1785 --------------------------#
[32m[20221213 15:33:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:32 @agent_ppo2.py:185][0m |          -0.0011 |          22.3999 |           0.2230 |
[32m[20221213 15:33:32 @agent_ppo2.py:185][0m |          -0.0056 |          21.8134 |           0.2229 |
[32m[20221213 15:33:32 @agent_ppo2.py:185][0m |          -0.0074 |          21.5432 |           0.2226 |
[32m[20221213 15:33:33 @agent_ppo2.py:185][0m |          -0.0075 |          21.3143 |           0.2222 |
[32m[20221213 15:33:33 @agent_ppo2.py:185][0m |          -0.0107 |          21.1729 |           0.2222 |
[32m[20221213 15:33:33 @agent_ppo2.py:185][0m |          -0.0120 |          21.0717 |           0.2220 |
[32m[20221213 15:33:33 @agent_ppo2.py:185][0m |          -0.0134 |          20.9585 |           0.2217 |
[32m[20221213 15:33:33 @agent_ppo2.py:185][0m |          -0.0108 |          20.8856 |           0.2216 |
[32m[20221213 15:33:33 @agent_ppo2.py:185][0m |          -0.0150 |          20.7951 |           0.2218 |
[32m[20221213 15:33:33 @agent_ppo2.py:185][0m |          -0.0123 |          20.8319 |           0.2215 |
[32m[20221213 15:33:33 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:33:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.12
[32m[20221213 15:33:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.76
[32m[20221213 15:33:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.14
[32m[20221213 15:33:33 @agent_ppo2.py:143][0m Total time:      40.63 min
[32m[20221213 15:33:33 @agent_ppo2.py:145][0m 3657728 total steps have happened
[32m[20221213 15:33:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1786 --------------------------#
[32m[20221213 15:33:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0022 |          23.5314 |           0.2182 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0003 |          23.9549 |           0.2180 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0085 |          22.7814 |           0.2175 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0100 |          22.6445 |           0.2175 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0111 |          22.5807 |           0.2175 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0131 |          22.5085 |           0.2172 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0117 |          22.5836 |           0.2172 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0143 |          22.4463 |           0.2172 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0176 |          22.3563 |           0.2172 |
[32m[20221213 15:33:34 @agent_ppo2.py:185][0m |          -0.0104 |          22.6369 |           0.2173 |
[32m[20221213 15:33:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:33:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.93
[32m[20221213 15:33:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.38
[32m[20221213 15:33:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.75
[32m[20221213 15:33:35 @agent_ppo2.py:143][0m Total time:      40.66 min
[32m[20221213 15:33:35 @agent_ppo2.py:145][0m 3659776 total steps have happened
[32m[20221213 15:33:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1787 --------------------------#
[32m[20221213 15:33:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:35 @agent_ppo2.py:185][0m |          -0.0028 |          22.0141 |           0.2207 |
[32m[20221213 15:33:35 @agent_ppo2.py:185][0m |          -0.0097 |          21.4251 |           0.2199 |
[32m[20221213 15:33:35 @agent_ppo2.py:185][0m |          -0.0090 |          21.1703 |           0.2198 |
[32m[20221213 15:33:35 @agent_ppo2.py:185][0m |          -0.0121 |          20.8191 |           0.2193 |
[32m[20221213 15:33:35 @agent_ppo2.py:185][0m |          -0.0160 |          20.5479 |           0.2190 |
[32m[20221213 15:33:36 @agent_ppo2.py:185][0m |          -0.0107 |          20.7309 |           0.2188 |
[32m[20221213 15:33:36 @agent_ppo2.py:185][0m |          -0.0160 |          20.2390 |           0.2187 |
[32m[20221213 15:33:36 @agent_ppo2.py:185][0m |          -0.0182 |          20.0658 |           0.2187 |
[32m[20221213 15:33:36 @agent_ppo2.py:185][0m |          -0.0109 |          20.6751 |           0.2188 |
[32m[20221213 15:33:36 @agent_ppo2.py:185][0m |          -0.0186 |          19.9023 |           0.2185 |
[32m[20221213 15:33:36 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:33:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.50
[32m[20221213 15:33:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.09
[32m[20221213 15:33:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.95
[32m[20221213 15:33:36 @agent_ppo2.py:143][0m Total time:      40.68 min
[32m[20221213 15:33:36 @agent_ppo2.py:145][0m 3661824 total steps have happened
[32m[20221213 15:33:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1788 --------------------------#
[32m[20221213 15:33:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0024 |          21.3493 |           0.2178 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0062 |          20.8158 |           0.2170 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0091 |          20.4837 |           0.2168 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0084 |          20.4226 |           0.2167 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0145 |          20.1502 |           0.2166 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0083 |          20.6033 |           0.2166 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0116 |          20.0441 |           0.2168 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0160 |          19.7137 |           0.2166 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0127 |          19.8952 |           0.2168 |
[32m[20221213 15:33:37 @agent_ppo2.py:185][0m |          -0.0166 |          19.4612 |           0.2164 |
[32m[20221213 15:33:37 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:33:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.63
[32m[20221213 15:33:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.39
[32m[20221213 15:33:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.81
[32m[20221213 15:33:38 @agent_ppo2.py:143][0m Total time:      40.71 min
[32m[20221213 15:33:38 @agent_ppo2.py:145][0m 3663872 total steps have happened
[32m[20221213 15:33:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1789 --------------------------#
[32m[20221213 15:33:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:38 @agent_ppo2.py:185][0m |          -0.0013 |          21.6703 |           0.2183 |
[32m[20221213 15:33:38 @agent_ppo2.py:185][0m |          -0.0071 |          21.2470 |           0.2180 |
[32m[20221213 15:33:38 @agent_ppo2.py:185][0m |          -0.0100 |          21.0060 |           0.2181 |
[32m[20221213 15:33:38 @agent_ppo2.py:185][0m |          -0.0032 |          21.4905 |           0.2181 |
[32m[20221213 15:33:38 @agent_ppo2.py:185][0m |          -0.0138 |          20.7697 |           0.2180 |
[32m[20221213 15:33:38 @agent_ppo2.py:185][0m |          -0.0147 |          20.6748 |           0.2182 |
[32m[20221213 15:33:39 @agent_ppo2.py:185][0m |          -0.0156 |          20.6034 |           0.2183 |
[32m[20221213 15:33:39 @agent_ppo2.py:185][0m |          -0.0095 |          21.1103 |           0.2183 |
[32m[20221213 15:33:39 @agent_ppo2.py:185][0m |          -0.0069 |          21.8625 |           0.2182 |
[32m[20221213 15:33:39 @agent_ppo2.py:185][0m |          -0.0160 |          20.3897 |           0.2178 |
[32m[20221213 15:33:39 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 15:33:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.92
[32m[20221213 15:33:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.06
[32m[20221213 15:33:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.93
[32m[20221213 15:33:39 @agent_ppo2.py:143][0m Total time:      40.73 min
[32m[20221213 15:33:39 @agent_ppo2.py:145][0m 3665920 total steps have happened
[32m[20221213 15:33:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1790 --------------------------#
[32m[20221213 15:33:39 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:33:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:39 @agent_ppo2.py:185][0m |           0.0000 |          22.9430 |           0.2223 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0040 |          22.3858 |           0.2221 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0015 |          22.4849 |           0.2216 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0112 |          21.9822 |           0.2216 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0014 |          22.9580 |           0.2216 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0151 |          21.8007 |           0.2215 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0123 |          21.7469 |           0.2213 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0147 |          21.6705 |           0.2216 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0157 |          21.6215 |           0.2212 |
[32m[20221213 15:33:40 @agent_ppo2.py:185][0m |          -0.0082 |          22.9865 |           0.2212 |
[32m[20221213 15:33:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:33:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.48
[32m[20221213 15:33:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.73
[32m[20221213 15:33:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.22
[32m[20221213 15:33:40 @agent_ppo2.py:143][0m Total time:      40.75 min
[32m[20221213 15:33:40 @agent_ppo2.py:145][0m 3667968 total steps have happened
[32m[20221213 15:33:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1791 --------------------------#
[32m[20221213 15:33:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:41 @agent_ppo2.py:185][0m |          -0.0026 |          22.5262 |           0.2142 |
[32m[20221213 15:33:41 @agent_ppo2.py:185][0m |          -0.0092 |          22.0595 |           0.2141 |
[32m[20221213 15:33:41 @agent_ppo2.py:185][0m |          -0.0114 |          21.8101 |           0.2139 |
[32m[20221213 15:33:41 @agent_ppo2.py:185][0m |          -0.0127 |          21.6193 |           0.2141 |
[32m[20221213 15:33:41 @agent_ppo2.py:185][0m |          -0.0134 |          21.5116 |           0.2139 |
[32m[20221213 15:33:41 @agent_ppo2.py:185][0m |          -0.0065 |          22.1394 |           0.2141 |
[32m[20221213 15:33:41 @agent_ppo2.py:185][0m |          -0.0030 |          22.9830 |           0.2140 |
[32m[20221213 15:33:42 @agent_ppo2.py:185][0m |          -0.0139 |          21.2589 |           0.2140 |
[32m[20221213 15:33:42 @agent_ppo2.py:185][0m |          -0.0078 |          21.6267 |           0.2139 |
[32m[20221213 15:33:42 @agent_ppo2.py:185][0m |          -0.0169 |          21.2171 |           0.2139 |
[32m[20221213 15:33:42 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:33:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.14
[32m[20221213 15:33:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.35
[32m[20221213 15:33:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.40
[32m[20221213 15:33:42 @agent_ppo2.py:143][0m Total time:      40.78 min
[32m[20221213 15:33:42 @agent_ppo2.py:145][0m 3670016 total steps have happened
[32m[20221213 15:33:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1792 --------------------------#
[32m[20221213 15:33:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:42 @agent_ppo2.py:185][0m |          -0.0011 |          22.5644 |           0.2180 |
[32m[20221213 15:33:42 @agent_ppo2.py:185][0m |          -0.0085 |          22.1356 |           0.2170 |
[32m[20221213 15:33:42 @agent_ppo2.py:185][0m |          -0.0093 |          21.9259 |           0.2173 |
[32m[20221213 15:33:43 @agent_ppo2.py:185][0m |          -0.0128 |          21.7697 |           0.2173 |
[32m[20221213 15:33:43 @agent_ppo2.py:185][0m |          -0.0126 |          21.6990 |           0.2170 |
[32m[20221213 15:33:43 @agent_ppo2.py:185][0m |          -0.0080 |          22.5220 |           0.2166 |
[32m[20221213 15:33:43 @agent_ppo2.py:185][0m |          -0.0121 |          21.6918 |           0.2170 |
[32m[20221213 15:33:43 @agent_ppo2.py:185][0m |          -0.0117 |          22.2007 |           0.2166 |
[32m[20221213 15:33:43 @agent_ppo2.py:185][0m |          -0.0055 |          22.2829 |           0.2165 |
[32m[20221213 15:33:43 @agent_ppo2.py:185][0m |          -0.0165 |          21.2249 |           0.2164 |
[32m[20221213 15:33:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:33:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.23
[32m[20221213 15:33:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.86
[32m[20221213 15:33:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.97
[32m[20221213 15:33:43 @agent_ppo2.py:143][0m Total time:      40.80 min
[32m[20221213 15:33:43 @agent_ppo2.py:145][0m 3672064 total steps have happened
[32m[20221213 15:33:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1793 --------------------------#
[32m[20221213 15:33:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0019 |          21.7720 |           0.2179 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0090 |          21.3328 |           0.2177 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0095 |          21.0960 |           0.2179 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0142 |          20.9618 |           0.2178 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0151 |          20.8433 |           0.2180 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0136 |          20.7254 |           0.2180 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0173 |          20.6278 |           0.2180 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0113 |          21.1502 |           0.2179 |
[32m[20221213 15:33:44 @agent_ppo2.py:185][0m |          -0.0175 |          20.4250 |           0.2183 |
[32m[20221213 15:33:45 @agent_ppo2.py:185][0m |          -0.0186 |          20.3460 |           0.2182 |
[32m[20221213 15:33:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:33:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.09
[32m[20221213 15:33:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.17
[32m[20221213 15:33:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.99
[32m[20221213 15:33:45 @agent_ppo2.py:143][0m Total time:      40.83 min
[32m[20221213 15:33:45 @agent_ppo2.py:145][0m 3674112 total steps have happened
[32m[20221213 15:33:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1794 --------------------------#
[32m[20221213 15:33:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:45 @agent_ppo2.py:185][0m |          -0.0034 |          21.2512 |           0.2225 |
[32m[20221213 15:33:45 @agent_ppo2.py:185][0m |          -0.0076 |          20.9383 |           0.2220 |
[32m[20221213 15:33:45 @agent_ppo2.py:185][0m |          -0.0098 |          20.8007 |           0.2217 |
[32m[20221213 15:33:45 @agent_ppo2.py:185][0m |          -0.0145 |          20.6723 |           0.2217 |
[32m[20221213 15:33:46 @agent_ppo2.py:185][0m |          -0.0136 |          20.5634 |           0.2215 |
[32m[20221213 15:33:46 @agent_ppo2.py:185][0m |          -0.0121 |          20.6491 |           0.2213 |
[32m[20221213 15:33:46 @agent_ppo2.py:185][0m |          -0.0169 |          20.4065 |           0.2213 |
[32m[20221213 15:33:46 @agent_ppo2.py:185][0m |          -0.0122 |          20.5199 |           0.2212 |
[32m[20221213 15:33:46 @agent_ppo2.py:185][0m |          -0.0183 |          20.3286 |           0.2211 |
[32m[20221213 15:33:46 @agent_ppo2.py:185][0m |          -0.0180 |          20.2640 |           0.2211 |
[32m[20221213 15:33:46 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221213 15:33:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.01
[32m[20221213 15:33:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.02
[32m[20221213 15:33:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.52
[32m[20221213 15:33:46 @agent_ppo2.py:143][0m Total time:      40.85 min
[32m[20221213 15:33:46 @agent_ppo2.py:145][0m 3676160 total steps have happened
[32m[20221213 15:33:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1795 --------------------------#
[32m[20221213 15:33:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0023 |          21.6589 |           0.2264 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0073 |          21.3427 |           0.2257 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0100 |          21.2264 |           0.2255 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0080 |          21.1538 |           0.2255 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0116 |          20.9997 |           0.2255 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0108 |          20.9401 |           0.2255 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0087 |          21.0630 |           0.2255 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0106 |          20.8725 |           0.2254 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0162 |          20.7180 |           0.2252 |
[32m[20221213 15:33:47 @agent_ppo2.py:185][0m |          -0.0172 |          20.6252 |           0.2256 |
[32m[20221213 15:33:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:33:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.25
[32m[20221213 15:33:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.82
[32m[20221213 15:33:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.12
[32m[20221213 15:33:48 @agent_ppo2.py:143][0m Total time:      40.87 min
[32m[20221213 15:33:48 @agent_ppo2.py:145][0m 3678208 total steps have happened
[32m[20221213 15:33:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1796 --------------------------#
[32m[20221213 15:33:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:48 @agent_ppo2.py:185][0m |          -0.0008 |          22.0480 |           0.2190 |
[32m[20221213 15:33:48 @agent_ppo2.py:185][0m |           0.0001 |          21.8196 |           0.2192 |
[32m[20221213 15:33:48 @agent_ppo2.py:185][0m |          -0.0081 |          21.4429 |           0.2190 |
[32m[20221213 15:33:48 @agent_ppo2.py:185][0m |          -0.0100 |          21.2942 |           0.2194 |
[32m[20221213 15:33:48 @agent_ppo2.py:185][0m |          -0.0103 |          21.1898 |           0.2196 |
[32m[20221213 15:33:48 @agent_ppo2.py:185][0m |          -0.0139 |          21.0650 |           0.2196 |
[32m[20221213 15:33:49 @agent_ppo2.py:185][0m |          -0.0129 |          20.9785 |           0.2200 |
[32m[20221213 15:33:49 @agent_ppo2.py:185][0m |           0.0009 |          22.6962 |           0.2198 |
[32m[20221213 15:33:49 @agent_ppo2.py:185][0m |          -0.0113 |          20.8629 |           0.2197 |
[32m[20221213 15:33:49 @agent_ppo2.py:185][0m |          -0.0174 |          20.7762 |           0.2198 |
[32m[20221213 15:33:49 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:33:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.80
[32m[20221213 15:33:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.73
[32m[20221213 15:33:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.18
[32m[20221213 15:33:49 @agent_ppo2.py:143][0m Total time:      40.90 min
[32m[20221213 15:33:49 @agent_ppo2.py:145][0m 3680256 total steps have happened
[32m[20221213 15:33:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1797 --------------------------#
[32m[20221213 15:33:49 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:33:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:49 @agent_ppo2.py:185][0m |           0.0023 |          22.4744 |           0.2261 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |           0.0040 |          23.3424 |           0.2254 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |          -0.0089 |          21.5843 |           0.2254 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |          -0.0100 |          21.3512 |           0.2255 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |           0.0059 |          23.4671 |           0.2255 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |          -0.0047 |          21.7547 |           0.2248 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |          -0.0086 |          21.3033 |           0.2254 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |          -0.0145 |          20.8659 |           0.2253 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |          -0.0139 |          20.7376 |           0.2253 |
[32m[20221213 15:33:50 @agent_ppo2.py:185][0m |          -0.0176 |          20.7369 |           0.2254 |
[32m[20221213 15:33:50 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:33:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.69
[32m[20221213 15:33:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.57
[32m[20221213 15:33:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.20
[32m[20221213 15:33:50 @agent_ppo2.py:143][0m Total time:      40.92 min
[32m[20221213 15:33:50 @agent_ppo2.py:145][0m 3682304 total steps have happened
[32m[20221213 15:33:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1798 --------------------------#
[32m[20221213 15:33:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:51 @agent_ppo2.py:185][0m |          -0.0042 |          22.9387 |           0.2255 |
[32m[20221213 15:33:51 @agent_ppo2.py:185][0m |          -0.0077 |          22.4012 |           0.2257 |
[32m[20221213 15:33:51 @agent_ppo2.py:185][0m |          -0.0011 |          25.1524 |           0.2258 |
[32m[20221213 15:33:51 @agent_ppo2.py:185][0m |          -0.0109 |          22.2423 |           0.2253 |
[32m[20221213 15:33:51 @agent_ppo2.py:185][0m |          -0.0141 |          22.1015 |           0.2254 |
[32m[20221213 15:33:51 @agent_ppo2.py:185][0m |          -0.0167 |          22.0999 |           0.2260 |
[32m[20221213 15:33:51 @agent_ppo2.py:185][0m |          -0.0150 |          22.0186 |           0.2258 |
[32m[20221213 15:33:52 @agent_ppo2.py:185][0m |          -0.0158 |          21.9722 |           0.2258 |
[32m[20221213 15:33:52 @agent_ppo2.py:185][0m |          -0.0177 |          21.9917 |           0.2258 |
[32m[20221213 15:33:52 @agent_ppo2.py:185][0m |          -0.0176 |          21.9335 |           0.2258 |
[32m[20221213 15:33:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:33:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.05
[32m[20221213 15:33:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.07
[32m[20221213 15:33:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.65
[32m[20221213 15:33:52 @agent_ppo2.py:143][0m Total time:      40.94 min
[32m[20221213 15:33:52 @agent_ppo2.py:145][0m 3684352 total steps have happened
[32m[20221213 15:33:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1799 --------------------------#
[32m[20221213 15:33:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:52 @agent_ppo2.py:185][0m |           0.0083 |          22.1285 |           0.2275 |
[32m[20221213 15:33:52 @agent_ppo2.py:185][0m |          -0.0058 |          19.9714 |           0.2275 |
[32m[20221213 15:33:52 @agent_ppo2.py:185][0m |           0.0015 |          21.5812 |           0.2274 |
[32m[20221213 15:33:53 @agent_ppo2.py:185][0m |          -0.0099 |          19.3116 |           0.2276 |
[32m[20221213 15:33:53 @agent_ppo2.py:185][0m |          -0.0095 |          19.0866 |           0.2273 |
[32m[20221213 15:33:53 @agent_ppo2.py:185][0m |          -0.0111 |          18.8972 |           0.2275 |
[32m[20221213 15:33:53 @agent_ppo2.py:185][0m |          -0.0138 |          18.7878 |           0.2275 |
[32m[20221213 15:33:53 @agent_ppo2.py:185][0m |          -0.0151 |          18.6295 |           0.2277 |
[32m[20221213 15:33:53 @agent_ppo2.py:185][0m |          -0.0155 |          18.5330 |           0.2275 |
[32m[20221213 15:33:53 @agent_ppo2.py:185][0m |          -0.0164 |          18.3521 |           0.2276 |
[32m[20221213 15:33:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:33:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.08
[32m[20221213 15:33:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 305.18
[32m[20221213 15:33:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.43
[32m[20221213 15:33:53 @agent_ppo2.py:143][0m Total time:      40.97 min
[32m[20221213 15:33:53 @agent_ppo2.py:145][0m 3686400 total steps have happened
[32m[20221213 15:33:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1800 --------------------------#
[32m[20221213 15:33:53 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:33:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0004 |          23.3304 |           0.2330 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0002 |          23.5201 |           0.2322 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0049 |          22.2818 |           0.2314 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0097 |          22.1542 |           0.2316 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0021 |          24.1894 |           0.2315 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0135 |          21.9455 |           0.2309 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0149 |          21.9005 |           0.2311 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0145 |          21.8211 |           0.2309 |
[32m[20221213 15:33:54 @agent_ppo2.py:185][0m |          -0.0145 |          21.7838 |           0.2306 |
[32m[20221213 15:33:55 @agent_ppo2.py:185][0m |          -0.0156 |          21.6970 |           0.2306 |
[32m[20221213 15:33:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:33:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.66
[32m[20221213 15:33:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.01
[32m[20221213 15:33:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.05
[32m[20221213 15:33:55 @agent_ppo2.py:143][0m Total time:      40.99 min
[32m[20221213 15:33:55 @agent_ppo2.py:145][0m 3688448 total steps have happened
[32m[20221213 15:33:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1801 --------------------------#
[32m[20221213 15:33:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:55 @agent_ppo2.py:185][0m |           0.0022 |          22.8555 |           0.2301 |
[32m[20221213 15:33:55 @agent_ppo2.py:185][0m |          -0.0099 |          22.1921 |           0.2294 |
[32m[20221213 15:33:55 @agent_ppo2.py:185][0m |          -0.0086 |          22.3347 |           0.2294 |
[32m[20221213 15:33:55 @agent_ppo2.py:185][0m |          -0.0129 |          21.8491 |           0.2295 |
[32m[20221213 15:33:55 @agent_ppo2.py:185][0m |          -0.0147 |          21.7373 |           0.2300 |
[32m[20221213 15:33:56 @agent_ppo2.py:185][0m |          -0.0165 |          21.6536 |           0.2294 |
[32m[20221213 15:33:56 @agent_ppo2.py:185][0m |          -0.0162 |          21.6829 |           0.2297 |
[32m[20221213 15:33:56 @agent_ppo2.py:185][0m |          -0.0174 |          21.5431 |           0.2300 |
[32m[20221213 15:33:56 @agent_ppo2.py:185][0m |          -0.0147 |          21.7151 |           0.2299 |
[32m[20221213 15:33:56 @agent_ppo2.py:185][0m |          -0.0180 |          21.4769 |           0.2297 |
[32m[20221213 15:33:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:33:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.94
[32m[20221213 15:33:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.25
[32m[20221213 15:33:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.02
[32m[20221213 15:33:56 @agent_ppo2.py:143][0m Total time:      41.02 min
[32m[20221213 15:33:56 @agent_ppo2.py:145][0m 3690496 total steps have happened
[32m[20221213 15:33:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1802 --------------------------#
[32m[20221213 15:33:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:33:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |           0.0003 |          23.9137 |           0.2280 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0048 |          23.6931 |           0.2274 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0048 |          23.6246 |           0.2268 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0113 |          23.2889 |           0.2268 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |           0.0016 |          25.0332 |           0.2268 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0107 |          23.2850 |           0.2264 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0147 |          23.0820 |           0.2263 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0160 |          23.0392 |           0.2263 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0152 |          22.9875 |           0.2262 |
[32m[20221213 15:33:57 @agent_ppo2.py:185][0m |          -0.0170 |          22.9512 |           0.2261 |
[32m[20221213 15:33:57 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:33:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.19
[32m[20221213 15:33:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 302.90
[32m[20221213 15:33:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.45
[32m[20221213 15:33:58 @agent_ppo2.py:143][0m Total time:      41.04 min
[32m[20221213 15:33:58 @agent_ppo2.py:145][0m 3692544 total steps have happened
[32m[20221213 15:33:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1803 --------------------------#
[32m[20221213 15:33:58 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:33:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:58 @agent_ppo2.py:185][0m |          -0.0032 |          22.6336 |           0.2295 |
[32m[20221213 15:33:58 @agent_ppo2.py:185][0m |           0.0051 |          25.5582 |           0.2289 |
[32m[20221213 15:33:58 @agent_ppo2.py:185][0m |          -0.0108 |          22.3314 |           0.2283 |
[32m[20221213 15:33:58 @agent_ppo2.py:185][0m |          -0.0125 |          22.1915 |           0.2285 |
[32m[20221213 15:33:58 @agent_ppo2.py:185][0m |          -0.0154 |          22.1251 |           0.2283 |
[32m[20221213 15:33:58 @agent_ppo2.py:185][0m |          -0.0083 |          22.7586 |           0.2284 |
[32m[20221213 15:33:59 @agent_ppo2.py:185][0m |          -0.0147 |          21.9685 |           0.2280 |
[32m[20221213 15:33:59 @agent_ppo2.py:185][0m |          -0.0188 |          21.9443 |           0.2280 |
[32m[20221213 15:33:59 @agent_ppo2.py:185][0m |          -0.0189 |          21.9078 |           0.2279 |
[32m[20221213 15:33:59 @agent_ppo2.py:185][0m |          -0.0182 |          21.8713 |           0.2276 |
[32m[20221213 15:33:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:33:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.63
[32m[20221213 15:33:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.92
[32m[20221213 15:33:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 312.03
[32m[20221213 15:33:59 @agent_ppo2.py:143][0m Total time:      41.06 min
[32m[20221213 15:33:59 @agent_ppo2.py:145][0m 3694592 total steps have happened
[32m[20221213 15:33:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1804 --------------------------#
[32m[20221213 15:33:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:33:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:33:59 @agent_ppo2.py:185][0m |          -0.0017 |          23.2115 |           0.2279 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0075 |          22.6525 |           0.2280 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0101 |          22.3415 |           0.2282 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0044 |          22.7036 |           0.2283 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0072 |          22.7065 |           0.2283 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0030 |          25.1336 |           0.2280 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0161 |          21.7755 |           0.2279 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0183 |          21.6644 |           0.2280 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0157 |          21.4711 |           0.2282 |
[32m[20221213 15:34:00 @agent_ppo2.py:185][0m |          -0.0191 |          21.3756 |           0.2281 |
[32m[20221213 15:34:00 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:34:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 247.32
[32m[20221213 15:34:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.36
[32m[20221213 15:34:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.26
[32m[20221213 15:34:01 @agent_ppo2.py:143][0m Total time:      41.09 min
[32m[20221213 15:34:01 @agent_ppo2.py:145][0m 3696640 total steps have happened
[32m[20221213 15:34:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1805 --------------------------#
[32m[20221213 15:34:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:01 @agent_ppo2.py:185][0m |          -0.0001 |          22.2860 |           0.2331 |
[32m[20221213 15:34:01 @agent_ppo2.py:185][0m |          -0.0085 |          21.8655 |           0.2323 |
[32m[20221213 15:34:01 @agent_ppo2.py:185][0m |          -0.0096 |          21.5889 |           0.2321 |
[32m[20221213 15:34:01 @agent_ppo2.py:185][0m |           0.0027 |          22.8230 |           0.2320 |
[32m[20221213 15:34:01 @agent_ppo2.py:185][0m |          -0.0134 |          21.1408 |           0.2320 |
[32m[20221213 15:34:01 @agent_ppo2.py:185][0m |          -0.0147 |          20.9938 |           0.2320 |
[32m[20221213 15:34:01 @agent_ppo2.py:185][0m |          -0.0169 |          20.7635 |           0.2319 |
[32m[20221213 15:34:02 @agent_ppo2.py:185][0m |          -0.0131 |          20.7074 |           0.2317 |
[32m[20221213 15:34:02 @agent_ppo2.py:185][0m |          -0.0089 |          21.0299 |           0.2317 |
[32m[20221213 15:34:02 @agent_ppo2.py:185][0m |          -0.0177 |          20.3324 |           0.2317 |
[32m[20221213 15:34:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:34:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.22
[32m[20221213 15:34:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.51
[32m[20221213 15:34:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.53
[32m[20221213 15:34:02 @agent_ppo2.py:143][0m Total time:      41.11 min
[32m[20221213 15:34:02 @agent_ppo2.py:145][0m 3698688 total steps have happened
[32m[20221213 15:34:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1806 --------------------------#
[32m[20221213 15:34:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:02 @agent_ppo2.py:185][0m |           0.0014 |          23.6930 |           0.2333 |
[32m[20221213 15:34:02 @agent_ppo2.py:185][0m |          -0.0115 |          23.0744 |           0.2326 |
[32m[20221213 15:34:02 @agent_ppo2.py:185][0m |          -0.0139 |          22.8868 |           0.2328 |
[32m[20221213 15:34:03 @agent_ppo2.py:185][0m |          -0.0144 |          22.7445 |           0.2330 |
[32m[20221213 15:34:03 @agent_ppo2.py:185][0m |          -0.0160 |          22.6102 |           0.2328 |
[32m[20221213 15:34:03 @agent_ppo2.py:185][0m |          -0.0171 |          22.5336 |           0.2329 |
[32m[20221213 15:34:03 @agent_ppo2.py:185][0m |          -0.0168 |          22.4260 |           0.2326 |
[32m[20221213 15:34:03 @agent_ppo2.py:185][0m |          -0.0159 |          22.3130 |           0.2328 |
[32m[20221213 15:34:03 @agent_ppo2.py:185][0m |          -0.0188 |          22.2818 |           0.2330 |
[32m[20221213 15:34:03 @agent_ppo2.py:185][0m |          -0.0172 |          22.2740 |           0.2325 |
[32m[20221213 15:34:03 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:34:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.50
[32m[20221213 15:34:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.26
[32m[20221213 15:34:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.45
[32m[20221213 15:34:03 @agent_ppo2.py:143][0m Total time:      41.14 min
[32m[20221213 15:34:03 @agent_ppo2.py:145][0m 3700736 total steps have happened
[32m[20221213 15:34:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1807 --------------------------#
[32m[20221213 15:34:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0021 |          23.0095 |           0.2291 |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0049 |          22.6943 |           0.2290 |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0071 |          22.6169 |           0.2287 |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0036 |          24.6335 |           0.2292 |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0083 |          22.9215 |           0.2288 |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0157 |          22.1999 |           0.2292 |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0155 |          22.0751 |           0.2293 |
[32m[20221213 15:34:04 @agent_ppo2.py:185][0m |          -0.0179 |          21.9796 |           0.2294 |
[32m[20221213 15:34:05 @agent_ppo2.py:185][0m |          -0.0167 |          21.9824 |           0.2297 |
[32m[20221213 15:34:05 @agent_ppo2.py:185][0m |          -0.0193 |          21.9177 |           0.2296 |
[32m[20221213 15:34:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:34:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.82
[32m[20221213 15:34:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.26
[32m[20221213 15:34:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.98
[32m[20221213 15:34:05 @agent_ppo2.py:143][0m Total time:      41.16 min
[32m[20221213 15:34:05 @agent_ppo2.py:145][0m 3702784 total steps have happened
[32m[20221213 15:34:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1808 --------------------------#
[32m[20221213 15:34:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:05 @agent_ppo2.py:185][0m |           0.0004 |          23.0197 |           0.2330 |
[32m[20221213 15:34:05 @agent_ppo2.py:185][0m |           0.0017 |          22.9818 |           0.2324 |
[32m[20221213 15:34:05 @agent_ppo2.py:185][0m |          -0.0054 |          22.5010 |           0.2323 |
[32m[20221213 15:34:05 @agent_ppo2.py:185][0m |          -0.0093 |          22.4042 |           0.2321 |
[32m[20221213 15:34:06 @agent_ppo2.py:185][0m |          -0.0110 |          22.3172 |           0.2320 |
[32m[20221213 15:34:06 @agent_ppo2.py:185][0m |          -0.0042 |          22.6870 |           0.2319 |
[32m[20221213 15:34:06 @agent_ppo2.py:185][0m |          -0.0125 |          22.2371 |           0.2319 |
[32m[20221213 15:34:06 @agent_ppo2.py:185][0m |          -0.0049 |          22.8069 |           0.2318 |
[32m[20221213 15:34:06 @agent_ppo2.py:185][0m |          -0.0125 |          22.1606 |           0.2315 |
[32m[20221213 15:34:06 @agent_ppo2.py:185][0m |          -0.0131 |          22.1110 |           0.2320 |
[32m[20221213 15:34:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:34:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.87
[32m[20221213 15:34:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.94
[32m[20221213 15:34:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.21
[32m[20221213 15:34:06 @agent_ppo2.py:143][0m Total time:      41.18 min
[32m[20221213 15:34:06 @agent_ppo2.py:145][0m 3704832 total steps have happened
[32m[20221213 15:34:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1809 --------------------------#
[32m[20221213 15:34:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |           0.0068 |          24.2783 |           0.2311 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |           0.0035 |          24.9975 |           0.2300 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0114 |          21.5892 |           0.2294 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0162 |          21.2605 |           0.2295 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0149 |          21.0174 |           0.2297 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0189 |          20.9438 |           0.2295 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0214 |          20.8436 |           0.2295 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0088 |          22.4108 |           0.2292 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0188 |          20.5821 |           0.2289 |
[32m[20221213 15:34:07 @agent_ppo2.py:185][0m |          -0.0201 |          20.4860 |           0.2291 |
[32m[20221213 15:34:07 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:34:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.43
[32m[20221213 15:34:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.47
[32m[20221213 15:34:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.57
[32m[20221213 15:34:08 @agent_ppo2.py:143][0m Total time:      41.21 min
[32m[20221213 15:34:08 @agent_ppo2.py:145][0m 3706880 total steps have happened
[32m[20221213 15:34:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1810 --------------------------#
[32m[20221213 15:34:08 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:34:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:08 @agent_ppo2.py:185][0m |           0.0008 |          23.0295 |           0.2326 |
[32m[20221213 15:34:08 @agent_ppo2.py:185][0m |          -0.0093 |          22.6057 |           0.2327 |
[32m[20221213 15:34:08 @agent_ppo2.py:185][0m |          -0.0087 |          22.3881 |           0.2329 |
[32m[20221213 15:34:08 @agent_ppo2.py:185][0m |          -0.0142 |          22.2234 |           0.2329 |
[32m[20221213 15:34:08 @agent_ppo2.py:185][0m |          -0.0156 |          22.1445 |           0.2331 |
[32m[20221213 15:34:09 @agent_ppo2.py:185][0m |          -0.0159 |          21.9855 |           0.2331 |
[32m[20221213 15:34:09 @agent_ppo2.py:185][0m |          -0.0093 |          22.4745 |           0.2331 |
[32m[20221213 15:34:09 @agent_ppo2.py:185][0m |          -0.0182 |          21.8155 |           0.2334 |
[32m[20221213 15:34:09 @agent_ppo2.py:185][0m |          -0.0193 |          21.6874 |           0.2332 |
[32m[20221213 15:34:09 @agent_ppo2.py:185][0m |          -0.0202 |          21.6353 |           0.2331 |
[32m[20221213 15:34:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:34:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.93
[32m[20221213 15:34:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.38
[32m[20221213 15:34:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.31
[32m[20221213 15:34:09 @agent_ppo2.py:143][0m Total time:      41.23 min
[32m[20221213 15:34:09 @agent_ppo2.py:145][0m 3708928 total steps have happened
[32m[20221213 15:34:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1811 --------------------------#
[32m[20221213 15:34:09 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:34:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:09 @agent_ppo2.py:185][0m |          -0.0034 |          23.3388 |           0.2302 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0039 |          23.1973 |           0.2297 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0036 |          23.6535 |           0.2296 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0026 |          23.9043 |           0.2295 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0109 |          22.7822 |           0.2293 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0085 |          22.9084 |           0.2298 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0138 |          22.6458 |           0.2298 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0141 |          22.5734 |           0.2298 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0082 |          22.9335 |           0.2298 |
[32m[20221213 15:34:10 @agent_ppo2.py:185][0m |          -0.0141 |          22.5067 |           0.2296 |
[32m[20221213 15:34:10 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:34:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 289.01
[32m[20221213 15:34:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.20
[32m[20221213 15:34:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.26
[32m[20221213 15:34:11 @agent_ppo2.py:143][0m Total time:      41.26 min
[32m[20221213 15:34:11 @agent_ppo2.py:145][0m 3710976 total steps have happened
[32m[20221213 15:34:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1812 --------------------------#
[32m[20221213 15:34:11 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:34:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:11 @agent_ppo2.py:185][0m |           0.0051 |          24.7745 |           0.2304 |
[32m[20221213 15:34:11 @agent_ppo2.py:185][0m |          -0.0073 |          24.0800 |           0.2301 |
[32m[20221213 15:34:11 @agent_ppo2.py:185][0m |          -0.0032 |          25.1336 |           0.2298 |
[32m[20221213 15:34:11 @agent_ppo2.py:185][0m |          -0.0131 |          23.8022 |           0.2296 |
[32m[20221213 15:34:11 @agent_ppo2.py:185][0m |          -0.0132 |          23.6894 |           0.2293 |
[32m[20221213 15:34:11 @agent_ppo2.py:185][0m |          -0.0153 |          23.6340 |           0.2293 |
[32m[20221213 15:34:12 @agent_ppo2.py:185][0m |          -0.0190 |          23.5449 |           0.2292 |
[32m[20221213 15:34:12 @agent_ppo2.py:185][0m |          -0.0151 |          23.4491 |           0.2287 |
[32m[20221213 15:34:12 @agent_ppo2.py:185][0m |          -0.0181 |          23.4187 |           0.2287 |
[32m[20221213 15:34:12 @agent_ppo2.py:185][0m |          -0.0180 |          23.3738 |           0.2286 |
[32m[20221213 15:34:12 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:34:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 241.78
[32m[20221213 15:34:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 266.17
[32m[20221213 15:34:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.35
[32m[20221213 15:34:12 @agent_ppo2.py:143][0m Total time:      41.28 min
[32m[20221213 15:34:12 @agent_ppo2.py:145][0m 3713024 total steps have happened
[32m[20221213 15:34:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1813 --------------------------#
[32m[20221213 15:34:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:12 @agent_ppo2.py:185][0m |           0.0027 |          23.0484 |           0.2340 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0057 |          22.5897 |           0.2332 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0090 |          22.2727 |           0.2335 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0150 |          22.1149 |           0.2336 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0125 |          21.8968 |           0.2335 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0158 |          21.7580 |           0.2337 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0166 |          21.6187 |           0.2338 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0166 |          21.5038 |           0.2338 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0162 |          21.3830 |           0.2336 |
[32m[20221213 15:34:13 @agent_ppo2.py:185][0m |          -0.0163 |          21.2489 |           0.2338 |
[32m[20221213 15:34:13 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221213 15:34:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.41
[32m[20221213 15:34:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.79
[32m[20221213 15:34:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.90
[32m[20221213 15:34:14 @agent_ppo2.py:143][0m Total time:      41.31 min
[32m[20221213 15:34:14 @agent_ppo2.py:145][0m 3715072 total steps have happened
[32m[20221213 15:34:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1814 --------------------------#
[32m[20221213 15:34:14 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:34:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:14 @agent_ppo2.py:185][0m |           0.0002 |          23.3090 |           0.2305 |
[32m[20221213 15:34:14 @agent_ppo2.py:185][0m |          -0.0031 |          22.8157 |           0.2308 |
[32m[20221213 15:34:14 @agent_ppo2.py:185][0m |          -0.0082 |          22.4782 |           0.2305 |
[32m[20221213 15:34:14 @agent_ppo2.py:185][0m |          -0.0109 |          22.2690 |           0.2304 |
[32m[20221213 15:34:14 @agent_ppo2.py:185][0m |          -0.0129 |          22.1244 |           0.2304 |
[32m[20221213 15:34:15 @agent_ppo2.py:185][0m |          -0.0149 |          22.0314 |           0.2304 |
[32m[20221213 15:34:15 @agent_ppo2.py:185][0m |          -0.0154 |          21.9730 |           0.2305 |
[32m[20221213 15:34:15 @agent_ppo2.py:185][0m |          -0.0160 |          21.8840 |           0.2304 |
[32m[20221213 15:34:15 @agent_ppo2.py:185][0m |          -0.0160 |          21.8414 |           0.2305 |
[32m[20221213 15:34:15 @agent_ppo2.py:185][0m |          -0.0188 |          21.8052 |           0.2306 |
[32m[20221213 15:34:15 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221213 15:34:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.80
[32m[20221213 15:34:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 303.82
[32m[20221213 15:34:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.06
[32m[20221213 15:34:15 @agent_ppo2.py:143][0m Total time:      41.33 min
[32m[20221213 15:34:15 @agent_ppo2.py:145][0m 3717120 total steps have happened
[32m[20221213 15:34:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1815 --------------------------#
[32m[20221213 15:34:15 @agent_ppo2.py:127][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 15:34:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0006 |          23.3948 |           0.2251 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |           0.0013 |          24.8806 |           0.2248 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0091 |          22.7159 |           0.2247 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0064 |          22.7527 |           0.2257 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0141 |          22.4575 |           0.2255 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0124 |          22.4133 |           0.2256 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0149 |          22.3722 |           0.2256 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0172 |          22.2403 |           0.2261 |
[32m[20221213 15:34:16 @agent_ppo2.py:185][0m |          -0.0179 |          22.1726 |           0.2260 |
[32m[20221213 15:34:17 @agent_ppo2.py:185][0m |          -0.0187 |          22.1032 |           0.2261 |
[32m[20221213 15:34:17 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:34:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.55
[32m[20221213 15:34:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.38
[32m[20221213 15:34:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.89
[32m[20221213 15:34:17 @agent_ppo2.py:143][0m Total time:      41.36 min
[32m[20221213 15:34:17 @agent_ppo2.py:145][0m 3719168 total steps have happened
[32m[20221213 15:34:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1816 --------------------------#
[32m[20221213 15:34:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:17 @agent_ppo2.py:185][0m |           0.0003 |          23.9548 |           0.2438 |
[32m[20221213 15:34:17 @agent_ppo2.py:185][0m |          -0.0047 |          23.4146 |           0.2424 |
[32m[20221213 15:34:17 @agent_ppo2.py:185][0m |          -0.0102 |          23.2070 |           0.2422 |
[32m[20221213 15:34:17 @agent_ppo2.py:185][0m |          -0.0110 |          23.0929 |           0.2421 |
[32m[20221213 15:34:18 @agent_ppo2.py:185][0m |          -0.0125 |          22.9905 |           0.2420 |
[32m[20221213 15:34:18 @agent_ppo2.py:185][0m |          -0.0135 |          22.9094 |           0.2418 |
[32m[20221213 15:34:18 @agent_ppo2.py:185][0m |          -0.0141 |          22.8697 |           0.2414 |
[32m[20221213 15:34:18 @agent_ppo2.py:185][0m |           0.0015 |          25.8470 |           0.2415 |
[32m[20221213 15:34:18 @agent_ppo2.py:185][0m |          -0.0140 |          22.8009 |           0.2416 |
[32m[20221213 15:34:18 @agent_ppo2.py:185][0m |          -0.0149 |          22.7180 |           0.2412 |
[32m[20221213 15:34:18 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:34:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.94
[32m[20221213 15:34:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.30
[32m[20221213 15:34:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.98
[32m[20221213 15:34:18 @agent_ppo2.py:143][0m Total time:      41.38 min
[32m[20221213 15:34:18 @agent_ppo2.py:145][0m 3721216 total steps have happened
[32m[20221213 15:34:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1817 --------------------------#
[32m[20221213 15:34:18 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:34:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |           0.0079 |          25.7797 |           0.2323 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |          -0.0071 |          24.2367 |           0.2318 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |          -0.0111 |          23.9537 |           0.2315 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |          -0.0138 |          23.7768 |           0.2314 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |          -0.0103 |          24.0005 |           0.2311 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |           0.0018 |          26.3359 |           0.2309 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |          -0.0153 |          23.6509 |           0.2301 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |          -0.0150 |          23.5237 |           0.2303 |
[32m[20221213 15:34:19 @agent_ppo2.py:185][0m |          -0.0153 |          23.4540 |           0.2304 |
[32m[20221213 15:34:20 @agent_ppo2.py:185][0m |          -0.0105 |          23.9617 |           0.2302 |
[32m[20221213 15:34:20 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:34:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.67
[32m[20221213 15:34:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.59
[32m[20221213 15:34:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.70
[32m[20221213 15:34:20 @agent_ppo2.py:143][0m Total time:      41.41 min
[32m[20221213 15:34:20 @agent_ppo2.py:145][0m 3723264 total steps have happened
[32m[20221213 15:34:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1818 --------------------------#
[32m[20221213 15:34:20 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:34:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:20 @agent_ppo2.py:185][0m |           0.0014 |          22.7523 |           0.2336 |
[32m[20221213 15:34:20 @agent_ppo2.py:185][0m |          -0.0063 |          21.9727 |           0.2331 |
[32m[20221213 15:34:20 @agent_ppo2.py:185][0m |          -0.0108 |          21.5406 |           0.2331 |
[32m[20221213 15:34:20 @agent_ppo2.py:185][0m |          -0.0142 |          21.2222 |           0.2328 |
[32m[20221213 15:34:21 @agent_ppo2.py:185][0m |          -0.0086 |          21.7066 |           0.2324 |
[32m[20221213 15:34:21 @agent_ppo2.py:185][0m |          -0.0157 |          20.8045 |           0.2321 |
[32m[20221213 15:34:21 @agent_ppo2.py:185][0m |          -0.0131 |          20.7993 |           0.2320 |
[32m[20221213 15:34:21 @agent_ppo2.py:185][0m |          -0.0172 |          20.4630 |           0.2315 |
[32m[20221213 15:34:21 @agent_ppo2.py:185][0m |          -0.0192 |          20.3635 |           0.2315 |
[32m[20221213 15:34:21 @agent_ppo2.py:185][0m |          -0.0186 |          20.2357 |           0.2312 |
[32m[20221213 15:34:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:34:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.64
[32m[20221213 15:34:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 310.53
[32m[20221213 15:34:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.33
[32m[20221213 15:34:21 @agent_ppo2.py:143][0m Total time:      41.43 min
[32m[20221213 15:34:21 @agent_ppo2.py:145][0m 3725312 total steps have happened
[32m[20221213 15:34:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1819 --------------------------#
[32m[20221213 15:34:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0014 |          22.9390 |           0.2334 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0084 |          22.5512 |           0.2334 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0089 |          22.4625 |           0.2333 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0124 |          22.1935 |           0.2333 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0143 |          22.0644 |           0.2333 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0140 |          21.9637 |           0.2332 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0171 |          21.8945 |           0.2333 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0183 |          21.7969 |           0.2334 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0197 |          21.7925 |           0.2335 |
[32m[20221213 15:34:22 @agent_ppo2.py:185][0m |          -0.0204 |          21.6925 |           0.2335 |
[32m[20221213 15:34:22 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:34:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.82
[32m[20221213 15:34:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.57
[32m[20221213 15:34:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.61
[32m[20221213 15:34:23 @agent_ppo2.py:143][0m Total time:      41.46 min
[32m[20221213 15:34:23 @agent_ppo2.py:145][0m 3727360 total steps have happened
[32m[20221213 15:34:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1820 --------------------------#
[32m[20221213 15:34:23 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:34:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:23 @agent_ppo2.py:185][0m |          -0.0018 |          22.7962 |           0.2253 |
[32m[20221213 15:34:23 @agent_ppo2.py:185][0m |           0.0207 |          27.1371 |           0.2252 |
[32m[20221213 15:34:23 @agent_ppo2.py:185][0m |          -0.0112 |          21.8864 |           0.2245 |
[32m[20221213 15:34:23 @agent_ppo2.py:185][0m |          -0.0135 |          21.5601 |           0.2247 |
[32m[20221213 15:34:23 @agent_ppo2.py:185][0m |          -0.0160 |          21.2893 |           0.2248 |
[32m[20221213 15:34:23 @agent_ppo2.py:185][0m |          -0.0173 |          21.1164 |           0.2249 |
[32m[20221213 15:34:24 @agent_ppo2.py:185][0m |          -0.0182 |          20.9710 |           0.2248 |
[32m[20221213 15:34:24 @agent_ppo2.py:185][0m |          -0.0174 |          20.8664 |           0.2247 |
[32m[20221213 15:34:24 @agent_ppo2.py:185][0m |          -0.0193 |          20.6757 |           0.2245 |
[32m[20221213 15:34:24 @agent_ppo2.py:185][0m |          -0.0169 |          21.0437 |           0.2245 |
[32m[20221213 15:34:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:34:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.17
[32m[20221213 15:34:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.41
[32m[20221213 15:34:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.98
[32m[20221213 15:34:24 @agent_ppo2.py:143][0m Total time:      41.48 min
[32m[20221213 15:34:24 @agent_ppo2.py:145][0m 3729408 total steps have happened
[32m[20221213 15:34:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1821 --------------------------#
[32m[20221213 15:34:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:24 @agent_ppo2.py:185][0m |           0.0034 |          25.9598 |           0.2280 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0021 |          25.0602 |           0.2274 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0034 |          26.5533 |           0.2273 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0074 |          25.2716 |           0.2270 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0153 |          23.9788 |           0.2268 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0177 |          23.8366 |           0.2272 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0139 |          23.7799 |           0.2273 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0164 |          23.6114 |           0.2272 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0185 |          23.4644 |           0.2276 |
[32m[20221213 15:34:25 @agent_ppo2.py:185][0m |          -0.0190 |          23.4154 |           0.2274 |
[32m[20221213 15:34:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:34:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.47
[32m[20221213 15:34:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.97
[32m[20221213 15:34:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.64
[32m[20221213 15:34:25 @agent_ppo2.py:143][0m Total time:      41.50 min
[32m[20221213 15:34:25 @agent_ppo2.py:145][0m 3731456 total steps have happened
[32m[20221213 15:34:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1822 --------------------------#
[32m[20221213 15:34:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0038 |          23.9678 |           0.2343 |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0079 |          23.6263 |           0.2338 |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0014 |          24.7709 |           0.2335 |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0116 |          23.4031 |           0.2333 |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0131 |          23.3238 |           0.2338 |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0126 |          23.2199 |           0.2334 |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0125 |          23.2461 |           0.2340 |
[32m[20221213 15:34:26 @agent_ppo2.py:185][0m |          -0.0162 |          23.1061 |           0.2338 |
[32m[20221213 15:34:27 @agent_ppo2.py:185][0m |          -0.0181 |          23.0358 |           0.2336 |
[32m[20221213 15:34:27 @agent_ppo2.py:185][0m |          -0.0179 |          22.9568 |           0.2339 |
[32m[20221213 15:34:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:34:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.15
[32m[20221213 15:34:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.75
[32m[20221213 15:34:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 252.77
[32m[20221213 15:34:27 @agent_ppo2.py:143][0m Total time:      41.53 min
[32m[20221213 15:34:27 @agent_ppo2.py:145][0m 3733504 total steps have happened
[32m[20221213 15:34:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1823 --------------------------#
[32m[20221213 15:34:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:27 @agent_ppo2.py:185][0m |           0.0006 |          22.8211 |           0.2266 |
[32m[20221213 15:34:27 @agent_ppo2.py:185][0m |          -0.0033 |          22.2218 |           0.2256 |
[32m[20221213 15:34:27 @agent_ppo2.py:185][0m |          -0.0022 |          23.2259 |           0.2255 |
[32m[20221213 15:34:27 @agent_ppo2.py:185][0m |          -0.0106 |          21.3117 |           0.2249 |
[32m[20221213 15:34:28 @agent_ppo2.py:185][0m |          -0.0126 |          20.9847 |           0.2250 |
[32m[20221213 15:34:28 @agent_ppo2.py:185][0m |          -0.0160 |          20.7902 |           0.2253 |
[32m[20221213 15:34:28 @agent_ppo2.py:185][0m |          -0.0165 |          20.5667 |           0.2250 |
[32m[20221213 15:34:28 @agent_ppo2.py:185][0m |           0.0074 |          25.7249 |           0.2250 |
[32m[20221213 15:34:28 @agent_ppo2.py:185][0m |          -0.0175 |          20.3649 |           0.2247 |
[32m[20221213 15:34:28 @agent_ppo2.py:185][0m |          -0.0188 |          20.0008 |           0.2250 |
[32m[20221213 15:34:28 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:34:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.70
[32m[20221213 15:34:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.86
[32m[20221213 15:34:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.95
[32m[20221213 15:34:28 @agent_ppo2.py:143][0m Total time:      41.55 min
[32m[20221213 15:34:28 @agent_ppo2.py:145][0m 3735552 total steps have happened
[32m[20221213 15:34:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1824 --------------------------#
[32m[20221213 15:34:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0010 |          25.1062 |           0.2295 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0089 |          23.7915 |           0.2289 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0134 |          23.2896 |           0.2288 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0148 |          22.8725 |           0.2286 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0164 |          22.6022 |           0.2290 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0201 |          22.3983 |           0.2288 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0183 |          22.2238 |           0.2287 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0228 |          22.1003 |           0.2287 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0214 |          21.9405 |           0.2286 |
[32m[20221213 15:34:29 @agent_ppo2.py:185][0m |          -0.0222 |          21.8100 |           0.2289 |
[32m[20221213 15:34:29 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:34:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.35
[32m[20221213 15:34:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.15
[32m[20221213 15:34:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.46
[32m[20221213 15:34:30 @agent_ppo2.py:143][0m Total time:      41.57 min
[32m[20221213 15:34:30 @agent_ppo2.py:145][0m 3737600 total steps have happened
[32m[20221213 15:34:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1825 --------------------------#
[32m[20221213 15:34:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:30 @agent_ppo2.py:185][0m |           0.0083 |          26.8037 |           0.2331 |
[32m[20221213 15:34:30 @agent_ppo2.py:185][0m |          -0.0066 |          24.6389 |           0.2326 |
[32m[20221213 15:34:30 @agent_ppo2.py:185][0m |          -0.0122 |          24.3369 |           0.2326 |
[32m[20221213 15:34:30 @agent_ppo2.py:185][0m |          -0.0119 |          24.2145 |           0.2328 |
[32m[20221213 15:34:30 @agent_ppo2.py:185][0m |          -0.0141 |          24.0982 |           0.2327 |
[32m[20221213 15:34:30 @agent_ppo2.py:185][0m |          -0.0172 |          24.0291 |           0.2327 |
[32m[20221213 15:34:31 @agent_ppo2.py:185][0m |          -0.0126 |          24.0582 |           0.2321 |
[32m[20221213 15:34:31 @agent_ppo2.py:185][0m |          -0.0160 |          23.8516 |           0.2324 |
[32m[20221213 15:34:31 @agent_ppo2.py:185][0m |          -0.0005 |          27.4514 |           0.2324 |
[32m[20221213 15:34:31 @agent_ppo2.py:185][0m |          -0.0174 |          23.7956 |           0.2321 |
[32m[20221213 15:34:31 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:34:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.94
[32m[20221213 15:34:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.88
[32m[20221213 15:34:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.41
[32m[20221213 15:34:31 @agent_ppo2.py:143][0m Total time:      41.60 min
[32m[20221213 15:34:31 @agent_ppo2.py:145][0m 3739648 total steps have happened
[32m[20221213 15:34:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1826 --------------------------#
[32m[20221213 15:34:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:31 @agent_ppo2.py:185][0m |          -0.0006 |          23.7175 |           0.2251 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0068 |          23.2705 |           0.2249 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0100 |          23.0377 |           0.2244 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0120 |          22.8674 |           0.2243 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0155 |          22.7649 |           0.2241 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0152 |          22.6847 |           0.2238 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0158 |          22.5374 |           0.2237 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0155 |          22.4246 |           0.2236 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0046 |          24.6003 |           0.2237 |
[32m[20221213 15:34:32 @agent_ppo2.py:185][0m |          -0.0180 |          22.2358 |           0.2234 |
[32m[20221213 15:34:32 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:34:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.48
[32m[20221213 15:34:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.69
[32m[20221213 15:34:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.84
[32m[20221213 15:34:33 @agent_ppo2.py:143][0m Total time:      41.62 min
[32m[20221213 15:34:33 @agent_ppo2.py:145][0m 3741696 total steps have happened
[32m[20221213 15:34:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1827 --------------------------#
[32m[20221213 15:34:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:33 @agent_ppo2.py:185][0m |           0.0015 |          23.1438 |           0.2333 |
[32m[20221213 15:34:33 @agent_ppo2.py:185][0m |           0.0042 |          24.7783 |           0.2330 |
[32m[20221213 15:34:33 @agent_ppo2.py:185][0m |          -0.0036 |          22.7132 |           0.2327 |
[32m[20221213 15:34:33 @agent_ppo2.py:185][0m |          -0.0110 |          22.5288 |           0.2328 |
[32m[20221213 15:34:33 @agent_ppo2.py:185][0m |          -0.0132 |          22.4811 |           0.2328 |
[32m[20221213 15:34:33 @agent_ppo2.py:185][0m |          -0.0046 |          23.1483 |           0.2327 |
[32m[20221213 15:34:33 @agent_ppo2.py:185][0m |          -0.0149 |          22.3900 |           0.2327 |
[32m[20221213 15:34:34 @agent_ppo2.py:185][0m |          -0.0156 |          22.3837 |           0.2331 |
[32m[20221213 15:34:34 @agent_ppo2.py:185][0m |          -0.0149 |          22.3591 |           0.2329 |
[32m[20221213 15:34:34 @agent_ppo2.py:185][0m |          -0.0147 |          22.3327 |           0.2329 |
[32m[20221213 15:34:34 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:34:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.92
[32m[20221213 15:34:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.08
[32m[20221213 15:34:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.90
[32m[20221213 15:34:34 @agent_ppo2.py:143][0m Total time:      41.64 min
[32m[20221213 15:34:34 @agent_ppo2.py:145][0m 3743744 total steps have happened
[32m[20221213 15:34:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1828 --------------------------#
[32m[20221213 15:34:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:34 @agent_ppo2.py:185][0m |          -0.0031 |          23.1110 |           0.2285 |
[32m[20221213 15:34:34 @agent_ppo2.py:185][0m |          -0.0071 |          22.8474 |           0.2279 |
[32m[20221213 15:34:34 @agent_ppo2.py:185][0m |          -0.0111 |          22.7555 |           0.2278 |
[32m[20221213 15:34:35 @agent_ppo2.py:185][0m |          -0.0109 |          22.6541 |           0.2279 |
[32m[20221213 15:34:35 @agent_ppo2.py:185][0m |          -0.0148 |          22.5835 |           0.2278 |
[32m[20221213 15:34:35 @agent_ppo2.py:185][0m |          -0.0162 |          22.5024 |           0.2279 |
[32m[20221213 15:34:35 @agent_ppo2.py:185][0m |          -0.0167 |          22.4390 |           0.2277 |
[32m[20221213 15:34:35 @agent_ppo2.py:185][0m |          -0.0076 |          23.5189 |           0.2276 |
[32m[20221213 15:34:35 @agent_ppo2.py:185][0m |          -0.0160 |          22.3287 |           0.2276 |
[32m[20221213 15:34:35 @agent_ppo2.py:185][0m |          -0.0183 |          22.2225 |           0.2273 |
[32m[20221213 15:34:35 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:34:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.60
[32m[20221213 15:34:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.34
[32m[20221213 15:34:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.15
[32m[20221213 15:34:35 @agent_ppo2.py:143][0m Total time:      41.67 min
[32m[20221213 15:34:35 @agent_ppo2.py:145][0m 3745792 total steps have happened
[32m[20221213 15:34:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1829 --------------------------#
[32m[20221213 15:34:36 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0007 |          23.8689 |           0.2315 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0074 |          23.3726 |           0.2312 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0104 |          23.1731 |           0.2310 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0036 |          23.4325 |           0.2312 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0004 |          24.7740 |           0.2312 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |           0.0105 |          27.7502 |           0.2306 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0010 |          24.6666 |           0.2305 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0134 |          22.6836 |           0.2308 |
[32m[20221213 15:34:36 @agent_ppo2.py:185][0m |          -0.0169 |          22.5470 |           0.2307 |
[32m[20221213 15:34:37 @agent_ppo2.py:185][0m |          -0.0164 |          22.5001 |           0.2307 |
[32m[20221213 15:34:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:34:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.52
[32m[20221213 15:34:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.71
[32m[20221213 15:34:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 249.46
[32m[20221213 15:34:37 @agent_ppo2.py:143][0m Total time:      41.69 min
[32m[20221213 15:34:37 @agent_ppo2.py:145][0m 3747840 total steps have happened
[32m[20221213 15:34:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1830 --------------------------#
[32m[20221213 15:34:37 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:34:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:37 @agent_ppo2.py:185][0m |          -0.0025 |          23.5886 |           0.2223 |
[32m[20221213 15:34:37 @agent_ppo2.py:185][0m |          -0.0071 |          23.2561 |           0.2219 |
[32m[20221213 15:34:37 @agent_ppo2.py:185][0m |          -0.0094 |          23.1901 |           0.2215 |
[32m[20221213 15:34:37 @agent_ppo2.py:185][0m |          -0.0114 |          23.0972 |           0.2218 |
[32m[20221213 15:34:37 @agent_ppo2.py:185][0m |          -0.0092 |          23.0015 |           0.2217 |
[32m[20221213 15:34:38 @agent_ppo2.py:185][0m |          -0.0130 |          22.9322 |           0.2215 |
[32m[20221213 15:34:38 @agent_ppo2.py:185][0m |          -0.0142 |          22.8521 |           0.2214 |
[32m[20221213 15:34:38 @agent_ppo2.py:185][0m |          -0.0147 |          22.8353 |           0.2209 |
[32m[20221213 15:34:38 @agent_ppo2.py:185][0m |          -0.0140 |          22.8046 |           0.2212 |
[32m[20221213 15:34:38 @agent_ppo2.py:185][0m |          -0.0094 |          23.5883 |           0.2212 |
[32m[20221213 15:34:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:34:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.78
[32m[20221213 15:34:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.59
[32m[20221213 15:34:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.23
[32m[20221213 15:34:38 @agent_ppo2.py:143][0m Total time:      41.72 min
[32m[20221213 15:34:38 @agent_ppo2.py:145][0m 3749888 total steps have happened
[32m[20221213 15:34:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1831 --------------------------#
[32m[20221213 15:34:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |           0.0017 |          22.7328 |           0.2239 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |           0.0080 |          24.9087 |           0.2238 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |          -0.0042 |          21.7454 |           0.2239 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |          -0.0017 |          22.1163 |           0.2239 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |          -0.0082 |          21.2978 |           0.2241 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |          -0.0106 |          21.1392 |           0.2242 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |          -0.0095 |          21.0959 |           0.2242 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |           0.0005 |          24.1206 |           0.2245 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |          -0.0046 |          21.7163 |           0.2240 |
[32m[20221213 15:34:39 @agent_ppo2.py:185][0m |          -0.0082 |          21.1685 |           0.2241 |
[32m[20221213 15:34:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:34:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.00
[32m[20221213 15:34:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.41
[32m[20221213 15:34:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.92
[32m[20221213 15:34:40 @agent_ppo2.py:143][0m Total time:      41.74 min
[32m[20221213 15:34:40 @agent_ppo2.py:145][0m 3751936 total steps have happened
[32m[20221213 15:34:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1832 --------------------------#
[32m[20221213 15:34:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:40 @agent_ppo2.py:185][0m |           0.0191 |          28.0358 |           0.2273 |
[32m[20221213 15:34:40 @agent_ppo2.py:185][0m |          -0.0080 |          23.2424 |           0.2264 |
[32m[20221213 15:34:40 @agent_ppo2.py:185][0m |          -0.0115 |          22.9302 |           0.2263 |
[32m[20221213 15:34:40 @agent_ppo2.py:185][0m |          -0.0118 |          22.7471 |           0.2263 |
[32m[20221213 15:34:40 @agent_ppo2.py:185][0m |          -0.0034 |          25.3897 |           0.2265 |
[32m[20221213 15:34:40 @agent_ppo2.py:185][0m |          -0.0163 |          22.5276 |           0.2263 |
[32m[20221213 15:34:40 @agent_ppo2.py:185][0m |          -0.0070 |          23.0333 |           0.2263 |
[32m[20221213 15:34:41 @agent_ppo2.py:185][0m |          -0.0151 |          22.3308 |           0.2261 |
[32m[20221213 15:34:41 @agent_ppo2.py:185][0m |          -0.0174 |          22.3094 |           0.2261 |
[32m[20221213 15:34:41 @agent_ppo2.py:185][0m |          -0.0174 |          22.2271 |           0.2262 |
[32m[20221213 15:34:41 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:34:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.15
[32m[20221213 15:34:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.17
[32m[20221213 15:34:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.03
[32m[20221213 15:34:41 @agent_ppo2.py:143][0m Total time:      41.76 min
[32m[20221213 15:34:41 @agent_ppo2.py:145][0m 3753984 total steps have happened
[32m[20221213 15:34:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1833 --------------------------#
[32m[20221213 15:34:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:41 @agent_ppo2.py:185][0m |          -0.0049 |          22.7386 |           0.2251 |
[32m[20221213 15:34:41 @agent_ppo2.py:185][0m |          -0.0019 |          22.1902 |           0.2249 |
[32m[20221213 15:34:41 @agent_ppo2.py:185][0m |          -0.0064 |          22.2050 |           0.2245 |
[32m[20221213 15:34:42 @agent_ppo2.py:185][0m |          -0.0167 |          21.0922 |           0.2249 |
[32m[20221213 15:34:42 @agent_ppo2.py:185][0m |          -0.0043 |          23.5697 |           0.2245 |
[32m[20221213 15:34:42 @agent_ppo2.py:185][0m |          -0.0180 |          20.7679 |           0.2248 |
[32m[20221213 15:34:42 @agent_ppo2.py:185][0m |          -0.0182 |          20.5108 |           0.2249 |
[32m[20221213 15:34:42 @agent_ppo2.py:185][0m |          -0.0205 |          20.3943 |           0.2249 |
[32m[20221213 15:34:42 @agent_ppo2.py:185][0m |          -0.0189 |          20.2788 |           0.2247 |
[32m[20221213 15:34:42 @agent_ppo2.py:185][0m |          -0.0167 |          20.3812 |           0.2249 |
[32m[20221213 15:34:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:34:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.83
[32m[20221213 15:34:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.72
[32m[20221213 15:34:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.18
[32m[20221213 15:34:42 @agent_ppo2.py:143][0m Total time:      41.79 min
[32m[20221213 15:34:42 @agent_ppo2.py:145][0m 3756032 total steps have happened
[32m[20221213 15:34:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1834 --------------------------#
[32m[20221213 15:34:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0015 |          23.7064 |           0.2278 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0124 |          23.2232 |           0.2273 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0077 |          23.5892 |           0.2275 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0113 |          23.1144 |           0.2273 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0141 |          22.9782 |           0.2276 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0147 |          22.6892 |           0.2272 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0096 |          24.0296 |           0.2275 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0178 |          22.4955 |           0.2270 |
[32m[20221213 15:34:43 @agent_ppo2.py:185][0m |          -0.0168 |          22.4375 |           0.2272 |
[32m[20221213 15:34:44 @agent_ppo2.py:185][0m |          -0.0165 |          22.6176 |           0.2272 |
[32m[20221213 15:34:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:34:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.57
[32m[20221213 15:34:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.88
[32m[20221213 15:34:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.65
[32m[20221213 15:34:44 @agent_ppo2.py:143][0m Total time:      41.81 min
[32m[20221213 15:34:44 @agent_ppo2.py:145][0m 3758080 total steps have happened
[32m[20221213 15:34:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1835 --------------------------#
[32m[20221213 15:34:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:44 @agent_ppo2.py:185][0m |          -0.0001 |          23.5338 |           0.2298 |
[32m[20221213 15:34:44 @agent_ppo2.py:185][0m |          -0.0082 |          22.7706 |           0.2295 |
[32m[20221213 15:34:44 @agent_ppo2.py:185][0m |          -0.0114 |          22.3414 |           0.2292 |
[32m[20221213 15:34:44 @agent_ppo2.py:185][0m |          -0.0074 |          22.1791 |           0.2293 |
[32m[20221213 15:34:44 @agent_ppo2.py:185][0m |          -0.0117 |          21.5680 |           0.2292 |
[32m[20221213 15:34:45 @agent_ppo2.py:185][0m |          -0.0147 |          21.3365 |           0.2293 |
[32m[20221213 15:34:45 @agent_ppo2.py:185][0m |          -0.0128 |          21.2892 |           0.2293 |
[32m[20221213 15:34:45 @agent_ppo2.py:185][0m |          -0.0171 |          21.0146 |           0.2291 |
[32m[20221213 15:34:45 @agent_ppo2.py:185][0m |          -0.0065 |          22.5879 |           0.2288 |
[32m[20221213 15:34:45 @agent_ppo2.py:185][0m |          -0.0134 |          20.9047 |           0.2290 |
[32m[20221213 15:34:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:34:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.49
[32m[20221213 15:34:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.96
[32m[20221213 15:34:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.36
[32m[20221213 15:34:45 @agent_ppo2.py:143][0m Total time:      41.83 min
[32m[20221213 15:34:45 @agent_ppo2.py:145][0m 3760128 total steps have happened
[32m[20221213 15:34:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1836 --------------------------#
[32m[20221213 15:34:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0003 |          23.5544 |           0.2329 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0074 |          23.3740 |           0.2331 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0101 |          23.2477 |           0.2329 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0121 |          23.2166 |           0.2328 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0090 |          23.3502 |           0.2328 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0112 |          23.2485 |           0.2328 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0141 |          23.0754 |           0.2330 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0143 |          23.0185 |           0.2328 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0157 |          22.9958 |           0.2331 |
[32m[20221213 15:34:46 @agent_ppo2.py:185][0m |          -0.0142 |          22.9621 |           0.2329 |
[32m[20221213 15:34:46 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:34:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 293.14
[32m[20221213 15:34:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.78
[32m[20221213 15:34:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.17
[32m[20221213 15:34:47 @agent_ppo2.py:143][0m Total time:      41.86 min
[32m[20221213 15:34:47 @agent_ppo2.py:145][0m 3762176 total steps have happened
[32m[20221213 15:34:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1837 --------------------------#
[32m[20221213 15:34:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:47 @agent_ppo2.py:185][0m |           0.0005 |          22.4711 |           0.2354 |
[32m[20221213 15:34:47 @agent_ppo2.py:185][0m |          -0.0068 |          21.4041 |           0.2353 |
[32m[20221213 15:34:47 @agent_ppo2.py:185][0m |          -0.0060 |          20.9961 |           0.2353 |
[32m[20221213 15:34:47 @agent_ppo2.py:185][0m |          -0.0112 |          20.6196 |           0.2350 |
[32m[20221213 15:34:47 @agent_ppo2.py:185][0m |          -0.0138 |          20.4139 |           0.2348 |
[32m[20221213 15:34:47 @agent_ppo2.py:185][0m |          -0.0152 |          20.1527 |           0.2349 |
[32m[20221213 15:34:47 @agent_ppo2.py:185][0m |          -0.0153 |          20.0083 |           0.2347 |
[32m[20221213 15:34:48 @agent_ppo2.py:185][0m |          -0.0169 |          19.8443 |           0.2345 |
[32m[20221213 15:34:48 @agent_ppo2.py:185][0m |          -0.0050 |          21.5076 |           0.2347 |
[32m[20221213 15:34:48 @agent_ppo2.py:185][0m |          -0.0062 |          21.8835 |           0.2343 |
[32m[20221213 15:34:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:34:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 295.72
[32m[20221213 15:34:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 308.26
[32m[20221213 15:34:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.92
[32m[20221213 15:34:48 @agent_ppo2.py:143][0m Total time:      41.88 min
[32m[20221213 15:34:48 @agent_ppo2.py:145][0m 3764224 total steps have happened
[32m[20221213 15:34:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1838 --------------------------#
[32m[20221213 15:34:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:48 @agent_ppo2.py:185][0m |           0.0108 |          26.6670 |           0.2307 |
[32m[20221213 15:34:48 @agent_ppo2.py:185][0m |          -0.0007 |          24.3054 |           0.2300 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |           0.0003 |          25.0396 |           0.2309 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |          -0.0120 |          23.2810 |           0.2303 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |          -0.0155 |          23.1133 |           0.2310 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |          -0.0144 |          22.9992 |           0.2310 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |          -0.0167 |          22.8743 |           0.2310 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |          -0.0178 |          22.7885 |           0.2310 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |          -0.0209 |          22.7783 |           0.2313 |
[32m[20221213 15:34:49 @agent_ppo2.py:185][0m |          -0.0189 |          22.7128 |           0.2313 |
[32m[20221213 15:34:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:34:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.18
[32m[20221213 15:34:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.46
[32m[20221213 15:34:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 278.21
[32m[20221213 15:34:49 @agent_ppo2.py:143][0m Total time:      41.90 min
[32m[20221213 15:34:49 @agent_ppo2.py:145][0m 3766272 total steps have happened
[32m[20221213 15:34:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1839 --------------------------#
[32m[20221213 15:34:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0025 |          24.4973 |           0.2333 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0103 |          23.9512 |           0.2327 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0128 |          23.6763 |           0.2329 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0131 |          23.5862 |           0.2325 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0171 |          23.3301 |           0.2326 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0170 |          23.2134 |           0.2328 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0150 |          23.1184 |           0.2324 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0074 |          24.7672 |           0.2326 |
[32m[20221213 15:34:50 @agent_ppo2.py:185][0m |          -0.0186 |          22.9706 |           0.2326 |
[32m[20221213 15:34:51 @agent_ppo2.py:185][0m |          -0.0186 |          22.9146 |           0.2324 |
[32m[20221213 15:34:51 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:34:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.10
[32m[20221213 15:34:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.22
[32m[20221213 15:34:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.69
[32m[20221213 15:34:51 @agent_ppo2.py:143][0m Total time:      41.93 min
[32m[20221213 15:34:51 @agent_ppo2.py:145][0m 3768320 total steps have happened
[32m[20221213 15:34:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1840 --------------------------#
[32m[20221213 15:34:51 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:34:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:51 @agent_ppo2.py:185][0m |          -0.0007 |          23.5916 |           0.2285 |
[32m[20221213 15:34:51 @agent_ppo2.py:185][0m |          -0.0036 |          23.2232 |           0.2280 |
[32m[20221213 15:34:51 @agent_ppo2.py:185][0m |           0.0041 |          23.6389 |           0.2276 |
[32m[20221213 15:34:51 @agent_ppo2.py:185][0m |          -0.0105 |          22.5618 |           0.2277 |
[32m[20221213 15:34:52 @agent_ppo2.py:185][0m |          -0.0108 |          22.3340 |           0.2274 |
[32m[20221213 15:34:52 @agent_ppo2.py:185][0m |          -0.0111 |          22.1706 |           0.2274 |
[32m[20221213 15:34:52 @agent_ppo2.py:185][0m |          -0.0159 |          22.0389 |           0.2276 |
[32m[20221213 15:34:52 @agent_ppo2.py:185][0m |          -0.0023 |          24.4889 |           0.2275 |
[32m[20221213 15:34:52 @agent_ppo2.py:185][0m |          -0.0155 |          21.8760 |           0.2272 |
[32m[20221213 15:34:52 @agent_ppo2.py:185][0m |          -0.0084 |          22.9120 |           0.2272 |
[32m[20221213 15:34:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:34:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.43
[32m[20221213 15:34:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 306.63
[32m[20221213 15:34:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.93
[32m[20221213 15:34:52 @agent_ppo2.py:143][0m Total time:      41.95 min
[32m[20221213 15:34:52 @agent_ppo2.py:145][0m 3770368 total steps have happened
[32m[20221213 15:34:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1841 --------------------------#
[32m[20221213 15:34:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |           0.0028 |          24.2871 |           0.2334 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0078 |          23.7333 |           0.2337 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0102 |          23.5613 |           0.2333 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0137 |          23.4218 |           0.2332 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0140 |          23.3402 |           0.2327 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0148 |          23.2530 |           0.2333 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0150 |          23.1915 |           0.2326 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0156 |          23.1562 |           0.2327 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0150 |          23.0387 |           0.2330 |
[32m[20221213 15:34:53 @agent_ppo2.py:185][0m |          -0.0149 |          23.0193 |           0.2327 |
[32m[20221213 15:34:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:34:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.29
[32m[20221213 15:34:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 311.66
[32m[20221213 15:34:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 270.36
[32m[20221213 15:34:54 @agent_ppo2.py:143][0m Total time:      41.97 min
[32m[20221213 15:34:54 @agent_ppo2.py:145][0m 3772416 total steps have happened
[32m[20221213 15:34:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1842 --------------------------#
[32m[20221213 15:34:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:34:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:54 @agent_ppo2.py:185][0m |          -0.0010 |          23.9215 |           0.2334 |
[32m[20221213 15:34:54 @agent_ppo2.py:185][0m |          -0.0069 |          23.5627 |           0.2334 |
[32m[20221213 15:34:54 @agent_ppo2.py:185][0m |          -0.0079 |          23.4187 |           0.2328 |
[32m[20221213 15:34:54 @agent_ppo2.py:185][0m |          -0.0120 |          23.3195 |           0.2326 |
[32m[20221213 15:34:54 @agent_ppo2.py:185][0m |          -0.0121 |          23.2312 |           0.2326 |
[32m[20221213 15:34:54 @agent_ppo2.py:185][0m |          -0.0135 |          23.1735 |           0.2323 |
[32m[20221213 15:34:54 @agent_ppo2.py:185][0m |          -0.0127 |          23.1139 |           0.2325 |
[32m[20221213 15:34:55 @agent_ppo2.py:185][0m |          -0.0156 |          23.0884 |           0.2322 |
[32m[20221213 15:34:55 @agent_ppo2.py:185][0m |          -0.0144 |          23.0697 |           0.2320 |
[32m[20221213 15:34:55 @agent_ppo2.py:185][0m |          -0.0150 |          23.0749 |           0.2321 |
[32m[20221213 15:34:55 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:34:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.80
[32m[20221213 15:34:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.99
[32m[20221213 15:34:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.68
[32m[20221213 15:34:55 @agent_ppo2.py:143][0m Total time:      42.00 min
[32m[20221213 15:34:55 @agent_ppo2.py:145][0m 3774464 total steps have happened
[32m[20221213 15:34:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1843 --------------------------#
[32m[20221213 15:34:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:55 @agent_ppo2.py:185][0m |           0.0017 |          24.4767 |           0.2328 |
[32m[20221213 15:34:55 @agent_ppo2.py:185][0m |          -0.0056 |          24.2624 |           0.2327 |
[32m[20221213 15:34:55 @agent_ppo2.py:185][0m |          -0.0115 |          23.9532 |           0.2325 |
[32m[20221213 15:34:56 @agent_ppo2.py:185][0m |          -0.0088 |          23.8547 |           0.2325 |
[32m[20221213 15:34:56 @agent_ppo2.py:185][0m |          -0.0154 |          23.7675 |           0.2323 |
[32m[20221213 15:34:56 @agent_ppo2.py:185][0m |          -0.0133 |          23.6583 |           0.2322 |
[32m[20221213 15:34:56 @agent_ppo2.py:185][0m |          -0.0123 |          24.1535 |           0.2323 |
[32m[20221213 15:34:56 @agent_ppo2.py:185][0m |          -0.0165 |          23.7176 |           0.2322 |
[32m[20221213 15:34:56 @agent_ppo2.py:185][0m |          -0.0108 |          25.0672 |           0.2321 |
[32m[20221213 15:34:56 @agent_ppo2.py:185][0m |          -0.0181 |          23.4239 |           0.2321 |
[32m[20221213 15:34:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:34:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.11
[32m[20221213 15:34:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.29
[32m[20221213 15:34:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.12
[32m[20221213 15:34:56 @agent_ppo2.py:143][0m Total time:      42.02 min
[32m[20221213 15:34:56 @agent_ppo2.py:145][0m 3776512 total steps have happened
[32m[20221213 15:34:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1844 --------------------------#
[32m[20221213 15:34:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0006 |          24.1819 |           0.2327 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0068 |          23.7736 |           0.2321 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0090 |          23.5746 |           0.2320 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0096 |          23.5266 |           0.2320 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0114 |          23.3901 |           0.2317 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0123 |          23.2890 |           0.2315 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0110 |          23.3353 |           0.2314 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0145 |          23.1592 |           0.2313 |
[32m[20221213 15:34:57 @agent_ppo2.py:185][0m |          -0.0134 |          23.1854 |           0.2310 |
[32m[20221213 15:34:58 @agent_ppo2.py:185][0m |          -0.0155 |          23.1079 |           0.2311 |
[32m[20221213 15:34:58 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:34:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.66
[32m[20221213 15:34:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.58
[32m[20221213 15:34:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.95
[32m[20221213 15:34:58 @agent_ppo2.py:143][0m Total time:      42.04 min
[32m[20221213 15:34:58 @agent_ppo2.py:145][0m 3778560 total steps have happened
[32m[20221213 15:34:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1845 --------------------------#
[32m[20221213 15:34:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:34:58 @agent_ppo2.py:185][0m |          -0.0044 |          23.4605 |           0.2289 |
[32m[20221213 15:34:58 @agent_ppo2.py:185][0m |          -0.0058 |          23.2177 |           0.2289 |
[32m[20221213 15:34:58 @agent_ppo2.py:185][0m |          -0.0025 |          23.7097 |           0.2285 |
[32m[20221213 15:34:58 @agent_ppo2.py:185][0m |          -0.0109 |          22.9488 |           0.2277 |
[32m[20221213 15:34:59 @agent_ppo2.py:185][0m |          -0.0127 |          22.9064 |           0.2276 |
[32m[20221213 15:34:59 @agent_ppo2.py:185][0m |          -0.0119 |          22.8908 |           0.2272 |
[32m[20221213 15:34:59 @agent_ppo2.py:185][0m |          -0.0132 |          22.7712 |           0.2271 |
[32m[20221213 15:34:59 @agent_ppo2.py:185][0m |          -0.0134 |          22.7285 |           0.2270 |
[32m[20221213 15:34:59 @agent_ppo2.py:185][0m |          -0.0139 |          22.7248 |           0.2270 |
[32m[20221213 15:34:59 @agent_ppo2.py:185][0m |          -0.0161 |          22.7229 |           0.2265 |
[32m[20221213 15:34:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:34:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.48
[32m[20221213 15:34:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.21
[32m[20221213 15:34:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.11
[32m[20221213 15:34:59 @agent_ppo2.py:143][0m Total time:      42.07 min
[32m[20221213 15:34:59 @agent_ppo2.py:145][0m 3780608 total steps have happened
[32m[20221213 15:34:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1846 --------------------------#
[32m[20221213 15:34:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:34:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0026 |          22.3142 |           0.2271 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |           0.0006 |          23.0562 |           0.2265 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0111 |          21.3829 |           0.2255 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0136 |          21.1518 |           0.2256 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0119 |          20.9846 |           0.2253 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0022 |          22.7432 |           0.2255 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0158 |          20.5734 |           0.2250 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0052 |          23.2517 |           0.2251 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0163 |          20.2453 |           0.2246 |
[32m[20221213 15:35:00 @agent_ppo2.py:185][0m |          -0.0156 |          20.1238 |           0.2253 |
[32m[20221213 15:35:00 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:35:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.63
[32m[20221213 15:35:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.77
[32m[20221213 15:35:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.86
[32m[20221213 15:35:01 @agent_ppo2.py:143][0m Total time:      42.09 min
[32m[20221213 15:35:01 @agent_ppo2.py:145][0m 3782656 total steps have happened
[32m[20221213 15:35:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1847 --------------------------#
[32m[20221213 15:35:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:01 @agent_ppo2.py:185][0m |          -0.0025 |          23.4468 |           0.2267 |
[32m[20221213 15:35:01 @agent_ppo2.py:185][0m |          -0.0055 |          22.9386 |           0.2267 |
[32m[20221213 15:35:01 @agent_ppo2.py:185][0m |          -0.0077 |          22.6970 |           0.2267 |
[32m[20221213 15:35:01 @agent_ppo2.py:185][0m |          -0.0106 |          22.6017 |           0.2268 |
[32m[20221213 15:35:01 @agent_ppo2.py:185][0m |          -0.0081 |          22.8813 |           0.2270 |
[32m[20221213 15:35:01 @agent_ppo2.py:185][0m |          -0.0130 |          22.4881 |           0.2269 |
[32m[20221213 15:35:02 @agent_ppo2.py:185][0m |          -0.0145 |          22.4514 |           0.2272 |
[32m[20221213 15:35:02 @agent_ppo2.py:185][0m |          -0.0150 |          22.4080 |           0.2273 |
[32m[20221213 15:35:02 @agent_ppo2.py:185][0m |          -0.0177 |          22.3576 |           0.2272 |
[32m[20221213 15:35:02 @agent_ppo2.py:185][0m |          -0.0148 |          22.3541 |           0.2274 |
[32m[20221213 15:35:02 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:35:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.33
[32m[20221213 15:35:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.77
[32m[20221213 15:35:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.50
[32m[20221213 15:35:02 @agent_ppo2.py:143][0m Total time:      42.11 min
[32m[20221213 15:35:02 @agent_ppo2.py:145][0m 3784704 total steps have happened
[32m[20221213 15:35:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1848 --------------------------#
[32m[20221213 15:35:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:02 @agent_ppo2.py:185][0m |           0.0054 |          22.9649 |           0.2248 |
[32m[20221213 15:35:02 @agent_ppo2.py:185][0m |          -0.0034 |          22.2593 |           0.2245 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0089 |          22.1326 |           0.2242 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0081 |          22.0034 |           0.2237 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0100 |          21.9409 |           0.2238 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0054 |          22.2000 |           0.2236 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0120 |          21.8433 |           0.2234 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0116 |          21.7923 |           0.2233 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0048 |          22.3486 |           0.2230 |
[32m[20221213 15:35:03 @agent_ppo2.py:185][0m |          -0.0139 |          21.6979 |           0.2230 |
[32m[20221213 15:35:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:35:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.51
[32m[20221213 15:35:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.91
[32m[20221213 15:35:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.73
[32m[20221213 15:35:03 @agent_ppo2.py:143][0m Total time:      42.14 min
[32m[20221213 15:35:03 @agent_ppo2.py:145][0m 3786752 total steps have happened
[32m[20221213 15:35:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1849 --------------------------#
[32m[20221213 15:35:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0023 |          22.8405 |           0.2252 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0076 |          22.4671 |           0.2250 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0099 |          22.2063 |           0.2252 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0117 |          22.0020 |           0.2253 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0101 |          22.2001 |           0.2256 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0144 |          21.7029 |           0.2252 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0141 |          21.6863 |           0.2256 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0153 |          21.5291 |           0.2255 |
[32m[20221213 15:35:04 @agent_ppo2.py:185][0m |          -0.0163 |          21.3911 |           0.2258 |
[32m[20221213 15:35:05 @agent_ppo2.py:185][0m |          -0.0192 |          21.3691 |           0.2260 |
[32m[20221213 15:35:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:35:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.22
[32m[20221213 15:35:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.19
[32m[20221213 15:35:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.38
[32m[20221213 15:35:05 @agent_ppo2.py:143][0m Total time:      42.16 min
[32m[20221213 15:35:05 @agent_ppo2.py:145][0m 3788800 total steps have happened
[32m[20221213 15:35:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1850 --------------------------#
[32m[20221213 15:35:05 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:35:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:05 @agent_ppo2.py:185][0m |          -0.0018 |          23.2139 |           0.2284 |
[32m[20221213 15:35:05 @agent_ppo2.py:185][0m |          -0.0102 |          22.7515 |           0.2279 |
[32m[20221213 15:35:05 @agent_ppo2.py:185][0m |          -0.0119 |          22.5991 |           0.2277 |
[32m[20221213 15:35:05 @agent_ppo2.py:185][0m |          -0.0137 |          22.5002 |           0.2275 |
[32m[20221213 15:35:05 @agent_ppo2.py:185][0m |          -0.0058 |          25.1579 |           0.2276 |
[32m[20221213 15:35:06 @agent_ppo2.py:185][0m |          -0.0022 |          23.6191 |           0.2274 |
[32m[20221213 15:35:06 @agent_ppo2.py:185][0m |          -0.0155 |          22.2820 |           0.2275 |
[32m[20221213 15:35:06 @agent_ppo2.py:185][0m |          -0.0183 |          22.2134 |           0.2274 |
[32m[20221213 15:35:06 @agent_ppo2.py:185][0m |          -0.0077 |          24.8488 |           0.2275 |
[32m[20221213 15:35:06 @agent_ppo2.py:185][0m |          -0.0161 |          22.1891 |           0.2267 |
[32m[20221213 15:35:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:35:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.40
[32m[20221213 15:35:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.65
[32m[20221213 15:35:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.22
[32m[20221213 15:35:06 @agent_ppo2.py:143][0m Total time:      42.18 min
[32m[20221213 15:35:06 @agent_ppo2.py:145][0m 3790848 total steps have happened
[32m[20221213 15:35:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1851 --------------------------#
[32m[20221213 15:35:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:06 @agent_ppo2.py:185][0m |           0.0032 |          24.0855 |           0.2284 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0072 |          22.6706 |           0.2283 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0104 |          22.4416 |           0.2287 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0019 |          23.0136 |           0.2290 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0102 |          22.1269 |           0.2287 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0023 |          23.0839 |           0.2291 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0150 |          21.8948 |           0.2291 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0103 |          22.3347 |           0.2288 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0152 |          21.7543 |           0.2290 |
[32m[20221213 15:35:07 @agent_ppo2.py:185][0m |          -0.0169 |          21.6887 |           0.2293 |
[32m[20221213 15:35:07 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 15:35:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.78
[32m[20221213 15:35:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.55
[32m[20221213 15:35:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.96
[32m[20221213 15:35:07 @agent_ppo2.py:143][0m Total time:      42.20 min
[32m[20221213 15:35:07 @agent_ppo2.py:145][0m 3792896 total steps have happened
[32m[20221213 15:35:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1852 --------------------------#
[32m[20221213 15:35:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0003 |          23.5397 |           0.2287 |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0053 |          23.0843 |           0.2280 |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0081 |          23.0774 |           0.2275 |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0120 |          22.8202 |           0.2272 |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0119 |          22.6123 |           0.2273 |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0152 |          22.5809 |           0.2267 |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0149 |          22.5559 |           0.2268 |
[32m[20221213 15:35:08 @agent_ppo2.py:185][0m |          -0.0143 |          22.4413 |           0.2264 |
[32m[20221213 15:35:09 @agent_ppo2.py:185][0m |          -0.0150 |          22.3236 |           0.2264 |
[32m[20221213 15:35:09 @agent_ppo2.py:185][0m |          -0.0162 |          22.2239 |           0.2260 |
[32m[20221213 15:35:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:35:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.06
[32m[20221213 15:35:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.28
[32m[20221213 15:35:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 318.07
[32m[20221213 15:35:09 @agent_ppo2.py:143][0m Total time:      42.23 min
[32m[20221213 15:35:09 @agent_ppo2.py:145][0m 3794944 total steps have happened
[32m[20221213 15:35:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1853 --------------------------#
[32m[20221213 15:35:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:09 @agent_ppo2.py:185][0m |          -0.0006 |          22.8398 |           0.2225 |
[32m[20221213 15:35:09 @agent_ppo2.py:185][0m |          -0.0074 |          22.3642 |           0.2221 |
[32m[20221213 15:35:09 @agent_ppo2.py:185][0m |          -0.0057 |          22.1986 |           0.2220 |
[32m[20221213 15:35:09 @agent_ppo2.py:185][0m |           0.0010 |          25.1465 |           0.2217 |
[32m[20221213 15:35:10 @agent_ppo2.py:185][0m |          -0.0003 |          23.2913 |           0.2210 |
[32m[20221213 15:35:10 @agent_ppo2.py:185][0m |          -0.0135 |          21.8820 |           0.2216 |
[32m[20221213 15:35:10 @agent_ppo2.py:185][0m |          -0.0019 |          24.3620 |           0.2212 |
[32m[20221213 15:35:10 @agent_ppo2.py:185][0m |          -0.0154 |          21.8693 |           0.2209 |
[32m[20221213 15:35:10 @agent_ppo2.py:185][0m |          -0.0180 |          21.7586 |           0.2210 |
[32m[20221213 15:35:10 @agent_ppo2.py:185][0m |          -0.0080 |          24.3599 |           0.2210 |
[32m[20221213 15:35:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:35:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.39
[32m[20221213 15:35:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.43
[32m[20221213 15:35:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.00
[32m[20221213 15:35:10 @agent_ppo2.py:143][0m Total time:      42.25 min
[32m[20221213 15:35:10 @agent_ppo2.py:145][0m 3796992 total steps have happened
[32m[20221213 15:35:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1854 --------------------------#
[32m[20221213 15:35:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |           0.0017 |          23.2729 |           0.2237 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0078 |          22.9528 |           0.2236 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0120 |          22.8223 |           0.2232 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0113 |          22.7395 |           0.2230 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0134 |          22.6721 |           0.2229 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0105 |          22.6251 |           0.2233 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0150 |          22.5685 |           0.2231 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0150 |          22.5384 |           0.2232 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0171 |          22.5073 |           0.2230 |
[32m[20221213 15:35:11 @agent_ppo2.py:185][0m |          -0.0153 |          22.4628 |           0.2232 |
[32m[20221213 15:35:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:35:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.67
[32m[20221213 15:35:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.86
[32m[20221213 15:35:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.44
[32m[20221213 15:35:12 @agent_ppo2.py:143][0m Total time:      42.27 min
[32m[20221213 15:35:12 @agent_ppo2.py:145][0m 3799040 total steps have happened
[32m[20221213 15:35:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1855 --------------------------#
[32m[20221213 15:35:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:12 @agent_ppo2.py:185][0m |           0.0025 |          22.3040 |           0.2228 |
[32m[20221213 15:35:12 @agent_ppo2.py:185][0m |           0.0009 |          23.7007 |           0.2224 |
[32m[20221213 15:35:12 @agent_ppo2.py:185][0m |          -0.0119 |          20.8891 |           0.2224 |
[32m[20221213 15:35:12 @agent_ppo2.py:185][0m |          -0.0129 |          20.6610 |           0.2225 |
[32m[20221213 15:35:12 @agent_ppo2.py:185][0m |          -0.0161 |          20.5077 |           0.2226 |
[32m[20221213 15:35:12 @agent_ppo2.py:185][0m |          -0.0152 |          20.2718 |           0.2222 |
[32m[20221213 15:35:12 @agent_ppo2.py:185][0m |          -0.0170 |          20.0737 |           0.2223 |
[32m[20221213 15:35:13 @agent_ppo2.py:185][0m |          -0.0172 |          19.9665 |           0.2220 |
[32m[20221213 15:35:13 @agent_ppo2.py:185][0m |          -0.0143 |          20.5277 |           0.2220 |
[32m[20221213 15:35:13 @agent_ppo2.py:185][0m |          -0.0194 |          19.8690 |           0.2218 |
[32m[20221213 15:35:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:35:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.41
[32m[20221213 15:35:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.11
[32m[20221213 15:35:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.97
[32m[20221213 15:35:13 @agent_ppo2.py:143][0m Total time:      42.30 min
[32m[20221213 15:35:13 @agent_ppo2.py:145][0m 3801088 total steps have happened
[32m[20221213 15:35:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1856 --------------------------#
[32m[20221213 15:35:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:13 @agent_ppo2.py:185][0m |          -0.0008 |          23.5704 |           0.2283 |
[32m[20221213 15:35:13 @agent_ppo2.py:185][0m |          -0.0027 |          23.2641 |           0.2281 |
[32m[20221213 15:35:13 @agent_ppo2.py:185][0m |          -0.0096 |          22.5296 |           0.2284 |
[32m[20221213 15:35:14 @agent_ppo2.py:185][0m |          -0.0117 |          22.3674 |           0.2281 |
[32m[20221213 15:35:14 @agent_ppo2.py:185][0m |          -0.0114 |          22.2546 |           0.2283 |
[32m[20221213 15:35:14 @agent_ppo2.py:185][0m |          -0.0139 |          22.1980 |           0.2286 |
[32m[20221213 15:35:14 @agent_ppo2.py:185][0m |          -0.0127 |          22.1584 |           0.2286 |
[32m[20221213 15:35:14 @agent_ppo2.py:185][0m |          -0.0153 |          22.0882 |           0.2285 |
[32m[20221213 15:35:14 @agent_ppo2.py:185][0m |          -0.0153 |          22.0077 |           0.2287 |
[32m[20221213 15:35:14 @agent_ppo2.py:185][0m |          -0.0161 |          21.9053 |           0.2287 |
[32m[20221213 15:35:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:35:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.78
[32m[20221213 15:35:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.73
[32m[20221213 15:35:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.24
[32m[20221213 15:35:14 @agent_ppo2.py:143][0m Total time:      42.32 min
[32m[20221213 15:35:14 @agent_ppo2.py:145][0m 3803136 total steps have happened
[32m[20221213 15:35:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1857 --------------------------#
[32m[20221213 15:35:15 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |           0.0043 |          22.0248 |           0.2321 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |           0.0005 |          21.5035 |           0.2316 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |          -0.0107 |          21.0240 |           0.2315 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |          -0.0006 |          21.5649 |           0.2315 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |          -0.0136 |          20.6853 |           0.2310 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |          -0.0057 |          20.7814 |           0.2310 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |          -0.0100 |          20.8849 |           0.2309 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |          -0.0147 |          20.2293 |           0.2306 |
[32m[20221213 15:35:15 @agent_ppo2.py:185][0m |          -0.0016 |          22.6577 |           0.2307 |
[32m[20221213 15:35:16 @agent_ppo2.py:185][0m |          -0.0135 |          20.0605 |           0.2296 |
[32m[20221213 15:35:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:35:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.48
[32m[20221213 15:35:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.85
[32m[20221213 15:35:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.49
[32m[20221213 15:35:16 @agent_ppo2.py:143][0m Total time:      42.34 min
[32m[20221213 15:35:16 @agent_ppo2.py:145][0m 3805184 total steps have happened
[32m[20221213 15:35:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1858 --------------------------#
[32m[20221213 15:35:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:16 @agent_ppo2.py:185][0m |          -0.0031 |          23.0253 |           0.2294 |
[32m[20221213 15:35:16 @agent_ppo2.py:185][0m |          -0.0068 |          22.4822 |           0.2293 |
[32m[20221213 15:35:16 @agent_ppo2.py:185][0m |          -0.0054 |          22.4109 |           0.2295 |
[32m[20221213 15:35:16 @agent_ppo2.py:185][0m |          -0.0118 |          22.1304 |           0.2293 |
[32m[20221213 15:35:17 @agent_ppo2.py:185][0m |          -0.0109 |          22.0246 |           0.2289 |
[32m[20221213 15:35:17 @agent_ppo2.py:185][0m |          -0.0126 |          21.9717 |           0.2291 |
[32m[20221213 15:35:17 @agent_ppo2.py:185][0m |          -0.0122 |          21.7825 |           0.2291 |
[32m[20221213 15:35:17 @agent_ppo2.py:185][0m |          -0.0133 |          21.7385 |           0.2291 |
[32m[20221213 15:35:17 @agent_ppo2.py:185][0m |          -0.0130 |          21.6212 |           0.2292 |
[32m[20221213 15:35:17 @agent_ppo2.py:185][0m |          -0.0158 |          21.5165 |           0.2290 |
[32m[20221213 15:35:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:35:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.50
[32m[20221213 15:35:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.12
[32m[20221213 15:35:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.06
[32m[20221213 15:35:17 @agent_ppo2.py:143][0m Total time:      42.37 min
[32m[20221213 15:35:17 @agent_ppo2.py:145][0m 3807232 total steps have happened
[32m[20221213 15:35:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1859 --------------------------#
[32m[20221213 15:35:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0009 |          22.7555 |           0.2254 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0081 |          22.4303 |           0.2250 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0082 |          22.2511 |           0.2248 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0080 |          22.0976 |           0.2250 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0114 |          21.9335 |           0.2248 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0139 |          21.8586 |           0.2251 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0135 |          21.7747 |           0.2249 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0146 |          21.7993 |           0.2246 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0135 |          21.7193 |           0.2249 |
[32m[20221213 15:35:18 @agent_ppo2.py:185][0m |          -0.0169 |          21.7008 |           0.2252 |
[32m[20221213 15:35:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:35:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.49
[32m[20221213 15:35:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.54
[32m[20221213 15:35:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 314.41
[32m[20221213 15:35:19 @agent_ppo2.py:143][0m Total time:      42.39 min
[32m[20221213 15:35:19 @agent_ppo2.py:145][0m 3809280 total steps have happened
[32m[20221213 15:35:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1860 --------------------------#
[32m[20221213 15:35:19 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:35:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:19 @agent_ppo2.py:185][0m |           0.0010 |          23.6862 |           0.2270 |
[32m[20221213 15:35:19 @agent_ppo2.py:185][0m |           0.0049 |          24.8578 |           0.2270 |
[32m[20221213 15:35:19 @agent_ppo2.py:185][0m |          -0.0060 |          23.1032 |           0.2265 |
[32m[20221213 15:35:19 @agent_ppo2.py:185][0m |          -0.0114 |          22.8806 |           0.2265 |
[32m[20221213 15:35:19 @agent_ppo2.py:185][0m |          -0.0132 |          22.8125 |           0.2266 |
[32m[20221213 15:35:19 @agent_ppo2.py:185][0m |          -0.0146 |          22.7621 |           0.2264 |
[32m[20221213 15:35:20 @agent_ppo2.py:185][0m |          -0.0129 |          22.6644 |           0.2264 |
[32m[20221213 15:35:20 @agent_ppo2.py:185][0m |          -0.0166 |          22.6244 |           0.2262 |
[32m[20221213 15:35:20 @agent_ppo2.py:185][0m |          -0.0171 |          22.5545 |           0.2264 |
[32m[20221213 15:35:20 @agent_ppo2.py:185][0m |          -0.0156 |          22.5491 |           0.2264 |
[32m[20221213 15:35:20 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:35:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.38
[32m[20221213 15:35:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.33
[32m[20221213 15:35:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.88
[32m[20221213 15:35:20 @agent_ppo2.py:143][0m Total time:      42.41 min
[32m[20221213 15:35:20 @agent_ppo2.py:145][0m 3811328 total steps have happened
[32m[20221213 15:35:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1861 --------------------------#
[32m[20221213 15:35:20 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:20 @agent_ppo2.py:185][0m |          -0.0014 |          21.6129 |           0.2278 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0074 |          21.0993 |           0.2276 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0115 |          20.8288 |           0.2276 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0133 |          20.6184 |           0.2275 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0140 |          20.5000 |           0.2275 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0180 |          20.3374 |           0.2273 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0036 |          23.6278 |           0.2276 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0135 |          20.4073 |           0.2272 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0161 |          20.1747 |           0.2274 |
[32m[20221213 15:35:21 @agent_ppo2.py:185][0m |          -0.0209 |          20.0441 |           0.2275 |
[32m[20221213 15:35:21 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:35:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.49
[32m[20221213 15:35:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.85
[32m[20221213 15:35:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.22
[32m[20221213 15:35:21 @agent_ppo2.py:143][0m Total time:      42.44 min
[32m[20221213 15:35:21 @agent_ppo2.py:145][0m 3813376 total steps have happened
[32m[20221213 15:35:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1862 --------------------------#
[32m[20221213 15:35:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0014 |          23.7474 |           0.2298 |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0080 |          23.4170 |           0.2290 |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0114 |          23.2494 |           0.2293 |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0123 |          23.1529 |           0.2291 |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0108 |          23.1348 |           0.2294 |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0163 |          23.0216 |           0.2294 |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0159 |          22.9620 |           0.2293 |
[32m[20221213 15:35:22 @agent_ppo2.py:185][0m |          -0.0183 |          22.9274 |           0.2295 |
[32m[20221213 15:35:23 @agent_ppo2.py:185][0m |          -0.0176 |          22.8522 |           0.2293 |
[32m[20221213 15:35:23 @agent_ppo2.py:185][0m |          -0.0178 |          22.8301 |           0.2292 |
[32m[20221213 15:35:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:35:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.79
[32m[20221213 15:35:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.69
[32m[20221213 15:35:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.22
[32m[20221213 15:35:23 @agent_ppo2.py:143][0m Total time:      42.46 min
[32m[20221213 15:35:23 @agent_ppo2.py:145][0m 3815424 total steps have happened
[32m[20221213 15:35:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1863 --------------------------#
[32m[20221213 15:35:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:23 @agent_ppo2.py:185][0m |          -0.0012 |          22.9744 |           0.2283 |
[32m[20221213 15:35:23 @agent_ppo2.py:185][0m |          -0.0056 |          22.7388 |           0.2286 |
[32m[20221213 15:35:23 @agent_ppo2.py:185][0m |          -0.0090 |          22.5827 |           0.2281 |
[32m[20221213 15:35:23 @agent_ppo2.py:185][0m |          -0.0035 |          23.0728 |           0.2280 |
[32m[20221213 15:35:24 @agent_ppo2.py:185][0m |          -0.0115 |          22.4565 |           0.2279 |
[32m[20221213 15:35:24 @agent_ppo2.py:185][0m |          -0.0124 |          22.4456 |           0.2277 |
[32m[20221213 15:35:24 @agent_ppo2.py:185][0m |          -0.0129 |          22.3543 |           0.2277 |
[32m[20221213 15:35:24 @agent_ppo2.py:185][0m |          -0.0119 |          22.3193 |           0.2277 |
[32m[20221213 15:35:24 @agent_ppo2.py:185][0m |          -0.0109 |          22.4629 |           0.2272 |
[32m[20221213 15:35:24 @agent_ppo2.py:185][0m |          -0.0125 |          22.2883 |           0.2276 |
[32m[20221213 15:35:24 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:35:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.17
[32m[20221213 15:35:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.26
[32m[20221213 15:35:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.42
[32m[20221213 15:35:24 @agent_ppo2.py:143][0m Total time:      42.48 min
[32m[20221213 15:35:24 @agent_ppo2.py:145][0m 3817472 total steps have happened
[32m[20221213 15:35:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1864 --------------------------#
[32m[20221213 15:35:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0018 |          23.1338 |           0.2310 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |           0.0002 |          23.2045 |           0.2303 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0084 |          22.7841 |           0.2297 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0109 |          22.6822 |           0.2296 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0063 |          23.3327 |           0.2293 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0130 |          22.5601 |           0.2291 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0135 |          22.4953 |           0.2288 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0127 |          22.5802 |           0.2288 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0184 |          22.4558 |           0.2287 |
[32m[20221213 15:35:25 @agent_ppo2.py:185][0m |          -0.0155 |          22.4441 |           0.2285 |
[32m[20221213 15:35:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:35:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 262.51
[32m[20221213 15:35:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.04
[32m[20221213 15:35:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.35
[32m[20221213 15:35:26 @agent_ppo2.py:143][0m Total time:      42.51 min
[32m[20221213 15:35:26 @agent_ppo2.py:145][0m 3819520 total steps have happened
[32m[20221213 15:35:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1865 --------------------------#
[32m[20221213 15:35:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:26 @agent_ppo2.py:185][0m |          -0.0017 |          23.2263 |           0.2248 |
[32m[20221213 15:35:26 @agent_ppo2.py:185][0m |          -0.0111 |          22.8466 |           0.2239 |
[32m[20221213 15:35:26 @agent_ppo2.py:185][0m |          -0.0125 |          22.6519 |           0.2236 |
[32m[20221213 15:35:26 @agent_ppo2.py:185][0m |           0.0083 |          26.3216 |           0.2232 |
[32m[20221213 15:35:26 @agent_ppo2.py:185][0m |          -0.0122 |          22.4502 |           0.2226 |
[32m[20221213 15:35:27 @agent_ppo2.py:185][0m |          -0.0007 |          25.8406 |           0.2227 |
[32m[20221213 15:35:27 @agent_ppo2.py:185][0m |          -0.0136 |          22.1069 |           0.2223 |
[32m[20221213 15:35:27 @agent_ppo2.py:185][0m |          -0.0159 |          22.0177 |           0.2223 |
[32m[20221213 15:35:27 @agent_ppo2.py:185][0m |          -0.0173 |          21.9703 |           0.2223 |
[32m[20221213 15:35:27 @agent_ppo2.py:185][0m |          -0.0189 |          21.8485 |           0.2221 |
[32m[20221213 15:35:27 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:35:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.94
[32m[20221213 15:35:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.93
[32m[20221213 15:35:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.28
[32m[20221213 15:35:27 @agent_ppo2.py:143][0m Total time:      42.53 min
[32m[20221213 15:35:27 @agent_ppo2.py:145][0m 3821568 total steps have happened
[32m[20221213 15:35:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1866 --------------------------#
[32m[20221213 15:35:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:27 @agent_ppo2.py:185][0m |          -0.0021 |          22.4419 |           0.2208 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0083 |          21.9316 |           0.2205 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0066 |          21.9444 |           0.2205 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0143 |          21.4367 |           0.2202 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0143 |          21.1205 |           0.2199 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0171 |          21.0068 |           0.2202 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0175 |          20.9021 |           0.2201 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0182 |          20.7542 |           0.2200 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0180 |          20.5881 |           0.2200 |
[32m[20221213 15:35:28 @agent_ppo2.py:185][0m |          -0.0182 |          20.5025 |           0.2199 |
[32m[20221213 15:35:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:35:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.93
[32m[20221213 15:35:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.32
[32m[20221213 15:35:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.34
[32m[20221213 15:35:28 @agent_ppo2.py:143][0m Total time:      42.55 min
[32m[20221213 15:35:28 @agent_ppo2.py:145][0m 3823616 total steps have happened
[32m[20221213 15:35:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1867 --------------------------#
[32m[20221213 15:35:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |           0.0113 |          25.3940 |           0.2249 |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |          -0.0053 |          23.3160 |           0.2248 |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |          -0.0132 |          23.0835 |           0.2253 |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |          -0.0111 |          23.0001 |           0.2251 |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |          -0.0097 |          23.0109 |           0.2251 |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |          -0.0067 |          23.0518 |           0.2254 |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |          -0.0155 |          22.7375 |           0.2253 |
[32m[20221213 15:35:29 @agent_ppo2.py:185][0m |          -0.0170 |          22.7032 |           0.2252 |
[32m[20221213 15:35:30 @agent_ppo2.py:185][0m |          -0.0164 |          22.6589 |           0.2252 |
[32m[20221213 15:35:30 @agent_ppo2.py:185][0m |          -0.0170 |          22.6265 |           0.2253 |
[32m[20221213 15:35:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:35:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.83
[32m[20221213 15:35:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.61
[32m[20221213 15:35:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.34
[32m[20221213 15:35:30 @agent_ppo2.py:143][0m Total time:      42.58 min
[32m[20221213 15:35:30 @agent_ppo2.py:145][0m 3825664 total steps have happened
[32m[20221213 15:35:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1868 --------------------------#
[32m[20221213 15:35:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:30 @agent_ppo2.py:185][0m |           0.0014 |          22.5071 |           0.2229 |
[32m[20221213 15:35:30 @agent_ppo2.py:185][0m |          -0.0089 |          22.0165 |           0.2226 |
[32m[20221213 15:35:30 @agent_ppo2.py:185][0m |          -0.0129 |          21.7918 |           0.2227 |
[32m[20221213 15:35:31 @agent_ppo2.py:185][0m |          -0.0141 |          21.6009 |           0.2224 |
[32m[20221213 15:35:31 @agent_ppo2.py:185][0m |          -0.0156 |          21.4861 |           0.2225 |
[32m[20221213 15:35:31 @agent_ppo2.py:185][0m |          -0.0063 |          23.1185 |           0.2225 |
[32m[20221213 15:35:31 @agent_ppo2.py:185][0m |          -0.0091 |          21.5934 |           0.2218 |
[32m[20221213 15:35:31 @agent_ppo2.py:185][0m |          -0.0173 |          21.1908 |           0.2222 |
[32m[20221213 15:35:31 @agent_ppo2.py:185][0m |          -0.0173 |          21.1539 |           0.2221 |
[32m[20221213 15:35:31 @agent_ppo2.py:185][0m |          -0.0193 |          21.1193 |           0.2224 |
[32m[20221213 15:35:31 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:35:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 255.41
[32m[20221213 15:35:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.49
[32m[20221213 15:35:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.66
[32m[20221213 15:35:31 @agent_ppo2.py:143][0m Total time:      42.60 min
[32m[20221213 15:35:31 @agent_ppo2.py:145][0m 3827712 total steps have happened
[32m[20221213 15:35:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1869 --------------------------#
[32m[20221213 15:35:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |           0.0143 |          23.8927 |           0.2292 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0045 |          22.0114 |           0.2291 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0094 |          21.8202 |           0.2292 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0100 |          21.6528 |           0.2290 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0133 |          21.5030 |           0.2290 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0098 |          21.4237 |           0.2290 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0146 |          21.2591 |           0.2290 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0154 |          21.1379 |           0.2290 |
[32m[20221213 15:35:32 @agent_ppo2.py:185][0m |          -0.0115 |          21.4383 |           0.2289 |
[32m[20221213 15:35:33 @agent_ppo2.py:185][0m |          -0.0086 |          22.6639 |           0.2288 |
[32m[20221213 15:35:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:35:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.46
[32m[20221213 15:35:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.78
[32m[20221213 15:35:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.36
[32m[20221213 15:35:33 @agent_ppo2.py:143][0m Total time:      42.63 min
[32m[20221213 15:35:33 @agent_ppo2.py:145][0m 3829760 total steps have happened
[32m[20221213 15:35:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1870 --------------------------#
[32m[20221213 15:35:33 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:35:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:33 @agent_ppo2.py:185][0m |          -0.0022 |          22.8776 |           0.2252 |
[32m[20221213 15:35:33 @agent_ppo2.py:185][0m |          -0.0117 |          22.3702 |           0.2252 |
[32m[20221213 15:35:33 @agent_ppo2.py:185][0m |          -0.0121 |          22.0791 |           0.2253 |
[32m[20221213 15:35:33 @agent_ppo2.py:185][0m |          -0.0130 |          21.8915 |           0.2250 |
[32m[20221213 15:35:34 @agent_ppo2.py:185][0m |          -0.0151 |          21.7915 |           0.2252 |
[32m[20221213 15:35:34 @agent_ppo2.py:185][0m |          -0.0165 |          21.6495 |           0.2250 |
[32m[20221213 15:35:34 @agent_ppo2.py:185][0m |          -0.0176 |          21.5354 |           0.2252 |
[32m[20221213 15:35:34 @agent_ppo2.py:185][0m |          -0.0198 |          21.4309 |           0.2251 |
[32m[20221213 15:35:34 @agent_ppo2.py:185][0m |          -0.0072 |          23.9737 |           0.2248 |
[32m[20221213 15:35:34 @agent_ppo2.py:185][0m |          -0.0170 |          21.3221 |           0.2246 |
[32m[20221213 15:35:34 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:35:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 265.54
[32m[20221213 15:35:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.55
[32m[20221213 15:35:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.49
[32m[20221213 15:35:34 @agent_ppo2.py:143][0m Total time:      42.65 min
[32m[20221213 15:35:34 @agent_ppo2.py:145][0m 3831808 total steps have happened
[32m[20221213 15:35:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1871 --------------------------#
[32m[20221213 15:35:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0040 |          22.8962 |           0.2303 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0077 |          22.2048 |           0.2293 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0053 |          22.4058 |           0.2295 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0070 |          21.8740 |           0.2290 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0154 |          21.5026 |           0.2288 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0153 |          21.2755 |           0.2288 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0109 |          21.3508 |           0.2286 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0138 |          21.1390 |           0.2284 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0116 |          21.2591 |           0.2284 |
[32m[20221213 15:35:35 @agent_ppo2.py:185][0m |          -0.0172 |          20.9059 |           0.2281 |
[32m[20221213 15:35:35 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:35:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.63
[32m[20221213 15:35:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.87
[32m[20221213 15:35:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.93
[32m[20221213 15:35:36 @agent_ppo2.py:143][0m Total time:      42.67 min
[32m[20221213 15:35:36 @agent_ppo2.py:145][0m 3833856 total steps have happened
[32m[20221213 15:35:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1872 --------------------------#
[32m[20221213 15:35:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:36 @agent_ppo2.py:185][0m |           0.0032 |          24.2320 |           0.2192 |
[32m[20221213 15:35:36 @agent_ppo2.py:185][0m |          -0.0087 |          23.3984 |           0.2193 |
[32m[20221213 15:35:36 @agent_ppo2.py:185][0m |          -0.0092 |          23.2747 |           0.2192 |
[32m[20221213 15:35:36 @agent_ppo2.py:185][0m |          -0.0155 |          23.1458 |           0.2189 |
[32m[20221213 15:35:36 @agent_ppo2.py:185][0m |          -0.0140 |          23.0952 |           0.2187 |
[32m[20221213 15:35:36 @agent_ppo2.py:185][0m |          -0.0166 |          22.9797 |           0.2188 |
[32m[20221213 15:35:36 @agent_ppo2.py:185][0m |          -0.0154 |          22.9308 |           0.2184 |
[32m[20221213 15:35:37 @agent_ppo2.py:185][0m |          -0.0089 |          24.7262 |           0.2190 |
[32m[20221213 15:35:37 @agent_ppo2.py:185][0m |          -0.0172 |          22.8271 |           0.2190 |
[32m[20221213 15:35:37 @agent_ppo2.py:185][0m |          -0.0029 |          25.2589 |           0.2188 |
[32m[20221213 15:35:37 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:35:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.21
[32m[20221213 15:35:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.72
[32m[20221213 15:35:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.82
[32m[20221213 15:35:37 @agent_ppo2.py:143][0m Total time:      42.70 min
[32m[20221213 15:35:37 @agent_ppo2.py:145][0m 3835904 total steps have happened
[32m[20221213 15:35:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1873 --------------------------#
[32m[20221213 15:35:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:37 @agent_ppo2.py:185][0m |          -0.0002 |          22.2541 |           0.2266 |
[32m[20221213 15:35:37 @agent_ppo2.py:185][0m |          -0.0080 |          21.7443 |           0.2263 |
[32m[20221213 15:35:37 @agent_ppo2.py:185][0m |          -0.0023 |          22.0007 |           0.2261 |
[32m[20221213 15:35:38 @agent_ppo2.py:185][0m |          -0.0115 |          21.2965 |           0.2262 |
[32m[20221213 15:35:38 @agent_ppo2.py:185][0m |          -0.0133 |          21.1415 |           0.2260 |
[32m[20221213 15:35:38 @agent_ppo2.py:185][0m |          -0.0005 |          23.2177 |           0.2257 |
[32m[20221213 15:35:38 @agent_ppo2.py:185][0m |          -0.0121 |          20.9085 |           0.2255 |
[32m[20221213 15:35:38 @agent_ppo2.py:185][0m |          -0.0149 |          20.7419 |           0.2254 |
[32m[20221213 15:35:38 @agent_ppo2.py:185][0m |          -0.0176 |          20.6310 |           0.2251 |
[32m[20221213 15:35:38 @agent_ppo2.py:185][0m |          -0.0159 |          20.5234 |           0.2252 |
[32m[20221213 15:35:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:35:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 298.75
[32m[20221213 15:35:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 305.72
[32m[20221213 15:35:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.10
[32m[20221213 15:35:38 @agent_ppo2.py:143][0m Total time:      42.72 min
[32m[20221213 15:35:38 @agent_ppo2.py:145][0m 3837952 total steps have happened
[32m[20221213 15:35:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1874 --------------------------#
[32m[20221213 15:35:39 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0034 |          22.9117 |           0.2222 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0103 |          22.1561 |           0.2222 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0129 |          21.7375 |           0.2221 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0027 |          22.6403 |           0.2219 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0140 |          21.3553 |           0.2216 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0159 |          21.1295 |           0.2216 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0183 |          20.9981 |           0.2215 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0184 |          20.8931 |           0.2214 |
[32m[20221213 15:35:39 @agent_ppo2.py:185][0m |          -0.0113 |          21.7377 |           0.2214 |
[32m[20221213 15:35:40 @agent_ppo2.py:185][0m |          -0.0181 |          20.7062 |           0.2210 |
[32m[20221213 15:35:40 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:35:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.48
[32m[20221213 15:35:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.57
[32m[20221213 15:35:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.74
[32m[20221213 15:35:40 @agent_ppo2.py:143][0m Total time:      42.74 min
[32m[20221213 15:35:40 @agent_ppo2.py:145][0m 3840000 total steps have happened
[32m[20221213 15:35:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1875 --------------------------#
[32m[20221213 15:35:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:40 @agent_ppo2.py:185][0m |           0.0003 |          22.7522 |           0.2219 |
[32m[20221213 15:35:40 @agent_ppo2.py:185][0m |          -0.0067 |          21.9809 |           0.2218 |
[32m[20221213 15:35:40 @agent_ppo2.py:185][0m |          -0.0091 |          21.6164 |           0.2216 |
[32m[20221213 15:35:40 @agent_ppo2.py:185][0m |          -0.0122 |          21.3347 |           0.2218 |
[32m[20221213 15:35:40 @agent_ppo2.py:185][0m |          -0.0159 |          21.1240 |           0.2217 |
[32m[20221213 15:35:41 @agent_ppo2.py:185][0m |          -0.0152 |          20.9371 |           0.2218 |
[32m[20221213 15:35:41 @agent_ppo2.py:185][0m |          -0.0182 |          20.7832 |           0.2216 |
[32m[20221213 15:35:41 @agent_ppo2.py:185][0m |          -0.0193 |          20.6028 |           0.2215 |
[32m[20221213 15:35:41 @agent_ppo2.py:185][0m |          -0.0022 |          23.4021 |           0.2215 |
[32m[20221213 15:35:41 @agent_ppo2.py:185][0m |          -0.0192 |          20.4392 |           0.2209 |
[32m[20221213 15:35:41 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:35:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.21
[32m[20221213 15:35:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.14
[32m[20221213 15:35:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.63
[32m[20221213 15:35:41 @agent_ppo2.py:143][0m Total time:      42.77 min
[32m[20221213 15:35:41 @agent_ppo2.py:145][0m 3842048 total steps have happened
[32m[20221213 15:35:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1876 --------------------------#
[32m[20221213 15:35:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:41 @agent_ppo2.py:185][0m |           0.0005 |          24.2359 |           0.2219 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0082 |          23.8261 |           0.2216 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0080 |          23.6593 |           0.2213 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0060 |          24.5618 |           0.2212 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0140 |          23.0729 |           0.2207 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0159 |          22.8311 |           0.2208 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0136 |          22.8970 |           0.2207 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0122 |          23.0800 |           0.2206 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0186 |          22.6386 |           0.2208 |
[32m[20221213 15:35:42 @agent_ppo2.py:185][0m |          -0.0183 |          22.3321 |           0.2206 |
[32m[20221213 15:35:42 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:35:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.88
[32m[20221213 15:35:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.92
[32m[20221213 15:35:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.12
[32m[20221213 15:35:43 @agent_ppo2.py:143][0m Total time:      42.79 min
[32m[20221213 15:35:43 @agent_ppo2.py:145][0m 3844096 total steps have happened
[32m[20221213 15:35:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1877 --------------------------#
[32m[20221213 15:35:43 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:43 @agent_ppo2.py:185][0m |          -0.0012 |          24.0714 |           0.2271 |
[32m[20221213 15:35:43 @agent_ppo2.py:185][0m |          -0.0067 |          23.7203 |           0.2271 |
[32m[20221213 15:35:43 @agent_ppo2.py:185][0m |          -0.0112 |          23.6003 |           0.2269 |
[32m[20221213 15:35:43 @agent_ppo2.py:185][0m |          -0.0093 |          23.4718 |           0.2267 |
[32m[20221213 15:35:43 @agent_ppo2.py:185][0m |          -0.0131 |          23.3906 |           0.2272 |
[32m[20221213 15:35:43 @agent_ppo2.py:185][0m |          -0.0148 |          23.3463 |           0.2271 |
[32m[20221213 15:35:43 @agent_ppo2.py:185][0m |          -0.0153 |          23.2988 |           0.2271 |
[32m[20221213 15:35:44 @agent_ppo2.py:185][0m |          -0.0169 |          23.3026 |           0.2270 |
[32m[20221213 15:35:44 @agent_ppo2.py:185][0m |          -0.0159 |          23.2192 |           0.2270 |
[32m[20221213 15:35:44 @agent_ppo2.py:185][0m |          -0.0169 |          23.1729 |           0.2274 |
[32m[20221213 15:35:44 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:35:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.36
[32m[20221213 15:35:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.66
[32m[20221213 15:35:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.90
[32m[20221213 15:35:44 @agent_ppo2.py:143][0m Total time:      42.81 min
[32m[20221213 15:35:44 @agent_ppo2.py:145][0m 3846144 total steps have happened
[32m[20221213 15:35:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1878 --------------------------#
[32m[20221213 15:35:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:44 @agent_ppo2.py:185][0m |          -0.0025 |          24.2030 |           0.2228 |
[32m[20221213 15:35:44 @agent_ppo2.py:185][0m |          -0.0064 |          23.9466 |           0.2226 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0079 |          23.7732 |           0.2225 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0026 |          24.6845 |           0.2219 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0107 |          23.5401 |           0.2220 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0131 |          23.4552 |           0.2218 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0059 |          24.5893 |           0.2217 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0107 |          23.6250 |           0.2215 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0151 |          23.2837 |           0.2214 |
[32m[20221213 15:35:45 @agent_ppo2.py:185][0m |          -0.0145 |          23.3065 |           0.2217 |
[32m[20221213 15:35:45 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:35:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.94
[32m[20221213 15:35:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.76
[32m[20221213 15:35:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.70
[32m[20221213 15:35:45 @agent_ppo2.py:143][0m Total time:      42.84 min
[32m[20221213 15:35:45 @agent_ppo2.py:145][0m 3848192 total steps have happened
[32m[20221213 15:35:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1879 --------------------------#
[32m[20221213 15:35:46 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:35:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |           0.0042 |          23.4726 |           0.2248 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0056 |          22.8091 |           0.2250 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0048 |          22.8791 |           0.2249 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0099 |          22.4991 |           0.2245 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0105 |          22.4098 |           0.2246 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0025 |          24.9105 |           0.2246 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0131 |          22.1846 |           0.2246 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0149 |          22.0974 |           0.2246 |
[32m[20221213 15:35:46 @agent_ppo2.py:185][0m |          -0.0159 |          22.0548 |           0.2246 |
[32m[20221213 15:35:47 @agent_ppo2.py:185][0m |          -0.0152 |          22.0314 |           0.2246 |
[32m[20221213 15:35:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:35:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.74
[32m[20221213 15:35:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.25
[32m[20221213 15:35:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.15
[32m[20221213 15:35:47 @agent_ppo2.py:143][0m Total time:      42.86 min
[32m[20221213 15:35:47 @agent_ppo2.py:145][0m 3850240 total steps have happened
[32m[20221213 15:35:47 @agent_ppo2.py:121][0m #------------------------ Iteration 1880 --------------------------#
[32m[20221213 15:35:47 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:35:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:47 @agent_ppo2.py:185][0m |          -0.0010 |          24.1716 |           0.2194 |
[32m[20221213 15:35:47 @agent_ppo2.py:185][0m |           0.0009 |          25.5171 |           0.2187 |
[32m[20221213 15:35:47 @agent_ppo2.py:185][0m |          -0.0122 |          23.7023 |           0.2185 |
[32m[20221213 15:35:47 @agent_ppo2.py:185][0m |          -0.0111 |          23.5714 |           0.2182 |
[32m[20221213 15:35:48 @agent_ppo2.py:185][0m |          -0.0150 |          23.5226 |           0.2181 |
[32m[20221213 15:35:48 @agent_ppo2.py:185][0m |          -0.0154 |          23.4360 |           0.2180 |
[32m[20221213 15:35:48 @agent_ppo2.py:185][0m |          -0.0159 |          23.3489 |           0.2177 |
[32m[20221213 15:35:48 @agent_ppo2.py:185][0m |          -0.0134 |          23.5336 |           0.2176 |
[32m[20221213 15:35:48 @agent_ppo2.py:185][0m |          -0.0074 |          24.9005 |           0.2175 |
[32m[20221213 15:35:48 @agent_ppo2.py:185][0m |          -0.0163 |          23.3526 |           0.2172 |
[32m[20221213 15:35:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:35:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.53
[32m[20221213 15:35:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.91
[32m[20221213 15:35:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.72
[32m[20221213 15:35:48 @agent_ppo2.py:143][0m Total time:      42.88 min
[32m[20221213 15:35:48 @agent_ppo2.py:145][0m 3852288 total steps have happened
[32m[20221213 15:35:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1881 --------------------------#
[32m[20221213 15:35:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0011 |          22.6327 |           0.2186 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0021 |          22.4947 |           0.2182 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0098 |          22.2659 |           0.2175 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0110 |          22.1284 |           0.2175 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0120 |          22.0570 |           0.2175 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0138 |          21.9754 |           0.2173 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0107 |          22.3321 |           0.2172 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0147 |          21.9063 |           0.2173 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0174 |          21.8343 |           0.2173 |
[32m[20221213 15:35:49 @agent_ppo2.py:185][0m |          -0.0157 |          21.8140 |           0.2172 |
[32m[20221213 15:35:49 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:35:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.13
[32m[20221213 15:35:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.30
[32m[20221213 15:35:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.23
[32m[20221213 15:35:50 @agent_ppo2.py:143][0m Total time:      42.91 min
[32m[20221213 15:35:50 @agent_ppo2.py:145][0m 3854336 total steps have happened
[32m[20221213 15:35:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1882 --------------------------#
[32m[20221213 15:35:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:50 @agent_ppo2.py:185][0m |          -0.0025 |          23.3982 |           0.2212 |
[32m[20221213 15:35:50 @agent_ppo2.py:185][0m |          -0.0076 |          23.0530 |           0.2210 |
[32m[20221213 15:35:50 @agent_ppo2.py:185][0m |          -0.0107 |          22.7746 |           0.2203 |
[32m[20221213 15:35:50 @agent_ppo2.py:185][0m |          -0.0058 |          22.9603 |           0.2200 |
[32m[20221213 15:35:50 @agent_ppo2.py:185][0m |          -0.0107 |          22.3596 |           0.2197 |
[32m[20221213 15:35:50 @agent_ppo2.py:185][0m |          -0.0146 |          22.1605 |           0.2194 |
[32m[20221213 15:35:50 @agent_ppo2.py:185][0m |          -0.0139 |          22.0638 |           0.2194 |
[32m[20221213 15:35:51 @agent_ppo2.py:185][0m |          -0.0002 |          24.0301 |           0.2191 |
[32m[20221213 15:35:51 @agent_ppo2.py:185][0m |          -0.0167 |          21.9074 |           0.2190 |
[32m[20221213 15:35:51 @agent_ppo2.py:185][0m |          -0.0171 |          21.7460 |           0.2189 |
[32m[20221213 15:35:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:35:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.30
[32m[20221213 15:35:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 278.22
[32m[20221213 15:35:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.78
[32m[20221213 15:35:51 @agent_ppo2.py:143][0m Total time:      42.93 min
[32m[20221213 15:35:51 @agent_ppo2.py:145][0m 3856384 total steps have happened
[32m[20221213 15:35:51 @agent_ppo2.py:121][0m #------------------------ Iteration 1883 --------------------------#
[32m[20221213 15:35:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:51 @agent_ppo2.py:185][0m |          -0.0025 |          22.6794 |           0.2208 |
[32m[20221213 15:35:51 @agent_ppo2.py:185][0m |          -0.0082 |          22.3067 |           0.2207 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0127 |          22.1670 |           0.2209 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0133 |          22.0786 |           0.2208 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0147 |          22.0425 |           0.2208 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0146 |          21.9501 |           0.2212 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0072 |          24.7962 |           0.2210 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0172 |          21.8895 |           0.2214 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0170 |          21.7836 |           0.2212 |
[32m[20221213 15:35:52 @agent_ppo2.py:185][0m |          -0.0140 |          21.8837 |           0.2212 |
[32m[20221213 15:35:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:35:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.78
[32m[20221213 15:35:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.75
[32m[20221213 15:35:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.96
[32m[20221213 15:35:52 @agent_ppo2.py:143][0m Total time:      42.95 min
[32m[20221213 15:35:52 @agent_ppo2.py:145][0m 3858432 total steps have happened
[32m[20221213 15:35:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1884 --------------------------#
[32m[20221213 15:35:53 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0006 |          22.8032 |           0.2144 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0079 |          22.4843 |           0.2143 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0103 |          22.3843 |           0.2143 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0085 |          22.3406 |           0.2143 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0130 |          22.2374 |           0.2144 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0080 |          22.8810 |           0.2143 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0159 |          22.1915 |           0.2141 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0107 |          22.3983 |           0.2141 |
[32m[20221213 15:35:53 @agent_ppo2.py:185][0m |          -0.0145 |          22.1451 |           0.2140 |
[32m[20221213 15:35:54 @agent_ppo2.py:185][0m |          -0.0182 |          22.0805 |           0.2142 |
[32m[20221213 15:35:54 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:35:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.13
[32m[20221213 15:35:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.32
[32m[20221213 15:35:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.22
[32m[20221213 15:35:54 @agent_ppo2.py:143][0m Total time:      42.98 min
[32m[20221213 15:35:54 @agent_ppo2.py:145][0m 3860480 total steps have happened
[32m[20221213 15:35:54 @agent_ppo2.py:121][0m #------------------------ Iteration 1885 --------------------------#
[32m[20221213 15:35:54 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:54 @agent_ppo2.py:185][0m |           0.0002 |          21.3924 |           0.2156 |
[32m[20221213 15:35:54 @agent_ppo2.py:185][0m |          -0.0081 |          21.1056 |           0.2148 |
[32m[20221213 15:35:54 @agent_ppo2.py:185][0m |          -0.0108 |          20.9425 |           0.2145 |
[32m[20221213 15:35:54 @agent_ppo2.py:185][0m |          -0.0123 |          20.8080 |           0.2143 |
[32m[20221213 15:35:54 @agent_ppo2.py:185][0m |          -0.0131 |          20.7237 |           0.2136 |
[32m[20221213 15:35:55 @agent_ppo2.py:185][0m |          -0.0154 |          20.5895 |           0.2134 |
[32m[20221213 15:35:55 @agent_ppo2.py:185][0m |          -0.0121 |          20.5332 |           0.2132 |
[32m[20221213 15:35:55 @agent_ppo2.py:185][0m |          -0.0156 |          20.4426 |           0.2127 |
[32m[20221213 15:35:55 @agent_ppo2.py:185][0m |          -0.0162 |          20.3552 |           0.2131 |
[32m[20221213 15:35:55 @agent_ppo2.py:185][0m |          -0.0146 |          20.2881 |           0.2125 |
[32m[20221213 15:35:55 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:35:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.85
[32m[20221213 15:35:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.36
[32m[20221213 15:35:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.34
[32m[20221213 15:35:55 @agent_ppo2.py:143][0m Total time:      43.00 min
[32m[20221213 15:35:55 @agent_ppo2.py:145][0m 3862528 total steps have happened
[32m[20221213 15:35:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1886 --------------------------#
[32m[20221213 15:35:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |          -0.0009 |          22.6989 |           0.2163 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |           0.0039 |          24.4577 |           0.2161 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |           0.0008 |          24.0585 |           0.2156 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |           0.0004 |          24.3514 |           0.2148 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |          -0.0125 |          21.9435 |           0.2152 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |          -0.0118 |          21.8127 |           0.2149 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |          -0.0139 |          21.7359 |           0.2148 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |          -0.0126 |          21.6978 |           0.2146 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |          -0.0165 |          21.5986 |           0.2145 |
[32m[20221213 15:35:56 @agent_ppo2.py:185][0m |          -0.0180 |          21.5413 |           0.2143 |
[32m[20221213 15:35:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:35:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.15
[32m[20221213 15:35:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.59
[32m[20221213 15:35:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.99
[32m[20221213 15:35:57 @agent_ppo2.py:143][0m Total time:      43.02 min
[32m[20221213 15:35:57 @agent_ppo2.py:145][0m 3864576 total steps have happened
[32m[20221213 15:35:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1887 --------------------------#
[32m[20221213 15:35:57 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:57 @agent_ppo2.py:185][0m |          -0.0023 |          22.6103 |           0.2115 |
[32m[20221213 15:35:57 @agent_ppo2.py:185][0m |          -0.0071 |          21.8971 |           0.2115 |
[32m[20221213 15:35:57 @agent_ppo2.py:185][0m |          -0.0017 |          22.4458 |           0.2112 |
[32m[20221213 15:35:57 @agent_ppo2.py:185][0m |          -0.0124 |          21.3532 |           0.2110 |
[32m[20221213 15:35:57 @agent_ppo2.py:185][0m |          -0.0122 |          21.1963 |           0.2112 |
[32m[20221213 15:35:57 @agent_ppo2.py:185][0m |          -0.0157 |          21.0817 |           0.2111 |
[32m[20221213 15:35:58 @agent_ppo2.py:185][0m |          -0.0018 |          24.0471 |           0.2110 |
[32m[20221213 15:35:58 @agent_ppo2.py:185][0m |          -0.0135 |          20.8043 |           0.2101 |
[32m[20221213 15:35:58 @agent_ppo2.py:185][0m |          -0.0166 |          20.8271 |           0.2110 |
[32m[20221213 15:35:58 @agent_ppo2.py:185][0m |          -0.0150 |          20.7322 |           0.2109 |
[32m[20221213 15:35:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:35:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.12
[32m[20221213 15:35:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.54
[32m[20221213 15:35:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.61
[32m[20221213 15:35:58 @agent_ppo2.py:143][0m Total time:      43.05 min
[32m[20221213 15:35:58 @agent_ppo2.py:145][0m 3866624 total steps have happened
[32m[20221213 15:35:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1888 --------------------------#
[32m[20221213 15:35:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:35:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:35:58 @agent_ppo2.py:185][0m |           0.0028 |          23.4592 |           0.2070 |
[32m[20221213 15:35:58 @agent_ppo2.py:185][0m |          -0.0072 |          22.7949 |           0.2067 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0110 |          22.6314 |           0.2065 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0081 |          22.6638 |           0.2063 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0083 |          22.4504 |           0.2060 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0116 |          22.3788 |           0.2062 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0127 |          22.2809 |           0.2060 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0137 |          22.2418 |           0.2058 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0160 |          22.1543 |           0.2057 |
[32m[20221213 15:35:59 @agent_ppo2.py:185][0m |          -0.0166 |          22.1316 |           0.2056 |
[32m[20221213 15:35:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:35:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.96
[32m[20221213 15:35:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.35
[32m[20221213 15:35:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 294.69
[32m[20221213 15:35:59 @agent_ppo2.py:143][0m Total time:      43.07 min
[32m[20221213 15:35:59 @agent_ppo2.py:145][0m 3868672 total steps have happened
[32m[20221213 15:35:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1889 --------------------------#
[32m[20221213 15:36:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |          -0.0047 |          22.4154 |           0.2137 |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |          -0.0025 |          22.8496 |           0.2132 |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |          -0.0086 |          22.0379 |           0.2127 |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |          -0.0119 |          21.9533 |           0.2130 |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |          -0.0134 |          21.9182 |           0.2127 |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |           0.0071 |          25.1413 |           0.2127 |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |          -0.0140 |          21.8553 |           0.2115 |
[32m[20221213 15:36:00 @agent_ppo2.py:185][0m |          -0.0116 |          21.8073 |           0.2120 |
[32m[20221213 15:36:01 @agent_ppo2.py:185][0m |          -0.0161 |          21.7359 |           0.2120 |
[32m[20221213 15:36:01 @agent_ppo2.py:185][0m |          -0.0172 |          21.7323 |           0.2117 |
[32m[20221213 15:36:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 287.31
[32m[20221213 15:36:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.13
[32m[20221213 15:36:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.36
[32m[20221213 15:36:01 @agent_ppo2.py:143][0m Total time:      43.09 min
[32m[20221213 15:36:01 @agent_ppo2.py:145][0m 3870720 total steps have happened
[32m[20221213 15:36:01 @agent_ppo2.py:121][0m #------------------------ Iteration 1890 --------------------------#
[32m[20221213 15:36:01 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:36:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:01 @agent_ppo2.py:185][0m |          -0.0005 |          22.7272 |           0.2055 |
[32m[20221213 15:36:01 @agent_ppo2.py:185][0m |          -0.0040 |          22.6266 |           0.2051 |
[32m[20221213 15:36:01 @agent_ppo2.py:185][0m |           0.0063 |          24.3145 |           0.2051 |
[32m[20221213 15:36:02 @agent_ppo2.py:185][0m |          -0.0130 |          21.7108 |           0.2046 |
[32m[20221213 15:36:02 @agent_ppo2.py:185][0m |          -0.0110 |          21.6132 |           0.2045 |
[32m[20221213 15:36:02 @agent_ppo2.py:185][0m |          -0.0167 |          21.3946 |           0.2046 |
[32m[20221213 15:36:02 @agent_ppo2.py:185][0m |           0.0043 |          23.7074 |           0.2043 |
[32m[20221213 15:36:02 @agent_ppo2.py:185][0m |          -0.0116 |          21.1953 |           0.2042 |
[32m[20221213 15:36:02 @agent_ppo2.py:185][0m |          -0.0134 |          21.1107 |           0.2041 |
[32m[20221213 15:36:02 @agent_ppo2.py:185][0m |          -0.0188 |          20.9931 |           0.2041 |
[32m[20221213 15:36:02 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:36:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 232.45
[32m[20221213 15:36:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.35
[32m[20221213 15:36:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.13
[32m[20221213 15:36:02 @agent_ppo2.py:143][0m Total time:      43.12 min
[32m[20221213 15:36:02 @agent_ppo2.py:145][0m 3872768 total steps have happened
[32m[20221213 15:36:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1891 --------------------------#
[32m[20221213 15:36:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0026 |          22.8444 |           0.2100 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0078 |          22.3287 |           0.2095 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0077 |          22.0967 |           0.2096 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0094 |          21.9806 |           0.2094 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0120 |          21.8708 |           0.2097 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0111 |          21.8088 |           0.2095 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0129 |          21.7269 |           0.2095 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0149 |          21.6852 |           0.2093 |
[32m[20221213 15:36:03 @agent_ppo2.py:185][0m |          -0.0070 |          22.4976 |           0.2094 |
[32m[20221213 15:36:04 @agent_ppo2.py:185][0m |          -0.0170 |          21.6101 |           0.2095 |
[32m[20221213 15:36:04 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:36:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.68
[32m[20221213 15:36:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 282.76
[32m[20221213 15:36:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.17
[32m[20221213 15:36:04 @agent_ppo2.py:143][0m Total time:      43.14 min
[32m[20221213 15:36:04 @agent_ppo2.py:145][0m 3874816 total steps have happened
[32m[20221213 15:36:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1892 --------------------------#
[32m[20221213 15:36:04 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:04 @agent_ppo2.py:185][0m |          -0.0012 |          22.5794 |           0.2102 |
[32m[20221213 15:36:04 @agent_ppo2.py:185][0m |          -0.0042 |          22.4503 |           0.2096 |
[32m[20221213 15:36:04 @agent_ppo2.py:185][0m |          -0.0097 |          22.1247 |           0.2094 |
[32m[20221213 15:36:04 @agent_ppo2.py:185][0m |          -0.0059 |          22.2850 |           0.2092 |
[32m[20221213 15:36:04 @agent_ppo2.py:185][0m |           0.0005 |          23.9027 |           0.2087 |
[32m[20221213 15:36:05 @agent_ppo2.py:185][0m |          -0.0030 |          23.1249 |           0.2084 |
[32m[20221213 15:36:05 @agent_ppo2.py:185][0m |          -0.0056 |          23.6616 |           0.2085 |
[32m[20221213 15:36:05 @agent_ppo2.py:185][0m |          -0.0113 |          21.6963 |           0.2080 |
[32m[20221213 15:36:05 @agent_ppo2.py:185][0m |          -0.0138 |          21.7727 |           0.2083 |
[32m[20221213 15:36:05 @agent_ppo2.py:185][0m |          -0.0130 |          21.7484 |           0.2081 |
[32m[20221213 15:36:05 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:36:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.63
[32m[20221213 15:36:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.42
[32m[20221213 15:36:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.49
[32m[20221213 15:36:05 @agent_ppo2.py:143][0m Total time:      43.17 min
[32m[20221213 15:36:05 @agent_ppo2.py:145][0m 3876864 total steps have happened
[32m[20221213 15:36:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1893 --------------------------#
[32m[20221213 15:36:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0016 |          21.7357 |           0.2011 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0045 |          21.0968 |           0.2006 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0093 |          20.7817 |           0.2004 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0089 |          20.5374 |           0.2004 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0131 |          20.3914 |           0.2003 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0129 |          20.3064 |           0.2001 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0151 |          20.1849 |           0.2001 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0149 |          20.0894 |           0.2000 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0160 |          20.0768 |           0.2000 |
[32m[20221213 15:36:06 @agent_ppo2.py:185][0m |          -0.0058 |          20.9465 |           0.1999 |
[32m[20221213 15:36:06 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:36:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.79
[32m[20221213 15:36:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.61
[32m[20221213 15:36:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.19
[32m[20221213 15:36:07 @agent_ppo2.py:143][0m Total time:      43.19 min
[32m[20221213 15:36:07 @agent_ppo2.py:145][0m 3878912 total steps have happened
[32m[20221213 15:36:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1894 --------------------------#
[32m[20221213 15:36:07 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:07 @agent_ppo2.py:185][0m |          -0.0015 |          23.6442 |           0.1987 |
[32m[20221213 15:36:07 @agent_ppo2.py:185][0m |          -0.0071 |          23.0182 |           0.1985 |
[32m[20221213 15:36:07 @agent_ppo2.py:185][0m |          -0.0114 |          22.7567 |           0.1986 |
[32m[20221213 15:36:07 @agent_ppo2.py:185][0m |          -0.0093 |          22.7403 |           0.1983 |
[32m[20221213 15:36:07 @agent_ppo2.py:185][0m |          -0.0137 |          22.4452 |           0.1983 |
[32m[20221213 15:36:07 @agent_ppo2.py:185][0m |          -0.0053 |          24.4697 |           0.1979 |
[32m[20221213 15:36:07 @agent_ppo2.py:185][0m |          -0.0175 |          22.3054 |           0.1980 |
[32m[20221213 15:36:08 @agent_ppo2.py:185][0m |          -0.0160 |          22.1854 |           0.1978 |
[32m[20221213 15:36:08 @agent_ppo2.py:185][0m |          -0.0196 |          22.1186 |           0.1979 |
[32m[20221213 15:36:08 @agent_ppo2.py:185][0m |          -0.0196 |          22.0362 |           0.1978 |
[32m[20221213 15:36:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:36:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 259.22
[32m[20221213 15:36:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.17
[32m[20221213 15:36:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.63
[32m[20221213 15:36:08 @agent_ppo2.py:143][0m Total time:      43.21 min
[32m[20221213 15:36:08 @agent_ppo2.py:145][0m 3880960 total steps have happened
[32m[20221213 15:36:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1895 --------------------------#
[32m[20221213 15:36:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:08 @agent_ppo2.py:185][0m |          -0.0012 |          22.2569 |           0.1991 |
[32m[20221213 15:36:08 @agent_ppo2.py:185][0m |          -0.0094 |          21.7100 |           0.1993 |
[32m[20221213 15:36:08 @agent_ppo2.py:185][0m |          -0.0100 |          21.2826 |           0.1989 |
[32m[20221213 15:36:09 @agent_ppo2.py:185][0m |          -0.0117 |          20.9744 |           0.1988 |
[32m[20221213 15:36:09 @agent_ppo2.py:185][0m |          -0.0136 |          20.7361 |           0.1988 |
[32m[20221213 15:36:09 @agent_ppo2.py:185][0m |          -0.0159 |          20.5797 |           0.1989 |
[32m[20221213 15:36:09 @agent_ppo2.py:185][0m |          -0.0072 |          21.5148 |           0.1987 |
[32m[20221213 15:36:09 @agent_ppo2.py:185][0m |          -0.0129 |          20.4406 |           0.1987 |
[32m[20221213 15:36:09 @agent_ppo2.py:185][0m |          -0.0176 |          20.1475 |           0.1986 |
[32m[20221213 15:36:09 @agent_ppo2.py:185][0m |          -0.0042 |          21.5523 |           0.1986 |
[32m[20221213 15:36:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.63
[32m[20221213 15:36:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.74
[32m[20221213 15:36:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.39
[32m[20221213 15:36:09 @agent_ppo2.py:143][0m Total time:      43.24 min
[32m[20221213 15:36:09 @agent_ppo2.py:145][0m 3883008 total steps have happened
[32m[20221213 15:36:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1896 --------------------------#
[32m[20221213 15:36:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |           0.0003 |          22.6021 |           0.2021 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |           0.0064 |          23.6036 |           0.2016 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0031 |          22.2624 |           0.2018 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0068 |          22.2612 |           0.2014 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0099 |          21.8936 |           0.2016 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0126 |          21.8059 |           0.2013 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0116 |          21.7338 |           0.2014 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0016 |          24.4093 |           0.2012 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0135 |          21.6820 |           0.2013 |
[32m[20221213 15:36:10 @agent_ppo2.py:185][0m |          -0.0127 |          21.6516 |           0.2011 |
[32m[20221213 15:36:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:36:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 264.12
[32m[20221213 15:36:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 268.40
[32m[20221213 15:36:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.99
[32m[20221213 15:36:11 @agent_ppo2.py:143][0m Total time:      43.26 min
[32m[20221213 15:36:11 @agent_ppo2.py:145][0m 3885056 total steps have happened
[32m[20221213 15:36:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1897 --------------------------#
[32m[20221213 15:36:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:11 @agent_ppo2.py:185][0m |          -0.0008 |          21.4938 |           0.2027 |
[32m[20221213 15:36:11 @agent_ppo2.py:185][0m |          -0.0102 |          20.8874 |           0.2025 |
[32m[20221213 15:36:11 @agent_ppo2.py:185][0m |          -0.0112 |          20.6294 |           0.2024 |
[32m[20221213 15:36:11 @agent_ppo2.py:185][0m |          -0.0132 |          20.4211 |           0.2023 |
[32m[20221213 15:36:11 @agent_ppo2.py:185][0m |          -0.0160 |          20.2802 |           0.2020 |
[32m[20221213 15:36:12 @agent_ppo2.py:185][0m |          -0.0159 |          20.1255 |           0.2021 |
[32m[20221213 15:36:12 @agent_ppo2.py:185][0m |          -0.0155 |          20.0079 |           0.2020 |
[32m[20221213 15:36:12 @agent_ppo2.py:185][0m |          -0.0154 |          19.8903 |           0.2019 |
[32m[20221213 15:36:12 @agent_ppo2.py:185][0m |          -0.0169 |          19.7846 |           0.2019 |
[32m[20221213 15:36:12 @agent_ppo2.py:185][0m |          -0.0176 |          19.6599 |           0.2018 |
[32m[20221213 15:36:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.92
[32m[20221213 15:36:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.77
[32m[20221213 15:36:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.58
[32m[20221213 15:36:12 @agent_ppo2.py:143][0m Total time:      43.28 min
[32m[20221213 15:36:12 @agent_ppo2.py:145][0m 3887104 total steps have happened
[32m[20221213 15:36:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1898 --------------------------#
[32m[20221213 15:36:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:12 @agent_ppo2.py:185][0m |          -0.0009 |          22.8348 |           0.1983 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0055 |          22.2822 |           0.1973 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0130 |          22.0205 |           0.1972 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0108 |          21.8602 |           0.1969 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0126 |          21.7571 |           0.1968 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0160 |          21.6426 |           0.1964 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0099 |          22.3255 |           0.1965 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0119 |          21.9002 |           0.1962 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0160 |          21.5757 |           0.1962 |
[32m[20221213 15:36:13 @agent_ppo2.py:185][0m |          -0.0178 |          21.4813 |           0.1959 |
[32m[20221213 15:36:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 282.07
[32m[20221213 15:36:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.62
[32m[20221213 15:36:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 299.04
[32m[20221213 15:36:14 @agent_ppo2.py:143][0m Total time:      43.31 min
[32m[20221213 15:36:14 @agent_ppo2.py:145][0m 3889152 total steps have happened
[32m[20221213 15:36:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1899 --------------------------#
[32m[20221213 15:36:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0032 |          21.5091 |           0.1983 |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0088 |          21.1570 |           0.1985 |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0101 |          21.0114 |           0.1982 |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0047 |          21.6434 |           0.1979 |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0067 |          21.0898 |           0.1984 |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0134 |          20.8091 |           0.1981 |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0151 |          20.7498 |           0.1984 |
[32m[20221213 15:36:14 @agent_ppo2.py:185][0m |          -0.0140 |          20.7014 |           0.1983 |
[32m[20221213 15:36:15 @agent_ppo2.py:185][0m |          -0.0151 |          20.6826 |           0.1981 |
[32m[20221213 15:36:15 @agent_ppo2.py:185][0m |          -0.0147 |          20.6012 |           0.1978 |
[32m[20221213 15:36:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.44
[32m[20221213 15:36:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.00
[32m[20221213 15:36:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.99
[32m[20221213 15:36:15 @agent_ppo2.py:143][0m Total time:      43.33 min
[32m[20221213 15:36:15 @agent_ppo2.py:145][0m 3891200 total steps have happened
[32m[20221213 15:36:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1900 --------------------------#
[32m[20221213 15:36:15 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:36:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:15 @agent_ppo2.py:185][0m |           0.0084 |          23.1402 |           0.2018 |
[32m[20221213 15:36:15 @agent_ppo2.py:185][0m |          -0.0050 |          21.4881 |           0.2014 |
[32m[20221213 15:36:15 @agent_ppo2.py:185][0m |          -0.0114 |          21.2553 |           0.2011 |
[32m[20221213 15:36:16 @agent_ppo2.py:185][0m |          -0.0090 |          21.1571 |           0.2011 |
[32m[20221213 15:36:16 @agent_ppo2.py:185][0m |          -0.0125 |          21.0220 |           0.2014 |
[32m[20221213 15:36:16 @agent_ppo2.py:185][0m |          -0.0131 |          20.9381 |           0.2010 |
[32m[20221213 15:36:16 @agent_ppo2.py:185][0m |          -0.0141 |          20.8714 |           0.2011 |
[32m[20221213 15:36:16 @agent_ppo2.py:185][0m |          -0.0168 |          20.7801 |           0.2008 |
[32m[20221213 15:36:16 @agent_ppo2.py:185][0m |          -0.0174 |          20.7363 |           0.2009 |
[32m[20221213 15:36:16 @agent_ppo2.py:185][0m |          -0.0188 |          20.6952 |           0.2007 |
[32m[20221213 15:36:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 254.34
[32m[20221213 15:36:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.19
[32m[20221213 15:36:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.36
[32m[20221213 15:36:16 @agent_ppo2.py:143][0m Total time:      43.35 min
[32m[20221213 15:36:16 @agent_ppo2.py:145][0m 3893248 total steps have happened
[32m[20221213 15:36:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1901 --------------------------#
[32m[20221213 15:36:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0019 |          21.7029 |           0.1995 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0062 |          21.5592 |           0.1989 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0095 |          21.1415 |           0.1988 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0017 |          21.6461 |           0.1991 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0121 |          20.8845 |           0.1990 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0153 |          20.7596 |           0.1989 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0160 |          20.6955 |           0.1989 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0161 |          20.5727 |           0.1992 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0160 |          20.5332 |           0.1991 |
[32m[20221213 15:36:17 @agent_ppo2.py:185][0m |          -0.0148 |          20.4827 |           0.1992 |
[32m[20221213 15:36:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.28
[32m[20221213 15:36:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.12
[32m[20221213 15:36:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 298.88
[32m[20221213 15:36:18 @agent_ppo2.py:143][0m Total time:      43.37 min
[32m[20221213 15:36:18 @agent_ppo2.py:145][0m 3895296 total steps have happened
[32m[20221213 15:36:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1902 --------------------------#
[32m[20221213 15:36:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:18 @agent_ppo2.py:185][0m |          -0.0025 |          21.5257 |           0.1990 |
[32m[20221213 15:36:18 @agent_ppo2.py:185][0m |          -0.0088 |          21.0678 |           0.1991 |
[32m[20221213 15:36:18 @agent_ppo2.py:185][0m |          -0.0111 |          20.8152 |           0.1988 |
[32m[20221213 15:36:18 @agent_ppo2.py:185][0m |          -0.0134 |          20.6725 |           0.1986 |
[32m[20221213 15:36:18 @agent_ppo2.py:185][0m |          -0.0135 |          20.4894 |           0.1985 |
[32m[20221213 15:36:18 @agent_ppo2.py:185][0m |          -0.0151 |          20.3827 |           0.1985 |
[32m[20221213 15:36:18 @agent_ppo2.py:185][0m |          -0.0104 |          20.9892 |           0.1984 |
[32m[20221213 15:36:19 @agent_ppo2.py:185][0m |          -0.0107 |          20.4097 |           0.1987 |
[32m[20221213 15:36:19 @agent_ppo2.py:185][0m |          -0.0115 |          20.4354 |           0.1987 |
[32m[20221213 15:36:19 @agent_ppo2.py:185][0m |          -0.0115 |          21.0426 |           0.1986 |
[32m[20221213 15:36:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.23
[32m[20221213 15:36:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.97
[32m[20221213 15:36:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.29
[32m[20221213 15:36:19 @agent_ppo2.py:143][0m Total time:      43.39 min
[32m[20221213 15:36:19 @agent_ppo2.py:145][0m 3897344 total steps have happened
[32m[20221213 15:36:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1903 --------------------------#
[32m[20221213 15:36:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:19 @agent_ppo2.py:185][0m |           0.0032 |          24.1782 |           0.2025 |
[32m[20221213 15:36:19 @agent_ppo2.py:185][0m |          -0.0074 |          22.2936 |           0.2028 |
[32m[20221213 15:36:19 @agent_ppo2.py:185][0m |          -0.0082 |          21.9278 |           0.2026 |
[32m[20221213 15:36:20 @agent_ppo2.py:185][0m |          -0.0121 |          21.6399 |           0.2024 |
[32m[20221213 15:36:20 @agent_ppo2.py:185][0m |          -0.0137 |          21.6135 |           0.2025 |
[32m[20221213 15:36:20 @agent_ppo2.py:185][0m |          -0.0152 |          21.3072 |           0.2022 |
[32m[20221213 15:36:20 @agent_ppo2.py:185][0m |          -0.0137 |          21.5772 |           0.2022 |
[32m[20221213 15:36:20 @agent_ppo2.py:185][0m |          -0.0160 |          21.0641 |           0.2022 |
[32m[20221213 15:36:20 @agent_ppo2.py:185][0m |          -0.0198 |          21.1163 |           0.2022 |
[32m[20221213 15:36:20 @agent_ppo2.py:185][0m |          -0.0204 |          20.9469 |           0.2022 |
[32m[20221213 15:36:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 235.11
[32m[20221213 15:36:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 246.49
[32m[20221213 15:36:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.77
[32m[20221213 15:36:20 @agent_ppo2.py:143][0m Total time:      43.42 min
[32m[20221213 15:36:20 @agent_ppo2.py:145][0m 3899392 total steps have happened
[32m[20221213 15:36:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1904 --------------------------#
[32m[20221213 15:36:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |           0.0065 |          22.8824 |           0.1983 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0067 |          22.0998 |           0.1981 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0083 |          21.9315 |           0.1983 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0074 |          21.8065 |           0.1982 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0098 |          21.6828 |           0.1980 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0117 |          21.6391 |           0.1981 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0109 |          21.5113 |           0.1982 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0169 |          21.4313 |           0.1982 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0165 |          21.3706 |           0.1982 |
[32m[20221213 15:36:21 @agent_ppo2.py:185][0m |          -0.0092 |          21.9052 |           0.1984 |
[32m[20221213 15:36:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:36:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.95
[32m[20221213 15:36:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.19
[32m[20221213 15:36:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.83
[32m[20221213 15:36:22 @agent_ppo2.py:143][0m Total time:      43.44 min
[32m[20221213 15:36:22 @agent_ppo2.py:145][0m 3901440 total steps have happened
[32m[20221213 15:36:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1905 --------------------------#
[32m[20221213 15:36:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:22 @agent_ppo2.py:185][0m |           0.0025 |          21.6169 |           0.2016 |
[32m[20221213 15:36:22 @agent_ppo2.py:185][0m |           0.0018 |          21.9801 |           0.2010 |
[32m[20221213 15:36:22 @agent_ppo2.py:185][0m |          -0.0052 |          21.0597 |           0.2010 |
[32m[20221213 15:36:22 @agent_ppo2.py:185][0m |          -0.0113 |          20.6896 |           0.2007 |
[32m[20221213 15:36:22 @agent_ppo2.py:185][0m |          -0.0078 |          21.0474 |           0.2008 |
[32m[20221213 15:36:22 @agent_ppo2.py:185][0m |          -0.0133 |          20.6155 |           0.2007 |
[32m[20221213 15:36:23 @agent_ppo2.py:185][0m |          -0.0153 |          20.5464 |           0.2005 |
[32m[20221213 15:36:23 @agent_ppo2.py:185][0m |          -0.0167 |          20.4798 |           0.2004 |
[32m[20221213 15:36:23 @agent_ppo2.py:185][0m |          -0.0179 |          20.4488 |           0.2003 |
[32m[20221213 15:36:23 @agent_ppo2.py:185][0m |          -0.0180 |          20.4185 |           0.2004 |
[32m[20221213 15:36:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:36:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.43
[32m[20221213 15:36:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.04
[32m[20221213 15:36:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.90
[32m[20221213 15:36:23 @agent_ppo2.py:143][0m Total time:      43.46 min
[32m[20221213 15:36:23 @agent_ppo2.py:145][0m 3903488 total steps have happened
[32m[20221213 15:36:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1906 --------------------------#
[32m[20221213 15:36:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:23 @agent_ppo2.py:185][0m |          -0.0004 |          21.6060 |           0.2036 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0074 |          21.0280 |           0.2030 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0073 |          20.7420 |           0.2029 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0099 |          20.5105 |           0.2026 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0131 |          20.3225 |           0.2027 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0114 |          20.2322 |           0.2024 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0157 |          19.9935 |           0.2024 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0051 |          20.7251 |           0.2022 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0174 |          19.7150 |           0.2022 |
[32m[20221213 15:36:24 @agent_ppo2.py:185][0m |          -0.0181 |          19.5912 |           0.2020 |
[32m[20221213 15:36:24 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:36:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.12
[32m[20221213 15:36:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.40
[32m[20221213 15:36:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.55
[32m[20221213 15:36:24 @agent_ppo2.py:143][0m Total time:      43.49 min
[32m[20221213 15:36:24 @agent_ppo2.py:145][0m 3905536 total steps have happened
[32m[20221213 15:36:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1907 --------------------------#
[32m[20221213 15:36:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |           0.0024 |          23.4274 |           0.2009 |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |           0.0018 |          24.0483 |           0.2007 |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |          -0.0077 |          22.7141 |           0.2003 |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |          -0.0094 |          22.5269 |           0.2002 |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |          -0.0015 |          23.0728 |           0.1999 |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |          -0.0132 |          22.3461 |           0.2000 |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |          -0.0157 |          22.2979 |           0.2001 |
[32m[20221213 15:36:25 @agent_ppo2.py:185][0m |          -0.0153 |          22.2435 |           0.2000 |
[32m[20221213 15:36:26 @agent_ppo2.py:185][0m |          -0.0116 |          22.2306 |           0.1998 |
[32m[20221213 15:36:26 @agent_ppo2.py:185][0m |          -0.0042 |          25.0086 |           0.2000 |
[32m[20221213 15:36:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.38
[32m[20221213 15:36:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.31
[32m[20221213 15:36:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.07
[32m[20221213 15:36:26 @agent_ppo2.py:143][0m Total time:      43.51 min
[32m[20221213 15:36:26 @agent_ppo2.py:145][0m 3907584 total steps have happened
[32m[20221213 15:36:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1908 --------------------------#
[32m[20221213 15:36:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:26 @agent_ppo2.py:185][0m |           0.0107 |          23.1753 |           0.1983 |
[32m[20221213 15:36:26 @agent_ppo2.py:185][0m |          -0.0059 |          21.6768 |           0.1981 |
[32m[20221213 15:36:26 @agent_ppo2.py:185][0m |           0.0007 |          22.2077 |           0.1982 |
[32m[20221213 15:36:26 @agent_ppo2.py:185][0m |          -0.0093 |          21.4234 |           0.1980 |
[32m[20221213 15:36:27 @agent_ppo2.py:185][0m |          -0.0015 |          22.9687 |           0.1977 |
[32m[20221213 15:36:27 @agent_ppo2.py:185][0m |          -0.0100 |          21.2941 |           0.1976 |
[32m[20221213 15:36:27 @agent_ppo2.py:185][0m |          -0.0126 |          21.1765 |           0.1980 |
[32m[20221213 15:36:27 @agent_ppo2.py:185][0m |          -0.0046 |          23.0124 |           0.1983 |
[32m[20221213 15:36:27 @agent_ppo2.py:185][0m |          -0.0106 |          21.1683 |           0.1976 |
[32m[20221213 15:36:27 @agent_ppo2.py:185][0m |          -0.0159 |          21.0682 |           0.1980 |
[32m[20221213 15:36:27 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.40
[32m[20221213 15:36:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.20
[32m[20221213 15:36:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.73
[32m[20221213 15:36:27 @agent_ppo2.py:143][0m Total time:      43.53 min
[32m[20221213 15:36:27 @agent_ppo2.py:145][0m 3909632 total steps have happened
[32m[20221213 15:36:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1909 --------------------------#
[32m[20221213 15:36:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0004 |          22.0011 |           0.2052 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0073 |          21.3948 |           0.2053 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0112 |          21.1068 |           0.2051 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0051 |          21.5734 |           0.2050 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0023 |          22.7148 |           0.2050 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0049 |          22.8356 |           0.2050 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0173 |          20.4361 |           0.2049 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0168 |          20.2287 |           0.2049 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0164 |          20.1623 |           0.2049 |
[32m[20221213 15:36:28 @agent_ppo2.py:185][0m |          -0.0169 |          20.0534 |           0.2048 |
[32m[20221213 15:36:28 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:36:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 286.67
[32m[20221213 15:36:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.02
[32m[20221213 15:36:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.77
[32m[20221213 15:36:28 @agent_ppo2.py:143][0m Total time:      43.55 min
[32m[20221213 15:36:28 @agent_ppo2.py:145][0m 3911680 total steps have happened
[32m[20221213 15:36:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1910 --------------------------#
[32m[20221213 15:36:29 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:36:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0029 |          23.7130 |           0.2061 |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0104 |          22.9543 |           0.2056 |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0070 |          23.1269 |           0.2056 |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0118 |          22.4373 |           0.2054 |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0150 |          22.2648 |           0.2051 |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0077 |          23.9925 |           0.2051 |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0133 |          21.9064 |           0.2050 |
[32m[20221213 15:36:29 @agent_ppo2.py:185][0m |          -0.0151 |          21.8326 |           0.2053 |
[32m[20221213 15:36:30 @agent_ppo2.py:185][0m |          -0.0185 |          21.5415 |           0.2052 |
[32m[20221213 15:36:30 @agent_ppo2.py:185][0m |          -0.0198 |          21.3595 |           0.2051 |
[32m[20221213 15:36:30 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:36:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 244.71
[32m[20221213 15:36:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.59
[32m[20221213 15:36:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.73
[32m[20221213 15:36:30 @agent_ppo2.py:143][0m Total time:      43.58 min
[32m[20221213 15:36:30 @agent_ppo2.py:145][0m 3913728 total steps have happened
[32m[20221213 15:36:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1911 --------------------------#
[32m[20221213 15:36:30 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:36:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:30 @agent_ppo2.py:185][0m |           0.0057 |          22.5294 |           0.2028 |
[32m[20221213 15:36:30 @agent_ppo2.py:185][0m |          -0.0069 |          21.6177 |           0.2025 |
[32m[20221213 15:36:30 @agent_ppo2.py:185][0m |          -0.0079 |          21.3476 |           0.2023 |
[32m[20221213 15:36:31 @agent_ppo2.py:185][0m |          -0.0104 |          21.1917 |           0.2020 |
[32m[20221213 15:36:31 @agent_ppo2.py:185][0m |          -0.0031 |          21.1493 |           0.2020 |
[32m[20221213 15:36:31 @agent_ppo2.py:185][0m |          -0.0106 |          20.8997 |           0.2018 |
[32m[20221213 15:36:31 @agent_ppo2.py:185][0m |          -0.0113 |          20.7307 |           0.2019 |
[32m[20221213 15:36:31 @agent_ppo2.py:185][0m |          -0.0130 |          20.6092 |           0.2017 |
[32m[20221213 15:36:31 @agent_ppo2.py:185][0m |          -0.0149 |          20.4783 |           0.2016 |
[32m[20221213 15:36:31 @agent_ppo2.py:185][0m |          -0.0163 |          20.3800 |           0.2015 |
[32m[20221213 15:36:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:36:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 222.66
[32m[20221213 15:36:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 265.85
[32m[20221213 15:36:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 254.28
[32m[20221213 15:36:31 @agent_ppo2.py:143][0m Total time:      43.60 min
[32m[20221213 15:36:31 @agent_ppo2.py:145][0m 3915776 total steps have happened
[32m[20221213 15:36:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1912 --------------------------#
[32m[20221213 15:36:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0025 |          22.2238 |           0.2056 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0032 |          22.2501 |           0.2053 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0143 |          21.0234 |           0.2051 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0140 |          20.8326 |           0.2051 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0120 |          20.6998 |           0.2050 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0164 |          20.4924 |           0.2050 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0172 |          20.3869 |           0.2048 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0168 |          20.4485 |           0.2052 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0189 |          20.2075 |           0.2053 |
[32m[20221213 15:36:32 @agent_ppo2.py:185][0m |          -0.0208 |          20.1160 |           0.2054 |
[32m[20221213 15:36:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:36:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.37
[32m[20221213 15:36:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.21
[32m[20221213 15:36:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.46
[32m[20221213 15:36:33 @agent_ppo2.py:143][0m Total time:      43.62 min
[32m[20221213 15:36:33 @agent_ppo2.py:145][0m 3917824 total steps have happened
[32m[20221213 15:36:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1913 --------------------------#
[32m[20221213 15:36:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:33 @agent_ppo2.py:185][0m |           0.0111 |          24.3066 |           0.2011 |
[32m[20221213 15:36:33 @agent_ppo2.py:185][0m |          -0.0032 |          21.6966 |           0.2002 |
[32m[20221213 15:36:33 @agent_ppo2.py:185][0m |          -0.0135 |          20.6666 |           0.2004 |
[32m[20221213 15:36:33 @agent_ppo2.py:185][0m |          -0.0131 |          20.4539 |           0.2002 |
[32m[20221213 15:36:33 @agent_ppo2.py:185][0m |          -0.0149 |          20.2659 |           0.2002 |
[32m[20221213 15:36:33 @agent_ppo2.py:185][0m |          -0.0171 |          20.1296 |           0.1999 |
[32m[20221213 15:36:33 @agent_ppo2.py:185][0m |          -0.0141 |          20.0987 |           0.1999 |
[32m[20221213 15:36:34 @agent_ppo2.py:185][0m |          -0.0128 |          19.8936 |           0.1997 |
[32m[20221213 15:36:34 @agent_ppo2.py:185][0m |          -0.0163 |          19.8486 |           0.1997 |
[32m[20221213 15:36:34 @agent_ppo2.py:185][0m |          -0.0180 |          19.7138 |           0.1996 |
[32m[20221213 15:36:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:36:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.80
[32m[20221213 15:36:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.14
[32m[20221213 15:36:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 300.81
[32m[20221213 15:36:34 @agent_ppo2.py:143][0m Total time:      43.65 min
[32m[20221213 15:36:34 @agent_ppo2.py:145][0m 3919872 total steps have happened
[32m[20221213 15:36:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1914 --------------------------#
[32m[20221213 15:36:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:34 @agent_ppo2.py:185][0m |          -0.0013 |          23.7918 |           0.2046 |
[32m[20221213 15:36:34 @agent_ppo2.py:185][0m |          -0.0090 |          23.3931 |           0.2038 |
[32m[20221213 15:36:34 @agent_ppo2.py:185][0m |          -0.0108 |          23.1993 |           0.2033 |
[32m[20221213 15:36:35 @agent_ppo2.py:185][0m |          -0.0098 |          23.1431 |           0.2030 |
[32m[20221213 15:36:35 @agent_ppo2.py:185][0m |          -0.0134 |          22.9938 |           0.2027 |
[32m[20221213 15:36:35 @agent_ppo2.py:185][0m |          -0.0130 |          22.9243 |           0.2027 |
[32m[20221213 15:36:35 @agent_ppo2.py:185][0m |          -0.0149 |          22.8596 |           0.2026 |
[32m[20221213 15:36:35 @agent_ppo2.py:185][0m |          -0.0156 |          22.8110 |           0.2025 |
[32m[20221213 15:36:35 @agent_ppo2.py:185][0m |          -0.0074 |          23.3248 |           0.2023 |
[32m[20221213 15:36:35 @agent_ppo2.py:185][0m |          -0.0177 |          22.7535 |           0.2021 |
[32m[20221213 15:36:35 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:36:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.45
[32m[20221213 15:36:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.97
[32m[20221213 15:36:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.05
[32m[20221213 15:36:35 @agent_ppo2.py:143][0m Total time:      43.67 min
[32m[20221213 15:36:35 @agent_ppo2.py:145][0m 3921920 total steps have happened
[32m[20221213 15:36:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1915 --------------------------#
[32m[20221213 15:36:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |           0.0016 |          23.1429 |           0.1988 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0053 |          22.7871 |           0.1994 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0090 |          22.6261 |           0.1992 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0099 |          22.4809 |           0.1990 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0074 |          22.4937 |           0.1990 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |           0.0001 |          24.4891 |           0.1992 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0116 |          22.3436 |           0.1993 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0126 |          22.2502 |           0.1991 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0136 |          22.2566 |           0.1993 |
[32m[20221213 15:36:36 @agent_ppo2.py:185][0m |          -0.0165 |          22.1734 |           0.1993 |
[32m[20221213 15:36:36 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:36:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 294.73
[32m[20221213 15:36:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 305.43
[32m[20221213 15:36:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.45
[32m[20221213 15:36:37 @agent_ppo2.py:143][0m Total time:      43.69 min
[32m[20221213 15:36:37 @agent_ppo2.py:145][0m 3923968 total steps have happened
[32m[20221213 15:36:37 @agent_ppo2.py:121][0m #------------------------ Iteration 1916 --------------------------#
[32m[20221213 15:36:37 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:37 @agent_ppo2.py:185][0m |          -0.0013 |          22.7190 |           0.2014 |
[32m[20221213 15:36:37 @agent_ppo2.py:185][0m |          -0.0086 |          22.4073 |           0.2010 |
[32m[20221213 15:36:37 @agent_ppo2.py:185][0m |          -0.0102 |          22.2459 |           0.2008 |
[32m[20221213 15:36:37 @agent_ppo2.py:185][0m |          -0.0123 |          22.1464 |           0.2008 |
[32m[20221213 15:36:37 @agent_ppo2.py:185][0m |          -0.0147 |          22.0719 |           0.2007 |
[32m[20221213 15:36:37 @agent_ppo2.py:185][0m |          -0.0168 |          22.0410 |           0.2004 |
[32m[20221213 15:36:38 @agent_ppo2.py:185][0m |          -0.0059 |          24.8045 |           0.2007 |
[32m[20221213 15:36:38 @agent_ppo2.py:185][0m |          -0.0154 |          22.0364 |           0.2004 |
[32m[20221213 15:36:38 @agent_ppo2.py:185][0m |          -0.0164 |          21.9082 |           0.2002 |
[32m[20221213 15:36:38 @agent_ppo2.py:185][0m |          -0.0155 |          21.8794 |           0.2003 |
[32m[20221213 15:36:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.25
[32m[20221213 15:36:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.35
[32m[20221213 15:36:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 253.31
[32m[20221213 15:36:38 @agent_ppo2.py:143][0m Total time:      43.71 min
[32m[20221213 15:36:38 @agent_ppo2.py:145][0m 3926016 total steps have happened
[32m[20221213 15:36:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1917 --------------------------#
[32m[20221213 15:36:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:38 @agent_ppo2.py:185][0m |          -0.0012 |          22.2617 |           0.2013 |
[32m[20221213 15:36:38 @agent_ppo2.py:185][0m |          -0.0049 |          21.7671 |           0.2005 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0095 |          21.4056 |           0.2003 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0121 |          21.2091 |           0.2005 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0122 |          21.0041 |           0.2002 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0137 |          20.8668 |           0.2000 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0101 |          20.8139 |           0.1998 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0155 |          20.7039 |           0.1997 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0155 |          20.6056 |           0.1999 |
[32m[20221213 15:36:39 @agent_ppo2.py:185][0m |          -0.0177 |          20.5779 |           0.1999 |
[32m[20221213 15:36:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.69
[32m[20221213 15:36:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 309.08
[32m[20221213 15:36:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.05
[32m[20221213 15:36:39 @agent_ppo2.py:143][0m Total time:      43.74 min
[32m[20221213 15:36:39 @agent_ppo2.py:145][0m 3928064 total steps have happened
[32m[20221213 15:36:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1918 --------------------------#
[32m[20221213 15:36:40 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |           0.0002 |          21.8155 |           0.1951 |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |          -0.0091 |          21.5444 |           0.1948 |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |          -0.0046 |          22.3381 |           0.1948 |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |          -0.0123 |          21.2337 |           0.1946 |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |          -0.0151 |          21.1499 |           0.1946 |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |          -0.0154 |          21.0941 |           0.1945 |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |          -0.0099 |          21.4749 |           0.1946 |
[32m[20221213 15:36:40 @agent_ppo2.py:185][0m |          -0.0171 |          20.9467 |           0.1941 |
[32m[20221213 15:36:41 @agent_ppo2.py:185][0m |          -0.0169 |          20.9310 |           0.1944 |
[32m[20221213 15:36:41 @agent_ppo2.py:185][0m |          -0.0179 |          20.8701 |           0.1942 |
[32m[20221213 15:36:41 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:36:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.52
[32m[20221213 15:36:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.37
[32m[20221213 15:36:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 247.03
[32m[20221213 15:36:41 @agent_ppo2.py:143][0m Total time:      43.76 min
[32m[20221213 15:36:41 @agent_ppo2.py:145][0m 3930112 total steps have happened
[32m[20221213 15:36:41 @agent_ppo2.py:121][0m #------------------------ Iteration 1919 --------------------------#
[32m[20221213 15:36:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:41 @agent_ppo2.py:185][0m |           0.0017 |          22.2853 |           0.1975 |
[32m[20221213 15:36:41 @agent_ppo2.py:185][0m |           0.0079 |          24.1966 |           0.1971 |
[32m[20221213 15:36:41 @agent_ppo2.py:185][0m |          -0.0033 |          22.0513 |           0.1974 |
[32m[20221213 15:36:41 @agent_ppo2.py:185][0m |          -0.0133 |          21.7605 |           0.1972 |
[32m[20221213 15:36:42 @agent_ppo2.py:185][0m |          -0.0083 |          22.0918 |           0.1969 |
[32m[20221213 15:36:42 @agent_ppo2.py:185][0m |          -0.0065 |          22.5300 |           0.1972 |
[32m[20221213 15:36:42 @agent_ppo2.py:185][0m |          -0.0145 |          21.5514 |           0.1971 |
[32m[20221213 15:36:42 @agent_ppo2.py:185][0m |          -0.0002 |          24.0618 |           0.1970 |
[32m[20221213 15:36:42 @agent_ppo2.py:185][0m |          -0.0117 |          21.7323 |           0.1972 |
[32m[20221213 15:36:42 @agent_ppo2.py:185][0m |          -0.0155 |          21.4016 |           0.1970 |
[32m[20221213 15:36:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:36:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.36
[32m[20221213 15:36:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.44
[32m[20221213 15:36:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.94
[32m[20221213 15:36:42 @agent_ppo2.py:143][0m Total time:      43.78 min
[32m[20221213 15:36:42 @agent_ppo2.py:145][0m 3932160 total steps have happened
[32m[20221213 15:36:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1920 --------------------------#
[32m[20221213 15:36:42 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:36:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |           0.0024 |          21.5662 |           0.2039 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0056 |          20.9107 |           0.2041 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0064 |          20.7818 |           0.2039 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0102 |          20.4824 |           0.2042 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0074 |          20.9403 |           0.2044 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0153 |          20.3136 |           0.2045 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0154 |          20.2515 |           0.2047 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0150 |          20.1607 |           0.2049 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0165 |          20.0972 |           0.2051 |
[32m[20221213 15:36:43 @agent_ppo2.py:185][0m |          -0.0175 |          20.0331 |           0.2052 |
[32m[20221213 15:36:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:36:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.57
[32m[20221213 15:36:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 292.23
[32m[20221213 15:36:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.94
[32m[20221213 15:36:44 @agent_ppo2.py:143][0m Total time:      43.81 min
[32m[20221213 15:36:44 @agent_ppo2.py:145][0m 3934208 total steps have happened
[32m[20221213 15:36:44 @agent_ppo2.py:121][0m #------------------------ Iteration 1921 --------------------------#
[32m[20221213 15:36:44 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:44 @agent_ppo2.py:185][0m |          -0.0020 |          20.4438 |           0.2076 |
[32m[20221213 15:36:44 @agent_ppo2.py:185][0m |          -0.0097 |          19.9880 |           0.2077 |
[32m[20221213 15:36:44 @agent_ppo2.py:185][0m |          -0.0094 |          19.8327 |           0.2073 |
[32m[20221213 15:36:44 @agent_ppo2.py:185][0m |          -0.0097 |          19.7832 |           0.2074 |
[32m[20221213 15:36:44 @agent_ppo2.py:185][0m |          -0.0141 |          19.6498 |           0.2074 |
[32m[20221213 15:36:44 @agent_ppo2.py:185][0m |          -0.0074 |          19.6526 |           0.2075 |
[32m[20221213 15:36:44 @agent_ppo2.py:185][0m |          -0.0062 |          20.7650 |           0.2076 |
[32m[20221213 15:36:45 @agent_ppo2.py:185][0m |          -0.0123 |          19.4323 |           0.2074 |
[32m[20221213 15:36:45 @agent_ppo2.py:185][0m |          -0.0111 |          19.7475 |           0.2077 |
[32m[20221213 15:36:45 @agent_ppo2.py:185][0m |          -0.0179 |          19.3827 |           0.2078 |
[32m[20221213 15:36:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:36:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.12
[32m[20221213 15:36:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 298.74
[32m[20221213 15:36:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.95
[32m[20221213 15:36:45 @agent_ppo2.py:143][0m Total time:      43.83 min
[32m[20221213 15:36:45 @agent_ppo2.py:145][0m 3936256 total steps have happened
[32m[20221213 15:36:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1922 --------------------------#
[32m[20221213 15:36:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:45 @agent_ppo2.py:185][0m |          -0.0045 |          22.6074 |           0.2001 |
[32m[20221213 15:36:45 @agent_ppo2.py:185][0m |          -0.0109 |          21.9934 |           0.1996 |
[32m[20221213 15:36:45 @agent_ppo2.py:185][0m |           0.0034 |          25.0758 |           0.1999 |
[32m[20221213 15:36:46 @agent_ppo2.py:185][0m |          -0.0137 |          21.5242 |           0.1995 |
[32m[20221213 15:36:46 @agent_ppo2.py:185][0m |          -0.0078 |          23.2477 |           0.1998 |
[32m[20221213 15:36:46 @agent_ppo2.py:185][0m |          -0.0050 |          22.4530 |           0.1999 |
[32m[20221213 15:36:46 @agent_ppo2.py:185][0m |          -0.0070 |          22.1824 |           0.2001 |
[32m[20221213 15:36:46 @agent_ppo2.py:185][0m |          -0.0183 |          20.9831 |           0.2000 |
[32m[20221213 15:36:46 @agent_ppo2.py:185][0m |          -0.0204 |          20.9165 |           0.2000 |
[32m[20221213 15:36:46 @agent_ppo2.py:185][0m |          -0.0205 |          20.7724 |           0.2000 |
[32m[20221213 15:36:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:36:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.65
[32m[20221213 15:36:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 271.50
[32m[20221213 15:36:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.42
[32m[20221213 15:36:46 @agent_ppo2.py:143][0m Total time:      43.85 min
[32m[20221213 15:36:46 @agent_ppo2.py:145][0m 3938304 total steps have happened
[32m[20221213 15:36:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1923 --------------------------#
[32m[20221213 15:36:47 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0049 |          22.6176 |           0.2080 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0097 |          21.9689 |           0.2076 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0119 |          21.7012 |           0.2075 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0136 |          21.5536 |           0.2073 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0158 |          21.4206 |           0.2074 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0159 |          21.3405 |           0.2072 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0171 |          21.2329 |           0.2070 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0169 |          21.1559 |           0.2071 |
[32m[20221213 15:36:47 @agent_ppo2.py:185][0m |          -0.0187 |          21.1240 |           0.2069 |
[32m[20221213 15:36:48 @agent_ppo2.py:185][0m |          -0.0108 |          22.2058 |           0.2070 |
[32m[20221213 15:36:48 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:36:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.46
[32m[20221213 15:36:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.06
[32m[20221213 15:36:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.76
[32m[20221213 15:36:48 @agent_ppo2.py:143][0m Total time:      43.88 min
[32m[20221213 15:36:48 @agent_ppo2.py:145][0m 3940352 total steps have happened
[32m[20221213 15:36:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1924 --------------------------#
[32m[20221213 15:36:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:48 @agent_ppo2.py:185][0m |           0.0009 |          22.1660 |           0.2054 |
[32m[20221213 15:36:48 @agent_ppo2.py:185][0m |          -0.0001 |          22.2240 |           0.2052 |
[32m[20221213 15:36:48 @agent_ppo2.py:185][0m |          -0.0111 |          21.4721 |           0.2050 |
[32m[20221213 15:36:48 @agent_ppo2.py:185][0m |          -0.0100 |          21.3672 |           0.2051 |
[32m[20221213 15:36:49 @agent_ppo2.py:185][0m |          -0.0121 |          21.1852 |           0.2048 |
[32m[20221213 15:36:49 @agent_ppo2.py:185][0m |          -0.0125 |          21.0485 |           0.2047 |
[32m[20221213 15:36:49 @agent_ppo2.py:185][0m |          -0.0156 |          21.0147 |           0.2046 |
[32m[20221213 15:36:49 @agent_ppo2.py:185][0m |          -0.0119 |          21.0894 |           0.2049 |
[32m[20221213 15:36:49 @agent_ppo2.py:185][0m |          -0.0153 |          20.8293 |           0.2046 |
[32m[20221213 15:36:49 @agent_ppo2.py:185][0m |          -0.0143 |          20.9268 |           0.2047 |
[32m[20221213 15:36:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 15:36:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.98
[32m[20221213 15:36:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.55
[32m[20221213 15:36:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.70
[32m[20221213 15:36:49 @agent_ppo2.py:143][0m Total time:      43.90 min
[32m[20221213 15:36:49 @agent_ppo2.py:145][0m 3942400 total steps have happened
[32m[20221213 15:36:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1925 --------------------------#
[32m[20221213 15:36:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0041 |          23.0048 |           0.2042 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0045 |          22.9453 |           0.2034 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0105 |          22.4863 |           0.2030 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0077 |          23.1026 |           0.2029 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0144 |          22.2882 |           0.2028 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0092 |          22.9900 |           0.2028 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0156 |          22.2130 |           0.2025 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0158 |          22.1144 |           0.2025 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0166 |          22.1197 |           0.2025 |
[32m[20221213 15:36:50 @agent_ppo2.py:185][0m |          -0.0168 |          22.0559 |           0.2024 |
[32m[20221213 15:36:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:36:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.01
[32m[20221213 15:36:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.00
[32m[20221213 15:36:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 232.31
[32m[20221213 15:36:50 @agent_ppo2.py:143][0m Total time:      43.92 min
[32m[20221213 15:36:50 @agent_ppo2.py:145][0m 3944448 total steps have happened
[32m[20221213 15:36:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1926 --------------------------#
[32m[20221213 15:36:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |           0.0041 |          23.1713 |           0.2010 |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |          -0.0101 |          22.3805 |           0.2005 |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |          -0.0104 |          22.1466 |           0.2001 |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |          -0.0115 |          21.9591 |           0.2000 |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |          -0.0135 |          21.7986 |           0.2001 |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |          -0.0173 |          21.6212 |           0.2000 |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |          -0.0148 |          21.4893 |           0.2001 |
[32m[20221213 15:36:51 @agent_ppo2.py:185][0m |          -0.0157 |          21.4971 |           0.2001 |
[32m[20221213 15:36:52 @agent_ppo2.py:185][0m |          -0.0077 |          22.9069 |           0.2002 |
[32m[20221213 15:36:52 @agent_ppo2.py:185][0m |          -0.0197 |          21.3384 |           0.1999 |
[32m[20221213 15:36:52 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:36:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.11
[32m[20221213 15:36:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.72
[32m[20221213 15:36:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 221.07
[32m[20221213 15:36:52 @agent_ppo2.py:143][0m Total time:      43.95 min
[32m[20221213 15:36:52 @agent_ppo2.py:145][0m 3946496 total steps have happened
[32m[20221213 15:36:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1927 --------------------------#
[32m[20221213 15:36:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:52 @agent_ppo2.py:185][0m |          -0.0024 |          23.0335 |           0.2049 |
[32m[20221213 15:36:52 @agent_ppo2.py:185][0m |          -0.0032 |          23.1360 |           0.2046 |
[32m[20221213 15:36:52 @agent_ppo2.py:185][0m |          -0.0105 |          22.3768 |           0.2040 |
[32m[20221213 15:36:53 @agent_ppo2.py:185][0m |          -0.0134 |          22.2910 |           0.2040 |
[32m[20221213 15:36:53 @agent_ppo2.py:185][0m |          -0.0116 |          22.0613 |           0.2036 |
[32m[20221213 15:36:53 @agent_ppo2.py:185][0m |          -0.0030 |          22.9805 |           0.2036 |
[32m[20221213 15:36:53 @agent_ppo2.py:185][0m |          -0.0116 |          22.0011 |           0.2036 |
[32m[20221213 15:36:53 @agent_ppo2.py:185][0m |          -0.0178 |          21.9182 |           0.2035 |
[32m[20221213 15:36:53 @agent_ppo2.py:185][0m |          -0.0169 |          21.7820 |           0.2032 |
[32m[20221213 15:36:53 @agent_ppo2.py:185][0m |          -0.0177 |          21.7432 |           0.2034 |
[32m[20221213 15:36:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:36:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.31
[32m[20221213 15:36:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.41
[32m[20221213 15:36:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 210.04
[32m[20221213 15:36:53 @agent_ppo2.py:143][0m Total time:      43.97 min
[32m[20221213 15:36:53 @agent_ppo2.py:145][0m 3948544 total steps have happened
[32m[20221213 15:36:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1928 --------------------------#
[32m[20221213 15:36:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |           0.0002 |          22.3381 |           0.2047 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |           0.0025 |          23.1378 |           0.2042 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0003 |          24.4286 |           0.2042 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0132 |          21.7770 |           0.2041 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0130 |          21.6282 |           0.2038 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0138 |          21.5494 |           0.2040 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0112 |          21.5328 |           0.2035 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0131 |          21.5942 |           0.2038 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0143 |          21.3528 |           0.2038 |
[32m[20221213 15:36:54 @agent_ppo2.py:185][0m |          -0.0137 |          21.4408 |           0.2036 |
[32m[20221213 15:36:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.32
[32m[20221213 15:36:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.89
[32m[20221213 15:36:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 244.84
[32m[20221213 15:36:55 @agent_ppo2.py:143][0m Total time:      43.99 min
[32m[20221213 15:36:55 @agent_ppo2.py:145][0m 3950592 total steps have happened
[32m[20221213 15:36:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1929 --------------------------#
[32m[20221213 15:36:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:55 @agent_ppo2.py:185][0m |          -0.0028 |          22.5971 |           0.2029 |
[32m[20221213 15:36:55 @agent_ppo2.py:185][0m |          -0.0058 |          21.7286 |           0.2027 |
[32m[20221213 15:36:55 @agent_ppo2.py:185][0m |          -0.0033 |          22.0037 |           0.2024 |
[32m[20221213 15:36:55 @agent_ppo2.py:185][0m |          -0.0137 |          21.3090 |           0.2022 |
[32m[20221213 15:36:55 @agent_ppo2.py:185][0m |          -0.0117 |          21.0417 |           0.2019 |
[32m[20221213 15:36:56 @agent_ppo2.py:185][0m |          -0.0030 |          23.0287 |           0.2019 |
[32m[20221213 15:36:56 @agent_ppo2.py:185][0m |          -0.0176 |          20.7923 |           0.2019 |
[32m[20221213 15:36:56 @agent_ppo2.py:185][0m |          -0.0164 |          20.6018 |           0.2019 |
[32m[20221213 15:36:56 @agent_ppo2.py:185][0m |          -0.0184 |          20.4596 |           0.2016 |
[32m[20221213 15:36:56 @agent_ppo2.py:185][0m |          -0.0203 |          20.3519 |           0.2015 |
[32m[20221213 15:36:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 246.35
[32m[20221213 15:36:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.52
[32m[20221213 15:36:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.84
[32m[20221213 15:36:56 @agent_ppo2.py:143][0m Total time:      44.01 min
[32m[20221213 15:36:56 @agent_ppo2.py:145][0m 3952640 total steps have happened
[32m[20221213 15:36:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1930 --------------------------#
[32m[20221213 15:36:56 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:36:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:56 @agent_ppo2.py:185][0m |          -0.0014 |          21.7784 |           0.1959 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0077 |          21.0793 |           0.1953 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0020 |          21.0645 |           0.1951 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0094 |          20.4858 |           0.1946 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0069 |          20.5530 |           0.1945 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0144 |          20.1435 |           0.1942 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0115 |          20.0620 |           0.1943 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0136 |          19.9223 |           0.1941 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0159 |          19.8566 |           0.1941 |
[32m[20221213 15:36:57 @agent_ppo2.py:185][0m |          -0.0117 |          20.0206 |           0.1941 |
[32m[20221213 15:36:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.59
[32m[20221213 15:36:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.25
[32m[20221213 15:36:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 235.89
[32m[20221213 15:36:58 @agent_ppo2.py:143][0m Total time:      44.04 min
[32m[20221213 15:36:58 @agent_ppo2.py:145][0m 3954688 total steps have happened
[32m[20221213 15:36:58 @agent_ppo2.py:121][0m #------------------------ Iteration 1931 --------------------------#
[32m[20221213 15:36:58 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:36:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:58 @agent_ppo2.py:185][0m |          -0.0038 |          22.3466 |           0.1938 |
[32m[20221213 15:36:58 @agent_ppo2.py:185][0m |          -0.0102 |          21.9427 |           0.1939 |
[32m[20221213 15:36:58 @agent_ppo2.py:185][0m |          -0.0140 |          21.7167 |           0.1938 |
[32m[20221213 15:36:58 @agent_ppo2.py:185][0m |          -0.0127 |          21.5579 |           0.1938 |
[32m[20221213 15:36:58 @agent_ppo2.py:185][0m |          -0.0130 |          21.4556 |           0.1939 |
[32m[20221213 15:36:58 @agent_ppo2.py:185][0m |          -0.0147 |          21.3827 |           0.1937 |
[32m[20221213 15:36:58 @agent_ppo2.py:185][0m |          -0.0176 |          21.3553 |           0.1939 |
[32m[20221213 15:36:59 @agent_ppo2.py:185][0m |          -0.0175 |          21.2592 |           0.1940 |
[32m[20221213 15:36:59 @agent_ppo2.py:185][0m |          -0.0174 |          21.1553 |           0.1940 |
[32m[20221213 15:36:59 @agent_ppo2.py:185][0m |          -0.0135 |          21.3427 |           0.1940 |
[32m[20221213 15:36:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:36:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.19
[32m[20221213 15:36:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.60
[32m[20221213 15:36:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.74
[32m[20221213 15:36:59 @agent_ppo2.py:143][0m Total time:      44.06 min
[32m[20221213 15:36:59 @agent_ppo2.py:145][0m 3956736 total steps have happened
[32m[20221213 15:36:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1932 --------------------------#
[32m[20221213 15:36:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:36:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:36:59 @agent_ppo2.py:185][0m |          -0.0002 |          23.3829 |           0.1949 |
[32m[20221213 15:36:59 @agent_ppo2.py:185][0m |          -0.0065 |          22.8925 |           0.1947 |
[32m[20221213 15:36:59 @agent_ppo2.py:185][0m |          -0.0031 |          23.3005 |           0.1948 |
[32m[20221213 15:37:00 @agent_ppo2.py:185][0m |          -0.0115 |          22.5890 |           0.1945 |
[32m[20221213 15:37:00 @agent_ppo2.py:185][0m |          -0.0134 |          22.4542 |           0.1945 |
[32m[20221213 15:37:00 @agent_ppo2.py:185][0m |          -0.0143 |          22.4016 |           0.1947 |
[32m[20221213 15:37:00 @agent_ppo2.py:185][0m |          -0.0169 |          22.3130 |           0.1944 |
[32m[20221213 15:37:00 @agent_ppo2.py:185][0m |          -0.0131 |          22.3278 |           0.1943 |
[32m[20221213 15:37:00 @agent_ppo2.py:185][0m |          -0.0086 |          22.8219 |           0.1944 |
[32m[20221213 15:37:00 @agent_ppo2.py:185][0m |          -0.0124 |          22.5368 |           0.1942 |
[32m[20221213 15:37:00 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:37:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.13
[32m[20221213 15:37:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.65
[32m[20221213 15:37:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 227.47
[32m[20221213 15:37:00 @agent_ppo2.py:143][0m Total time:      44.09 min
[32m[20221213 15:37:00 @agent_ppo2.py:145][0m 3958784 total steps have happened
[32m[20221213 15:37:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1933 --------------------------#
[32m[20221213 15:37:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |          -0.0011 |          22.2732 |           0.2007 |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |          -0.0061 |          21.7879 |           0.2004 |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |          -0.0106 |          21.5707 |           0.2001 |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |          -0.0109 |          21.4583 |           0.2002 |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |          -0.0162 |          21.3652 |           0.2000 |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |          -0.0044 |          22.5009 |           0.2001 |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |          -0.0113 |          21.0839 |           0.1991 |
[32m[20221213 15:37:01 @agent_ppo2.py:185][0m |           0.0046 |          23.5006 |           0.1999 |
[32m[20221213 15:37:02 @agent_ppo2.py:185][0m |          -0.0167 |          21.0100 |           0.1999 |
[32m[20221213 15:37:02 @agent_ppo2.py:185][0m |          -0.0177 |          20.9075 |           0.1998 |
[32m[20221213 15:37:02 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:37:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.84
[32m[20221213 15:37:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.63
[32m[20221213 15:37:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 256.75
[32m[20221213 15:37:02 @agent_ppo2.py:143][0m Total time:      44.11 min
[32m[20221213 15:37:02 @agent_ppo2.py:145][0m 3960832 total steps have happened
[32m[20221213 15:37:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1934 --------------------------#
[32m[20221213 15:37:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:37:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:02 @agent_ppo2.py:185][0m |          -0.0019 |          22.3245 |           0.1973 |
[32m[20221213 15:37:02 @agent_ppo2.py:185][0m |          -0.0039 |          22.1385 |           0.1971 |
[32m[20221213 15:37:02 @agent_ppo2.py:185][0m |          -0.0090 |          21.8606 |           0.1968 |
[32m[20221213 15:37:02 @agent_ppo2.py:185][0m |          -0.0099 |          21.7536 |           0.1965 |
[32m[20221213 15:37:03 @agent_ppo2.py:185][0m |          -0.0111 |          21.7136 |           0.1963 |
[32m[20221213 15:37:03 @agent_ppo2.py:185][0m |          -0.0124 |          21.6930 |           0.1963 |
[32m[20221213 15:37:03 @agent_ppo2.py:185][0m |          -0.0139 |          21.5830 |           0.1964 |
[32m[20221213 15:37:03 @agent_ppo2.py:185][0m |          -0.0142 |          21.5410 |           0.1960 |
[32m[20221213 15:37:03 @agent_ppo2.py:185][0m |          -0.0138 |          21.4932 |           0.1960 |
[32m[20221213 15:37:03 @agent_ppo2.py:185][0m |          -0.0136 |          21.4310 |           0.1959 |
[32m[20221213 15:37:03 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:37:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.41
[32m[20221213 15:37:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.65
[32m[20221213 15:37:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.52
[32m[20221213 15:37:03 @agent_ppo2.py:143][0m Total time:      44.13 min
[32m[20221213 15:37:03 @agent_ppo2.py:145][0m 3962880 total steps have happened
[32m[20221213 15:37:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1935 --------------------------#
[32m[20221213 15:37:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |           0.0051 |          22.1719 |           0.1939 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0088 |          21.1221 |           0.1933 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0103 |          20.8520 |           0.1934 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0128 |          20.6386 |           0.1928 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0136 |          20.4713 |           0.1929 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0124 |          20.3534 |           0.1928 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0146 |          20.2020 |           0.1927 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0166 |          20.1457 |           0.1928 |
[32m[20221213 15:37:04 @agent_ppo2.py:185][0m |          -0.0139 |          20.0875 |           0.1925 |
[32m[20221213 15:37:05 @agent_ppo2.py:185][0m |          -0.0178 |          19.9312 |           0.1925 |
[32m[20221213 15:37:05 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:37:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.56
[32m[20221213 15:37:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.89
[32m[20221213 15:37:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.40
[32m[20221213 15:37:05 @agent_ppo2.py:143][0m Total time:      44.16 min
[32m[20221213 15:37:05 @agent_ppo2.py:145][0m 3964928 total steps have happened
[32m[20221213 15:37:05 @agent_ppo2.py:121][0m #------------------------ Iteration 1936 --------------------------#
[32m[20221213 15:37:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:05 @agent_ppo2.py:185][0m |           0.0049 |          22.3692 |           0.1947 |
[32m[20221213 15:37:05 @agent_ppo2.py:185][0m |          -0.0006 |          22.2214 |           0.1942 |
[32m[20221213 15:37:05 @agent_ppo2.py:185][0m |          -0.0019 |          22.1599 |           0.1941 |
[32m[20221213 15:37:05 @agent_ppo2.py:185][0m |          -0.0017 |          21.9384 |           0.1944 |
[32m[20221213 15:37:05 @agent_ppo2.py:185][0m |          -0.0099 |          21.4480 |           0.1942 |
[32m[20221213 15:37:06 @agent_ppo2.py:185][0m |          -0.0127 |          21.4087 |           0.1940 |
[32m[20221213 15:37:06 @agent_ppo2.py:185][0m |          -0.0124 |          21.3255 |           0.1940 |
[32m[20221213 15:37:06 @agent_ppo2.py:185][0m |          -0.0082 |          21.4450 |           0.1940 |
[32m[20221213 15:37:06 @agent_ppo2.py:185][0m |          -0.0070 |          21.7443 |           0.1940 |
[32m[20221213 15:37:06 @agent_ppo2.py:185][0m |          -0.0096 |          21.5126 |           0.1938 |
[32m[20221213 15:37:06 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:37:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.95
[32m[20221213 15:37:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.21
[32m[20221213 15:37:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.52
[32m[20221213 15:37:06 @agent_ppo2.py:143][0m Total time:      44.18 min
[32m[20221213 15:37:06 @agent_ppo2.py:145][0m 3966976 total steps have happened
[32m[20221213 15:37:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1937 --------------------------#
[32m[20221213 15:37:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0024 |          21.9466 |           0.1964 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |           0.0064 |          24.3979 |           0.1960 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0073 |          21.4730 |           0.1953 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0070 |          21.2131 |           0.1956 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0104 |          21.0321 |           0.1955 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0118 |          20.9405 |           0.1954 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0151 |          20.9104 |           0.1953 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0160 |          20.7706 |           0.1953 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0151 |          20.7001 |           0.1950 |
[32m[20221213 15:37:07 @agent_ppo2.py:185][0m |          -0.0152 |          20.6139 |           0.1950 |
[32m[20221213 15:37:07 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:37:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.54
[32m[20221213 15:37:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.27
[32m[20221213 15:37:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 242.63
[32m[20221213 15:37:08 @agent_ppo2.py:143][0m Total time:      44.21 min
[32m[20221213 15:37:08 @agent_ppo2.py:145][0m 3969024 total steps have happened
[32m[20221213 15:37:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1938 --------------------------#
[32m[20221213 15:37:08 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:08 @agent_ppo2.py:185][0m |          -0.0019 |          22.3699 |           0.1901 |
[32m[20221213 15:37:08 @agent_ppo2.py:185][0m |          -0.0075 |          21.9207 |           0.1896 |
[32m[20221213 15:37:08 @agent_ppo2.py:185][0m |          -0.0108 |          21.7210 |           0.1898 |
[32m[20221213 15:37:08 @agent_ppo2.py:185][0m |          -0.0102 |          21.6002 |           0.1894 |
[32m[20221213 15:37:08 @agent_ppo2.py:185][0m |          -0.0137 |          21.4234 |           0.1894 |
[32m[20221213 15:37:08 @agent_ppo2.py:185][0m |          -0.0139 |          21.3566 |           0.1893 |
[32m[20221213 15:37:09 @agent_ppo2.py:185][0m |          -0.0164 |          21.2665 |           0.1891 |
[32m[20221213 15:37:09 @agent_ppo2.py:185][0m |          -0.0155 |          21.2104 |           0.1892 |
[32m[20221213 15:37:09 @agent_ppo2.py:185][0m |          -0.0097 |          21.4513 |           0.1890 |
[32m[20221213 15:37:09 @agent_ppo2.py:185][0m |          -0.0162 |          21.0914 |           0.1891 |
[32m[20221213 15:37:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:37:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.45
[32m[20221213 15:37:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.65
[32m[20221213 15:37:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.75
[32m[20221213 15:37:09 @agent_ppo2.py:143][0m Total time:      44.23 min
[32m[20221213 15:37:09 @agent_ppo2.py:145][0m 3971072 total steps have happened
[32m[20221213 15:37:09 @agent_ppo2.py:121][0m #------------------------ Iteration 1939 --------------------------#
[32m[20221213 15:37:09 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:09 @agent_ppo2.py:185][0m |           0.0011 |          21.8051 |           0.1885 |
[32m[20221213 15:37:09 @agent_ppo2.py:185][0m |          -0.0048 |          21.2712 |           0.1877 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0110 |          21.0092 |           0.1878 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0125 |          20.8913 |           0.1878 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0135 |          20.7701 |           0.1877 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0149 |          20.6786 |           0.1878 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0178 |          20.5737 |           0.1877 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0170 |          20.4930 |           0.1874 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0174 |          20.4487 |           0.1874 |
[32m[20221213 15:37:10 @agent_ppo2.py:185][0m |          -0.0180 |          20.3377 |           0.1876 |
[32m[20221213 15:37:10 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:37:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.12
[32m[20221213 15:37:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.85
[32m[20221213 15:37:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.94
[32m[20221213 15:37:10 @agent_ppo2.py:143][0m Total time:      44.25 min
[32m[20221213 15:37:10 @agent_ppo2.py:145][0m 3973120 total steps have happened
[32m[20221213 15:37:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1940 --------------------------#
[32m[20221213 15:37:11 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:37:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0003 |          21.8582 |           0.1914 |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0094 |          21.4294 |           0.1908 |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0092 |          21.2278 |           0.1905 |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0120 |          21.0824 |           0.1903 |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0116 |          20.9043 |           0.1904 |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0131 |          20.8158 |           0.1903 |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0125 |          20.7627 |           0.1903 |
[32m[20221213 15:37:11 @agent_ppo2.py:185][0m |          -0.0120 |          20.6119 |           0.1901 |
[32m[20221213 15:37:12 @agent_ppo2.py:185][0m |          -0.0005 |          22.3932 |           0.1900 |
[32m[20221213 15:37:12 @agent_ppo2.py:185][0m |          -0.0150 |          20.6786 |           0.1897 |
[32m[20221213 15:37:12 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221213 15:37:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 288.96
[32m[20221213 15:37:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.97
[32m[20221213 15:37:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.46
[32m[20221213 15:37:12 @agent_ppo2.py:143][0m Total time:      44.28 min
[32m[20221213 15:37:12 @agent_ppo2.py:145][0m 3975168 total steps have happened
[32m[20221213 15:37:12 @agent_ppo2.py:121][0m #------------------------ Iteration 1941 --------------------------#
[32m[20221213 15:37:12 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:12 @agent_ppo2.py:185][0m |           0.0015 |          22.5946 |           0.1891 |
[32m[20221213 15:37:12 @agent_ppo2.py:185][0m |          -0.0078 |          22.0837 |           0.1885 |
[32m[20221213 15:37:12 @agent_ppo2.py:185][0m |          -0.0050 |          21.8620 |           0.1885 |
[32m[20221213 15:37:13 @agent_ppo2.py:185][0m |           0.0004 |          24.3999 |           0.1885 |
[32m[20221213 15:37:13 @agent_ppo2.py:185][0m |          -0.0114 |          21.4185 |           0.1881 |
[32m[20221213 15:37:13 @agent_ppo2.py:185][0m |          -0.0084 |          21.3450 |           0.1882 |
[32m[20221213 15:37:13 @agent_ppo2.py:185][0m |          -0.0152 |          20.9578 |           0.1880 |
[32m[20221213 15:37:13 @agent_ppo2.py:185][0m |          -0.0079 |          21.1534 |           0.1879 |
[32m[20221213 15:37:13 @agent_ppo2.py:185][0m |          -0.0164 |          20.6305 |           0.1879 |
[32m[20221213 15:37:13 @agent_ppo2.py:185][0m |          -0.0145 |          20.6108 |           0.1878 |
[32m[20221213 15:37:13 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:37:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.50
[32m[20221213 15:37:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.58
[32m[20221213 15:37:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.66
[32m[20221213 15:37:13 @agent_ppo2.py:143][0m Total time:      44.30 min
[32m[20221213 15:37:13 @agent_ppo2.py:145][0m 3977216 total steps have happened
[32m[20221213 15:37:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1942 --------------------------#
[32m[20221213 15:37:14 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:37:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0005 |          23.0085 |           0.1895 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0077 |          22.6238 |           0.1890 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0103 |          22.4592 |           0.1888 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0140 |          22.4472 |           0.1886 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0118 |          22.4736 |           0.1885 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0054 |          24.9406 |           0.1885 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0127 |          22.4266 |           0.1879 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0170 |          22.2918 |           0.1885 |
[32m[20221213 15:37:14 @agent_ppo2.py:185][0m |          -0.0181 |          22.3223 |           0.1883 |
[32m[20221213 15:37:15 @agent_ppo2.py:185][0m |          -0.0171 |          22.2185 |           0.1882 |
[32m[20221213 15:37:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:37:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.67
[32m[20221213 15:37:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.96
[32m[20221213 15:37:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.91
[32m[20221213 15:37:15 @agent_ppo2.py:143][0m Total time:      44.33 min
[32m[20221213 15:37:15 @agent_ppo2.py:145][0m 3979264 total steps have happened
[32m[20221213 15:37:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1943 --------------------------#
[32m[20221213 15:37:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:37:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:15 @agent_ppo2.py:185][0m |           0.0001 |          22.1038 |           0.1929 |
[32m[20221213 15:37:15 @agent_ppo2.py:185][0m |          -0.0070 |          21.6657 |           0.1923 |
[32m[20221213 15:37:15 @agent_ppo2.py:185][0m |          -0.0086 |          21.4713 |           0.1925 |
[32m[20221213 15:37:15 @agent_ppo2.py:185][0m |          -0.0060 |          21.5173 |           0.1923 |
[32m[20221213 15:37:15 @agent_ppo2.py:185][0m |          -0.0102 |          21.2660 |           0.1921 |
[32m[20221213 15:37:16 @agent_ppo2.py:185][0m |          -0.0118 |          21.2123 |           0.1922 |
[32m[20221213 15:37:16 @agent_ppo2.py:185][0m |          -0.0125 |          21.0900 |           0.1922 |
[32m[20221213 15:37:16 @agent_ppo2.py:185][0m |          -0.0126 |          21.0545 |           0.1921 |
[32m[20221213 15:37:16 @agent_ppo2.py:185][0m |          -0.0074 |          22.3344 |           0.1921 |
[32m[20221213 15:37:16 @agent_ppo2.py:185][0m |          -0.0123 |          20.9421 |           0.1921 |
[32m[20221213 15:37:16 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:37:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.24
[32m[20221213 15:37:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.93
[32m[20221213 15:37:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.69
[32m[20221213 15:37:16 @agent_ppo2.py:143][0m Total time:      44.35 min
[32m[20221213 15:37:16 @agent_ppo2.py:145][0m 3981312 total steps have happened
[32m[20221213 15:37:16 @agent_ppo2.py:121][0m #------------------------ Iteration 1944 --------------------------#
[32m[20221213 15:37:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0013 |          22.5306 |           0.1827 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0047 |          22.1880 |           0.1824 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0089 |          22.0393 |           0.1824 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0122 |          21.9331 |           0.1824 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0026 |          23.8684 |           0.1823 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0140 |          21.7467 |           0.1822 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0073 |          22.7590 |           0.1825 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0154 |          21.6043 |           0.1822 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0177 |          21.5344 |           0.1822 |
[32m[20221213 15:37:17 @agent_ppo2.py:185][0m |          -0.0156 |          21.4745 |           0.1823 |
[32m[20221213 15:37:17 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:37:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.76
[32m[20221213 15:37:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.82
[32m[20221213 15:37:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 192.67
[32m[20221213 15:37:18 @agent_ppo2.py:143][0m Total time:      44.37 min
[32m[20221213 15:37:18 @agent_ppo2.py:145][0m 3983360 total steps have happened
[32m[20221213 15:37:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1945 --------------------------#
[32m[20221213 15:37:18 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:18 @agent_ppo2.py:185][0m |          -0.0027 |          22.8687 |           0.1917 |
[32m[20221213 15:37:18 @agent_ppo2.py:185][0m |          -0.0093 |          22.2161 |           0.1913 |
[32m[20221213 15:37:18 @agent_ppo2.py:185][0m |          -0.0018 |          22.5488 |           0.1911 |
[32m[20221213 15:37:18 @agent_ppo2.py:185][0m |          -0.0140 |          21.9149 |           0.1912 |
[32m[20221213 15:37:18 @agent_ppo2.py:185][0m |          -0.0145 |          21.7458 |           0.1910 |
[32m[20221213 15:37:19 @agent_ppo2.py:185][0m |          -0.0179 |          21.6540 |           0.1910 |
[32m[20221213 15:37:19 @agent_ppo2.py:185][0m |          -0.0169 |          21.5830 |           0.1909 |
[32m[20221213 15:37:19 @agent_ppo2.py:185][0m |          -0.0176 |          21.4925 |           0.1910 |
[32m[20221213 15:37:19 @agent_ppo2.py:185][0m |          -0.0079 |          22.6289 |           0.1907 |
[32m[20221213 15:37:19 @agent_ppo2.py:185][0m |          -0.0100 |          23.2792 |           0.1907 |
[32m[20221213 15:37:19 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:37:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.47
[32m[20221213 15:37:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 264.41
[32m[20221213 15:37:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.34
[32m[20221213 15:37:19 @agent_ppo2.py:143][0m Total time:      44.40 min
[32m[20221213 15:37:19 @agent_ppo2.py:145][0m 3985408 total steps have happened
[32m[20221213 15:37:19 @agent_ppo2.py:121][0m #------------------------ Iteration 1946 --------------------------#
[32m[20221213 15:37:19 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:19 @agent_ppo2.py:185][0m |          -0.0017 |          22.3526 |           0.1879 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0077 |          21.6113 |           0.1873 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0013 |          22.5099 |           0.1876 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0049 |          21.3901 |           0.1870 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0119 |          20.3752 |           0.1868 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0133 |          20.0673 |           0.1868 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0142 |          19.8180 |           0.1865 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0121 |          19.5846 |           0.1865 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0162 |          19.5310 |           0.1863 |
[32m[20221213 15:37:20 @agent_ppo2.py:185][0m |          -0.0153 |          19.3086 |           0.1863 |
[32m[20221213 15:37:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:37:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 292.68
[32m[20221213 15:37:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.38
[32m[20221213 15:37:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 255.39
[32m[20221213 15:37:20 @agent_ppo2.py:143][0m Total time:      44.42 min
[32m[20221213 15:37:20 @agent_ppo2.py:145][0m 3987456 total steps have happened
[32m[20221213 15:37:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1947 --------------------------#
[32m[20221213 15:37:21 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |           0.0036 |          23.9040 |           0.1837 |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |          -0.0059 |          22.9442 |           0.1834 |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |          -0.0113 |          22.5484 |           0.1831 |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |          -0.0044 |          22.4194 |           0.1832 |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |          -0.0140 |          22.1685 |           0.1830 |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |          -0.0154 |          21.9983 |           0.1830 |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |          -0.0164 |          21.9059 |           0.1827 |
[32m[20221213 15:37:21 @agent_ppo2.py:185][0m |          -0.0163 |          21.7381 |           0.1827 |
[32m[20221213 15:37:22 @agent_ppo2.py:185][0m |          -0.0171 |          21.6311 |           0.1827 |
[32m[20221213 15:37:22 @agent_ppo2.py:185][0m |          -0.0071 |          24.1689 |           0.1826 |
[32m[20221213 15:37:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:37:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 290.04
[32m[20221213 15:37:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.27
[32m[20221213 15:37:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.12
[32m[20221213 15:37:22 @agent_ppo2.py:143][0m Total time:      44.44 min
[32m[20221213 15:37:22 @agent_ppo2.py:145][0m 3989504 total steps have happened
[32m[20221213 15:37:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1948 --------------------------#
[32m[20221213 15:37:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:37:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:22 @agent_ppo2.py:185][0m |          -0.0026 |          23.8623 |           0.1803 |
[32m[20221213 15:37:22 @agent_ppo2.py:185][0m |          -0.0018 |          23.4039 |           0.1802 |
[32m[20221213 15:37:22 @agent_ppo2.py:185][0m |          -0.0089 |          23.0466 |           0.1801 |
[32m[20221213 15:37:22 @agent_ppo2.py:185][0m |          -0.0089 |          22.9391 |           0.1798 |
[32m[20221213 15:37:23 @agent_ppo2.py:185][0m |          -0.0129 |          22.7115 |           0.1795 |
[32m[20221213 15:37:23 @agent_ppo2.py:185][0m |          -0.0118 |          22.6459 |           0.1793 |
[32m[20221213 15:37:23 @agent_ppo2.py:185][0m |          -0.0157 |          22.5613 |           0.1794 |
[32m[20221213 15:37:23 @agent_ppo2.py:185][0m |          -0.0150 |          22.4402 |           0.1792 |
[32m[20221213 15:37:23 @agent_ppo2.py:185][0m |          -0.0166 |          22.3574 |           0.1787 |
[32m[20221213 15:37:23 @agent_ppo2.py:185][0m |          -0.0174 |          22.2490 |           0.1788 |
[32m[20221213 15:37:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:37:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 273.37
[32m[20221213 15:37:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.38
[32m[20221213 15:37:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.90
[32m[20221213 15:37:23 @agent_ppo2.py:143][0m Total time:      44.47 min
[32m[20221213 15:37:23 @agent_ppo2.py:145][0m 3991552 total steps have happened
[32m[20221213 15:37:23 @agent_ppo2.py:121][0m #------------------------ Iteration 1949 --------------------------#
[32m[20221213 15:37:23 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |           0.0001 |          22.7386 |           0.1835 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0041 |          22.5666 |           0.1828 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0065 |          22.6295 |           0.1824 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0136 |          22.0918 |           0.1825 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0161 |          22.0304 |           0.1822 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0167 |          21.9122 |           0.1822 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0156 |          21.8137 |           0.1821 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0155 |          21.7235 |           0.1820 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0033 |          23.8390 |           0.1820 |
[32m[20221213 15:37:24 @agent_ppo2.py:185][0m |          -0.0156 |          21.6593 |           0.1820 |
[32m[20221213 15:37:24 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:37:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.15
[32m[20221213 15:37:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.12
[32m[20221213 15:37:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 178.23
[32m[20221213 15:37:25 @agent_ppo2.py:143][0m Total time:      44.49 min
[32m[20221213 15:37:25 @agent_ppo2.py:145][0m 3993600 total steps have happened
[32m[20221213 15:37:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1950 --------------------------#
[32m[20221213 15:37:25 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:37:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:25 @agent_ppo2.py:185][0m |          -0.0012 |          24.3892 |           0.1805 |
[32m[20221213 15:37:25 @agent_ppo2.py:185][0m |           0.0007 |          25.7873 |           0.1804 |
[32m[20221213 15:37:25 @agent_ppo2.py:185][0m |           0.0023 |          25.1774 |           0.1802 |
[32m[20221213 15:37:25 @agent_ppo2.py:185][0m |          -0.0100 |          23.4852 |           0.1803 |
[32m[20221213 15:37:25 @agent_ppo2.py:185][0m |          -0.0136 |          23.3090 |           0.1800 |
[32m[20221213 15:37:25 @agent_ppo2.py:185][0m |          -0.0123 |          23.2680 |           0.1799 |
[32m[20221213 15:37:26 @agent_ppo2.py:185][0m |          -0.0160 |          23.1777 |           0.1799 |
[32m[20221213 15:37:26 @agent_ppo2.py:185][0m |          -0.0138 |          23.1286 |           0.1800 |
[32m[20221213 15:37:26 @agent_ppo2.py:185][0m |          -0.0164 |          23.0746 |           0.1799 |
[32m[20221213 15:37:26 @agent_ppo2.py:185][0m |          -0.0185 |          23.0178 |           0.1798 |
[32m[20221213 15:37:26 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:37:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 230.20
[32m[20221213 15:37:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 267.58
[32m[20221213 15:37:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.26
[32m[20221213 15:37:26 @agent_ppo2.py:143][0m Total time:      44.51 min
[32m[20221213 15:37:26 @agent_ppo2.py:145][0m 3995648 total steps have happened
[32m[20221213 15:37:26 @agent_ppo2.py:121][0m #------------------------ Iteration 1951 --------------------------#
[32m[20221213 15:37:26 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:26 @agent_ppo2.py:185][0m |          -0.0014 |          22.1417 |           0.1824 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |           0.0022 |          23.1316 |           0.1821 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0079 |          22.1229 |           0.1818 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0133 |          21.7673 |           0.1817 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0065 |          22.4197 |           0.1815 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0137 |          21.6160 |           0.1814 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0126 |          21.5996 |           0.1813 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0161 |          21.5634 |           0.1811 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0175 |          21.4491 |           0.1810 |
[32m[20221213 15:37:27 @agent_ppo2.py:185][0m |          -0.0187 |          21.4247 |           0.1809 |
[32m[20221213 15:37:27 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:37:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 275.65
[32m[20221213 15:37:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 295.22
[32m[20221213 15:37:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.23
[32m[20221213 15:37:28 @agent_ppo2.py:143][0m Total time:      44.54 min
[32m[20221213 15:37:28 @agent_ppo2.py:145][0m 3997696 total steps have happened
[32m[20221213 15:37:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1952 --------------------------#
[32m[20221213 15:37:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:28 @agent_ppo2.py:185][0m |          -0.0005 |          22.1620 |           0.1754 |
[32m[20221213 15:37:28 @agent_ppo2.py:185][0m |          -0.0066 |          21.5472 |           0.1749 |
[32m[20221213 15:37:28 @agent_ppo2.py:185][0m |          -0.0023 |          21.8782 |           0.1749 |
[32m[20221213 15:37:28 @agent_ppo2.py:185][0m |          -0.0108 |          21.2224 |           0.1747 |
[32m[20221213 15:37:28 @agent_ppo2.py:185][0m |          -0.0022 |          23.3989 |           0.1749 |
[32m[20221213 15:37:28 @agent_ppo2.py:185][0m |          -0.0106 |          21.0333 |           0.1744 |
[32m[20221213 15:37:28 @agent_ppo2.py:185][0m |          -0.0097 |          21.0702 |           0.1747 |
[32m[20221213 15:37:29 @agent_ppo2.py:185][0m |          -0.0117 |          20.9349 |           0.1748 |
[32m[20221213 15:37:29 @agent_ppo2.py:185][0m |          -0.0154 |          20.7413 |           0.1749 |
[32m[20221213 15:37:29 @agent_ppo2.py:185][0m |          -0.0170 |          20.7379 |           0.1750 |
[32m[20221213 15:37:29 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:37:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.49
[32m[20221213 15:37:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.74
[32m[20221213 15:37:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.66
[32m[20221213 15:37:29 @agent_ppo2.py:143][0m Total time:      44.56 min
[32m[20221213 15:37:29 @agent_ppo2.py:145][0m 3999744 total steps have happened
[32m[20221213 15:37:29 @agent_ppo2.py:121][0m #------------------------ Iteration 1953 --------------------------#
[32m[20221213 15:37:29 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:29 @agent_ppo2.py:185][0m |          -0.0024 |          22.0253 |           0.1737 |
[32m[20221213 15:37:29 @agent_ppo2.py:185][0m |          -0.0042 |          21.5118 |           0.1736 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |           0.0031 |          23.6355 |           0.1732 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |          -0.0060 |          21.1606 |           0.1731 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |          -0.0127 |          21.0929 |           0.1734 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |          -0.0122 |          20.9810 |           0.1735 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |          -0.0137 |          20.9394 |           0.1733 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |          -0.0144 |          20.8540 |           0.1731 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |          -0.0150 |          20.7915 |           0.1730 |
[32m[20221213 15:37:30 @agent_ppo2.py:185][0m |          -0.0136 |          20.7722 |           0.1728 |
[32m[20221213 15:37:30 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:37:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 269.49
[32m[20221213 15:37:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 280.38
[32m[20221213 15:37:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 274.12
[32m[20221213 15:37:30 @agent_ppo2.py:143][0m Total time:      44.59 min
[32m[20221213 15:37:30 @agent_ppo2.py:145][0m 4001792 total steps have happened
[32m[20221213 15:37:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1954 --------------------------#
[32m[20221213 15:37:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0009 |          22.7676 |           0.1797 |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0089 |          22.3451 |           0.1797 |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0129 |          22.0868 |           0.1798 |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0142 |          21.8933 |           0.1797 |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0168 |          21.7962 |           0.1795 |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0151 |          21.6670 |           0.1795 |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0176 |          21.5018 |           0.1793 |
[32m[20221213 15:37:31 @agent_ppo2.py:185][0m |          -0.0157 |          21.3645 |           0.1793 |
[32m[20221213 15:37:32 @agent_ppo2.py:185][0m |          -0.0165 |          21.3145 |           0.1793 |
[32m[20221213 15:37:32 @agent_ppo2.py:185][0m |          -0.0154 |          21.5633 |           0.1792 |
[32m[20221213 15:37:32 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:37:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.73
[32m[20221213 15:37:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.74
[32m[20221213 15:37:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.30
[32m[20221213 15:37:32 @agent_ppo2.py:143][0m Total time:      44.61 min
[32m[20221213 15:37:32 @agent_ppo2.py:145][0m 4003840 total steps have happened
[32m[20221213 15:37:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1955 --------------------------#
[32m[20221213 15:37:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:32 @agent_ppo2.py:185][0m |           0.0104 |          25.0508 |           0.1788 |
[32m[20221213 15:37:32 @agent_ppo2.py:185][0m |          -0.0078 |          22.3440 |           0.1785 |
[32m[20221213 15:37:32 @agent_ppo2.py:185][0m |          -0.0118 |          22.1569 |           0.1785 |
[32m[20221213 15:37:32 @agent_ppo2.py:185][0m |          -0.0104 |          22.0156 |           0.1784 |
[32m[20221213 15:37:33 @agent_ppo2.py:185][0m |          -0.0019 |          23.7302 |           0.1782 |
[32m[20221213 15:37:33 @agent_ppo2.py:185][0m |          -0.0146 |          21.7729 |           0.1779 |
[32m[20221213 15:37:33 @agent_ppo2.py:185][0m |          -0.0140 |          21.6648 |           0.1781 |
[32m[20221213 15:37:33 @agent_ppo2.py:185][0m |          -0.0149 |          21.6091 |           0.1779 |
[32m[20221213 15:37:33 @agent_ppo2.py:185][0m |          -0.0162 |          21.5355 |           0.1779 |
[32m[20221213 15:37:33 @agent_ppo2.py:185][0m |          -0.0082 |          22.7026 |           0.1778 |
[32m[20221213 15:37:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:37:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.90
[32m[20221213 15:37:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 297.64
[32m[20221213 15:37:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.35
[32m[20221213 15:37:33 @agent_ppo2.py:143][0m Total time:      44.63 min
[32m[20221213 15:37:33 @agent_ppo2.py:145][0m 4005888 total steps have happened
[32m[20221213 15:37:33 @agent_ppo2.py:121][0m #------------------------ Iteration 1956 --------------------------#
[32m[20221213 15:37:33 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0033 |          23.5145 |           0.1767 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0091 |          23.0897 |           0.1764 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0089 |          22.9979 |           0.1763 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0075 |          23.2903 |           0.1765 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0151 |          22.5598 |           0.1766 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0157 |          22.3892 |           0.1765 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0168 |          22.3302 |           0.1767 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0201 |          22.2322 |           0.1767 |
[32m[20221213 15:37:34 @agent_ppo2.py:185][0m |          -0.0073 |          25.4998 |           0.1768 |
[32m[20221213 15:37:35 @agent_ppo2.py:185][0m |          -0.0198 |          22.1424 |           0.1766 |
[32m[20221213 15:37:35 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:37:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.53
[32m[20221213 15:37:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 281.17
[32m[20221213 15:37:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.66
[32m[20221213 15:37:35 @agent_ppo2.py:143][0m Total time:      44.66 min
[32m[20221213 15:37:35 @agent_ppo2.py:145][0m 4007936 total steps have happened
[32m[20221213 15:37:35 @agent_ppo2.py:121][0m #------------------------ Iteration 1957 --------------------------#
[32m[20221213 15:37:35 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:35 @agent_ppo2.py:185][0m |           0.0043 |          23.9672 |           0.1803 |
[32m[20221213 15:37:35 @agent_ppo2.py:185][0m |          -0.0081 |          22.8926 |           0.1802 |
[32m[20221213 15:37:35 @agent_ppo2.py:185][0m |          -0.0110 |          22.6525 |           0.1802 |
[32m[20221213 15:37:35 @agent_ppo2.py:185][0m |          -0.0118 |          22.4964 |           0.1800 |
[32m[20221213 15:37:36 @agent_ppo2.py:185][0m |          -0.0142 |          22.3077 |           0.1801 |
[32m[20221213 15:37:36 @agent_ppo2.py:185][0m |          -0.0181 |          22.1323 |           0.1801 |
[32m[20221213 15:37:36 @agent_ppo2.py:185][0m |          -0.0171 |          22.0384 |           0.1801 |
[32m[20221213 15:37:36 @agent_ppo2.py:185][0m |          -0.0167 |          21.9930 |           0.1802 |
[32m[20221213 15:37:36 @agent_ppo2.py:185][0m |          -0.0184 |          21.9241 |           0.1801 |
[32m[20221213 15:37:36 @agent_ppo2.py:185][0m |          -0.0190 |          21.8435 |           0.1803 |
[32m[20221213 15:37:36 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:37:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.93
[32m[20221213 15:37:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 293.30
[32m[20221213 15:37:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 221.25
[32m[20221213 15:37:36 @agent_ppo2.py:143][0m Total time:      44.68 min
[32m[20221213 15:37:36 @agent_ppo2.py:145][0m 4009984 total steps have happened
[32m[20221213 15:37:36 @agent_ppo2.py:121][0m #------------------------ Iteration 1958 --------------------------#
[32m[20221213 15:37:36 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0005 |          22.5518 |           0.1790 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0049 |          22.6011 |           0.1782 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0106 |          21.9792 |           0.1780 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0134 |          21.7967 |           0.1777 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0171 |          21.6925 |           0.1773 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0129 |          21.9462 |           0.1772 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0132 |          21.5684 |           0.1773 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0169 |          21.3571 |           0.1770 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0185 |          21.2860 |           0.1767 |
[32m[20221213 15:37:37 @agent_ppo2.py:185][0m |          -0.0166 |          21.1880 |           0.1768 |
[32m[20221213 15:37:37 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 15:37:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.62
[32m[20221213 15:37:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.92
[32m[20221213 15:37:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.99
[32m[20221213 15:37:38 @agent_ppo2.py:143][0m Total time:      44.71 min
[32m[20221213 15:37:38 @agent_ppo2.py:145][0m 4012032 total steps have happened
[32m[20221213 15:37:38 @agent_ppo2.py:121][0m #------------------------ Iteration 1959 --------------------------#
[32m[20221213 15:37:38 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:38 @agent_ppo2.py:185][0m |           0.0000 |          22.4100 |           0.1761 |
[32m[20221213 15:37:38 @agent_ppo2.py:185][0m |           0.0152 |          26.1686 |           0.1756 |
[32m[20221213 15:37:38 @agent_ppo2.py:185][0m |          -0.0057 |          21.7612 |           0.1757 |
[32m[20221213 15:37:38 @agent_ppo2.py:185][0m |          -0.0107 |          21.4977 |           0.1759 |
[32m[20221213 15:37:38 @agent_ppo2.py:185][0m |          -0.0129 |          21.4270 |           0.1759 |
[32m[20221213 15:37:38 @agent_ppo2.py:185][0m |          -0.0134 |          21.2808 |           0.1759 |
[32m[20221213 15:37:39 @agent_ppo2.py:185][0m |          -0.0153 |          21.1804 |           0.1758 |
[32m[20221213 15:37:39 @agent_ppo2.py:185][0m |          -0.0159 |          21.0451 |           0.1759 |
[32m[20221213 15:37:39 @agent_ppo2.py:185][0m |          -0.0169 |          20.9509 |           0.1760 |
[32m[20221213 15:37:39 @agent_ppo2.py:185][0m |          -0.0163 |          20.8454 |           0.1761 |
[32m[20221213 15:37:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:37:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 285.45
[32m[20221213 15:37:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.24
[32m[20221213 15:37:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 236.57
[32m[20221213 15:37:39 @agent_ppo2.py:143][0m Total time:      44.73 min
[32m[20221213 15:37:39 @agent_ppo2.py:145][0m 4014080 total steps have happened
[32m[20221213 15:37:39 @agent_ppo2.py:121][0m #------------------------ Iteration 1960 --------------------------#
[32m[20221213 15:37:39 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:37:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:39 @agent_ppo2.py:185][0m |          -0.0037 |          22.8353 |           0.1780 |
[32m[20221213 15:37:39 @agent_ppo2.py:185][0m |          -0.0085 |          22.4888 |           0.1779 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0107 |          22.2297 |           0.1776 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0148 |          21.9181 |           0.1773 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0139 |          21.8032 |           0.1774 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0141 |          21.7157 |           0.1774 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0182 |          21.6190 |           0.1772 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0195 |          21.4299 |           0.1770 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0195 |          21.3143 |           0.1769 |
[32m[20221213 15:37:40 @agent_ppo2.py:185][0m |          -0.0202 |          21.2211 |           0.1769 |
[32m[20221213 15:37:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:37:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 242.32
[32m[20221213 15:37:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 259.48
[32m[20221213 15:37:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.16
[32m[20221213 15:37:40 @agent_ppo2.py:143][0m Total time:      44.75 min
[32m[20221213 15:37:40 @agent_ppo2.py:145][0m 4016128 total steps have happened
[32m[20221213 15:37:40 @agent_ppo2.py:121][0m #------------------------ Iteration 1961 --------------------------#
[32m[20221213 15:37:41 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0002 |          23.8084 |           0.1751 |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0041 |          23.4003 |           0.1749 |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0092 |          23.1626 |           0.1751 |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0111 |          22.9745 |           0.1750 |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0141 |          22.9217 |           0.1750 |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0172 |          22.8035 |           0.1749 |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0154 |          22.7632 |           0.1749 |
[32m[20221213 15:37:41 @agent_ppo2.py:185][0m |          -0.0160 |          22.7733 |           0.1749 |
[32m[20221213 15:37:42 @agent_ppo2.py:185][0m |          -0.0180 |          22.5929 |           0.1748 |
[32m[20221213 15:37:42 @agent_ppo2.py:185][0m |          -0.0135 |          22.9054 |           0.1748 |
[32m[20221213 15:37:42 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:37:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 279.07
[32m[20221213 15:37:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 304.29
[32m[20221213 15:37:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 225.10
[32m[20221213 15:37:42 @agent_ppo2.py:143][0m Total time:      44.78 min
[32m[20221213 15:37:42 @agent_ppo2.py:145][0m 4018176 total steps have happened
[32m[20221213 15:37:42 @agent_ppo2.py:121][0m #------------------------ Iteration 1962 --------------------------#
[32m[20221213 15:37:42 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:42 @agent_ppo2.py:185][0m |          -0.0028 |          24.1422 |           0.1806 |
[32m[20221213 15:37:42 @agent_ppo2.py:185][0m |          -0.0084 |          23.3912 |           0.1801 |
[32m[20221213 15:37:42 @agent_ppo2.py:185][0m |          -0.0090 |          23.0736 |           0.1801 |
[32m[20221213 15:37:43 @agent_ppo2.py:185][0m |          -0.0007 |          25.6765 |           0.1803 |
[32m[20221213 15:37:43 @agent_ppo2.py:185][0m |          -0.0102 |          22.9840 |           0.1802 |
[32m[20221213 15:37:43 @agent_ppo2.py:185][0m |          -0.0136 |          22.7176 |           0.1802 |
[32m[20221213 15:37:43 @agent_ppo2.py:185][0m |          -0.0098 |          22.8410 |           0.1801 |
[32m[20221213 15:37:43 @agent_ppo2.py:185][0m |          -0.0175 |          22.6075 |           0.1803 |
[32m[20221213 15:37:43 @agent_ppo2.py:185][0m |          -0.0151 |          22.5227 |           0.1803 |
[32m[20221213 15:37:43 @agent_ppo2.py:185][0m |          -0.0095 |          22.9263 |           0.1801 |
[32m[20221213 15:37:43 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:37:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 250.13
[32m[20221213 15:37:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.29
[32m[20221213 15:37:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.78
[32m[20221213 15:37:43 @agent_ppo2.py:143][0m Total time:      44.80 min
[32m[20221213 15:37:43 @agent_ppo2.py:145][0m 4020224 total steps have happened
[32m[20221213 15:37:43 @agent_ppo2.py:121][0m #------------------------ Iteration 1963 --------------------------#
[32m[20221213 15:37:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |           0.0002 |          22.3494 |           0.1739 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |           0.0052 |          23.9251 |           0.1733 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |          -0.0048 |          21.8494 |           0.1730 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |          -0.0114 |          21.4441 |           0.1732 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |          -0.0138 |          21.1953 |           0.1733 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |          -0.0158 |          20.9739 |           0.1731 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |          -0.0138 |          20.7204 |           0.1731 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |          -0.0160 |          20.5299 |           0.1730 |
[32m[20221213 15:37:44 @agent_ppo2.py:185][0m |          -0.0162 |          20.3334 |           0.1729 |
[32m[20221213 15:37:45 @agent_ppo2.py:185][0m |          -0.0173 |          20.0603 |           0.1729 |
[32m[20221213 15:37:45 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:37:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 263.89
[32m[20221213 15:37:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.76
[32m[20221213 15:37:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 248.99
[32m[20221213 15:37:45 @agent_ppo2.py:143][0m Total time:      44.83 min
[32m[20221213 15:37:45 @agent_ppo2.py:145][0m 4022272 total steps have happened
[32m[20221213 15:37:45 @agent_ppo2.py:121][0m #------------------------ Iteration 1964 --------------------------#
[32m[20221213 15:37:45 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:45 @agent_ppo2.py:185][0m |           0.0041 |          23.1985 |           0.1769 |
[32m[20221213 15:37:45 @agent_ppo2.py:185][0m |          -0.0038 |          22.5457 |           0.1768 |
[32m[20221213 15:37:45 @agent_ppo2.py:185][0m |          -0.0084 |          22.2732 |           0.1767 |
[32m[20221213 15:37:45 @agent_ppo2.py:185][0m |          -0.0065 |          22.2529 |           0.1766 |
[32m[20221213 15:37:46 @agent_ppo2.py:185][0m |          -0.0131 |          21.9894 |           0.1764 |
[32m[20221213 15:37:46 @agent_ppo2.py:185][0m |          -0.0161 |          21.8707 |           0.1766 |
[32m[20221213 15:37:46 @agent_ppo2.py:185][0m |          -0.0138 |          21.7803 |           0.1765 |
[32m[20221213 15:37:46 @agent_ppo2.py:185][0m |          -0.0161 |          21.6656 |           0.1765 |
[32m[20221213 15:37:46 @agent_ppo2.py:185][0m |          -0.0173 |          21.5724 |           0.1764 |
[32m[20221213 15:37:46 @agent_ppo2.py:185][0m |          -0.0175 |          21.4582 |           0.1763 |
[32m[20221213 15:37:46 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:37:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 276.57
[32m[20221213 15:37:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.39
[32m[20221213 15:37:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 259.24
[32m[20221213 15:37:46 @agent_ppo2.py:143][0m Total time:      44.85 min
[32m[20221213 15:37:46 @agent_ppo2.py:145][0m 4024320 total steps have happened
[32m[20221213 15:37:46 @agent_ppo2.py:121][0m #------------------------ Iteration 1965 --------------------------#
[32m[20221213 15:37:46 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |           0.0004 |          23.3904 |           0.1772 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |           0.0018 |          23.8505 |           0.1774 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0098 |          22.4234 |           0.1770 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0083 |          22.3083 |           0.1771 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0130 |          22.0979 |           0.1771 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0136 |          22.0637 |           0.1769 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0065 |          22.6915 |           0.1771 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0155 |          21.8459 |           0.1770 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0177 |          21.8035 |           0.1770 |
[32m[20221213 15:37:47 @agent_ppo2.py:185][0m |          -0.0144 |          21.7253 |           0.1772 |
[32m[20221213 15:37:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:37:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 278.83
[32m[20221213 15:37:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.81
[32m[20221213 15:37:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.81
[32m[20221213 15:37:48 @agent_ppo2.py:143][0m Total time:      44.87 min
[32m[20221213 15:37:48 @agent_ppo2.py:145][0m 4026368 total steps have happened
[32m[20221213 15:37:48 @agent_ppo2.py:121][0m #------------------------ Iteration 1966 --------------------------#
[32m[20221213 15:37:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:48 @agent_ppo2.py:185][0m |           0.0007 |          24.0762 |           0.1858 |
[32m[20221213 15:37:48 @agent_ppo2.py:185][0m |          -0.0095 |          23.5218 |           0.1856 |
[32m[20221213 15:37:48 @agent_ppo2.py:185][0m |          -0.0097 |          23.3566 |           0.1854 |
[32m[20221213 15:37:48 @agent_ppo2.py:185][0m |          -0.0121 |          23.1644 |           0.1854 |
[32m[20221213 15:37:48 @agent_ppo2.py:185][0m |          -0.0073 |          23.6771 |           0.1852 |
[32m[20221213 15:37:48 @agent_ppo2.py:185][0m |          -0.0139 |          22.9818 |           0.1851 |
[32m[20221213 15:37:49 @agent_ppo2.py:185][0m |          -0.0157 |          22.9258 |           0.1849 |
[32m[20221213 15:37:49 @agent_ppo2.py:185][0m |          -0.0146 |          22.8194 |           0.1849 |
[32m[20221213 15:37:49 @agent_ppo2.py:185][0m |          -0.0141 |          22.8482 |           0.1848 |
[32m[20221213 15:37:49 @agent_ppo2.py:185][0m |          -0.0163 |          22.6797 |           0.1847 |
[32m[20221213 15:37:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:37:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.16
[32m[20221213 15:37:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 301.69
[32m[20221213 15:37:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.32
[32m[20221213 15:37:49 @agent_ppo2.py:143][0m Total time:      44.90 min
[32m[20221213 15:37:49 @agent_ppo2.py:145][0m 4028416 total steps have happened
[32m[20221213 15:37:49 @agent_ppo2.py:121][0m #------------------------ Iteration 1967 --------------------------#
[32m[20221213 15:37:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:49 @agent_ppo2.py:185][0m |           0.0078 |          23.4721 |           0.1751 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |          -0.0055 |          21.8189 |           0.1740 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |          -0.0067 |          21.6530 |           0.1743 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |          -0.0096 |          21.5416 |           0.1742 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |          -0.0077 |          21.7308 |           0.1740 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |          -0.0102 |          21.4385 |           0.1742 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |          -0.0124 |          21.3629 |           0.1742 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |           0.0016 |          23.8414 |           0.1741 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |           0.0050 |          23.6098 |           0.1739 |
[32m[20221213 15:37:50 @agent_ppo2.py:185][0m |          -0.0144 |          21.2335 |           0.1739 |
[32m[20221213 15:37:50 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221213 15:37:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 270.16
[32m[20221213 15:37:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 283.51
[32m[20221213 15:37:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 271.05
[32m[20221213 15:37:50 @agent_ppo2.py:143][0m Total time:      44.92 min
[32m[20221213 15:37:50 @agent_ppo2.py:145][0m 4030464 total steps have happened
[32m[20221213 15:37:50 @agent_ppo2.py:121][0m #------------------------ Iteration 1968 --------------------------#
[32m[20221213 15:37:51 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0005 |          22.7900 |           0.1759 |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0047 |          22.0662 |           0.1754 |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0097 |          21.8367 |           0.1753 |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0113 |          21.6624 |           0.1751 |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0101 |          21.7135 |           0.1749 |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0125 |          21.4927 |           0.1748 |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0060 |          22.8983 |           0.1747 |
[32m[20221213 15:37:51 @agent_ppo2.py:185][0m |          -0.0150 |          21.3652 |           0.1744 |
[32m[20221213 15:37:52 @agent_ppo2.py:185][0m |          -0.0185 |          21.2432 |           0.1743 |
[32m[20221213 15:37:52 @agent_ppo2.py:185][0m |          -0.0189 |          21.1907 |           0.1742 |
[32m[20221213 15:37:52 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:37:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.20
[32m[20221213 15:37:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.37
[32m[20221213 15:37:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.94
[32m[20221213 15:37:52 @agent_ppo2.py:143][0m Total time:      44.94 min
[32m[20221213 15:37:52 @agent_ppo2.py:145][0m 4032512 total steps have happened
[32m[20221213 15:37:52 @agent_ppo2.py:121][0m #------------------------ Iteration 1969 --------------------------#
[32m[20221213 15:37:52 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:52 @agent_ppo2.py:185][0m |          -0.0017 |          22.4475 |           0.1746 |
[32m[20221213 15:37:52 @agent_ppo2.py:185][0m |          -0.0080 |          22.0314 |           0.1743 |
[32m[20221213 15:37:52 @agent_ppo2.py:185][0m |          -0.0094 |          21.8563 |           0.1741 |
[32m[20221213 15:37:52 @agent_ppo2.py:185][0m |          -0.0127 |          21.6785 |           0.1741 |
[32m[20221213 15:37:53 @agent_ppo2.py:185][0m |          -0.0150 |          21.6025 |           0.1739 |
[32m[20221213 15:37:53 @agent_ppo2.py:185][0m |          -0.0161 |          21.4917 |           0.1739 |
[32m[20221213 15:37:53 @agent_ppo2.py:185][0m |          -0.0115 |          21.6053 |           0.1738 |
[32m[20221213 15:37:53 @agent_ppo2.py:185][0m |          -0.0171 |          21.3594 |           0.1740 |
[32m[20221213 15:37:53 @agent_ppo2.py:185][0m |          -0.0154 |          21.3288 |           0.1739 |
[32m[20221213 15:37:53 @agent_ppo2.py:185][0m |          -0.0180 |          21.2808 |           0.1739 |
[32m[20221213 15:37:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:37:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.23
[32m[20221213 15:37:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.98
[32m[20221213 15:37:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 295.68
[32m[20221213 15:37:53 @agent_ppo2.py:143][0m Total time:      44.97 min
[32m[20221213 15:37:53 @agent_ppo2.py:145][0m 4034560 total steps have happened
[32m[20221213 15:37:53 @agent_ppo2.py:121][0m #------------------------ Iteration 1970 --------------------------#
[32m[20221213 15:37:53 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 15:37:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |           0.0002 |          21.4715 |           0.1780 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0067 |          21.1866 |           0.1777 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0081 |          21.0401 |           0.1775 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0086 |          21.0136 |           0.1773 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0062 |          21.5546 |           0.1771 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0118 |          20.8105 |           0.1769 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0054 |          22.0886 |           0.1770 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0071 |          21.2436 |           0.1769 |
[32m[20221213 15:37:54 @agent_ppo2.py:185][0m |          -0.0134 |          20.6453 |           0.1768 |
[32m[20221213 15:37:55 @agent_ppo2.py:185][0m |          -0.0137 |          20.5761 |           0.1767 |
[32m[20221213 15:37:55 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 15:37:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.38
[32m[20221213 15:37:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 279.04
[32m[20221213 15:37:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.21
[32m[20221213 15:37:55 @agent_ppo2.py:143][0m Total time:      44.99 min
[32m[20221213 15:37:55 @agent_ppo2.py:145][0m 4036608 total steps have happened
[32m[20221213 15:37:55 @agent_ppo2.py:121][0m #------------------------ Iteration 1971 --------------------------#
[32m[20221213 15:37:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:55 @agent_ppo2.py:185][0m |          -0.0014 |          22.3279 |           0.1728 |
[32m[20221213 15:37:55 @agent_ppo2.py:185][0m |          -0.0099 |          21.8821 |           0.1727 |
[32m[20221213 15:37:55 @agent_ppo2.py:185][0m |          -0.0065 |          21.8558 |           0.1725 |
[32m[20221213 15:37:55 @agent_ppo2.py:185][0m |          -0.0064 |          22.5859 |           0.1724 |
[32m[20221213 15:37:55 @agent_ppo2.py:185][0m |          -0.0165 |          21.3094 |           0.1724 |
[32m[20221213 15:37:56 @agent_ppo2.py:185][0m |          -0.0187 |          21.1440 |           0.1726 |
[32m[20221213 15:37:56 @agent_ppo2.py:185][0m |          -0.0108 |          21.6386 |           0.1725 |
[32m[20221213 15:37:56 @agent_ppo2.py:185][0m |          -0.0149 |          21.2155 |           0.1723 |
[32m[20221213 15:37:56 @agent_ppo2.py:185][0m |          -0.0194 |          20.8665 |           0.1723 |
[32m[20221213 15:37:56 @agent_ppo2.py:185][0m |          -0.0223 |          20.7147 |           0.1724 |
[32m[20221213 15:37:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:37:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.69
[32m[20221213 15:37:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 272.26
[32m[20221213 15:37:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.15
[32m[20221213 15:37:56 @agent_ppo2.py:143][0m Total time:      45.02 min
[32m[20221213 15:37:56 @agent_ppo2.py:145][0m 4038656 total steps have happened
[32m[20221213 15:37:56 @agent_ppo2.py:121][0m #------------------------ Iteration 1972 --------------------------#
[32m[20221213 15:37:56 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:37:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:56 @agent_ppo2.py:185][0m |           0.0011 |          22.1411 |           0.1795 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0046 |          21.7660 |           0.1791 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0086 |          21.5714 |           0.1791 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0103 |          21.4425 |           0.1790 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0131 |          21.3201 |           0.1790 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0007 |          24.2623 |           0.1788 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0102 |          21.1495 |           0.1786 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0148 |          21.0729 |           0.1784 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0159 |          20.9518 |           0.1782 |
[32m[20221213 15:37:57 @agent_ppo2.py:185][0m |          -0.0067 |          22.3405 |           0.1783 |
[32m[20221213 15:37:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:37:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 271.94
[32m[20221213 15:37:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 289.37
[32m[20221213 15:37:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 260.92
[32m[20221213 15:37:57 @agent_ppo2.py:143][0m Total time:      45.04 min
[32m[20221213 15:37:57 @agent_ppo2.py:145][0m 4040704 total steps have happened
[32m[20221213 15:37:57 @agent_ppo2.py:121][0m #------------------------ Iteration 1973 --------------------------#
[32m[20221213 15:37:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:37:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |           0.0115 |          26.2110 |           0.1831 |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |          -0.0081 |          22.7411 |           0.1828 |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |          -0.0099 |          22.4428 |           0.1827 |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |          -0.0107 |          22.2258 |           0.1826 |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |          -0.0139 |          22.0077 |           0.1824 |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |          -0.0113 |          21.8257 |           0.1823 |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |          -0.0151 |          21.3817 |           0.1822 |
[32m[20221213 15:37:58 @agent_ppo2.py:185][0m |          -0.0137 |          21.2832 |           0.1821 |
[32m[20221213 15:37:59 @agent_ppo2.py:185][0m |          -0.0170 |          20.7888 |           0.1820 |
[32m[20221213 15:37:59 @agent_ppo2.py:185][0m |          -0.0094 |          20.6930 |           0.1821 |
[32m[20221213 15:37:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:37:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.10
[32m[20221213 15:37:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.22
[32m[20221213 15:37:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.01
[32m[20221213 15:37:59 @agent_ppo2.py:143][0m Total time:      45.06 min
[32m[20221213 15:37:59 @agent_ppo2.py:145][0m 4042752 total steps have happened
[32m[20221213 15:37:59 @agent_ppo2.py:121][0m #------------------------ Iteration 1974 --------------------------#
[32m[20221213 15:37:59 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:37:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:37:59 @agent_ppo2.py:185][0m |          -0.0013 |          23.2465 |           0.1737 |
[32m[20221213 15:37:59 @agent_ppo2.py:185][0m |          -0.0014 |          22.2687 |           0.1736 |
[32m[20221213 15:37:59 @agent_ppo2.py:185][0m |          -0.0079 |          21.8378 |           0.1733 |
[32m[20221213 15:37:59 @agent_ppo2.py:185][0m |          -0.0082 |          21.6176 |           0.1731 |
[32m[20221213 15:38:00 @agent_ppo2.py:185][0m |          -0.0125 |          21.3605 |           0.1731 |
[32m[20221213 15:38:00 @agent_ppo2.py:185][0m |          -0.0142 |          21.1858 |           0.1733 |
[32m[20221213 15:38:00 @agent_ppo2.py:185][0m |          -0.0162 |          21.0441 |           0.1733 |
[32m[20221213 15:38:00 @agent_ppo2.py:185][0m |          -0.0168 |          20.9242 |           0.1730 |
[32m[20221213 15:38:00 @agent_ppo2.py:185][0m |          -0.0147 |          20.7087 |           0.1733 |
[32m[20221213 15:38:00 @agent_ppo2.py:185][0m |          -0.0194 |          20.6385 |           0.1732 |
[32m[20221213 15:38:00 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:38:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 249.52
[32m[20221213 15:38:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 275.94
[32m[20221213 15:38:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 210.31
[32m[20221213 15:38:00 @agent_ppo2.py:143][0m Total time:      45.08 min
[32m[20221213 15:38:00 @agent_ppo2.py:145][0m 4044800 total steps have happened
[32m[20221213 15:38:00 @agent_ppo2.py:121][0m #------------------------ Iteration 1975 --------------------------#
[32m[20221213 15:38:00 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |           0.0116 |          26.2942 |           0.1739 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0032 |          23.2170 |           0.1739 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0104 |          22.4202 |           0.1736 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0132 |          22.1698 |           0.1734 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0145 |          22.0019 |           0.1734 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0145 |          21.9309 |           0.1732 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0147 |          21.8398 |           0.1730 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0154 |          21.7759 |           0.1731 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0069 |          22.4985 |           0.1729 |
[32m[20221213 15:38:01 @agent_ppo2.py:185][0m |          -0.0165 |          21.6235 |           0.1728 |
[32m[20221213 15:38:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:38:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.83
[32m[20221213 15:38:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 274.34
[32m[20221213 15:38:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 262.69
[32m[20221213 15:38:02 @agent_ppo2.py:143][0m Total time:      45.11 min
[32m[20221213 15:38:02 @agent_ppo2.py:145][0m 4046848 total steps have happened
[32m[20221213 15:38:02 @agent_ppo2.py:121][0m #------------------------ Iteration 1976 --------------------------#
[32m[20221213 15:38:02 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:02 @agent_ppo2.py:185][0m |          -0.0022 |          22.6487 |           0.1681 |
[32m[20221213 15:38:02 @agent_ppo2.py:185][0m |          -0.0104 |          21.8275 |           0.1673 |
[32m[20221213 15:38:02 @agent_ppo2.py:185][0m |          -0.0147 |          21.4120 |           0.1672 |
[32m[20221213 15:38:02 @agent_ppo2.py:185][0m |          -0.0150 |          21.2832 |           0.1673 |
[32m[20221213 15:38:02 @agent_ppo2.py:185][0m |          -0.0165 |          21.1464 |           0.1671 |
[32m[20221213 15:38:02 @agent_ppo2.py:185][0m |          -0.0166 |          21.0219 |           0.1672 |
[32m[20221213 15:38:02 @agent_ppo2.py:185][0m |          -0.0184 |          20.9245 |           0.1669 |
[32m[20221213 15:38:03 @agent_ppo2.py:185][0m |          -0.0187 |          20.8560 |           0.1670 |
[32m[20221213 15:38:03 @agent_ppo2.py:185][0m |          -0.0191 |          20.8345 |           0.1669 |
[32m[20221213 15:38:03 @agent_ppo2.py:185][0m |          -0.0153 |          21.2686 |           0.1670 |
[32m[20221213 15:38:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 15:38:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.98
[32m[20221213 15:38:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.71
[32m[20221213 15:38:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.10
[32m[20221213 15:38:03 @agent_ppo2.py:143][0m Total time:      45.13 min
[32m[20221213 15:38:03 @agent_ppo2.py:145][0m 4048896 total steps have happened
[32m[20221213 15:38:03 @agent_ppo2.py:121][0m #------------------------ Iteration 1977 --------------------------#
[32m[20221213 15:38:03 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:03 @agent_ppo2.py:185][0m |          -0.0021 |          21.6415 |           0.1724 |
[32m[20221213 15:38:03 @agent_ppo2.py:185][0m |          -0.0094 |          20.8849 |           0.1719 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |          -0.0121 |          20.3308 |           0.1717 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |           0.0045 |          22.4782 |           0.1717 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |          -0.0139 |          19.7785 |           0.1711 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |          -0.0152 |          19.3322 |           0.1714 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |          -0.0154 |          19.2495 |           0.1712 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |          -0.0193 |          18.9544 |           0.1713 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |          -0.0208 |          18.7700 |           0.1711 |
[32m[20221213 15:38:04 @agent_ppo2.py:185][0m |          -0.0211 |          18.6288 |           0.1710 |
[32m[20221213 15:38:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:38:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.11
[32m[20221213 15:38:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 300.33
[32m[20221213 15:38:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 265.84
[32m[20221213 15:38:04 @agent_ppo2.py:143][0m Total time:      45.15 min
[32m[20221213 15:38:04 @agent_ppo2.py:145][0m 4050944 total steps have happened
[32m[20221213 15:38:04 @agent_ppo2.py:121][0m #------------------------ Iteration 1978 --------------------------#
[32m[20221213 15:38:05 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |           0.0022 |          23.5547 |           0.1674 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0060 |          22.9233 |           0.1675 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0116 |          22.6463 |           0.1674 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0067 |          23.3925 |           0.1673 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0149 |          22.2869 |           0.1675 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0156 |          22.2330 |           0.1675 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0143 |          22.1044 |           0.1676 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0152 |          21.9704 |           0.1678 |
[32m[20221213 15:38:05 @agent_ppo2.py:185][0m |          -0.0162 |          21.9157 |           0.1678 |
[32m[20221213 15:38:06 @agent_ppo2.py:185][0m |          -0.0169 |          21.8549 |           0.1678 |
[32m[20221213 15:38:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:38:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.16
[32m[20221213 15:38:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.96
[32m[20221213 15:38:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 268.94
[32m[20221213 15:38:06 @agent_ppo2.py:143][0m Total time:      45.18 min
[32m[20221213 15:38:06 @agent_ppo2.py:145][0m 4052992 total steps have happened
[32m[20221213 15:38:06 @agent_ppo2.py:121][0m #------------------------ Iteration 1979 --------------------------#
[32m[20221213 15:38:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:38:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:06 @agent_ppo2.py:185][0m |          -0.0016 |          22.1464 |           0.1743 |
[32m[20221213 15:38:06 @agent_ppo2.py:185][0m |           0.0046 |          24.2151 |           0.1738 |
[32m[20221213 15:38:06 @agent_ppo2.py:185][0m |          -0.0096 |          21.5786 |           0.1736 |
[32m[20221213 15:38:06 @agent_ppo2.py:185][0m |          -0.0088 |          21.8553 |           0.1735 |
[32m[20221213 15:38:06 @agent_ppo2.py:185][0m |          -0.0095 |          21.4783 |           0.1735 |
[32m[20221213 15:38:07 @agent_ppo2.py:185][0m |          -0.0144 |          21.2131 |           0.1732 |
[32m[20221213 15:38:07 @agent_ppo2.py:185][0m |          -0.0167 |          21.1659 |           0.1731 |
[32m[20221213 15:38:07 @agent_ppo2.py:185][0m |          -0.0179 |          21.0255 |           0.1731 |
[32m[20221213 15:38:07 @agent_ppo2.py:185][0m |          -0.0132 |          21.0863 |           0.1730 |
[32m[20221213 15:38:07 @agent_ppo2.py:185][0m |          -0.0172 |          20.8880 |           0.1730 |
[32m[20221213 15:38:07 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:38:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.12
[32m[20221213 15:38:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 258.41
[32m[20221213 15:38:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 267.90
[32m[20221213 15:38:07 @agent_ppo2.py:143][0m Total time:      45.20 min
[32m[20221213 15:38:07 @agent_ppo2.py:145][0m 4055040 total steps have happened
[32m[20221213 15:38:07 @agent_ppo2.py:121][0m #------------------------ Iteration 1980 --------------------------#
[32m[20221213 15:38:07 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 15:38:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:07 @agent_ppo2.py:185][0m |           0.0027 |          22.4924 |           0.1733 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0081 |          22.0603 |           0.1732 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0101 |          21.9202 |           0.1733 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0133 |          21.7682 |           0.1729 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0125 |          21.6918 |           0.1728 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0163 |          21.5832 |           0.1726 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0170 |          21.5255 |           0.1724 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0178 |          21.4886 |           0.1723 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0143 |          21.6438 |           0.1724 |
[32m[20221213 15:38:08 @agent_ppo2.py:185][0m |          -0.0168 |          21.3647 |           0.1722 |
[32m[20221213 15:38:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:38:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.63
[32m[20221213 15:38:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 269.42
[32m[20221213 15:38:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 272.11
[32m[20221213 15:38:08 @agent_ppo2.py:143][0m Total time:      45.22 min
[32m[20221213 15:38:08 @agent_ppo2.py:145][0m 4057088 total steps have happened
[32m[20221213 15:38:08 @agent_ppo2.py:121][0m #------------------------ Iteration 1981 --------------------------#
[32m[20221213 15:38:09 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:38:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0009 |          21.9797 |           0.1726 |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0047 |          21.7638 |           0.1723 |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0098 |          21.6388 |           0.1720 |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0077 |          21.5545 |           0.1719 |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0100 |          21.4821 |           0.1717 |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0117 |          21.4340 |           0.1719 |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0151 |          21.3883 |           0.1720 |
[32m[20221213 15:38:09 @agent_ppo2.py:185][0m |          -0.0146 |          21.3170 |           0.1717 |
[32m[20221213 15:38:10 @agent_ppo2.py:185][0m |          -0.0105 |          21.6965 |           0.1718 |
[32m[20221213 15:38:10 @agent_ppo2.py:185][0m |          -0.0157 |          21.2854 |           0.1718 |
[32m[20221213 15:38:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:38:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 256.45
[32m[20221213 15:38:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.92
[32m[20221213 15:38:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.22
[32m[20221213 15:38:10 @agent_ppo2.py:143][0m Total time:      45.24 min
[32m[20221213 15:38:10 @agent_ppo2.py:145][0m 4059136 total steps have happened
[32m[20221213 15:38:10 @agent_ppo2.py:121][0m #------------------------ Iteration 1982 --------------------------#
[32m[20221213 15:38:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:38:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:10 @agent_ppo2.py:185][0m |           0.0033 |          21.7509 |           0.1718 |
[32m[20221213 15:38:10 @agent_ppo2.py:185][0m |          -0.0084 |          21.2572 |           0.1712 |
[32m[20221213 15:38:10 @agent_ppo2.py:185][0m |          -0.0102 |          21.0870 |           0.1712 |
[32m[20221213 15:38:11 @agent_ppo2.py:185][0m |          -0.0111 |          20.9927 |           0.1707 |
[32m[20221213 15:38:11 @agent_ppo2.py:185][0m |          -0.0120 |          20.9429 |           0.1706 |
[32m[20221213 15:38:11 @agent_ppo2.py:185][0m |          -0.0081 |          21.1490 |           0.1707 |
[32m[20221213 15:38:11 @agent_ppo2.py:185][0m |          -0.0138 |          20.8268 |           0.1703 |
[32m[20221213 15:38:11 @agent_ppo2.py:185][0m |          -0.0163 |          20.7050 |           0.1704 |
[32m[20221213 15:38:11 @agent_ppo2.py:185][0m |          -0.0166 |          20.6780 |           0.1702 |
[32m[20221213 15:38:11 @agent_ppo2.py:185][0m |          -0.0166 |          20.5967 |           0.1700 |
[32m[20221213 15:38:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:38:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 291.08
[32m[20221213 15:38:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 299.84
[32m[20221213 15:38:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 291.44
[32m[20221213 15:38:11 @agent_ppo2.py:143][0m Total time:      45.27 min
[32m[20221213 15:38:11 @agent_ppo2.py:145][0m 4061184 total steps have happened
[32m[20221213 15:38:11 @agent_ppo2.py:121][0m #------------------------ Iteration 1983 --------------------------#
[32m[20221213 15:38:11 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |           0.0019 |          21.9745 |           0.1683 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0078 |          21.4337 |           0.1679 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0108 |          21.2588 |           0.1679 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0133 |          21.1311 |           0.1676 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0090 |          21.5567 |           0.1678 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0030 |          21.7937 |           0.1675 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0114 |          20.8831 |           0.1676 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0101 |          20.9672 |           0.1674 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |          -0.0169 |          20.7107 |           0.1674 |
[32m[20221213 15:38:12 @agent_ppo2.py:185][0m |           0.0015 |          24.1794 |           0.1674 |
[32m[20221213 15:38:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:38:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.42
[32m[20221213 15:38:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 296.63
[32m[20221213 15:38:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.22
[32m[20221213 15:38:13 @agent_ppo2.py:143][0m Total time:      45.29 min
[32m[20221213 15:38:13 @agent_ppo2.py:145][0m 4063232 total steps have happened
[32m[20221213 15:38:13 @agent_ppo2.py:121][0m #------------------------ Iteration 1984 --------------------------#
[32m[20221213 15:38:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:13 @agent_ppo2.py:185][0m |          -0.0019 |          21.9426 |           0.1653 |
[32m[20221213 15:38:13 @agent_ppo2.py:185][0m |          -0.0044 |          21.4316 |           0.1651 |
[32m[20221213 15:38:13 @agent_ppo2.py:185][0m |          -0.0094 |          21.1043 |           0.1651 |
[32m[20221213 15:38:13 @agent_ppo2.py:185][0m |          -0.0133 |          20.9501 |           0.1654 |
[32m[20221213 15:38:13 @agent_ppo2.py:185][0m |          -0.0089 |          20.9186 |           0.1655 |
[32m[20221213 15:38:13 @agent_ppo2.py:185][0m |          -0.0075 |          21.3635 |           0.1656 |
[32m[20221213 15:38:14 @agent_ppo2.py:185][0m |          -0.0154 |          20.6822 |           0.1654 |
[32m[20221213 15:38:14 @agent_ppo2.py:185][0m |          -0.0123 |          20.6523 |           0.1655 |
[32m[20221213 15:38:14 @agent_ppo2.py:185][0m |          -0.0103 |          21.1167 |           0.1655 |
[32m[20221213 15:38:14 @agent_ppo2.py:185][0m |          -0.0145 |          20.6125 |           0.1654 |
[32m[20221213 15:38:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:38:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 272.94
[32m[20221213 15:38:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 291.32
[32m[20221213 15:38:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.33
[32m[20221213 15:38:14 @agent_ppo2.py:143][0m Total time:      45.31 min
[32m[20221213 15:38:14 @agent_ppo2.py:145][0m 4065280 total steps have happened
[32m[20221213 15:38:14 @agent_ppo2.py:121][0m #------------------------ Iteration 1985 --------------------------#
[32m[20221213 15:38:14 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:14 @agent_ppo2.py:185][0m |          -0.0032 |          22.4256 |           0.1707 |
[32m[20221213 15:38:14 @agent_ppo2.py:185][0m |          -0.0099 |          22.2396 |           0.1702 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0115 |          22.1535 |           0.1702 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0135 |          21.9970 |           0.1700 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0131 |          21.9339 |           0.1699 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0121 |          21.8819 |           0.1696 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0170 |          21.7991 |           0.1695 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0161 |          21.7537 |           0.1694 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0164 |          21.7291 |           0.1693 |
[32m[20221213 15:38:15 @agent_ppo2.py:185][0m |          -0.0155 |          21.6978 |           0.1693 |
[32m[20221213 15:38:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:38:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.84
[32m[20221213 15:38:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 285.97
[32m[20221213 15:38:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.90
[32m[20221213 15:38:15 @agent_ppo2.py:143][0m Total time:      45.34 min
[32m[20221213 15:38:15 @agent_ppo2.py:145][0m 4067328 total steps have happened
[32m[20221213 15:38:15 @agent_ppo2.py:121][0m #------------------------ Iteration 1986 --------------------------#
[32m[20221213 15:38:16 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |           0.0016 |          21.5845 |           0.1662 |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |          -0.0057 |          21.4420 |           0.1661 |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |          -0.0084 |          21.3280 |           0.1662 |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |          -0.0077 |          21.2926 |           0.1660 |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |          -0.0100 |          21.2053 |           0.1659 |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |          -0.0094 |          21.1891 |           0.1659 |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |          -0.0121 |          21.1220 |           0.1659 |
[32m[20221213 15:38:16 @agent_ppo2.py:185][0m |           0.0101 |          23.9733 |           0.1660 |
[32m[20221213 15:38:17 @agent_ppo2.py:185][0m |          -0.0139 |          21.1258 |           0.1658 |
[32m[20221213 15:38:17 @agent_ppo2.py:185][0m |          -0.0132 |          21.0045 |           0.1654 |
[32m[20221213 15:38:17 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 15:38:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 280.13
[32m[20221213 15:38:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.60
[32m[20221213 15:38:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.90
[32m[20221213 15:38:17 @agent_ppo2.py:143][0m Total time:      45.36 min
[32m[20221213 15:38:17 @agent_ppo2.py:145][0m 4069376 total steps have happened
[32m[20221213 15:38:17 @agent_ppo2.py:121][0m #------------------------ Iteration 1987 --------------------------#
[32m[20221213 15:38:17 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:17 @agent_ppo2.py:185][0m |           0.0003 |          21.4558 |           0.1665 |
[32m[20221213 15:38:17 @agent_ppo2.py:185][0m |          -0.0039 |          20.8789 |           0.1659 |
[32m[20221213 15:38:17 @agent_ppo2.py:185][0m |          -0.0081 |          20.4882 |           0.1658 |
[32m[20221213 15:38:17 @agent_ppo2.py:185][0m |          -0.0136 |          20.3104 |           0.1656 |
[32m[20221213 15:38:18 @agent_ppo2.py:185][0m |          -0.0139 |          20.1766 |           0.1653 |
[32m[20221213 15:38:18 @agent_ppo2.py:185][0m |          -0.0113 |          20.1316 |           0.1651 |
[32m[20221213 15:38:18 @agent_ppo2.py:185][0m |          -0.0138 |          19.9722 |           0.1650 |
[32m[20221213 15:38:18 @agent_ppo2.py:185][0m |          -0.0142 |          19.8932 |           0.1646 |
[32m[20221213 15:38:18 @agent_ppo2.py:185][0m |          -0.0155 |          19.8038 |           0.1649 |
[32m[20221213 15:38:18 @agent_ppo2.py:185][0m |          -0.0181 |          19.7160 |           0.1644 |
[32m[20221213 15:38:18 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 15:38:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.60
[32m[20221213 15:38:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 284.06
[32m[20221213 15:38:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 293.20
[32m[20221213 15:38:18 @agent_ppo2.py:143][0m Total time:      45.38 min
[32m[20221213 15:38:18 @agent_ppo2.py:145][0m 4071424 total steps have happened
[32m[20221213 15:38:18 @agent_ppo2.py:121][0m #------------------------ Iteration 1988 --------------------------#
[32m[20221213 15:38:18 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 15:38:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |           0.0056 |          22.4620 |           0.1628 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0068 |          21.8196 |           0.1629 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0104 |          21.6875 |           0.1630 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0087 |          21.6389 |           0.1629 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0112 |          21.5326 |           0.1627 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0126 |          21.4330 |           0.1630 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0140 |          21.3539 |           0.1628 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0142 |          21.3273 |           0.1628 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0165 |          21.2831 |           0.1629 |
[32m[20221213 15:38:19 @agent_ppo2.py:185][0m |          -0.0169 |          21.2371 |           0.1629 |
[32m[20221213 15:38:19 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 15:38:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.97
[32m[20221213 15:38:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.83
[32m[20221213 15:38:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.29
[32m[20221213 15:38:20 @agent_ppo2.py:143][0m Total time:      45.41 min
[32m[20221213 15:38:20 @agent_ppo2.py:145][0m 4073472 total steps have happened
[32m[20221213 15:38:20 @agent_ppo2.py:121][0m #------------------------ Iteration 1989 --------------------------#
[32m[20221213 15:38:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221213 15:38:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:20 @agent_ppo2.py:185][0m |          -0.0014 |          22.0492 |           0.1697 |
[32m[20221213 15:38:20 @agent_ppo2.py:185][0m |          -0.0096 |          21.4353 |           0.1698 |
[32m[20221213 15:38:20 @agent_ppo2.py:185][0m |          -0.0049 |          21.4311 |           0.1697 |
[32m[20221213 15:38:20 @agent_ppo2.py:185][0m |          -0.0119 |          21.0300 |           0.1695 |
[32m[20221213 15:38:20 @agent_ppo2.py:185][0m |          -0.0065 |          21.5194 |           0.1696 |
[32m[20221213 15:38:20 @agent_ppo2.py:185][0m |          -0.0000 |          23.0711 |           0.1695 |
[32m[20221213 15:38:20 @agent_ppo2.py:185][0m |          -0.0156 |          20.7007 |           0.1695 |
[32m[20221213 15:38:21 @agent_ppo2.py:185][0m |          -0.0120 |          20.5821 |           0.1695 |
[32m[20221213 15:38:21 @agent_ppo2.py:185][0m |          -0.0155 |          20.5634 |           0.1698 |
[32m[20221213 15:38:21 @agent_ppo2.py:185][0m |          -0.0160 |          20.4798 |           0.1699 |
[32m[20221213 15:38:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:38:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 258.96
[32m[20221213 15:38:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 277.78
[32m[20221213 15:38:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.14
[32m[20221213 15:38:21 @agent_ppo2.py:143][0m Total time:      45.43 min
[32m[20221213 15:38:21 @agent_ppo2.py:145][0m 4075520 total steps have happened
[32m[20221213 15:38:21 @agent_ppo2.py:121][0m #------------------------ Iteration 1990 --------------------------#
[32m[20221213 15:38:21 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 15:38:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:21 @agent_ppo2.py:185][0m |           0.0002 |          22.2981 |           0.1672 |
[32m[20221213 15:38:21 @agent_ppo2.py:185][0m |          -0.0003 |          22.9385 |           0.1665 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |           0.0040 |          23.4490 |           0.1665 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |          -0.0102 |          21.8699 |           0.1666 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |          -0.0124 |          21.6867 |           0.1664 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |          -0.0133 |          21.6287 |           0.1665 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |          -0.0097 |          21.7078 |           0.1664 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |          -0.0141 |          21.5635 |           0.1665 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |          -0.0054 |          22.3142 |           0.1665 |
[32m[20221213 15:38:22 @agent_ppo2.py:185][0m |          -0.0139 |          21.4870 |           0.1663 |
[32m[20221213 15:38:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:38:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 284.23
[32m[20221213 15:38:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.78
[32m[20221213 15:38:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 304.08
[32m[20221213 15:38:22 @agent_ppo2.py:143][0m Total time:      45.45 min
[32m[20221213 15:38:22 @agent_ppo2.py:145][0m 4077568 total steps have happened
[32m[20221213 15:38:22 @agent_ppo2.py:121][0m #------------------------ Iteration 1991 --------------------------#
[32m[20221213 15:38:23 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 15:38:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0022 |          22.1087 |           0.1677 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0066 |          21.4130 |           0.1672 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0076 |          21.1728 |           0.1669 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0132 |          20.5604 |           0.1665 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0133 |          20.3584 |           0.1665 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0158 |          20.1943 |           0.1665 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0166 |          20.0415 |           0.1662 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0169 |          19.8746 |           0.1662 |
[32m[20221213 15:38:23 @agent_ppo2.py:185][0m |          -0.0171 |          19.7321 |           0.1659 |
[32m[20221213 15:38:24 @agent_ppo2.py:185][0m |          -0.0200 |          19.6487 |           0.1659 |
[32m[20221213 15:38:24 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:38:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 261.11
[32m[20221213 15:38:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 286.07
[32m[20221213 15:38:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 309.59
[32m[20221213 15:38:24 @agent_ppo2.py:143][0m Total time:      45.48 min
[32m[20221213 15:38:24 @agent_ppo2.py:145][0m 4079616 total steps have happened
[32m[20221213 15:38:24 @agent_ppo2.py:121][0m #------------------------ Iteration 1992 --------------------------#
[32m[20221213 15:38:24 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:24 @agent_ppo2.py:185][0m |          -0.0023 |          21.5263 |           0.1648 |
[32m[20221213 15:38:24 @agent_ppo2.py:185][0m |          -0.0047 |          20.8276 |           0.1645 |
[32m[20221213 15:38:24 @agent_ppo2.py:185][0m |          -0.0034 |          20.6950 |           0.1644 |
[32m[20221213 15:38:24 @agent_ppo2.py:185][0m |          -0.0111 |          20.0924 |           0.1644 |
[32m[20221213 15:38:25 @agent_ppo2.py:185][0m |          -0.0005 |          22.6567 |           0.1645 |
[32m[20221213 15:38:25 @agent_ppo2.py:185][0m |          -0.0120 |          19.8486 |           0.1643 |
[32m[20221213 15:38:25 @agent_ppo2.py:185][0m |          -0.0132 |          19.4816 |           0.1643 |
[32m[20221213 15:38:25 @agent_ppo2.py:185][0m |          -0.0149 |          19.3740 |           0.1644 |
[32m[20221213 15:38:25 @agent_ppo2.py:185][0m |          -0.0156 |          19.2254 |           0.1643 |
[32m[20221213 15:38:25 @agent_ppo2.py:185][0m |          -0.0167 |          19.1123 |           0.1644 |
[32m[20221213 15:38:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 15:38:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 253.55
[32m[20221213 15:38:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 262.61
[32m[20221213 15:38:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.24
[32m[20221213 15:38:25 @agent_ppo2.py:143][0m Total time:      45.50 min
[32m[20221213 15:38:25 @agent_ppo2.py:145][0m 4081664 total steps have happened
[32m[20221213 15:38:25 @agent_ppo2.py:121][0m #------------------------ Iteration 1993 --------------------------#
[32m[20221213 15:38:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0015 |          23.6431 |           0.1665 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0092 |          22.7242 |           0.1664 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0118 |          22.3337 |           0.1665 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0140 |          22.0172 |           0.1664 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0163 |          21.7425 |           0.1665 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0168 |          21.6476 |           0.1664 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0177 |          21.4480 |           0.1663 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0219 |          21.3297 |           0.1663 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0200 |          21.1802 |           0.1664 |
[32m[20221213 15:38:26 @agent_ppo2.py:185][0m |          -0.0215 |          21.0297 |           0.1665 |
[32m[20221213 15:38:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 15:38:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 267.05
[32m[20221213 15:38:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.00
[32m[20221213 15:38:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.24
[32m[20221213 15:38:27 @agent_ppo2.py:143][0m Total time:      45.52 min
[32m[20221213 15:38:27 @agent_ppo2.py:145][0m 4083712 total steps have happened
[32m[20221213 15:38:27 @agent_ppo2.py:121][0m #------------------------ Iteration 1994 --------------------------#
[32m[20221213 15:38:27 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:27 @agent_ppo2.py:185][0m |          -0.0062 |          23.4445 |           0.1717 |
[32m[20221213 15:38:27 @agent_ppo2.py:185][0m |          -0.0018 |          24.2103 |           0.1716 |
[32m[20221213 15:38:27 @agent_ppo2.py:185][0m |          -0.0111 |          22.2961 |           0.1715 |
[32m[20221213 15:38:27 @agent_ppo2.py:185][0m |          -0.0143 |          22.0371 |           0.1716 |
[32m[20221213 15:38:27 @agent_ppo2.py:185][0m |          -0.0140 |          22.0061 |           0.1715 |
[32m[20221213 15:38:28 @agent_ppo2.py:185][0m |          -0.0163 |          21.7725 |           0.1715 |
[32m[20221213 15:38:28 @agent_ppo2.py:185][0m |          -0.0135 |          21.6977 |           0.1714 |
[32m[20221213 15:38:28 @agent_ppo2.py:185][0m |          -0.0176 |          21.6372 |           0.1715 |
[32m[20221213 15:38:28 @agent_ppo2.py:185][0m |          -0.0168 |          21.4926 |           0.1716 |
[32m[20221213 15:38:28 @agent_ppo2.py:185][0m |          -0.0223 |          21.4153 |           0.1716 |
[32m[20221213 15:38:28 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 15:38:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 257.59
[32m[20221213 15:38:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 287.11
[32m[20221213 15:38:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 307.55
[32m[20221213 15:38:28 @agent_ppo2.py:143][0m Total time:      45.55 min
[32m[20221213 15:38:28 @agent_ppo2.py:145][0m 4085760 total steps have happened
[32m[20221213 15:38:28 @agent_ppo2.py:121][0m #------------------------ Iteration 1995 --------------------------#
[32m[20221213 15:38:28 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:28 @agent_ppo2.py:185][0m |           0.0046 |          21.8958 |           0.1705 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0053 |          21.6453 |           0.1700 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0070 |          21.5628 |           0.1698 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0124 |          21.4624 |           0.1697 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0127 |          21.3857 |           0.1696 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0134 |          21.3506 |           0.1696 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0065 |          22.6403 |           0.1697 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0148 |          21.2637 |           0.1695 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0127 |          21.2820 |           0.1695 |
[32m[20221213 15:38:29 @agent_ppo2.py:185][0m |          -0.0172 |          21.1973 |           0.1694 |
[32m[20221213 15:38:29 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 15:38:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 283.26
[32m[20221213 15:38:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 294.53
[32m[20221213 15:38:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.93
[32m[20221213 15:38:30 @agent_ppo2.py:143][0m Total time:      45.57 min
[32m[20221213 15:38:30 @agent_ppo2.py:145][0m 4087808 total steps have happened
[32m[20221213 15:38:30 @agent_ppo2.py:121][0m #------------------------ Iteration 1996 --------------------------#
[32m[20221213 15:38:30 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:30 @agent_ppo2.py:185][0m |           0.0013 |          20.6485 |           0.1687 |
[32m[20221213 15:38:30 @agent_ppo2.py:185][0m |          -0.0081 |          20.0765 |           0.1687 |
[32m[20221213 15:38:30 @agent_ppo2.py:185][0m |          -0.0064 |          19.8318 |           0.1684 |
[32m[20221213 15:38:30 @agent_ppo2.py:185][0m |          -0.0149 |          19.5067 |           0.1683 |
[32m[20221213 15:38:30 @agent_ppo2.py:185][0m |          -0.0145 |          19.2971 |           0.1681 |
[32m[20221213 15:38:30 @agent_ppo2.py:185][0m |          -0.0174 |          19.1204 |           0.1681 |
[32m[20221213 15:38:30 @agent_ppo2.py:185][0m |          -0.0155 |          19.0061 |           0.1680 |
[32m[20221213 15:38:31 @agent_ppo2.py:185][0m |          -0.0193 |          18.8041 |           0.1677 |
[32m[20221213 15:38:31 @agent_ppo2.py:185][0m |          -0.0152 |          18.7473 |           0.1677 |
[32m[20221213 15:38:31 @agent_ppo2.py:185][0m |          -0.0203 |          18.5201 |           0.1674 |
[32m[20221213 15:38:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:38:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 252.89
[32m[20221213 15:38:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 273.50
[32m[20221213 15:38:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.64
[32m[20221213 15:38:31 @agent_ppo2.py:143][0m Total time:      45.60 min
[32m[20221213 15:38:31 @agent_ppo2.py:145][0m 4089856 total steps have happened
[32m[20221213 15:38:31 @agent_ppo2.py:121][0m #------------------------ Iteration 1997 --------------------------#
[32m[20221213 15:38:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:31 @agent_ppo2.py:185][0m |           0.0003 |          22.2755 |           0.1692 |
[32m[20221213 15:38:31 @agent_ppo2.py:185][0m |          -0.0063 |          21.3543 |           0.1689 |
[32m[20221213 15:38:31 @agent_ppo2.py:185][0m |          -0.0127 |          20.9556 |           0.1690 |
[32m[20221213 15:38:32 @agent_ppo2.py:185][0m |          -0.0015 |          21.8942 |           0.1688 |
[32m[20221213 15:38:32 @agent_ppo2.py:185][0m |          -0.0150 |          20.4521 |           0.1686 |
[32m[20221213 15:38:32 @agent_ppo2.py:185][0m |          -0.0051 |          22.3178 |           0.1683 |
[32m[20221213 15:38:32 @agent_ppo2.py:185][0m |          -0.0162 |          20.0587 |           0.1677 |
[32m[20221213 15:38:32 @agent_ppo2.py:185][0m |          -0.0200 |          19.9187 |           0.1680 |
[32m[20221213 15:38:32 @agent_ppo2.py:185][0m |          -0.0197 |          19.8220 |           0.1680 |
[32m[20221213 15:38:32 @agent_ppo2.py:185][0m |          -0.0206 |          19.6473 |           0.1680 |
[32m[20221213 15:38:32 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 15:38:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 266.80
[32m[20221213 15:38:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 288.33
[32m[20221213 15:38:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 306.19
[32m[20221213 15:38:32 @agent_ppo2.py:143][0m Total time:      45.62 min
[32m[20221213 15:38:32 @agent_ppo2.py:145][0m 4091904 total steps have happened
[32m[20221213 15:38:32 @agent_ppo2.py:121][0m #------------------------ Iteration 1998 --------------------------#
[32m[20221213 15:38:32 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0012 |          22.8303 |           0.1687 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0069 |          22.4996 |           0.1682 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0075 |          22.3867 |           0.1679 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0113 |          22.2075 |           0.1679 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0147 |          22.1168 |           0.1681 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0128 |          21.9998 |           0.1681 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0148 |          21.9620 |           0.1680 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0087 |          22.9067 |           0.1680 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0141 |          21.8889 |           0.1680 |
[32m[20221213 15:38:33 @agent_ppo2.py:185][0m |          -0.0173 |          21.7841 |           0.1680 |
[32m[20221213 15:38:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:38:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 281.64
[32m[20221213 15:38:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 290.98
[32m[20221213 15:38:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 311.03
[32m[20221213 15:38:34 @agent_ppo2.py:143][0m Total time:      45.64 min
[32m[20221213 15:38:34 @agent_ppo2.py:145][0m 4093952 total steps have happened
[32m[20221213 15:38:34 @agent_ppo2.py:121][0m #------------------------ Iteration 1999 --------------------------#
[32m[20221213 15:38:34 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 15:38:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 15:38:34 @agent_ppo2.py:185][0m |          -0.0018 |          20.4381 |           0.1648 |
[32m[20221213 15:38:34 @agent_ppo2.py:185][0m |          -0.0074 |          19.6465 |           0.1648 |
[32m[20221213 15:38:34 @agent_ppo2.py:185][0m |          -0.0090 |          19.3650 |           0.1647 |
[32m[20221213 15:38:34 @agent_ppo2.py:185][0m |          -0.0118 |          19.1553 |           0.1645 |
[32m[20221213 15:38:34 @agent_ppo2.py:185][0m |          -0.0129 |          19.0038 |           0.1643 |
[32m[20221213 15:38:34 @agent_ppo2.py:185][0m |          -0.0114 |          18.9247 |           0.1641 |
[32m[20221213 15:38:34 @agent_ppo2.py:185][0m |          -0.0041 |          20.5395 |           0.1639 |
[32m[20221213 15:38:35 @agent_ppo2.py:185][0m |          -0.0129 |          18.8661 |           0.1638 |
[32m[20221213 15:38:35 @agent_ppo2.py:185][0m |          -0.0188 |          18.6650 |           0.1637 |
[32m[20221213 15:38:35 @agent_ppo2.py:185][0m |          -0.0080 |          19.6912 |           0.1638 |
[32m[20221213 15:38:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 15:38:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 260.75
[32m[20221213 15:38:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 276.76
[32m[20221213 15:38:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.63
[32m[20221213 15:38:35 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 324.22
[32m[20221213 15:38:35 @agent_ppo2.py:143][0m Total time:      45.66 min
[32m[20221213 15:38:35 @agent_ppo2.py:145][0m 4096000 total steps have happened
[32m[20221213 15:38:35 @train.py:58][0m [4m[34mCRITICAL[0m Training completed!
