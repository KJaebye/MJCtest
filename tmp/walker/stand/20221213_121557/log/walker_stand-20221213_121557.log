[32m[20221213 12:15:57 @logger.py:105][0m Log file set to ./tmp/walker/stand/20221213_121557/log/walker_stand-20221213_121557.log
[32m[20221213 12:15:57 @agent_ppo2.py:115][0m #------------------------ Iteration 0 --------------------------#
[32m[20221213 12:15:57 @agent_ppo2.py:121][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 12:15:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:15:57 @agent_ppo2.py:179][0m |          -0.0022 |           0.1616 |           0.2196 |
[32m[20221213 12:15:57 @agent_ppo2.py:179][0m |          -0.0062 |           0.0901 |           0.2182 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0201 |           0.0771 |           0.2178 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0124 |           0.0659 |           0.2178 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0136 |           0.0577 |           0.2172 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0185 |           0.0514 |           0.2169 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0334 |           0.0465 |           0.2172 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0187 |           0.0422 |           0.2174 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0164 |           0.0388 |           0.2172 |
[32m[20221213 12:15:58 @agent_ppo2.py:179][0m |          -0.0163 |           0.0362 |           0.2170 |
[32m[20221213 12:15:58 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:15:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.23
[32m[20221213 12:15:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 26.10
[32m[20221213 12:15:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.94
[32m[20221213 12:15:59 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 23.94
[32m[20221213 12:15:59 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 23.94
[32m[20221213 12:15:59 @agent_ppo2.py:137][0m Total time:       0.03 min
[32m[20221213 12:15:59 @agent_ppo2.py:139][0m 2048 total steps have happened
[32m[20221213 12:15:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1 --------------------------#
[32m[20221213 12:15:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:15:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:15:59 @agent_ppo2.py:179][0m |          -0.0039 |           0.0439 |           0.2209 |
[32m[20221213 12:15:59 @agent_ppo2.py:179][0m |          -0.0058 |           0.0278 |           0.2213 |
[32m[20221213 12:15:59 @agent_ppo2.py:179][0m |          -0.0076 |           0.0261 |           0.2212 |
[32m[20221213 12:15:59 @agent_ppo2.py:179][0m |          -0.0131 |           0.0247 |           0.2218 |
[32m[20221213 12:16:00 @agent_ppo2.py:179][0m |          -0.0256 |           0.0241 |           0.2221 |
[32m[20221213 12:16:00 @agent_ppo2.py:179][0m |          -0.0096 |           0.0227 |           0.2217 |
[32m[20221213 12:16:00 @agent_ppo2.py:179][0m |          -0.0100 |           0.0216 |           0.2218 |
[32m[20221213 12:16:00 @agent_ppo2.py:179][0m |          -0.0095 |           0.0209 |           0.2218 |
[32m[20221213 12:16:00 @agent_ppo2.py:179][0m |          -0.0170 |           0.0203 |           0.2228 |
[32m[20221213 12:16:00 @agent_ppo2.py:179][0m |          -0.0176 |           0.0199 |           0.2233 |
[32m[20221213 12:16:00 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:16:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.57
[32m[20221213 12:16:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 28.47
[32m[20221213 12:16:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 26.13
[32m[20221213 12:16:00 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 26.13
[32m[20221213 12:16:00 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 26.13
[32m[20221213 12:16:00 @agent_ppo2.py:137][0m Total time:       0.06 min
[32m[20221213 12:16:00 @agent_ppo2.py:139][0m 4096 total steps have happened
[32m[20221213 12:16:00 @agent_ppo2.py:115][0m #------------------------ Iteration 2 --------------------------#
[32m[20221213 12:16:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:01 @agent_ppo2.py:179][0m |          -0.0088 |           0.0494 |           0.2361 |
[32m[20221213 12:16:01 @agent_ppo2.py:179][0m |          -0.0047 |           0.0344 |           0.2362 |
[32m[20221213 12:16:01 @agent_ppo2.py:179][0m |          -0.0096 |           0.0325 |           0.2352 |
[32m[20221213 12:16:01 @agent_ppo2.py:179][0m |          -0.0105 |           0.0320 |           0.2345 |
[32m[20221213 12:16:01 @agent_ppo2.py:179][0m |          -0.0146 |           0.0312 |           0.2348 |
[32m[20221213 12:16:01 @agent_ppo2.py:179][0m |          -0.0149 |           0.0304 |           0.2352 |
[32m[20221213 12:16:02 @agent_ppo2.py:179][0m |          -0.0264 |           0.0309 |           0.2349 |
[32m[20221213 12:16:02 @agent_ppo2.py:179][0m |          -0.0182 |           0.0298 |           0.2351 |
[32m[20221213 12:16:02 @agent_ppo2.py:179][0m |          -0.0212 |           0.0293 |           0.2352 |
[32m[20221213 12:16:02 @agent_ppo2.py:179][0m |          -0.0247 |           0.0291 |           0.2352 |
[32m[20221213 12:16:02 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:16:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.32
[32m[20221213 12:16:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 26.22
[32m[20221213 12:16:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 26.40
[32m[20221213 12:16:02 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 26.40
[32m[20221213 12:16:02 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 26.40
[32m[20221213 12:16:02 @agent_ppo2.py:137][0m Total time:       0.09 min
[32m[20221213 12:16:02 @agent_ppo2.py:139][0m 6144 total steps have happened
[32m[20221213 12:16:02 @agent_ppo2.py:115][0m #------------------------ Iteration 3 --------------------------#
[32m[20221213 12:16:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0012 |           0.0493 |           0.2288 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0086 |           0.0397 |           0.2297 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0273 |           0.0399 |           0.2298 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0113 |           0.0394 |           0.2308 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0168 |           0.0385 |           0.2315 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0118 |           0.0406 |           0.2317 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0152 |           0.0386 |           0.2317 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0226 |           0.0378 |           0.2322 |
[32m[20221213 12:16:03 @agent_ppo2.py:179][0m |          -0.0240 |           0.0374 |           0.2325 |
[32m[20221213 12:16:04 @agent_ppo2.py:179][0m |          -0.0246 |           0.0373 |           0.2330 |
[32m[20221213 12:16:04 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:16:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.68
[32m[20221213 12:16:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 26.28
[32m[20221213 12:16:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.77
[32m[20221213 12:16:04 @agent_ppo2.py:137][0m Total time:       0.12 min
[32m[20221213 12:16:04 @agent_ppo2.py:139][0m 8192 total steps have happened
[32m[20221213 12:16:04 @agent_ppo2.py:115][0m #------------------------ Iteration 4 --------------------------#
[32m[20221213 12:16:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:04 @agent_ppo2.py:179][0m |           0.0021 |           0.0554 |           0.2411 |
[32m[20221213 12:16:04 @agent_ppo2.py:179][0m |          -0.0086 |           0.0482 |           0.2407 |
[32m[20221213 12:16:04 @agent_ppo2.py:179][0m |          -0.0130 |           0.0483 |           0.2409 |
[32m[20221213 12:16:05 @agent_ppo2.py:179][0m |          -0.0119 |           0.0479 |           0.2412 |
[32m[20221213 12:16:05 @agent_ppo2.py:179][0m |          -0.0151 |           0.0472 |           0.2413 |
[32m[20221213 12:16:05 @agent_ppo2.py:179][0m |          -0.0108 |           0.0470 |           0.2406 |
[32m[20221213 12:16:05 @agent_ppo2.py:179][0m |          -0.0146 |           0.0462 |           0.2414 |
[32m[20221213 12:16:05 @agent_ppo2.py:179][0m |          -0.0067 |           0.0476 |           0.2416 |
[32m[20221213 12:16:05 @agent_ppo2.py:179][0m |          -0.0136 |           0.0457 |           0.2410 |
[32m[20221213 12:16:05 @agent_ppo2.py:179][0m |          -0.0231 |           0.0460 |           0.2416 |
[32m[20221213 12:16:05 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:16:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 25.51
[32m[20221213 12:16:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 27.21
[32m[20221213 12:16:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.06
[32m[20221213 12:16:05 @agent_ppo2.py:137][0m Total time:       0.15 min
[32m[20221213 12:16:05 @agent_ppo2.py:139][0m 10240 total steps have happened
[32m[20221213 12:16:05 @agent_ppo2.py:115][0m #------------------------ Iteration 5 --------------------------#
[32m[20221213 12:16:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:06 @agent_ppo2.py:179][0m |          -0.0006 |           0.0815 |           0.2422 |
[32m[20221213 12:16:06 @agent_ppo2.py:179][0m |          -0.0086 |           0.0717 |           0.2424 |
[32m[20221213 12:16:06 @agent_ppo2.py:179][0m |          -0.0156 |           0.0678 |           0.2423 |
[32m[20221213 12:16:06 @agent_ppo2.py:179][0m |          -0.0143 |           0.0647 |           0.2432 |
[32m[20221213 12:16:06 @agent_ppo2.py:179][0m |          -0.0173 |           0.0644 |           0.2431 |
[32m[20221213 12:16:06 @agent_ppo2.py:179][0m |          -0.0225 |           0.0654 |           0.2434 |
[32m[20221213 12:16:07 @agent_ppo2.py:179][0m |          -0.0116 |           0.0664 |           0.2433 |
[32m[20221213 12:16:07 @agent_ppo2.py:179][0m |          -0.0130 |           0.0633 |           0.2441 |
[32m[20221213 12:16:07 @agent_ppo2.py:179][0m |          -0.0258 |           0.0642 |           0.2440 |
[32m[20221213 12:16:07 @agent_ppo2.py:179][0m |          -0.0161 |           0.0630 |           0.2445 |
[32m[20221213 12:16:07 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:16:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 27.50
[32m[20221213 12:16:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.60
[32m[20221213 12:16:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.79
[32m[20221213 12:16:07 @agent_ppo2.py:137][0m Total time:       0.17 min
[32m[20221213 12:16:07 @agent_ppo2.py:139][0m 12288 total steps have happened
[32m[20221213 12:16:07 @agent_ppo2.py:115][0m #------------------------ Iteration 6 --------------------------#
[32m[20221213 12:16:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0002 |           0.1161 |           0.2483 |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0068 |           0.1039 |           0.2470 |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0077 |           0.1018 |           0.2472 |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0166 |           0.1003 |           0.2470 |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0072 |           0.1051 |           0.2467 |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0163 |           0.0989 |           0.2464 |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0246 |           0.1002 |           0.2471 |
[32m[20221213 12:16:08 @agent_ppo2.py:179][0m |          -0.0107 |           0.0974 |           0.2471 |
[32m[20221213 12:16:09 @agent_ppo2.py:179][0m |          -0.0164 |           0.0957 |           0.2469 |
[32m[20221213 12:16:09 @agent_ppo2.py:179][0m |          -0.0282 |           0.0955 |           0.2467 |
[32m[20221213 12:16:09 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:16:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 30.35
[32m[20221213 12:16:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 35.03
[32m[20221213 12:16:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.62
[32m[20221213 12:16:09 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 30.62
[32m[20221213 12:16:09 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 30.62
[32m[20221213 12:16:09 @agent_ppo2.py:137][0m Total time:       0.20 min
[32m[20221213 12:16:09 @agent_ppo2.py:139][0m 14336 total steps have happened
[32m[20221213 12:16:09 @agent_ppo2.py:115][0m #------------------------ Iteration 7 --------------------------#
[32m[20221213 12:16:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:09 @agent_ppo2.py:179][0m |           0.0081 |           0.1472 |           0.2489 |
[32m[20221213 12:16:09 @agent_ppo2.py:179][0m |          -0.0134 |           0.1383 |           0.2482 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0141 |           0.1323 |           0.2486 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0161 |           0.1275 |           0.2495 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0193 |           0.1253 |           0.2500 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0213 |           0.1258 |           0.2501 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0139 |           0.1224 |           0.2506 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0194 |           0.1200 |           0.2506 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0191 |           0.1188 |           0.2514 |
[32m[20221213 12:16:10 @agent_ppo2.py:179][0m |          -0.0188 |           0.1178 |           0.2517 |
[32m[20221213 12:16:10 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:16:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 28.60
[32m[20221213 12:16:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 30.44
[32m[20221213 12:16:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.01
[32m[20221213 12:16:11 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 46.01
[32m[20221213 12:16:11 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 46.01
[32m[20221213 12:16:11 @agent_ppo2.py:137][0m Total time:       0.23 min
[32m[20221213 12:16:11 @agent_ppo2.py:139][0m 16384 total steps have happened
[32m[20221213 12:16:11 @agent_ppo2.py:115][0m #------------------------ Iteration 8 --------------------------#
[32m[20221213 12:16:11 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:16:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:11 @agent_ppo2.py:179][0m |           0.0041 |           0.2174 |           0.2623 |
[32m[20221213 12:16:11 @agent_ppo2.py:179][0m |          -0.0027 |           0.1984 |           0.2617 |
[32m[20221213 12:16:11 @agent_ppo2.py:179][0m |          -0.0096 |           0.1944 |           0.2612 |
[32m[20221213 12:16:11 @agent_ppo2.py:179][0m |          -0.0111 |           0.1961 |           0.2613 |
[32m[20221213 12:16:12 @agent_ppo2.py:179][0m |          -0.0139 |           0.1929 |           0.2616 |
[32m[20221213 12:16:12 @agent_ppo2.py:179][0m |          -0.0154 |           0.1907 |           0.2615 |
[32m[20221213 12:16:12 @agent_ppo2.py:179][0m |          -0.0176 |           0.1949 |           0.2613 |
[32m[20221213 12:16:12 @agent_ppo2.py:179][0m |          -0.0191 |           0.1898 |           0.2614 |
[32m[20221213 12:16:12 @agent_ppo2.py:179][0m |          -0.0241 |           0.1919 |           0.2614 |
[32m[20221213 12:16:12 @agent_ppo2.py:179][0m |          -0.0252 |           0.1938 |           0.2608 |
[32m[20221213 12:16:12 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:16:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 36.69
[32m[20221213 12:16:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 57.11
[32m[20221213 12:16:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.41
[32m[20221213 12:16:12 @agent_ppo2.py:137][0m Total time:       0.26 min
[32m[20221213 12:16:12 @agent_ppo2.py:139][0m 18432 total steps have happened
[32m[20221213 12:16:12 @agent_ppo2.py:115][0m #------------------------ Iteration 9 --------------------------#
[32m[20221213 12:16:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:13 @agent_ppo2.py:179][0m |           0.0038 |           0.1775 |           0.2640 |
[32m[20221213 12:16:13 @agent_ppo2.py:179][0m |          -0.0091 |           0.1622 |           0.2636 |
[32m[20221213 12:16:13 @agent_ppo2.py:179][0m |          -0.0060 |           0.1714 |           0.2631 |
[32m[20221213 12:16:13 @agent_ppo2.py:179][0m |          -0.0118 |           0.1600 |           0.2622 |
[32m[20221213 12:16:13 @agent_ppo2.py:179][0m |          -0.0173 |           0.1567 |           0.2625 |
[32m[20221213 12:16:13 @agent_ppo2.py:179][0m |          -0.0254 |           0.1573 |           0.2628 |
[32m[20221213 12:16:14 @agent_ppo2.py:179][0m |          -0.0193 |           0.1555 |           0.2628 |
[32m[20221213 12:16:14 @agent_ppo2.py:179][0m |          -0.0183 |           0.1549 |           0.2631 |
[32m[20221213 12:16:14 @agent_ppo2.py:179][0m |          -0.0228 |           0.1534 |           0.2634 |
[32m[20221213 12:16:14 @agent_ppo2.py:179][0m |          -0.0199 |           0.1526 |           0.2630 |
[32m[20221213 12:16:14 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:16:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 30.96
[32m[20221213 12:16:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 34.44
[32m[20221213 12:16:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.29
[32m[20221213 12:16:14 @agent_ppo2.py:137][0m Total time:       0.29 min
[32m[20221213 12:16:14 @agent_ppo2.py:139][0m 20480 total steps have happened
[32m[20221213 12:16:14 @agent_ppo2.py:115][0m #------------------------ Iteration 10 --------------------------#
[32m[20221213 12:16:14 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:16:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |           0.0032 |           0.2501 |           0.2678 |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |           0.0007 |           0.2247 |           0.2686 |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |          -0.0069 |           0.2247 |           0.2679 |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |          -0.0061 |           0.2185 |           0.2682 |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |          -0.0086 |           0.2260 |           0.2692 |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |          -0.0084 |           0.2182 |           0.2699 |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |          -0.0211 |           0.2172 |           0.2700 |
[32m[20221213 12:16:15 @agent_ppo2.py:179][0m |          -0.0161 |           0.2165 |           0.2698 |
[32m[20221213 12:16:16 @agent_ppo2.py:179][0m |          -0.0207 |           0.2145 |           0.2701 |
[32m[20221213 12:16:16 @agent_ppo2.py:179][0m |          -0.0222 |           0.2118 |           0.2707 |
[32m[20221213 12:16:16 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:16:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.95
[32m[20221213 12:16:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 55.43
[32m[20221213 12:16:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.28
[32m[20221213 12:16:16 @agent_ppo2.py:137][0m Total time:       0.32 min
[32m[20221213 12:16:16 @agent_ppo2.py:139][0m 22528 total steps have happened
[32m[20221213 12:16:16 @agent_ppo2.py:115][0m #------------------------ Iteration 11 --------------------------#
[32m[20221213 12:16:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:16 @agent_ppo2.py:179][0m |          -0.0119 |           0.1271 |           0.2711 |
[32m[20221213 12:16:16 @agent_ppo2.py:179][0m |          -0.0056 |           0.1136 |           0.2722 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0135 |           0.1108 |           0.2736 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0124 |           0.1094 |           0.2741 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0195 |           0.1093 |           0.2747 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0214 |           0.1076 |           0.2741 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0221 |           0.1057 |           0.2754 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0222 |           0.1051 |           0.2751 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0230 |           0.1060 |           0.2750 |
[32m[20221213 12:16:17 @agent_ppo2.py:179][0m |          -0.0260 |           0.1049 |           0.2740 |
[32m[20221213 12:16:17 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:16:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 36.66
[32m[20221213 12:16:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 41.14
[32m[20221213 12:16:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.68
[32m[20221213 12:16:18 @agent_ppo2.py:137][0m Total time:       0.35 min
[32m[20221213 12:16:18 @agent_ppo2.py:139][0m 24576 total steps have happened
[32m[20221213 12:16:18 @agent_ppo2.py:115][0m #------------------------ Iteration 12 --------------------------#
[32m[20221213 12:16:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:18 @agent_ppo2.py:179][0m |          -0.0031 |           0.2166 |           0.2823 |
[32m[20221213 12:16:18 @agent_ppo2.py:179][0m |          -0.0058 |           0.2028 |           0.2831 |
[32m[20221213 12:16:18 @agent_ppo2.py:179][0m |          -0.0119 |           0.1992 |           0.2833 |
[32m[20221213 12:16:18 @agent_ppo2.py:179][0m |          -0.0144 |           0.1964 |           0.2845 |
[32m[20221213 12:16:19 @agent_ppo2.py:179][0m |          -0.0198 |           0.2005 |           0.2848 |
[32m[20221213 12:16:19 @agent_ppo2.py:179][0m |          -0.0160 |           0.1956 |           0.2847 |
[32m[20221213 12:16:19 @agent_ppo2.py:179][0m |          -0.0239 |           0.1952 |           0.2849 |
[32m[20221213 12:16:19 @agent_ppo2.py:179][0m |          -0.0255 |           0.1936 |           0.2851 |
[32m[20221213 12:16:19 @agent_ppo2.py:179][0m |          -0.0204 |           0.1920 |           0.2852 |
[32m[20221213 12:16:19 @agent_ppo2.py:179][0m |          -0.0255 |           0.1917 |           0.2853 |
[32m[20221213 12:16:19 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:16:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.97
[32m[20221213 12:16:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.11
[32m[20221213 12:16:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.71
[32m[20221213 12:16:19 @agent_ppo2.py:137][0m Total time:       0.38 min
[32m[20221213 12:16:19 @agent_ppo2.py:139][0m 26624 total steps have happened
[32m[20221213 12:16:19 @agent_ppo2.py:115][0m #------------------------ Iteration 13 --------------------------#
[32m[20221213 12:16:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:20 @agent_ppo2.py:179][0m |          -0.0035 |           0.2735 |           0.2857 |
[32m[20221213 12:16:20 @agent_ppo2.py:179][0m |          -0.0017 |           0.2658 |           0.2866 |
[32m[20221213 12:16:20 @agent_ppo2.py:179][0m |          -0.0080 |           0.2566 |           0.2868 |
[32m[20221213 12:16:20 @agent_ppo2.py:179][0m |          -0.0167 |           0.2485 |           0.2861 |
[32m[20221213 12:16:20 @agent_ppo2.py:179][0m |          -0.0187 |           0.2455 |           0.2868 |
[32m[20221213 12:16:20 @agent_ppo2.py:179][0m |          -0.0194 |           0.2446 |           0.2874 |
[32m[20221213 12:16:20 @agent_ppo2.py:179][0m |          -0.0227 |           0.2448 |           0.2877 |
[32m[20221213 12:16:21 @agent_ppo2.py:179][0m |          -0.0235 |           0.2425 |           0.2882 |
[32m[20221213 12:16:21 @agent_ppo2.py:179][0m |          -0.0243 |           0.2397 |           0.2877 |
[32m[20221213 12:16:21 @agent_ppo2.py:179][0m |          -0.0267 |           0.2384 |           0.2881 |
[32m[20221213 12:16:21 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:16:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.66
[32m[20221213 12:16:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 53.18
[32m[20221213 12:16:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.04
[32m[20221213 12:16:21 @agent_ppo2.py:137][0m Total time:       0.41 min
[32m[20221213 12:16:21 @agent_ppo2.py:139][0m 28672 total steps have happened
[32m[20221213 12:16:21 @agent_ppo2.py:115][0m #------------------------ Iteration 14 --------------------------#
[32m[20221213 12:16:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:21 @agent_ppo2.py:179][0m |           0.0119 |           0.2177 |           0.2954 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0088 |           0.1800 |           0.2946 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0106 |           0.1771 |           0.2955 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0149 |           0.1790 |           0.2961 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0151 |           0.1732 |           0.2968 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0161 |           0.1753 |           0.2982 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0124 |           0.1744 |           0.2980 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0220 |           0.1734 |           0.2975 |
[32m[20221213 12:16:22 @agent_ppo2.py:179][0m |          -0.0186 |           0.1723 |           0.2979 |
[32m[20221213 12:16:23 @agent_ppo2.py:179][0m |          -0.0215 |           0.1732 |           0.2978 |
[32m[20221213 12:16:23 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:16:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 41.40
[32m[20221213 12:16:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.54
[32m[20221213 12:16:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.34
[32m[20221213 12:16:23 @agent_ppo2.py:137][0m Total time:       0.43 min
[32m[20221213 12:16:23 @agent_ppo2.py:139][0m 30720 total steps have happened
[32m[20221213 12:16:23 @agent_ppo2.py:115][0m #------------------------ Iteration 15 --------------------------#
[32m[20221213 12:16:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:23 @agent_ppo2.py:179][0m |           0.0117 |           0.2548 |           0.3044 |
[32m[20221213 12:16:23 @agent_ppo2.py:179][0m |          -0.0045 |           0.2259 |           0.3054 |
[32m[20221213 12:16:23 @agent_ppo2.py:179][0m |          -0.0119 |           0.2209 |           0.3071 |
[32m[20221213 12:16:23 @agent_ppo2.py:179][0m |           0.0042 |           0.2244 |           0.3076 |
[32m[20221213 12:16:24 @agent_ppo2.py:179][0m |          -0.0074 |           0.2188 |           0.3069 |
[32m[20221213 12:16:24 @agent_ppo2.py:179][0m |           0.0000 |           0.2331 |           0.3080 |
[32m[20221213 12:16:24 @agent_ppo2.py:179][0m |          -0.0057 |           0.2173 |           0.3041 |
[32m[20221213 12:16:24 @agent_ppo2.py:179][0m |          -0.0127 |           0.2144 |           0.3076 |
[32m[20221213 12:16:24 @agent_ppo2.py:179][0m |          -0.0175 |           0.2139 |           0.3074 |
[32m[20221213 12:16:24 @agent_ppo2.py:179][0m |          -0.0180 |           0.2131 |           0.3077 |
[32m[20221213 12:16:24 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:16:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.84
[32m[20221213 12:16:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.29
[32m[20221213 12:16:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.24
[32m[20221213 12:16:24 @agent_ppo2.py:137][0m Total time:       0.46 min
[32m[20221213 12:16:24 @agent_ppo2.py:139][0m 32768 total steps have happened
[32m[20221213 12:16:24 @agent_ppo2.py:115][0m #------------------------ Iteration 16 --------------------------#
[32m[20221213 12:16:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:25 @agent_ppo2.py:179][0m |          -0.0026 |           0.2613 |           0.3092 |
[32m[20221213 12:16:25 @agent_ppo2.py:179][0m |          -0.0120 |           0.2331 |           0.3091 |
[32m[20221213 12:16:25 @agent_ppo2.py:179][0m |          -0.0125 |           0.2277 |           0.3089 |
[32m[20221213 12:16:25 @agent_ppo2.py:179][0m |          -0.0124 |           0.2269 |           0.3092 |
[32m[20221213 12:16:25 @agent_ppo2.py:179][0m |          -0.0101 |           0.2278 |           0.3103 |
[32m[20221213 12:16:25 @agent_ppo2.py:179][0m |          -0.0074 |           0.2432 |           0.3098 |
[32m[20221213 12:16:25 @agent_ppo2.py:179][0m |          -0.0187 |           0.2239 |           0.3079 |
[32m[20221213 12:16:26 @agent_ppo2.py:179][0m |          -0.0228 |           0.2234 |           0.3092 |
[32m[20221213 12:16:26 @agent_ppo2.py:179][0m |          -0.0180 |           0.2254 |           0.3106 |
[32m[20221213 12:16:26 @agent_ppo2.py:179][0m |          -0.0212 |           0.2199 |           0.3098 |
[32m[20221213 12:16:26 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:16:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.50
[32m[20221213 12:16:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.35
[32m[20221213 12:16:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.86
[32m[20221213 12:16:26 @agent_ppo2.py:137][0m Total time:       0.49 min
[32m[20221213 12:16:26 @agent_ppo2.py:139][0m 34816 total steps have happened
[32m[20221213 12:16:26 @agent_ppo2.py:115][0m #------------------------ Iteration 17 --------------------------#
[32m[20221213 12:16:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:26 @agent_ppo2.py:179][0m |           0.0021 |           0.2410 |           0.3145 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0053 |           0.2304 |           0.3154 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0119 |           0.2290 |           0.3162 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0122 |           0.2291 |           0.3172 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0043 |           0.2340 |           0.3180 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0087 |           0.2300 |           0.3182 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0131 |           0.2282 |           0.3190 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0139 |           0.2270 |           0.3194 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0150 |           0.2262 |           0.3199 |
[32m[20221213 12:16:27 @agent_ppo2.py:179][0m |          -0.0135 |           0.2271 |           0.3215 |
[32m[20221213 12:16:27 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:16:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.48
[32m[20221213 12:16:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.18
[32m[20221213 12:16:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.79
[32m[20221213 12:16:28 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 59.79
[32m[20221213 12:16:28 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 59.79
[32m[20221213 12:16:28 @agent_ppo2.py:137][0m Total time:       0.52 min
[32m[20221213 12:16:28 @agent_ppo2.py:139][0m 36864 total steps have happened
[32m[20221213 12:16:28 @agent_ppo2.py:115][0m #------------------------ Iteration 18 --------------------------#
[32m[20221213 12:16:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:28 @agent_ppo2.py:179][0m |           0.0123 |           0.3565 |           0.3282 |
[32m[20221213 12:16:28 @agent_ppo2.py:179][0m |          -0.0058 |           0.3110 |           0.3286 |
[32m[20221213 12:16:28 @agent_ppo2.py:179][0m |          -0.0105 |           0.2979 |           0.3296 |
[32m[20221213 12:16:28 @agent_ppo2.py:179][0m |          -0.0153 |           0.2954 |           0.3298 |
[32m[20221213 12:16:29 @agent_ppo2.py:179][0m |          -0.0140 |           0.2928 |           0.3301 |
[32m[20221213 12:16:29 @agent_ppo2.py:179][0m |          -0.0225 |           0.2939 |           0.3309 |
[32m[20221213 12:16:29 @agent_ppo2.py:179][0m |          -0.0138 |           0.2898 |           0.3316 |
[32m[20221213 12:16:29 @agent_ppo2.py:179][0m |          -0.0169 |           0.2887 |           0.3314 |
[32m[20221213 12:16:29 @agent_ppo2.py:179][0m |          -0.0180 |           0.2880 |           0.3322 |
[32m[20221213 12:16:29 @agent_ppo2.py:179][0m |          -0.0168 |           0.2979 |           0.3326 |
[32m[20221213 12:16:29 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:16:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 50.84
[32m[20221213 12:16:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.70
[32m[20221213 12:16:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.03
[32m[20221213 12:16:29 @agent_ppo2.py:137][0m Total time:       0.54 min
[32m[20221213 12:16:29 @agent_ppo2.py:139][0m 38912 total steps have happened
[32m[20221213 12:16:29 @agent_ppo2.py:115][0m #------------------------ Iteration 19 --------------------------#
[32m[20221213 12:16:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:30 @agent_ppo2.py:179][0m |          -0.0016 |           0.2753 |           0.3448 |
[32m[20221213 12:16:30 @agent_ppo2.py:179][0m |          -0.0056 |           0.2620 |           0.3441 |
[32m[20221213 12:16:30 @agent_ppo2.py:179][0m |          -0.0069 |           0.2617 |           0.3427 |
[32m[20221213 12:16:30 @agent_ppo2.py:179][0m |          -0.0020 |           0.2679 |           0.3440 |
[32m[20221213 12:16:30 @agent_ppo2.py:179][0m |          -0.0053 |           0.2604 |           0.3411 |
[32m[20221213 12:16:30 @agent_ppo2.py:179][0m |          -0.0124 |           0.2579 |           0.3423 |
[32m[20221213 12:16:30 @agent_ppo2.py:179][0m |          -0.0114 |           0.2582 |           0.3420 |
[32m[20221213 12:16:31 @agent_ppo2.py:179][0m |          -0.0128 |           0.2581 |           0.3419 |
[32m[20221213 12:16:31 @agent_ppo2.py:179][0m |          -0.0144 |           0.2595 |           0.3409 |
[32m[20221213 12:16:31 @agent_ppo2.py:179][0m |          -0.0093 |           0.2582 |           0.3411 |
[32m[20221213 12:16:31 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:16:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.89
[32m[20221213 12:16:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.31
[32m[20221213 12:16:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.23
[32m[20221213 12:16:31 @agent_ppo2.py:137][0m Total time:       0.57 min
[32m[20221213 12:16:31 @agent_ppo2.py:139][0m 40960 total steps have happened
[32m[20221213 12:16:31 @agent_ppo2.py:115][0m #------------------------ Iteration 20 --------------------------#
[32m[20221213 12:16:31 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:16:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:31 @agent_ppo2.py:179][0m |          -0.0002 |           0.2908 |           0.3417 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |           0.0060 |           0.3114 |           0.3396 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |           0.0028 |           0.3191 |           0.3381 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |          -0.0025 |           0.2851 |           0.3349 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |          -0.0103 |           0.2843 |           0.3372 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |          -0.0124 |           0.2857 |           0.3366 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |          -0.0116 |           0.2826 |           0.3373 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |          -0.0143 |           0.2830 |           0.3359 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |          -0.0014 |           0.3153 |           0.3361 |
[32m[20221213 12:16:32 @agent_ppo2.py:179][0m |          -0.0108 |           0.2843 |           0.3348 |
[32m[20221213 12:16:32 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:16:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.27
[32m[20221213 12:16:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.74
[32m[20221213 12:16:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.91
[32m[20221213 12:16:33 @agent_ppo2.py:137][0m Total time:       0.60 min
[32m[20221213 12:16:33 @agent_ppo2.py:139][0m 43008 total steps have happened
[32m[20221213 12:16:33 @agent_ppo2.py:115][0m #------------------------ Iteration 21 --------------------------#
[32m[20221213 12:16:33 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:16:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:33 @agent_ppo2.py:179][0m |          -0.0058 |           0.3227 |           0.3263 |
[32m[20221213 12:16:33 @agent_ppo2.py:179][0m |          -0.0106 |           0.3110 |           0.3261 |
[32m[20221213 12:16:33 @agent_ppo2.py:179][0m |          -0.0127 |           0.3126 |           0.3251 |
[32m[20221213 12:16:33 @agent_ppo2.py:179][0m |          -0.0144 |           0.3106 |           0.3241 |
[32m[20221213 12:16:34 @agent_ppo2.py:179][0m |          -0.0159 |           0.3096 |           0.3233 |
[32m[20221213 12:16:34 @agent_ppo2.py:179][0m |          -0.0156 |           0.3101 |           0.3224 |
[32m[20221213 12:16:34 @agent_ppo2.py:179][0m |          -0.0183 |           0.3098 |           0.3223 |
[32m[20221213 12:16:34 @agent_ppo2.py:179][0m |          -0.0199 |           0.3120 |           0.3224 |
[32m[20221213 12:16:34 @agent_ppo2.py:179][0m |          -0.0172 |           0.3099 |           0.3208 |
[32m[20221213 12:16:34 @agent_ppo2.py:179][0m |          -0.0176 |           0.3107 |           0.3212 |
[32m[20221213 12:16:34 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:16:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.12
[32m[20221213 12:16:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.62
[32m[20221213 12:16:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 27.92
[32m[20221213 12:16:34 @agent_ppo2.py:137][0m Total time:       0.63 min
[32m[20221213 12:16:34 @agent_ppo2.py:139][0m 45056 total steps have happened
[32m[20221213 12:16:34 @agent_ppo2.py:115][0m #------------------------ Iteration 22 --------------------------#
[32m[20221213 12:16:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:35 @agent_ppo2.py:179][0m |           0.0058 |           0.5336 |           0.3175 |
[32m[20221213 12:16:35 @agent_ppo2.py:179][0m |          -0.0091 |           0.4957 |           0.3164 |
[32m[20221213 12:16:35 @agent_ppo2.py:179][0m |          -0.0131 |           0.4897 |           0.3161 |
[32m[20221213 12:16:35 @agent_ppo2.py:179][0m |          -0.0166 |           0.4834 |           0.3159 |
[32m[20221213 12:16:35 @agent_ppo2.py:179][0m |          -0.0222 |           0.4830 |           0.3166 |
[32m[20221213 12:16:35 @agent_ppo2.py:179][0m |          -0.0181 |           0.4807 |           0.3165 |
[32m[20221213 12:16:35 @agent_ppo2.py:179][0m |          -0.0260 |           0.4758 |           0.3165 |
[32m[20221213 12:16:36 @agent_ppo2.py:179][0m |          -0.0227 |           0.4706 |           0.3171 |
[32m[20221213 12:16:36 @agent_ppo2.py:179][0m |          -0.0265 |           0.4715 |           0.3178 |
[32m[20221213 12:16:36 @agent_ppo2.py:179][0m |          -0.0299 |           0.4698 |           0.3180 |
[32m[20221213 12:16:36 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:16:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 51.28
[32m[20221213 12:16:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.99
[32m[20221213 12:16:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.79
[32m[20221213 12:16:36 @agent_ppo2.py:137][0m Total time:       0.65 min
[32m[20221213 12:16:36 @agent_ppo2.py:139][0m 47104 total steps have happened
[32m[20221213 12:16:36 @agent_ppo2.py:115][0m #------------------------ Iteration 23 --------------------------#
[32m[20221213 12:16:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:36 @agent_ppo2.py:179][0m |          -0.0018 |           0.3683 |           0.3081 |
[32m[20221213 12:16:37 @agent_ppo2.py:179][0m |          -0.0064 |           0.3593 |           0.3075 |
[32m[20221213 12:16:37 @agent_ppo2.py:179][0m |           0.0005 |           0.3851 |           0.3067 |
[32m[20221213 12:16:37 @agent_ppo2.py:179][0m |          -0.0045 |           0.3596 |           0.3040 |
[32m[20221213 12:16:37 @agent_ppo2.py:179][0m |          -0.0148 |           0.3561 |           0.3072 |
[32m[20221213 12:16:37 @agent_ppo2.py:179][0m |          -0.0174 |           0.3563 |           0.3068 |
[32m[20221213 12:16:37 @agent_ppo2.py:179][0m |          -0.0175 |           0.3567 |           0.3070 |
[32m[20221213 12:16:37 @agent_ppo2.py:179][0m |          -0.0174 |           0.3528 |           0.3074 |
[32m[20221213 12:16:38 @agent_ppo2.py:179][0m |          -0.0177 |           0.3542 |           0.3061 |
[32m[20221213 12:16:38 @agent_ppo2.py:179][0m |          -0.0203 |           0.3526 |           0.3071 |
[32m[20221213 12:16:38 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:16:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 46.74
[32m[20221213 12:16:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.06
[32m[20221213 12:16:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.36
[32m[20221213 12:16:38 @agent_ppo2.py:137][0m Total time:       0.69 min
[32m[20221213 12:16:38 @agent_ppo2.py:139][0m 49152 total steps have happened
[32m[20221213 12:16:38 @agent_ppo2.py:115][0m #------------------------ Iteration 24 --------------------------#
[32m[20221213 12:16:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 5 slaves
[32m[20221213 12:16:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:39 @agent_ppo2.py:179][0m |           0.0035 |           0.3615 |           0.3162 |
[32m[20221213 12:16:39 @agent_ppo2.py:179][0m |          -0.0064 |           0.3573 |           0.3160 |
[32m[20221213 12:16:39 @agent_ppo2.py:179][0m |          -0.0074 |           0.3564 |           0.3149 |
[32m[20221213 12:16:39 @agent_ppo2.py:179][0m |          -0.0087 |           0.3563 |           0.3145 |
[32m[20221213 12:16:40 @agent_ppo2.py:179][0m |          -0.0076 |           0.3606 |           0.3154 |
[32m[20221213 12:16:40 @agent_ppo2.py:179][0m |          -0.0115 |           0.3556 |           0.3141 |
[32m[20221213 12:16:40 @agent_ppo2.py:179][0m |          -0.0140 |           0.3595 |           0.3148 |
[32m[20221213 12:16:40 @agent_ppo2.py:179][0m |          -0.0135 |           0.3556 |           0.3152 |
[32m[20221213 12:16:40 @agent_ppo2.py:179][0m |          -0.0144 |           0.3548 |           0.3148 |
[32m[20221213 12:16:40 @agent_ppo2.py:179][0m |          -0.0102 |           0.3633 |           0.3143 |
[32m[20221213 12:16:40 @agent_ppo2.py:124][0m Policy update time: 1.85 s
[32m[20221213 12:16:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.00
[32m[20221213 12:16:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.58
[32m[20221213 12:16:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.09
[32m[20221213 12:16:41 @agent_ppo2.py:137][0m Total time:       0.73 min
[32m[20221213 12:16:41 @agent_ppo2.py:139][0m 51200 total steps have happened
[32m[20221213 12:16:41 @agent_ppo2.py:115][0m #------------------------ Iteration 25 --------------------------#
[32m[20221213 12:16:41 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:16:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:41 @agent_ppo2.py:179][0m |           0.0070 |           0.5434 |           0.3123 |
[32m[20221213 12:16:41 @agent_ppo2.py:179][0m |          -0.0084 |           0.5274 |           0.3113 |
[32m[20221213 12:16:41 @agent_ppo2.py:179][0m |          -0.0120 |           0.5182 |           0.3098 |
[32m[20221213 12:16:41 @agent_ppo2.py:179][0m |          -0.0099 |           0.5141 |           0.3109 |
[32m[20221213 12:16:42 @agent_ppo2.py:179][0m |          -0.0126 |           0.5175 |           0.3103 |
[32m[20221213 12:16:42 @agent_ppo2.py:179][0m |          -0.0191 |           0.5093 |           0.3110 |
[32m[20221213 12:16:42 @agent_ppo2.py:179][0m |          -0.0110 |           0.5353 |           0.3097 |
[32m[20221213 12:16:42 @agent_ppo2.py:179][0m |          -0.0159 |           0.5149 |           0.3106 |
[32m[20221213 12:16:42 @agent_ppo2.py:179][0m |          -0.0216 |           0.5074 |           0.3113 |
[32m[20221213 12:16:42 @agent_ppo2.py:179][0m |          -0.0175 |           0.5187 |           0.3112 |
[32m[20221213 12:16:42 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:16:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.97
[32m[20221213 12:16:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 59.95
[32m[20221213 12:16:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.39
[32m[20221213 12:16:43 @agent_ppo2.py:137][0m Total time:       0.76 min
[32m[20221213 12:16:43 @agent_ppo2.py:139][0m 53248 total steps have happened
[32m[20221213 12:16:43 @agent_ppo2.py:115][0m #------------------------ Iteration 26 --------------------------#
[32m[20221213 12:16:43 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:16:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:43 @agent_ppo2.py:179][0m |          -0.0025 |           0.5678 |           0.3066 |
[32m[20221213 12:16:43 @agent_ppo2.py:179][0m |          -0.0103 |           0.5449 |           0.3062 |
[32m[20221213 12:16:43 @agent_ppo2.py:179][0m |          -0.0128 |           0.5411 |           0.3067 |
[32m[20221213 12:16:44 @agent_ppo2.py:179][0m |          -0.0186 |           0.5402 |           0.3069 |
[32m[20221213 12:16:44 @agent_ppo2.py:179][0m |          -0.0224 |           0.5402 |           0.3074 |
[32m[20221213 12:16:44 @agent_ppo2.py:179][0m |          -0.0055 |           0.5921 |           0.3078 |
[32m[20221213 12:16:44 @agent_ppo2.py:179][0m |          -0.0200 |           0.5379 |           0.3085 |
[32m[20221213 12:16:44 @agent_ppo2.py:179][0m |          -0.0222 |           0.5318 |           0.3090 |
[32m[20221213 12:16:44 @agent_ppo2.py:179][0m |          -0.0232 |           0.5306 |           0.3088 |
[32m[20221213 12:16:44 @agent_ppo2.py:179][0m |          -0.0228 |           0.5290 |           0.3092 |
[32m[20221213 12:16:44 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:16:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.60
[32m[20221213 12:16:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.52
[32m[20221213 12:16:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.37
[32m[20221213 12:16:45 @agent_ppo2.py:137][0m Total time:       0.80 min
[32m[20221213 12:16:45 @agent_ppo2.py:139][0m 55296 total steps have happened
[32m[20221213 12:16:45 @agent_ppo2.py:115][0m #------------------------ Iteration 27 --------------------------#
[32m[20221213 12:16:45 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:16:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:45 @agent_ppo2.py:179][0m |           0.0052 |           0.3910 |           0.3188 |
[32m[20221213 12:16:45 @agent_ppo2.py:179][0m |           0.0031 |           0.3886 |           0.3183 |
[32m[20221213 12:16:45 @agent_ppo2.py:179][0m |          -0.0058 |           0.3815 |           0.3186 |
[32m[20221213 12:16:46 @agent_ppo2.py:179][0m |          -0.0000 |           0.3996 |           0.3191 |
[32m[20221213 12:16:46 @agent_ppo2.py:179][0m |          -0.0004 |           0.3848 |           0.3181 |
[32m[20221213 12:16:46 @agent_ppo2.py:179][0m |           0.0004 |           0.4194 |           0.3201 |
[32m[20221213 12:16:46 @agent_ppo2.py:179][0m |          -0.0105 |           0.3820 |           0.3192 |
[32m[20221213 12:16:46 @agent_ppo2.py:179][0m |          -0.0126 |           0.3821 |           0.3187 |
[32m[20221213 12:16:46 @agent_ppo2.py:179][0m |          -0.0172 |           0.3818 |           0.3195 |
[32m[20221213 12:16:47 @agent_ppo2.py:179][0m |          -0.0162 |           0.3794 |           0.3194 |
[32m[20221213 12:16:47 @agent_ppo2.py:124][0m Policy update time: 1.58 s
[32m[20221213 12:16:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.57
[32m[20221213 12:16:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.10
[32m[20221213 12:16:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 9.53
[32m[20221213 12:16:47 @agent_ppo2.py:137][0m Total time:       0.83 min
[32m[20221213 12:16:47 @agent_ppo2.py:139][0m 57344 total steps have happened
[32m[20221213 12:16:47 @agent_ppo2.py:115][0m #------------------------ Iteration 28 --------------------------#
[32m[20221213 12:16:47 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:16:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:47 @agent_ppo2.py:179][0m |           0.0028 |           0.4134 |           0.3217 |
[32m[20221213 12:16:47 @agent_ppo2.py:179][0m |          -0.0022 |           0.4154 |           0.3190 |
[32m[20221213 12:16:48 @agent_ppo2.py:179][0m |          -0.0063 |           0.4106 |           0.3192 |
[32m[20221213 12:16:48 @agent_ppo2.py:179][0m |          -0.0060 |           0.4122 |           0.3190 |
[32m[20221213 12:16:48 @agent_ppo2.py:179][0m |          -0.0145 |           0.4096 |           0.3190 |
[32m[20221213 12:16:48 @agent_ppo2.py:179][0m |          -0.0136 |           0.4070 |           0.3189 |
[32m[20221213 12:16:48 @agent_ppo2.py:179][0m |          -0.0059 |           0.4191 |           0.3193 |
[32m[20221213 12:16:48 @agent_ppo2.py:179][0m |          -0.0134 |           0.4087 |           0.3191 |
[32m[20221213 12:16:48 @agent_ppo2.py:179][0m |          -0.0159 |           0.4045 |           0.3189 |
[32m[20221213 12:16:49 @agent_ppo2.py:179][0m |          -0.0179 |           0.4064 |           0.3184 |
[32m[20221213 12:16:49 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:16:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.54
[32m[20221213 12:16:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.07
[32m[20221213 12:16:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.73
[32m[20221213 12:16:49 @agent_ppo2.py:137][0m Total time:       0.87 min
[32m[20221213 12:16:49 @agent_ppo2.py:139][0m 59392 total steps have happened
[32m[20221213 12:16:49 @agent_ppo2.py:115][0m #------------------------ Iteration 29 --------------------------#
[32m[20221213 12:16:49 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:16:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:49 @agent_ppo2.py:179][0m |           0.0034 |           0.4445 |           0.3191 |
[32m[20221213 12:16:49 @agent_ppo2.py:179][0m |           0.0071 |           0.4913 |           0.3198 |
[32m[20221213 12:16:50 @agent_ppo2.py:179][0m |          -0.0045 |           0.4360 |           0.3173 |
[32m[20221213 12:16:50 @agent_ppo2.py:179][0m |          -0.0103 |           0.4326 |           0.3193 |
[32m[20221213 12:16:50 @agent_ppo2.py:179][0m |          -0.0079 |           0.4451 |           0.3214 |
[32m[20221213 12:16:50 @agent_ppo2.py:179][0m |          -0.0128 |           0.4340 |           0.3200 |
[32m[20221213 12:16:50 @agent_ppo2.py:179][0m |          -0.0155 |           0.4305 |           0.3221 |
[32m[20221213 12:16:50 @agent_ppo2.py:179][0m |          -0.0167 |           0.4356 |           0.3228 |
[32m[20221213 12:16:50 @agent_ppo2.py:179][0m |          -0.0181 |           0.4317 |           0.3225 |
[32m[20221213 12:16:51 @agent_ppo2.py:179][0m |          -0.0198 |           0.4312 |           0.3221 |
[32m[20221213 12:16:51 @agent_ppo2.py:124][0m Policy update time: 1.60 s
[32m[20221213 12:16:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.60
[32m[20221213 12:16:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.88
[32m[20221213 12:16:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.57
[32m[20221213 12:16:51 @agent_ppo2.py:137][0m Total time:       0.90 min
[32m[20221213 12:16:51 @agent_ppo2.py:139][0m 61440 total steps have happened
[32m[20221213 12:16:51 @agent_ppo2.py:115][0m #------------------------ Iteration 30 --------------------------#
[32m[20221213 12:16:51 @agent_ppo2.py:121][0m Sampling time: 0.27 s by 5 slaves
[32m[20221213 12:16:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:51 @agent_ppo2.py:179][0m |           0.0038 |           0.6091 |           0.3305 |
[32m[20221213 12:16:52 @agent_ppo2.py:179][0m |          -0.0060 |           0.5864 |           0.3303 |
[32m[20221213 12:16:52 @agent_ppo2.py:179][0m |          -0.0081 |           0.5741 |           0.3300 |
[32m[20221213 12:16:52 @agent_ppo2.py:179][0m |          -0.0111 |           0.5701 |           0.3297 |
[32m[20221213 12:16:52 @agent_ppo2.py:179][0m |          -0.0152 |           0.5682 |           0.3291 |
[32m[20221213 12:16:52 @agent_ppo2.py:179][0m |          -0.0172 |           0.5644 |           0.3302 |
[32m[20221213 12:16:52 @agent_ppo2.py:179][0m |          -0.0260 |           0.5610 |           0.3308 |
[32m[20221213 12:16:52 @agent_ppo2.py:179][0m |          -0.0258 |           0.5564 |           0.3304 |
[32m[20221213 12:16:53 @agent_ppo2.py:179][0m |          -0.0271 |           0.5551 |           0.3311 |
[32m[20221213 12:16:53 @agent_ppo2.py:179][0m |          -0.0241 |           0.5536 |           0.3299 |
[32m[20221213 12:16:53 @agent_ppo2.py:124][0m Policy update time: 1.57 s
[32m[20221213 12:16:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.49
[32m[20221213 12:16:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.41
[32m[20221213 12:16:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.95
[32m[20221213 12:16:53 @agent_ppo2.py:137][0m Total time:       0.94 min
[32m[20221213 12:16:53 @agent_ppo2.py:139][0m 63488 total steps have happened
[32m[20221213 12:16:53 @agent_ppo2.py:115][0m #------------------------ Iteration 31 --------------------------#
[32m[20221213 12:16:53 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:16:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:53 @agent_ppo2.py:179][0m |           0.0035 |           0.5357 |           0.3233 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |           0.0010 |           0.5501 |           0.3244 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |          -0.0112 |           0.5136 |           0.3233 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |          -0.0081 |           0.5078 |           0.3243 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |          -0.0061 |           0.5542 |           0.3244 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |          -0.0237 |           0.5009 |           0.3244 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |          -0.0278 |           0.4902 |           0.3254 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |          -0.0264 |           0.4876 |           0.3252 |
[32m[20221213 12:16:54 @agent_ppo2.py:179][0m |          -0.0310 |           0.4924 |           0.3247 |
[32m[20221213 12:16:55 @agent_ppo2.py:179][0m |          -0.0213 |           0.5154 |           0.3257 |
[32m[20221213 12:16:55 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:16:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.25
[32m[20221213 12:16:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.25
[32m[20221213 12:16:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.48
[32m[20221213 12:16:55 @agent_ppo2.py:137][0m Total time:       0.97 min
[32m[20221213 12:16:55 @agent_ppo2.py:139][0m 65536 total steps have happened
[32m[20221213 12:16:55 @agent_ppo2.py:115][0m #------------------------ Iteration 32 --------------------------#
[32m[20221213 12:16:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:55 @agent_ppo2.py:179][0m |           0.0061 |           0.7087 |           0.3376 |
[32m[20221213 12:16:55 @agent_ppo2.py:179][0m |          -0.0073 |           0.6827 |           0.3372 |
[32m[20221213 12:16:55 @agent_ppo2.py:179][0m |          -0.0171 |           0.6734 |           0.3356 |
[32m[20221213 12:16:56 @agent_ppo2.py:179][0m |          -0.0123 |           0.6833 |           0.3360 |
[32m[20221213 12:16:56 @agent_ppo2.py:179][0m |          -0.0299 |           0.6726 |           0.3359 |
[32m[20221213 12:16:56 @agent_ppo2.py:179][0m |          -0.0266 |           0.6660 |           0.3362 |
[32m[20221213 12:16:56 @agent_ppo2.py:179][0m |          -0.0259 |           0.6639 |           0.3360 |
[32m[20221213 12:16:56 @agent_ppo2.py:179][0m |          -0.0297 |           0.6619 |           0.3363 |
[32m[20221213 12:16:56 @agent_ppo2.py:179][0m |          -0.0273 |           0.6594 |           0.3362 |
[32m[20221213 12:16:56 @agent_ppo2.py:179][0m |          -0.0305 |           0.6531 |           0.3358 |
[32m[20221213 12:16:56 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:16:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.33
[32m[20221213 12:16:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.42
[32m[20221213 12:16:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.34
[32m[20221213 12:16:56 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 61.34
[32m[20221213 12:16:56 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 61.34
[32m[20221213 12:16:56 @agent_ppo2.py:137][0m Total time:       1.00 min
[32m[20221213 12:16:56 @agent_ppo2.py:139][0m 67584 total steps have happened
[32m[20221213 12:16:56 @agent_ppo2.py:115][0m #------------------------ Iteration 33 --------------------------#
[32m[20221213 12:16:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:57 @agent_ppo2.py:179][0m |           0.0021 |           0.4786 |           0.3342 |
[32m[20221213 12:16:57 @agent_ppo2.py:179][0m |          -0.0084 |           0.4677 |           0.3331 |
[32m[20221213 12:16:57 @agent_ppo2.py:179][0m |          -0.0064 |           0.4682 |           0.3318 |
[32m[20221213 12:16:57 @agent_ppo2.py:179][0m |          -0.0097 |           0.4680 |           0.3294 |
[32m[20221213 12:16:57 @agent_ppo2.py:179][0m |          -0.0114 |           0.4635 |           0.3286 |
[32m[20221213 12:16:57 @agent_ppo2.py:179][0m |          -0.0138 |           0.4589 |           0.3287 |
[32m[20221213 12:16:58 @agent_ppo2.py:179][0m |          -0.0110 |           0.4651 |           0.3284 |
[32m[20221213 12:16:58 @agent_ppo2.py:179][0m |          -0.0097 |           0.4861 |           0.3280 |
[32m[20221213 12:16:58 @agent_ppo2.py:179][0m |          -0.0158 |           0.4562 |           0.3271 |
[32m[20221213 12:16:58 @agent_ppo2.py:179][0m |          -0.0111 |           0.4786 |           0.3268 |
[32m[20221213 12:16:58 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:16:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.01
[32m[20221213 12:16:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.68
[32m[20221213 12:16:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.02
[32m[20221213 12:16:58 @agent_ppo2.py:137][0m Total time:       1.02 min
[32m[20221213 12:16:58 @agent_ppo2.py:139][0m 69632 total steps have happened
[32m[20221213 12:16:58 @agent_ppo2.py:115][0m #------------------------ Iteration 34 --------------------------#
[32m[20221213 12:16:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:16:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |           0.0058 |           0.6017 |           0.3200 |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |          -0.0096 |           0.5731 |           0.3198 |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |          -0.0176 |           0.5670 |           0.3206 |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |          -0.0124 |           0.5662 |           0.3222 |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |          -0.0106 |           0.5746 |           0.3229 |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |          -0.0207 |           0.5599 |           0.3234 |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |          -0.0270 |           0.5610 |           0.3242 |
[32m[20221213 12:16:59 @agent_ppo2.py:179][0m |          -0.0237 |           0.5539 |           0.3238 |
[32m[20221213 12:17:00 @agent_ppo2.py:179][0m |          -0.0325 |           0.5554 |           0.3238 |
[32m[20221213 12:17:00 @agent_ppo2.py:179][0m |          -0.0302 |           0.5595 |           0.3247 |
[32m[20221213 12:17:00 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:17:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.13
[32m[20221213 12:17:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.67
[32m[20221213 12:17:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.39
[32m[20221213 12:17:00 @agent_ppo2.py:137][0m Total time:       1.05 min
[32m[20221213 12:17:00 @agent_ppo2.py:139][0m 71680 total steps have happened
[32m[20221213 12:17:00 @agent_ppo2.py:115][0m #------------------------ Iteration 35 --------------------------#
[32m[20221213 12:17:00 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:17:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:00 @agent_ppo2.py:179][0m |          -0.0008 |           0.5114 |           0.3306 |
[32m[20221213 12:17:00 @agent_ppo2.py:179][0m |          -0.0016 |           0.4963 |           0.3295 |
[32m[20221213 12:17:01 @agent_ppo2.py:179][0m |          -0.0099 |           0.4933 |           0.3292 |
[32m[20221213 12:17:01 @agent_ppo2.py:179][0m |          -0.0109 |           0.4922 |           0.3280 |
[32m[20221213 12:17:01 @agent_ppo2.py:179][0m |          -0.0134 |           0.4957 |           0.3283 |
[32m[20221213 12:17:01 @agent_ppo2.py:179][0m |          -0.0138 |           0.4937 |           0.3284 |
[32m[20221213 12:17:01 @agent_ppo2.py:179][0m |          -0.0133 |           0.4903 |           0.3289 |
[32m[20221213 12:17:01 @agent_ppo2.py:179][0m |          -0.0069 |           0.5037 |           0.3269 |
[32m[20221213 12:17:01 @agent_ppo2.py:179][0m |          -0.0161 |           0.4895 |           0.3271 |
[32m[20221213 12:17:02 @agent_ppo2.py:179][0m |          -0.0150 |           0.4889 |           0.3288 |
[32m[20221213 12:17:02 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:17:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.74
[32m[20221213 12:17:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.69
[32m[20221213 12:17:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.56
[32m[20221213 12:17:02 @agent_ppo2.py:137][0m Total time:       1.08 min
[32m[20221213 12:17:02 @agent_ppo2.py:139][0m 73728 total steps have happened
[32m[20221213 12:17:02 @agent_ppo2.py:115][0m #------------------------ Iteration 36 --------------------------#
[32m[20221213 12:17:02 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:02 @agent_ppo2.py:179][0m |           0.0128 |           0.5432 |           0.3171 |
[32m[20221213 12:17:02 @agent_ppo2.py:179][0m |          -0.0050 |           0.5207 |           0.3154 |
[32m[20221213 12:17:03 @agent_ppo2.py:179][0m |          -0.0105 |           0.5164 |           0.3180 |
[32m[20221213 12:17:03 @agent_ppo2.py:179][0m |          -0.0112 |           0.5144 |           0.3180 |
[32m[20221213 12:17:03 @agent_ppo2.py:179][0m |          -0.0147 |           0.5137 |           0.3187 |
[32m[20221213 12:17:03 @agent_ppo2.py:179][0m |          -0.0115 |           0.5200 |           0.3187 |
[32m[20221213 12:17:03 @agent_ppo2.py:179][0m |          -0.0166 |           0.5132 |           0.3185 |
[32m[20221213 12:17:03 @agent_ppo2.py:179][0m |          -0.0171 |           0.5173 |           0.3174 |
[32m[20221213 12:17:04 @agent_ppo2.py:179][0m |          -0.0166 |           0.5138 |           0.3185 |
[32m[20221213 12:17:04 @agent_ppo2.py:179][0m |          -0.0130 |           0.5219 |           0.3185 |
[32m[20221213 12:17:04 @agent_ppo2.py:124][0m Policy update time: 1.70 s
[32m[20221213 12:17:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.77
[32m[20221213 12:17:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.23
[32m[20221213 12:17:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.99
[32m[20221213 12:17:04 @agent_ppo2.py:137][0m Total time:       1.12 min
[32m[20221213 12:17:04 @agent_ppo2.py:139][0m 75776 total steps have happened
[32m[20221213 12:17:04 @agent_ppo2.py:115][0m #------------------------ Iteration 37 --------------------------#
[32m[20221213 12:17:04 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:17:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:04 @agent_ppo2.py:179][0m |           0.0065 |           0.6915 |           0.3277 |
[32m[20221213 12:17:05 @agent_ppo2.py:179][0m |          -0.0074 |           0.6859 |           0.3265 |
[32m[20221213 12:17:05 @agent_ppo2.py:179][0m |          -0.0152 |           0.6820 |           0.3239 |
[32m[20221213 12:17:05 @agent_ppo2.py:179][0m |          -0.0158 |           0.6772 |           0.3235 |
[32m[20221213 12:17:05 @agent_ppo2.py:179][0m |          -0.0200 |           0.6817 |           0.3235 |
[32m[20221213 12:17:05 @agent_ppo2.py:179][0m |          -0.0226 |           0.6805 |           0.3242 |
[32m[20221213 12:17:05 @agent_ppo2.py:179][0m |          -0.0206 |           0.6753 |           0.3245 |
[32m[20221213 12:17:05 @agent_ppo2.py:179][0m |          -0.0214 |           0.6929 |           0.3251 |
[32m[20221213 12:17:06 @agent_ppo2.py:179][0m |          -0.0212 |           0.6771 |           0.3242 |
[32m[20221213 12:17:06 @agent_ppo2.py:179][0m |          -0.0262 |           0.6675 |           0.3235 |
[32m[20221213 12:17:06 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:17:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.83
[32m[20221213 12:17:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 60.93
[32m[20221213 12:17:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.25
[32m[20221213 12:17:06 @agent_ppo2.py:137][0m Total time:       1.15 min
[32m[20221213 12:17:06 @agent_ppo2.py:139][0m 77824 total steps have happened
[32m[20221213 12:17:06 @agent_ppo2.py:115][0m #------------------------ Iteration 38 --------------------------#
[32m[20221213 12:17:06 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:17:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:06 @agent_ppo2.py:179][0m |           0.0008 |           0.6006 |           0.3271 |
[32m[20221213 12:17:07 @agent_ppo2.py:179][0m |          -0.0098 |           0.5918 |           0.3247 |
[32m[20221213 12:17:07 @agent_ppo2.py:179][0m |          -0.0171 |           0.5908 |           0.3240 |
[32m[20221213 12:17:07 @agent_ppo2.py:179][0m |          -0.0191 |           0.5881 |           0.3247 |
[32m[20221213 12:17:07 @agent_ppo2.py:179][0m |          -0.0205 |           0.5861 |           0.3233 |
[32m[20221213 12:17:07 @agent_ppo2.py:179][0m |          -0.0195 |           0.5860 |           0.3237 |
[32m[20221213 12:17:07 @agent_ppo2.py:179][0m |          -0.0242 |           0.5825 |           0.3234 |
[32m[20221213 12:17:08 @agent_ppo2.py:179][0m |          -0.0192 |           0.5915 |           0.3235 |
[32m[20221213 12:17:08 @agent_ppo2.py:179][0m |          -0.0300 |           0.5842 |           0.3226 |
[32m[20221213 12:17:08 @agent_ppo2.py:179][0m |          -0.0295 |           0.5836 |           0.3234 |
[32m[20221213 12:17:08 @agent_ppo2.py:124][0m Policy update time: 1.58 s
[32m[20221213 12:17:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 54.33
[32m[20221213 12:17:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.74
[32m[20221213 12:17:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.99
[32m[20221213 12:17:08 @agent_ppo2.py:137][0m Total time:       1.19 min
[32m[20221213 12:17:08 @agent_ppo2.py:139][0m 79872 total steps have happened
[32m[20221213 12:17:08 @agent_ppo2.py:115][0m #------------------------ Iteration 39 --------------------------#
[32m[20221213 12:17:08 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |           0.0067 |           0.6758 |           0.3165 |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |          -0.0061 |           0.6638 |           0.3154 |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |          -0.0053 |           0.6541 |           0.3153 |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |          -0.0141 |           0.6477 |           0.3155 |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |          -0.0149 |           0.6428 |           0.3157 |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |          -0.0194 |           0.6400 |           0.3157 |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |          -0.0233 |           0.6349 |           0.3159 |
[32m[20221213 12:17:09 @agent_ppo2.py:179][0m |          -0.0236 |           0.6341 |           0.3159 |
[32m[20221213 12:17:10 @agent_ppo2.py:179][0m |          -0.0258 |           0.6351 |           0.3163 |
[32m[20221213 12:17:10 @agent_ppo2.py:179][0m |          -0.0257 |           0.6324 |           0.3163 |
[32m[20221213 12:17:10 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:17:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.40
[32m[20221213 12:17:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.88
[32m[20221213 12:17:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.79
[32m[20221213 12:17:10 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 78.79
[32m[20221213 12:17:10 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 78.79
[32m[20221213 12:17:10 @agent_ppo2.py:137][0m Total time:       1.22 min
[32m[20221213 12:17:10 @agent_ppo2.py:139][0m 81920 total steps have happened
[32m[20221213 12:17:10 @agent_ppo2.py:115][0m #------------------------ Iteration 40 --------------------------#
[32m[20221213 12:17:10 @agent_ppo2.py:121][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 12:17:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:10 @agent_ppo2.py:179][0m |           0.0045 |           0.7046 |           0.3258 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0039 |           0.6895 |           0.3243 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0166 |           0.6809 |           0.3244 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0180 |           0.6800 |           0.3251 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0204 |           0.6837 |           0.3251 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0141 |           0.7095 |           0.3261 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0244 |           0.6757 |           0.3275 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0286 |           0.6755 |           0.3266 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0212 |           0.6878 |           0.3282 |
[32m[20221213 12:17:11 @agent_ppo2.py:179][0m |          -0.0322 |           0.6738 |           0.3274 |
[32m[20221213 12:17:11 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:17:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 59.99
[32m[20221213 12:17:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.92
[32m[20221213 12:17:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.77
[32m[20221213 12:17:12 @agent_ppo2.py:137][0m Total time:       1.25 min
[32m[20221213 12:17:12 @agent_ppo2.py:139][0m 83968 total steps have happened
[32m[20221213 12:17:12 @agent_ppo2.py:115][0m #------------------------ Iteration 41 --------------------------#
[32m[20221213 12:17:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:12 @agent_ppo2.py:179][0m |           0.0050 |           0.6252 |           0.3300 |
[32m[20221213 12:17:12 @agent_ppo2.py:179][0m |          -0.0110 |           0.6110 |           0.3275 |
[32m[20221213 12:17:12 @agent_ppo2.py:179][0m |          -0.0047 |           0.6128 |           0.3272 |
[32m[20221213 12:17:12 @agent_ppo2.py:179][0m |          -0.0157 |           0.6045 |           0.3265 |
[32m[20221213 12:17:13 @agent_ppo2.py:179][0m |          -0.0183 |           0.5964 |           0.3269 |
[32m[20221213 12:17:13 @agent_ppo2.py:179][0m |          -0.0235 |           0.5969 |           0.3272 |
[32m[20221213 12:17:13 @agent_ppo2.py:179][0m |          -0.0233 |           0.5924 |           0.3273 |
[32m[20221213 12:17:13 @agent_ppo2.py:179][0m |          -0.0230 |           0.5912 |           0.3273 |
[32m[20221213 12:17:13 @agent_ppo2.py:179][0m |          -0.0268 |           0.5889 |           0.3281 |
[32m[20221213 12:17:13 @agent_ppo2.py:179][0m |          -0.0271 |           0.5984 |           0.3265 |
[32m[20221213 12:17:13 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:17:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.60
[32m[20221213 12:17:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 79.91
[32m[20221213 12:17:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.03
[32m[20221213 12:17:13 @agent_ppo2.py:137][0m Total time:       1.28 min
[32m[20221213 12:17:13 @agent_ppo2.py:139][0m 86016 total steps have happened
[32m[20221213 12:17:13 @agent_ppo2.py:115][0m #------------------------ Iteration 42 --------------------------#
[32m[20221213 12:17:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:14 @agent_ppo2.py:179][0m |           0.0048 |           0.8557 |           0.3276 |
[32m[20221213 12:17:14 @agent_ppo2.py:179][0m |          -0.0081 |           0.8411 |           0.3288 |
[32m[20221213 12:17:14 @agent_ppo2.py:179][0m |          -0.0066 |           0.8549 |           0.3309 |
[32m[20221213 12:17:14 @agent_ppo2.py:179][0m |          -0.0067 |           0.9184 |           0.3318 |
[32m[20221213 12:17:14 @agent_ppo2.py:179][0m |          -0.0052 |           0.8296 |           0.3276 |
[32m[20221213 12:17:14 @agent_ppo2.py:179][0m |          -0.0196 |           0.8175 |           0.3317 |
[32m[20221213 12:17:14 @agent_ppo2.py:179][0m |          -0.0236 |           0.8191 |           0.3311 |
[32m[20221213 12:17:15 @agent_ppo2.py:179][0m |          -0.0284 |           0.8124 |           0.3326 |
[32m[20221213 12:17:15 @agent_ppo2.py:179][0m |          -0.0279 |           0.8172 |           0.3319 |
[32m[20221213 12:17:15 @agent_ppo2.py:179][0m |          -0.0330 |           0.8189 |           0.3324 |
[32m[20221213 12:17:15 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:17:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.02
[32m[20221213 12:17:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.65
[32m[20221213 12:17:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.98
[32m[20221213 12:17:15 @agent_ppo2.py:137][0m Total time:       1.31 min
[32m[20221213 12:17:15 @agent_ppo2.py:139][0m 88064 total steps have happened
[32m[20221213 12:17:15 @agent_ppo2.py:115][0m #------------------------ Iteration 43 --------------------------#
[32m[20221213 12:17:15 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:16 @agent_ppo2.py:179][0m |           0.0002 |           0.8005 |           0.3441 |
[32m[20221213 12:17:16 @agent_ppo2.py:179][0m |          -0.0091 |           0.7842 |           0.3424 |
[32m[20221213 12:17:16 @agent_ppo2.py:179][0m |          -0.0121 |           0.7698 |           0.3423 |
[32m[20221213 12:17:16 @agent_ppo2.py:179][0m |          -0.0141 |           0.7687 |           0.3409 |
[32m[20221213 12:17:16 @agent_ppo2.py:179][0m |          -0.0213 |           0.7583 |           0.3421 |
[32m[20221213 12:17:16 @agent_ppo2.py:179][0m |          -0.0221 |           0.7555 |           0.3424 |
[32m[20221213 12:17:16 @agent_ppo2.py:179][0m |          -0.0248 |           0.7569 |           0.3424 |
[32m[20221213 12:17:17 @agent_ppo2.py:179][0m |          -0.0280 |           0.7472 |           0.3433 |
[32m[20221213 12:17:17 @agent_ppo2.py:179][0m |          -0.0294 |           0.7506 |           0.3437 |
[32m[20221213 12:17:17 @agent_ppo2.py:179][0m |          -0.0303 |           0.7488 |           0.3437 |
[32m[20221213 12:17:17 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:17:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.56
[32m[20221213 12:17:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.53
[32m[20221213 12:17:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.04
[32m[20221213 12:17:17 @agent_ppo2.py:137][0m Total time:       1.34 min
[32m[20221213 12:17:17 @agent_ppo2.py:139][0m 90112 total steps have happened
[32m[20221213 12:17:17 @agent_ppo2.py:115][0m #------------------------ Iteration 44 --------------------------#
[32m[20221213 12:17:17 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:17:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:17 @agent_ppo2.py:179][0m |           0.0026 |           0.6862 |           0.3352 |
[32m[20221213 12:17:18 @agent_ppo2.py:179][0m |          -0.0112 |           0.6738 |           0.3363 |
[32m[20221213 12:17:18 @agent_ppo2.py:179][0m |          -0.0109 |           0.6695 |           0.3355 |
[32m[20221213 12:17:18 @agent_ppo2.py:179][0m |          -0.0134 |           0.6659 |           0.3358 |
[32m[20221213 12:17:18 @agent_ppo2.py:179][0m |          -0.0220 |           0.6659 |           0.3351 |
[32m[20221213 12:17:18 @agent_ppo2.py:179][0m |          -0.0238 |           0.6643 |           0.3352 |
[32m[20221213 12:17:18 @agent_ppo2.py:179][0m |          -0.0238 |           0.6733 |           0.3356 |
[32m[20221213 12:17:18 @agent_ppo2.py:179][0m |          -0.0221 |           0.6684 |           0.3355 |
[32m[20221213 12:17:19 @agent_ppo2.py:179][0m |          -0.0250 |           0.6665 |           0.3361 |
[32m[20221213 12:17:19 @agent_ppo2.py:179][0m |          -0.0188 |           0.7016 |           0.3369 |
[32m[20221213 12:17:19 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:17:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.54
[32m[20221213 12:17:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.60
[32m[20221213 12:17:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.43
[32m[20221213 12:17:19 @agent_ppo2.py:137][0m Total time:       1.37 min
[32m[20221213 12:17:19 @agent_ppo2.py:139][0m 92160 total steps have happened
[32m[20221213 12:17:19 @agent_ppo2.py:115][0m #------------------------ Iteration 45 --------------------------#
[32m[20221213 12:17:19 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:19 @agent_ppo2.py:179][0m |           0.0079 |           0.6225 |           0.3436 |
[32m[20221213 12:17:19 @agent_ppo2.py:179][0m |          -0.0043 |           0.6100 |           0.3437 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0101 |           0.6003 |           0.3435 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0163 |           0.5936 |           0.3440 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0142 |           0.5942 |           0.3437 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0216 |           0.5908 |           0.3434 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0214 |           0.5887 |           0.3444 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0243 |           0.5847 |           0.3442 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0246 |           0.5795 |           0.3450 |
[32m[20221213 12:17:20 @agent_ppo2.py:179][0m |          -0.0273 |           0.5770 |           0.3449 |
[32m[20221213 12:17:20 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:17:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.65
[32m[20221213 12:17:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.70
[32m[20221213 12:17:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.42
[32m[20221213 12:17:21 @agent_ppo2.py:137][0m Total time:       1.40 min
[32m[20221213 12:17:21 @agent_ppo2.py:139][0m 94208 total steps have happened
[32m[20221213 12:17:21 @agent_ppo2.py:115][0m #------------------------ Iteration 46 --------------------------#
[32m[20221213 12:17:21 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:21 @agent_ppo2.py:179][0m |           0.0059 |           0.5020 |           0.3398 |
[32m[20221213 12:17:21 @agent_ppo2.py:179][0m |          -0.0035 |           0.4933 |           0.3381 |
[32m[20221213 12:17:21 @agent_ppo2.py:179][0m |          -0.0039 |           0.5206 |           0.3399 |
[32m[20221213 12:17:22 @agent_ppo2.py:179][0m |          -0.0087 |           0.4948 |           0.3388 |
[32m[20221213 12:17:22 @agent_ppo2.py:179][0m |          -0.0150 |           0.4931 |           0.3383 |
[32m[20221213 12:17:22 @agent_ppo2.py:179][0m |          -0.0086 |           0.5106 |           0.3380 |
[32m[20221213 12:17:22 @agent_ppo2.py:179][0m |          -0.0192 |           0.4902 |           0.3373 |
[32m[20221213 12:17:22 @agent_ppo2.py:179][0m |          -0.0188 |           0.4891 |           0.3375 |
[32m[20221213 12:17:22 @agent_ppo2.py:179][0m |          -0.0204 |           0.4899 |           0.3378 |
[32m[20221213 12:17:22 @agent_ppo2.py:179][0m |          -0.0117 |           0.5189 |           0.3373 |
[32m[20221213 12:17:22 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:17:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.59
[32m[20221213 12:17:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.29
[32m[20221213 12:17:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.44
[32m[20221213 12:17:23 @agent_ppo2.py:137][0m Total time:       1.43 min
[32m[20221213 12:17:23 @agent_ppo2.py:139][0m 96256 total steps have happened
[32m[20221213 12:17:23 @agent_ppo2.py:115][0m #------------------------ Iteration 47 --------------------------#
[32m[20221213 12:17:23 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:23 @agent_ppo2.py:179][0m |           0.0071 |           0.5243 |           0.3328 |
[32m[20221213 12:17:23 @agent_ppo2.py:179][0m |          -0.0036 |           0.5145 |           0.3347 |
[32m[20221213 12:17:23 @agent_ppo2.py:179][0m |          -0.0091 |           0.5113 |           0.3368 |
[32m[20221213 12:17:23 @agent_ppo2.py:179][0m |          -0.0122 |           0.5096 |           0.3387 |
[32m[20221213 12:17:23 @agent_ppo2.py:179][0m |          -0.0142 |           0.5055 |           0.3394 |
[32m[20221213 12:17:24 @agent_ppo2.py:179][0m |          -0.0129 |           0.5053 |           0.3400 |
[32m[20221213 12:17:24 @agent_ppo2.py:179][0m |          -0.0148 |           0.5068 |           0.3408 |
[32m[20221213 12:17:24 @agent_ppo2.py:179][0m |          -0.0160 |           0.5021 |           0.3402 |
[32m[20221213 12:17:24 @agent_ppo2.py:179][0m |          -0.0229 |           0.5033 |           0.3413 |
[32m[20221213 12:17:24 @agent_ppo2.py:179][0m |          -0.0195 |           0.5019 |           0.3414 |
[32m[20221213 12:17:24 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:17:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 48.26
[32m[20221213 12:17:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.96
[32m[20221213 12:17:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.27
[32m[20221213 12:17:24 @agent_ppo2.py:137][0m Total time:       1.46 min
[32m[20221213 12:17:24 @agent_ppo2.py:139][0m 98304 total steps have happened
[32m[20221213 12:17:24 @agent_ppo2.py:115][0m #------------------------ Iteration 48 --------------------------#
[32m[20221213 12:17:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:25 @agent_ppo2.py:179][0m |           0.0051 |           0.7305 |           0.3737 |
[32m[20221213 12:17:25 @agent_ppo2.py:179][0m |          -0.0052 |           0.7078 |           0.3722 |
[32m[20221213 12:17:25 @agent_ppo2.py:179][0m |          -0.0137 |           0.7066 |           0.3718 |
[32m[20221213 12:17:25 @agent_ppo2.py:179][0m |          -0.0192 |           0.7025 |           0.3718 |
[32m[20221213 12:17:25 @agent_ppo2.py:179][0m |          -0.0169 |           0.6987 |           0.3724 |
[32m[20221213 12:17:25 @agent_ppo2.py:179][0m |          -0.0236 |           0.6890 |           0.3721 |
[32m[20221213 12:17:25 @agent_ppo2.py:179][0m |          -0.0252 |           0.6925 |           0.3723 |
[32m[20221213 12:17:26 @agent_ppo2.py:179][0m |          -0.0255 |           0.6914 |           0.3739 |
[32m[20221213 12:17:26 @agent_ppo2.py:179][0m |          -0.0253 |           0.6973 |           0.3734 |
[32m[20221213 12:17:26 @agent_ppo2.py:179][0m |          -0.0278 |           0.6887 |           0.3733 |
[32m[20221213 12:17:26 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:17:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.25
[32m[20221213 12:17:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.27
[32m[20221213 12:17:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.76
[32m[20221213 12:17:26 @agent_ppo2.py:137][0m Total time:       1.49 min
[32m[20221213 12:17:26 @agent_ppo2.py:139][0m 100352 total steps have happened
[32m[20221213 12:17:26 @agent_ppo2.py:115][0m #------------------------ Iteration 49 --------------------------#
[32m[20221213 12:17:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:26 @agent_ppo2.py:179][0m |           0.0032 |           0.8243 |           0.3612 |
[32m[20221213 12:17:27 @agent_ppo2.py:179][0m |          -0.0062 |           0.8071 |           0.3613 |
[32m[20221213 12:17:27 @agent_ppo2.py:179][0m |          -0.0123 |           0.7983 |           0.3602 |
[32m[20221213 12:17:27 @agent_ppo2.py:179][0m |          -0.0153 |           0.7912 |           0.3599 |
[32m[20221213 12:17:27 @agent_ppo2.py:179][0m |          -0.0257 |           0.7947 |           0.3601 |
[32m[20221213 12:17:27 @agent_ppo2.py:179][0m |          -0.0107 |           0.8061 |           0.3601 |
[32m[20221213 12:17:27 @agent_ppo2.py:179][0m |          -0.0225 |           0.7883 |           0.3605 |
[32m[20221213 12:17:27 @agent_ppo2.py:179][0m |          -0.0269 |           0.7881 |           0.3608 |
[32m[20221213 12:17:28 @agent_ppo2.py:179][0m |          -0.0264 |           0.7838 |           0.3606 |
[32m[20221213 12:17:28 @agent_ppo2.py:179][0m |          -0.0258 |           0.7866 |           0.3612 |
[32m[20221213 12:17:28 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:17:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.83
[32m[20221213 12:17:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 72.11
[32m[20221213 12:17:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.80
[32m[20221213 12:17:28 @agent_ppo2.py:137][0m Total time:       1.52 min
[32m[20221213 12:17:28 @agent_ppo2.py:139][0m 102400 total steps have happened
[32m[20221213 12:17:28 @agent_ppo2.py:115][0m #------------------------ Iteration 50 --------------------------#
[32m[20221213 12:17:28 @agent_ppo2.py:121][0m Sampling time: 0.28 s by 5 slaves
[32m[20221213 12:17:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:28 @agent_ppo2.py:179][0m |           0.0087 |           0.6276 |           0.3618 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0068 |           0.5926 |           0.3584 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0115 |           0.5868 |           0.3590 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0125 |           0.5837 |           0.3588 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0211 |           0.5776 |           0.3590 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0214 |           0.5758 |           0.3584 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0209 |           0.5731 |           0.3586 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0252 |           0.5735 |           0.3578 |
[32m[20221213 12:17:29 @agent_ppo2.py:179][0m |          -0.0253 |           0.5702 |           0.3581 |
[32m[20221213 12:17:30 @agent_ppo2.py:179][0m |          -0.0258 |           0.5687 |           0.3585 |
[32m[20221213 12:17:30 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:17:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.46
[32m[20221213 12:17:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 58.52
[32m[20221213 12:17:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.29
[32m[20221213 12:17:30 @agent_ppo2.py:137][0m Total time:       1.55 min
[32m[20221213 12:17:30 @agent_ppo2.py:139][0m 104448 total steps have happened
[32m[20221213 12:17:30 @agent_ppo2.py:115][0m #------------------------ Iteration 51 --------------------------#
[32m[20221213 12:17:30 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:30 @agent_ppo2.py:179][0m |           0.0064 |           0.7866 |           0.3657 |
[32m[20221213 12:17:30 @agent_ppo2.py:179][0m |          -0.0081 |           0.7489 |           0.3644 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0139 |           0.7373 |           0.3661 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0195 |           0.7269 |           0.3667 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0134 |           0.7574 |           0.3674 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0135 |           0.7188 |           0.3662 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0253 |           0.7090 |           0.3687 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0208 |           0.7454 |           0.3695 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0232 |           0.7043 |           0.3678 |
[32m[20221213 12:17:31 @agent_ppo2.py:179][0m |          -0.0336 |           0.6987 |           0.3702 |
[32m[20221213 12:17:31 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:17:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 57.48
[32m[20221213 12:17:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.40
[32m[20221213 12:17:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.57
[32m[20221213 12:17:32 @agent_ppo2.py:137][0m Total time:       1.58 min
[32m[20221213 12:17:32 @agent_ppo2.py:139][0m 106496 total steps have happened
[32m[20221213 12:17:32 @agent_ppo2.py:115][0m #------------------------ Iteration 52 --------------------------#
[32m[20221213 12:17:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:32 @agent_ppo2.py:179][0m |           0.0088 |           1.2997 |           0.3733 |
[32m[20221213 12:17:32 @agent_ppo2.py:179][0m |          -0.0097 |           1.2699 |           0.3732 |
[32m[20221213 12:17:32 @agent_ppo2.py:179][0m |          -0.0141 |           1.2421 |           0.3733 |
[32m[20221213 12:17:32 @agent_ppo2.py:179][0m |          -0.0181 |           1.2322 |           0.3738 |
[32m[20221213 12:17:32 @agent_ppo2.py:179][0m |          -0.0189 |           1.2256 |           0.3746 |
[32m[20221213 12:17:33 @agent_ppo2.py:179][0m |          -0.0158 |           1.3798 |           0.3743 |
[32m[20221213 12:17:33 @agent_ppo2.py:179][0m |          -0.0244 |           1.2287 |           0.3745 |
[32m[20221213 12:17:33 @agent_ppo2.py:179][0m |          -0.0160 |           1.3821 |           0.3748 |
[32m[20221213 12:17:33 @agent_ppo2.py:179][0m |          -0.0299 |           1.2379 |           0.3754 |
[32m[20221213 12:17:33 @agent_ppo2.py:179][0m |          -0.0373 |           1.2112 |           0.3749 |
[32m[20221213 12:17:33 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:17:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.76
[32m[20221213 12:17:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.06
[32m[20221213 12:17:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.31
[32m[20221213 12:17:33 @agent_ppo2.py:137][0m Total time:       1.61 min
[32m[20221213 12:17:33 @agent_ppo2.py:139][0m 108544 total steps have happened
[32m[20221213 12:17:33 @agent_ppo2.py:115][0m #------------------------ Iteration 53 --------------------------#
[32m[20221213 12:17:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:34 @agent_ppo2.py:179][0m |           0.0109 |           0.6944 |           0.3716 |
[32m[20221213 12:17:34 @agent_ppo2.py:179][0m |          -0.0114 |           0.6700 |           0.3709 |
[32m[20221213 12:17:34 @agent_ppo2.py:179][0m |          -0.0040 |           0.6650 |           0.3720 |
[32m[20221213 12:17:34 @agent_ppo2.py:179][0m |          -0.0219 |           0.6542 |           0.3714 |
[32m[20221213 12:17:34 @agent_ppo2.py:179][0m |          -0.0299 |           0.6510 |           0.3720 |
[32m[20221213 12:17:34 @agent_ppo2.py:179][0m |          -0.0222 |           0.6449 |           0.3718 |
[32m[20221213 12:17:34 @agent_ppo2.py:179][0m |          -0.0311 |           0.6413 |           0.3721 |
[32m[20221213 12:17:35 @agent_ppo2.py:179][0m |          -0.0338 |           0.6387 |           0.3720 |
[32m[20221213 12:17:35 @agent_ppo2.py:179][0m |          -0.0335 |           0.6326 |           0.3723 |
[32m[20221213 12:17:35 @agent_ppo2.py:179][0m |          -0.0357 |           0.6312 |           0.3723 |
[32m[20221213 12:17:35 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:17:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 61.30
[32m[20221213 12:17:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.32
[32m[20221213 12:17:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.33
[32m[20221213 12:17:35 @agent_ppo2.py:137][0m Total time:       1.64 min
[32m[20221213 12:17:35 @agent_ppo2.py:139][0m 110592 total steps have happened
[32m[20221213 12:17:35 @agent_ppo2.py:115][0m #------------------------ Iteration 54 --------------------------#
[32m[20221213 12:17:35 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:35 @agent_ppo2.py:179][0m |           0.0074 |           0.3771 |           0.3804 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0039 |           0.3503 |           0.3810 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0114 |           0.3415 |           0.3788 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0147 |           0.3377 |           0.3791 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0185 |           0.3346 |           0.3778 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0210 |           0.3336 |           0.3781 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0212 |           0.3316 |           0.3778 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0256 |           0.3316 |           0.3777 |
[32m[20221213 12:17:36 @agent_ppo2.py:179][0m |          -0.0182 |           0.3340 |           0.3778 |
[32m[20221213 12:17:37 @agent_ppo2.py:179][0m |          -0.0261 |           0.3273 |           0.3778 |
[32m[20221213 12:17:37 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:17:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.95
[32m[20221213 12:17:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 49.77
[32m[20221213 12:17:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.57
[32m[20221213 12:17:37 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 81.57
[32m[20221213 12:17:37 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 81.57
[32m[20221213 12:17:37 @agent_ppo2.py:137][0m Total time:       1.67 min
[32m[20221213 12:17:37 @agent_ppo2.py:139][0m 112640 total steps have happened
[32m[20221213 12:17:37 @agent_ppo2.py:115][0m #------------------------ Iteration 55 --------------------------#
[32m[20221213 12:17:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:37 @agent_ppo2.py:179][0m |           0.0183 |           1.1207 |           0.3659 |
[32m[20221213 12:17:37 @agent_ppo2.py:179][0m |           0.0109 |           1.0039 |           0.3636 |
[32m[20221213 12:17:37 @agent_ppo2.py:179][0m |          -0.0182 |           0.9855 |           0.3639 |
[32m[20221213 12:17:38 @agent_ppo2.py:179][0m |          -0.0232 |           0.9764 |           0.3646 |
[32m[20221213 12:17:38 @agent_ppo2.py:179][0m |          -0.0243 |           0.9637 |           0.3645 |
[32m[20221213 12:17:38 @agent_ppo2.py:179][0m |          -0.0283 |           0.9618 |           0.3646 |
[32m[20221213 12:17:38 @agent_ppo2.py:179][0m |          -0.0322 |           0.9611 |           0.3654 |
[32m[20221213 12:17:38 @agent_ppo2.py:179][0m |          -0.0303 |           0.9600 |           0.3656 |
[32m[20221213 12:17:38 @agent_ppo2.py:179][0m |          -0.0348 |           0.9529 |           0.3650 |
[32m[20221213 12:17:38 @agent_ppo2.py:179][0m |          -0.0359 |           0.9447 |           0.3662 |
[32m[20221213 12:17:38 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:17:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.76
[32m[20221213 12:17:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.38
[32m[20221213 12:17:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 47.36
[32m[20221213 12:17:38 @agent_ppo2.py:137][0m Total time:       1.70 min
[32m[20221213 12:17:38 @agent_ppo2.py:139][0m 114688 total steps have happened
[32m[20221213 12:17:38 @agent_ppo2.py:115][0m #------------------------ Iteration 56 --------------------------#
[32m[20221213 12:17:39 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:39 @agent_ppo2.py:179][0m |           0.0025 |           0.3843 |           0.3744 |
[32m[20221213 12:17:39 @agent_ppo2.py:179][0m |          -0.0067 |           0.3662 |           0.3741 |
[32m[20221213 12:17:39 @agent_ppo2.py:179][0m |          -0.0109 |           0.3610 |           0.3760 |
[32m[20221213 12:17:39 @agent_ppo2.py:179][0m |          -0.0155 |           0.3591 |           0.3749 |
[32m[20221213 12:17:39 @agent_ppo2.py:179][0m |          -0.0141 |           0.3552 |           0.3757 |
[32m[20221213 12:17:39 @agent_ppo2.py:179][0m |          -0.0164 |           0.3537 |           0.3752 |
[32m[20221213 12:17:40 @agent_ppo2.py:179][0m |          -0.0265 |           0.3527 |           0.3739 |
[32m[20221213 12:17:40 @agent_ppo2.py:179][0m |          -0.0143 |           0.3513 |           0.3736 |
[32m[20221213 12:17:40 @agent_ppo2.py:179][0m |          -0.0230 |           0.3508 |           0.3739 |
[32m[20221213 12:17:40 @agent_ppo2.py:179][0m |          -0.0225 |           0.3505 |           0.3732 |
[32m[20221213 12:17:40 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:17:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.08
[32m[20221213 12:17:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 47.92
[32m[20221213 12:17:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.52
[32m[20221213 12:17:40 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 96.52
[32m[20221213 12:17:40 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 96.52
[32m[20221213 12:17:40 @agent_ppo2.py:137][0m Total time:       1.72 min
[32m[20221213 12:17:40 @agent_ppo2.py:139][0m 116736 total steps have happened
[32m[20221213 12:17:40 @agent_ppo2.py:115][0m #------------------------ Iteration 57 --------------------------#
[32m[20221213 12:17:40 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |           0.0003 |           0.3603 |           0.3788 |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |          -0.0079 |           0.3587 |           0.3804 |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |          -0.0114 |           0.3550 |           0.3795 |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |          -0.0146 |           0.3561 |           0.3797 |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |          -0.0147 |           0.3541 |           0.3795 |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |          -0.0156 |           0.3559 |           0.3803 |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |          -0.0178 |           0.3542 |           0.3798 |
[32m[20221213 12:17:41 @agent_ppo2.py:179][0m |          -0.0183 |           0.3524 |           0.3793 |
[32m[20221213 12:17:42 @agent_ppo2.py:179][0m |          -0.0205 |           0.3544 |           0.3785 |
[32m[20221213 12:17:42 @agent_ppo2.py:179][0m |          -0.0197 |           0.3513 |           0.3788 |
[32m[20221213 12:17:42 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:17:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.30
[32m[20221213 12:17:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.85
[32m[20221213 12:17:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.78
[32m[20221213 12:17:42 @agent_ppo2.py:137][0m Total time:       1.75 min
[32m[20221213 12:17:42 @agent_ppo2.py:139][0m 118784 total steps have happened
[32m[20221213 12:17:42 @agent_ppo2.py:115][0m #------------------------ Iteration 58 --------------------------#
[32m[20221213 12:17:42 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:42 @agent_ppo2.py:179][0m |           0.0048 |           0.3888 |           0.3615 |
[32m[20221213 12:17:42 @agent_ppo2.py:179][0m |          -0.0065 |           0.3876 |           0.3600 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0077 |           0.3869 |           0.3593 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0122 |           0.3845 |           0.3575 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0085 |           0.3901 |           0.3563 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0128 |           0.3870 |           0.3563 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0157 |           0.3844 |           0.3553 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0184 |           0.3859 |           0.3548 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0202 |           0.3866 |           0.3539 |
[32m[20221213 12:17:43 @agent_ppo2.py:179][0m |          -0.0189 |           0.3833 |           0.3535 |
[32m[20221213 12:17:43 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:17:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.30
[32m[20221213 12:17:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 48.38
[32m[20221213 12:17:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.93
[32m[20221213 12:17:44 @agent_ppo2.py:137][0m Total time:       1.78 min
[32m[20221213 12:17:44 @agent_ppo2.py:139][0m 120832 total steps have happened
[32m[20221213 12:17:44 @agent_ppo2.py:115][0m #------------------------ Iteration 59 --------------------------#
[32m[20221213 12:17:44 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:44 @agent_ppo2.py:179][0m |           0.0029 |           0.4408 |           0.3487 |
[32m[20221213 12:17:44 @agent_ppo2.py:179][0m |          -0.0098 |           0.4305 |           0.3481 |
[32m[20221213 12:17:44 @agent_ppo2.py:179][0m |          -0.0145 |           0.4261 |           0.3466 |
[32m[20221213 12:17:44 @agent_ppo2.py:179][0m |          -0.0198 |           0.4264 |           0.3462 |
[32m[20221213 12:17:44 @agent_ppo2.py:179][0m |          -0.0207 |           0.4191 |           0.3463 |
[32m[20221213 12:17:45 @agent_ppo2.py:179][0m |          -0.0240 |           0.4183 |           0.3445 |
[32m[20221213 12:17:45 @agent_ppo2.py:179][0m |          -0.0231 |           0.4189 |           0.3447 |
[32m[20221213 12:17:45 @agent_ppo2.py:179][0m |          -0.0262 |           0.4152 |           0.3439 |
[32m[20221213 12:17:45 @agent_ppo2.py:179][0m |          -0.0240 |           0.4137 |           0.3440 |
[32m[20221213 12:17:45 @agent_ppo2.py:179][0m |          -0.0245 |           0.4120 |           0.3443 |
[32m[20221213 12:17:45 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:17:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 47.64
[32m[20221213 12:17:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.75
[32m[20221213 12:17:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.13
[32m[20221213 12:17:45 @agent_ppo2.py:137][0m Total time:       1.81 min
[32m[20221213 12:17:45 @agent_ppo2.py:139][0m 122880 total steps have happened
[32m[20221213 12:17:45 @agent_ppo2.py:115][0m #------------------------ Iteration 60 --------------------------#
[32m[20221213 12:17:46 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:17:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:46 @agent_ppo2.py:179][0m |           0.0008 |           0.4697 |           0.3484 |
[32m[20221213 12:17:46 @agent_ppo2.py:179][0m |          -0.0096 |           0.4466 |           0.3488 |
[32m[20221213 12:17:46 @agent_ppo2.py:179][0m |          -0.0124 |           0.4382 |           0.3492 |
[32m[20221213 12:17:46 @agent_ppo2.py:179][0m |          -0.0190 |           0.4372 |           0.3505 |
[32m[20221213 12:17:46 @agent_ppo2.py:179][0m |          -0.0197 |           0.4339 |           0.3517 |
[32m[20221213 12:17:46 @agent_ppo2.py:179][0m |          -0.0261 |           0.4337 |           0.3528 |
[32m[20221213 12:17:47 @agent_ppo2.py:179][0m |          -0.0262 |           0.4329 |           0.3521 |
[32m[20221213 12:17:47 @agent_ppo2.py:179][0m |          -0.0240 |           0.4299 |           0.3536 |
[32m[20221213 12:17:47 @agent_ppo2.py:179][0m |          -0.0286 |           0.4289 |           0.3549 |
[32m[20221213 12:17:47 @agent_ppo2.py:179][0m |          -0.0319 |           0.4288 |           0.3554 |
[32m[20221213 12:17:47 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:17:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.86
[32m[20221213 12:17:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.75
[32m[20221213 12:17:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.67
[32m[20221213 12:17:47 @agent_ppo2.py:137][0m Total time:       1.84 min
[32m[20221213 12:17:47 @agent_ppo2.py:139][0m 124928 total steps have happened
[32m[20221213 12:17:47 @agent_ppo2.py:115][0m #------------------------ Iteration 61 --------------------------#
[32m[20221213 12:17:47 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |           0.0039 |           0.7133 |           0.3601 |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |          -0.0137 |           0.6821 |           0.3625 |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |          -0.0169 |           0.6756 |           0.3630 |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |          -0.0264 |           0.6762 |           0.3627 |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |          -0.0118 |           0.7087 |           0.3628 |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |          -0.0178 |           0.7121 |           0.3628 |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |          -0.0284 |           0.6749 |           0.3648 |
[32m[20221213 12:17:48 @agent_ppo2.py:179][0m |          -0.0331 |           0.6663 |           0.3644 |
[32m[20221213 12:17:49 @agent_ppo2.py:179][0m |          -0.0362 |           0.6869 |           0.3657 |
[32m[20221213 12:17:49 @agent_ppo2.py:179][0m |          -0.0359 |           0.6587 |           0.3653 |
[32m[20221213 12:17:49 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:17:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.92
[32m[20221213 12:17:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.87
[32m[20221213 12:17:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.56
[32m[20221213 12:17:49 @agent_ppo2.py:137][0m Total time:       1.87 min
[32m[20221213 12:17:49 @agent_ppo2.py:139][0m 126976 total steps have happened
[32m[20221213 12:17:49 @agent_ppo2.py:115][0m #------------------------ Iteration 62 --------------------------#
[32m[20221213 12:17:49 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:49 @agent_ppo2.py:179][0m |           0.0117 |           0.5987 |           0.3681 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |           0.0025 |           0.6116 |           0.3693 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |          -0.0075 |           0.5756 |           0.3670 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |          -0.0173 |           0.5666 |           0.3699 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |          -0.0155 |           0.6007 |           0.3707 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |          -0.0187 |           0.5612 |           0.3710 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |          -0.0332 |           0.5612 |           0.3730 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |          -0.0274 |           0.5566 |           0.3736 |
[32m[20221213 12:17:50 @agent_ppo2.py:179][0m |          -0.0328 |           0.5570 |           0.3751 |
[32m[20221213 12:17:51 @agent_ppo2.py:179][0m |          -0.0366 |           0.5530 |           0.3763 |
[32m[20221213 12:17:51 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:17:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 56.89
[32m[20221213 12:17:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.15
[32m[20221213 12:17:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 57.31
[32m[20221213 12:17:51 @agent_ppo2.py:137][0m Total time:       1.90 min
[32m[20221213 12:17:51 @agent_ppo2.py:139][0m 129024 total steps have happened
[32m[20221213 12:17:51 @agent_ppo2.py:115][0m #------------------------ Iteration 63 --------------------------#
[32m[20221213 12:17:51 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:51 @agent_ppo2.py:179][0m |          -0.0017 |           0.4744 |           0.3883 |
[32m[20221213 12:17:51 @agent_ppo2.py:179][0m |          -0.0081 |           0.4674 |           0.3874 |
[32m[20221213 12:17:51 @agent_ppo2.py:179][0m |          -0.0130 |           0.4626 |           0.3884 |
[32m[20221213 12:17:52 @agent_ppo2.py:179][0m |          -0.0132 |           0.4598 |           0.3885 |
[32m[20221213 12:17:52 @agent_ppo2.py:179][0m |          -0.0175 |           0.4599 |           0.3904 |
[32m[20221213 12:17:52 @agent_ppo2.py:179][0m |          -0.0209 |           0.4582 |           0.3893 |
[32m[20221213 12:17:52 @agent_ppo2.py:179][0m |          -0.0215 |           0.4598 |           0.3894 |
[32m[20221213 12:17:52 @agent_ppo2.py:179][0m |          -0.0219 |           0.4595 |           0.3911 |
[32m[20221213 12:17:52 @agent_ppo2.py:179][0m |          -0.0225 |           0.4641 |           0.3899 |
[32m[20221213 12:17:52 @agent_ppo2.py:179][0m |          -0.0256 |           0.4567 |           0.3925 |
[32m[20221213 12:17:52 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:17:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 52.01
[32m[20221213 12:17:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.18
[32m[20221213 12:17:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 54.94
[32m[20221213 12:17:53 @agent_ppo2.py:137][0m Total time:       1.93 min
[32m[20221213 12:17:53 @agent_ppo2.py:139][0m 131072 total steps have happened
[32m[20221213 12:17:53 @agent_ppo2.py:115][0m #------------------------ Iteration 64 --------------------------#
[32m[20221213 12:17:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:53 @agent_ppo2.py:179][0m |           0.0076 |           0.6698 |           0.3921 |
[32m[20221213 12:17:53 @agent_ppo2.py:179][0m |          -0.0065 |           0.6457 |           0.3930 |
[32m[20221213 12:17:53 @agent_ppo2.py:179][0m |          -0.0145 |           0.6469 |           0.3933 |
[32m[20221213 12:17:53 @agent_ppo2.py:179][0m |          -0.0268 |           0.6380 |           0.3936 |
[32m[20221213 12:17:53 @agent_ppo2.py:179][0m |          -0.0277 |           0.6320 |           0.3942 |
[32m[20221213 12:17:54 @agent_ppo2.py:179][0m |          -0.0289 |           0.6282 |           0.3953 |
[32m[20221213 12:17:54 @agent_ppo2.py:179][0m |          -0.0342 |           0.6272 |           0.3958 |
[32m[20221213 12:17:54 @agent_ppo2.py:179][0m |          -0.0329 |           0.6209 |           0.3962 |
[32m[20221213 12:17:54 @agent_ppo2.py:179][0m |          -0.0417 |           0.6218 |           0.3962 |
[32m[20221213 12:17:54 @agent_ppo2.py:179][0m |          -0.0394 |           0.6185 |           0.3963 |
[32m[20221213 12:17:54 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:17:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.36
[32m[20221213 12:17:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 79.41
[32m[20221213 12:17:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.92
[32m[20221213 12:17:54 @agent_ppo2.py:137][0m Total time:       1.96 min
[32m[20221213 12:17:54 @agent_ppo2.py:139][0m 133120 total steps have happened
[32m[20221213 12:17:54 @agent_ppo2.py:115][0m #------------------------ Iteration 65 --------------------------#
[32m[20221213 12:17:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:17:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:55 @agent_ppo2.py:179][0m |           0.0088 |           0.9132 |           0.4148 |
[32m[20221213 12:17:55 @agent_ppo2.py:179][0m |          -0.0018 |           0.9022 |           0.4141 |
[32m[20221213 12:17:55 @agent_ppo2.py:179][0m |          -0.0082 |           0.8734 |           0.4106 |
[32m[20221213 12:17:55 @agent_ppo2.py:179][0m |          -0.0195 |           0.8643 |           0.4116 |
[32m[20221213 12:17:55 @agent_ppo2.py:179][0m |          -0.0216 |           0.8679 |           0.4117 |
[32m[20221213 12:17:55 @agent_ppo2.py:179][0m |          -0.0283 |           0.8604 |           0.4114 |
[32m[20221213 12:17:56 @agent_ppo2.py:179][0m |          -0.0316 |           0.8513 |           0.4119 |
[32m[20221213 12:17:56 @agent_ppo2.py:179][0m |          -0.0231 |           0.9198 |           0.4118 |
[32m[20221213 12:17:56 @agent_ppo2.py:179][0m |          -0.0283 |           0.8489 |           0.4116 |
[32m[20221213 12:17:56 @agent_ppo2.py:179][0m |          -0.0338 |           0.8381 |           0.4113 |
[32m[20221213 12:17:56 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:17:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.33
[32m[20221213 12:17:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.52
[32m[20221213 12:17:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.22
[32m[20221213 12:17:56 @agent_ppo2.py:137][0m Total time:       1.99 min
[32m[20221213 12:17:56 @agent_ppo2.py:139][0m 135168 total steps have happened
[32m[20221213 12:17:56 @agent_ppo2.py:115][0m #------------------------ Iteration 66 --------------------------#
[32m[20221213 12:17:56 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |           0.0040 |           0.7477 |           0.4161 |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |          -0.0096 |           0.7216 |           0.4195 |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |          -0.0098 |           0.7443 |           0.4184 |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |          -0.0188 |           0.7051 |           0.4155 |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |          -0.0277 |           0.6996 |           0.4188 |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |          -0.0162 |           0.7482 |           0.4180 |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |          -0.0260 |           0.7036 |           0.4169 |
[32m[20221213 12:17:57 @agent_ppo2.py:179][0m |          -0.0342 |           0.6972 |           0.4190 |
[32m[20221213 12:17:58 @agent_ppo2.py:179][0m |          -0.0350 |           0.6958 |           0.4181 |
[32m[20221213 12:17:58 @agent_ppo2.py:179][0m |          -0.0316 |           0.6965 |           0.4185 |
[32m[20221213 12:17:58 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:17:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.74
[32m[20221213 12:17:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.11
[32m[20221213 12:17:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.94
[32m[20221213 12:17:58 @agent_ppo2.py:137][0m Total time:       2.02 min
[32m[20221213 12:17:58 @agent_ppo2.py:139][0m 137216 total steps have happened
[32m[20221213 12:17:58 @agent_ppo2.py:115][0m #------------------------ Iteration 67 --------------------------#
[32m[20221213 12:17:58 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:17:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:17:58 @agent_ppo2.py:179][0m |           0.0086 |           0.6305 |           0.3992 |
[32m[20221213 12:17:58 @agent_ppo2.py:179][0m |          -0.0001 |           0.6198 |           0.3984 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0087 |           0.6153 |           0.3992 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0107 |           0.6171 |           0.3990 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0163 |           0.6183 |           0.4000 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0131 |           0.6343 |           0.3987 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0133 |           0.6584 |           0.3996 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0200 |           0.6198 |           0.3969 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0237 |           0.6188 |           0.3994 |
[32m[20221213 12:17:59 @agent_ppo2.py:179][0m |          -0.0239 |           0.6121 |           0.3993 |
[32m[20221213 12:17:59 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:18:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 55.44
[32m[20221213 12:18:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 61.30
[32m[20221213 12:18:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.79
[32m[20221213 12:18:00 @agent_ppo2.py:137][0m Total time:       2.05 min
[32m[20221213 12:18:00 @agent_ppo2.py:139][0m 139264 total steps have happened
[32m[20221213 12:18:00 @agent_ppo2.py:115][0m #------------------------ Iteration 68 --------------------------#
[32m[20221213 12:18:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:00 @agent_ppo2.py:179][0m |           0.0101 |           0.6794 |           0.4078 |
[32m[20221213 12:18:00 @agent_ppo2.py:179][0m |          -0.0133 |           0.6547 |           0.4076 |
[32m[20221213 12:18:00 @agent_ppo2.py:179][0m |          -0.0173 |           0.6547 |           0.4073 |
[32m[20221213 12:18:00 @agent_ppo2.py:179][0m |          -0.0235 |           0.6494 |           0.4077 |
[32m[20221213 12:18:01 @agent_ppo2.py:179][0m |          -0.0203 |           0.6502 |           0.4074 |
[32m[20221213 12:18:01 @agent_ppo2.py:179][0m |          -0.0249 |           0.6434 |           0.4065 |
[32m[20221213 12:18:01 @agent_ppo2.py:179][0m |          -0.0319 |           0.6396 |           0.4083 |
[32m[20221213 12:18:01 @agent_ppo2.py:179][0m |          -0.0347 |           0.6365 |           0.4086 |
[32m[20221213 12:18:01 @agent_ppo2.py:179][0m |          -0.0274 |           0.6772 |           0.4096 |
[32m[20221213 12:18:01 @agent_ppo2.py:179][0m |          -0.0251 |           0.6445 |           0.4075 |
[32m[20221213 12:18:01 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:18:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.72
[32m[20221213 12:18:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.01
[32m[20221213 12:18:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.16
[32m[20221213 12:18:01 @agent_ppo2.py:137][0m Total time:       2.08 min
[32m[20221213 12:18:01 @agent_ppo2.py:139][0m 141312 total steps have happened
[32m[20221213 12:18:01 @agent_ppo2.py:115][0m #------------------------ Iteration 69 --------------------------#
[32m[20221213 12:18:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:02 @agent_ppo2.py:179][0m |           0.0101 |           0.7134 |           0.4213 |
[32m[20221213 12:18:02 @agent_ppo2.py:179][0m |          -0.0043 |           0.6703 |           0.4177 |
[32m[20221213 12:18:02 @agent_ppo2.py:179][0m |          -0.0054 |           0.6927 |           0.4191 |
[32m[20221213 12:18:02 @agent_ppo2.py:179][0m |          -0.0192 |           0.6583 |           0.4193 |
[32m[20221213 12:18:02 @agent_ppo2.py:179][0m |          -0.0105 |           0.7174 |           0.4195 |
[32m[20221213 12:18:02 @agent_ppo2.py:179][0m |          -0.0162 |           0.6883 |           0.4199 |
[32m[20221213 12:18:03 @agent_ppo2.py:179][0m |          -0.0205 |           0.6556 |           0.4196 |
[32m[20221213 12:18:03 @agent_ppo2.py:179][0m |          -0.0219 |           0.6555 |           0.4199 |
[32m[20221213 12:18:03 @agent_ppo2.py:179][0m |          -0.0305 |           0.6484 |           0.4194 |
[32m[20221213 12:18:03 @agent_ppo2.py:179][0m |          -0.0354 |           0.6437 |           0.4202 |
[32m[20221213 12:18:03 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:18:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 60.22
[32m[20221213 12:18:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 62.06
[32m[20221213 12:18:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.47
[32m[20221213 12:18:03 @agent_ppo2.py:137][0m Total time:       2.11 min
[32m[20221213 12:18:03 @agent_ppo2.py:139][0m 143360 total steps have happened
[32m[20221213 12:18:03 @agent_ppo2.py:115][0m #------------------------ Iteration 70 --------------------------#
[32m[20221213 12:18:03 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:18:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |           0.0068 |           0.8377 |           0.4076 |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |          -0.0119 |           0.8129 |           0.4075 |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |          -0.0199 |           0.8023 |           0.4072 |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |          -0.0290 |           0.8021 |           0.4064 |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |          -0.0287 |           0.7918 |           0.4063 |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |          -0.0264 |           0.7905 |           0.4072 |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |          -0.0349 |           0.7831 |           0.4072 |
[32m[20221213 12:18:04 @agent_ppo2.py:179][0m |          -0.0366 |           0.7788 |           0.4091 |
[32m[20221213 12:18:05 @agent_ppo2.py:179][0m |          -0.0343 |           0.7897 |           0.4091 |
[32m[20221213 12:18:05 @agent_ppo2.py:179][0m |          -0.0397 |           0.7783 |           0.4091 |
[32m[20221213 12:18:05 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:18:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.54
[32m[20221213 12:18:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.52
[32m[20221213 12:18:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 45.33
[32m[20221213 12:18:05 @agent_ppo2.py:137][0m Total time:       2.14 min
[32m[20221213 12:18:05 @agent_ppo2.py:139][0m 145408 total steps have happened
[32m[20221213 12:18:05 @agent_ppo2.py:115][0m #------------------------ Iteration 71 --------------------------#
[32m[20221213 12:18:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:05 @agent_ppo2.py:179][0m |           0.0111 |           0.9472 |           0.4066 |
[32m[20221213 12:18:05 @agent_ppo2.py:179][0m |          -0.0170 |           0.9156 |           0.4051 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0161 |           0.9161 |           0.4056 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0296 |           0.8972 |           0.4061 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0378 |           0.8878 |           0.4063 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0324 |           0.8880 |           0.4067 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0320 |           0.8922 |           0.4074 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0415 |           0.8719 |           0.4080 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0413 |           0.8646 |           0.4090 |
[32m[20221213 12:18:06 @agent_ppo2.py:179][0m |          -0.0486 |           0.8664 |           0.4096 |
[32m[20221213 12:18:06 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:18:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.08
[32m[20221213 12:18:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.80
[32m[20221213 12:18:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.51
[32m[20221213 12:18:07 @agent_ppo2.py:137][0m Total time:       2.16 min
[32m[20221213 12:18:07 @agent_ppo2.py:139][0m 147456 total steps have happened
[32m[20221213 12:18:07 @agent_ppo2.py:115][0m #------------------------ Iteration 72 --------------------------#
[32m[20221213 12:18:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:07 @agent_ppo2.py:179][0m |           0.0040 |           0.7719 |           0.4246 |
[32m[20221213 12:18:07 @agent_ppo2.py:179][0m |          -0.0097 |           0.7427 |           0.4253 |
[32m[20221213 12:18:07 @agent_ppo2.py:179][0m |          -0.0098 |           0.8144 |           0.4258 |
[32m[20221213 12:18:07 @agent_ppo2.py:179][0m |          -0.0227 |           0.7401 |           0.4255 |
[32m[20221213 12:18:08 @agent_ppo2.py:179][0m |          -0.0299 |           0.7280 |           0.4264 |
[32m[20221213 12:18:08 @agent_ppo2.py:179][0m |          -0.0295 |           0.7284 |           0.4271 |
[32m[20221213 12:18:08 @agent_ppo2.py:179][0m |          -0.0294 |           0.7176 |           0.4279 |
[32m[20221213 12:18:08 @agent_ppo2.py:179][0m |          -0.0319 |           0.7162 |           0.4271 |
[32m[20221213 12:18:08 @agent_ppo2.py:179][0m |          -0.0350 |           0.7175 |           0.4279 |
[32m[20221213 12:18:08 @agent_ppo2.py:179][0m |          -0.0375 |           0.7172 |           0.4290 |
[32m[20221213 12:18:08 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:18:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.29
[32m[20221213 12:18:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 63.36
[32m[20221213 12:18:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 76.03
[32m[20221213 12:18:08 @agent_ppo2.py:137][0m Total time:       2.19 min
[32m[20221213 12:18:08 @agent_ppo2.py:139][0m 149504 total steps have happened
[32m[20221213 12:18:08 @agent_ppo2.py:115][0m #------------------------ Iteration 73 --------------------------#
[32m[20221213 12:18:09 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:18:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:09 @agent_ppo2.py:179][0m |           0.0036 |           0.7704 |           0.4247 |
[32m[20221213 12:18:09 @agent_ppo2.py:179][0m |          -0.0146 |           0.7502 |           0.4229 |
[32m[20221213 12:18:09 @agent_ppo2.py:179][0m |          -0.0075 |           0.8209 |           0.4209 |
[32m[20221213 12:18:09 @agent_ppo2.py:179][0m |          -0.0163 |           0.7452 |           0.4191 |
[32m[20221213 12:18:09 @agent_ppo2.py:179][0m |          -0.0275 |           0.7413 |           0.4212 |
[32m[20221213 12:18:09 @agent_ppo2.py:179][0m |          -0.0296 |           0.7382 |           0.4223 |
[32m[20221213 12:18:10 @agent_ppo2.py:179][0m |          -0.0283 |           0.7605 |           0.4239 |
[32m[20221213 12:18:10 @agent_ppo2.py:179][0m |          -0.0269 |           0.7503 |           0.4232 |
[32m[20221213 12:18:10 @agent_ppo2.py:179][0m |          -0.0331 |           0.7328 |           0.4247 |
[32m[20221213 12:18:10 @agent_ppo2.py:179][0m |          -0.0364 |           0.7332 |           0.4245 |
[32m[20221213 12:18:10 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:18:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 63.68
[32m[20221213 12:18:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 65.88
[32m[20221213 12:18:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.40
[32m[20221213 12:18:10 @agent_ppo2.py:137][0m Total time:       2.22 min
[32m[20221213 12:18:10 @agent_ppo2.py:139][0m 151552 total steps have happened
[32m[20221213 12:18:10 @agent_ppo2.py:115][0m #------------------------ Iteration 74 --------------------------#
[32m[20221213 12:18:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:11 @agent_ppo2.py:179][0m |           0.0102 |           0.8468 |           0.4394 |
[32m[20221213 12:18:11 @agent_ppo2.py:179][0m |          -0.0050 |           0.8307 |           0.4401 |
[32m[20221213 12:18:11 @agent_ppo2.py:179][0m |          -0.0190 |           0.8207 |           0.4406 |
[32m[20221213 12:18:11 @agent_ppo2.py:179][0m |          -0.0080 |           0.8740 |           0.4414 |
[32m[20221213 12:18:11 @agent_ppo2.py:179][0m |          -0.0038 |           0.8136 |           0.4318 |
[32m[20221213 12:18:11 @agent_ppo2.py:179][0m |          -0.0251 |           0.7967 |           0.4411 |
[32m[20221213 12:18:11 @agent_ppo2.py:179][0m |          -0.0287 |           0.7973 |           0.4416 |
[32m[20221213 12:18:12 @agent_ppo2.py:179][0m |          -0.0337 |           0.7916 |           0.4435 |
[32m[20221213 12:18:12 @agent_ppo2.py:179][0m |          -0.0338 |           0.7879 |           0.4436 |
[32m[20221213 12:18:12 @agent_ppo2.py:179][0m |          -0.0364 |           0.7887 |           0.4435 |
[32m[20221213 12:18:12 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:18:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.24
[32m[20221213 12:18:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.75
[32m[20221213 12:18:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.46
[32m[20221213 12:18:12 @agent_ppo2.py:137][0m Total time:       2.25 min
[32m[20221213 12:18:12 @agent_ppo2.py:139][0m 153600 total steps have happened
[32m[20221213 12:18:12 @agent_ppo2.py:115][0m #------------------------ Iteration 75 --------------------------#
[32m[20221213 12:18:12 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:18:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:12 @agent_ppo2.py:179][0m |           0.0164 |           0.8263 |           0.4464 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0097 |           0.7909 |           0.4497 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0104 |           0.8246 |           0.4535 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0217 |           0.7944 |           0.4494 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0159 |           0.8079 |           0.4539 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0180 |           0.7815 |           0.4493 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0299 |           0.7791 |           0.4552 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0306 |           0.7816 |           0.4553 |
[32m[20221213 12:18:13 @agent_ppo2.py:179][0m |          -0.0329 |           0.7808 |           0.4577 |
[32m[20221213 12:18:14 @agent_ppo2.py:179][0m |          -0.0311 |           0.7732 |           0.4581 |
[32m[20221213 12:18:14 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:18:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 62.56
[32m[20221213 12:18:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.22
[32m[20221213 12:18:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.36
[32m[20221213 12:18:14 @agent_ppo2.py:137][0m Total time:       2.28 min
[32m[20221213 12:18:14 @agent_ppo2.py:139][0m 155648 total steps have happened
[32m[20221213 12:18:14 @agent_ppo2.py:115][0m #------------------------ Iteration 76 --------------------------#
[32m[20221213 12:18:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:14 @agent_ppo2.py:179][0m |           0.0015 |           0.8855 |           0.4752 |
[32m[20221213 12:18:14 @agent_ppo2.py:179][0m |          -0.0141 |           0.8487 |           0.4716 |
[32m[20221213 12:18:14 @agent_ppo2.py:179][0m |          -0.0243 |           0.8412 |           0.4733 |
[32m[20221213 12:18:15 @agent_ppo2.py:179][0m |          -0.0269 |           0.8345 |           0.4739 |
[32m[20221213 12:18:15 @agent_ppo2.py:179][0m |          -0.0275 |           0.8378 |           0.4756 |
[32m[20221213 12:18:15 @agent_ppo2.py:179][0m |          -0.0192 |           0.8668 |           0.4756 |
[32m[20221213 12:18:15 @agent_ppo2.py:179][0m |          -0.0305 |           0.8363 |           0.4744 |
[32m[20221213 12:18:15 @agent_ppo2.py:179][0m |          -0.0290 |           0.8397 |           0.4776 |
[32m[20221213 12:18:15 @agent_ppo2.py:179][0m |          -0.0409 |           0.8213 |           0.4785 |
[32m[20221213 12:18:15 @agent_ppo2.py:179][0m |          -0.0396 |           0.8163 |           0.4790 |
[32m[20221213 12:18:15 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:18:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.78
[32m[20221213 12:18:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.11
[32m[20221213 12:18:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.09
[32m[20221213 12:18:16 @agent_ppo2.py:137][0m Total time:       2.31 min
[32m[20221213 12:18:16 @agent_ppo2.py:139][0m 157696 total steps have happened
[32m[20221213 12:18:16 @agent_ppo2.py:115][0m #------------------------ Iteration 77 --------------------------#
[32m[20221213 12:18:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:16 @agent_ppo2.py:179][0m |           0.0088 |           0.8345 |           0.4857 |
[32m[20221213 12:18:16 @agent_ppo2.py:179][0m |          -0.0106 |           0.8137 |           0.4877 |
[32m[20221213 12:18:16 @agent_ppo2.py:179][0m |          -0.0145 |           0.8062 |           0.4856 |
[32m[20221213 12:18:16 @agent_ppo2.py:179][0m |          -0.0173 |           0.8574 |           0.4863 |
[32m[20221213 12:18:17 @agent_ppo2.py:179][0m |          -0.0222 |           0.8003 |           0.4873 |
[32m[20221213 12:18:17 @agent_ppo2.py:179][0m |          -0.0270 |           0.7918 |           0.4867 |
[32m[20221213 12:18:17 @agent_ppo2.py:179][0m |          -0.0275 |           0.7885 |           0.4899 |
[32m[20221213 12:18:17 @agent_ppo2.py:179][0m |          -0.0277 |           0.8138 |           0.4889 |
[32m[20221213 12:18:17 @agent_ppo2.py:179][0m |          -0.0342 |           0.7879 |           0.4912 |
[32m[20221213 12:18:17 @agent_ppo2.py:179][0m |          -0.0349 |           0.7851 |           0.4919 |
[32m[20221213 12:18:17 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:18:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.42
[32m[20221213 12:18:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.72
[32m[20221213 12:18:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.01
[32m[20221213 12:18:17 @agent_ppo2.py:137][0m Total time:       2.34 min
[32m[20221213 12:18:17 @agent_ppo2.py:139][0m 159744 total steps have happened
[32m[20221213 12:18:17 @agent_ppo2.py:115][0m #------------------------ Iteration 78 --------------------------#
[32m[20221213 12:18:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:18 @agent_ppo2.py:179][0m |           0.0035 |           1.0652 |           0.4822 |
[32m[20221213 12:18:18 @agent_ppo2.py:179][0m |          -0.0117 |           1.0170 |           0.4817 |
[32m[20221213 12:18:18 @agent_ppo2.py:179][0m |          -0.0187 |           1.0008 |           0.4818 |
[32m[20221213 12:18:18 @agent_ppo2.py:179][0m |          -0.0260 |           0.9963 |           0.4843 |
[32m[20221213 12:18:18 @agent_ppo2.py:179][0m |          -0.0302 |           0.9812 |           0.4848 |
[32m[20221213 12:18:18 @agent_ppo2.py:179][0m |          -0.0331 |           0.9813 |           0.4875 |
[32m[20221213 12:18:19 @agent_ppo2.py:179][0m |          -0.0356 |           0.9767 |           0.4868 |
[32m[20221213 12:18:19 @agent_ppo2.py:179][0m |          -0.0329 |           0.9753 |           0.4875 |
[32m[20221213 12:18:19 @agent_ppo2.py:179][0m |          -0.0389 |           0.9692 |           0.4880 |
[32m[20221213 12:18:19 @agent_ppo2.py:179][0m |          -0.0424 |           0.9655 |           0.4896 |
[32m[20221213 12:18:19 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:18:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.38
[32m[20221213 12:18:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.18
[32m[20221213 12:18:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.83
[32m[20221213 12:18:19 @agent_ppo2.py:137][0m Total time:       2.37 min
[32m[20221213 12:18:19 @agent_ppo2.py:139][0m 161792 total steps have happened
[32m[20221213 12:18:19 @agent_ppo2.py:115][0m #------------------------ Iteration 79 --------------------------#
[32m[20221213 12:18:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |           0.0097 |           0.9085 |           0.4903 |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |          -0.0010 |           0.9042 |           0.4898 |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |          -0.0193 |           0.8838 |           0.4899 |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |          -0.0202 |           0.8798 |           0.4877 |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |          -0.0284 |           0.8738 |           0.4889 |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |          -0.0296 |           0.8697 |           0.4896 |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |          -0.0285 |           0.8755 |           0.4894 |
[32m[20221213 12:18:20 @agent_ppo2.py:179][0m |          -0.0310 |           0.8768 |           0.4906 |
[32m[20221213 12:18:21 @agent_ppo2.py:179][0m |          -0.0381 |           0.8702 |           0.4898 |
[32m[20221213 12:18:21 @agent_ppo2.py:179][0m |          -0.0403 |           0.8655 |           0.4911 |
[32m[20221213 12:18:21 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:18:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.88
[32m[20221213 12:18:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 79.71
[32m[20221213 12:18:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.01
[32m[20221213 12:18:21 @agent_ppo2.py:137][0m Total time:       2.40 min
[32m[20221213 12:18:21 @agent_ppo2.py:139][0m 163840 total steps have happened
[32m[20221213 12:18:21 @agent_ppo2.py:115][0m #------------------------ Iteration 80 --------------------------#
[32m[20221213 12:18:21 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:18:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:21 @agent_ppo2.py:179][0m |           0.0069 |           0.8541 |           0.4995 |
[32m[20221213 12:18:21 @agent_ppo2.py:179][0m |          -0.0043 |           0.8454 |           0.4975 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0148 |           0.8346 |           0.4982 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0154 |           0.8364 |           0.4985 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0228 |           0.8307 |           0.4995 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0260 |           0.8333 |           0.4983 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0313 |           0.8315 |           0.4996 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0292 |           0.8345 |           0.4984 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0332 |           0.8288 |           0.4999 |
[32m[20221213 12:18:22 @agent_ppo2.py:179][0m |          -0.0332 |           0.8267 |           0.4989 |
[32m[20221213 12:18:22 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:18:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 65.15
[32m[20221213 12:18:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.64
[32m[20221213 12:18:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.55
[32m[20221213 12:18:23 @agent_ppo2.py:137][0m Total time:       2.43 min
[32m[20221213 12:18:23 @agent_ppo2.py:139][0m 165888 total steps have happened
[32m[20221213 12:18:23 @agent_ppo2.py:115][0m #------------------------ Iteration 81 --------------------------#
[32m[20221213 12:18:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:23 @agent_ppo2.py:179][0m |           0.0106 |           0.9736 |           0.4989 |
[32m[20221213 12:18:23 @agent_ppo2.py:179][0m |          -0.0105 |           0.9675 |           0.4961 |
[32m[20221213 12:18:23 @agent_ppo2.py:179][0m |          -0.0186 |           0.9490 |           0.4970 |
[32m[20221213 12:18:23 @agent_ppo2.py:179][0m |          -0.0173 |           0.9518 |           0.4958 |
[32m[20221213 12:18:24 @agent_ppo2.py:179][0m |          -0.0304 |           0.9511 |           0.4977 |
[32m[20221213 12:18:24 @agent_ppo2.py:179][0m |          -0.0290 |           0.9705 |           0.4988 |
[32m[20221213 12:18:24 @agent_ppo2.py:179][0m |          -0.0242 |           0.9666 |           0.4969 |
[32m[20221213 12:18:24 @agent_ppo2.py:179][0m |          -0.0286 |           0.9617 |           0.4971 |
[32m[20221213 12:18:24 @agent_ppo2.py:179][0m |          -0.0350 |           0.9456 |           0.5006 |
[32m[20221213 12:18:24 @agent_ppo2.py:179][0m |          -0.0334 |           0.9409 |           0.5009 |
[32m[20221213 12:18:24 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:18:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.87
[32m[20221213 12:18:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 84.20
[32m[20221213 12:18:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.09
[32m[20221213 12:18:24 @agent_ppo2.py:137][0m Total time:       2.46 min
[32m[20221213 12:18:24 @agent_ppo2.py:139][0m 167936 total steps have happened
[32m[20221213 12:18:24 @agent_ppo2.py:115][0m #------------------------ Iteration 82 --------------------------#
[32m[20221213 12:18:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:25 @agent_ppo2.py:179][0m |           0.0086 |           0.8843 |           0.4884 |
[32m[20221213 12:18:25 @agent_ppo2.py:179][0m |          -0.0095 |           0.8772 |           0.4859 |
[32m[20221213 12:18:25 @agent_ppo2.py:179][0m |          -0.0186 |           0.8710 |           0.4879 |
[32m[20221213 12:18:25 @agent_ppo2.py:179][0m |          -0.0207 |           0.8661 |           0.4873 |
[32m[20221213 12:18:25 @agent_ppo2.py:179][0m |          -0.0233 |           0.8655 |           0.4879 |
[32m[20221213 12:18:25 @agent_ppo2.py:179][0m |          -0.0248 |           0.8651 |           0.4876 |
[32m[20221213 12:18:26 @agent_ppo2.py:179][0m |          -0.0259 |           0.8690 |           0.4883 |
[32m[20221213 12:18:26 @agent_ppo2.py:179][0m |          -0.0263 |           0.8598 |           0.4878 |
[32m[20221213 12:18:26 @agent_ppo2.py:179][0m |          -0.0295 |           0.8691 |           0.4880 |
[32m[20221213 12:18:26 @agent_ppo2.py:179][0m |          -0.0324 |           0.8590 |           0.4883 |
[32m[20221213 12:18:26 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:18:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.98
[32m[20221213 12:18:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 66.42
[32m[20221213 12:18:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.94
[32m[20221213 12:18:26 @agent_ppo2.py:137][0m Total time:       2.49 min
[32m[20221213 12:18:26 @agent_ppo2.py:139][0m 169984 total steps have happened
[32m[20221213 12:18:26 @agent_ppo2.py:115][0m #------------------------ Iteration 83 --------------------------#
[32m[20221213 12:18:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:27 @agent_ppo2.py:179][0m |           0.0216 |           0.9595 |           0.5019 |
[32m[20221213 12:18:27 @agent_ppo2.py:179][0m |          -0.0000 |           0.8945 |           0.4937 |
[32m[20221213 12:18:27 @agent_ppo2.py:179][0m |          -0.0172 |           0.8862 |           0.5014 |
[32m[20221213 12:18:27 @agent_ppo2.py:179][0m |          -0.0248 |           0.8838 |           0.5045 |
[32m[20221213 12:18:27 @agent_ppo2.py:179][0m |          -0.0293 |           0.8784 |           0.5066 |
[32m[20221213 12:18:27 @agent_ppo2.py:179][0m |          -0.0305 |           0.8775 |           0.5096 |
[32m[20221213 12:18:27 @agent_ppo2.py:179][0m |          -0.0342 |           0.8723 |           0.5100 |
[32m[20221213 12:18:28 @agent_ppo2.py:179][0m |          -0.0349 |           0.8760 |           0.5113 |
[32m[20221213 12:18:28 @agent_ppo2.py:179][0m |          -0.0363 |           0.8664 |           0.5119 |
[32m[20221213 12:18:28 @agent_ppo2.py:179][0m |          -0.0338 |           0.8743 |           0.5141 |
[32m[20221213 12:18:28 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:18:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.70
[32m[20221213 12:18:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 67.87
[32m[20221213 12:18:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.17
[32m[20221213 12:18:28 @agent_ppo2.py:137][0m Total time:       2.52 min
[32m[20221213 12:18:28 @agent_ppo2.py:139][0m 172032 total steps have happened
[32m[20221213 12:18:28 @agent_ppo2.py:115][0m #------------------------ Iteration 84 --------------------------#
[32m[20221213 12:18:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:28 @agent_ppo2.py:179][0m |           0.0039 |           0.9404 |           0.5115 |
[32m[20221213 12:18:29 @agent_ppo2.py:179][0m |          -0.0134 |           0.9354 |           0.5122 |
[32m[20221213 12:18:29 @agent_ppo2.py:179][0m |          -0.0161 |           0.9394 |           0.5131 |
[32m[20221213 12:18:29 @agent_ppo2.py:179][0m |          -0.0243 |           0.9205 |           0.5160 |
[32m[20221213 12:18:29 @agent_ppo2.py:179][0m |          -0.0268 |           0.9211 |           0.5176 |
[32m[20221213 12:18:29 @agent_ppo2.py:179][0m |          -0.0310 |           0.9194 |           0.5187 |
[32m[20221213 12:18:29 @agent_ppo2.py:179][0m |          -0.0309 |           0.9130 |           0.5195 |
[32m[20221213 12:18:29 @agent_ppo2.py:179][0m |          -0.0339 |           0.9125 |           0.5211 |
[32m[20221213 12:18:30 @agent_ppo2.py:179][0m |          -0.0343 |           0.9116 |           0.5215 |
[32m[20221213 12:18:30 @agent_ppo2.py:179][0m |          -0.0346 |           0.9116 |           0.5229 |
[32m[20221213 12:18:30 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:18:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.49
[32m[20221213 12:18:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 68.96
[32m[20221213 12:18:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.07
[32m[20221213 12:18:30 @agent_ppo2.py:137][0m Total time:       2.55 min
[32m[20221213 12:18:30 @agent_ppo2.py:139][0m 174080 total steps have happened
[32m[20221213 12:18:30 @agent_ppo2.py:115][0m #------------------------ Iteration 85 --------------------------#
[32m[20221213 12:18:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:30 @agent_ppo2.py:179][0m |           0.0157 |           1.0763 |           0.5355 |
[32m[20221213 12:18:30 @agent_ppo2.py:179][0m |          -0.0098 |           0.9498 |           0.5340 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0185 |           0.9194 |           0.5344 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0233 |           0.9086 |           0.5362 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0311 |           0.9022 |           0.5362 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0263 |           0.9056 |           0.5380 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0316 |           0.8866 |           0.5397 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0213 |           0.9496 |           0.5423 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0346 |           0.8884 |           0.5437 |
[32m[20221213 12:18:31 @agent_ppo2.py:179][0m |          -0.0347 |           0.9064 |           0.5430 |
[32m[20221213 12:18:31 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:18:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.15
[32m[20221213 12:18:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.56
[32m[20221213 12:18:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.56
[32m[20221213 12:18:32 @agent_ppo2.py:137][0m Total time:       2.58 min
[32m[20221213 12:18:32 @agent_ppo2.py:139][0m 176128 total steps have happened
[32m[20221213 12:18:32 @agent_ppo2.py:115][0m #------------------------ Iteration 86 --------------------------#
[32m[20221213 12:18:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:32 @agent_ppo2.py:179][0m |           0.0092 |           1.0584 |           0.5472 |
[32m[20221213 12:18:32 @agent_ppo2.py:179][0m |          -0.0119 |           1.0333 |           0.5462 |
[32m[20221213 12:18:32 @agent_ppo2.py:179][0m |          -0.0204 |           1.0218 |           0.5474 |
[32m[20221213 12:18:32 @agent_ppo2.py:179][0m |          -0.0101 |           1.1290 |           0.5475 |
[32m[20221213 12:18:32 @agent_ppo2.py:179][0m |          -0.0214 |           1.0234 |           0.5417 |
[32m[20221213 12:18:33 @agent_ppo2.py:179][0m |          -0.0369 |           1.0119 |           0.5441 |
[32m[20221213 12:18:33 @agent_ppo2.py:179][0m |          -0.0383 |           1.0030 |           0.5457 |
[32m[20221213 12:18:33 @agent_ppo2.py:179][0m |          -0.0389 |           1.0020 |           0.5472 |
[32m[20221213 12:18:33 @agent_ppo2.py:179][0m |          -0.0432 |           1.0021 |           0.5486 |
[32m[20221213 12:18:33 @agent_ppo2.py:179][0m |          -0.0386 |           0.9964 |           0.5503 |
[32m[20221213 12:18:33 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:18:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.43
[32m[20221213 12:18:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.00
[32m[20221213 12:18:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.66
[32m[20221213 12:18:33 @agent_ppo2.py:137][0m Total time:       2.61 min
[32m[20221213 12:18:33 @agent_ppo2.py:139][0m 178176 total steps have happened
[32m[20221213 12:18:33 @agent_ppo2.py:115][0m #------------------------ Iteration 87 --------------------------#
[32m[20221213 12:18:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:34 @agent_ppo2.py:179][0m |           0.0190 |           1.0789 |           0.5309 |
[32m[20221213 12:18:34 @agent_ppo2.py:179][0m |          -0.0088 |           1.0454 |           0.5318 |
[32m[20221213 12:18:34 @agent_ppo2.py:179][0m |          -0.0161 |           1.0330 |           0.5316 |
[32m[20221213 12:18:34 @agent_ppo2.py:179][0m |          -0.0186 |           1.0321 |           0.5327 |
[32m[20221213 12:18:34 @agent_ppo2.py:179][0m |          -0.0249 |           1.0161 |           0.5327 |
[32m[20221213 12:18:34 @agent_ppo2.py:179][0m |          -0.0229 |           1.1434 |           0.5349 |
[32m[20221213 12:18:34 @agent_ppo2.py:179][0m |          -0.0328 |           1.0256 |           0.5362 |
[32m[20221213 12:18:35 @agent_ppo2.py:179][0m |          -0.0366 |           1.0103 |           0.5362 |
[32m[20221213 12:18:35 @agent_ppo2.py:179][0m |          -0.0368 |           1.0065 |           0.5373 |
[32m[20221213 12:18:35 @agent_ppo2.py:179][0m |          -0.0401 |           1.0039 |           0.5381 |
[32m[20221213 12:18:35 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:18:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.65
[32m[20221213 12:18:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.94
[32m[20221213 12:18:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.34
[32m[20221213 12:18:35 @agent_ppo2.py:137][0m Total time:       2.64 min
[32m[20221213 12:18:35 @agent_ppo2.py:139][0m 180224 total steps have happened
[32m[20221213 12:18:35 @agent_ppo2.py:115][0m #------------------------ Iteration 88 --------------------------#
[32m[20221213 12:18:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:35 @agent_ppo2.py:179][0m |           0.0160 |           1.0271 |           0.5682 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0133 |           1.0000 |           0.5679 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0177 |           1.0002 |           0.5672 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0242 |           0.9941 |           0.5698 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0178 |           1.0447 |           0.5675 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0290 |           0.9870 |           0.5666 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0211 |           1.0349 |           0.5683 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0322 |           0.9907 |           0.5707 |
[32m[20221213 12:18:36 @agent_ppo2.py:179][0m |          -0.0232 |           1.1243 |           0.5712 |
[32m[20221213 12:18:37 @agent_ppo2.py:179][0m |          -0.0360 |           0.9909 |           0.5735 |
[32m[20221213 12:18:37 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:18:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.08
[32m[20221213 12:18:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.45
[32m[20221213 12:18:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 66.75
[32m[20221213 12:18:37 @agent_ppo2.py:137][0m Total time:       2.67 min
[32m[20221213 12:18:37 @agent_ppo2.py:139][0m 182272 total steps have happened
[32m[20221213 12:18:37 @agent_ppo2.py:115][0m #------------------------ Iteration 89 --------------------------#
[32m[20221213 12:18:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:37 @agent_ppo2.py:179][0m |           0.0098 |           1.0740 |           0.5579 |
[32m[20221213 12:18:37 @agent_ppo2.py:179][0m |          -0.0062 |           1.0527 |           0.5574 |
[32m[20221213 12:18:37 @agent_ppo2.py:179][0m |          -0.0199 |           1.0426 |           0.5594 |
[32m[20221213 12:18:38 @agent_ppo2.py:179][0m |          -0.0259 |           1.0464 |           0.5614 |
[32m[20221213 12:18:38 @agent_ppo2.py:179][0m |          -0.0299 |           1.0259 |           0.5622 |
[32m[20221213 12:18:38 @agent_ppo2.py:179][0m |          -0.0336 |           1.0252 |           0.5643 |
[32m[20221213 12:18:38 @agent_ppo2.py:179][0m |          -0.0308 |           1.0460 |           0.5642 |
[32m[20221213 12:18:38 @agent_ppo2.py:179][0m |          -0.0320 |           1.0206 |           0.5639 |
[32m[20221213 12:18:38 @agent_ppo2.py:179][0m |          -0.0359 |           1.0399 |           0.5663 |
[32m[20221213 12:18:38 @agent_ppo2.py:179][0m |          -0.0371 |           1.0161 |           0.5688 |
[32m[20221213 12:18:38 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:18:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.81
[32m[20221213 12:18:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 82.94
[32m[20221213 12:18:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 62.37
[32m[20221213 12:18:38 @agent_ppo2.py:137][0m Total time:       2.70 min
[32m[20221213 12:18:38 @agent_ppo2.py:139][0m 184320 total steps have happened
[32m[20221213 12:18:38 @agent_ppo2.py:115][0m #------------------------ Iteration 90 --------------------------#
[32m[20221213 12:18:39 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:18:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:39 @agent_ppo2.py:179][0m |           0.0167 |           1.2200 |           0.5629 |
[32m[20221213 12:18:39 @agent_ppo2.py:179][0m |          -0.0063 |           1.1981 |           0.5665 |
[32m[20221213 12:18:39 @agent_ppo2.py:179][0m |          -0.0225 |           1.1872 |           0.5719 |
[32m[20221213 12:18:39 @agent_ppo2.py:179][0m |          -0.0263 |           1.1802 |           0.5734 |
[32m[20221213 12:18:39 @agent_ppo2.py:179][0m |          -0.0310 |           1.1723 |           0.5746 |
[32m[20221213 12:18:40 @agent_ppo2.py:179][0m |          -0.0326 |           1.1675 |           0.5737 |
[32m[20221213 12:18:40 @agent_ppo2.py:179][0m |          -0.0324 |           1.3355 |           0.5733 |
[32m[20221213 12:18:40 @agent_ppo2.py:179][0m |          -0.0389 |           1.1753 |           0.5759 |
[32m[20221213 12:18:40 @agent_ppo2.py:179][0m |          -0.0411 |           1.1572 |           0.5774 |
[32m[20221213 12:18:40 @agent_ppo2.py:179][0m |          -0.0383 |           1.1510 |           0.5757 |
[32m[20221213 12:18:40 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:18:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.77
[32m[20221213 12:18:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 95.95
[32m[20221213 12:18:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.30
[32m[20221213 12:18:40 @agent_ppo2.py:137][0m Total time:       2.72 min
[32m[20221213 12:18:40 @agent_ppo2.py:139][0m 186368 total steps have happened
[32m[20221213 12:18:40 @agent_ppo2.py:115][0m #------------------------ Iteration 91 --------------------------#
[32m[20221213 12:18:40 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:18:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |           0.0152 |           1.1199 |           0.5737 |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |          -0.0021 |           1.1389 |           0.5695 |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |          -0.0194 |           1.0787 |           0.5692 |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |          -0.0272 |           1.0613 |           0.5713 |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |          -0.0312 |           1.0594 |           0.5716 |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |          -0.0282 |           1.0638 |           0.5715 |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |          -0.0349 |           1.0583 |           0.5722 |
[32m[20221213 12:18:41 @agent_ppo2.py:179][0m |          -0.0322 |           1.0649 |           0.5731 |
[32m[20221213 12:18:42 @agent_ppo2.py:179][0m |          -0.0318 |           1.0894 |           0.5745 |
[32m[20221213 12:18:42 @agent_ppo2.py:179][0m |          -0.0379 |           1.0419 |           0.5750 |
[32m[20221213 12:18:42 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:18:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.55
[32m[20221213 12:18:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.21
[32m[20221213 12:18:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.93
[32m[20221213 12:18:42 @agent_ppo2.py:137][0m Total time:       2.75 min
[32m[20221213 12:18:42 @agent_ppo2.py:139][0m 188416 total steps have happened
[32m[20221213 12:18:42 @agent_ppo2.py:115][0m #------------------------ Iteration 92 --------------------------#
[32m[20221213 12:18:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:42 @agent_ppo2.py:179][0m |           0.0062 |           1.2689 |           0.5880 |
[32m[20221213 12:18:42 @agent_ppo2.py:179][0m |          -0.0125 |           1.2518 |           0.5864 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0229 |           1.2426 |           0.5889 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0252 |           1.2469 |           0.5902 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0258 |           1.2313 |           0.5905 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0311 |           1.2303 |           0.5908 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0230 |           1.3803 |           0.5931 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0377 |           1.2440 |           0.5951 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0351 |           1.2196 |           0.5952 |
[32m[20221213 12:18:43 @agent_ppo2.py:179][0m |          -0.0399 |           1.2153 |           0.5953 |
[32m[20221213 12:18:43 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:18:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.80
[32m[20221213 12:18:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.51
[32m[20221213 12:18:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.71
[32m[20221213 12:18:44 @agent_ppo2.py:137][0m Total time:       2.78 min
[32m[20221213 12:18:44 @agent_ppo2.py:139][0m 190464 total steps have happened
[32m[20221213 12:18:44 @agent_ppo2.py:115][0m #------------------------ Iteration 93 --------------------------#
[32m[20221213 12:18:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:44 @agent_ppo2.py:179][0m |           0.0100 |           1.1002 |           0.5941 |
[32m[20221213 12:18:44 @agent_ppo2.py:179][0m |          -0.0101 |           1.0633 |           0.5908 |
[32m[20221213 12:18:44 @agent_ppo2.py:179][0m |          -0.0202 |           1.0469 |           0.5965 |
[32m[20221213 12:18:44 @agent_ppo2.py:179][0m |          -0.0251 |           1.0400 |           0.5967 |
[32m[20221213 12:18:44 @agent_ppo2.py:179][0m |          -0.0294 |           1.0289 |           0.5991 |
[32m[20221213 12:18:45 @agent_ppo2.py:179][0m |          -0.0273 |           1.0241 |           0.6006 |
[32m[20221213 12:18:45 @agent_ppo2.py:179][0m |          -0.0296 |           1.0169 |           0.6002 |
[32m[20221213 12:18:45 @agent_ppo2.py:179][0m |          -0.0276 |           1.0348 |           0.6005 |
[32m[20221213 12:18:45 @agent_ppo2.py:179][0m |          -0.0329 |           1.0062 |           0.5982 |
[32m[20221213 12:18:45 @agent_ppo2.py:179][0m |          -0.0343 |           1.0088 |           0.6034 |
[32m[20221213 12:18:45 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:18:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.72
[32m[20221213 12:18:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 70.62
[32m[20221213 12:18:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.63
[32m[20221213 12:18:45 @agent_ppo2.py:137][0m Total time:       2.81 min
[32m[20221213 12:18:45 @agent_ppo2.py:139][0m 192512 total steps have happened
[32m[20221213 12:18:45 @agent_ppo2.py:115][0m #------------------------ Iteration 94 --------------------------#
[32m[20221213 12:18:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:46 @agent_ppo2.py:179][0m |           0.0115 |           1.3166 |           0.6129 |
[32m[20221213 12:18:46 @agent_ppo2.py:179][0m |          -0.0125 |           1.2926 |           0.6097 |
[32m[20221213 12:18:46 @agent_ppo2.py:179][0m |          -0.0216 |           1.2800 |           0.6121 |
[32m[20221213 12:18:46 @agent_ppo2.py:179][0m |          -0.0273 |           1.2658 |           0.6109 |
[32m[20221213 12:18:46 @agent_ppo2.py:179][0m |          -0.0321 |           1.2665 |           0.6108 |
[32m[20221213 12:18:46 @agent_ppo2.py:179][0m |          -0.0392 |           1.2627 |           0.6097 |
[32m[20221213 12:18:46 @agent_ppo2.py:179][0m |          -0.0410 |           1.2493 |           0.6125 |
[32m[20221213 12:18:47 @agent_ppo2.py:179][0m |          -0.0441 |           1.2497 |           0.6136 |
[32m[20221213 12:18:47 @agent_ppo2.py:179][0m |          -0.0427 |           1.2377 |           0.6148 |
[32m[20221213 12:18:47 @agent_ppo2.py:179][0m |          -0.0459 |           1.2398 |           0.6144 |
[32m[20221213 12:18:47 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:18:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.43
[32m[20221213 12:18:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 86.29
[32m[20221213 12:18:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 81.54
[32m[20221213 12:18:47 @agent_ppo2.py:137][0m Total time:       2.84 min
[32m[20221213 12:18:47 @agent_ppo2.py:139][0m 194560 total steps have happened
[32m[20221213 12:18:47 @agent_ppo2.py:115][0m #------------------------ Iteration 95 --------------------------#
[32m[20221213 12:18:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:47 @agent_ppo2.py:179][0m |           0.0111 |           1.0982 |           0.6070 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0036 |           1.1087 |           0.6052 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0179 |           1.0856 |           0.6032 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0222 |           1.0839 |           0.6036 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0281 |           1.0755 |           0.6040 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0265 |           1.0711 |           0.6047 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0325 |           1.0731 |           0.6040 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0306 |           1.0751 |           0.6051 |
[32m[20221213 12:18:48 @agent_ppo2.py:179][0m |          -0.0332 |           1.0673 |           0.6039 |
[32m[20221213 12:18:49 @agent_ppo2.py:179][0m |          -0.0364 |           1.0685 |           0.6057 |
[32m[20221213 12:18:49 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:18:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 68.68
[32m[20221213 12:18:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.41
[32m[20221213 12:18:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 77.03
[32m[20221213 12:18:49 @agent_ppo2.py:137][0m Total time:       2.87 min
[32m[20221213 12:18:49 @agent_ppo2.py:139][0m 196608 total steps have happened
[32m[20221213 12:18:49 @agent_ppo2.py:115][0m #------------------------ Iteration 96 --------------------------#
[32m[20221213 12:18:49 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:18:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:49 @agent_ppo2.py:179][0m |           0.0131 |           1.2969 |           0.5869 |
[32m[20221213 12:18:49 @agent_ppo2.py:179][0m |          -0.0037 |           1.3150 |           0.5859 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0002 |           1.4813 |           0.5810 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0281 |           1.2720 |           0.5839 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0298 |           1.2573 |           0.5865 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0341 |           1.2511 |           0.5879 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0309 |           1.2548 |           0.5885 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0390 |           1.2439 |           0.5875 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0420 |           1.2427 |           0.5904 |
[32m[20221213 12:18:50 @agent_ppo2.py:179][0m |          -0.0435 |           1.2373 |           0.5911 |
[32m[20221213 12:18:50 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:18:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.42
[32m[20221213 12:18:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 80.90
[32m[20221213 12:18:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.74
[32m[20221213 12:18:51 @agent_ppo2.py:137][0m Total time:       2.90 min
[32m[20221213 12:18:51 @agent_ppo2.py:139][0m 198656 total steps have happened
[32m[20221213 12:18:51 @agent_ppo2.py:115][0m #------------------------ Iteration 97 --------------------------#
[32m[20221213 12:18:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:51 @agent_ppo2.py:179][0m |           0.0084 |           1.1765 |           0.6001 |
[32m[20221213 12:18:51 @agent_ppo2.py:179][0m |          -0.0130 |           1.1711 |           0.6011 |
[32m[20221213 12:18:51 @agent_ppo2.py:179][0m |          -0.0200 |           1.1521 |           0.6005 |
[32m[20221213 12:18:52 @agent_ppo2.py:179][0m |          -0.0246 |           1.1565 |           0.6029 |
[32m[20221213 12:18:52 @agent_ppo2.py:179][0m |          -0.0272 |           1.1632 |           0.6070 |
[32m[20221213 12:18:52 @agent_ppo2.py:179][0m |          -0.0280 |           1.1363 |           0.6062 |
[32m[20221213 12:18:52 @agent_ppo2.py:179][0m |          -0.0280 |           1.2637 |           0.6076 |
[32m[20221213 12:18:52 @agent_ppo2.py:179][0m |          -0.0429 |           1.1505 |           0.6093 |
[32m[20221213 12:18:52 @agent_ppo2.py:179][0m |          -0.0417 |           1.1323 |           0.6113 |
[32m[20221213 12:18:52 @agent_ppo2.py:179][0m |          -0.0425 |           1.1222 |           0.6139 |
[32m[20221213 12:18:52 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:18:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.67
[32m[20221213 12:18:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.67
[32m[20221213 12:18:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.21
[32m[20221213 12:18:53 @agent_ppo2.py:137][0m Total time:       2.93 min
[32m[20221213 12:18:53 @agent_ppo2.py:139][0m 200704 total steps have happened
[32m[20221213 12:18:53 @agent_ppo2.py:115][0m #------------------------ Iteration 98 --------------------------#
[32m[20221213 12:18:53 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:18:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:53 @agent_ppo2.py:179][0m |           0.0075 |           1.1604 |           0.6041 |
[32m[20221213 12:18:53 @agent_ppo2.py:179][0m |          -0.0122 |           1.1271 |           0.6053 |
[32m[20221213 12:18:53 @agent_ppo2.py:179][0m |          -0.0227 |           1.1234 |           0.6070 |
[32m[20221213 12:18:53 @agent_ppo2.py:179][0m |          -0.0207 |           1.1447 |           0.6090 |
[32m[20221213 12:18:53 @agent_ppo2.py:179][0m |          -0.0295 |           1.1128 |           0.6088 |
[32m[20221213 12:18:54 @agent_ppo2.py:179][0m |          -0.0330 |           1.1065 |           0.6101 |
[32m[20221213 12:18:54 @agent_ppo2.py:179][0m |          -0.0329 |           1.1044 |           0.6103 |
[32m[20221213 12:18:54 @agent_ppo2.py:179][0m |          -0.0355 |           1.1078 |           0.6131 |
[32m[20221213 12:18:54 @agent_ppo2.py:179][0m |          -0.0411 |           1.0976 |           0.6129 |
[32m[20221213 12:18:54 @agent_ppo2.py:179][0m |          -0.0387 |           1.0900 |           0.6146 |
[32m[20221213 12:18:54 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:18:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.80
[32m[20221213 12:18:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 69.49
[32m[20221213 12:18:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.48
[32m[20221213 12:18:54 @agent_ppo2.py:137][0m Total time:       2.96 min
[32m[20221213 12:18:54 @agent_ppo2.py:139][0m 202752 total steps have happened
[32m[20221213 12:18:54 @agent_ppo2.py:115][0m #------------------------ Iteration 99 --------------------------#
[32m[20221213 12:18:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:55 @agent_ppo2.py:179][0m |           0.0062 |           1.1853 |           0.6161 |
[32m[20221213 12:18:55 @agent_ppo2.py:179][0m |          -0.0101 |           1.1701 |           0.6225 |
[32m[20221213 12:18:55 @agent_ppo2.py:179][0m |          -0.0243 |           1.1639 |           0.6224 |
[32m[20221213 12:18:55 @agent_ppo2.py:179][0m |          -0.0305 |           1.1595 |           0.6246 |
[32m[20221213 12:18:55 @agent_ppo2.py:179][0m |          -0.0295 |           1.1594 |           0.6227 |
[32m[20221213 12:18:55 @agent_ppo2.py:179][0m |          -0.0313 |           1.1596 |           0.6266 |
[32m[20221213 12:18:56 @agent_ppo2.py:179][0m |          -0.0380 |           1.1474 |           0.6254 |
[32m[20221213 12:18:56 @agent_ppo2.py:179][0m |          -0.0380 |           1.1447 |           0.6306 |
[32m[20221213 12:18:56 @agent_ppo2.py:179][0m |          -0.0375 |           1.1481 |           0.6321 |
[32m[20221213 12:18:56 @agent_ppo2.py:179][0m |          -0.0286 |           1.2224 |           0.6317 |
[32m[20221213 12:18:56 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:18:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.79
[32m[20221213 12:18:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.88
[32m[20221213 12:18:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.40
[32m[20221213 12:18:56 @agent_ppo2.py:137][0m Total time:       2.99 min
[32m[20221213 12:18:56 @agent_ppo2.py:139][0m 204800 total steps have happened
[32m[20221213 12:18:56 @agent_ppo2.py:115][0m #------------------------ Iteration 100 --------------------------#
[32m[20221213 12:18:56 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:18:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:57 @agent_ppo2.py:179][0m |           0.0196 |           1.1470 |           0.6167 |
[32m[20221213 12:18:57 @agent_ppo2.py:179][0m |          -0.0137 |           1.1317 |           0.6136 |
[32m[20221213 12:18:57 @agent_ppo2.py:179][0m |          -0.0150 |           1.1629 |           0.6140 |
[32m[20221213 12:18:57 @agent_ppo2.py:179][0m |          -0.0213 |           1.1337 |           0.6161 |
[32m[20221213 12:18:57 @agent_ppo2.py:179][0m |          -0.0135 |           1.2200 |           0.6163 |
[32m[20221213 12:18:57 @agent_ppo2.py:179][0m |          -0.0300 |           1.1348 |           0.6147 |
[32m[20221213 12:18:57 @agent_ppo2.py:179][0m |          -0.0318 |           1.1227 |           0.6135 |
[32m[20221213 12:18:58 @agent_ppo2.py:179][0m |          -0.0320 |           1.1195 |           0.6177 |
[32m[20221213 12:18:58 @agent_ppo2.py:179][0m |          -0.0367 |           1.1166 |           0.6204 |
[32m[20221213 12:18:58 @agent_ppo2.py:179][0m |          -0.0347 |           1.1150 |           0.6201 |
[32m[20221213 12:18:58 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:18:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.81
[32m[20221213 12:18:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 73.64
[32m[20221213 12:18:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.58
[32m[20221213 12:18:58 @agent_ppo2.py:137][0m Total time:       3.02 min
[32m[20221213 12:18:58 @agent_ppo2.py:139][0m 206848 total steps have happened
[32m[20221213 12:18:58 @agent_ppo2.py:115][0m #------------------------ Iteration 101 --------------------------#
[32m[20221213 12:18:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:18:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:18:58 @agent_ppo2.py:179][0m |           0.0129 |           1.1641 |           0.6203 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0117 |           1.1605 |           0.6207 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0191 |           1.1611 |           0.6226 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0130 |           1.2382 |           0.6234 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0267 |           1.1549 |           0.6270 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0256 |           1.1463 |           0.6281 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0268 |           1.1504 |           0.6286 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0170 |           1.2429 |           0.6298 |
[32m[20221213 12:18:59 @agent_ppo2.py:179][0m |          -0.0110 |           1.2756 |           0.6279 |
[32m[20221213 12:19:00 @agent_ppo2.py:179][0m |          -0.0236 |           1.1503 |           0.6267 |
[32m[20221213 12:19:00 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:19:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.35
[32m[20221213 12:19:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 74.71
[32m[20221213 12:19:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.73
[32m[20221213 12:19:00 @agent_ppo2.py:137][0m Total time:       3.05 min
[32m[20221213 12:19:00 @agent_ppo2.py:139][0m 208896 total steps have happened
[32m[20221213 12:19:00 @agent_ppo2.py:115][0m #------------------------ Iteration 102 --------------------------#
[32m[20221213 12:19:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:00 @agent_ppo2.py:179][0m |           0.0114 |           1.2256 |           0.6412 |
[32m[20221213 12:19:00 @agent_ppo2.py:179][0m |          -0.0140 |           1.2126 |           0.6452 |
[32m[20221213 12:19:00 @agent_ppo2.py:179][0m |          -0.0214 |           1.2090 |           0.6455 |
[32m[20221213 12:19:01 @agent_ppo2.py:179][0m |          -0.0256 |           1.2074 |           0.6458 |
[32m[20221213 12:19:01 @agent_ppo2.py:179][0m |          -0.0296 |           1.1973 |           0.6473 |
[32m[20221213 12:19:01 @agent_ppo2.py:179][0m |          -0.0322 |           1.1975 |           0.6493 |
[32m[20221213 12:19:01 @agent_ppo2.py:179][0m |          -0.0321 |           1.1930 |           0.6494 |
[32m[20221213 12:19:01 @agent_ppo2.py:179][0m |          -0.0323 |           1.1906 |           0.6514 |
[32m[20221213 12:19:01 @agent_ppo2.py:179][0m |          -0.0332 |           1.1901 |           0.6520 |
[32m[20221213 12:19:01 @agent_ppo2.py:179][0m |          -0.0371 |           1.1849 |           0.6539 |
[32m[20221213 12:19:01 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:19:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.70
[32m[20221213 12:19:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.55
[32m[20221213 12:19:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.05
[32m[20221213 12:19:02 @agent_ppo2.py:137][0m Total time:       3.08 min
[32m[20221213 12:19:02 @agent_ppo2.py:139][0m 210944 total steps have happened
[32m[20221213 12:19:02 @agent_ppo2.py:115][0m #------------------------ Iteration 103 --------------------------#
[32m[20221213 12:19:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:02 @agent_ppo2.py:179][0m |           0.0101 |           1.1906 |           0.6788 |
[32m[20221213 12:19:02 @agent_ppo2.py:179][0m |          -0.0122 |           1.1822 |           0.6733 |
[32m[20221213 12:19:02 @agent_ppo2.py:179][0m |          -0.0184 |           1.1782 |           0.6807 |
[32m[20221213 12:19:02 @agent_ppo2.py:179][0m |          -0.0214 |           1.1767 |           0.6801 |
[32m[20221213 12:19:02 @agent_ppo2.py:179][0m |          -0.0224 |           1.1827 |           0.6831 |
[32m[20221213 12:19:03 @agent_ppo2.py:179][0m |          -0.0275 |           1.1724 |           0.6851 |
[32m[20221213 12:19:03 @agent_ppo2.py:179][0m |          -0.0239 |           1.1896 |           0.6861 |
[32m[20221213 12:19:03 @agent_ppo2.py:179][0m |          -0.0257 |           1.1890 |           0.6864 |
[32m[20221213 12:19:03 @agent_ppo2.py:179][0m |          -0.0284 |           1.1625 |           0.6886 |
[32m[20221213 12:19:03 @agent_ppo2.py:179][0m |          -0.0271 |           1.1888 |           0.6898 |
[32m[20221213 12:19:03 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:19:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.23
[32m[20221213 12:19:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.71
[32m[20221213 12:19:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 61.65
[32m[20221213 12:19:03 @agent_ppo2.py:137][0m Total time:       3.11 min
[32m[20221213 12:19:03 @agent_ppo2.py:139][0m 212992 total steps have happened
[32m[20221213 12:19:03 @agent_ppo2.py:115][0m #------------------------ Iteration 104 --------------------------#
[32m[20221213 12:19:04 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:19:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:04 @agent_ppo2.py:179][0m |           0.0097 |           1.2100 |           0.6936 |
[32m[20221213 12:19:04 @agent_ppo2.py:179][0m |          -0.0000 |           1.2889 |           0.6900 |
[32m[20221213 12:19:04 @agent_ppo2.py:179][0m |          -0.0177 |           1.2018 |           0.6935 |
[32m[20221213 12:19:04 @agent_ppo2.py:179][0m |          -0.0238 |           1.1875 |           0.6962 |
[32m[20221213 12:19:04 @agent_ppo2.py:179][0m |          -0.0276 |           1.1883 |           0.6971 |
[32m[20221213 12:19:04 @agent_ppo2.py:179][0m |          -0.0271 |           1.1923 |           0.6973 |
[32m[20221213 12:19:05 @agent_ppo2.py:179][0m |          -0.0181 |           1.2524 |           0.6988 |
[32m[20221213 12:19:05 @agent_ppo2.py:179][0m |          -0.0276 |           1.1943 |           0.6962 |
[32m[20221213 12:19:05 @agent_ppo2.py:179][0m |          -0.0312 |           1.1815 |           0.7023 |
[32m[20221213 12:19:05 @agent_ppo2.py:179][0m |          -0.0346 |           1.1816 |           0.7008 |
[32m[20221213 12:19:05 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:19:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 73.40
[32m[20221213 12:19:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 75.84
[32m[20221213 12:19:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.74
[32m[20221213 12:19:05 @agent_ppo2.py:137][0m Total time:       3.14 min
[32m[20221213 12:19:05 @agent_ppo2.py:139][0m 215040 total steps have happened
[32m[20221213 12:19:05 @agent_ppo2.py:115][0m #------------------------ Iteration 105 --------------------------#
[32m[20221213 12:19:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:06 @agent_ppo2.py:179][0m |           0.0085 |           1.3374 |           0.6807 |
[32m[20221213 12:19:06 @agent_ppo2.py:179][0m |          -0.0149 |           1.3144 |           0.6800 |
[32m[20221213 12:19:06 @agent_ppo2.py:179][0m |          -0.0248 |           1.3050 |           0.6837 |
[32m[20221213 12:19:06 @agent_ppo2.py:179][0m |          -0.0269 |           1.3090 |           0.6840 |
[32m[20221213 12:19:06 @agent_ppo2.py:179][0m |          -0.0301 |           1.2925 |           0.6829 |
[32m[20221213 12:19:06 @agent_ppo2.py:179][0m |          -0.0329 |           1.2934 |           0.6855 |
[32m[20221213 12:19:06 @agent_ppo2.py:179][0m |          -0.0365 |           1.2874 |           0.6891 |
[32m[20221213 12:19:07 @agent_ppo2.py:179][0m |          -0.0408 |           1.2849 |           0.6883 |
[32m[20221213 12:19:07 @agent_ppo2.py:179][0m |          -0.0408 |           1.2839 |           0.6890 |
[32m[20221213 12:19:07 @agent_ppo2.py:179][0m |          -0.0277 |           1.3484 |           0.6913 |
[32m[20221213 12:19:07 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:19:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.14
[32m[20221213 12:19:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.14
[32m[20221213 12:19:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.82
[32m[20221213 12:19:07 @agent_ppo2.py:137][0m Total time:       3.17 min
[32m[20221213 12:19:07 @agent_ppo2.py:139][0m 217088 total steps have happened
[32m[20221213 12:19:07 @agent_ppo2.py:115][0m #------------------------ Iteration 106 --------------------------#
[32m[20221213 12:19:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:07 @agent_ppo2.py:179][0m |           0.0180 |           1.4278 |           0.6866 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0167 |           1.4010 |           0.6864 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0236 |           1.3965 |           0.6897 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0334 |           1.3877 |           0.6918 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0275 |           1.4510 |           0.6955 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0268 |           1.4426 |           0.6972 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0429 |           1.3771 |           0.6994 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0339 |           1.4791 |           0.7006 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0448 |           1.3712 |           0.7027 |
[32m[20221213 12:19:08 @agent_ppo2.py:179][0m |          -0.0435 |           1.3570 |           0.7045 |
[32m[20221213 12:19:08 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:19:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.88
[32m[20221213 12:19:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.53
[32m[20221213 12:19:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.51
[32m[20221213 12:19:09 @agent_ppo2.py:137][0m Total time:       3.20 min
[32m[20221213 12:19:09 @agent_ppo2.py:139][0m 219136 total steps have happened
[32m[20221213 12:19:09 @agent_ppo2.py:115][0m #------------------------ Iteration 107 --------------------------#
[32m[20221213 12:19:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:09 @agent_ppo2.py:179][0m |           0.0211 |           1.2944 |           0.7080 |
[32m[20221213 12:19:09 @agent_ppo2.py:179][0m |          -0.0023 |           1.3249 |           0.7075 |
[32m[20221213 12:19:09 @agent_ppo2.py:179][0m |          -0.0201 |           1.2640 |           0.7155 |
[32m[20221213 12:19:10 @agent_ppo2.py:179][0m |          -0.0256 |           1.2568 |           0.7120 |
[32m[20221213 12:19:10 @agent_ppo2.py:179][0m |          -0.0318 |           1.2599 |           0.7144 |
[32m[20221213 12:19:10 @agent_ppo2.py:179][0m |          -0.0314 |           1.2521 |           0.7154 |
[32m[20221213 12:19:10 @agent_ppo2.py:179][0m |          -0.0299 |           1.2500 |           0.7159 |
[32m[20221213 12:19:10 @agent_ppo2.py:179][0m |          -0.0347 |           1.2449 |           0.7199 |
[32m[20221213 12:19:10 @agent_ppo2.py:179][0m |          -0.0338 |           1.2574 |           0.7188 |
[32m[20221213 12:19:10 @agent_ppo2.py:179][0m |          -0.0338 |           1.2419 |           0.7165 |
[32m[20221213 12:19:10 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:19:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.58
[32m[20221213 12:19:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.40
[32m[20221213 12:19:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.98
[32m[20221213 12:19:10 @agent_ppo2.py:137][0m Total time:       3.23 min
[32m[20221213 12:19:10 @agent_ppo2.py:139][0m 221184 total steps have happened
[32m[20221213 12:19:10 @agent_ppo2.py:115][0m #------------------------ Iteration 108 --------------------------#
[32m[20221213 12:19:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:11 @agent_ppo2.py:179][0m |           0.0106 |           1.2685 |           0.7085 |
[32m[20221213 12:19:11 @agent_ppo2.py:179][0m |          -0.0088 |           1.2697 |           0.7024 |
[32m[20221213 12:19:11 @agent_ppo2.py:179][0m |          -0.0202 |           1.2587 |           0.7022 |
[32m[20221213 12:19:11 @agent_ppo2.py:179][0m |          -0.0212 |           1.2569 |           0.7040 |
[32m[20221213 12:19:11 @agent_ppo2.py:179][0m |          -0.0205 |           1.2576 |           0.7003 |
[32m[20221213 12:19:12 @agent_ppo2.py:179][0m |          -0.0232 |           1.2699 |           0.7011 |
[32m[20221213 12:19:12 @agent_ppo2.py:179][0m |          -0.0297 |           1.2509 |           0.7029 |
[32m[20221213 12:19:12 @agent_ppo2.py:179][0m |          -0.0321 |           1.2465 |           0.7040 |
[32m[20221213 12:19:12 @agent_ppo2.py:179][0m |          -0.0310 |           1.2529 |           0.7043 |
[32m[20221213 12:19:12 @agent_ppo2.py:179][0m |          -0.0346 |           1.2460 |           0.7072 |
[32m[20221213 12:19:12 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:19:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 74.05
[32m[20221213 12:19:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 77.68
[32m[20221213 12:19:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.69
[32m[20221213 12:19:12 @agent_ppo2.py:137][0m Total time:       3.26 min
[32m[20221213 12:19:12 @agent_ppo2.py:139][0m 223232 total steps have happened
[32m[20221213 12:19:12 @agent_ppo2.py:115][0m #------------------------ Iteration 109 --------------------------#
[32m[20221213 12:19:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:13 @agent_ppo2.py:179][0m |           0.0152 |           1.4700 |           0.6701 |
[32m[20221213 12:19:13 @agent_ppo2.py:179][0m |          -0.0112 |           1.4514 |           0.6734 |
[32m[20221213 12:19:13 @agent_ppo2.py:179][0m |          -0.0168 |           1.4485 |           0.6770 |
[32m[20221213 12:19:13 @agent_ppo2.py:179][0m |          -0.0231 |           1.4334 |           0.6746 |
[32m[20221213 12:19:13 @agent_ppo2.py:179][0m |          -0.0268 |           1.4402 |           0.6758 |
[32m[20221213 12:19:13 @agent_ppo2.py:179][0m |          -0.0326 |           1.4184 |           0.6782 |
[32m[20221213 12:19:14 @agent_ppo2.py:179][0m |          -0.0302 |           1.4375 |           0.6782 |
[32m[20221213 12:19:14 @agent_ppo2.py:179][0m |          -0.0317 |           1.4091 |           0.6794 |
[32m[20221213 12:19:14 @agent_ppo2.py:179][0m |          -0.0422 |           1.4048 |           0.6813 |
[32m[20221213 12:19:14 @agent_ppo2.py:179][0m |          -0.0405 |           1.4079 |           0.6823 |
[32m[20221213 12:19:14 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:19:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.90
[32m[20221213 12:19:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 86.48
[32m[20221213 12:19:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.83
[32m[20221213 12:19:14 @agent_ppo2.py:137][0m Total time:       3.29 min
[32m[20221213 12:19:14 @agent_ppo2.py:139][0m 225280 total steps have happened
[32m[20221213 12:19:14 @agent_ppo2.py:115][0m #------------------------ Iteration 110 --------------------------#
[32m[20221213 12:19:14 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:19:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |           0.0089 |           1.4017 |           0.6868 |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |          -0.0119 |           1.3717 |           0.6894 |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |          -0.0232 |           1.3608 |           0.6870 |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |          -0.0233 |           1.3825 |           0.6868 |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |          -0.0334 |           1.3546 |           0.6877 |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |          -0.0328 |           1.3459 |           0.6909 |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |          -0.0316 |           1.3475 |           0.6892 |
[32m[20221213 12:19:15 @agent_ppo2.py:179][0m |          -0.0293 |           1.5034 |           0.6902 |
[32m[20221213 12:19:16 @agent_ppo2.py:179][0m |          -0.0415 |           1.3438 |           0.6903 |
[32m[20221213 12:19:16 @agent_ppo2.py:179][0m |          -0.0412 |           1.3441 |           0.6918 |
[32m[20221213 12:19:16 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:19:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.24
[32m[20221213 12:19:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.74
[32m[20221213 12:19:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.90
[32m[20221213 12:19:16 @agent_ppo2.py:137][0m Total time:       3.32 min
[32m[20221213 12:19:16 @agent_ppo2.py:139][0m 227328 total steps have happened
[32m[20221213 12:19:16 @agent_ppo2.py:115][0m #------------------------ Iteration 111 --------------------------#
[32m[20221213 12:19:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:16 @agent_ppo2.py:179][0m |           0.0114 |           1.3728 |           0.6822 |
[32m[20221213 12:19:16 @agent_ppo2.py:179][0m |          -0.0144 |           1.3453 |           0.6848 |
[32m[20221213 12:19:17 @agent_ppo2.py:179][0m |          -0.0217 |           1.3440 |           0.6890 |
[32m[20221213 12:19:17 @agent_ppo2.py:179][0m |          -0.0255 |           1.3351 |           0.6898 |
[32m[20221213 12:19:17 @agent_ppo2.py:179][0m |          -0.0147 |           1.4940 |           0.6895 |
[32m[20221213 12:19:17 @agent_ppo2.py:179][0m |          -0.0289 |           1.3443 |           0.6924 |
[32m[20221213 12:19:17 @agent_ppo2.py:179][0m |          -0.0357 |           1.3254 |           0.6945 |
[32m[20221213 12:19:17 @agent_ppo2.py:179][0m |          -0.0331 |           1.3228 |           0.6930 |
[32m[20221213 12:19:17 @agent_ppo2.py:179][0m |          -0.0356 |           1.3200 |           0.6943 |
[32m[20221213 12:19:18 @agent_ppo2.py:179][0m |          -0.0358 |           1.3200 |           0.6973 |
[32m[20221213 12:19:18 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:19:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 72.60
[32m[20221213 12:19:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 76.47
[32m[20221213 12:19:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.51
[32m[20221213 12:19:18 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 99.51
[32m[20221213 12:19:18 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 99.51
[32m[20221213 12:19:18 @agent_ppo2.py:137][0m Total time:       3.35 min
[32m[20221213 12:19:18 @agent_ppo2.py:139][0m 229376 total steps have happened
[32m[20221213 12:19:18 @agent_ppo2.py:115][0m #------------------------ Iteration 112 --------------------------#
[32m[20221213 12:19:18 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:19:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:18 @agent_ppo2.py:179][0m |           0.0159 |           1.3798 |           0.7085 |
[32m[20221213 12:19:18 @agent_ppo2.py:179][0m |          -0.0079 |           1.3511 |           0.7006 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0184 |           1.3430 |           0.7023 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0227 |           1.3410 |           0.7027 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0190 |           1.3780 |           0.7019 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0196 |           1.3387 |           0.6910 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0295 |           1.3393 |           0.7008 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0283 |           1.3407 |           0.7042 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0318 |           1.3365 |           0.7035 |
[32m[20221213 12:19:19 @agent_ppo2.py:179][0m |          -0.0282 |           1.3396 |           0.7058 |
[32m[20221213 12:19:19 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:19:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.18
[32m[20221213 12:19:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 81.58
[32m[20221213 12:19:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.03
[32m[20221213 12:19:20 @agent_ppo2.py:137][0m Total time:       3.38 min
[32m[20221213 12:19:20 @agent_ppo2.py:139][0m 231424 total steps have happened
[32m[20221213 12:19:20 @agent_ppo2.py:115][0m #------------------------ Iteration 113 --------------------------#
[32m[20221213 12:19:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:20 @agent_ppo2.py:179][0m |           0.0083 |           1.4712 |           0.6832 |
[32m[20221213 12:19:20 @agent_ppo2.py:179][0m |          -0.0070 |           1.4341 |           0.6782 |
[32m[20221213 12:19:20 @agent_ppo2.py:179][0m |          -0.0201 |           1.4271 |           0.6804 |
[32m[20221213 12:19:21 @agent_ppo2.py:179][0m |          -0.0222 |           1.4246 |           0.6811 |
[32m[20221213 12:19:21 @agent_ppo2.py:179][0m |          -0.0281 |           1.4123 |           0.6825 |
[32m[20221213 12:19:21 @agent_ppo2.py:179][0m |          -0.0334 |           1.4242 |           0.6844 |
[32m[20221213 12:19:21 @agent_ppo2.py:179][0m |          -0.0341 |           1.4155 |           0.6846 |
[32m[20221213 12:19:21 @agent_ppo2.py:179][0m |          -0.0356 |           1.4106 |           0.6851 |
[32m[20221213 12:19:21 @agent_ppo2.py:179][0m |          -0.0376 |           1.4082 |           0.6896 |
[32m[20221213 12:19:21 @agent_ppo2.py:179][0m |          -0.0414 |           1.4070 |           0.6903 |
[32m[20221213 12:19:21 @agent_ppo2.py:124][0m Policy update time: 1.54 s
[32m[20221213 12:19:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.95
[32m[20221213 12:19:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 93.45
[32m[20221213 12:19:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.12
[32m[20221213 12:19:22 @agent_ppo2.py:137][0m Total time:       3.42 min
[32m[20221213 12:19:22 @agent_ppo2.py:139][0m 233472 total steps have happened
[32m[20221213 12:19:22 @agent_ppo2.py:115][0m #------------------------ Iteration 114 --------------------------#
[32m[20221213 12:19:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:22 @agent_ppo2.py:179][0m |           0.0067 |           1.4491 |           0.6902 |
[32m[20221213 12:19:22 @agent_ppo2.py:179][0m |          -0.0119 |           1.4337 |           0.6857 |
[32m[20221213 12:19:22 @agent_ppo2.py:179][0m |          -0.0212 |           1.4253 |           0.6846 |
[32m[20221213 12:19:23 @agent_ppo2.py:179][0m |          -0.0240 |           1.4219 |           0.6872 |
[32m[20221213 12:19:23 @agent_ppo2.py:179][0m |          -0.0295 |           1.4224 |           0.6860 |
[32m[20221213 12:19:23 @agent_ppo2.py:179][0m |          -0.0290 |           1.4304 |           0.6861 |
[32m[20221213 12:19:23 @agent_ppo2.py:179][0m |          -0.0265 |           1.5183 |           0.6830 |
[32m[20221213 12:19:23 @agent_ppo2.py:179][0m |          -0.0274 |           1.4168 |           0.6791 |
[32m[20221213 12:19:23 @agent_ppo2.py:179][0m |          -0.0356 |           1.4211 |           0.6834 |
[32m[20221213 12:19:23 @agent_ppo2.py:179][0m |          -0.0393 |           1.4170 |           0.6876 |
[32m[20221213 12:19:23 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:19:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.97
[32m[20221213 12:19:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.93
[32m[20221213 12:19:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 69.87
[32m[20221213 12:19:24 @agent_ppo2.py:137][0m Total time:       3.45 min
[32m[20221213 12:19:24 @agent_ppo2.py:139][0m 235520 total steps have happened
[32m[20221213 12:19:24 @agent_ppo2.py:115][0m #------------------------ Iteration 115 --------------------------#
[32m[20221213 12:19:24 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:19:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:24 @agent_ppo2.py:179][0m |           0.0132 |           1.5734 |           0.6686 |
[32m[20221213 12:19:24 @agent_ppo2.py:179][0m |          -0.0029 |           1.5412 |           0.6630 |
[32m[20221213 12:19:24 @agent_ppo2.py:179][0m |          -0.0130 |           1.5311 |           0.6719 |
[32m[20221213 12:19:24 @agent_ppo2.py:179][0m |          -0.0229 |           1.5210 |           0.6770 |
[32m[20221213 12:19:24 @agent_ppo2.py:179][0m |          -0.0298 |           1.5128 |           0.6823 |
[32m[20221213 12:19:25 @agent_ppo2.py:179][0m |          -0.0295 |           1.5134 |           0.6819 |
[32m[20221213 12:19:25 @agent_ppo2.py:179][0m |          -0.0253 |           1.5929 |           0.6853 |
[32m[20221213 12:19:25 @agent_ppo2.py:179][0m |          -0.0373 |           1.5067 |           0.6856 |
[32m[20221213 12:19:25 @agent_ppo2.py:179][0m |          -0.0374 |           1.4988 |           0.6875 |
[32m[20221213 12:19:25 @agent_ppo2.py:179][0m |          -0.0353 |           1.5371 |           0.6888 |
[32m[20221213 12:19:25 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:19:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.03
[32m[20221213 12:19:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.94
[32m[20221213 12:19:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 89.70
[32m[20221213 12:19:25 @agent_ppo2.py:137][0m Total time:       3.48 min
[32m[20221213 12:19:25 @agent_ppo2.py:139][0m 237568 total steps have happened
[32m[20221213 12:19:25 @agent_ppo2.py:115][0m #------------------------ Iteration 116 --------------------------#
[32m[20221213 12:19:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:26 @agent_ppo2.py:179][0m |           0.0144 |           1.5444 |           0.7100 |
[32m[20221213 12:19:26 @agent_ppo2.py:179][0m |          -0.0008 |           1.6121 |           0.7093 |
[32m[20221213 12:19:26 @agent_ppo2.py:179][0m |          -0.0170 |           1.5017 |           0.7097 |
[32m[20221213 12:19:26 @agent_ppo2.py:179][0m |          -0.0193 |           1.6101 |           0.7120 |
[32m[20221213 12:19:26 @agent_ppo2.py:179][0m |          -0.0248 |           1.4977 |           0.7139 |
[32m[20221213 12:19:26 @agent_ppo2.py:179][0m |          -0.0344 |           1.4812 |           0.7149 |
[32m[20221213 12:19:27 @agent_ppo2.py:179][0m |          -0.0256 |           1.6636 |           0.7164 |
[32m[20221213 12:19:27 @agent_ppo2.py:179][0m |          -0.0396 |           1.4932 |           0.7200 |
[32m[20221213 12:19:27 @agent_ppo2.py:179][0m |          -0.0421 |           1.4751 |           0.7232 |
[32m[20221213 12:19:27 @agent_ppo2.py:179][0m |          -0.0419 |           1.4736 |           0.7228 |
[32m[20221213 12:19:27 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:19:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.64
[32m[20221213 12:19:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 88.26
[32m[20221213 12:19:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.69
[32m[20221213 12:19:27 @agent_ppo2.py:137][0m Total time:       3.51 min
[32m[20221213 12:19:27 @agent_ppo2.py:139][0m 239616 total steps have happened
[32m[20221213 12:19:27 @agent_ppo2.py:115][0m #------------------------ Iteration 117 --------------------------#
[32m[20221213 12:19:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |           0.0119 |           1.6191 |           0.7247 |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |          -0.0121 |           1.5709 |           0.7195 |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |          -0.0251 |           1.5742 |           0.7262 |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |          -0.0292 |           1.5620 |           0.7256 |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |          -0.0337 |           1.5775 |           0.7280 |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |          -0.0371 |           1.5449 |           0.7255 |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |          -0.0363 |           1.5813 |           0.7313 |
[32m[20221213 12:19:28 @agent_ppo2.py:179][0m |          -0.0455 |           1.5365 |           0.7338 |
[32m[20221213 12:19:29 @agent_ppo2.py:179][0m |          -0.0456 |           1.5289 |           0.7362 |
[32m[20221213 12:19:29 @agent_ppo2.py:179][0m |          -0.0491 |           1.5255 |           0.7379 |
[32m[20221213 12:19:29 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:19:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.01
[32m[20221213 12:19:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.67
[32m[20221213 12:19:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.33
[32m[20221213 12:19:29 @agent_ppo2.py:137][0m Total time:       3.54 min
[32m[20221213 12:19:29 @agent_ppo2.py:139][0m 241664 total steps have happened
[32m[20221213 12:19:29 @agent_ppo2.py:115][0m #------------------------ Iteration 118 --------------------------#
[32m[20221213 12:19:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:29 @agent_ppo2.py:179][0m |           0.0218 |           1.6625 |           0.7431 |
[32m[20221213 12:19:29 @agent_ppo2.py:179][0m |          -0.0029 |           1.6314 |           0.7413 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0094 |           1.6276 |           0.7456 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0333 |           1.5418 |           0.7518 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0367 |           1.5059 |           0.7520 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0348 |           1.5170 |           0.7539 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0444 |           1.4944 |           0.7563 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0418 |           1.4813 |           0.7589 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0447 |           1.4650 |           0.7614 |
[32m[20221213 12:19:30 @agent_ppo2.py:179][0m |          -0.0458 |           1.4656 |           0.7624 |
[32m[20221213 12:19:30 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:19:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.85
[32m[20221213 12:19:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.87
[32m[20221213 12:19:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.05
[32m[20221213 12:19:31 @agent_ppo2.py:137][0m Total time:       3.57 min
[32m[20221213 12:19:31 @agent_ppo2.py:139][0m 243712 total steps have happened
[32m[20221213 12:19:31 @agent_ppo2.py:115][0m #------------------------ Iteration 119 --------------------------#
[32m[20221213 12:19:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:31 @agent_ppo2.py:179][0m |           0.0215 |           1.3873 |           0.7794 |
[32m[20221213 12:19:31 @agent_ppo2.py:179][0m |           0.0007 |           1.3439 |           0.7770 |
[32m[20221213 12:19:31 @agent_ppo2.py:179][0m |          -0.0176 |           1.3237 |           0.7771 |
[32m[20221213 12:19:31 @agent_ppo2.py:179][0m |          -0.0267 |           1.3137 |           0.7796 |
[32m[20221213 12:19:32 @agent_ppo2.py:179][0m |          -0.0260 |           1.3129 |           0.7799 |
[32m[20221213 12:19:32 @agent_ppo2.py:179][0m |          -0.0299 |           1.2995 |           0.7834 |
[32m[20221213 12:19:32 @agent_ppo2.py:179][0m |          -0.0219 |           1.3825 |           0.7812 |
[32m[20221213 12:19:32 @agent_ppo2.py:179][0m |          -0.0215 |           1.2931 |           0.7771 |
[32m[20221213 12:19:32 @agent_ppo2.py:179][0m |          -0.0306 |           1.2809 |           0.7812 |
[32m[20221213 12:19:32 @agent_ppo2.py:179][0m |          -0.0268 |           1.3817 |           0.7849 |
[32m[20221213 12:19:32 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:19:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.57
[32m[20221213 12:19:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 78.50
[32m[20221213 12:19:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.18
[32m[20221213 12:19:32 @agent_ppo2.py:137][0m Total time:       3.60 min
[32m[20221213 12:19:32 @agent_ppo2.py:139][0m 245760 total steps have happened
[32m[20221213 12:19:32 @agent_ppo2.py:115][0m #------------------------ Iteration 120 --------------------------#
[32m[20221213 12:19:33 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:19:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:33 @agent_ppo2.py:179][0m |           0.0098 |           1.6536 |           0.7995 |
[32m[20221213 12:19:33 @agent_ppo2.py:179][0m |          -0.0116 |           1.5876 |           0.7985 |
[32m[20221213 12:19:33 @agent_ppo2.py:179][0m |          -0.0180 |           1.5709 |           0.7997 |
[32m[20221213 12:19:33 @agent_ppo2.py:179][0m |          -0.0304 |           1.5477 |           0.8018 |
[32m[20221213 12:19:34 @agent_ppo2.py:179][0m |          -0.0312 |           1.5313 |           0.8068 |
[32m[20221213 12:19:34 @agent_ppo2.py:179][0m |          -0.0327 |           1.5355 |           0.8088 |
[32m[20221213 12:19:34 @agent_ppo2.py:179][0m |          -0.0335 |           1.5200 |           0.8093 |
[32m[20221213 12:19:34 @agent_ppo2.py:179][0m |          -0.0344 |           1.5323 |           0.8099 |
[32m[20221213 12:19:34 @agent_ppo2.py:179][0m |          -0.0393 |           1.5118 |           0.8104 |
[32m[20221213 12:19:34 @agent_ppo2.py:179][0m |          -0.0392 |           1.5050 |           0.8121 |
[32m[20221213 12:19:34 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:19:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.80
[32m[20221213 12:19:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.92
[32m[20221213 12:19:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.93
[32m[20221213 12:19:34 @agent_ppo2.py:137][0m Total time:       3.63 min
[32m[20221213 12:19:34 @agent_ppo2.py:139][0m 247808 total steps have happened
[32m[20221213 12:19:34 @agent_ppo2.py:115][0m #------------------------ Iteration 121 --------------------------#
[32m[20221213 12:19:35 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:19:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:35 @agent_ppo2.py:179][0m |           0.0080 |           1.5666 |           0.8176 |
[32m[20221213 12:19:35 @agent_ppo2.py:179][0m |          -0.0029 |           1.6900 |           0.8200 |
[32m[20221213 12:19:35 @agent_ppo2.py:179][0m |          -0.0168 |           1.5283 |           0.8109 |
[32m[20221213 12:19:35 @agent_ppo2.py:179][0m |          -0.0295 |           1.4981 |           0.8217 |
[32m[20221213 12:19:35 @agent_ppo2.py:179][0m |          -0.0368 |           1.4857 |           0.8229 |
[32m[20221213 12:19:36 @agent_ppo2.py:179][0m |          -0.0364 |           1.5314 |           0.8273 |
[32m[20221213 12:19:36 @agent_ppo2.py:179][0m |          -0.0420 |           1.4745 |           0.8313 |
[32m[20221213 12:19:36 @agent_ppo2.py:179][0m |          -0.0423 |           1.4786 |           0.8294 |
[32m[20221213 12:19:36 @agent_ppo2.py:179][0m |          -0.0445 |           1.4688 |           0.8324 |
[32m[20221213 12:19:36 @agent_ppo2.py:179][0m |          -0.0450 |           1.4667 |           0.8358 |
[32m[20221213 12:19:36 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:19:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.67
[32m[20221213 12:19:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.95
[32m[20221213 12:19:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.03
[32m[20221213 12:19:36 @agent_ppo2.py:137][0m Total time:       3.66 min
[32m[20221213 12:19:36 @agent_ppo2.py:139][0m 249856 total steps have happened
[32m[20221213 12:19:36 @agent_ppo2.py:115][0m #------------------------ Iteration 122 --------------------------#
[32m[20221213 12:19:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:37 @agent_ppo2.py:179][0m |           0.0141 |           1.4731 |           0.8379 |
[32m[20221213 12:19:37 @agent_ppo2.py:179][0m |          -0.0005 |           1.4692 |           0.8318 |
[32m[20221213 12:19:37 @agent_ppo2.py:179][0m |          -0.0205 |           1.4565 |           0.8391 |
[32m[20221213 12:19:37 @agent_ppo2.py:179][0m |          -0.0239 |           1.4544 |           0.8415 |
[32m[20221213 12:19:37 @agent_ppo2.py:179][0m |          -0.0139 |           1.6193 |           0.8404 |
[32m[20221213 12:19:37 @agent_ppo2.py:179][0m |          -0.0293 |           1.4664 |           0.8409 |
[32m[20221213 12:19:38 @agent_ppo2.py:179][0m |          -0.0319 |           1.4483 |           0.8440 |
[32m[20221213 12:19:38 @agent_ppo2.py:179][0m |          -0.0336 |           1.4423 |           0.8482 |
[32m[20221213 12:19:38 @agent_ppo2.py:179][0m |          -0.0347 |           1.4400 |           0.8489 |
[32m[20221213 12:19:38 @agent_ppo2.py:179][0m |          -0.0384 |           1.4423 |           0.8513 |
[32m[20221213 12:19:38 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:19:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.05
[32m[20221213 12:19:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 87.10
[32m[20221213 12:19:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.46
[32m[20221213 12:19:38 @agent_ppo2.py:137][0m Total time:       3.69 min
[32m[20221213 12:19:38 @agent_ppo2.py:139][0m 251904 total steps have happened
[32m[20221213 12:19:38 @agent_ppo2.py:115][0m #------------------------ Iteration 123 --------------------------#
[32m[20221213 12:19:38 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:19:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:39 @agent_ppo2.py:179][0m |           0.0111 |           1.5452 |           0.8276 |
[32m[20221213 12:19:39 @agent_ppo2.py:179][0m |          -0.0015 |           1.5856 |           0.8214 |
[32m[20221213 12:19:39 @agent_ppo2.py:179][0m |          -0.0176 |           1.5271 |           0.8311 |
[32m[20221213 12:19:39 @agent_ppo2.py:179][0m |          -0.0246 |           1.4950 |           0.8325 |
[32m[20221213 12:19:39 @agent_ppo2.py:179][0m |          -0.0237 |           1.5162 |           0.8386 |
[32m[20221213 12:19:39 @agent_ppo2.py:179][0m |          -0.0315 |           1.4785 |           0.8350 |
[32m[20221213 12:19:39 @agent_ppo2.py:179][0m |          -0.0360 |           1.4676 |           0.8425 |
[32m[20221213 12:19:40 @agent_ppo2.py:179][0m |          -0.0393 |           1.4645 |           0.8438 |
[32m[20221213 12:19:40 @agent_ppo2.py:179][0m |          -0.0384 |           1.4890 |           0.8466 |
[32m[20221213 12:19:40 @agent_ppo2.py:179][0m |          -0.0408 |           1.4545 |           0.8493 |
[32m[20221213 12:19:40 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:19:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 76.99
[32m[20221213 12:19:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 79.71
[32m[20221213 12:19:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.25
[32m[20221213 12:19:40 @agent_ppo2.py:137][0m Total time:       3.72 min
[32m[20221213 12:19:40 @agent_ppo2.py:139][0m 253952 total steps have happened
[32m[20221213 12:19:40 @agent_ppo2.py:115][0m #------------------------ Iteration 124 --------------------------#
[32m[20221213 12:19:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:40 @agent_ppo2.py:179][0m |           0.0324 |           1.7403 |           0.8938 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |           0.0015 |           1.5699 |           0.8803 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0109 |           1.5584 |           0.8777 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0249 |           1.5529 |           0.8855 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0265 |           1.5454 |           0.8886 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0289 |           1.5420 |           0.8911 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0392 |           1.5342 |           0.8952 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0331 |           1.5667 |           0.8981 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0420 |           1.5351 |           0.8971 |
[32m[20221213 12:19:41 @agent_ppo2.py:179][0m |          -0.0432 |           1.5312 |           0.8999 |
[32m[20221213 12:19:41 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:19:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.19
[32m[20221213 12:19:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.72
[32m[20221213 12:19:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.89
[32m[20221213 12:19:42 @agent_ppo2.py:137][0m Total time:       3.75 min
[32m[20221213 12:19:42 @agent_ppo2.py:139][0m 256000 total steps have happened
[32m[20221213 12:19:42 @agent_ppo2.py:115][0m #------------------------ Iteration 125 --------------------------#
[32m[20221213 12:19:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:42 @agent_ppo2.py:179][0m |           0.0196 |           1.5752 |           0.8601 |
[32m[20221213 12:19:42 @agent_ppo2.py:179][0m |          -0.0090 |           1.5379 |           0.8661 |
[32m[20221213 12:19:42 @agent_ppo2.py:179][0m |          -0.0135 |           1.5605 |           0.8683 |
[32m[20221213 12:19:43 @agent_ppo2.py:179][0m |          -0.0213 |           1.5061 |           0.8646 |
[32m[20221213 12:19:43 @agent_ppo2.py:179][0m |          -0.0304 |           1.5032 |           0.8735 |
[32m[20221213 12:19:43 @agent_ppo2.py:179][0m |          -0.0315 |           1.4920 |           0.8766 |
[32m[20221213 12:19:43 @agent_ppo2.py:179][0m |          -0.0345 |           1.4932 |           0.8792 |
[32m[20221213 12:19:43 @agent_ppo2.py:179][0m |          -0.0383 |           1.4792 |           0.8815 |
[32m[20221213 12:19:43 @agent_ppo2.py:179][0m |          -0.0177 |           1.7051 |           0.8813 |
[32m[20221213 12:19:43 @agent_ppo2.py:179][0m |          -0.0420 |           1.4912 |           0.8839 |
[32m[20221213 12:19:43 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:19:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.57
[32m[20221213 12:19:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.14
[32m[20221213 12:19:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.73
[32m[20221213 12:19:44 @agent_ppo2.py:137][0m Total time:       3.78 min
[32m[20221213 12:19:44 @agent_ppo2.py:139][0m 258048 total steps have happened
[32m[20221213 12:19:44 @agent_ppo2.py:115][0m #------------------------ Iteration 126 --------------------------#
[32m[20221213 12:19:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:44 @agent_ppo2.py:179][0m |           0.0110 |           1.8004 |           0.8932 |
[32m[20221213 12:19:44 @agent_ppo2.py:179][0m |          -0.0126 |           1.7404 |           0.8984 |
[32m[20221213 12:19:44 @agent_ppo2.py:179][0m |          -0.0247 |           1.7306 |           0.9030 |
[32m[20221213 12:19:44 @agent_ppo2.py:179][0m |          -0.0319 |           1.7268 |           0.9083 |
[32m[20221213 12:19:45 @agent_ppo2.py:179][0m |          -0.0374 |           1.7062 |           0.9113 |
[32m[20221213 12:19:45 @agent_ppo2.py:179][0m |          -0.0447 |           1.7076 |           0.9131 |
[32m[20221213 12:19:45 @agent_ppo2.py:179][0m |          -0.0398 |           1.7167 |           0.9125 |
[32m[20221213 12:19:45 @agent_ppo2.py:179][0m |          -0.0458 |           1.7111 |           0.9163 |
[32m[20221213 12:19:45 @agent_ppo2.py:179][0m |          -0.0453 |           1.7284 |           0.9180 |
[32m[20221213 12:19:45 @agent_ppo2.py:179][0m |          -0.0461 |           1.6942 |           0.9185 |
[32m[20221213 12:19:45 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:19:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.16
[32m[20221213 12:19:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.03
[32m[20221213 12:19:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.03
[32m[20221213 12:19:45 @agent_ppo2.py:137][0m Total time:       3.81 min
[32m[20221213 12:19:45 @agent_ppo2.py:139][0m 260096 total steps have happened
[32m[20221213 12:19:45 @agent_ppo2.py:115][0m #------------------------ Iteration 127 --------------------------#
[32m[20221213 12:19:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:46 @agent_ppo2.py:179][0m |           0.0187 |           1.5521 |           0.9492 |
[32m[20221213 12:19:46 @agent_ppo2.py:179][0m |          -0.0058 |           1.5133 |           0.9384 |
[32m[20221213 12:19:46 @agent_ppo2.py:179][0m |          -0.0283 |           1.5073 |           0.9455 |
[32m[20221213 12:19:46 @agent_ppo2.py:179][0m |          -0.0295 |           1.5038 |           0.9435 |
[32m[20221213 12:19:46 @agent_ppo2.py:179][0m |          -0.0323 |           1.4812 |           0.9486 |
[32m[20221213 12:19:47 @agent_ppo2.py:179][0m |          -0.0361 |           1.4851 |           0.9469 |
[32m[20221213 12:19:47 @agent_ppo2.py:179][0m |          -0.0377 |           1.4767 |           0.9507 |
[32m[20221213 12:19:47 @agent_ppo2.py:179][0m |          -0.0422 |           1.4761 |           0.9511 |
[32m[20221213 12:19:47 @agent_ppo2.py:179][0m |          -0.0486 |           1.4628 |           0.9544 |
[32m[20221213 12:19:47 @agent_ppo2.py:179][0m |          -0.0459 |           1.4659 |           0.9568 |
[32m[20221213 12:19:47 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:19:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 79.84
[32m[20221213 12:19:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.23
[32m[20221213 12:19:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.06
[32m[20221213 12:19:47 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 103.06
[32m[20221213 12:19:47 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 103.06
[32m[20221213 12:19:47 @agent_ppo2.py:137][0m Total time:       3.84 min
[32m[20221213 12:19:47 @agent_ppo2.py:139][0m 262144 total steps have happened
[32m[20221213 12:19:47 @agent_ppo2.py:115][0m #------------------------ Iteration 128 --------------------------#
[32m[20221213 12:19:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:48 @agent_ppo2.py:179][0m |           0.0128 |           1.5675 |           0.9286 |
[32m[20221213 12:19:48 @agent_ppo2.py:179][0m |           0.0000 |           1.5542 |           0.9196 |
[32m[20221213 12:19:48 @agent_ppo2.py:179][0m |          -0.0200 |           1.5301 |           0.9245 |
[32m[20221213 12:19:48 @agent_ppo2.py:179][0m |          -0.0259 |           1.5427 |           0.9285 |
[32m[20221213 12:19:48 @agent_ppo2.py:179][0m |          -0.0326 |           1.5233 |           0.9291 |
[32m[20221213 12:19:48 @agent_ppo2.py:179][0m |          -0.0347 |           1.5269 |           0.9290 |
[32m[20221213 12:19:49 @agent_ppo2.py:179][0m |          -0.0367 |           1.5215 |           0.9346 |
[32m[20221213 12:19:49 @agent_ppo2.py:179][0m |          -0.0303 |           1.6081 |           0.9335 |
[32m[20221213 12:19:49 @agent_ppo2.py:179][0m |          -0.0434 |           1.5190 |           0.9380 |
[32m[20221213 12:19:49 @agent_ppo2.py:179][0m |          -0.0434 |           1.5086 |           0.9418 |
[32m[20221213 12:19:49 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:19:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.36
[32m[20221213 12:19:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 97.25
[32m[20221213 12:19:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.85
[32m[20221213 12:19:49 @agent_ppo2.py:137][0m Total time:       3.87 min
[32m[20221213 12:19:49 @agent_ppo2.py:139][0m 264192 total steps have happened
[32m[20221213 12:19:49 @agent_ppo2.py:115][0m #------------------------ Iteration 129 --------------------------#
[32m[20221213 12:19:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |           0.0216 |           1.7134 |           0.9399 |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |          -0.0096 |           1.6345 |           0.9430 |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |          -0.0214 |           1.6049 |           0.9418 |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |          -0.0284 |           1.5976 |           0.9432 |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |          -0.0337 |           1.5888 |           0.9417 |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |          -0.0356 |           1.5744 |           0.9447 |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |          -0.0298 |           1.6273 |           0.9468 |
[32m[20221213 12:19:50 @agent_ppo2.py:179][0m |          -0.0292 |           1.5816 |           0.9415 |
[32m[20221213 12:19:51 @agent_ppo2.py:179][0m |          -0.0398 |           1.5590 |           0.9514 |
[32m[20221213 12:19:51 @agent_ppo2.py:179][0m |          -0.0389 |           1.5446 |           0.9488 |
[32m[20221213 12:19:51 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:19:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.13
[32m[20221213 12:19:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.54
[32m[20221213 12:19:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 82.31
[32m[20221213 12:19:51 @agent_ppo2.py:137][0m Total time:       3.90 min
[32m[20221213 12:19:51 @agent_ppo2.py:139][0m 266240 total steps have happened
[32m[20221213 12:19:51 @agent_ppo2.py:115][0m #------------------------ Iteration 130 --------------------------#
[32m[20221213 12:19:51 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:19:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:51 @agent_ppo2.py:179][0m |           0.0207 |           1.5343 |           0.9306 |
[32m[20221213 12:19:51 @agent_ppo2.py:179][0m |          -0.0112 |           1.5121 |           0.9306 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0253 |           1.5106 |           0.9288 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0261 |           1.5017 |           0.9301 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0237 |           1.6734 |           0.9300 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0265 |           1.5731 |           0.9327 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0393 |           1.4845 |           0.9333 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0355 |           1.4968 |           0.9352 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0254 |           1.5624 |           0.9317 |
[32m[20221213 12:19:52 @agent_ppo2.py:179][0m |          -0.0427 |           1.4922 |           0.9358 |
[32m[20221213 12:19:52 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:19:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.68
[32m[20221213 12:19:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.69
[32m[20221213 12:19:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 75.70
[32m[20221213 12:19:53 @agent_ppo2.py:137][0m Total time:       3.93 min
[32m[20221213 12:19:53 @agent_ppo2.py:139][0m 268288 total steps have happened
[32m[20221213 12:19:53 @agent_ppo2.py:115][0m #------------------------ Iteration 131 --------------------------#
[32m[20221213 12:19:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:53 @agent_ppo2.py:179][0m |           0.0219 |           1.8672 |           0.9641 |
[32m[20221213 12:19:53 @agent_ppo2.py:179][0m |           0.0144 |           1.7396 |           0.9323 |
[32m[20221213 12:19:53 @agent_ppo2.py:179][0m |          -0.0142 |           1.7281 |           0.9541 |
[32m[20221213 12:19:53 @agent_ppo2.py:179][0m |          -0.0276 |           1.7215 |           0.9592 |
[32m[20221213 12:19:53 @agent_ppo2.py:179][0m |          -0.0336 |           1.7112 |           0.9668 |
[32m[20221213 12:19:54 @agent_ppo2.py:179][0m |          -0.0325 |           1.7169 |           0.9703 |
[32m[20221213 12:19:54 @agent_ppo2.py:179][0m |          -0.0390 |           1.7048 |           0.9708 |
[32m[20221213 12:19:54 @agent_ppo2.py:179][0m |          -0.0398 |           1.6964 |           0.9745 |
[32m[20221213 12:19:54 @agent_ppo2.py:179][0m |          -0.0438 |           1.6911 |           0.9761 |
[32m[20221213 12:19:54 @agent_ppo2.py:179][0m |          -0.0429 |           1.7292 |           0.9781 |
[32m[20221213 12:19:54 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:19:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.64
[32m[20221213 12:19:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.64
[32m[20221213 12:19:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.83
[32m[20221213 12:19:54 @agent_ppo2.py:137][0m Total time:       3.96 min
[32m[20221213 12:19:54 @agent_ppo2.py:139][0m 270336 total steps have happened
[32m[20221213 12:19:54 @agent_ppo2.py:115][0m #------------------------ Iteration 132 --------------------------#
[32m[20221213 12:19:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |           0.0153 |           1.4522 |           0.9473 |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |          -0.0010 |           1.4262 |           0.9426 |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |          -0.0193 |           1.4275 |           0.9519 |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |          -0.0295 |           1.4144 |           0.9484 |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |          -0.0348 |           1.4138 |           0.9481 |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |          -0.0304 |           1.4089 |           0.9474 |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |          -0.0343 |           1.4027 |           0.9531 |
[32m[20221213 12:19:55 @agent_ppo2.py:179][0m |          -0.0322 |           1.4184 |           0.9535 |
[32m[20221213 12:19:56 @agent_ppo2.py:179][0m |          -0.0373 |           1.3922 |           0.9555 |
[32m[20221213 12:19:56 @agent_ppo2.py:179][0m |          -0.0398 |           1.3876 |           0.9601 |
[32m[20221213 12:19:56 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:19:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 77.17
[32m[20221213 12:19:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 83.79
[32m[20221213 12:19:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.08
[32m[20221213 12:19:56 @agent_ppo2.py:137][0m Total time:       3.99 min
[32m[20221213 12:19:56 @agent_ppo2.py:139][0m 272384 total steps have happened
[32m[20221213 12:19:56 @agent_ppo2.py:115][0m #------------------------ Iteration 133 --------------------------#
[32m[20221213 12:19:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:56 @agent_ppo2.py:179][0m |           0.0162 |           1.5923 |           0.9742 |
[32m[20221213 12:19:56 @agent_ppo2.py:179][0m |          -0.0098 |           1.5512 |           0.9797 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0244 |           1.5335 |           0.9854 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0309 |           1.5314 |           0.9899 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0353 |           1.5191 |           0.9909 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0350 |           1.5131 |           0.9898 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0322 |           1.6063 |           0.9973 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0471 |           1.5256 |           1.0023 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0421 |           1.5131 |           1.0024 |
[32m[20221213 12:19:57 @agent_ppo2.py:179][0m |          -0.0471 |           1.4901 |           1.0021 |
[32m[20221213 12:19:57 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:19:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.37
[32m[20221213 12:19:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.23
[32m[20221213 12:19:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.87
[32m[20221213 12:19:58 @agent_ppo2.py:137][0m Total time:       4.01 min
[32m[20221213 12:19:58 @agent_ppo2.py:139][0m 274432 total steps have happened
[32m[20221213 12:19:58 @agent_ppo2.py:115][0m #------------------------ Iteration 134 --------------------------#
[32m[20221213 12:19:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:19:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:19:58 @agent_ppo2.py:179][0m |           0.0153 |           1.4877 |           0.9907 |
[32m[20221213 12:19:58 @agent_ppo2.py:179][0m |          -0.0085 |           1.4695 |           0.9882 |
[32m[20221213 12:19:58 @agent_ppo2.py:179][0m |          -0.0246 |           1.4674 |           0.9889 |
[32m[20221213 12:19:58 @agent_ppo2.py:179][0m |          -0.0228 |           1.4568 |           0.9899 |
[32m[20221213 12:19:59 @agent_ppo2.py:179][0m |          -0.0298 |           1.4462 |           0.9874 |
[32m[20221213 12:19:59 @agent_ppo2.py:179][0m |          -0.0330 |           1.4430 |           0.9864 |
[32m[20221213 12:19:59 @agent_ppo2.py:179][0m |          -0.0341 |           1.4389 |           0.9902 |
[32m[20221213 12:19:59 @agent_ppo2.py:179][0m |          -0.0363 |           1.4338 |           0.9917 |
[32m[20221213 12:19:59 @agent_ppo2.py:179][0m |          -0.0403 |           1.4313 |           0.9939 |
[32m[20221213 12:19:59 @agent_ppo2.py:179][0m |          -0.0342 |           1.4887 |           0.9956 |
[32m[20221213 12:19:59 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:19:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.59
[32m[20221213 12:19:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.94
[32m[20221213 12:19:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.70
[32m[20221213 12:19:59 @agent_ppo2.py:137][0m Total time:       4.04 min
[32m[20221213 12:19:59 @agent_ppo2.py:139][0m 276480 total steps have happened
[32m[20221213 12:19:59 @agent_ppo2.py:115][0m #------------------------ Iteration 135 --------------------------#
[32m[20221213 12:20:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:00 @agent_ppo2.py:179][0m |           0.0267 |           1.6617 |           1.0042 |
[32m[20221213 12:20:00 @agent_ppo2.py:179][0m |           0.0004 |           1.6170 |           1.0000 |
[32m[20221213 12:20:00 @agent_ppo2.py:179][0m |          -0.0238 |           1.6022 |           1.0131 |
[32m[20221213 12:20:00 @agent_ppo2.py:179][0m |          -0.0235 |           1.6262 |           1.0122 |
[32m[20221213 12:20:00 @agent_ppo2.py:179][0m |          -0.0265 |           1.6346 |           1.0152 |
[32m[20221213 12:20:00 @agent_ppo2.py:179][0m |          -0.0302 |           1.7300 |           1.0164 |
[32m[20221213 12:20:00 @agent_ppo2.py:179][0m |          -0.0433 |           1.5856 |           1.0122 |
[32m[20221213 12:20:01 @agent_ppo2.py:179][0m |          -0.0391 |           1.6210 |           1.0188 |
[32m[20221213 12:20:01 @agent_ppo2.py:179][0m |          -0.0427 |           1.5904 |           1.0190 |
[32m[20221213 12:20:01 @agent_ppo2.py:179][0m |          -0.0437 |           1.5819 |           1.0179 |
[32m[20221213 12:20:01 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:20:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.92
[32m[20221213 12:20:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.77
[32m[20221213 12:20:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.22
[32m[20221213 12:20:01 @agent_ppo2.py:137][0m Total time:       4.07 min
[32m[20221213 12:20:01 @agent_ppo2.py:139][0m 278528 total steps have happened
[32m[20221213 12:20:01 @agent_ppo2.py:115][0m #------------------------ Iteration 136 --------------------------#
[32m[20221213 12:20:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:01 @agent_ppo2.py:179][0m |           0.0107 |           1.5890 |           1.0145 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0073 |           1.5512 |           1.0227 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0249 |           1.5082 |           1.0187 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0280 |           1.5093 |           1.0168 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0349 |           1.4973 |           1.0198 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0350 |           1.4932 |           1.0208 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0397 |           1.4879 |           1.0246 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0334 |           1.5506 |           1.0221 |
[32m[20221213 12:20:02 @agent_ppo2.py:179][0m |          -0.0459 |           1.4861 |           1.0296 |
[32m[20221213 12:20:03 @agent_ppo2.py:179][0m |          -0.0424 |           1.4737 |           1.0300 |
[32m[20221213 12:20:03 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:20:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.91
[32m[20221213 12:20:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 85.03
[32m[20221213 12:20:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.49
[32m[20221213 12:20:03 @agent_ppo2.py:137][0m Total time:       4.10 min
[32m[20221213 12:20:03 @agent_ppo2.py:139][0m 280576 total steps have happened
[32m[20221213 12:20:03 @agent_ppo2.py:115][0m #------------------------ Iteration 137 --------------------------#
[32m[20221213 12:20:03 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:20:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:04 @agent_ppo2.py:179][0m |           0.0170 |           1.4803 |           0.9956 |
[32m[20221213 12:20:04 @agent_ppo2.py:179][0m |          -0.0041 |           1.3895 |           0.9817 |
[32m[20221213 12:20:04 @agent_ppo2.py:179][0m |          -0.0243 |           1.3608 |           0.9925 |
[32m[20221213 12:20:04 @agent_ppo2.py:179][0m |          -0.0294 |           1.3469 |           0.9938 |
[32m[20221213 12:20:04 @agent_ppo2.py:179][0m |          -0.0356 |           1.3409 |           0.9975 |
[32m[20221213 12:20:04 @agent_ppo2.py:179][0m |          -0.0359 |           1.3260 |           0.9950 |
[32m[20221213 12:20:05 @agent_ppo2.py:179][0m |          -0.0360 |           1.3103 |           0.9982 |
[32m[20221213 12:20:05 @agent_ppo2.py:179][0m |          -0.0386 |           1.3051 |           0.9998 |
[32m[20221213 12:20:05 @agent_ppo2.py:179][0m |          -0.0407 |           1.3042 |           1.0039 |
[32m[20221213 12:20:05 @agent_ppo2.py:179][0m |          -0.0411 |           1.3018 |           1.0090 |
[32m[20221213 12:20:05 @agent_ppo2.py:124][0m Policy update time: 1.91 s
[32m[20221213 12:20:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.00
[32m[20221213 12:20:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.66
[32m[20221213 12:20:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.69
[32m[20221213 12:20:05 @agent_ppo2.py:137][0m Total time:       4.14 min
[32m[20221213 12:20:05 @agent_ppo2.py:139][0m 282624 total steps have happened
[32m[20221213 12:20:05 @agent_ppo2.py:115][0m #------------------------ Iteration 138 --------------------------#
[32m[20221213 12:20:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:06 @agent_ppo2.py:179][0m |           0.0119 |           1.7132 |           1.0311 |
[32m[20221213 12:20:06 @agent_ppo2.py:179][0m |          -0.0064 |           1.6540 |           1.0210 |
[32m[20221213 12:20:06 @agent_ppo2.py:179][0m |          -0.0112 |           1.6791 |           1.0361 |
[32m[20221213 12:20:06 @agent_ppo2.py:179][0m |          -0.0286 |           1.6467 |           1.0432 |
[32m[20221213 12:20:06 @agent_ppo2.py:179][0m |          -0.0358 |           1.6236 |           1.0477 |
[32m[20221213 12:20:06 @agent_ppo2.py:179][0m |          -0.0302 |           1.6095 |           1.0461 |
[32m[20221213 12:20:06 @agent_ppo2.py:179][0m |          -0.0389 |           1.6053 |           1.0533 |
[32m[20221213 12:20:07 @agent_ppo2.py:179][0m |          -0.0310 |           1.6513 |           1.0581 |
[32m[20221213 12:20:07 @agent_ppo2.py:179][0m |          -0.0376 |           1.6504 |           1.0537 |
[32m[20221213 12:20:07 @agent_ppo2.py:179][0m |          -0.0413 |           1.5988 |           1.0614 |
[32m[20221213 12:20:07 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:20:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.06
[32m[20221213 12:20:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.82
[32m[20221213 12:20:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.20
[32m[20221213 12:20:07 @agent_ppo2.py:137][0m Total time:       4.17 min
[32m[20221213 12:20:07 @agent_ppo2.py:139][0m 284672 total steps have happened
[32m[20221213 12:20:07 @agent_ppo2.py:115][0m #------------------------ Iteration 139 --------------------------#
[32m[20221213 12:20:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:07 @agent_ppo2.py:179][0m |           0.0108 |           1.7336 |           1.0629 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0118 |           1.6868 |           1.0560 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0233 |           1.6837 |           1.0551 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0302 |           1.6648 |           1.0594 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0244 |           1.7249 |           1.0613 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0320 |           1.6528 |           1.0574 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0360 |           1.6780 |           1.0639 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0379 |           1.7686 |           1.0669 |
[32m[20221213 12:20:08 @agent_ppo2.py:179][0m |          -0.0386 |           1.6527 |           1.0657 |
[32m[20221213 12:20:09 @agent_ppo2.py:179][0m |          -0.0408 |           1.6299 |           1.0671 |
[32m[20221213 12:20:09 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:20:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.12
[32m[20221213 12:20:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.65
[32m[20221213 12:20:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 103.39
[32m[20221213 12:20:09 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 103.39
[32m[20221213 12:20:09 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 103.39
[32m[20221213 12:20:09 @agent_ppo2.py:137][0m Total time:       4.20 min
[32m[20221213 12:20:09 @agent_ppo2.py:139][0m 286720 total steps have happened
[32m[20221213 12:20:09 @agent_ppo2.py:115][0m #------------------------ Iteration 140 --------------------------#
[32m[20221213 12:20:09 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:20:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:09 @agent_ppo2.py:179][0m |           0.0086 |           1.7687 |           1.0480 |
[32m[20221213 12:20:09 @agent_ppo2.py:179][0m |          -0.0124 |           1.7331 |           1.0490 |
[32m[20221213 12:20:09 @agent_ppo2.py:179][0m |          -0.0293 |           1.7148 |           1.0507 |
[32m[20221213 12:20:10 @agent_ppo2.py:179][0m |          -0.0327 |           1.6985 |           1.0525 |
[32m[20221213 12:20:10 @agent_ppo2.py:179][0m |          -0.0332 |           1.7036 |           1.0568 |
[32m[20221213 12:20:10 @agent_ppo2.py:179][0m |          -0.0390 |           1.6780 |           1.0570 |
[32m[20221213 12:20:10 @agent_ppo2.py:179][0m |          -0.0370 |           1.6734 |           1.0640 |
[32m[20221213 12:20:10 @agent_ppo2.py:179][0m |          -0.0422 |           1.6679 |           1.0609 |
[32m[20221213 12:20:10 @agent_ppo2.py:179][0m |          -0.0410 |           1.6621 |           1.0673 |
[32m[20221213 12:20:10 @agent_ppo2.py:179][0m |          -0.0454 |           1.6459 |           1.0699 |
[32m[20221213 12:20:10 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:20:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.31
[32m[20221213 12:20:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 90.76
[32m[20221213 12:20:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.91
[32m[20221213 12:20:11 @agent_ppo2.py:137][0m Total time:       4.23 min
[32m[20221213 12:20:11 @agent_ppo2.py:139][0m 288768 total steps have happened
[32m[20221213 12:20:11 @agent_ppo2.py:115][0m #------------------------ Iteration 141 --------------------------#
[32m[20221213 12:20:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:11 @agent_ppo2.py:179][0m |           0.0147 |           1.8217 |           1.0510 |
[32m[20221213 12:20:11 @agent_ppo2.py:179][0m |          -0.0087 |           1.7917 |           1.0518 |
[32m[20221213 12:20:11 @agent_ppo2.py:179][0m |          -0.0246 |           1.7791 |           1.0620 |
[32m[20221213 12:20:11 @agent_ppo2.py:179][0m |          -0.0313 |           1.7719 |           1.0639 |
[32m[20221213 12:20:12 @agent_ppo2.py:179][0m |          -0.0244 |           1.8995 |           1.0672 |
[32m[20221213 12:20:12 @agent_ppo2.py:179][0m |          -0.0357 |           1.7719 |           1.0690 |
[32m[20221213 12:20:12 @agent_ppo2.py:179][0m |          -0.0396 |           1.7567 |           1.0692 |
[32m[20221213 12:20:12 @agent_ppo2.py:179][0m |          -0.0425 |           1.7488 |           1.0752 |
[32m[20221213 12:20:12 @agent_ppo2.py:179][0m |          -0.0409 |           1.7521 |           1.0779 |
[32m[20221213 12:20:12 @agent_ppo2.py:179][0m |          -0.0425 |           1.7462 |           1.0810 |
[32m[20221213 12:20:12 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:20:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.65
[32m[20221213 12:20:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 86.54
[32m[20221213 12:20:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.28
[32m[20221213 12:20:12 @agent_ppo2.py:137][0m Total time:       4.26 min
[32m[20221213 12:20:12 @agent_ppo2.py:139][0m 290816 total steps have happened
[32m[20221213 12:20:12 @agent_ppo2.py:115][0m #------------------------ Iteration 142 --------------------------#
[32m[20221213 12:20:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:13 @agent_ppo2.py:179][0m |           0.0090 |           1.8070 |           1.0855 |
[32m[20221213 12:20:13 @agent_ppo2.py:179][0m |          -0.0070 |           1.7872 |           1.0880 |
[32m[20221213 12:20:13 @agent_ppo2.py:179][0m |          -0.0224 |           1.7901 |           1.0904 |
[32m[20221213 12:20:13 @agent_ppo2.py:179][0m |          -0.0253 |           1.7900 |           1.0909 |
[32m[20221213 12:20:13 @agent_ppo2.py:179][0m |          -0.0260 |           1.8003 |           1.0976 |
[32m[20221213 12:20:13 @agent_ppo2.py:179][0m |          -0.0299 |           1.7765 |           1.1006 |
[32m[20221213 12:20:13 @agent_ppo2.py:179][0m |          -0.0228 |           1.9634 |           1.1040 |
[32m[20221213 12:20:14 @agent_ppo2.py:179][0m |          -0.0245 |           1.7843 |           1.0893 |
[32m[20221213 12:20:14 @agent_ppo2.py:179][0m |          -0.0366 |           1.7767 |           1.1011 |
[32m[20221213 12:20:14 @agent_ppo2.py:179][0m |          -0.0277 |           1.9791 |           1.1078 |
[32m[20221213 12:20:14 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:20:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.90
[32m[20221213 12:20:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 92.72
[32m[20221213 12:20:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 96.98
[32m[20221213 12:20:14 @agent_ppo2.py:137][0m Total time:       4.29 min
[32m[20221213 12:20:14 @agent_ppo2.py:139][0m 292864 total steps have happened
[32m[20221213 12:20:14 @agent_ppo2.py:115][0m #------------------------ Iteration 143 --------------------------#
[32m[20221213 12:20:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:14 @agent_ppo2.py:179][0m |           0.0234 |           2.2019 |           1.1098 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0088 |           2.1645 |           1.1099 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0205 |           2.1479 |           1.1100 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0324 |           2.1445 |           1.1159 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0342 |           2.1498 |           1.1207 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0364 |           2.1401 |           1.1251 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0431 |           2.1182 |           1.1270 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0500 |           2.1246 |           1.1298 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0466 |           2.1150 |           1.1317 |
[32m[20221213 12:20:15 @agent_ppo2.py:179][0m |          -0.0397 |           2.3204 |           1.1337 |
[32m[20221213 12:20:16 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:20:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.43
[32m[20221213 12:20:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.69
[32m[20221213 12:20:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.53
[32m[20221213 12:20:16 @agent_ppo2.py:137][0m Total time:       4.32 min
[32m[20221213 12:20:16 @agent_ppo2.py:139][0m 294912 total steps have happened
[32m[20221213 12:20:16 @agent_ppo2.py:115][0m #------------------------ Iteration 144 --------------------------#
[32m[20221213 12:20:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:16 @agent_ppo2.py:179][0m |           0.0465 |           2.1928 |           1.1608 |
[32m[20221213 12:20:16 @agent_ppo2.py:179][0m |           0.0257 |           1.9690 |           1.1091 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0173 |           1.9420 |           1.1545 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0278 |           1.9307 |           1.1662 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0269 |           2.0151 |           1.1754 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0363 |           1.9191 |           1.1801 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0320 |           2.0098 |           1.1825 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0428 |           1.9148 |           1.1902 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0440 |           1.8896 |           1.1923 |
[32m[20221213 12:20:17 @agent_ppo2.py:179][0m |          -0.0454 |           1.8924 |           1.1969 |
[32m[20221213 12:20:17 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:20:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.31
[32m[20221213 12:20:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.70
[32m[20221213 12:20:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.53
[32m[20221213 12:20:18 @agent_ppo2.py:137][0m Total time:       4.35 min
[32m[20221213 12:20:18 @agent_ppo2.py:139][0m 296960 total steps have happened
[32m[20221213 12:20:18 @agent_ppo2.py:115][0m #------------------------ Iteration 145 --------------------------#
[32m[20221213 12:20:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:18 @agent_ppo2.py:179][0m |           0.0235 |           1.8321 |           1.1893 |
[32m[20221213 12:20:18 @agent_ppo2.py:179][0m |          -0.0077 |           1.7686 |           1.1959 |
[32m[20221213 12:20:18 @agent_ppo2.py:179][0m |          -0.0157 |           1.7669 |           1.1964 |
[32m[20221213 12:20:18 @agent_ppo2.py:179][0m |          -0.0274 |           1.7339 |           1.1963 |
[32m[20221213 12:20:19 @agent_ppo2.py:179][0m |          -0.0339 |           1.7235 |           1.1979 |
[32m[20221213 12:20:19 @agent_ppo2.py:179][0m |          -0.0374 |           1.7087 |           1.2017 |
[32m[20221213 12:20:19 @agent_ppo2.py:179][0m |          -0.0393 |           1.7025 |           1.2033 |
[32m[20221213 12:20:19 @agent_ppo2.py:179][0m |          -0.0402 |           1.6898 |           1.2072 |
[32m[20221213 12:20:19 @agent_ppo2.py:179][0m |          -0.0412 |           1.6885 |           1.2072 |
[32m[20221213 12:20:19 @agent_ppo2.py:179][0m |          -0.0465 |           1.6780 |           1.2145 |
[32m[20221213 12:20:19 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:20:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.23
[32m[20221213 12:20:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.49
[32m[20221213 12:20:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 83.23
[32m[20221213 12:20:19 @agent_ppo2.py:137][0m Total time:       4.38 min
[32m[20221213 12:20:19 @agent_ppo2.py:139][0m 299008 total steps have happened
[32m[20221213 12:20:19 @agent_ppo2.py:115][0m #------------------------ Iteration 146 --------------------------#
[32m[20221213 12:20:20 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:20:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:20 @agent_ppo2.py:179][0m |           0.0135 |           2.3209 |           1.1833 |
[32m[20221213 12:20:20 @agent_ppo2.py:179][0m |          -0.0207 |           2.2016 |           1.1754 |
[32m[20221213 12:20:20 @agent_ppo2.py:179][0m |          -0.0278 |           2.1555 |           1.1832 |
[32m[20221213 12:20:20 @agent_ppo2.py:179][0m |          -0.0315 |           2.1437 |           1.1836 |
[32m[20221213 12:20:20 @agent_ppo2.py:179][0m |          -0.0396 |           2.1174 |           1.1858 |
[32m[20221213 12:20:20 @agent_ppo2.py:179][0m |          -0.0429 |           2.1053 |           1.1915 |
[32m[20221213 12:20:21 @agent_ppo2.py:179][0m |          -0.0511 |           2.0952 |           1.1969 |
[32m[20221213 12:20:21 @agent_ppo2.py:179][0m |          -0.0540 |           2.0747 |           1.1951 |
[32m[20221213 12:20:21 @agent_ppo2.py:179][0m |          -0.0478 |           2.0736 |           1.1962 |
[32m[20221213 12:20:21 @agent_ppo2.py:179][0m |          -0.0517 |           2.0625 |           1.2014 |
[32m[20221213 12:20:21 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:20:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.81
[32m[20221213 12:20:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.69
[32m[20221213 12:20:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.18
[32m[20221213 12:20:21 @agent_ppo2.py:137][0m Total time:       4.41 min
[32m[20221213 12:20:21 @agent_ppo2.py:139][0m 301056 total steps have happened
[32m[20221213 12:20:21 @agent_ppo2.py:115][0m #------------------------ Iteration 147 --------------------------#
[32m[20221213 12:20:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |           0.0163 |           1.9136 |           1.2075 |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |          -0.0037 |           1.8412 |           1.2011 |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |          -0.0164 |           1.8339 |           1.2093 |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |          -0.0241 |           1.8195 |           1.2202 |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |          -0.0345 |           1.8055 |           1.2197 |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |          -0.0440 |           1.7956 |           1.2276 |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |          -0.0403 |           1.7993 |           1.2299 |
[32m[20221213 12:20:22 @agent_ppo2.py:179][0m |          -0.0475 |           1.7837 |           1.2353 |
[32m[20221213 12:20:23 @agent_ppo2.py:179][0m |          -0.0368 |           1.8604 |           1.2376 |
[32m[20221213 12:20:23 @agent_ppo2.py:179][0m |          -0.0473 |           1.7767 |           1.2360 |
[32m[20221213 12:20:23 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:20:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.47
[32m[20221213 12:20:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.69
[32m[20221213 12:20:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 65.74
[32m[20221213 12:20:23 @agent_ppo2.py:137][0m Total time:       4.44 min
[32m[20221213 12:20:23 @agent_ppo2.py:139][0m 303104 total steps have happened
[32m[20221213 12:20:23 @agent_ppo2.py:115][0m #------------------------ Iteration 148 --------------------------#
[32m[20221213 12:20:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:23 @agent_ppo2.py:179][0m |           0.0153 |           2.2394 |           1.2826 |
[32m[20221213 12:20:23 @agent_ppo2.py:179][0m |          -0.0112 |           2.2221 |           1.2796 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0171 |           2.1658 |           1.2783 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0319 |           2.1404 |           1.2846 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0364 |           2.1362 |           1.2918 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0406 |           2.1206 |           1.2947 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0409 |           2.1023 |           1.3028 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0466 |           2.0964 |           1.3056 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0474 |           2.0963 |           1.3058 |
[32m[20221213 12:20:24 @agent_ppo2.py:179][0m |          -0.0505 |           2.0883 |           1.3086 |
[32m[20221213 12:20:24 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:20:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.39
[32m[20221213 12:20:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.92
[32m[20221213 12:20:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.51
[32m[20221213 12:20:25 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 104.51
[32m[20221213 12:20:25 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 104.51
[32m[20221213 12:20:25 @agent_ppo2.py:137][0m Total time:       4.47 min
[32m[20221213 12:20:25 @agent_ppo2.py:139][0m 305152 total steps have happened
[32m[20221213 12:20:25 @agent_ppo2.py:115][0m #------------------------ Iteration 149 --------------------------#
[32m[20221213 12:20:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:25 @agent_ppo2.py:179][0m |           0.0109 |           1.9206 |           1.3184 |
[32m[20221213 12:20:25 @agent_ppo2.py:179][0m |          -0.0107 |           1.8854 |           1.3058 |
[32m[20221213 12:20:25 @agent_ppo2.py:179][0m |          -0.0200 |           1.8882 |           1.3015 |
[32m[20221213 12:20:25 @agent_ppo2.py:179][0m |          -0.0268 |           1.8602 |           1.3026 |
[32m[20221213 12:20:26 @agent_ppo2.py:179][0m |          -0.0333 |           1.8585 |           1.3049 |
[32m[20221213 12:20:26 @agent_ppo2.py:179][0m |          -0.0297 |           1.8934 |           1.3052 |
[32m[20221213 12:20:26 @agent_ppo2.py:179][0m |          -0.0383 |           1.8446 |           1.3083 |
[32m[20221213 12:20:26 @agent_ppo2.py:179][0m |          -0.0434 |           1.8390 |           1.3097 |
[32m[20221213 12:20:26 @agent_ppo2.py:179][0m |          -0.0458 |           1.8354 |           1.3092 |
[32m[20221213 12:20:26 @agent_ppo2.py:179][0m |          -0.0403 |           1.8363 |           1.3154 |
[32m[20221213 12:20:26 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:20:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.10
[32m[20221213 12:20:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.83
[32m[20221213 12:20:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.42
[32m[20221213 12:20:26 @agent_ppo2.py:137][0m Total time:       4.50 min
[32m[20221213 12:20:26 @agent_ppo2.py:139][0m 307200 total steps have happened
[32m[20221213 12:20:26 @agent_ppo2.py:115][0m #------------------------ Iteration 150 --------------------------#
[32m[20221213 12:20:27 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:20:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:27 @agent_ppo2.py:179][0m |           0.0195 |           2.0795 |           1.3084 |
[32m[20221213 12:20:27 @agent_ppo2.py:179][0m |          -0.0168 |           1.9999 |           1.3165 |
[32m[20221213 12:20:27 @agent_ppo2.py:179][0m |          -0.0293 |           1.9835 |           1.3140 |
[32m[20221213 12:20:27 @agent_ppo2.py:179][0m |          -0.0329 |           1.9720 |           1.3210 |
[32m[20221213 12:20:27 @agent_ppo2.py:179][0m |          -0.0342 |           1.9633 |           1.3171 |
[32m[20221213 12:20:28 @agent_ppo2.py:179][0m |          -0.0402 |           1.9774 |           1.3188 |
[32m[20221213 12:20:28 @agent_ppo2.py:179][0m |          -0.0484 |           1.9645 |           1.3252 |
[32m[20221213 12:20:28 @agent_ppo2.py:179][0m |          -0.0429 |           1.9614 |           1.3245 |
[32m[20221213 12:20:28 @agent_ppo2.py:179][0m |          -0.0476 |           1.9488 |           1.3293 |
[32m[20221213 12:20:28 @agent_ppo2.py:179][0m |          -0.0504 |           1.9502 |           1.3316 |
[32m[20221213 12:20:28 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:20:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.01
[32m[20221213 12:20:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.21
[32m[20221213 12:20:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.92
[32m[20221213 12:20:28 @agent_ppo2.py:137][0m Total time:       4.52 min
[32m[20221213 12:20:28 @agent_ppo2.py:139][0m 309248 total steps have happened
[32m[20221213 12:20:28 @agent_ppo2.py:115][0m #------------------------ Iteration 151 --------------------------#
[32m[20221213 12:20:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:29 @agent_ppo2.py:179][0m |           0.0155 |           2.2740 |           1.2865 |
[32m[20221213 12:20:29 @agent_ppo2.py:179][0m |           0.0015 |           2.5050 |           1.2941 |
[32m[20221213 12:20:29 @agent_ppo2.py:179][0m |          -0.0189 |           2.2460 |           1.2823 |
[32m[20221213 12:20:29 @agent_ppo2.py:179][0m |          -0.0300 |           2.2183 |           1.2869 |
[32m[20221213 12:20:29 @agent_ppo2.py:179][0m |          -0.0394 |           2.1939 |           1.2914 |
[32m[20221213 12:20:29 @agent_ppo2.py:179][0m |          -0.0424 |           2.1981 |           1.2958 |
[32m[20221213 12:20:29 @agent_ppo2.py:179][0m |          -0.0444 |           2.1804 |           1.3005 |
[32m[20221213 12:20:30 @agent_ppo2.py:179][0m |          -0.0455 |           2.1840 |           1.3057 |
[32m[20221213 12:20:30 @agent_ppo2.py:179][0m |          -0.0474 |           2.1706 |           1.3069 |
[32m[20221213 12:20:30 @agent_ppo2.py:179][0m |          -0.0490 |           2.1588 |           1.3126 |
[32m[20221213 12:20:30 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:20:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.42
[32m[20221213 12:20:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.51
[32m[20221213 12:20:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.36
[32m[20221213 12:20:30 @agent_ppo2.py:137][0m Total time:       4.55 min
[32m[20221213 12:20:30 @agent_ppo2.py:139][0m 311296 total steps have happened
[32m[20221213 12:20:30 @agent_ppo2.py:115][0m #------------------------ Iteration 152 --------------------------#
[32m[20221213 12:20:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:30 @agent_ppo2.py:179][0m |           0.0132 |           2.1949 |           1.3163 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0124 |           2.1679 |           1.3083 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0255 |           2.1497 |           1.3128 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0279 |           2.1395 |           1.3146 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0331 |           2.1399 |           1.3182 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0384 |           2.1170 |           1.3183 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0304 |           2.2967 |           1.3249 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0344 |           2.3216 |           1.3266 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0446 |           2.1133 |           1.3285 |
[32m[20221213 12:20:31 @agent_ppo2.py:179][0m |          -0.0441 |           2.1185 |           1.3330 |
[32m[20221213 12:20:31 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:20:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.81
[32m[20221213 12:20:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.09
[32m[20221213 12:20:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.47
[32m[20221213 12:20:32 @agent_ppo2.py:137][0m Total time:       4.58 min
[32m[20221213 12:20:32 @agent_ppo2.py:139][0m 313344 total steps have happened
[32m[20221213 12:20:32 @agent_ppo2.py:115][0m #------------------------ Iteration 153 --------------------------#
[32m[20221213 12:20:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:32 @agent_ppo2.py:179][0m |           0.0201 |           1.9940 |           1.2972 |
[32m[20221213 12:20:32 @agent_ppo2.py:179][0m |          -0.0097 |           1.9622 |           1.2932 |
[32m[20221213 12:20:32 @agent_ppo2.py:179][0m |          -0.0232 |           1.9501 |           1.2955 |
[32m[20221213 12:20:32 @agent_ppo2.py:179][0m |          -0.0288 |           1.9350 |           1.3001 |
[32m[20221213 12:20:33 @agent_ppo2.py:179][0m |          -0.0349 |           1.9510 |           1.3002 |
[32m[20221213 12:20:33 @agent_ppo2.py:179][0m |          -0.0344 |           1.9475 |           1.3018 |
[32m[20221213 12:20:33 @agent_ppo2.py:179][0m |          -0.0422 |           1.9401 |           1.3069 |
[32m[20221213 12:20:33 @agent_ppo2.py:179][0m |          -0.0447 |           1.9239 |           1.3084 |
[32m[20221213 12:20:33 @agent_ppo2.py:179][0m |          -0.0437 |           1.9164 |           1.3095 |
[32m[20221213 12:20:33 @agent_ppo2.py:179][0m |          -0.0450 |           1.9246 |           1.3171 |
[32m[20221213 12:20:33 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:20:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.11
[32m[20221213 12:20:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.94
[32m[20221213 12:20:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.89
[32m[20221213 12:20:33 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 113.89
[32m[20221213 12:20:33 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 113.89
[32m[20221213 12:20:33 @agent_ppo2.py:137][0m Total time:       4.61 min
[32m[20221213 12:20:33 @agent_ppo2.py:139][0m 315392 total steps have happened
[32m[20221213 12:20:33 @agent_ppo2.py:115][0m #------------------------ Iteration 154 --------------------------#
[32m[20221213 12:20:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:34 @agent_ppo2.py:179][0m |           0.0344 |           2.9371 |           1.2975 |
[32m[20221213 12:20:34 @agent_ppo2.py:179][0m |          -0.0032 |           2.7385 |           1.3064 |
[32m[20221213 12:20:34 @agent_ppo2.py:179][0m |          -0.0238 |           2.6870 |           1.3274 |
[32m[20221213 12:20:34 @agent_ppo2.py:179][0m |          -0.0247 |           2.6794 |           1.3369 |
[32m[20221213 12:20:34 @agent_ppo2.py:179][0m |          -0.0293 |           2.6732 |           1.3387 |
[32m[20221213 12:20:34 @agent_ppo2.py:179][0m |          -0.0417 |           2.6531 |           1.3430 |
[32m[20221213 12:20:35 @agent_ppo2.py:179][0m |          -0.0427 |           2.6328 |           1.3481 |
[32m[20221213 12:20:35 @agent_ppo2.py:179][0m |          -0.0451 |           2.6283 |           1.3553 |
[32m[20221213 12:20:35 @agent_ppo2.py:179][0m |          -0.0407 |           2.6295 |           1.3530 |
[32m[20221213 12:20:35 @agent_ppo2.py:179][0m |          -0.0470 |           2.6259 |           1.3565 |
[32m[20221213 12:20:35 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:20:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.08
[32m[20221213 12:20:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.78
[32m[20221213 12:20:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.91
[32m[20221213 12:20:35 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 115.91
[32m[20221213 12:20:35 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 115.91
[32m[20221213 12:20:35 @agent_ppo2.py:137][0m Total time:       4.64 min
[32m[20221213 12:20:35 @agent_ppo2.py:139][0m 317440 total steps have happened
[32m[20221213 12:20:35 @agent_ppo2.py:115][0m #------------------------ Iteration 155 --------------------------#
[32m[20221213 12:20:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:35 @agent_ppo2.py:179][0m |           0.0170 |           1.9580 |           1.3407 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0078 |           1.9024 |           1.3328 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0255 |           1.8801 |           1.3484 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0162 |           2.0962 |           1.3467 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0317 |           1.8696 |           1.3562 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0336 |           1.8557 |           1.3597 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0383 |           1.8438 |           1.3636 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0393 |           1.8391 |           1.3680 |
[32m[20221213 12:20:36 @agent_ppo2.py:179][0m |          -0.0401 |           1.8428 |           1.3710 |
[32m[20221213 12:20:37 @agent_ppo2.py:179][0m |          -0.0416 |           1.8382 |           1.3758 |
[32m[20221213 12:20:37 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:20:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.64
[32m[20221213 12:20:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.84
[32m[20221213 12:20:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.63
[32m[20221213 12:20:37 @agent_ppo2.py:137][0m Total time:       4.67 min
[32m[20221213 12:20:37 @agent_ppo2.py:139][0m 319488 total steps have happened
[32m[20221213 12:20:37 @agent_ppo2.py:115][0m #------------------------ Iteration 156 --------------------------#
[32m[20221213 12:20:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:37 @agent_ppo2.py:179][0m |           0.0237 |           2.0205 |           1.3779 |
[32m[20221213 12:20:37 @agent_ppo2.py:179][0m |           0.0103 |           1.9413 |           1.3439 |
[32m[20221213 12:20:37 @agent_ppo2.py:179][0m |          -0.0173 |           1.9471 |           1.3766 |
[32m[20221213 12:20:37 @agent_ppo2.py:179][0m |          -0.0287 |           1.9161 |           1.3719 |
[32m[20221213 12:20:38 @agent_ppo2.py:179][0m |          -0.0327 |           1.9065 |           1.3881 |
[32m[20221213 12:20:38 @agent_ppo2.py:179][0m |          -0.0370 |           1.9114 |           1.3951 |
[32m[20221213 12:20:38 @agent_ppo2.py:179][0m |          -0.0394 |           1.8956 |           1.3971 |
[32m[20221213 12:20:38 @agent_ppo2.py:179][0m |          -0.0475 |           1.8910 |           1.3992 |
[32m[20221213 12:20:38 @agent_ppo2.py:179][0m |          -0.0409 |           1.8910 |           1.4031 |
[32m[20221213 12:20:38 @agent_ppo2.py:179][0m |          -0.0433 |           1.9002 |           1.4038 |
[32m[20221213 12:20:38 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:20:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.12
[32m[20221213 12:20:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.04
[32m[20221213 12:20:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.16
[32m[20221213 12:20:38 @agent_ppo2.py:137][0m Total time:       4.69 min
[32m[20221213 12:20:38 @agent_ppo2.py:139][0m 321536 total steps have happened
[32m[20221213 12:20:38 @agent_ppo2.py:115][0m #------------------------ Iteration 157 --------------------------#
[32m[20221213 12:20:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:39 @agent_ppo2.py:179][0m |           0.0241 |           2.0186 |           1.3807 |
[32m[20221213 12:20:39 @agent_ppo2.py:179][0m |           0.0059 |           1.8988 |           1.3527 |
[32m[20221213 12:20:39 @agent_ppo2.py:179][0m |          -0.0199 |           1.8852 |           1.3839 |
[32m[20221213 12:20:39 @agent_ppo2.py:179][0m |          -0.0191 |           1.9030 |           1.3836 |
[32m[20221213 12:20:39 @agent_ppo2.py:179][0m |          -0.0286 |           1.8780 |           1.3913 |
[32m[20221213 12:20:39 @agent_ppo2.py:179][0m |          -0.0326 |           1.8768 |           1.3926 |
[32m[20221213 12:20:40 @agent_ppo2.py:179][0m |          -0.0384 |           1.8729 |           1.3988 |
[32m[20221213 12:20:40 @agent_ppo2.py:179][0m |          -0.0373 |           1.8716 |           1.4019 |
[32m[20221213 12:20:40 @agent_ppo2.py:179][0m |          -0.0393 |           1.8784 |           1.4011 |
[32m[20221213 12:20:40 @agent_ppo2.py:179][0m |          -0.0390 |           1.8680 |           1.4072 |
[32m[20221213 12:20:40 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:20:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.57
[32m[20221213 12:20:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.50
[32m[20221213 12:20:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.91
[32m[20221213 12:20:40 @agent_ppo2.py:137][0m Total time:       4.72 min
[32m[20221213 12:20:40 @agent_ppo2.py:139][0m 323584 total steps have happened
[32m[20221213 12:20:40 @agent_ppo2.py:115][0m #------------------------ Iteration 158 --------------------------#
[32m[20221213 12:20:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |           0.0271 |           2.1500 |           1.4256 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0057 |           2.1290 |           1.4155 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0249 |           2.1254 |           1.4270 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0315 |           2.1118 |           1.4328 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0317 |           2.1966 |           1.4375 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0338 |           2.1386 |           1.4471 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0423 |           2.0962 |           1.4540 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0376 |           2.3411 |           1.4538 |
[32m[20221213 12:20:41 @agent_ppo2.py:179][0m |          -0.0481 |           2.0978 |           1.4554 |
[32m[20221213 12:20:42 @agent_ppo2.py:179][0m |          -0.0421 |           2.1048 |           1.4623 |
[32m[20221213 12:20:42 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:20:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.37
[32m[20221213 12:20:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.69
[32m[20221213 12:20:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 100.96
[32m[20221213 12:20:42 @agent_ppo2.py:137][0m Total time:       4.75 min
[32m[20221213 12:20:42 @agent_ppo2.py:139][0m 325632 total steps have happened
[32m[20221213 12:20:42 @agent_ppo2.py:115][0m #------------------------ Iteration 159 --------------------------#
[32m[20221213 12:20:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:42 @agent_ppo2.py:179][0m |           0.0154 |           2.1390 |           1.4756 |
[32m[20221213 12:20:42 @agent_ppo2.py:179][0m |          -0.0087 |           2.1280 |           1.4568 |
[32m[20221213 12:20:42 @agent_ppo2.py:179][0m |          -0.0003 |           2.3406 |           1.4748 |
[32m[20221213 12:20:43 @agent_ppo2.py:179][0m |          -0.0255 |           2.1315 |           1.4782 |
[32m[20221213 12:20:43 @agent_ppo2.py:179][0m |          -0.0295 |           2.0990 |           1.4793 |
[32m[20221213 12:20:43 @agent_ppo2.py:179][0m |          -0.0336 |           2.0985 |           1.4845 |
[32m[20221213 12:20:43 @agent_ppo2.py:179][0m |          -0.0350 |           2.0994 |           1.4877 |
[32m[20221213 12:20:43 @agent_ppo2.py:179][0m |          -0.0298 |           2.1566 |           1.4918 |
[32m[20221213 12:20:43 @agent_ppo2.py:179][0m |          -0.0331 |           2.0906 |           1.4808 |
[32m[20221213 12:20:43 @agent_ppo2.py:179][0m |          -0.0371 |           2.0837 |           1.5006 |
[32m[20221213 12:20:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:20:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.62
[32m[20221213 12:20:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.27
[32m[20221213 12:20:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.72
[32m[20221213 12:20:43 @agent_ppo2.py:137][0m Total time:       4.78 min
[32m[20221213 12:20:43 @agent_ppo2.py:139][0m 327680 total steps have happened
[32m[20221213 12:20:43 @agent_ppo2.py:115][0m #------------------------ Iteration 160 --------------------------#
[32m[20221213 12:20:44 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:20:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:44 @agent_ppo2.py:179][0m |           0.0063 |           1.9862 |           1.4536 |
[32m[20221213 12:20:44 @agent_ppo2.py:179][0m |          -0.0166 |           1.9505 |           1.4588 |
[32m[20221213 12:20:44 @agent_ppo2.py:179][0m |          -0.0235 |           1.9360 |           1.4663 |
[32m[20221213 12:20:44 @agent_ppo2.py:179][0m |          -0.0312 |           1.9193 |           1.4701 |
[32m[20221213 12:20:44 @agent_ppo2.py:179][0m |          -0.0383 |           1.9163 |           1.4783 |
[32m[20221213 12:20:44 @agent_ppo2.py:179][0m |          -0.0402 |           1.9070 |           1.4798 |
[32m[20221213 12:20:45 @agent_ppo2.py:179][0m |          -0.0438 |           1.9012 |           1.4887 |
[32m[20221213 12:20:45 @agent_ppo2.py:179][0m |          -0.0464 |           1.9010 |           1.4880 |
[32m[20221213 12:20:45 @agent_ppo2.py:179][0m |          -0.0459 |           1.8881 |           1.4927 |
[32m[20221213 12:20:45 @agent_ppo2.py:179][0m |          -0.0454 |           1.9086 |           1.4945 |
[32m[20221213 12:20:45 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:20:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.97
[32m[20221213 12:20:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 99.33
[32m[20221213 12:20:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.00
[32m[20221213 12:20:45 @agent_ppo2.py:137][0m Total time:       4.81 min
[32m[20221213 12:20:45 @agent_ppo2.py:139][0m 329728 total steps have happened
[32m[20221213 12:20:45 @agent_ppo2.py:115][0m #------------------------ Iteration 161 --------------------------#
[32m[20221213 12:20:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |           0.0067 |           2.0324 |           1.5171 |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |           0.0045 |           1.9960 |           1.5113 |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |          -0.0186 |           1.9698 |           1.5103 |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |          -0.0289 |           1.9849 |           1.5183 |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |          -0.0345 |           1.9592 |           1.5223 |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |          -0.0315 |           1.9527 |           1.5318 |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |          -0.0332 |           1.9641 |           1.5366 |
[32m[20221213 12:20:46 @agent_ppo2.py:179][0m |          -0.0414 |           1.9548 |           1.5375 |
[32m[20221213 12:20:47 @agent_ppo2.py:179][0m |          -0.0352 |           1.9730 |           1.5380 |
[32m[20221213 12:20:47 @agent_ppo2.py:179][0m |          -0.0448 |           1.9361 |           1.5477 |
[32m[20221213 12:20:47 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:20:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 88.51
[32m[20221213 12:20:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 91.32
[32m[20221213 12:20:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 95.26
[32m[20221213 12:20:47 @agent_ppo2.py:137][0m Total time:       4.84 min
[32m[20221213 12:20:47 @agent_ppo2.py:139][0m 331776 total steps have happened
[32m[20221213 12:20:47 @agent_ppo2.py:115][0m #------------------------ Iteration 162 --------------------------#
[32m[20221213 12:20:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:47 @agent_ppo2.py:179][0m |           0.0187 |           2.2478 |           1.5609 |
[32m[20221213 12:20:48 @agent_ppo2.py:179][0m |           0.0273 |           2.1970 |           1.5172 |
[32m[20221213 12:20:48 @agent_ppo2.py:179][0m |          -0.0018 |           2.1794 |           1.5419 |
[32m[20221213 12:20:48 @agent_ppo2.py:179][0m |          -0.0200 |           2.1730 |           1.5406 |
[32m[20221213 12:20:48 @agent_ppo2.py:179][0m |          -0.0271 |           2.1567 |           1.5582 |
[32m[20221213 12:20:48 @agent_ppo2.py:179][0m |          -0.0310 |           2.1688 |           1.5651 |
[32m[20221213 12:20:48 @agent_ppo2.py:179][0m |          -0.0301 |           2.2074 |           1.5679 |
[32m[20221213 12:20:48 @agent_ppo2.py:179][0m |          -0.0360 |           2.1467 |           1.5715 |
[32m[20221213 12:20:49 @agent_ppo2.py:179][0m |          -0.0393 |           2.1428 |           1.5744 |
[32m[20221213 12:20:49 @agent_ppo2.py:179][0m |          -0.0387 |           2.1385 |           1.5842 |
[32m[20221213 12:20:49 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:20:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.46
[32m[20221213 12:20:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.25
[32m[20221213 12:20:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.66
[32m[20221213 12:20:49 @agent_ppo2.py:137][0m Total time:       4.87 min
[32m[20221213 12:20:49 @agent_ppo2.py:139][0m 333824 total steps have happened
[32m[20221213 12:20:49 @agent_ppo2.py:115][0m #------------------------ Iteration 163 --------------------------#
[32m[20221213 12:20:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:49 @agent_ppo2.py:179][0m |           0.0080 |           2.1358 |           1.5737 |
[32m[20221213 12:20:49 @agent_ppo2.py:179][0m |          -0.0114 |           2.0939 |           1.5658 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0294 |           2.0798 |           1.5656 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0332 |           2.0439 |           1.5723 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0368 |           2.0328 |           1.5755 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0364 |           2.0263 |           1.5779 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0464 |           2.0120 |           1.5789 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0443 |           2.0043 |           1.5865 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0240 |           2.4201 |           1.5931 |
[32m[20221213 12:20:50 @agent_ppo2.py:179][0m |          -0.0420 |           2.0412 |           1.5896 |
[32m[20221213 12:20:50 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:20:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.94
[32m[20221213 12:20:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.64
[32m[20221213 12:20:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 73.33
[32m[20221213 12:20:51 @agent_ppo2.py:137][0m Total time:       4.90 min
[32m[20221213 12:20:51 @agent_ppo2.py:139][0m 335872 total steps have happened
[32m[20221213 12:20:51 @agent_ppo2.py:115][0m #------------------------ Iteration 164 --------------------------#
[32m[20221213 12:20:51 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:20:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:51 @agent_ppo2.py:179][0m |           0.0224 |           2.1101 |           1.5659 |
[32m[20221213 12:20:51 @agent_ppo2.py:179][0m |          -0.0024 |           2.0016 |           1.5416 |
[32m[20221213 12:20:51 @agent_ppo2.py:179][0m |          -0.0209 |           1.9947 |           1.5657 |
[32m[20221213 12:20:51 @agent_ppo2.py:179][0m |          -0.0291 |           1.9935 |           1.5704 |
[32m[20221213 12:20:52 @agent_ppo2.py:179][0m |          -0.0192 |           2.2132 |           1.5747 |
[32m[20221213 12:20:52 @agent_ppo2.py:179][0m |          -0.0328 |           1.9937 |           1.5774 |
[32m[20221213 12:20:52 @agent_ppo2.py:179][0m |          -0.0275 |           2.0045 |           1.5842 |
[32m[20221213 12:20:52 @agent_ppo2.py:179][0m |          -0.0317 |           1.9972 |           1.5804 |
[32m[20221213 12:20:52 @agent_ppo2.py:179][0m |          -0.0339 |           1.9905 |           1.5795 |
[32m[20221213 12:20:52 @agent_ppo2.py:179][0m |          -0.0378 |           1.9732 |           1.5880 |
[32m[20221213 12:20:52 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:20:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.70
[32m[20221213 12:20:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 95.94
[32m[20221213 12:20:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 94.95
[32m[20221213 12:20:52 @agent_ppo2.py:137][0m Total time:       4.93 min
[32m[20221213 12:20:52 @agent_ppo2.py:139][0m 337920 total steps have happened
[32m[20221213 12:20:52 @agent_ppo2.py:115][0m #------------------------ Iteration 165 --------------------------#
[32m[20221213 12:20:53 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:20:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:53 @agent_ppo2.py:179][0m |           0.0365 |           2.2890 |           1.5576 |
[32m[20221213 12:20:53 @agent_ppo2.py:179][0m |           0.0475 |           2.1841 |           1.3521 |
[32m[20221213 12:20:53 @agent_ppo2.py:179][0m |          -0.0054 |           2.1603 |           1.3899 |
[32m[20221213 12:20:53 @agent_ppo2.py:179][0m |          -0.0121 |           2.1552 |           1.4538 |
[32m[20221213 12:20:53 @agent_ppo2.py:179][0m |          -0.0220 |           2.1115 |           1.4711 |
[32m[20221213 12:20:54 @agent_ppo2.py:179][0m |          -0.0240 |           2.0883 |           1.4850 |
[32m[20221213 12:20:54 @agent_ppo2.py:179][0m |          -0.0271 |           2.0794 |           1.5054 |
[32m[20221213 12:20:54 @agent_ppo2.py:179][0m |          -0.0281 |           2.0619 |           1.5100 |
[32m[20221213 12:20:54 @agent_ppo2.py:179][0m |          -0.0340 |           2.0503 |           1.5088 |
[32m[20221213 12:20:54 @agent_ppo2.py:179][0m |          -0.0342 |           2.0559 |           1.5199 |
[32m[20221213 12:20:54 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:20:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.21
[32m[20221213 12:20:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.82
[32m[20221213 12:20:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.68
[32m[20221213 12:20:54 @agent_ppo2.py:137][0m Total time:       4.96 min
[32m[20221213 12:20:54 @agent_ppo2.py:139][0m 339968 total steps have happened
[32m[20221213 12:20:54 @agent_ppo2.py:115][0m #------------------------ Iteration 166 --------------------------#
[32m[20221213 12:20:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:55 @agent_ppo2.py:179][0m |           0.0295 |           2.3773 |           1.5692 |
[32m[20221213 12:20:55 @agent_ppo2.py:179][0m |          -0.0097 |           2.0827 |           1.5360 |
[32m[20221213 12:20:55 @agent_ppo2.py:179][0m |          -0.0201 |           2.0317 |           1.5579 |
[32m[20221213 12:20:55 @agent_ppo2.py:179][0m |          -0.0283 |           2.0396 |           1.5614 |
[32m[20221213 12:20:55 @agent_ppo2.py:179][0m |          -0.0345 |           2.0045 |           1.5601 |
[32m[20221213 12:20:55 @agent_ppo2.py:179][0m |          -0.0390 |           1.9917 |           1.5717 |
[32m[20221213 12:20:55 @agent_ppo2.py:179][0m |          -0.0355 |           2.0368 |           1.5706 |
[32m[20221213 12:20:56 @agent_ppo2.py:179][0m |          -0.0469 |           1.9852 |           1.5774 |
[32m[20221213 12:20:56 @agent_ppo2.py:179][0m |          -0.0469 |           1.9788 |           1.5781 |
[32m[20221213 12:20:56 @agent_ppo2.py:179][0m |          -0.0423 |           1.9723 |           1.5845 |
[32m[20221213 12:20:56 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:20:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.43
[32m[20221213 12:20:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.68
[32m[20221213 12:20:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.29
[32m[20221213 12:20:56 @agent_ppo2.py:137][0m Total time:       4.99 min
[32m[20221213 12:20:56 @agent_ppo2.py:139][0m 342016 total steps have happened
[32m[20221213 12:20:56 @agent_ppo2.py:115][0m #------------------------ Iteration 167 --------------------------#
[32m[20221213 12:20:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:56 @agent_ppo2.py:179][0m |           0.0176 |           1.8965 |           1.5896 |
[32m[20221213 12:20:57 @agent_ppo2.py:179][0m |          -0.0087 |           1.8519 |           1.5947 |
[32m[20221213 12:20:57 @agent_ppo2.py:179][0m |          -0.0250 |           1.8254 |           1.6052 |
[32m[20221213 12:20:57 @agent_ppo2.py:179][0m |          -0.0292 |           1.8166 |           1.6065 |
[32m[20221213 12:20:57 @agent_ppo2.py:179][0m |          -0.0332 |           1.7973 |           1.6128 |
[32m[20221213 12:20:57 @agent_ppo2.py:179][0m |          -0.0353 |           1.7841 |           1.6113 |
[32m[20221213 12:20:57 @agent_ppo2.py:179][0m |          -0.0354 |           1.7706 |           1.6192 |
[32m[20221213 12:20:57 @agent_ppo2.py:179][0m |          -0.0446 |           1.7590 |           1.6239 |
[32m[20221213 12:20:58 @agent_ppo2.py:179][0m |          -0.0429 |           1.7549 |           1.6292 |
[32m[20221213 12:20:58 @agent_ppo2.py:179][0m |          -0.0362 |           1.8366 |           1.6321 |
[32m[20221213 12:20:58 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:20:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 94.14
[32m[20221213 12:20:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.78
[32m[20221213 12:20:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 86.67
[32m[20221213 12:20:58 @agent_ppo2.py:137][0m Total time:       5.02 min
[32m[20221213 12:20:58 @agent_ppo2.py:139][0m 344064 total steps have happened
[32m[20221213 12:20:58 @agent_ppo2.py:115][0m #------------------------ Iteration 168 --------------------------#
[32m[20221213 12:20:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:20:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:20:58 @agent_ppo2.py:179][0m |           0.0202 |           2.0961 |           1.6472 |
[32m[20221213 12:20:59 @agent_ppo2.py:179][0m |          -0.0078 |           2.0145 |           1.6347 |
[32m[20221213 12:20:59 @agent_ppo2.py:179][0m |          -0.0243 |           2.0288 |           1.6484 |
[32m[20221213 12:20:59 @agent_ppo2.py:179][0m |          -0.0317 |           1.9869 |           1.6496 |
[32m[20221213 12:20:59 @agent_ppo2.py:179][0m |          -0.0401 |           1.9761 |           1.6492 |
[32m[20221213 12:20:59 @agent_ppo2.py:179][0m |          -0.0412 |           1.9699 |           1.6532 |
[32m[20221213 12:20:59 @agent_ppo2.py:179][0m |          -0.0453 |           1.9611 |           1.6582 |
[32m[20221213 12:20:59 @agent_ppo2.py:179][0m |          -0.0457 |           1.9609 |           1.6532 |
[32m[20221213 12:21:00 @agent_ppo2.py:179][0m |          -0.0482 |           1.9480 |           1.6571 |
[32m[20221213 12:21:00 @agent_ppo2.py:179][0m |          -0.0458 |           1.9593 |           1.6642 |
[32m[20221213 12:21:00 @agent_ppo2.py:124][0m Policy update time: 1.66 s
[32m[20221213 12:21:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.91
[32m[20221213 12:21:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.35
[32m[20221213 12:21:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 87.26
[32m[20221213 12:21:00 @agent_ppo2.py:137][0m Total time:       5.05 min
[32m[20221213 12:21:00 @agent_ppo2.py:139][0m 346112 total steps have happened
[32m[20221213 12:21:00 @agent_ppo2.py:115][0m #------------------------ Iteration 169 --------------------------#
[32m[20221213 12:21:00 @agent_ppo2.py:121][0m Sampling time: 0.27 s by 5 slaves
[32m[20221213 12:21:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:01 @agent_ppo2.py:179][0m |           0.0211 |           1.9895 |           1.5956 |
[32m[20221213 12:21:01 @agent_ppo2.py:179][0m |          -0.0035 |           1.9526 |           1.6039 |
[32m[20221213 12:21:01 @agent_ppo2.py:179][0m |          -0.0225 |           1.9539 |           1.6136 |
[32m[20221213 12:21:01 @agent_ppo2.py:179][0m |          -0.0320 |           1.9429 |           1.6164 |
[32m[20221213 12:21:01 @agent_ppo2.py:179][0m |          -0.0324 |           1.9319 |           1.6186 |
[32m[20221213 12:21:01 @agent_ppo2.py:179][0m |          -0.0343 |           1.9802 |           1.6254 |
[32m[20221213 12:21:01 @agent_ppo2.py:179][0m |          -0.0427 |           1.9328 |           1.6288 |
[32m[20221213 12:21:02 @agent_ppo2.py:179][0m |          -0.0379 |           1.9275 |           1.6381 |
[32m[20221213 12:21:02 @agent_ppo2.py:179][0m |          -0.0391 |           1.9210 |           1.6373 |
[32m[20221213 12:21:02 @agent_ppo2.py:179][0m |          -0.0448 |           1.9187 |           1.6465 |
[32m[20221213 12:21:02 @agent_ppo2.py:124][0m Policy update time: 1.65 s
[32m[20221213 12:21:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 87.85
[32m[20221213 12:21:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 89.82
[32m[20221213 12:21:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.23
[32m[20221213 12:21:02 @agent_ppo2.py:137][0m Total time:       5.09 min
[32m[20221213 12:21:02 @agent_ppo2.py:139][0m 348160 total steps have happened
[32m[20221213 12:21:02 @agent_ppo2.py:115][0m #------------------------ Iteration 170 --------------------------#
[32m[20221213 12:21:02 @agent_ppo2.py:121][0m Sampling time: 0.27 s by 5 slaves
[32m[20221213 12:21:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:03 @agent_ppo2.py:179][0m |           0.0269 |           2.2112 |           1.6568 |
[32m[20221213 12:21:03 @agent_ppo2.py:179][0m |          -0.0016 |           2.1586 |           1.6582 |
[32m[20221213 12:21:03 @agent_ppo2.py:179][0m |          -0.0269 |           2.1444 |           1.6721 |
[32m[20221213 12:21:03 @agent_ppo2.py:179][0m |          -0.0369 |           2.1186 |           1.6758 |
[32m[20221213 12:21:03 @agent_ppo2.py:179][0m |          -0.0375 |           2.1117 |           1.6896 |
[32m[20221213 12:21:03 @agent_ppo2.py:179][0m |          -0.0394 |           2.0876 |           1.6924 |
[32m[20221213 12:21:04 @agent_ppo2.py:179][0m |          -0.0407 |           2.1034 |           1.7026 |
[32m[20221213 12:21:04 @agent_ppo2.py:179][0m |          -0.0453 |           2.0757 |           1.7040 |
[32m[20221213 12:21:04 @agent_ppo2.py:179][0m |          -0.0518 |           2.0562 |           1.7102 |
[32m[20221213 12:21:04 @agent_ppo2.py:179][0m |          -0.0488 |           2.0468 |           1.7124 |
[32m[20221213 12:21:04 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:21:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.90
[32m[20221213 12:21:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 101.79
[32m[20221213 12:21:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.76
[32m[20221213 12:21:04 @agent_ppo2.py:137][0m Total time:       5.12 min
[32m[20221213 12:21:04 @agent_ppo2.py:139][0m 350208 total steps have happened
[32m[20221213 12:21:04 @agent_ppo2.py:115][0m #------------------------ Iteration 171 --------------------------#
[32m[20221213 12:21:04 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:21:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:05 @agent_ppo2.py:179][0m |           0.0101 |           2.0486 |           1.7224 |
[32m[20221213 12:21:05 @agent_ppo2.py:179][0m |          -0.0149 |           2.0064 |           1.7149 |
[32m[20221213 12:21:05 @agent_ppo2.py:179][0m |          -0.0232 |           1.9668 |           1.7200 |
[32m[20221213 12:21:05 @agent_ppo2.py:179][0m |          -0.0274 |           1.9715 |           1.7370 |
[32m[20221213 12:21:05 @agent_ppo2.py:179][0m |          -0.0356 |           1.9275 |           1.7366 |
[32m[20221213 12:21:06 @agent_ppo2.py:179][0m |          -0.0439 |           1.9159 |           1.7408 |
[32m[20221213 12:21:06 @agent_ppo2.py:179][0m |          -0.0449 |           1.8942 |           1.7544 |
[32m[20221213 12:21:06 @agent_ppo2.py:179][0m |          -0.0453 |           1.8744 |           1.7612 |
[32m[20221213 12:21:06 @agent_ppo2.py:179][0m |          -0.0483 |           1.8542 |           1.7620 |
[32m[20221213 12:21:06 @agent_ppo2.py:179][0m |          -0.0537 |           1.8669 |           1.7695 |
[32m[20221213 12:21:06 @agent_ppo2.py:124][0m Policy update time: 1.68 s
[32m[20221213 12:21:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.99
[32m[20221213 12:21:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 109.97
[32m[20221213 12:21:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.31
[32m[20221213 12:21:06 @agent_ppo2.py:137][0m Total time:       5.16 min
[32m[20221213 12:21:06 @agent_ppo2.py:139][0m 352256 total steps have happened
[32m[20221213 12:21:06 @agent_ppo2.py:115][0m #------------------------ Iteration 172 --------------------------#
[32m[20221213 12:21:07 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:21:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:07 @agent_ppo2.py:179][0m |           0.0156 |           1.9640 |           1.8186 |
[32m[20221213 12:21:07 @agent_ppo2.py:179][0m |          -0.0064 |           1.9419 |           1.8041 |
[32m[20221213 12:21:07 @agent_ppo2.py:179][0m |          -0.0202 |           1.9621 |           1.8232 |
[32m[20221213 12:21:07 @agent_ppo2.py:179][0m |          -0.0274 |           1.9155 |           1.8281 |
[32m[20221213 12:21:07 @agent_ppo2.py:179][0m |          -0.0317 |           1.9006 |           1.8409 |
[32m[20221213 12:21:08 @agent_ppo2.py:179][0m |          -0.0318 |           1.8958 |           1.8441 |
[32m[20221213 12:21:08 @agent_ppo2.py:179][0m |          -0.0365 |           1.8948 |           1.8383 |
[32m[20221213 12:21:08 @agent_ppo2.py:179][0m |          -0.0393 |           1.8853 |           1.8448 |
[32m[20221213 12:21:08 @agent_ppo2.py:179][0m |          -0.0436 |           1.8832 |           1.8500 |
[32m[20221213 12:21:08 @agent_ppo2.py:179][0m |          -0.0390 |           1.8815 |           1.8478 |
[32m[20221213 12:21:08 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:21:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.24
[32m[20221213 12:21:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.48
[32m[20221213 12:21:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 88.63
[32m[20221213 12:21:08 @agent_ppo2.py:137][0m Total time:       5.19 min
[32m[20221213 12:21:08 @agent_ppo2.py:139][0m 354304 total steps have happened
[32m[20221213 12:21:08 @agent_ppo2.py:115][0m #------------------------ Iteration 173 --------------------------#
[32m[20221213 12:21:09 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:21:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:09 @agent_ppo2.py:179][0m |           0.0227 |           1.9501 |           1.8070 |
[32m[20221213 12:21:09 @agent_ppo2.py:179][0m |          -0.0080 |           1.8728 |           1.7859 |
[32m[20221213 12:21:09 @agent_ppo2.py:179][0m |          -0.0229 |           1.8494 |           1.7940 |
[32m[20221213 12:21:09 @agent_ppo2.py:179][0m |          -0.0301 |           1.8313 |           1.8043 |
[32m[20221213 12:21:10 @agent_ppo2.py:179][0m |          -0.0249 |           1.8651 |           1.8068 |
[32m[20221213 12:21:10 @agent_ppo2.py:179][0m |          -0.0328 |           1.8035 |           1.8094 |
[32m[20221213 12:21:10 @agent_ppo2.py:179][0m |          -0.0367 |           1.7861 |           1.8147 |
[32m[20221213 12:21:10 @agent_ppo2.py:179][0m |          -0.0376 |           1.7823 |           1.8247 |
[32m[20221213 12:21:10 @agent_ppo2.py:179][0m |          -0.0371 |           1.7713 |           1.8296 |
[32m[20221213 12:21:10 @agent_ppo2.py:179][0m |          -0.0364 |           1.7633 |           1.8303 |
[32m[20221213 12:21:10 @agent_ppo2.py:124][0m Policy update time: 1.67 s
[32m[20221213 12:21:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.58
[32m[20221213 12:21:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.66
[32m[20221213 12:21:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.43
[32m[20221213 12:21:11 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 118.43
[32m[20221213 12:21:11 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 118.43
[32m[20221213 12:21:11 @agent_ppo2.py:137][0m Total time:       5.23 min
[32m[20221213 12:21:11 @agent_ppo2.py:139][0m 356352 total steps have happened
[32m[20221213 12:21:11 @agent_ppo2.py:115][0m #------------------------ Iteration 174 --------------------------#
[32m[20221213 12:21:11 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:21:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:11 @agent_ppo2.py:179][0m |           0.0292 |           2.1826 |           1.8255 |
[32m[20221213 12:21:11 @agent_ppo2.py:179][0m |           0.0093 |           2.2373 |           1.7948 |
[32m[20221213 12:21:11 @agent_ppo2.py:179][0m |          -0.0066 |           2.1258 |           1.7945 |
[32m[20221213 12:21:12 @agent_ppo2.py:179][0m |          -0.0253 |           2.1058 |           1.8151 |
[32m[20221213 12:21:12 @agent_ppo2.py:179][0m |          -0.0281 |           2.0852 |           1.8250 |
[32m[20221213 12:21:12 @agent_ppo2.py:179][0m |          -0.0342 |           2.0710 |           1.8315 |
[32m[20221213 12:21:12 @agent_ppo2.py:179][0m |          -0.0335 |           2.0718 |           1.8366 |
[32m[20221213 12:21:12 @agent_ppo2.py:179][0m |          -0.0355 |           2.0771 |           1.8367 |
[32m[20221213 12:21:12 @agent_ppo2.py:179][0m |          -0.0380 |           2.0564 |           1.8394 |
[32m[20221213 12:21:12 @agent_ppo2.py:179][0m |          -0.0416 |           2.0664 |           1.8499 |
[32m[20221213 12:21:12 @agent_ppo2.py:124][0m Policy update time: 1.64 s
[32m[20221213 12:21:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.66
[32m[20221213 12:21:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 93.14
[32m[20221213 12:21:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 90.28
[32m[20221213 12:21:13 @agent_ppo2.py:137][0m Total time:       5.27 min
[32m[20221213 12:21:13 @agent_ppo2.py:139][0m 358400 total steps have happened
[32m[20221213 12:21:13 @agent_ppo2.py:115][0m #------------------------ Iteration 175 --------------------------#
[32m[20221213 12:21:13 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:21:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:13 @agent_ppo2.py:179][0m |           0.0193 |           2.2195 |           1.8002 |
[32m[20221213 12:21:13 @agent_ppo2.py:179][0m |           0.0019 |           2.4318 |           1.7887 |
[32m[20221213 12:21:13 @agent_ppo2.py:179][0m |          -0.0247 |           2.1550 |           1.8115 |
[32m[20221213 12:21:14 @agent_ppo2.py:179][0m |          -0.0301 |           2.1345 |           1.8174 |
[32m[20221213 12:21:14 @agent_ppo2.py:179][0m |          -0.0342 |           2.1216 |           1.8278 |
[32m[20221213 12:21:14 @agent_ppo2.py:179][0m |          -0.0410 |           2.1281 |           1.8386 |
[32m[20221213 12:21:14 @agent_ppo2.py:179][0m |          -0.0408 |           2.1112 |           1.8448 |
[32m[20221213 12:21:14 @agent_ppo2.py:179][0m |          -0.0450 |           2.1105 |           1.8543 |
[32m[20221213 12:21:14 @agent_ppo2.py:179][0m |          -0.0473 |           2.1117 |           1.8560 |
[32m[20221213 12:21:15 @agent_ppo2.py:179][0m |          -0.0553 |           2.1035 |           1.8579 |
[32m[20221213 12:21:15 @agent_ppo2.py:124][0m Policy update time: 1.58 s
[32m[20221213 12:21:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.38
[32m[20221213 12:21:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 110.07
[32m[20221213 12:21:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 93.10
[32m[20221213 12:21:15 @agent_ppo2.py:137][0m Total time:       5.30 min
[32m[20221213 12:21:15 @agent_ppo2.py:139][0m 360448 total steps have happened
[32m[20221213 12:21:15 @agent_ppo2.py:115][0m #------------------------ Iteration 176 --------------------------#
[32m[20221213 12:21:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:15 @agent_ppo2.py:179][0m |           0.0137 |           2.1254 |           1.8877 |
[32m[20221213 12:21:15 @agent_ppo2.py:179][0m |          -0.0143 |           2.0780 |           1.8868 |
[32m[20221213 12:21:15 @agent_ppo2.py:179][0m |          -0.0282 |           2.0646 |           1.9035 |
[32m[20221213 12:21:16 @agent_ppo2.py:179][0m |          -0.0249 |           2.1928 |           1.9110 |
[32m[20221213 12:21:16 @agent_ppo2.py:179][0m |          -0.0345 |           2.0555 |           1.9065 |
[32m[20221213 12:21:16 @agent_ppo2.py:179][0m |          -0.0388 |           2.0535 |           1.9204 |
[32m[20221213 12:21:16 @agent_ppo2.py:179][0m |          -0.0459 |           2.0282 |           1.9256 |
[32m[20221213 12:21:16 @agent_ppo2.py:179][0m |          -0.0417 |           2.1015 |           1.9420 |
[32m[20221213 12:21:16 @agent_ppo2.py:179][0m |          -0.0463 |           2.0431 |           1.9300 |
[32m[20221213 12:21:16 @agent_ppo2.py:179][0m |          -0.0407 |           2.2922 |           1.9446 |
[32m[20221213 12:21:16 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:21:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.52
[32m[20221213 12:21:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.55
[32m[20221213 12:21:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.43
[32m[20221213 12:21:17 @agent_ppo2.py:137][0m Total time:       5.33 min
[32m[20221213 12:21:17 @agent_ppo2.py:139][0m 362496 total steps have happened
[32m[20221213 12:21:17 @agent_ppo2.py:115][0m #------------------------ Iteration 177 --------------------------#
[32m[20221213 12:21:17 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:21:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:17 @agent_ppo2.py:179][0m |           0.0138 |           2.0072 |           1.9619 |
[32m[20221213 12:21:17 @agent_ppo2.py:179][0m |          -0.0103 |           1.9744 |           1.9500 |
[32m[20221213 12:21:17 @agent_ppo2.py:179][0m |          -0.0249 |           1.9706 |           1.9653 |
[32m[20221213 12:21:17 @agent_ppo2.py:179][0m |          -0.0237 |           2.0354 |           1.9784 |
[32m[20221213 12:21:18 @agent_ppo2.py:179][0m |          -0.0247 |           2.0354 |           1.9684 |
[32m[20221213 12:21:18 @agent_ppo2.py:179][0m |          -0.0280 |           1.9736 |           1.9594 |
[32m[20221213 12:21:18 @agent_ppo2.py:179][0m |          -0.0360 |           1.9523 |           1.9700 |
[32m[20221213 12:21:18 @agent_ppo2.py:179][0m |          -0.0442 |           1.9472 |           1.9835 |
[32m[20221213 12:21:18 @agent_ppo2.py:179][0m |          -0.0444 |           1.9430 |           1.9935 |
[32m[20221213 12:21:18 @agent_ppo2.py:179][0m |          -0.0474 |           1.9377 |           1.9981 |
[32m[20221213 12:21:18 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:21:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 97.42
[32m[20221213 12:21:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 107.62
[32m[20221213 12:21:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.85
[32m[20221213 12:21:18 @agent_ppo2.py:137][0m Total time:       5.36 min
[32m[20221213 12:21:18 @agent_ppo2.py:139][0m 364544 total steps have happened
[32m[20221213 12:21:18 @agent_ppo2.py:115][0m #------------------------ Iteration 178 --------------------------#
[32m[20221213 12:21:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:19 @agent_ppo2.py:179][0m |           0.0170 |           2.3139 |           2.0067 |
[32m[20221213 12:21:19 @agent_ppo2.py:179][0m |          -0.0072 |           2.2875 |           2.0046 |
[32m[20221213 12:21:19 @agent_ppo2.py:179][0m |          -0.0223 |           2.2975 |           2.0176 |
[32m[20221213 12:21:19 @agent_ppo2.py:179][0m |          -0.0294 |           2.2665 |           2.0432 |
[32m[20221213 12:21:19 @agent_ppo2.py:179][0m |          -0.0372 |           2.2632 |           2.0416 |
[32m[20221213 12:21:19 @agent_ppo2.py:179][0m |          -0.0408 |           2.2619 |           2.0553 |
[32m[20221213 12:21:19 @agent_ppo2.py:179][0m |          -0.0407 |           2.2546 |           2.0646 |
[32m[20221213 12:21:20 @agent_ppo2.py:179][0m |          -0.0440 |           2.2545 |           2.0762 |
[32m[20221213 12:21:20 @agent_ppo2.py:179][0m |          -0.0453 |           2.2412 |           2.0893 |
[32m[20221213 12:21:20 @agent_ppo2.py:179][0m |          -0.0471 |           2.2374 |           2.0926 |
[32m[20221213 12:21:20 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:21:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.17
[32m[20221213 12:21:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 105.34
[32m[20221213 12:21:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.54
[32m[20221213 12:21:20 @agent_ppo2.py:137][0m Total time:       5.39 min
[32m[20221213 12:21:20 @agent_ppo2.py:139][0m 366592 total steps have happened
[32m[20221213 12:21:20 @agent_ppo2.py:115][0m #------------------------ Iteration 179 --------------------------#
[32m[20221213 12:21:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:20 @agent_ppo2.py:179][0m |           0.0304 |           2.1189 |           2.0443 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |           0.0062 |           2.1199 |           2.0483 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |          -0.0184 |           2.0830 |           2.0535 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |          -0.0289 |           2.0647 |           2.0708 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |          -0.0238 |           2.0985 |           2.0797 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |          -0.0372 |           2.0568 |           2.0649 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |          -0.0391 |           2.0438 |           2.0751 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |          -0.0421 |           2.0388 |           2.0969 |
[32m[20221213 12:21:21 @agent_ppo2.py:179][0m |          -0.0385 |           2.0932 |           2.1010 |
[32m[20221213 12:21:22 @agent_ppo2.py:179][0m |          -0.0453 |           2.0289 |           2.1067 |
[32m[20221213 12:21:22 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:21:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 93.66
[32m[20221213 12:21:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 96.61
[32m[20221213 12:21:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 85.79
[32m[20221213 12:21:22 @agent_ppo2.py:137][0m Total time:       5.42 min
[32m[20221213 12:21:22 @agent_ppo2.py:139][0m 368640 total steps have happened
[32m[20221213 12:21:22 @agent_ppo2.py:115][0m #------------------------ Iteration 180 --------------------------#
[32m[20221213 12:21:22 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:21:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:22 @agent_ppo2.py:179][0m |           0.0201 |           2.0807 |           2.1242 |
[32m[20221213 12:21:22 @agent_ppo2.py:179][0m |          -0.0054 |           2.0294 |           2.1185 |
[32m[20221213 12:21:22 @agent_ppo2.py:179][0m |          -0.0151 |           2.0364 |           2.1249 |
[32m[20221213 12:21:23 @agent_ppo2.py:179][0m |          -0.0263 |           2.0215 |           2.1280 |
[32m[20221213 12:21:23 @agent_ppo2.py:179][0m |          -0.0318 |           2.0144 |           2.1489 |
[32m[20221213 12:21:23 @agent_ppo2.py:179][0m |          -0.0341 |           2.0036 |           2.1546 |
[32m[20221213 12:21:23 @agent_ppo2.py:179][0m |          -0.0344 |           2.0100 |           2.1658 |
[32m[20221213 12:21:23 @agent_ppo2.py:179][0m |          -0.0361 |           2.0107 |           2.1727 |
[32m[20221213 12:21:23 @agent_ppo2.py:179][0m |          -0.0374 |           2.0030 |           2.1733 |
[32m[20221213 12:21:23 @agent_ppo2.py:179][0m |          -0.0393 |           2.0012 |           2.1837 |
[32m[20221213 12:21:23 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:21:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.80
[32m[20221213 12:21:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 94.95
[32m[20221213 12:21:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.02
[32m[20221213 12:21:24 @agent_ppo2.py:137][0m Total time:       5.45 min
[32m[20221213 12:21:24 @agent_ppo2.py:139][0m 370688 total steps have happened
[32m[20221213 12:21:24 @agent_ppo2.py:115][0m #------------------------ Iteration 181 --------------------------#
[32m[20221213 12:21:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:24 @agent_ppo2.py:179][0m |           0.0180 |           2.5709 |           2.1532 |
[32m[20221213 12:21:24 @agent_ppo2.py:179][0m |          -0.0049 |           2.5027 |           2.1598 |
[32m[20221213 12:21:24 @agent_ppo2.py:179][0m |          -0.0302 |           2.4960 |           2.1890 |
[32m[20221213 12:21:24 @agent_ppo2.py:179][0m |          -0.0346 |           2.4909 |           2.1977 |
[32m[20221213 12:21:25 @agent_ppo2.py:179][0m |          -0.0389 |           2.4571 |           2.1968 |
[32m[20221213 12:21:25 @agent_ppo2.py:179][0m |          -0.0436 |           2.4621 |           2.2007 |
[32m[20221213 12:21:25 @agent_ppo2.py:179][0m |          -0.0478 |           2.4573 |           2.1961 |
[32m[20221213 12:21:25 @agent_ppo2.py:179][0m |          -0.0506 |           2.4407 |           2.2118 |
[32m[20221213 12:21:25 @agent_ppo2.py:179][0m |          -0.0537 |           2.4365 |           2.2159 |
[32m[20221213 12:21:25 @agent_ppo2.py:179][0m |          -0.0553 |           2.4316 |           2.2175 |
[32m[20221213 12:21:25 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:21:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.88
[32m[20221213 12:21:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 116.24
[32m[20221213 12:21:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.67
[32m[20221213 12:21:25 @agent_ppo2.py:137][0m Total time:       5.48 min
[32m[20221213 12:21:25 @agent_ppo2.py:139][0m 372736 total steps have happened
[32m[20221213 12:21:25 @agent_ppo2.py:115][0m #------------------------ Iteration 182 --------------------------#
[32m[20221213 12:21:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:26 @agent_ppo2.py:179][0m |           0.0159 |           2.3453 |           2.1882 |
[32m[20221213 12:21:26 @agent_ppo2.py:179][0m |          -0.0079 |           2.3016 |           2.1708 |
[32m[20221213 12:21:26 @agent_ppo2.py:179][0m |          -0.0166 |           2.3758 |           2.1979 |
[32m[20221213 12:21:26 @agent_ppo2.py:179][0m |          -0.0241 |           2.2724 |           2.1809 |
[32m[20221213 12:21:26 @agent_ppo2.py:179][0m |          -0.0358 |           2.2671 |           2.2023 |
[32m[20221213 12:21:26 @agent_ppo2.py:179][0m |          -0.0370 |           2.2652 |           2.2072 |
[32m[20221213 12:21:27 @agent_ppo2.py:179][0m |          -0.0416 |           2.2644 |           2.2189 |
[32m[20221213 12:21:27 @agent_ppo2.py:179][0m |          -0.0453 |           2.2546 |           2.2222 |
[32m[20221213 12:21:27 @agent_ppo2.py:179][0m |          -0.0429 |           2.2472 |           2.2250 |
[32m[20221213 12:21:27 @agent_ppo2.py:179][0m |          -0.0485 |           2.2539 |           2.2360 |
[32m[20221213 12:21:27 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:21:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.19
[32m[20221213 12:21:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 103.96
[32m[20221213 12:21:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.64
[32m[20221213 12:21:27 @agent_ppo2.py:137][0m Total time:       5.51 min
[32m[20221213 12:21:27 @agent_ppo2.py:139][0m 374784 total steps have happened
[32m[20221213 12:21:27 @agent_ppo2.py:115][0m #------------------------ Iteration 183 --------------------------#
[32m[20221213 12:21:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:27 @agent_ppo2.py:179][0m |           0.0130 |           2.0683 |           2.2018 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0080 |           2.0437 |           2.1722 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0249 |           2.0102 |           2.2129 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0286 |           2.0030 |           2.2306 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0328 |           1.9930 |           2.2372 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0283 |           2.0139 |           2.2447 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0221 |           2.1086 |           2.2271 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0245 |           1.9955 |           2.2229 |
[32m[20221213 12:21:28 @agent_ppo2.py:179][0m |          -0.0334 |           1.9969 |           2.2579 |
[32m[20221213 12:21:29 @agent_ppo2.py:179][0m |          -0.0396 |           1.9723 |           2.2665 |
[32m[20221213 12:21:29 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:21:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.02
[32m[20221213 12:21:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 98.41
[32m[20221213 12:21:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.22
[32m[20221213 12:21:29 @agent_ppo2.py:137][0m Total time:       5.53 min
[32m[20221213 12:21:29 @agent_ppo2.py:139][0m 376832 total steps have happened
[32m[20221213 12:21:29 @agent_ppo2.py:115][0m #------------------------ Iteration 184 --------------------------#
[32m[20221213 12:21:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:29 @agent_ppo2.py:179][0m |           0.0322 |           2.3172 |           2.2993 |
[32m[20221213 12:21:29 @agent_ppo2.py:179][0m |          -0.0102 |           2.2152 |           2.2991 |
[32m[20221213 12:21:29 @agent_ppo2.py:179][0m |          -0.0206 |           2.1966 |           2.3172 |
[32m[20221213 12:21:30 @agent_ppo2.py:179][0m |          -0.0220 |           2.1737 |           2.3123 |
[32m[20221213 12:21:30 @agent_ppo2.py:179][0m |          -0.0345 |           2.1709 |           2.3265 |
[32m[20221213 12:21:30 @agent_ppo2.py:179][0m |          -0.0406 |           2.1636 |           2.3292 |
[32m[20221213 12:21:30 @agent_ppo2.py:179][0m |          -0.0433 |           2.1642 |           2.3369 |
[32m[20221213 12:21:30 @agent_ppo2.py:179][0m |          -0.0422 |           2.1515 |           2.3550 |
[32m[20221213 12:21:30 @agent_ppo2.py:179][0m |          -0.0482 |           2.1384 |           2.3610 |
[32m[20221213 12:21:30 @agent_ppo2.py:179][0m |          -0.0413 |           2.3830 |           2.3722 |
[32m[20221213 12:21:30 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:21:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 107.90
[32m[20221213 12:21:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.34
[32m[20221213 12:21:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.38
[32m[20221213 12:21:30 @agent_ppo2.py:137][0m Total time:       5.56 min
[32m[20221213 12:21:30 @agent_ppo2.py:139][0m 378880 total steps have happened
[32m[20221213 12:21:30 @agent_ppo2.py:115][0m #------------------------ Iteration 185 --------------------------#
[32m[20221213 12:21:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:31 @agent_ppo2.py:179][0m |           0.0159 |           2.7753 |           2.3542 |
[32m[20221213 12:21:31 @agent_ppo2.py:179][0m |           0.0032 |           2.8281 |           2.3412 |
[32m[20221213 12:21:31 @agent_ppo2.py:179][0m |          -0.0248 |           2.6409 |           2.3576 |
[32m[20221213 12:21:31 @agent_ppo2.py:179][0m |          -0.0349 |           2.6047 |           2.3605 |
[32m[20221213 12:21:31 @agent_ppo2.py:179][0m |          -0.0388 |           2.5752 |           2.3730 |
[32m[20221213 12:21:32 @agent_ppo2.py:179][0m |          -0.0426 |           2.5445 |           2.3738 |
[32m[20221213 12:21:32 @agent_ppo2.py:179][0m |          -0.0379 |           2.6092 |           2.3847 |
[32m[20221213 12:21:32 @agent_ppo2.py:179][0m |          -0.0447 |           2.5180 |           2.3824 |
[32m[20221213 12:21:32 @agent_ppo2.py:179][0m |          -0.0462 |           2.4943 |           2.3950 |
[32m[20221213 12:21:32 @agent_ppo2.py:179][0m |          -0.0482 |           2.4796 |           2.3998 |
[32m[20221213 12:21:32 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:21:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.20
[32m[20221213 12:21:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.92
[32m[20221213 12:21:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 97.62
[32m[20221213 12:21:32 @agent_ppo2.py:137][0m Total time:       5.59 min
[32m[20221213 12:21:32 @agent_ppo2.py:139][0m 380928 total steps have happened
[32m[20221213 12:21:32 @agent_ppo2.py:115][0m #------------------------ Iteration 186 --------------------------#
[32m[20221213 12:21:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:33 @agent_ppo2.py:179][0m |           0.0169 |           2.4485 |           2.3732 |
[32m[20221213 12:21:33 @agent_ppo2.py:179][0m |          -0.0108 |           2.4084 |           2.3543 |
[32m[20221213 12:21:33 @agent_ppo2.py:179][0m |          -0.0274 |           2.3797 |           2.3695 |
[32m[20221213 12:21:33 @agent_ppo2.py:179][0m |          -0.0319 |           2.3695 |           2.3761 |
[32m[20221213 12:21:33 @agent_ppo2.py:179][0m |          -0.0316 |           2.4744 |           2.3804 |
[32m[20221213 12:21:33 @agent_ppo2.py:179][0m |          -0.0408 |           2.3682 |           2.3853 |
[32m[20221213 12:21:34 @agent_ppo2.py:179][0m |          -0.0479 |           2.3480 |           2.3943 |
[32m[20221213 12:21:34 @agent_ppo2.py:179][0m |          -0.0463 |           2.3267 |           2.4048 |
[32m[20221213 12:21:34 @agent_ppo2.py:179][0m |          -0.0482 |           2.3182 |           2.4095 |
[32m[20221213 12:21:34 @agent_ppo2.py:179][0m |          -0.0526 |           2.3071 |           2.4193 |
[32m[20221213 12:21:34 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:21:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.75
[32m[20221213 12:21:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 108.53
[32m[20221213 12:21:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.66
[32m[20221213 12:21:34 @agent_ppo2.py:137][0m Total time:       5.62 min
[32m[20221213 12:21:34 @agent_ppo2.py:139][0m 382976 total steps have happened
[32m[20221213 12:21:34 @agent_ppo2.py:115][0m #------------------------ Iteration 187 --------------------------#
[32m[20221213 12:21:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |           0.0316 |           2.6037 |           2.4852 |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |          -0.0071 |           2.5617 |           2.4719 |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |          -0.0187 |           2.5331 |           2.5145 |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |          -0.0322 |           2.5020 |           2.5212 |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |          -0.0298 |           2.5506 |           2.5331 |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |          -0.0327 |           2.5777 |           2.5451 |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |          -0.0290 |           2.6790 |           2.5378 |
[32m[20221213 12:21:35 @agent_ppo2.py:179][0m |          -0.0442 |           2.4544 |           2.5566 |
[32m[20221213 12:21:36 @agent_ppo2.py:179][0m |          -0.0491 |           2.4314 |           2.5646 |
[32m[20221213 12:21:36 @agent_ppo2.py:179][0m |          -0.0459 |           2.4311 |           2.5618 |
[32m[20221213 12:21:36 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:21:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.41
[32m[20221213 12:21:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.18
[32m[20221213 12:21:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 104.88
[32m[20221213 12:21:36 @agent_ppo2.py:137][0m Total time:       5.65 min
[32m[20221213 12:21:36 @agent_ppo2.py:139][0m 385024 total steps have happened
[32m[20221213 12:21:36 @agent_ppo2.py:115][0m #------------------------ Iteration 188 --------------------------#
[32m[20221213 12:21:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:36 @agent_ppo2.py:179][0m |           0.0133 |           2.4670 |           2.4941 |
[32m[20221213 12:21:36 @agent_ppo2.py:179][0m |          -0.0098 |           2.4673 |           2.4666 |
[32m[20221213 12:21:37 @agent_ppo2.py:179][0m |          -0.0301 |           2.3857 |           2.4794 |
[32m[20221213 12:21:37 @agent_ppo2.py:179][0m |          -0.0351 |           2.3644 |           2.4891 |
[32m[20221213 12:21:37 @agent_ppo2.py:179][0m |          -0.0414 |           2.3612 |           2.5047 |
[32m[20221213 12:21:37 @agent_ppo2.py:179][0m |          -0.0318 |           2.5473 |           2.5122 |
[32m[20221213 12:21:37 @agent_ppo2.py:179][0m |          -0.0357 |           2.4662 |           2.5067 |
[32m[20221213 12:21:37 @agent_ppo2.py:179][0m |          -0.0461 |           2.3431 |           2.5115 |
[32m[20221213 12:21:37 @agent_ppo2.py:179][0m |          -0.0504 |           2.3363 |           2.5227 |
[32m[20221213 12:21:38 @agent_ppo2.py:179][0m |          -0.0512 |           2.3389 |           2.5286 |
[32m[20221213 12:21:38 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:21:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.22
[32m[20221213 12:21:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.96
[32m[20221213 12:21:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.60
[32m[20221213 12:21:38 @agent_ppo2.py:137][0m Total time:       5.68 min
[32m[20221213 12:21:38 @agent_ppo2.py:139][0m 387072 total steps have happened
[32m[20221213 12:21:38 @agent_ppo2.py:115][0m #------------------------ Iteration 189 --------------------------#
[32m[20221213 12:21:38 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:21:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:38 @agent_ppo2.py:179][0m |           0.0179 |           2.8239 |           2.5540 |
[32m[20221213 12:21:38 @agent_ppo2.py:179][0m |          -0.0067 |           2.7813 |           2.5507 |
[32m[20221213 12:21:38 @agent_ppo2.py:179][0m |          -0.0136 |           2.7657 |           2.5585 |
[32m[20221213 12:21:39 @agent_ppo2.py:179][0m |          -0.0245 |           2.8153 |           2.5812 |
[32m[20221213 12:21:39 @agent_ppo2.py:179][0m |          -0.0343 |           2.7580 |           2.5989 |
[32m[20221213 12:21:39 @agent_ppo2.py:179][0m |          -0.0386 |           2.7325 |           2.6084 |
[32m[20221213 12:21:39 @agent_ppo2.py:179][0m |          -0.0423 |           2.7305 |           2.6092 |
[32m[20221213 12:21:39 @agent_ppo2.py:179][0m |          -0.0412 |           2.7038 |           2.6144 |
[32m[20221213 12:21:39 @agent_ppo2.py:179][0m |          -0.0411 |           2.6978 |           2.6281 |
[32m[20221213 12:21:39 @agent_ppo2.py:179][0m |          -0.0464 |           2.6963 |           2.6317 |
[32m[20221213 12:21:39 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:21:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 111.23
[32m[20221213 12:21:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.03
[32m[20221213 12:21:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.98
[32m[20221213 12:21:40 @agent_ppo2.py:137][0m Total time:       5.71 min
[32m[20221213 12:21:40 @agent_ppo2.py:139][0m 389120 total steps have happened
[32m[20221213 12:21:40 @agent_ppo2.py:115][0m #------------------------ Iteration 190 --------------------------#
[32m[20221213 12:21:40 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:21:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:40 @agent_ppo2.py:179][0m |           0.0237 |           2.8950 |           2.5104 |
[32m[20221213 12:21:40 @agent_ppo2.py:179][0m |          -0.0075 |           2.8005 |           2.5005 |
[32m[20221213 12:21:40 @agent_ppo2.py:179][0m |          -0.0259 |           2.7855 |           2.5104 |
[32m[20221213 12:21:40 @agent_ppo2.py:179][0m |          -0.0334 |           2.7558 |           2.5309 |
[32m[20221213 12:21:40 @agent_ppo2.py:179][0m |          -0.0275 |           2.8090 |           2.5343 |
[32m[20221213 12:21:41 @agent_ppo2.py:179][0m |          -0.0330 |           2.7347 |           2.5315 |
[32m[20221213 12:21:41 @agent_ppo2.py:179][0m |          -0.0411 |           2.7231 |           2.5539 |
[32m[20221213 12:21:41 @agent_ppo2.py:179][0m |          -0.0463 |           2.7209 |           2.5546 |
[32m[20221213 12:21:41 @agent_ppo2.py:179][0m |          -0.0408 |           2.7897 |           2.5666 |
[32m[20221213 12:21:41 @agent_ppo2.py:179][0m |          -0.0441 |           2.7027 |           2.5704 |
[32m[20221213 12:21:41 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:21:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.30
[32m[20221213 12:21:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 120.70
[32m[20221213 12:21:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 98.86
[32m[20221213 12:21:41 @agent_ppo2.py:137][0m Total time:       5.74 min
[32m[20221213 12:21:41 @agent_ppo2.py:139][0m 391168 total steps have happened
[32m[20221213 12:21:41 @agent_ppo2.py:115][0m #------------------------ Iteration 191 --------------------------#
[32m[20221213 12:21:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:42 @agent_ppo2.py:179][0m |           0.0173 |           3.0775 |           2.6187 |
[32m[20221213 12:21:42 @agent_ppo2.py:179][0m |          -0.0090 |           3.0106 |           2.5888 |
[32m[20221213 12:21:42 @agent_ppo2.py:179][0m |          -0.0200 |           2.9642 |           2.6077 |
[32m[20221213 12:21:42 @agent_ppo2.py:179][0m |          -0.0323 |           2.9462 |           2.6061 |
[32m[20221213 12:21:42 @agent_ppo2.py:179][0m |          -0.0405 |           2.9127 |           2.6233 |
[32m[20221213 12:21:42 @agent_ppo2.py:179][0m |          -0.0362 |           2.9091 |           2.6314 |
[32m[20221213 12:21:42 @agent_ppo2.py:179][0m |          -0.0460 |           2.8882 |           2.6399 |
[32m[20221213 12:21:43 @agent_ppo2.py:179][0m |          -0.0497 |           2.8745 |           2.6524 |
[32m[20221213 12:21:43 @agent_ppo2.py:179][0m |          -0.0504 |           2.8534 |           2.6581 |
[32m[20221213 12:21:43 @agent_ppo2.py:179][0m |          -0.0366 |           3.0660 |           2.6719 |
[32m[20221213 12:21:43 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:21:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 104.52
[32m[20221213 12:21:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.55
[32m[20221213 12:21:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.54
[32m[20221213 12:21:43 @agent_ppo2.py:137][0m Total time:       5.77 min
[32m[20221213 12:21:43 @agent_ppo2.py:139][0m 393216 total steps have happened
[32m[20221213 12:21:43 @agent_ppo2.py:115][0m #------------------------ Iteration 192 --------------------------#
[32m[20221213 12:21:43 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:21:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:43 @agent_ppo2.py:179][0m |           0.0334 |           3.2368 |           2.6489 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0065 |           3.1279 |           2.6473 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0254 |           3.0817 |           2.6787 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0301 |           3.0277 |           2.6812 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0428 |           3.0360 |           2.6994 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0325 |           3.1413 |           2.7143 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0432 |           2.9783 |           2.7235 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0497 |           2.9396 |           2.7334 |
[32m[20221213 12:21:44 @agent_ppo2.py:179][0m |          -0.0417 |           3.0739 |           2.7417 |
[32m[20221213 12:21:45 @agent_ppo2.py:179][0m |          -0.0546 |           2.9053 |           2.7490 |
[32m[20221213 12:21:45 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:21:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 106.51
[32m[20221213 12:21:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 122.58
[32m[20221213 12:21:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.63
[32m[20221213 12:21:45 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 119.63
[32m[20221213 12:21:45 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 119.63
[32m[20221213 12:21:45 @agent_ppo2.py:137][0m Total time:       5.80 min
[32m[20221213 12:21:45 @agent_ppo2.py:139][0m 395264 total steps have happened
[32m[20221213 12:21:45 @agent_ppo2.py:115][0m #------------------------ Iteration 193 --------------------------#
[32m[20221213 12:21:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:45 @agent_ppo2.py:179][0m |           0.0109 |           2.7936 |           2.7009 |
[32m[20221213 12:21:45 @agent_ppo2.py:179][0m |           0.0140 |           2.6832 |           2.6900 |
[32m[20221213 12:21:45 @agent_ppo2.py:179][0m |           0.0096 |           2.6261 |           2.5770 |
[32m[20221213 12:21:46 @agent_ppo2.py:179][0m |          -0.0265 |           2.6241 |           2.7086 |
[32m[20221213 12:21:46 @agent_ppo2.py:179][0m |          -0.0377 |           2.5938 |           2.7305 |
[32m[20221213 12:21:46 @agent_ppo2.py:179][0m |          -0.0395 |           2.5833 |           2.7466 |
[32m[20221213 12:21:46 @agent_ppo2.py:179][0m |          -0.0391 |           2.6000 |           2.7508 |
[32m[20221213 12:21:46 @agent_ppo2.py:179][0m |          -0.0467 |           2.5619 |           2.7695 |
[32m[20221213 12:21:46 @agent_ppo2.py:179][0m |          -0.0482 |           2.5593 |           2.7719 |
[32m[20221213 12:21:46 @agent_ppo2.py:179][0m |          -0.0509 |           2.5601 |           2.7858 |
[32m[20221213 12:21:46 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:21:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.72
[32m[20221213 12:21:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.68
[32m[20221213 12:21:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.51
[32m[20221213 12:21:47 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 122.51
[32m[20221213 12:21:47 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 122.51
[32m[20221213 12:21:47 @agent_ppo2.py:137][0m Total time:       5.83 min
[32m[20221213 12:21:47 @agent_ppo2.py:139][0m 397312 total steps have happened
[32m[20221213 12:21:47 @agent_ppo2.py:115][0m #------------------------ Iteration 194 --------------------------#
[32m[20221213 12:21:47 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:21:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:47 @agent_ppo2.py:179][0m |           0.0217 |           3.2422 |           2.8212 |
[32m[20221213 12:21:47 @agent_ppo2.py:179][0m |           0.0139 |           3.4538 |           2.8255 |
[32m[20221213 12:21:47 @agent_ppo2.py:179][0m |           0.0170 |           3.3042 |           2.6809 |
[32m[20221213 12:21:47 @agent_ppo2.py:179][0m |          -0.0227 |           3.1300 |           2.8022 |
[32m[20221213 12:21:48 @agent_ppo2.py:179][0m |          -0.0313 |           3.0956 |           2.8200 |
[32m[20221213 12:21:48 @agent_ppo2.py:179][0m |          -0.0375 |           3.0836 |           2.8272 |
[32m[20221213 12:21:48 @agent_ppo2.py:179][0m |          -0.0379 |           3.0875 |           2.8327 |
[32m[20221213 12:21:48 @agent_ppo2.py:179][0m |          -0.0429 |           3.0820 |           2.8473 |
[32m[20221213 12:21:48 @agent_ppo2.py:179][0m |          -0.0426 |           3.0662 |           2.8636 |
[32m[20221213 12:21:48 @agent_ppo2.py:179][0m |          -0.0442 |           3.0501 |           2.8631 |
[32m[20221213 12:21:48 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:21:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 121.34
[32m[20221213 12:21:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.50
[32m[20221213 12:21:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 113.61
[32m[20221213 12:21:48 @agent_ppo2.py:137][0m Total time:       5.86 min
[32m[20221213 12:21:48 @agent_ppo2.py:139][0m 399360 total steps have happened
[32m[20221213 12:21:48 @agent_ppo2.py:115][0m #------------------------ Iteration 195 --------------------------#
[32m[20221213 12:21:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:49 @agent_ppo2.py:179][0m |           0.0252 |           2.3923 |           2.7892 |
[32m[20221213 12:21:49 @agent_ppo2.py:179][0m |          -0.0112 |           2.2246 |           2.7867 |
[32m[20221213 12:21:49 @agent_ppo2.py:179][0m |          -0.0209 |           2.1648 |           2.7919 |
[32m[20221213 12:21:49 @agent_ppo2.py:179][0m |          -0.0287 |           2.1552 |           2.7960 |
[32m[20221213 12:21:49 @agent_ppo2.py:179][0m |          -0.0408 |           2.1178 |           2.8040 |
[32m[20221213 12:21:49 @agent_ppo2.py:179][0m |          -0.0398 |           2.1120 |           2.7919 |
[32m[20221213 12:21:50 @agent_ppo2.py:179][0m |          -0.0397 |           2.0906 |           2.8003 |
[32m[20221213 12:21:50 @agent_ppo2.py:179][0m |          -0.0430 |           2.0701 |           2.8105 |
[32m[20221213 12:21:50 @agent_ppo2.py:179][0m |          -0.0422 |           2.1930 |           2.8141 |
[32m[20221213 12:21:50 @agent_ppo2.py:179][0m |          -0.0466 |           2.0914 |           2.8134 |
[32m[20221213 12:21:50 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:21:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.26
[32m[20221213 12:21:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 119.46
[32m[20221213 12:21:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.12
[32m[20221213 12:21:50 @agent_ppo2.py:137][0m Total time:       5.89 min
[32m[20221213 12:21:50 @agent_ppo2.py:139][0m 401408 total steps have happened
[32m[20221213 12:21:50 @agent_ppo2.py:115][0m #------------------------ Iteration 196 --------------------------#
[32m[20221213 12:21:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:50 @agent_ppo2.py:179][0m |           0.0192 |           3.0243 |           2.7630 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |           0.0006 |           3.0551 |           2.7429 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |          -0.0310 |           2.9117 |           2.7819 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |          -0.0344 |           2.9037 |           2.7890 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |          -0.0318 |           2.9499 |           2.7945 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |          -0.0307 |           3.2369 |           2.8055 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |          -0.0428 |           2.8613 |           2.8029 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |          -0.0480 |           2.8391 |           2.8123 |
[32m[20221213 12:21:51 @agent_ppo2.py:179][0m |          -0.0503 |           2.8096 |           2.8180 |
[32m[20221213 12:21:52 @agent_ppo2.py:179][0m |          -0.0579 |           2.8391 |           2.8262 |
[32m[20221213 12:21:52 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:21:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 103.53
[32m[20221213 12:21:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.82
[32m[20221213 12:21:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.94
[32m[20221213 12:21:52 @agent_ppo2.py:137][0m Total time:       5.92 min
[32m[20221213 12:21:52 @agent_ppo2.py:139][0m 403456 total steps have happened
[32m[20221213 12:21:52 @agent_ppo2.py:115][0m #------------------------ Iteration 197 --------------------------#
[32m[20221213 12:21:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:52 @agent_ppo2.py:179][0m |           0.0344 |           3.2763 |           2.8189 |
[32m[20221213 12:21:52 @agent_ppo2.py:179][0m |          -0.0026 |           3.2766 |           2.8027 |
[32m[20221213 12:21:52 @agent_ppo2.py:179][0m |          -0.0285 |           3.1371 |           2.8458 |
[32m[20221213 12:21:53 @agent_ppo2.py:179][0m |          -0.0308 |           3.1406 |           2.8511 |
[32m[20221213 12:21:53 @agent_ppo2.py:179][0m |          -0.0375 |           3.1103 |           2.8521 |
[32m[20221213 12:21:53 @agent_ppo2.py:179][0m |          -0.0436 |           3.1023 |           2.8762 |
[32m[20221213 12:21:53 @agent_ppo2.py:179][0m |          -0.0466 |           3.0918 |           2.8785 |
[32m[20221213 12:21:53 @agent_ppo2.py:179][0m |          -0.0472 |           3.0664 |           2.8791 |
[32m[20221213 12:21:53 @agent_ppo2.py:179][0m |          -0.0544 |           3.0624 |           2.9073 |
[32m[20221213 12:21:53 @agent_ppo2.py:179][0m |          -0.0514 |           3.0647 |           2.9074 |
[32m[20221213 12:21:53 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:21:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.36
[32m[20221213 12:21:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.42
[32m[20221213 12:21:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.38
[32m[20221213 12:21:53 @agent_ppo2.py:137][0m Total time:       5.95 min
[32m[20221213 12:21:53 @agent_ppo2.py:139][0m 405504 total steps have happened
[32m[20221213 12:21:53 @agent_ppo2.py:115][0m #------------------------ Iteration 198 --------------------------#
[32m[20221213 12:21:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:54 @agent_ppo2.py:179][0m |           0.0155 |           2.4403 |           2.8050 |
[32m[20221213 12:21:54 @agent_ppo2.py:179][0m |          -0.0099 |           2.2744 |           2.8056 |
[32m[20221213 12:21:54 @agent_ppo2.py:179][0m |          -0.0161 |           2.3163 |           2.8213 |
[32m[20221213 12:21:54 @agent_ppo2.py:179][0m |          -0.0320 |           2.2097 |           2.8519 |
[32m[20221213 12:21:54 @agent_ppo2.py:179][0m |          -0.0355 |           2.1708 |           2.8579 |
[32m[20221213 12:21:54 @agent_ppo2.py:179][0m |          -0.0391 |           2.1571 |           2.8707 |
[32m[20221213 12:21:55 @agent_ppo2.py:179][0m |          -0.0417 |           2.1459 |           2.8751 |
[32m[20221213 12:21:55 @agent_ppo2.py:179][0m |          -0.0422 |           2.1225 |           2.8823 |
[32m[20221213 12:21:55 @agent_ppo2.py:179][0m |          -0.0407 |           2.1391 |           2.8922 |
[32m[20221213 12:21:55 @agent_ppo2.py:179][0m |          -0.0444 |           2.1071 |           2.8983 |
[32m[20221213 12:21:55 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:21:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.00
[32m[20221213 12:21:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.22
[32m[20221213 12:21:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 107.71
[32m[20221213 12:21:55 @agent_ppo2.py:137][0m Total time:       5.97 min
[32m[20221213 12:21:55 @agent_ppo2.py:139][0m 407552 total steps have happened
[32m[20221213 12:21:55 @agent_ppo2.py:115][0m #------------------------ Iteration 199 --------------------------#
[32m[20221213 12:21:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |           0.0234 |           2.6359 |           3.0329 |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |          -0.0075 |           2.4708 |           3.0111 |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |          -0.0244 |           2.4098 |           3.0238 |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |          -0.0325 |           2.3920 |           3.0584 |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |          -0.0375 |           2.3988 |           3.0635 |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |          -0.0371 |           2.3198 |           3.0596 |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |          -0.0457 |           2.3010 |           3.0854 |
[32m[20221213 12:21:56 @agent_ppo2.py:179][0m |          -0.0428 |           2.2982 |           3.1101 |
[32m[20221213 12:21:57 @agent_ppo2.py:179][0m |          -0.0458 |           2.2750 |           3.1079 |
[32m[20221213 12:21:57 @agent_ppo2.py:179][0m |          -0.0504 |           2.2497 |           3.1090 |
[32m[20221213 12:21:57 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:21:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 84.00
[32m[20221213 12:21:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 114.05
[32m[20221213 12:21:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 99.69
[32m[20221213 12:21:57 @agent_ppo2.py:137][0m Total time:       6.00 min
[32m[20221213 12:21:57 @agent_ppo2.py:139][0m 409600 total steps have happened
[32m[20221213 12:21:57 @agent_ppo2.py:115][0m #------------------------ Iteration 200 --------------------------#
[32m[20221213 12:21:57 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:21:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:57 @agent_ppo2.py:179][0m |           0.0502 |           3.7034 |           3.0758 |
[32m[20221213 12:21:57 @agent_ppo2.py:179][0m |           0.0192 |           3.1704 |           2.9066 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0088 |           3.0522 |           3.0373 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0217 |           3.0261 |           3.0443 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0374 |           3.0012 |           3.0839 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0378 |           2.9832 |           3.0986 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0478 |           3.0035 |           3.1101 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0428 |           2.9731 |           3.1181 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0536 |           2.9715 |           3.1353 |
[32m[20221213 12:21:58 @agent_ppo2.py:179][0m |          -0.0470 |           2.9694 |           3.1396 |
[32m[20221213 12:21:58 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:21:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.84
[32m[20221213 12:21:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 138.22
[32m[20221213 12:21:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.10
[32m[20221213 12:21:59 @agent_ppo2.py:137][0m Total time:       6.03 min
[32m[20221213 12:21:59 @agent_ppo2.py:139][0m 411648 total steps have happened
[32m[20221213 12:21:59 @agent_ppo2.py:115][0m #------------------------ Iteration 201 --------------------------#
[32m[20221213 12:21:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:21:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:21:59 @agent_ppo2.py:179][0m |           0.0217 |           3.2301 |           3.1543 |
[32m[20221213 12:21:59 @agent_ppo2.py:179][0m |          -0.0119 |           3.1484 |           3.1384 |
[32m[20221213 12:21:59 @agent_ppo2.py:179][0m |          -0.0271 |           3.1121 |           3.1381 |
[32m[20221213 12:21:59 @agent_ppo2.py:179][0m |          -0.0341 |           3.1079 |           3.1354 |
[32m[20221213 12:21:59 @agent_ppo2.py:179][0m |          -0.0394 |           3.0846 |           3.1405 |
[32m[20221213 12:22:00 @agent_ppo2.py:179][0m |          -0.0430 |           3.0855 |           3.1518 |
[32m[20221213 12:22:00 @agent_ppo2.py:179][0m |          -0.0429 |           3.0604 |           3.1604 |
[32m[20221213 12:22:00 @agent_ppo2.py:179][0m |          -0.0515 |           3.0641 |           3.1746 |
[32m[20221213 12:22:00 @agent_ppo2.py:179][0m |          -0.0492 |           3.0417 |           3.1791 |
[32m[20221213 12:22:00 @agent_ppo2.py:179][0m |          -0.0552 |           3.0341 |           3.1770 |
[32m[20221213 12:22:00 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:22:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.47
[32m[20221213 12:22:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 133.43
[32m[20221213 12:22:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.65
[32m[20221213 12:22:00 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 122.65
[32m[20221213 12:22:00 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 122.65
[32m[20221213 12:22:00 @agent_ppo2.py:137][0m Total time:       6.06 min
[32m[20221213 12:22:00 @agent_ppo2.py:139][0m 413696 total steps have happened
[32m[20221213 12:22:00 @agent_ppo2.py:115][0m #------------------------ Iteration 202 --------------------------#
[32m[20221213 12:22:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:01 @agent_ppo2.py:179][0m |           0.0507 |           2.7837 |           3.2074 |
[32m[20221213 12:22:01 @agent_ppo2.py:179][0m |           0.0291 |           2.6717 |           3.1245 |
[32m[20221213 12:22:01 @agent_ppo2.py:179][0m |           0.0062 |           2.7643 |           3.1963 |
[32m[20221213 12:22:01 @agent_ppo2.py:179][0m |          -0.0036 |           2.6730 |           3.1344 |
[32m[20221213 12:22:01 @agent_ppo2.py:179][0m |          -0.0246 |           2.6123 |           3.2015 |
[32m[20221213 12:22:01 @agent_ppo2.py:179][0m |          -0.0305 |           2.6051 |           3.2329 |
[32m[20221213 12:22:01 @agent_ppo2.py:179][0m |          -0.0387 |           2.5868 |           3.2606 |
[32m[20221213 12:22:02 @agent_ppo2.py:179][0m |          -0.0386 |           2.5937 |           3.2638 |
[32m[20221213 12:22:02 @agent_ppo2.py:179][0m |          -0.0326 |           2.5687 |           3.2720 |
[32m[20221213 12:22:02 @agent_ppo2.py:179][0m |          -0.0417 |           2.5618 |           3.2541 |
[32m[20221213 12:22:02 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:22:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.27
[32m[20221213 12:22:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.28
[32m[20221213 12:22:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 116.08
[32m[20221213 12:22:02 @agent_ppo2.py:137][0m Total time:       6.09 min
[32m[20221213 12:22:02 @agent_ppo2.py:139][0m 415744 total steps have happened
[32m[20221213 12:22:02 @agent_ppo2.py:115][0m #------------------------ Iteration 203 --------------------------#
[32m[20221213 12:22:02 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:22:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:02 @agent_ppo2.py:179][0m |           0.0192 |           3.0835 |           3.3045 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0131 |           3.0046 |           3.2989 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0249 |           2.9651 |           3.3169 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0351 |           2.9567 |           3.3148 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0374 |           2.9331 |           3.3246 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0375 |           2.9349 |           3.3331 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0446 |           2.9169 |           3.3318 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0517 |           2.9348 |           3.3444 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0456 |           2.9228 |           3.3389 |
[32m[20221213 12:22:03 @agent_ppo2.py:179][0m |          -0.0514 |           2.8940 |           3.3544 |
[32m[20221213 12:22:03 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:22:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.29
[32m[20221213 12:22:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 129.42
[32m[20221213 12:22:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 101.31
[32m[20221213 12:22:04 @agent_ppo2.py:137][0m Total time:       6.12 min
[32m[20221213 12:22:04 @agent_ppo2.py:139][0m 417792 total steps have happened
[32m[20221213 12:22:04 @agent_ppo2.py:115][0m #------------------------ Iteration 204 --------------------------#
[32m[20221213 12:22:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:04 @agent_ppo2.py:179][0m |           0.0224 |           3.2599 |           3.2784 |
[32m[20221213 12:22:04 @agent_ppo2.py:179][0m |          -0.0118 |           3.1782 |           3.2679 |
[32m[20221213 12:22:04 @agent_ppo2.py:179][0m |          -0.0215 |           3.1405 |           3.2730 |
[32m[20221213 12:22:04 @agent_ppo2.py:179][0m |          -0.0291 |           3.1988 |           3.2739 |
[32m[20221213 12:22:05 @agent_ppo2.py:179][0m |          -0.0366 |           3.0892 |           3.2848 |
[32m[20221213 12:22:05 @agent_ppo2.py:179][0m |          -0.0390 |           3.0569 |           3.2925 |
[32m[20221213 12:22:05 @agent_ppo2.py:179][0m |          -0.0457 |           3.0542 |           3.3040 |
[32m[20221213 12:22:05 @agent_ppo2.py:179][0m |          -0.0503 |           3.0482 |           3.3219 |
[32m[20221213 12:22:05 @agent_ppo2.py:179][0m |          -0.0416 |           3.2651 |           3.3128 |
[32m[20221213 12:22:05 @agent_ppo2.py:179][0m |          -0.0550 |           3.0545 |           3.3199 |
[32m[20221213 12:22:05 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:22:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.16
[32m[20221213 12:22:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 131.10
[32m[20221213 12:22:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.07
[32m[20221213 12:22:05 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 133.07
[32m[20221213 12:22:05 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 133.07
[32m[20221213 12:22:05 @agent_ppo2.py:137][0m Total time:       6.14 min
[32m[20221213 12:22:05 @agent_ppo2.py:139][0m 419840 total steps have happened
[32m[20221213 12:22:05 @agent_ppo2.py:115][0m #------------------------ Iteration 205 --------------------------#
[32m[20221213 12:22:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:06 @agent_ppo2.py:179][0m |           0.0166 |           2.5170 |           3.3465 |
[32m[20221213 12:22:06 @agent_ppo2.py:179][0m |          -0.0054 |           2.4524 |           3.3254 |
[32m[20221213 12:22:06 @agent_ppo2.py:179][0m |          -0.0197 |           2.4245 |           3.3435 |
[32m[20221213 12:22:06 @agent_ppo2.py:179][0m |          -0.0295 |           2.4125 |           3.3637 |
[32m[20221213 12:22:06 @agent_ppo2.py:179][0m |          -0.0312 |           2.4072 |           3.3733 |
[32m[20221213 12:22:06 @agent_ppo2.py:179][0m |          -0.0359 |           2.3995 |           3.3818 |
[32m[20221213 12:22:07 @agent_ppo2.py:179][0m |          -0.0343 |           2.3939 |           3.3745 |
[32m[20221213 12:22:07 @agent_ppo2.py:179][0m |          -0.0384 |           2.3806 |           3.3930 |
[32m[20221213 12:22:07 @agent_ppo2.py:179][0m |          -0.0404 |           2.3853 |           3.3995 |
[32m[20221213 12:22:07 @agent_ppo2.py:179][0m |          -0.0412 |           2.3711 |           3.4053 |
[32m[20221213 12:22:07 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:22:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.52
[32m[20221213 12:22:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 104.53
[32m[20221213 12:22:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 102.11
[32m[20221213 12:22:07 @agent_ppo2.py:137][0m Total time:       6.17 min
[32m[20221213 12:22:07 @agent_ppo2.py:139][0m 421888 total steps have happened
[32m[20221213 12:22:07 @agent_ppo2.py:115][0m #------------------------ Iteration 206 --------------------------#
[32m[20221213 12:22:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |           0.0411 |           2.7572 |           3.3386 |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |           0.0114 |           2.6564 |           3.3340 |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |          -0.0179 |           2.6215 |           3.3803 |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |          -0.0285 |           2.6536 |           3.3916 |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |          -0.0277 |           2.5852 |           3.3719 |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |          -0.0327 |           2.5685 |           3.4045 |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |          -0.0380 |           2.5615 |           3.4169 |
[32m[20221213 12:22:08 @agent_ppo2.py:179][0m |          -0.0421 |           2.5454 |           3.4244 |
[32m[20221213 12:22:09 @agent_ppo2.py:179][0m |          -0.0384 |           2.5733 |           3.4410 |
[32m[20221213 12:22:09 @agent_ppo2.py:179][0m |          -0.0374 |           2.6022 |           3.4512 |
[32m[20221213 12:22:09 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:22:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.29
[32m[20221213 12:22:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 115.84
[32m[20221213 12:22:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 119.70
[32m[20221213 12:22:09 @agent_ppo2.py:137][0m Total time:       6.20 min
[32m[20221213 12:22:09 @agent_ppo2.py:139][0m 423936 total steps have happened
[32m[20221213 12:22:09 @agent_ppo2.py:115][0m #------------------------ Iteration 207 --------------------------#
[32m[20221213 12:22:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:09 @agent_ppo2.py:179][0m |           0.0294 |           2.8467 |           3.2794 |
[32m[20221213 12:22:09 @agent_ppo2.py:179][0m |          -0.0068 |           2.7602 |           3.2881 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0229 |           2.7300 |           3.3061 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0305 |           2.6728 |           3.3275 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0269 |           2.6508 |           3.3320 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0383 |           2.6231 |           3.3326 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0408 |           2.5987 |           3.3469 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0459 |           2.5548 |           3.3614 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0451 |           2.5489 |           3.3922 |
[32m[20221213 12:22:10 @agent_ppo2.py:179][0m |          -0.0367 |           2.7927 |           3.4034 |
[32m[20221213 12:22:10 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:22:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.26
[32m[20221213 12:22:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.98
[32m[20221213 12:22:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.84
[32m[20221213 12:22:11 @agent_ppo2.py:137][0m Total time:       6.23 min
[32m[20221213 12:22:11 @agent_ppo2.py:139][0m 425984 total steps have happened
[32m[20221213 12:22:11 @agent_ppo2.py:115][0m #------------------------ Iteration 208 --------------------------#
[32m[20221213 12:22:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:11 @agent_ppo2.py:179][0m |           0.0173 |           3.1765 |           3.5518 |
[32m[20221213 12:22:11 @agent_ppo2.py:179][0m |           0.0004 |           3.1517 |           3.5346 |
[32m[20221213 12:22:11 @agent_ppo2.py:179][0m |          -0.0189 |           3.0782 |           3.5328 |
[32m[20221213 12:22:11 @agent_ppo2.py:179][0m |          -0.0342 |           3.0677 |           3.5667 |
[32m[20221213 12:22:12 @agent_ppo2.py:179][0m |          -0.0410 |           3.0456 |           3.5825 |
[32m[20221213 12:22:12 @agent_ppo2.py:179][0m |          -0.0394 |           3.0212 |           3.5939 |
[32m[20221213 12:22:12 @agent_ppo2.py:179][0m |          -0.0455 |           3.0200 |           3.6059 |
[32m[20221213 12:22:12 @agent_ppo2.py:179][0m |          -0.0454 |           2.9947 |           3.6082 |
[32m[20221213 12:22:12 @agent_ppo2.py:179][0m |          -0.0419 |           2.9840 |           3.6250 |
[32m[20221213 12:22:12 @agent_ppo2.py:179][0m |          -0.0482 |           2.9744 |           3.5996 |
[32m[20221213 12:22:12 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:22:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 112.42
[32m[20221213 12:22:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 123.76
[32m[20221213 12:22:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 35.91
[32m[20221213 12:22:12 @agent_ppo2.py:137][0m Total time:       6.26 min
[32m[20221213 12:22:12 @agent_ppo2.py:139][0m 428032 total steps have happened
[32m[20221213 12:22:12 @agent_ppo2.py:115][0m #------------------------ Iteration 209 --------------------------#
[32m[20221213 12:22:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:13 @agent_ppo2.py:179][0m |           0.0276 |           3.1792 |           3.5658 |
[32m[20221213 12:22:13 @agent_ppo2.py:179][0m |           0.0341 |           3.2481 |           3.4580 |
[32m[20221213 12:22:13 @agent_ppo2.py:179][0m |           0.0085 |           3.0796 |           3.3889 |
[32m[20221213 12:22:13 @agent_ppo2.py:179][0m |          -0.0191 |           3.0531 |           3.4783 |
[32m[20221213 12:22:13 @agent_ppo2.py:179][0m |          -0.0164 |           3.4695 |           3.5138 |
[32m[20221213 12:22:13 @agent_ppo2.py:179][0m |          -0.0353 |           3.0442 |           3.5451 |
[32m[20221213 12:22:13 @agent_ppo2.py:179][0m |          -0.0370 |           3.0202 |           3.5661 |
[32m[20221213 12:22:14 @agent_ppo2.py:179][0m |          -0.0412 |           3.0304 |           3.5685 |
[32m[20221213 12:22:14 @agent_ppo2.py:179][0m |          -0.0456 |           3.0210 |           3.5815 |
[32m[20221213 12:22:14 @agent_ppo2.py:179][0m |          -0.0487 |           2.9949 |           3.6025 |
[32m[20221213 12:22:14 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:22:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 120.39
[32m[20221213 12:22:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.78
[32m[20221213 12:22:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.87
[32m[20221213 12:22:14 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 133.87
[32m[20221213 12:22:14 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 133.87
[32m[20221213 12:22:14 @agent_ppo2.py:137][0m Total time:       6.29 min
[32m[20221213 12:22:14 @agent_ppo2.py:139][0m 430080 total steps have happened
[32m[20221213 12:22:14 @agent_ppo2.py:115][0m #------------------------ Iteration 210 --------------------------#
[32m[20221213 12:22:14 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:22:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |           0.0284 |           2.7648 |           3.4765 |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |           0.0062 |           2.6510 |           3.4686 |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |          -0.0150 |           2.5987 |           3.4928 |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |          -0.0246 |           2.5478 |           3.4961 |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |          -0.0357 |           2.5336 |           3.5190 |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |          -0.0377 |           2.5361 |           3.5308 |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |          -0.0377 |           2.4904 |           3.5136 |
[32m[20221213 12:22:15 @agent_ppo2.py:179][0m |          -0.0308 |           2.8592 |           3.5329 |
[32m[20221213 12:22:16 @agent_ppo2.py:179][0m |          -0.0423 |           2.4922 |           3.5562 |
[32m[20221213 12:22:16 @agent_ppo2.py:179][0m |          -0.0447 |           2.4816 |           3.5585 |
[32m[20221213 12:22:16 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:22:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 105.12
[32m[20221213 12:22:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 117.68
[32m[20221213 12:22:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 105.51
[32m[20221213 12:22:16 @agent_ppo2.py:137][0m Total time:       6.32 min
[32m[20221213 12:22:16 @agent_ppo2.py:139][0m 432128 total steps have happened
[32m[20221213 12:22:16 @agent_ppo2.py:115][0m #------------------------ Iteration 211 --------------------------#
[32m[20221213 12:22:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:16 @agent_ppo2.py:179][0m |           0.0375 |           3.9259 |           3.6455 |
[32m[20221213 12:22:16 @agent_ppo2.py:179][0m |           0.0073 |           4.0806 |           3.6289 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0261 |           3.7559 |           3.6640 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0332 |           3.6742 |           3.6481 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0447 |           3.6358 |           3.6731 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0506 |           3.5934 |           3.6778 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0495 |           3.6035 |           3.6947 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0537 |           3.5912 |           3.7007 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0556 |           3.5440 |           3.7184 |
[32m[20221213 12:22:17 @agent_ppo2.py:179][0m |          -0.0597 |           3.5277 |           3.7135 |
[32m[20221213 12:22:17 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:22:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.12
[32m[20221213 12:22:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.94
[32m[20221213 12:22:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.52
[32m[20221213 12:22:18 @agent_ppo2.py:137][0m Total time:       6.35 min
[32m[20221213 12:22:18 @agent_ppo2.py:139][0m 434176 total steps have happened
[32m[20221213 12:22:18 @agent_ppo2.py:115][0m #------------------------ Iteration 212 --------------------------#
[32m[20221213 12:22:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:18 @agent_ppo2.py:179][0m |           0.0471 |           3.8259 |           3.6964 |
[32m[20221213 12:22:18 @agent_ppo2.py:179][0m |          -0.0008 |           3.5211 |           3.6339 |
[32m[20221213 12:22:18 @agent_ppo2.py:179][0m |          -0.0278 |           3.4748 |           3.7403 |
[32m[20221213 12:22:18 @agent_ppo2.py:179][0m |          -0.0352 |           3.4438 |           3.7762 |
[32m[20221213 12:22:19 @agent_ppo2.py:179][0m |          -0.0416 |           3.4195 |           3.7808 |
[32m[20221213 12:22:19 @agent_ppo2.py:179][0m |          -0.0465 |           3.4234 |           3.7885 |
[32m[20221213 12:22:19 @agent_ppo2.py:179][0m |          -0.0471 |           3.3890 |           3.7963 |
[32m[20221213 12:22:19 @agent_ppo2.py:179][0m |          -0.0489 |           3.4850 |           3.7900 |
[32m[20221213 12:22:19 @agent_ppo2.py:179][0m |          -0.0521 |           3.3714 |           3.8000 |
[32m[20221213 12:22:19 @agent_ppo2.py:179][0m |          -0.0551 |           3.3360 |           3.8070 |
[32m[20221213 12:22:19 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:22:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.74
[32m[20221213 12:22:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.62
[32m[20221213 12:22:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 130.86
[32m[20221213 12:22:19 @agent_ppo2.py:137][0m Total time:       6.38 min
[32m[20221213 12:22:19 @agent_ppo2.py:139][0m 436224 total steps have happened
[32m[20221213 12:22:19 @agent_ppo2.py:115][0m #------------------------ Iteration 213 --------------------------#
[32m[20221213 12:22:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:20 @agent_ppo2.py:179][0m |           0.0174 |           2.8138 |           3.7439 |
[32m[20221213 12:22:20 @agent_ppo2.py:179][0m |           0.0028 |           2.6749 |           3.7038 |
[32m[20221213 12:22:20 @agent_ppo2.py:179][0m |          -0.0234 |           2.6260 |           3.7968 |
[32m[20221213 12:22:20 @agent_ppo2.py:179][0m |          -0.0225 |           2.5750 |           3.8008 |
[32m[20221213 12:22:20 @agent_ppo2.py:179][0m |          -0.0336 |           2.5323 |           3.7999 |
[32m[20221213 12:22:20 @agent_ppo2.py:179][0m |          -0.0373 |           2.5280 |           3.8327 |
[32m[20221213 12:22:21 @agent_ppo2.py:179][0m |          -0.0365 |           2.5030 |           3.8364 |
[32m[20221213 12:22:21 @agent_ppo2.py:179][0m |          -0.0437 |           2.4804 |           3.8320 |
[32m[20221213 12:22:21 @agent_ppo2.py:179][0m |          -0.0437 |           2.4462 |           3.8373 |
[32m[20221213 12:22:21 @agent_ppo2.py:179][0m |          -0.0412 |           2.4540 |           3.8430 |
[32m[20221213 12:22:21 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:22:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.43
[32m[20221213 12:22:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 102.67
[32m[20221213 12:22:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.48
[32m[20221213 12:22:21 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 138.48
[32m[20221213 12:22:21 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 138.48
[32m[20221213 12:22:21 @agent_ppo2.py:137][0m Total time:       6.41 min
[32m[20221213 12:22:21 @agent_ppo2.py:139][0m 438272 total steps have happened
[32m[20221213 12:22:21 @agent_ppo2.py:115][0m #------------------------ Iteration 214 --------------------------#
[32m[20221213 12:22:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:22 @agent_ppo2.py:179][0m |           0.0231 |           3.5505 |           3.8781 |
[32m[20221213 12:22:22 @agent_ppo2.py:179][0m |          -0.0019 |           3.3403 |           3.8203 |
[32m[20221213 12:22:22 @agent_ppo2.py:179][0m |          -0.0289 |           3.3017 |           3.9086 |
[32m[20221213 12:22:22 @agent_ppo2.py:179][0m |          -0.0431 |           3.2670 |           3.9037 |
[32m[20221213 12:22:22 @agent_ppo2.py:179][0m |          -0.0437 |           3.2581 |           3.9254 |
[32m[20221213 12:22:22 @agent_ppo2.py:179][0m |          -0.0476 |           3.2479 |           3.9304 |
[32m[20221213 12:22:22 @agent_ppo2.py:179][0m |          -0.0503 |           3.2356 |           3.9336 |
[32m[20221213 12:22:23 @agent_ppo2.py:179][0m |          -0.0506 |           3.2302 |           3.9361 |
[32m[20221213 12:22:23 @agent_ppo2.py:179][0m |          -0.0521 |           3.1985 |           3.9411 |
[32m[20221213 12:22:23 @agent_ppo2.py:179][0m |          -0.0604 |           3.1949 |           3.9451 |
[32m[20221213 12:22:23 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:22:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 118.83
[32m[20221213 12:22:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.65
[32m[20221213 12:22:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 9.99
[32m[20221213 12:22:23 @agent_ppo2.py:137][0m Total time:       6.44 min
[32m[20221213 12:22:23 @agent_ppo2.py:139][0m 440320 total steps have happened
[32m[20221213 12:22:23 @agent_ppo2.py:115][0m #------------------------ Iteration 215 --------------------------#
[32m[20221213 12:22:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:23 @agent_ppo2.py:179][0m |           0.0424 |           3.7259 |           3.8448 |
[32m[20221213 12:22:24 @agent_ppo2.py:179][0m |           0.0068 |           3.4881 |           3.9082 |
[32m[20221213 12:22:24 @agent_ppo2.py:179][0m |          -0.0173 |           3.4565 |           3.9657 |
[32m[20221213 12:22:24 @agent_ppo2.py:179][0m |          -0.0294 |           3.4969 |           3.9787 |
[32m[20221213 12:22:24 @agent_ppo2.py:179][0m |          -0.0222 |           3.4192 |           3.9627 |
[32m[20221213 12:22:24 @agent_ppo2.py:179][0m |          -0.0302 |           3.7519 |           3.9914 |
[32m[20221213 12:22:24 @agent_ppo2.py:179][0m |          -0.0452 |           3.4469 |           3.9988 |
[32m[20221213 12:22:24 @agent_ppo2.py:179][0m |          -0.0452 |           3.3529 |           4.0184 |
[32m[20221213 12:22:25 @agent_ppo2.py:179][0m |          -0.0462 |           3.3489 |           4.0176 |
[32m[20221213 12:22:25 @agent_ppo2.py:179][0m |          -0.0510 |           3.3525 |           4.0244 |
[32m[20221213 12:22:25 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:22:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.03
[32m[20221213 12:22:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 128.01
[32m[20221213 12:22:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 112.46
[32m[20221213 12:22:25 @agent_ppo2.py:137][0m Total time:       6.47 min
[32m[20221213 12:22:25 @agent_ppo2.py:139][0m 442368 total steps have happened
[32m[20221213 12:22:25 @agent_ppo2.py:115][0m #------------------------ Iteration 216 --------------------------#
[32m[20221213 12:22:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:25 @agent_ppo2.py:179][0m |           0.0344 |           3.6756 |           3.9353 |
[32m[20221213 12:22:25 @agent_ppo2.py:179][0m |          -0.0088 |           3.5514 |           3.9158 |
[32m[20221213 12:22:26 @agent_ppo2.py:179][0m |          -0.0247 |           3.5178 |           3.9281 |
[32m[20221213 12:22:26 @agent_ppo2.py:179][0m |          -0.0298 |           3.4898 |           3.9243 |
[32m[20221213 12:22:26 @agent_ppo2.py:179][0m |          -0.0398 |           3.4510 |           3.9604 |
[32m[20221213 12:22:26 @agent_ppo2.py:179][0m |          -0.0480 |           3.4291 |           3.9699 |
[32m[20221213 12:22:26 @agent_ppo2.py:179][0m |          -0.0514 |           3.4350 |           3.9734 |
[32m[20221213 12:22:26 @agent_ppo2.py:179][0m |          -0.0534 |           3.3874 |           3.9737 |
[32m[20221213 12:22:26 @agent_ppo2.py:179][0m |          -0.0467 |           3.5584 |           3.9858 |
[32m[20221213 12:22:27 @agent_ppo2.py:179][0m |          -0.0574 |           3.3843 |           3.9874 |
[32m[20221213 12:22:27 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:22:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 114.08
[32m[20221213 12:22:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.78
[32m[20221213 12:22:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 153.17
[32m[20221213 12:22:27 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 153.17
[32m[20221213 12:22:27 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 153.17
[32m[20221213 12:22:27 @agent_ppo2.py:137][0m Total time:       6.50 min
[32m[20221213 12:22:27 @agent_ppo2.py:139][0m 444416 total steps have happened
[32m[20221213 12:22:27 @agent_ppo2.py:115][0m #------------------------ Iteration 217 --------------------------#
[32m[20221213 12:22:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:27 @agent_ppo2.py:179][0m |           0.0365 |           3.1614 |           4.0260 |
[32m[20221213 12:22:27 @agent_ppo2.py:179][0m |          -0.0060 |           2.9836 |           4.0314 |
[32m[20221213 12:22:27 @agent_ppo2.py:179][0m |          -0.0094 |           2.9937 |           4.0684 |
[32m[20221213 12:22:28 @agent_ppo2.py:179][0m |          -0.0182 |           2.9453 |           4.0689 |
[32m[20221213 12:22:28 @agent_ppo2.py:179][0m |          -0.0246 |           2.8674 |           4.0756 |
[32m[20221213 12:22:28 @agent_ppo2.py:179][0m |          -0.0285 |           2.8826 |           4.0704 |
[32m[20221213 12:22:28 @agent_ppo2.py:179][0m |          -0.0379 |           2.8099 |           4.0993 |
[32m[20221213 12:22:28 @agent_ppo2.py:179][0m |          -0.0387 |           2.7891 |           4.0946 |
[32m[20221213 12:22:28 @agent_ppo2.py:179][0m |          -0.0418 |           2.7949 |           4.1145 |
[32m[20221213 12:22:28 @agent_ppo2.py:179][0m |          -0.0405 |           2.7655 |           4.0972 |
[32m[20221213 12:22:28 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:22:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 110.07
[32m[20221213 12:22:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.11
[32m[20221213 12:22:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.83
[32m[20221213 12:22:29 @agent_ppo2.py:137][0m Total time:       6.53 min
[32m[20221213 12:22:29 @agent_ppo2.py:139][0m 446464 total steps have happened
[32m[20221213 12:22:29 @agent_ppo2.py:115][0m #------------------------ Iteration 218 --------------------------#
[32m[20221213 12:22:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:29 @agent_ppo2.py:179][0m |           0.0424 |           3.5909 |           4.0538 |
[32m[20221213 12:22:29 @agent_ppo2.py:179][0m |           0.0167 |           3.4960 |           3.9453 |
[32m[20221213 12:22:29 @agent_ppo2.py:179][0m |          -0.0174 |           3.4376 |           4.0681 |
[32m[20221213 12:22:30 @agent_ppo2.py:179][0m |          -0.0205 |           3.4972 |           4.1051 |
[32m[20221213 12:22:30 @agent_ppo2.py:179][0m |          -0.0351 |           3.3932 |           4.1149 |
[32m[20221213 12:22:30 @agent_ppo2.py:179][0m |          -0.0353 |           3.7392 |           4.1406 |
[32m[20221213 12:22:30 @agent_ppo2.py:179][0m |          -0.0469 |           3.3925 |           4.1452 |
[32m[20221213 12:22:30 @agent_ppo2.py:179][0m |          -0.0524 |           3.3581 |           4.1624 |
[32m[20221213 12:22:30 @agent_ppo2.py:179][0m |          -0.0512 |           3.3531 |           4.1774 |
[32m[20221213 12:22:30 @agent_ppo2.py:179][0m |          -0.0512 |           3.3367 |           4.1799 |
[32m[20221213 12:22:30 @agent_ppo2.py:124][0m Policy update time: 1.56 s
[32m[20221213 12:22:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.89
[32m[20221213 12:22:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.33
[32m[20221213 12:22:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.66
[32m[20221213 12:22:31 @agent_ppo2.py:137][0m Total time:       6.56 min
[32m[20221213 12:22:31 @agent_ppo2.py:139][0m 448512 total steps have happened
[32m[20221213 12:22:31 @agent_ppo2.py:115][0m #------------------------ Iteration 219 --------------------------#
[32m[20221213 12:22:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:31 @agent_ppo2.py:179][0m |           0.0200 |           3.4570 |           4.0981 |
[32m[20221213 12:22:31 @agent_ppo2.py:179][0m |          -0.0094 |           3.3562 |           4.0762 |
[32m[20221213 12:22:31 @agent_ppo2.py:179][0m |          -0.0230 |           3.3364 |           4.0803 |
[32m[20221213 12:22:31 @agent_ppo2.py:179][0m |          -0.0255 |           3.4733 |           4.0858 |
[32m[20221213 12:22:31 @agent_ppo2.py:179][0m |          -0.0386 |           3.3018 |           4.1061 |
[32m[20221213 12:22:32 @agent_ppo2.py:179][0m |          -0.0442 |           3.2872 |           4.1253 |
[32m[20221213 12:22:32 @agent_ppo2.py:179][0m |          -0.0417 |           3.2934 |           4.1157 |
[32m[20221213 12:22:32 @agent_ppo2.py:179][0m |          -0.0530 |           3.2763 |           4.1318 |
[32m[20221213 12:22:32 @agent_ppo2.py:179][0m |          -0.0543 |           3.2550 |           4.1505 |
[32m[20221213 12:22:32 @agent_ppo2.py:179][0m |          -0.0476 |           3.2651 |           4.1425 |
[32m[20221213 12:22:32 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:22:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 115.00
[32m[20221213 12:22:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.57
[32m[20221213 12:22:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.25
[32m[20221213 12:22:32 @agent_ppo2.py:137][0m Total time:       6.59 min
[32m[20221213 12:22:32 @agent_ppo2.py:139][0m 450560 total steps have happened
[32m[20221213 12:22:32 @agent_ppo2.py:115][0m #------------------------ Iteration 220 --------------------------#
[32m[20221213 12:22:33 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:22:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:33 @agent_ppo2.py:179][0m |           0.0398 |           3.9302 |           4.1317 |
[32m[20221213 12:22:33 @agent_ppo2.py:179][0m |          -0.0047 |           3.7238 |           4.0730 |
[32m[20221213 12:22:33 @agent_ppo2.py:179][0m |          -0.0229 |           3.7503 |           4.1764 |
[32m[20221213 12:22:33 @agent_ppo2.py:179][0m |          -0.0307 |           3.6711 |           4.1961 |
[32m[20221213 12:22:33 @agent_ppo2.py:179][0m |          -0.0301 |           3.8874 |           4.2141 |
[32m[20221213 12:22:33 @agent_ppo2.py:179][0m |          -0.0274 |           3.6223 |           4.1668 |
[32m[20221213 12:22:34 @agent_ppo2.py:179][0m |          -0.0230 |           4.3815 |           4.2294 |
[32m[20221213 12:22:34 @agent_ppo2.py:179][0m |          -0.0363 |           3.6247 |           4.1684 |
[32m[20221213 12:22:34 @agent_ppo2.py:179][0m |          -0.0324 |           3.7489 |           4.2153 |
[32m[20221213 12:22:34 @agent_ppo2.py:179][0m |          -0.0472 |           3.5450 |           4.2325 |
[32m[20221213 12:22:34 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:22:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 117.71
[32m[20221213 12:22:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.07
[32m[20221213 12:22:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 115.58
[32m[20221213 12:22:34 @agent_ppo2.py:137][0m Total time:       6.62 min
[32m[20221213 12:22:34 @agent_ppo2.py:139][0m 452608 total steps have happened
[32m[20221213 12:22:34 @agent_ppo2.py:115][0m #------------------------ Iteration 221 --------------------------#
[32m[20221213 12:22:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:35 @agent_ppo2.py:179][0m |           0.0153 |           3.7148 |           4.2750 |
[32m[20221213 12:22:35 @agent_ppo2.py:179][0m |           0.0307 |           3.5776 |           4.1418 |
[32m[20221213 12:22:35 @agent_ppo2.py:179][0m |          -0.0092 |           3.5517 |           4.1141 |
[32m[20221213 12:22:35 @agent_ppo2.py:179][0m |          -0.0182 |           3.5147 |           4.2031 |
[32m[20221213 12:22:35 @agent_ppo2.py:179][0m |          -0.0317 |           3.4815 |           4.2244 |
[32m[20221213 12:22:35 @agent_ppo2.py:179][0m |          -0.0315 |           3.4524 |           4.2442 |
[32m[20221213 12:22:35 @agent_ppo2.py:179][0m |          -0.0349 |           3.4801 |           4.2676 |
[32m[20221213 12:22:36 @agent_ppo2.py:179][0m |          -0.0361 |           3.4319 |           4.2528 |
[32m[20221213 12:22:36 @agent_ppo2.py:179][0m |          -0.0406 |           3.4049 |           4.2817 |
[32m[20221213 12:22:36 @agent_ppo2.py:179][0m |          -0.0445 |           3.4047 |           4.3181 |
[32m[20221213 12:22:36 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:22:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.70
[32m[20221213 12:22:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 134.72
[32m[20221213 12:22:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 131.54
[32m[20221213 12:22:36 @agent_ppo2.py:137][0m Total time:       6.66 min
[32m[20221213 12:22:36 @agent_ppo2.py:139][0m 454656 total steps have happened
[32m[20221213 12:22:36 @agent_ppo2.py:115][0m #------------------------ Iteration 222 --------------------------#
[32m[20221213 12:22:36 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:22:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:37 @agent_ppo2.py:179][0m |           0.0303 |           4.4174 |           4.2169 |
[32m[20221213 12:22:37 @agent_ppo2.py:179][0m |           0.0029 |           4.2826 |           4.1742 |
[32m[20221213 12:22:37 @agent_ppo2.py:179][0m |          -0.0224 |           4.3314 |           4.2443 |
[32m[20221213 12:22:37 @agent_ppo2.py:179][0m |          -0.0389 |           4.1940 |           4.2781 |
[32m[20221213 12:22:37 @agent_ppo2.py:179][0m |          -0.0423 |           4.1523 |           4.2821 |
[32m[20221213 12:22:37 @agent_ppo2.py:179][0m |          -0.0402 |           4.1262 |           4.2909 |
[32m[20221213 12:22:37 @agent_ppo2.py:179][0m |          -0.0492 |           4.1196 |           4.3109 |
[32m[20221213 12:22:38 @agent_ppo2.py:179][0m |          -0.0505 |           4.1219 |           4.3301 |
[32m[20221213 12:22:38 @agent_ppo2.py:179][0m |          -0.0520 |           4.0935 |           4.3359 |
[32m[20221213 12:22:38 @agent_ppo2.py:179][0m |          -0.0583 |           4.0835 |           4.3454 |
[32m[20221213 12:22:38 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:22:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.62
[32m[20221213 12:22:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 130.95
[32m[20221213 12:22:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 139.23
[32m[20221213 12:22:38 @agent_ppo2.py:137][0m Total time:       6.69 min
[32m[20221213 12:22:38 @agent_ppo2.py:139][0m 456704 total steps have happened
[32m[20221213 12:22:38 @agent_ppo2.py:115][0m #------------------------ Iteration 223 --------------------------#
[32m[20221213 12:22:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:38 @agent_ppo2.py:179][0m |           0.0337 |           3.7831 |           4.3478 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |           0.0139 |           3.6588 |           4.2784 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |          -0.0153 |           3.6333 |           4.3897 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |          -0.0169 |           3.6172 |           4.4025 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |          -0.0280 |           3.6058 |           4.4144 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |          -0.0282 |           3.5850 |           4.4335 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |          -0.0185 |           4.0266 |           4.4660 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |          -0.0330 |           3.5763 |           4.4748 |
[32m[20221213 12:22:39 @agent_ppo2.py:179][0m |          -0.0359 |           3.5740 |           4.4819 |
[32m[20221213 12:22:40 @agent_ppo2.py:179][0m |          -0.0367 |           3.5440 |           4.4985 |
[32m[20221213 12:22:40 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:22:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.45
[32m[20221213 12:22:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 126.31
[32m[20221213 12:22:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 134.81
[32m[20221213 12:22:40 @agent_ppo2.py:137][0m Total time:       6.72 min
[32m[20221213 12:22:40 @agent_ppo2.py:139][0m 458752 total steps have happened
[32m[20221213 12:22:40 @agent_ppo2.py:115][0m #------------------------ Iteration 224 --------------------------#
[32m[20221213 12:22:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:40 @agent_ppo2.py:179][0m |           0.0129 |           4.2443 |           4.4600 |
[32m[20221213 12:22:40 @agent_ppo2.py:179][0m |          -0.0102 |           4.1501 |           4.4644 |
[32m[20221213 12:22:40 @agent_ppo2.py:179][0m |          -0.0292 |           4.1277 |           4.5116 |
[32m[20221213 12:22:41 @agent_ppo2.py:179][0m |          -0.0368 |           4.0954 |           4.5347 |
[32m[20221213 12:22:41 @agent_ppo2.py:179][0m |          -0.0396 |           4.0713 |           4.5344 |
[32m[20221213 12:22:41 @agent_ppo2.py:179][0m |          -0.0330 |           4.4199 |           4.5496 |
[32m[20221213 12:22:41 @agent_ppo2.py:179][0m |          -0.0423 |           4.0922 |           4.5509 |
[32m[20221213 12:22:41 @agent_ppo2.py:179][0m |          -0.0497 |           4.0140 |           4.5720 |
[32m[20221213 12:22:41 @agent_ppo2.py:179][0m |          -0.0514 |           4.0256 |           4.5862 |
[32m[20221213 12:22:41 @agent_ppo2.py:179][0m |          -0.0480 |           3.9941 |           4.5985 |
[32m[20221213 12:22:41 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:22:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.63
[32m[20221213 12:22:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.65
[32m[20221213 12:22:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 133.52
[32m[20221213 12:22:42 @agent_ppo2.py:137][0m Total time:       6.75 min
[32m[20221213 12:22:42 @agent_ppo2.py:139][0m 460800 total steps have happened
[32m[20221213 12:22:42 @agent_ppo2.py:115][0m #------------------------ Iteration 225 --------------------------#
[32m[20221213 12:22:42 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:22:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:42 @agent_ppo2.py:179][0m |           0.0428 |           4.1579 |           4.3862 |
[32m[20221213 12:22:42 @agent_ppo2.py:179][0m |           0.0306 |           4.0330 |           4.2677 |
[32m[20221213 12:22:43 @agent_ppo2.py:179][0m |          -0.0146 |           3.9755 |           4.4164 |
[32m[20221213 12:22:43 @agent_ppo2.py:179][0m |          -0.0277 |           3.9262 |           4.4612 |
[32m[20221213 12:22:43 @agent_ppo2.py:179][0m |          -0.0274 |           4.0156 |           4.4867 |
[32m[20221213 12:22:43 @agent_ppo2.py:179][0m |          -0.0284 |           3.9208 |           4.4744 |
[32m[20221213 12:22:43 @agent_ppo2.py:179][0m |          -0.0415 |           3.8562 |           4.5169 |
[32m[20221213 12:22:43 @agent_ppo2.py:179][0m |          -0.0407 |           3.8385 |           4.5306 |
[32m[20221213 12:22:43 @agent_ppo2.py:179][0m |          -0.0461 |           3.8141 |           4.5623 |
[32m[20221213 12:22:44 @agent_ppo2.py:179][0m |          -0.0438 |           3.8229 |           4.5637 |
[32m[20221213 12:22:44 @agent_ppo2.py:124][0m Policy update time: 1.70 s
[32m[20221213 12:22:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.08
[32m[20221213 12:22:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 148.57
[32m[20221213 12:22:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.61
[32m[20221213 12:22:44 @agent_ppo2.py:137][0m Total time:       6.78 min
[32m[20221213 12:22:44 @agent_ppo2.py:139][0m 462848 total steps have happened
[32m[20221213 12:22:44 @agent_ppo2.py:115][0m #------------------------ Iteration 226 --------------------------#
[32m[20221213 12:22:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:44 @agent_ppo2.py:179][0m |           0.0427 |           4.3601 |           4.6902 |
[32m[20221213 12:22:44 @agent_ppo2.py:179][0m |           0.0039 |           4.1382 |           4.6412 |
[32m[20221213 12:22:44 @agent_ppo2.py:179][0m |          -0.0236 |           4.1395 |           4.7223 |
[32m[20221213 12:22:45 @agent_ppo2.py:179][0m |          -0.0386 |           4.0544 |           4.7504 |
[32m[20221213 12:22:45 @agent_ppo2.py:179][0m |          -0.0401 |           4.0472 |           4.7661 |
[32m[20221213 12:22:45 @agent_ppo2.py:179][0m |          -0.0385 |           4.2571 |           4.7953 |
[32m[20221213 12:22:45 @agent_ppo2.py:179][0m |          -0.0509 |           3.9886 |           4.8116 |
[32m[20221213 12:22:45 @agent_ppo2.py:179][0m |          -0.0527 |           3.9795 |           4.8150 |
[32m[20221213 12:22:45 @agent_ppo2.py:179][0m |          -0.0445 |           4.2889 |           4.8393 |
[32m[20221213 12:22:45 @agent_ppo2.py:179][0m |          -0.0550 |           3.9959 |           4.8425 |
[32m[20221213 12:22:45 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:22:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.89
[32m[20221213 12:22:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 136.91
[32m[20221213 12:22:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.60
[32m[20221213 12:22:46 @agent_ppo2.py:137][0m Total time:       6.82 min
[32m[20221213 12:22:46 @agent_ppo2.py:139][0m 464896 total steps have happened
[32m[20221213 12:22:46 @agent_ppo2.py:115][0m #------------------------ Iteration 227 --------------------------#
[32m[20221213 12:22:46 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:22:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:46 @agent_ppo2.py:179][0m |           0.0478 |           4.3470 |           4.7523 |
[32m[20221213 12:22:46 @agent_ppo2.py:179][0m |          -0.0029 |           4.1891 |           4.7994 |
[32m[20221213 12:22:46 @agent_ppo2.py:179][0m |          -0.0166 |           4.4631 |           4.8703 |
[32m[20221213 12:22:47 @agent_ppo2.py:179][0m |          -0.0322 |           4.1322 |           4.9112 |
[32m[20221213 12:22:47 @agent_ppo2.py:179][0m |          -0.0380 |           4.0783 |           4.9217 |
[32m[20221213 12:22:47 @agent_ppo2.py:179][0m |          -0.0408 |           4.0711 |           4.9432 |
[32m[20221213 12:22:47 @agent_ppo2.py:179][0m |          -0.0424 |           4.0747 |           4.9544 |
[32m[20221213 12:22:47 @agent_ppo2.py:179][0m |          -0.0414 |           4.0477 |           4.9625 |
[32m[20221213 12:22:47 @agent_ppo2.py:179][0m |          -0.0497 |           4.0463 |           4.9774 |
[32m[20221213 12:22:47 @agent_ppo2.py:179][0m |          -0.0500 |           4.0346 |           4.9916 |
[32m[20221213 12:22:47 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:22:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.80
[32m[20221213 12:22:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 145.87
[32m[20221213 12:22:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 162.07
[32m[20221213 12:22:48 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 162.07
[32m[20221213 12:22:48 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 162.07
[32m[20221213 12:22:48 @agent_ppo2.py:137][0m Total time:       6.85 min
[32m[20221213 12:22:48 @agent_ppo2.py:139][0m 466944 total steps have happened
[32m[20221213 12:22:48 @agent_ppo2.py:115][0m #------------------------ Iteration 228 --------------------------#
[32m[20221213 12:22:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:48 @agent_ppo2.py:179][0m |           0.0352 |           4.4269 |           4.8631 |
[32m[20221213 12:22:48 @agent_ppo2.py:179][0m |           0.0633 |           4.3315 |           4.6247 |
[32m[20221213 12:22:48 @agent_ppo2.py:179][0m |          -0.0057 |           4.2391 |           4.7960 |
[32m[20221213 12:22:48 @agent_ppo2.py:179][0m |          -0.0248 |           4.1974 |           4.9188 |
[32m[20221213 12:22:49 @agent_ppo2.py:179][0m |          -0.0344 |           4.1624 |           4.9495 |
[32m[20221213 12:22:49 @agent_ppo2.py:179][0m |          -0.0408 |           4.1511 |           4.9844 |
[32m[20221213 12:22:49 @agent_ppo2.py:179][0m |          -0.0411 |           4.1466 |           4.9871 |
[32m[20221213 12:22:49 @agent_ppo2.py:179][0m |          -0.0425 |           4.1130 |           5.0091 |
[32m[20221213 12:22:49 @agent_ppo2.py:179][0m |          -0.0473 |           4.1075 |           5.0381 |
[32m[20221213 12:22:49 @agent_ppo2.py:179][0m |          -0.0476 |           4.0877 |           5.0555 |
[32m[20221213 12:22:49 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:22:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.14
[32m[20221213 12:22:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 142.40
[32m[20221213 12:22:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.27
[32m[20221213 12:22:49 @agent_ppo2.py:137][0m Total time:       6.88 min
[32m[20221213 12:22:49 @agent_ppo2.py:139][0m 468992 total steps have happened
[32m[20221213 12:22:49 @agent_ppo2.py:115][0m #------------------------ Iteration 229 --------------------------#
[32m[20221213 12:22:50 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:22:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:50 @agent_ppo2.py:179][0m |           0.0227 |           3.7593 |           5.0714 |
[32m[20221213 12:22:50 @agent_ppo2.py:179][0m |           0.0082 |           3.6434 |           5.0501 |
[32m[20221213 12:22:50 @agent_ppo2.py:179][0m |          -0.0130 |           3.6142 |           5.1061 |
[32m[20221213 12:22:50 @agent_ppo2.py:179][0m |          -0.0276 |           3.5821 |           5.1486 |
[32m[20221213 12:22:50 @agent_ppo2.py:179][0m |          -0.0249 |           3.5634 |           5.1440 |
[32m[20221213 12:22:51 @agent_ppo2.py:179][0m |          -0.0297 |           3.5702 |           5.1645 |
[32m[20221213 12:22:51 @agent_ppo2.py:179][0m |          -0.0304 |           3.6739 |           5.1969 |
[32m[20221213 12:22:51 @agent_ppo2.py:179][0m |          -0.0419 |           3.5325 |           5.2265 |
[32m[20221213 12:22:51 @agent_ppo2.py:179][0m |          -0.0413 |           3.5172 |           5.2371 |
[32m[20221213 12:22:51 @agent_ppo2.py:179][0m |          -0.0345 |           3.5223 |           5.2183 |
[32m[20221213 12:22:51 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:22:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 95.21
[32m[20221213 12:22:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 121.75
[32m[20221213 12:22:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 160.73
[32m[20221213 12:22:51 @agent_ppo2.py:137][0m Total time:       6.91 min
[32m[20221213 12:22:51 @agent_ppo2.py:139][0m 471040 total steps have happened
[32m[20221213 12:22:51 @agent_ppo2.py:115][0m #------------------------ Iteration 230 --------------------------#
[32m[20221213 12:22:52 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:22:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:52 @agent_ppo2.py:179][0m |           0.0187 |           4.5191 |           5.1197 |
[32m[20221213 12:22:52 @agent_ppo2.py:179][0m |          -0.0102 |           4.4143 |           5.1140 |
[32m[20221213 12:22:52 @agent_ppo2.py:179][0m |          -0.0314 |           4.4036 |           5.1201 |
[32m[20221213 12:22:52 @agent_ppo2.py:179][0m |          -0.0183 |           4.6195 |           5.1562 |
[32m[20221213 12:22:52 @agent_ppo2.py:179][0m |          -0.0192 |           4.3299 |           5.0609 |
[32m[20221213 12:22:52 @agent_ppo2.py:179][0m |          -0.0425 |           4.2634 |           5.1545 |
[32m[20221213 12:22:53 @agent_ppo2.py:179][0m |          -0.0476 |           4.2426 |           5.1911 |
[32m[20221213 12:22:53 @agent_ppo2.py:179][0m |          -0.0474 |           4.2258 |           5.2008 |
[32m[20221213 12:22:53 @agent_ppo2.py:179][0m |          -0.0549 |           4.2162 |           5.2181 |
[32m[20221213 12:22:53 @agent_ppo2.py:179][0m |          -0.0508 |           4.1931 |           5.2426 |
[32m[20221213 12:22:53 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:22:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 125.95
[32m[20221213 12:22:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 132.60
[32m[20221213 12:22:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.54
[32m[20221213 12:22:53 @agent_ppo2.py:137][0m Total time:       6.94 min
[32m[20221213 12:22:53 @agent_ppo2.py:139][0m 473088 total steps have happened
[32m[20221213 12:22:53 @agent_ppo2.py:115][0m #------------------------ Iteration 231 --------------------------#
[32m[20221213 12:22:53 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:22:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:54 @agent_ppo2.py:179][0m |           0.0395 |           4.7145 |           5.3424 |
[32m[20221213 12:22:54 @agent_ppo2.py:179][0m |           0.0158 |           4.5968 |           5.2231 |
[32m[20221213 12:22:54 @agent_ppo2.py:179][0m |          -0.0173 |           4.5377 |           5.4124 |
[32m[20221213 12:22:54 @agent_ppo2.py:179][0m |          -0.0263 |           4.5265 |           5.4649 |
[32m[20221213 12:22:54 @agent_ppo2.py:179][0m |          -0.0325 |           4.4761 |           5.4769 |
[32m[20221213 12:22:54 @agent_ppo2.py:179][0m |          -0.0395 |           4.4380 |           5.4838 |
[32m[20221213 12:22:54 @agent_ppo2.py:179][0m |          -0.0423 |           4.4281 |           5.4943 |
[32m[20221213 12:22:55 @agent_ppo2.py:179][0m |          -0.0464 |           4.4578 |           5.5106 |
[32m[20221213 12:22:55 @agent_ppo2.py:179][0m |          -0.0401 |           4.6182 |           5.5264 |
[32m[20221213 12:22:55 @agent_ppo2.py:179][0m |          -0.0523 |           4.4361 |           5.5384 |
[32m[20221213 12:22:55 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:22:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 123.24
[32m[20221213 12:22:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 135.58
[32m[20221213 12:22:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 164.65
[32m[20221213 12:22:55 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 164.65
[32m[20221213 12:22:55 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 164.65
[32m[20221213 12:22:55 @agent_ppo2.py:137][0m Total time:       6.97 min
[32m[20221213 12:22:55 @agent_ppo2.py:139][0m 475136 total steps have happened
[32m[20221213 12:22:55 @agent_ppo2.py:115][0m #------------------------ Iteration 232 --------------------------#
[32m[20221213 12:22:55 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:22:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:55 @agent_ppo2.py:179][0m |           0.0375 |           5.1012 |           5.2855 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |           0.0036 |           4.9872 |           5.2185 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |          -0.0225 |           4.9885 |           5.3710 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |          -0.0270 |           4.9472 |           5.3852 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |          -0.0324 |           4.8959 |           5.3816 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |          -0.0372 |           4.8754 |           5.3943 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |          -0.0381 |           4.8497 |           5.4182 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |          -0.0424 |           4.8348 |           5.4254 |
[32m[20221213 12:22:56 @agent_ppo2.py:179][0m |          -0.0502 |           4.8004 |           5.4501 |
[32m[20221213 12:22:57 @agent_ppo2.py:179][0m |          -0.0491 |           4.7751 |           5.4608 |
[32m[20221213 12:22:57 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:22:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.23
[32m[20221213 12:22:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.72
[32m[20221213 12:22:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 150.05
[32m[20221213 12:22:57 @agent_ppo2.py:137][0m Total time:       7.00 min
[32m[20221213 12:22:57 @agent_ppo2.py:139][0m 477184 total steps have happened
[32m[20221213 12:22:57 @agent_ppo2.py:115][0m #------------------------ Iteration 233 --------------------------#
[32m[20221213 12:22:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:57 @agent_ppo2.py:179][0m |           0.0237 |           4.8403 |           5.5121 |
[32m[20221213 12:22:57 @agent_ppo2.py:179][0m |           0.0047 |           4.6764 |           5.4438 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0240 |           4.6040 |           5.5224 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0329 |           4.5735 |           5.5566 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0373 |           4.5268 |           5.5495 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0421 |           4.4989 |           5.5872 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0472 |           4.4844 |           5.6100 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0493 |           4.4958 |           5.6338 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0502 |           4.4480 |           5.6341 |
[32m[20221213 12:22:58 @agent_ppo2.py:179][0m |          -0.0529 |           4.4736 |           5.6609 |
[32m[20221213 12:22:58 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:22:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.17
[32m[20221213 12:22:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 141.34
[32m[20221213 12:22:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 159.30
[32m[20221213 12:22:59 @agent_ppo2.py:137][0m Total time:       7.03 min
[32m[20221213 12:22:59 @agent_ppo2.py:139][0m 479232 total steps have happened
[32m[20221213 12:22:59 @agent_ppo2.py:115][0m #------------------------ Iteration 234 --------------------------#
[32m[20221213 12:22:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:22:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:22:59 @agent_ppo2.py:179][0m |           0.0288 |           4.8612 |           5.5646 |
[32m[20221213 12:22:59 @agent_ppo2.py:179][0m |           0.0119 |           4.7071 |           5.4772 |
[32m[20221213 12:22:59 @agent_ppo2.py:179][0m |          -0.0128 |           4.6822 |           5.5549 |
[32m[20221213 12:23:00 @agent_ppo2.py:179][0m |          -0.0259 |           4.6061 |           5.5742 |
[32m[20221213 12:23:00 @agent_ppo2.py:179][0m |          -0.0291 |           4.5759 |           5.5914 |
[32m[20221213 12:23:00 @agent_ppo2.py:179][0m |          -0.0372 |           4.5774 |           5.6413 |
[32m[20221213 12:23:00 @agent_ppo2.py:179][0m |          -0.0347 |           4.7553 |           5.6411 |
[32m[20221213 12:23:00 @agent_ppo2.py:179][0m |          -0.0406 |           4.5622 |           5.6532 |
[32m[20221213 12:23:00 @agent_ppo2.py:179][0m |          -0.0419 |           4.5074 |           5.6515 |
[32m[20221213 12:23:00 @agent_ppo2.py:179][0m |          -0.0438 |           4.5351 |           5.6748 |
[32m[20221213 12:23:00 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:23:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.00
[32m[20221213 12:23:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 150.89
[32m[20221213 12:23:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 118.91
[32m[20221213 12:23:01 @agent_ppo2.py:137][0m Total time:       7.07 min
[32m[20221213 12:23:01 @agent_ppo2.py:139][0m 481280 total steps have happened
[32m[20221213 12:23:01 @agent_ppo2.py:115][0m #------------------------ Iteration 235 --------------------------#
[32m[20221213 12:23:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:23:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:01 @agent_ppo2.py:179][0m |           0.0278 |           4.9844 |           5.6351 |
[32m[20221213 12:23:01 @agent_ppo2.py:179][0m |           0.0007 |           4.8219 |           5.5523 |
[32m[20221213 12:23:01 @agent_ppo2.py:179][0m |          -0.0169 |           4.9293 |           5.6463 |
[32m[20221213 12:23:01 @agent_ppo2.py:179][0m |          -0.0236 |           4.7215 |           5.6778 |
[32m[20221213 12:23:02 @agent_ppo2.py:179][0m |          -0.0319 |           4.6973 |           5.6635 |
[32m[20221213 12:23:02 @agent_ppo2.py:179][0m |          -0.0395 |           4.6517 |           5.6998 |
[32m[20221213 12:23:02 @agent_ppo2.py:179][0m |          -0.0457 |           4.6286 |           5.7408 |
[32m[20221213 12:23:02 @agent_ppo2.py:179][0m |          -0.0429 |           4.6203 |           5.7578 |
[32m[20221213 12:23:02 @agent_ppo2.py:179][0m |          -0.0479 |           4.5976 |           5.7609 |
[32m[20221213 12:23:02 @agent_ppo2.py:179][0m |          -0.0499 |           4.5921 |           5.7735 |
[32m[20221213 12:23:02 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:23:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 138.62
[32m[20221213 12:23:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 154.51
[32m[20221213 12:23:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 128.95
[32m[20221213 12:23:02 @agent_ppo2.py:137][0m Total time:       7.10 min
[32m[20221213 12:23:02 @agent_ppo2.py:139][0m 483328 total steps have happened
[32m[20221213 12:23:02 @agent_ppo2.py:115][0m #------------------------ Iteration 236 --------------------------#
[32m[20221213 12:23:03 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:03 @agent_ppo2.py:179][0m |           0.0446 |           5.4689 |           5.7341 |
[32m[20221213 12:23:03 @agent_ppo2.py:179][0m |           0.0270 |           5.5802 |           5.6618 |
[32m[20221213 12:23:03 @agent_ppo2.py:179][0m |           0.0098 |           6.0645 |           5.6744 |
[32m[20221213 12:23:03 @agent_ppo2.py:179][0m |          -0.0215 |           5.3184 |           5.8243 |
[32m[20221213 12:23:04 @agent_ppo2.py:179][0m |          -0.0351 |           5.2953 |           5.8920 |
[32m[20221213 12:23:04 @agent_ppo2.py:179][0m |          -0.0373 |           5.2574 |           5.9157 |
[32m[20221213 12:23:04 @agent_ppo2.py:179][0m |          -0.0395 |           5.2441 |           5.9393 |
[32m[20221213 12:23:04 @agent_ppo2.py:179][0m |          -0.0430 |           5.2374 |           5.9775 |
[32m[20221213 12:23:04 @agent_ppo2.py:179][0m |          -0.0495 |           5.2164 |           5.9741 |
[32m[20221213 12:23:04 @agent_ppo2.py:179][0m |          -0.0478 |           5.2104 |           6.0020 |
[32m[20221213 12:23:04 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:23:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.61
[32m[20221213 12:23:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 149.46
[32m[20221213 12:23:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 148.53
[32m[20221213 12:23:04 @agent_ppo2.py:137][0m Total time:       7.13 min
[32m[20221213 12:23:04 @agent_ppo2.py:139][0m 485376 total steps have happened
[32m[20221213 12:23:04 @agent_ppo2.py:115][0m #------------------------ Iteration 237 --------------------------#
[32m[20221213 12:23:05 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:05 @agent_ppo2.py:179][0m |           0.0297 |           5.2470 |           5.8712 |
[32m[20221213 12:23:05 @agent_ppo2.py:179][0m |          -0.0131 |           5.1255 |           5.8655 |
[32m[20221213 12:23:05 @agent_ppo2.py:179][0m |          -0.0152 |           5.4818 |           5.9169 |
[32m[20221213 12:23:05 @agent_ppo2.py:179][0m |          -0.0384 |           5.0756 |           5.9662 |
[32m[20221213 12:23:05 @agent_ppo2.py:179][0m |          -0.0356 |           5.0159 |           5.9419 |
[32m[20221213 12:23:06 @agent_ppo2.py:179][0m |          -0.0381 |           4.9898 |           5.9607 |
[32m[20221213 12:23:06 @agent_ppo2.py:179][0m |          -0.0425 |           5.0953 |           6.0015 |
[32m[20221213 12:23:06 @agent_ppo2.py:179][0m |          -0.0447 |           4.9547 |           6.0079 |
[32m[20221213 12:23:06 @agent_ppo2.py:179][0m |          -0.0504 |           4.9175 |           6.0450 |
[32m[20221213 12:23:06 @agent_ppo2.py:179][0m |          -0.0538 |           4.9054 |           6.0646 |
[32m[20221213 12:23:06 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:23:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.49
[32m[20221213 12:23:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 143.91
[32m[20221213 12:23:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.30
[32m[20221213 12:23:06 @agent_ppo2.py:137][0m Total time:       7.16 min
[32m[20221213 12:23:06 @agent_ppo2.py:139][0m 487424 total steps have happened
[32m[20221213 12:23:06 @agent_ppo2.py:115][0m #------------------------ Iteration 238 --------------------------#
[32m[20221213 12:23:07 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:07 @agent_ppo2.py:179][0m |           0.0336 |           4.9205 |           6.0434 |
[32m[20221213 12:23:07 @agent_ppo2.py:179][0m |           0.0313 |           4.7548 |           5.9379 |
[32m[20221213 12:23:07 @agent_ppo2.py:179][0m |          -0.0005 |           4.6695 |           6.0275 |
[32m[20221213 12:23:07 @agent_ppo2.py:179][0m |          -0.0178 |           4.6413 |           6.1154 |
[32m[20221213 12:23:07 @agent_ppo2.py:179][0m |          -0.0249 |           4.9364 |           6.1628 |
[32m[20221213 12:23:07 @agent_ppo2.py:179][0m |          -0.0306 |           4.5378 |           6.1448 |
[32m[20221213 12:23:08 @agent_ppo2.py:179][0m |          -0.0251 |           4.5352 |           6.1994 |
[32m[20221213 12:23:08 @agent_ppo2.py:179][0m |          -0.0214 |           4.4596 |           5.7973 |
[32m[20221213 12:23:08 @agent_ppo2.py:179][0m |          -0.0296 |           4.4546 |           5.5919 |
[32m[20221213 12:23:08 @agent_ppo2.py:179][0m |          -0.0489 |           4.4427 |           5.6390 |
[32m[20221213 12:23:08 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:23:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 124.92
[32m[20221213 12:23:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 146.12
[32m[20221213 12:23:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.70
[32m[20221213 12:23:08 @agent_ppo2.py:137][0m Total time:       7.19 min
[32m[20221213 12:23:08 @agent_ppo2.py:139][0m 489472 total steps have happened
[32m[20221213 12:23:08 @agent_ppo2.py:115][0m #------------------------ Iteration 239 --------------------------#
[32m[20221213 12:23:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:23:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:09 @agent_ppo2.py:179][0m |           0.0206 |           5.5920 |           6.0870 |
[32m[20221213 12:23:09 @agent_ppo2.py:179][0m |           0.0045 |           5.4192 |           6.0167 |
[32m[20221213 12:23:09 @agent_ppo2.py:179][0m |          -0.0246 |           5.3488 |           6.1107 |
[32m[20221213 12:23:09 @agent_ppo2.py:179][0m |          -0.0312 |           5.2968 |           6.1723 |
[32m[20221213 12:23:09 @agent_ppo2.py:179][0m |          -0.0343 |           5.3870 |           6.1714 |
[32m[20221213 12:23:09 @agent_ppo2.py:179][0m |          -0.0437 |           5.2590 |           6.2027 |
[32m[20221213 12:23:09 @agent_ppo2.py:179][0m |          -0.0434 |           5.2141 |           6.2157 |
[32m[20221213 12:23:10 @agent_ppo2.py:179][0m |          -0.0453 |           5.2178 |           6.2180 |
[32m[20221213 12:23:10 @agent_ppo2.py:179][0m |          -0.0455 |           5.1877 |           6.2062 |
[32m[20221213 12:23:10 @agent_ppo2.py:179][0m |          -0.0524 |           5.1928 |           6.2494 |
[32m[20221213 12:23:10 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:23:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 130.89
[32m[20221213 12:23:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.69
[32m[20221213 12:23:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 162.97
[32m[20221213 12:23:10 @agent_ppo2.py:137][0m Total time:       7.22 min
[32m[20221213 12:23:10 @agent_ppo2.py:139][0m 491520 total steps have happened
[32m[20221213 12:23:10 @agent_ppo2.py:115][0m #------------------------ Iteration 240 --------------------------#
[32m[20221213 12:23:10 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:23:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:11 @agent_ppo2.py:179][0m |           0.0279 |           5.4967 |           6.0601 |
[32m[20221213 12:23:11 @agent_ppo2.py:179][0m |           0.0063 |           5.5548 |           6.0342 |
[32m[20221213 12:23:11 @agent_ppo2.py:179][0m |          -0.0215 |           5.3533 |           6.1080 |
[32m[20221213 12:23:11 @agent_ppo2.py:179][0m |          -0.0284 |           5.3148 |           6.1568 |
[32m[20221213 12:23:11 @agent_ppo2.py:179][0m |          -0.0373 |           5.2846 |           6.1792 |
[32m[20221213 12:23:11 @agent_ppo2.py:179][0m |          -0.0350 |           5.3558 |           6.1945 |
[32m[20221213 12:23:11 @agent_ppo2.py:179][0m |          -0.0396 |           5.2406 |           6.2072 |
[32m[20221213 12:23:12 @agent_ppo2.py:179][0m |          -0.0432 |           5.2441 |           6.2161 |
[32m[20221213 12:23:12 @agent_ppo2.py:179][0m |          -0.0493 |           5.2206 |           6.2316 |
[32m[20221213 12:23:12 @agent_ppo2.py:179][0m |          -0.0511 |           5.1948 |           6.2735 |
[32m[20221213 12:23:12 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:23:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.39
[32m[20221213 12:23:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 147.41
[32m[20221213 12:23:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 157.66
[32m[20221213 12:23:12 @agent_ppo2.py:137][0m Total time:       7.25 min
[32m[20221213 12:23:12 @agent_ppo2.py:139][0m 493568 total steps have happened
[32m[20221213 12:23:12 @agent_ppo2.py:115][0m #------------------------ Iteration 241 --------------------------#
[32m[20221213 12:23:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:23:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:12 @agent_ppo2.py:179][0m |           0.0275 |           5.7890 |           6.0848 |
[32m[20221213 12:23:13 @agent_ppo2.py:179][0m |           0.0057 |           5.7103 |           6.0443 |
[32m[20221213 12:23:13 @agent_ppo2.py:179][0m |          -0.0078 |           5.6654 |           6.0790 |
[32m[20221213 12:23:13 @agent_ppo2.py:179][0m |          -0.0206 |           5.7649 |           6.1612 |
[32m[20221213 12:23:13 @agent_ppo2.py:179][0m |          -0.0132 |           6.4075 |           6.1997 |
[32m[20221213 12:23:13 @agent_ppo2.py:179][0m |          -0.0032 |           6.4892 |           6.0185 |
[32m[20221213 12:23:13 @agent_ppo2.py:179][0m |          -0.0357 |           5.6409 |           6.1832 |
[32m[20221213 12:23:13 @agent_ppo2.py:179][0m |          -0.0413 |           5.5886 |           6.2202 |
[32m[20221213 12:23:14 @agent_ppo2.py:179][0m |          -0.0414 |           5.6807 |           6.2632 |
[32m[20221213 12:23:14 @agent_ppo2.py:179][0m |          -0.0435 |           5.5375 |           6.2434 |
[32m[20221213 12:23:14 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:23:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 157.47
[32m[20221213 12:23:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 166.55
[32m[20221213 12:23:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.10
[32m[20221213 12:23:14 @agent_ppo2.py:137][0m Total time:       7.29 min
[32m[20221213 12:23:14 @agent_ppo2.py:139][0m 495616 total steps have happened
[32m[20221213 12:23:14 @agent_ppo2.py:115][0m #------------------------ Iteration 242 --------------------------#
[32m[20221213 12:23:14 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:14 @agent_ppo2.py:179][0m |           0.0286 |           5.7277 |           6.1311 |
[32m[20221213 12:23:14 @agent_ppo2.py:179][0m |           0.0089 |           5.5051 |           6.0055 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0148 |           5.4209 |           6.0544 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0293 |           5.3785 |           6.1113 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0271 |           5.7787 |           6.1386 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0369 |           5.3138 |           6.1341 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0460 |           5.2976 |           6.1671 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0434 |           5.2771 |           6.1782 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0442 |           5.2694 |           6.1800 |
[32m[20221213 12:23:15 @agent_ppo2.py:179][0m |          -0.0487 |           5.2227 |           6.2307 |
[32m[20221213 12:23:15 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:23:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.03
[32m[20221213 12:23:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.56
[32m[20221213 12:23:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.54
[32m[20221213 12:23:16 @agent_ppo2.py:137][0m Total time:       7.32 min
[32m[20221213 12:23:16 @agent_ppo2.py:139][0m 497664 total steps have happened
[32m[20221213 12:23:16 @agent_ppo2.py:115][0m #------------------------ Iteration 243 --------------------------#
[32m[20221213 12:23:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:23:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:16 @agent_ppo2.py:179][0m |           0.0366 |           5.7651 |           6.1589 |
[32m[20221213 12:23:16 @agent_ppo2.py:179][0m |          -0.0058 |           5.5508 |           6.1657 |
[32m[20221213 12:23:16 @agent_ppo2.py:179][0m |          -0.0293 |           5.5196 |           6.1745 |
[32m[20221213 12:23:16 @agent_ppo2.py:179][0m |          -0.0345 |           5.5044 |           6.2062 |
[32m[20221213 12:23:17 @agent_ppo2.py:179][0m |          -0.0434 |           5.4420 |           6.2215 |
[32m[20221213 12:23:17 @agent_ppo2.py:179][0m |          -0.0453 |           5.4253 |           6.2535 |
[32m[20221213 12:23:17 @agent_ppo2.py:179][0m |          -0.0429 |           5.6316 |           6.2581 |
[32m[20221213 12:23:17 @agent_ppo2.py:179][0m |          -0.0557 |           5.4149 |           6.2650 |
[32m[20221213 12:23:17 @agent_ppo2.py:179][0m |          -0.0562 |           5.3433 |           6.2741 |
[32m[20221213 12:23:17 @agent_ppo2.py:179][0m |          -0.0598 |           5.3339 |           6.2915 |
[32m[20221213 12:23:17 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:23:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.73
[32m[20221213 12:23:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 158.23
[32m[20221213 12:23:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 91.68
[32m[20221213 12:23:18 @agent_ppo2.py:137][0m Total time:       7.35 min
[32m[20221213 12:23:18 @agent_ppo2.py:139][0m 499712 total steps have happened
[32m[20221213 12:23:18 @agent_ppo2.py:115][0m #------------------------ Iteration 244 --------------------------#
[32m[20221213 12:23:18 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:18 @agent_ppo2.py:179][0m |           0.0366 |           6.4332 |           6.1618 |
[32m[20221213 12:23:18 @agent_ppo2.py:179][0m |           0.0284 |           6.1638 |           6.0133 |
[32m[20221213 12:23:18 @agent_ppo2.py:179][0m |          -0.0030 |           6.0748 |           6.0823 |
[32m[20221213 12:23:18 @agent_ppo2.py:179][0m |          -0.0147 |           6.5114 |           6.2219 |
[32m[20221213 12:23:19 @agent_ppo2.py:179][0m |          -0.0321 |           5.9922 |           6.2655 |
[32m[20221213 12:23:19 @agent_ppo2.py:179][0m |          -0.0432 |           5.9082 |           6.2858 |
[32m[20221213 12:23:19 @agent_ppo2.py:179][0m |          -0.0472 |           5.9113 |           6.3257 |
[32m[20221213 12:23:19 @agent_ppo2.py:179][0m |          -0.0447 |           5.8914 |           6.3019 |
[32m[20221213 12:23:19 @agent_ppo2.py:179][0m |          -0.0496 |           5.8271 |           6.3355 |
[32m[20221213 12:23:19 @agent_ppo2.py:179][0m |          -0.0503 |           5.8330 |           6.3353 |
[32m[20221213 12:23:19 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:23:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.11
[32m[20221213 12:23:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.50
[32m[20221213 12:23:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 106.43
[32m[20221213 12:23:19 @agent_ppo2.py:137][0m Total time:       7.38 min
[32m[20221213 12:23:19 @agent_ppo2.py:139][0m 501760 total steps have happened
[32m[20221213 12:23:19 @agent_ppo2.py:115][0m #------------------------ Iteration 245 --------------------------#
[32m[20221213 12:23:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:23:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:20 @agent_ppo2.py:179][0m |           0.0385 |           5.5386 |           6.4102 |
[32m[20221213 12:23:20 @agent_ppo2.py:179][0m |           0.0082 |           5.5821 |           6.2800 |
[32m[20221213 12:23:20 @agent_ppo2.py:179][0m |          -0.0183 |           5.4901 |           6.4599 |
[32m[20221213 12:23:20 @agent_ppo2.py:179][0m |          -0.0285 |           5.3464 |           6.5200 |
[32m[20221213 12:23:20 @agent_ppo2.py:179][0m |          -0.0334 |           5.1988 |           6.5052 |
[32m[20221213 12:23:21 @agent_ppo2.py:179][0m |          -0.0438 |           5.1408 |           6.5691 |
[32m[20221213 12:23:21 @agent_ppo2.py:179][0m |          -0.0446 |           5.1073 |           6.5964 |
[32m[20221213 12:23:21 @agent_ppo2.py:179][0m |          -0.0459 |           5.1102 |           6.6156 |
[32m[20221213 12:23:21 @agent_ppo2.py:179][0m |          -0.0496 |           5.0777 |           6.6184 |
[32m[20221213 12:23:21 @agent_ppo2.py:179][0m |          -0.0522 |           5.0692 |           6.6353 |
[32m[20221213 12:23:21 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:23:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.37
[32m[20221213 12:23:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.01
[32m[20221213 12:23:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 173.45
[32m[20221213 12:23:21 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 173.45
[32m[20221213 12:23:21 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 173.45
[32m[20221213 12:23:21 @agent_ppo2.py:137][0m Total time:       7.41 min
[32m[20221213 12:23:21 @agent_ppo2.py:139][0m 503808 total steps have happened
[32m[20221213 12:23:21 @agent_ppo2.py:115][0m #------------------------ Iteration 246 --------------------------#
[32m[20221213 12:23:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:23:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:22 @agent_ppo2.py:179][0m |           0.0251 |           4.9925 |           6.5286 |
[32m[20221213 12:23:22 @agent_ppo2.py:179][0m |           0.0004 |           4.5981 |           6.5000 |
[32m[20221213 12:23:22 @agent_ppo2.py:179][0m |          -0.0229 |           4.5094 |           6.5434 |
[32m[20221213 12:23:22 @agent_ppo2.py:179][0m |          -0.0317 |           4.4975 |           6.5497 |
[32m[20221213 12:23:22 @agent_ppo2.py:179][0m |          -0.0308 |           4.5058 |           6.5851 |
[32m[20221213 12:23:23 @agent_ppo2.py:179][0m |          -0.0409 |           4.4057 |           6.5729 |
[32m[20221213 12:23:23 @agent_ppo2.py:179][0m |          -0.0464 |           4.4594 |           6.6156 |
[32m[20221213 12:23:23 @agent_ppo2.py:179][0m |          -0.0494 |           4.4064 |           6.6137 |
[32m[20221213 12:23:23 @agent_ppo2.py:179][0m |          -0.0474 |           4.3514 |           6.6409 |
[32m[20221213 12:23:23 @agent_ppo2.py:179][0m |          -0.0552 |           4.3345 |           6.6411 |
[32m[20221213 12:23:23 @agent_ppo2.py:124][0m Policy update time: 1.66 s
[32m[20221213 12:23:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.03
[32m[20221213 12:23:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 160.35
[32m[20221213 12:23:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 135.77
[32m[20221213 12:23:23 @agent_ppo2.py:137][0m Total time:       7.44 min
[32m[20221213 12:23:23 @agent_ppo2.py:139][0m 505856 total steps have happened
[32m[20221213 12:23:23 @agent_ppo2.py:115][0m #------------------------ Iteration 247 --------------------------#
[32m[20221213 12:23:24 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:24 @agent_ppo2.py:179][0m |           0.0557 |           6.3556 |           6.5494 |
[32m[20221213 12:23:24 @agent_ppo2.py:179][0m |           0.0121 |           6.0808 |           6.5244 |
[32m[20221213 12:23:24 @agent_ppo2.py:179][0m |          -0.0214 |           5.9556 |           6.6592 |
[32m[20221213 12:23:24 @agent_ppo2.py:179][0m |          -0.0319 |           5.8676 |           6.7219 |
[32m[20221213 12:23:25 @agent_ppo2.py:179][0m |          -0.0447 |           5.8301 |           6.7348 |
[32m[20221213 12:23:25 @agent_ppo2.py:179][0m |          -0.0469 |           5.7893 |           6.7593 |
[32m[20221213 12:23:25 @agent_ppo2.py:179][0m |          -0.0439 |           5.7320 |           6.7652 |
[32m[20221213 12:23:25 @agent_ppo2.py:179][0m |          -0.0559 |           5.7296 |           6.7793 |
[32m[20221213 12:23:25 @agent_ppo2.py:179][0m |          -0.0497 |           5.7699 |           6.7868 |
[32m[20221213 12:23:25 @agent_ppo2.py:179][0m |          -0.0580 |           5.6471 |           6.7965 |
[32m[20221213 12:23:25 @agent_ppo2.py:124][0m Policy update time: 1.66 s
[32m[20221213 12:23:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 144.34
[32m[20221213 12:23:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.20
[32m[20221213 12:23:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 168.55
[32m[20221213 12:23:26 @agent_ppo2.py:137][0m Total time:       7.48 min
[32m[20221213 12:23:26 @agent_ppo2.py:139][0m 507904 total steps have happened
[32m[20221213 12:23:26 @agent_ppo2.py:115][0m #------------------------ Iteration 248 --------------------------#
[32m[20221213 12:23:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:23:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:26 @agent_ppo2.py:179][0m |           0.0391 |           6.5015 |           7.0313 |
[32m[20221213 12:23:26 @agent_ppo2.py:179][0m |           0.0228 |           6.2905 |           7.0134 |
[32m[20221213 12:23:26 @agent_ppo2.py:179][0m |           0.0143 |           6.2284 |           6.7894 |
[32m[20221213 12:23:26 @agent_ppo2.py:179][0m |          -0.0137 |           6.2523 |           7.0835 |
[32m[20221213 12:23:27 @agent_ppo2.py:179][0m |          -0.0308 |           6.1603 |           7.1278 |
[32m[20221213 12:23:27 @agent_ppo2.py:179][0m |          -0.0383 |           6.1280 |           7.1629 |
[32m[20221213 12:23:27 @agent_ppo2.py:179][0m |          -0.0430 |           6.0881 |           7.1743 |
[32m[20221213 12:23:27 @agent_ppo2.py:179][0m |          -0.0468 |           6.1085 |           7.2075 |
[32m[20221213 12:23:27 @agent_ppo2.py:179][0m |          -0.0456 |           6.1085 |           7.2279 |
[32m[20221213 12:23:27 @agent_ppo2.py:179][0m |          -0.0576 |           6.0281 |           7.2278 |
[32m[20221213 12:23:27 @agent_ppo2.py:124][0m Policy update time: 1.63 s
[32m[20221213 12:23:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.40
[32m[20221213 12:23:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 164.65
[32m[20221213 12:23:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.22
[32m[20221213 12:23:28 @agent_ppo2.py:137][0m Total time:       7.51 min
[32m[20221213 12:23:28 @agent_ppo2.py:139][0m 509952 total steps have happened
[32m[20221213 12:23:28 @agent_ppo2.py:115][0m #------------------------ Iteration 249 --------------------------#
[32m[20221213 12:23:28 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:28 @agent_ppo2.py:179][0m |           0.0475 |           6.4251 |           7.1453 |
[32m[20221213 12:23:28 @agent_ppo2.py:179][0m |           0.0618 |           7.0478 |           6.9074 |
[32m[20221213 12:23:28 @agent_ppo2.py:179][0m |          -0.0133 |           6.0832 |           7.1447 |
[32m[20221213 12:23:29 @agent_ppo2.py:179][0m |          -0.0194 |           5.9995 |           7.1998 |
[32m[20221213 12:23:29 @agent_ppo2.py:179][0m |          -0.0351 |           5.9264 |           7.2639 |
[32m[20221213 12:23:29 @agent_ppo2.py:179][0m |          -0.0396 |           5.8638 |           7.2729 |
[32m[20221213 12:23:29 @agent_ppo2.py:179][0m |          -0.0409 |           5.8718 |           7.2763 |
[32m[20221213 12:23:29 @agent_ppo2.py:179][0m |          -0.0469 |           5.7794 |           7.3078 |
[32m[20221213 12:23:29 @agent_ppo2.py:179][0m |          -0.0395 |           6.0822 |           7.3308 |
[32m[20221213 12:23:29 @agent_ppo2.py:179][0m |          -0.0440 |           5.8395 |           7.3480 |
[32m[20221213 12:23:29 @agent_ppo2.py:124][0m Policy update time: 1.63 s
[32m[20221213 12:23:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.35
[32m[20221213 12:23:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.36
[32m[20221213 12:23:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 163.37
[32m[20221213 12:23:30 @agent_ppo2.py:137][0m Total time:       7.55 min
[32m[20221213 12:23:30 @agent_ppo2.py:139][0m 512000 total steps have happened
[32m[20221213 12:23:30 @agent_ppo2.py:115][0m #------------------------ Iteration 250 --------------------------#
[32m[20221213 12:23:30 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:23:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:30 @agent_ppo2.py:179][0m |           0.0346 |           6.7882 |           7.3539 |
[32m[20221213 12:23:30 @agent_ppo2.py:179][0m |           0.0109 |           6.6529 |           7.2576 |
[32m[20221213 12:23:30 @agent_ppo2.py:179][0m |          -0.0199 |           6.5678 |           7.4136 |
[32m[20221213 12:23:31 @agent_ppo2.py:179][0m |          -0.0303 |           6.4997 |           7.4668 |
[32m[20221213 12:23:31 @agent_ppo2.py:179][0m |          -0.0382 |           6.5030 |           7.5034 |
[32m[20221213 12:23:31 @agent_ppo2.py:179][0m |          -0.0396 |           6.4690 |           7.5117 |
[32m[20221213 12:23:31 @agent_ppo2.py:179][0m |          -0.0436 |           6.4462 |           7.5378 |
[32m[20221213 12:23:31 @agent_ppo2.py:179][0m |          -0.0463 |           6.4187 |           7.5617 |
[32m[20221213 12:23:31 @agent_ppo2.py:179][0m |          -0.0440 |           6.4509 |           7.5876 |
[32m[20221213 12:23:32 @agent_ppo2.py:179][0m |          -0.0476 |           6.3921 |           7.5858 |
[32m[20221213 12:23:32 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:23:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.33
[32m[20221213 12:23:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.64
[32m[20221213 12:23:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.86
[32m[20221213 12:23:32 @agent_ppo2.py:137][0m Total time:       7.58 min
[32m[20221213 12:23:32 @agent_ppo2.py:139][0m 514048 total steps have happened
[32m[20221213 12:23:32 @agent_ppo2.py:115][0m #------------------------ Iteration 251 --------------------------#
[32m[20221213 12:23:32 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:32 @agent_ppo2.py:179][0m |           0.0849 |           5.8377 |           7.3828 |
[32m[20221213 12:23:32 @agent_ppo2.py:179][0m |           0.0102 |           5.6404 |           7.2070 |
[32m[20221213 12:23:33 @agent_ppo2.py:179][0m |          -0.0189 |           5.5648 |           7.4652 |
[32m[20221213 12:23:33 @agent_ppo2.py:179][0m |          -0.0263 |           5.5417 |           7.5402 |
[32m[20221213 12:23:33 @agent_ppo2.py:179][0m |          -0.0254 |           5.6598 |           7.6057 |
[32m[20221213 12:23:33 @agent_ppo2.py:179][0m |          -0.0118 |           6.1051 |           7.5054 |
[32m[20221213 12:23:33 @agent_ppo2.py:179][0m |          -0.0381 |           5.4602 |           7.6153 |
[32m[20221213 12:23:33 @agent_ppo2.py:179][0m |          -0.0433 |           5.4311 |           7.6829 |
[32m[20221213 12:23:33 @agent_ppo2.py:179][0m |          -0.0394 |           5.4519 |           7.6797 |
[32m[20221213 12:23:34 @agent_ppo2.py:179][0m |          -0.0486 |           5.4025 |           7.7175 |
[32m[20221213 12:23:34 @agent_ppo2.py:124][0m Policy update time: 1.67 s
[32m[20221213 12:23:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.98
[32m[20221213 12:23:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.83
[32m[20221213 12:23:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 147.43
[32m[20221213 12:23:34 @agent_ppo2.py:137][0m Total time:       7.62 min
[32m[20221213 12:23:34 @agent_ppo2.py:139][0m 516096 total steps have happened
[32m[20221213 12:23:34 @agent_ppo2.py:115][0m #------------------------ Iteration 252 --------------------------#
[32m[20221213 12:23:34 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:34 @agent_ppo2.py:179][0m |           0.0519 |           6.5844 |           7.7000 |
[32m[20221213 12:23:34 @agent_ppo2.py:179][0m |           0.0544 |           6.4895 |           7.1291 |
[32m[20221213 12:23:35 @agent_ppo2.py:179][0m |          -0.0022 |           6.3928 |           7.6246 |
[32m[20221213 12:23:35 @agent_ppo2.py:179][0m |          -0.0211 |           6.3446 |           7.7628 |
[32m[20221213 12:23:35 @agent_ppo2.py:179][0m |          -0.0324 |           6.3100 |           7.8442 |
[32m[20221213 12:23:35 @agent_ppo2.py:179][0m |          -0.0430 |           6.3370 |           7.8921 |
[32m[20221213 12:23:35 @agent_ppo2.py:179][0m |          -0.0443 |           6.2855 |           7.9192 |
[32m[20221213 12:23:35 @agent_ppo2.py:179][0m |          -0.0446 |           6.2660 |           7.9451 |
[32m[20221213 12:23:36 @agent_ppo2.py:179][0m |          -0.0465 |           6.2349 |           7.9663 |
[32m[20221213 12:23:36 @agent_ppo2.py:179][0m |          -0.0527 |           6.2390 |           7.9954 |
[32m[20221213 12:23:36 @agent_ppo2.py:124][0m Policy update time: 1.67 s
[32m[20221213 12:23:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.58
[32m[20221213 12:23:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.24
[32m[20221213 12:23:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 192.87
[32m[20221213 12:23:36 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 192.87
[32m[20221213 12:23:36 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 192.87
[32m[20221213 12:23:36 @agent_ppo2.py:137][0m Total time:       7.65 min
[32m[20221213 12:23:36 @agent_ppo2.py:139][0m 518144 total steps have happened
[32m[20221213 12:23:36 @agent_ppo2.py:115][0m #------------------------ Iteration 253 --------------------------#
[32m[20221213 12:23:36 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:36 @agent_ppo2.py:179][0m |           0.0316 |           6.7035 |           7.7655 |
[32m[20221213 12:23:37 @agent_ppo2.py:179][0m |          -0.0038 |           6.5601 |           7.7322 |
[32m[20221213 12:23:37 @agent_ppo2.py:179][0m |          -0.0127 |           6.9952 |           7.7861 |
[32m[20221213 12:23:37 @agent_ppo2.py:179][0m |          -0.0331 |           6.4508 |           7.8543 |
[32m[20221213 12:23:37 @agent_ppo2.py:179][0m |          -0.0404 |           6.4016 |           7.9040 |
[32m[20221213 12:23:37 @agent_ppo2.py:179][0m |          -0.0426 |           6.3744 |           7.9075 |
[32m[20221213 12:23:37 @agent_ppo2.py:179][0m |          -0.0437 |           6.3574 |           7.9391 |
[32m[20221213 12:23:37 @agent_ppo2.py:179][0m |          -0.0496 |           6.3161 |           7.9697 |
[32m[20221213 12:23:38 @agent_ppo2.py:179][0m |          -0.0510 |           6.3199 |           7.9716 |
[32m[20221213 12:23:38 @agent_ppo2.py:179][0m |          -0.0495 |           6.3068 |           7.9763 |
[32m[20221213 12:23:38 @agent_ppo2.py:124][0m Policy update time: 1.62 s
[32m[20221213 12:23:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.17
[32m[20221213 12:23:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 161.05
[32m[20221213 12:23:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 164.25
[32m[20221213 12:23:38 @agent_ppo2.py:137][0m Total time:       7.69 min
[32m[20221213 12:23:38 @agent_ppo2.py:139][0m 520192 total steps have happened
[32m[20221213 12:23:38 @agent_ppo2.py:115][0m #------------------------ Iteration 254 --------------------------#
[32m[20221213 12:23:38 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:39 @agent_ppo2.py:179][0m |           0.0528 |           6.7845 |           7.5550 |
[32m[20221213 12:23:39 @agent_ppo2.py:179][0m |           0.0153 |           6.6605 |           7.5268 |
[32m[20221213 12:23:39 @agent_ppo2.py:179][0m |          -0.0051 |           6.7789 |           7.7078 |
[32m[20221213 12:23:39 @agent_ppo2.py:179][0m |          -0.0283 |           6.5546 |           7.8576 |
[32m[20221213 12:23:39 @agent_ppo2.py:179][0m |          -0.0348 |           6.4942 |           7.8900 |
[32m[20221213 12:23:39 @agent_ppo2.py:179][0m |          -0.0326 |           6.5735 |           7.9352 |
[32m[20221213 12:23:39 @agent_ppo2.py:179][0m |          -0.0381 |           6.4677 |           7.9217 |
[32m[20221213 12:23:40 @agent_ppo2.py:179][0m |          -0.0439 |           6.4222 |           7.9504 |
[32m[20221213 12:23:40 @agent_ppo2.py:179][0m |          -0.0440 |           6.4678 |           7.9626 |
[32m[20221213 12:23:40 @agent_ppo2.py:179][0m |          -0.0496 |           6.4288 |           7.9884 |
[32m[20221213 12:23:40 @agent_ppo2.py:124][0m Policy update time: 1.63 s
[32m[20221213 12:23:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.38
[32m[20221213 12:23:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.24
[32m[20221213 12:23:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 174.60
[32m[20221213 12:23:40 @agent_ppo2.py:137][0m Total time:       7.72 min
[32m[20221213 12:23:40 @agent_ppo2.py:139][0m 522240 total steps have happened
[32m[20221213 12:23:40 @agent_ppo2.py:115][0m #------------------------ Iteration 255 --------------------------#
[32m[20221213 12:23:40 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:41 @agent_ppo2.py:179][0m |           0.0431 |           7.2377 |           7.8633 |
[32m[20221213 12:23:41 @agent_ppo2.py:179][0m |           0.0113 |           7.1036 |           7.9000 |
[32m[20221213 12:23:41 @agent_ppo2.py:179][0m |          -0.0108 |           7.0516 |           7.8967 |
[32m[20221213 12:23:41 @agent_ppo2.py:179][0m |          -0.0172 |           7.5952 |           8.0405 |
[32m[20221213 12:23:41 @agent_ppo2.py:179][0m |          -0.0329 |           7.0345 |           8.0633 |
[32m[20221213 12:23:41 @agent_ppo2.py:179][0m |          -0.0322 |           7.0113 |           8.1084 |
[32m[20221213 12:23:42 @agent_ppo2.py:179][0m |          -0.0393 |           6.9561 |           8.1230 |
[32m[20221213 12:23:42 @agent_ppo2.py:179][0m |          -0.0400 |           6.9502 |           8.1407 |
[32m[20221213 12:23:42 @agent_ppo2.py:179][0m |          -0.0445 |           6.9286 |           8.1484 |
[32m[20221213 12:23:42 @agent_ppo2.py:179][0m |          -0.0482 |           6.9409 |           8.1682 |
[32m[20221213 12:23:42 @agent_ppo2.py:124][0m Policy update time: 1.71 s
[32m[20221213 12:23:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.31
[32m[20221213 12:23:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.03
[32m[20221213 12:23:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 136.72
[32m[20221213 12:23:42 @agent_ppo2.py:137][0m Total time:       7.76 min
[32m[20221213 12:23:42 @agent_ppo2.py:139][0m 524288 total steps have happened
[32m[20221213 12:23:42 @agent_ppo2.py:115][0m #------------------------ Iteration 256 --------------------------#
[32m[20221213 12:23:43 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:43 @agent_ppo2.py:179][0m |           0.0447 |           6.4262 |           8.0381 |
[32m[20221213 12:23:43 @agent_ppo2.py:179][0m |           0.0088 |           6.2396 |           7.9048 |
[32m[20221213 12:23:43 @agent_ppo2.py:179][0m |          -0.0140 |           6.1973 |           8.0105 |
[32m[20221213 12:23:43 @agent_ppo2.py:179][0m |          -0.0295 |           6.1627 |           8.1402 |
[32m[20221213 12:23:43 @agent_ppo2.py:179][0m |          -0.0357 |           6.1347 |           8.1714 |
[32m[20221213 12:23:44 @agent_ppo2.py:179][0m |          -0.0306 |           6.2614 |           8.2308 |
[32m[20221213 12:23:44 @agent_ppo2.py:179][0m |          -0.0435 |           6.2182 |           8.2807 |
[32m[20221213 12:23:44 @agent_ppo2.py:179][0m |          -0.0279 |           7.1280 |           8.2940 |
[32m[20221213 12:23:44 @agent_ppo2.py:179][0m |          -0.0469 |           6.0989 |           8.3217 |
[32m[20221213 12:23:44 @agent_ppo2.py:179][0m |          -0.0406 |           6.4834 |           8.3561 |
[32m[20221213 12:23:44 @agent_ppo2.py:124][0m Policy update time: 1.60 s
[32m[20221213 12:23:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 153.91
[32m[20221213 12:23:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.33
[32m[20221213 12:23:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 160.73
[32m[20221213 12:23:44 @agent_ppo2.py:137][0m Total time:       7.79 min
[32m[20221213 12:23:44 @agent_ppo2.py:139][0m 526336 total steps have happened
[32m[20221213 12:23:44 @agent_ppo2.py:115][0m #------------------------ Iteration 257 --------------------------#
[32m[20221213 12:23:45 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:45 @agent_ppo2.py:179][0m |           0.0516 |           6.7484 |           8.2247 |
[32m[20221213 12:23:45 @agent_ppo2.py:179][0m |           0.0407 |           6.4805 |           7.9025 |
[32m[20221213 12:23:45 @agent_ppo2.py:179][0m |           0.0350 |           6.3750 |           7.7053 |
[32m[20221213 12:23:45 @agent_ppo2.py:179][0m |           0.0025 |           6.7774 |           8.2239 |
[32m[20221213 12:23:46 @agent_ppo2.py:179][0m |          -0.0166 |           6.2964 |           8.2768 |
[32m[20221213 12:23:46 @agent_ppo2.py:179][0m |          -0.0295 |           6.2133 |           8.3718 |
[32m[20221213 12:23:46 @agent_ppo2.py:179][0m |          -0.0365 |           6.1773 |           8.4379 |
[32m[20221213 12:23:46 @agent_ppo2.py:179][0m |          -0.0392 |           6.1184 |           8.4622 |
[32m[20221213 12:23:46 @agent_ppo2.py:179][0m |          -0.0443 |           6.1272 |           8.4997 |
[32m[20221213 12:23:46 @agent_ppo2.py:179][0m |          -0.0446 |           6.0971 |           8.5249 |
[32m[20221213 12:23:46 @agent_ppo2.py:124][0m Policy update time: 1.67 s
[32m[20221213 12:23:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.72
[32m[20221213 12:23:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 162.20
[32m[20221213 12:23:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 138.35
[32m[20221213 12:23:46 @agent_ppo2.py:137][0m Total time:       7.83 min
[32m[20221213 12:23:46 @agent_ppo2.py:139][0m 528384 total steps have happened
[32m[20221213 12:23:46 @agent_ppo2.py:115][0m #------------------------ Iteration 258 --------------------------#
[32m[20221213 12:23:47 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:47 @agent_ppo2.py:179][0m |           0.0454 |           6.9155 |           8.2933 |
[32m[20221213 12:23:47 @agent_ppo2.py:179][0m |           0.0244 |           6.6413 |           8.0957 |
[32m[20221213 12:23:47 @agent_ppo2.py:179][0m |           0.0026 |           7.2895 |           8.3265 |
[32m[20221213 12:23:47 @agent_ppo2.py:179][0m |          -0.0204 |           6.5573 |           8.4010 |
[32m[20221213 12:23:48 @agent_ppo2.py:179][0m |          -0.0307 |           6.5268 |           8.4960 |
[32m[20221213 12:23:48 @agent_ppo2.py:179][0m |          -0.0344 |           6.5290 |           8.4901 |
[32m[20221213 12:23:48 @agent_ppo2.py:179][0m |          -0.0296 |           6.5524 |           8.5031 |
[32m[20221213 12:23:48 @agent_ppo2.py:179][0m |          -0.0363 |           6.4867 |           8.5275 |
[32m[20221213 12:23:48 @agent_ppo2.py:179][0m |          -0.0411 |           6.4803 |           8.5423 |
[32m[20221213 12:23:48 @agent_ppo2.py:179][0m |          -0.0336 |           6.9748 |           8.5450 |
[32m[20221213 12:23:48 @agent_ppo2.py:124][0m Policy update time: 1.62 s
[32m[20221213 12:23:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.91
[32m[20221213 12:23:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 159.57
[32m[20221213 12:23:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.06
[32m[20221213 12:23:49 @agent_ppo2.py:137][0m Total time:       7.86 min
[32m[20221213 12:23:49 @agent_ppo2.py:139][0m 530432 total steps have happened
[32m[20221213 12:23:49 @agent_ppo2.py:115][0m #------------------------ Iteration 259 --------------------------#
[32m[20221213 12:23:49 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:49 @agent_ppo2.py:179][0m |           0.0280 |           7.2537 |           8.4161 |
[32m[20221213 12:23:49 @agent_ppo2.py:179][0m |           0.0235 |           7.8334 |           8.1969 |
[32m[20221213 12:23:49 @agent_ppo2.py:179][0m |          -0.0187 |           7.0668 |           8.4575 |
[32m[20221213 12:23:49 @agent_ppo2.py:179][0m |          -0.0213 |           7.0059 |           8.4746 |
[32m[20221213 12:23:50 @agent_ppo2.py:179][0m |          -0.0299 |           6.9842 |           8.5109 |
[32m[20221213 12:23:50 @agent_ppo2.py:179][0m |          -0.0285 |           6.9677 |           8.5117 |
[32m[20221213 12:23:50 @agent_ppo2.py:179][0m |          -0.0310 |           6.9401 |           8.4974 |
[32m[20221213 12:23:50 @agent_ppo2.py:179][0m |          -0.0321 |           7.4368 |           8.5348 |
[32m[20221213 12:23:50 @agent_ppo2.py:179][0m |          -0.0443 |           6.9356 |           8.5700 |
[32m[20221213 12:23:50 @agent_ppo2.py:179][0m |          -0.0395 |           6.9264 |           8.5543 |
[32m[20221213 12:23:50 @agent_ppo2.py:124][0m Policy update time: 1.60 s
[32m[20221213 12:23:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 155.79
[32m[20221213 12:23:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.11
[32m[20221213 12:23:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 161.90
[32m[20221213 12:23:51 @agent_ppo2.py:137][0m Total time:       7.90 min
[32m[20221213 12:23:51 @agent_ppo2.py:139][0m 532480 total steps have happened
[32m[20221213 12:23:51 @agent_ppo2.py:115][0m #------------------------ Iteration 260 --------------------------#
[32m[20221213 12:23:51 @agent_ppo2.py:121][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 12:23:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:51 @agent_ppo2.py:179][0m |           0.0417 |           6.4461 |           8.5095 |
[32m[20221213 12:23:51 @agent_ppo2.py:179][0m |           0.0231 |           6.3318 |           8.3440 |
[32m[20221213 12:23:51 @agent_ppo2.py:179][0m |           0.0237 |           6.2652 |           8.2369 |
[32m[20221213 12:23:52 @agent_ppo2.py:179][0m |          -0.0211 |           6.2233 |           8.6534 |
[32m[20221213 12:23:52 @agent_ppo2.py:179][0m |          -0.0269 |           6.2433 |           8.7145 |
[32m[20221213 12:23:52 @agent_ppo2.py:179][0m |          -0.0246 |           6.3385 |           8.7334 |
[32m[20221213 12:23:52 @agent_ppo2.py:179][0m |          -0.0381 |           6.1410 |           8.7642 |
[32m[20221213 12:23:52 @agent_ppo2.py:179][0m |          -0.0428 |           6.1446 |           8.7980 |
[32m[20221213 12:23:52 @agent_ppo2.py:179][0m |          -0.0368 |           6.1958 |           8.8004 |
[32m[20221213 12:23:53 @agent_ppo2.py:179][0m |          -0.0415 |           6.1309 |           8.8169 |
[32m[20221213 12:23:53 @agent_ppo2.py:124][0m Policy update time: 1.65 s
[32m[20221213 12:23:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.11
[32m[20221213 12:23:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 153.44
[32m[20221213 12:23:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 187.77
[32m[20221213 12:23:53 @agent_ppo2.py:137][0m Total time:       7.93 min
[32m[20221213 12:23:53 @agent_ppo2.py:139][0m 534528 total steps have happened
[32m[20221213 12:23:53 @agent_ppo2.py:115][0m #------------------------ Iteration 261 --------------------------#
[32m[20221213 12:23:53 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:53 @agent_ppo2.py:179][0m |           0.0467 |           6.5179 |           8.2558 |
[32m[20221213 12:23:53 @agent_ppo2.py:179][0m |           0.0265 |           6.3056 |           8.0283 |
[32m[20221213 12:23:54 @agent_ppo2.py:179][0m |          -0.0076 |           6.3084 |           8.3533 |
[32m[20221213 12:23:54 @agent_ppo2.py:179][0m |          -0.0269 |           6.2549 |           8.4300 |
[32m[20221213 12:23:54 @agent_ppo2.py:179][0m |          -0.0311 |           6.1893 |           8.4791 |
[32m[20221213 12:23:54 @agent_ppo2.py:179][0m |          -0.0335 |           6.1666 |           8.5183 |
[32m[20221213 12:23:54 @agent_ppo2.py:179][0m |          -0.0376 |           6.2825 |           8.5339 |
[32m[20221213 12:23:54 @agent_ppo2.py:179][0m |          -0.0475 |           6.1458 |           8.5567 |
[32m[20221213 12:23:54 @agent_ppo2.py:179][0m |          -0.0460 |           6.1096 |           8.5859 |
[32m[20221213 12:23:55 @agent_ppo2.py:179][0m |          -0.0446 |           6.1930 |           8.5925 |
[32m[20221213 12:23:55 @agent_ppo2.py:124][0m Policy update time: 1.68 s
[32m[20221213 12:23:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.52
[32m[20221213 12:23:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.24
[32m[20221213 12:23:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 173.69
[32m[20221213 12:23:55 @agent_ppo2.py:137][0m Total time:       7.97 min
[32m[20221213 12:23:55 @agent_ppo2.py:139][0m 536576 total steps have happened
[32m[20221213 12:23:55 @agent_ppo2.py:115][0m #------------------------ Iteration 262 --------------------------#
[32m[20221213 12:23:55 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:55 @agent_ppo2.py:179][0m |           0.0220 |           6.5313 |           8.5223 |
[32m[20221213 12:23:55 @agent_ppo2.py:179][0m |          -0.0087 |           6.0560 |           8.5178 |
[32m[20221213 12:23:56 @agent_ppo2.py:179][0m |          -0.0237 |           5.9347 |           8.5432 |
[32m[20221213 12:23:56 @agent_ppo2.py:179][0m |          -0.0304 |           5.8619 |           8.5874 |
[32m[20221213 12:23:56 @agent_ppo2.py:179][0m |          -0.0348 |           5.7892 |           8.6186 |
[32m[20221213 12:23:56 @agent_ppo2.py:179][0m |          -0.0398 |           5.7017 |           8.6127 |
[32m[20221213 12:23:56 @agent_ppo2.py:179][0m |          -0.0413 |           5.6493 |           8.6803 |
[32m[20221213 12:23:56 @agent_ppo2.py:179][0m |          -0.0469 |           5.5883 |           8.7133 |
[32m[20221213 12:23:57 @agent_ppo2.py:179][0m |          -0.0489 |           5.5486 |           8.7195 |
[32m[20221213 12:23:57 @agent_ppo2.py:179][0m |          -0.0517 |           5.5369 |           8.7229 |
[32m[20221213 12:23:57 @agent_ppo2.py:124][0m Policy update time: 1.60 s
[32m[20221213 12:23:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.99
[32m[20221213 12:23:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 174.40
[32m[20221213 12:23:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.26
[32m[20221213 12:23:57 @agent_ppo2.py:137][0m Total time:       8.00 min
[32m[20221213 12:23:57 @agent_ppo2.py:139][0m 538624 total steps have happened
[32m[20221213 12:23:57 @agent_ppo2.py:115][0m #------------------------ Iteration 263 --------------------------#
[32m[20221213 12:23:57 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:23:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:57 @agent_ppo2.py:179][0m |           0.0452 |           7.1765 |           8.4765 |
[32m[20221213 12:23:58 @agent_ppo2.py:179][0m |           0.0117 |           7.0267 |           8.5218 |
[32m[20221213 12:23:58 @agent_ppo2.py:179][0m |          -0.0124 |           7.0924 |           8.6423 |
[32m[20221213 12:23:58 @agent_ppo2.py:179][0m |          -0.0277 |           6.8754 |           8.7504 |
[32m[20221213 12:23:58 @agent_ppo2.py:179][0m |          -0.0349 |           6.8104 |           8.8222 |
[32m[20221213 12:23:58 @agent_ppo2.py:179][0m |          -0.0388 |           6.8236 |           8.8330 |
[32m[20221213 12:23:58 @agent_ppo2.py:179][0m |          -0.0396 |           6.7519 |           8.8668 |
[32m[20221213 12:23:58 @agent_ppo2.py:179][0m |          -0.0447 |           6.7173 |           8.8954 |
[32m[20221213 12:23:59 @agent_ppo2.py:179][0m |          -0.0427 |           6.6968 |           8.9006 |
[32m[20221213 12:23:59 @agent_ppo2.py:179][0m |          -0.0451 |           6.7352 |           8.9422 |
[32m[20221213 12:23:59 @agent_ppo2.py:124][0m Policy update time: 1.52 s
[32m[20221213 12:23:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.88
[32m[20221213 12:23:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.54
[32m[20221213 12:23:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 168.25
[32m[20221213 12:23:59 @agent_ppo2.py:137][0m Total time:       8.04 min
[32m[20221213 12:23:59 @agent_ppo2.py:139][0m 540672 total steps have happened
[32m[20221213 12:23:59 @agent_ppo2.py:115][0m #------------------------ Iteration 264 --------------------------#
[32m[20221213 12:23:59 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:23:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:23:59 @agent_ppo2.py:179][0m |           0.0672 |           7.6241 |           8.6843 |
[32m[20221213 12:24:00 @agent_ppo2.py:179][0m |           0.0495 |           7.0846 |           8.3603 |
[32m[20221213 12:24:00 @agent_ppo2.py:179][0m |          -0.0024 |           6.7374 |           8.5429 |
[32m[20221213 12:24:00 @agent_ppo2.py:179][0m |          -0.0240 |           6.5071 |           8.7067 |
[32m[20221213 12:24:00 @agent_ppo2.py:179][0m |          -0.0336 |           6.4016 |           8.8750 |
[32m[20221213 12:24:00 @agent_ppo2.py:179][0m |          -0.0308 |           7.0548 |           8.9181 |
[32m[20221213 12:24:00 @agent_ppo2.py:179][0m |          -0.0438 |           6.3167 |           8.9655 |
[32m[20221213 12:24:00 @agent_ppo2.py:179][0m |          -0.0482 |           6.1922 |           8.9815 |
[32m[20221213 12:24:01 @agent_ppo2.py:179][0m |          -0.0522 |           6.1712 |           9.0013 |
[32m[20221213 12:24:01 @agent_ppo2.py:179][0m |          -0.0431 |           6.7292 |           9.0430 |
[32m[20221213 12:24:01 @agent_ppo2.py:124][0m Policy update time: 1.54 s
[32m[20221213 12:24:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.31
[32m[20221213 12:24:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.26
[32m[20221213 12:24:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 178.94
[32m[20221213 12:24:01 @agent_ppo2.py:137][0m Total time:       8.07 min
[32m[20221213 12:24:01 @agent_ppo2.py:139][0m 542720 total steps have happened
[32m[20221213 12:24:01 @agent_ppo2.py:115][0m #------------------------ Iteration 265 --------------------------#
[32m[20221213 12:24:01 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:01 @agent_ppo2.py:179][0m |           0.0473 |           8.2240 |           8.9084 |
[32m[20221213 12:24:02 @agent_ppo2.py:179][0m |           0.0282 |           8.3851 |           8.6396 |
[32m[20221213 12:24:02 @agent_ppo2.py:179][0m |          -0.0053 |           7.7929 |           8.8363 |
[32m[20221213 12:24:02 @agent_ppo2.py:179][0m |          -0.0179 |           7.7347 |           8.9636 |
[32m[20221213 12:24:02 @agent_ppo2.py:179][0m |          -0.0273 |           7.6875 |           9.0010 |
[32m[20221213 12:24:02 @agent_ppo2.py:179][0m |          -0.0342 |           7.6423 |           9.0539 |
[32m[20221213 12:24:02 @agent_ppo2.py:179][0m |          -0.0392 |           7.5971 |           9.0869 |
[32m[20221213 12:24:02 @agent_ppo2.py:179][0m |          -0.0394 |           7.5968 |           9.1112 |
[32m[20221213 12:24:03 @agent_ppo2.py:179][0m |          -0.0447 |           7.5970 |           9.1375 |
[32m[20221213 12:24:03 @agent_ppo2.py:179][0m |          -0.0466 |           7.5923 |           9.1531 |
[32m[20221213 12:24:03 @agent_ppo2.py:124][0m Policy update time: 1.57 s
[32m[20221213 12:24:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 161.39
[32m[20221213 12:24:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 167.71
[32m[20221213 12:24:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.85
[32m[20221213 12:24:03 @agent_ppo2.py:137][0m Total time:       8.10 min
[32m[20221213 12:24:03 @agent_ppo2.py:139][0m 544768 total steps have happened
[32m[20221213 12:24:03 @agent_ppo2.py:115][0m #------------------------ Iteration 266 --------------------------#
[32m[20221213 12:24:03 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:03 @agent_ppo2.py:179][0m |           0.0456 |           7.3418 |           8.9664 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |           0.0096 |           7.0857 |           8.8278 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |          -0.0163 |           6.9497 |           9.0374 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |          -0.0309 |           6.8975 |           9.1048 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |          -0.0323 |           6.8569 |           9.1583 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |          -0.0312 |           7.5132 |           9.2192 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |          -0.0427 |           6.8102 |           9.2356 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |          -0.0487 |           6.7287 |           9.2709 |
[32m[20221213 12:24:04 @agent_ppo2.py:179][0m |          -0.0457 |           6.7072 |           9.2709 |
[32m[20221213 12:24:05 @agent_ppo2.py:179][0m |          -0.0487 |           6.7235 |           9.2840 |
[32m[20221213 12:24:05 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:24:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 169.75
[32m[20221213 12:24:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 180.91
[32m[20221213 12:24:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 149.86
[32m[20221213 12:24:05 @agent_ppo2.py:137][0m Total time:       8.14 min
[32m[20221213 12:24:05 @agent_ppo2.py:139][0m 546816 total steps have happened
[32m[20221213 12:24:05 @agent_ppo2.py:115][0m #------------------------ Iteration 267 --------------------------#
[32m[20221213 12:24:05 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:05 @agent_ppo2.py:179][0m |           0.0505 |           8.0581 |           9.0865 |
[32m[20221213 12:24:05 @agent_ppo2.py:179][0m |           0.0107 |           7.7845 |           8.9696 |
[32m[20221213 12:24:06 @agent_ppo2.py:179][0m |          -0.0184 |           7.7361 |           9.2335 |
[32m[20221213 12:24:06 @agent_ppo2.py:179][0m |          -0.0331 |           7.6452 |           9.3634 |
[32m[20221213 12:24:06 @agent_ppo2.py:179][0m |          -0.0362 |           7.6211 |           9.4067 |
[32m[20221213 12:24:06 @agent_ppo2.py:179][0m |          -0.0447 |           7.6394 |           9.4073 |
[32m[20221213 12:24:06 @agent_ppo2.py:179][0m |          -0.0481 |           7.5624 |           9.4469 |
[32m[20221213 12:24:06 @agent_ppo2.py:179][0m |          -0.0464 |           7.5505 |           9.4580 |
[32m[20221213 12:24:06 @agent_ppo2.py:179][0m |          -0.0440 |           7.5092 |           9.4238 |
[32m[20221213 12:24:07 @agent_ppo2.py:179][0m |          -0.0465 |           7.5138 |           9.4318 |
[32m[20221213 12:24:07 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:24:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.62
[32m[20221213 12:24:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.82
[32m[20221213 12:24:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 196.39
[32m[20221213 12:24:07 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 196.39
[32m[20221213 12:24:07 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 196.39
[32m[20221213 12:24:07 @agent_ppo2.py:137][0m Total time:       8.17 min
[32m[20221213 12:24:07 @agent_ppo2.py:139][0m 548864 total steps have happened
[32m[20221213 12:24:07 @agent_ppo2.py:115][0m #------------------------ Iteration 268 --------------------------#
[32m[20221213 12:24:07 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:07 @agent_ppo2.py:179][0m |           0.0281 |           7.6769 |           9.6218 |
[32m[20221213 12:24:07 @agent_ppo2.py:179][0m |           0.0069 |           7.5070 |           9.5784 |
[32m[20221213 12:24:07 @agent_ppo2.py:179][0m |          -0.0051 |           8.5443 |           9.7233 |
[32m[20221213 12:24:08 @agent_ppo2.py:179][0m |          -0.0124 |           7.3369 |           9.4929 |
[32m[20221213 12:24:08 @agent_ppo2.py:179][0m |          -0.0286 |           7.3052 |           9.7268 |
[32m[20221213 12:24:08 @agent_ppo2.py:179][0m |          -0.0394 |           7.2031 |           9.7930 |
[32m[20221213 12:24:08 @agent_ppo2.py:179][0m |          -0.0485 |           7.1759 |           9.8476 |
[32m[20221213 12:24:08 @agent_ppo2.py:179][0m |          -0.0416 |           7.1672 |           9.8336 |
[32m[20221213 12:24:08 @agent_ppo2.py:179][0m |          -0.0472 |           7.1101 |           9.8827 |
[32m[20221213 12:24:08 @agent_ppo2.py:179][0m |          -0.0453 |           7.5069 |           9.8957 |
[32m[20221213 12:24:08 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:24:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.39
[32m[20221213 12:24:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 172.45
[32m[20221213 12:24:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 176.28
[32m[20221213 12:24:09 @agent_ppo2.py:137][0m Total time:       8.20 min
[32m[20221213 12:24:09 @agent_ppo2.py:139][0m 550912 total steps have happened
[32m[20221213 12:24:09 @agent_ppo2.py:115][0m #------------------------ Iteration 269 --------------------------#
[32m[20221213 12:24:09 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:09 @agent_ppo2.py:179][0m |           0.0410 |           8.3798 |           9.2438 |
[32m[20221213 12:24:09 @agent_ppo2.py:179][0m |           0.0097 |           8.2160 |           9.2220 |
[32m[20221213 12:24:09 @agent_ppo2.py:179][0m |          -0.0166 |           8.1574 |           9.4067 |
[32m[20221213 12:24:09 @agent_ppo2.py:179][0m |          -0.0219 |           8.1442 |           9.4370 |
[32m[20221213 12:24:10 @agent_ppo2.py:179][0m |          -0.0310 |           8.1179 |           9.4661 |
[32m[20221213 12:24:10 @agent_ppo2.py:179][0m |          -0.0317 |           8.1248 |           9.5186 |
[32m[20221213 12:24:10 @agent_ppo2.py:179][0m |          -0.0404 |           8.1077 |           9.5669 |
[32m[20221213 12:24:10 @agent_ppo2.py:179][0m |          -0.0347 |           8.2408 |           9.6144 |
[32m[20221213 12:24:10 @agent_ppo2.py:179][0m |          -0.0414 |           8.0797 |           9.5920 |
[32m[20221213 12:24:10 @agent_ppo2.py:179][0m |          -0.0376 |           8.1422 |           9.5627 |
[32m[20221213 12:24:10 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:24:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.06
[32m[20221213 12:24:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 182.44
[32m[20221213 12:24:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 208.19
[32m[20221213 12:24:11 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 208.19
[32m[20221213 12:24:11 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 208.19
[32m[20221213 12:24:11 @agent_ppo2.py:137][0m Total time:       8.23 min
[32m[20221213 12:24:11 @agent_ppo2.py:139][0m 552960 total steps have happened
[32m[20221213 12:24:11 @agent_ppo2.py:115][0m #------------------------ Iteration 270 --------------------------#
[32m[20221213 12:24:11 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:24:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:11 @agent_ppo2.py:179][0m |           0.0613 |           8.0774 |           9.5131 |
[32m[20221213 12:24:11 @agent_ppo2.py:179][0m |           0.0092 |           7.2612 |           9.3943 |
[32m[20221213 12:24:11 @agent_ppo2.py:179][0m |          -0.0132 |           7.1352 |           9.6268 |
[32m[20221213 12:24:11 @agent_ppo2.py:179][0m |          -0.0304 |           7.0596 |           9.7634 |
[32m[20221213 12:24:12 @agent_ppo2.py:179][0m |          -0.0380 |           7.0229 |           9.8676 |
[32m[20221213 12:24:12 @agent_ppo2.py:179][0m |          -0.0368 |           7.0216 |           9.8970 |
[32m[20221213 12:24:12 @agent_ppo2.py:179][0m |          -0.0423 |           6.9418 |           9.9113 |
[32m[20221213 12:24:12 @agent_ppo2.py:179][0m |          -0.0446 |           6.9923 |           9.9406 |
[32m[20221213 12:24:12 @agent_ppo2.py:179][0m |          -0.0484 |           6.8779 |           9.9456 |
[32m[20221213 12:24:12 @agent_ppo2.py:179][0m |          -0.0471 |           6.8223 |           9.9744 |
[32m[20221213 12:24:12 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:24:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.63
[32m[20221213 12:24:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.18
[32m[20221213 12:24:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 185.96
[32m[20221213 12:24:12 @agent_ppo2.py:137][0m Total time:       8.26 min
[32m[20221213 12:24:12 @agent_ppo2.py:139][0m 555008 total steps have happened
[32m[20221213 12:24:12 @agent_ppo2.py:115][0m #------------------------ Iteration 271 --------------------------#
[32m[20221213 12:24:13 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:13 @agent_ppo2.py:179][0m |           0.0591 |           8.9550 |           9.7031 |
[32m[20221213 12:24:13 @agent_ppo2.py:179][0m |           0.0532 |           8.6661 |           9.3829 |
[32m[20221213 12:24:13 @agent_ppo2.py:179][0m |           0.0067 |           8.5625 |           9.8114 |
[32m[20221213 12:24:13 @agent_ppo2.py:179][0m |          -0.0193 |           8.5225 |           9.9710 |
[32m[20221213 12:24:13 @agent_ppo2.py:179][0m |          -0.0189 |           9.6330 |          10.0308 |
[32m[20221213 12:24:14 @agent_ppo2.py:179][0m |          -0.0377 |           8.4503 |          10.1031 |
[32m[20221213 12:24:14 @agent_ppo2.py:179][0m |          -0.0203 |           9.8306 |          10.1202 |
[32m[20221213 12:24:14 @agent_ppo2.py:179][0m |          -0.0302 |           8.4181 |           9.9357 |
[32m[20221213 12:24:14 @agent_ppo2.py:179][0m |          -0.0477 |           8.3198 |          10.1454 |
[32m[20221213 12:24:14 @agent_ppo2.py:179][0m |          -0.0513 |           8.3213 |          10.1624 |
[32m[20221213 12:24:14 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:24:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.15
[32m[20221213 12:24:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 199.74
[32m[20221213 12:24:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 182.70
[32m[20221213 12:24:14 @agent_ppo2.py:137][0m Total time:       8.29 min
[32m[20221213 12:24:14 @agent_ppo2.py:139][0m 557056 total steps have happened
[32m[20221213 12:24:14 @agent_ppo2.py:115][0m #------------------------ Iteration 272 --------------------------#
[32m[20221213 12:24:14 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:15 @agent_ppo2.py:179][0m |           0.0330 |           8.6077 |           9.8514 |
[32m[20221213 12:24:15 @agent_ppo2.py:179][0m |           0.0048 |           8.3478 |           9.7607 |
[32m[20221213 12:24:15 @agent_ppo2.py:179][0m |          -0.0181 |           8.3116 |           9.9131 |
[32m[20221213 12:24:15 @agent_ppo2.py:179][0m |          -0.0239 |           8.3033 |           9.9854 |
[32m[20221213 12:24:15 @agent_ppo2.py:179][0m |          -0.0335 |           8.2569 |          10.0229 |
[32m[20221213 12:24:15 @agent_ppo2.py:179][0m |          -0.0383 |           8.2353 |          10.0502 |
[32m[20221213 12:24:16 @agent_ppo2.py:179][0m |          -0.0321 |           8.7330 |          10.0801 |
[32m[20221213 12:24:16 @agent_ppo2.py:179][0m |          -0.0444 |           8.5523 |          10.0689 |
[32m[20221213 12:24:16 @agent_ppo2.py:179][0m |          -0.0437 |           8.4267 |          10.1053 |
[32m[20221213 12:24:16 @agent_ppo2.py:179][0m |          -0.0507 |           8.2239 |          10.1053 |
[32m[20221213 12:24:16 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:24:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 164.15
[32m[20221213 12:24:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.93
[32m[20221213 12:24:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.49
[32m[20221213 12:24:16 @agent_ppo2.py:137][0m Total time:       8.32 min
[32m[20221213 12:24:16 @agent_ppo2.py:139][0m 559104 total steps have happened
[32m[20221213 12:24:16 @agent_ppo2.py:115][0m #------------------------ Iteration 273 --------------------------#
[32m[20221213 12:24:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:24:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:17 @agent_ppo2.py:179][0m |           0.0476 |           8.0413 |           9.6088 |
[32m[20221213 12:24:17 @agent_ppo2.py:179][0m |           0.0148 |           7.7582 |           9.5793 |
[32m[20221213 12:24:17 @agent_ppo2.py:179][0m |          -0.0068 |           7.7536 |           9.8306 |
[32m[20221213 12:24:17 @agent_ppo2.py:179][0m |          -0.0161 |           7.6679 |           9.8779 |
[32m[20221213 12:24:17 @agent_ppo2.py:179][0m |          -0.0273 |           7.6803 |          10.0044 |
[32m[20221213 12:24:17 @agent_ppo2.py:179][0m |          -0.0374 |           7.6525 |          10.0296 |
[32m[20221213 12:24:17 @agent_ppo2.py:179][0m |          -0.0360 |           7.6383 |          10.0206 |
[32m[20221213 12:24:18 @agent_ppo2.py:179][0m |          -0.0361 |           7.6101 |          10.0676 |
[32m[20221213 12:24:18 @agent_ppo2.py:179][0m |          -0.0389 |           7.6065 |          10.0774 |
[32m[20221213 12:24:18 @agent_ppo2.py:179][0m |          -0.0447 |           7.5851 |          10.0972 |
[32m[20221213 12:24:18 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:24:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 180.49
[32m[20221213 12:24:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.44
[32m[20221213 12:24:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 124.32
[32m[20221213 12:24:18 @agent_ppo2.py:137][0m Total time:       8.36 min
[32m[20221213 12:24:18 @agent_ppo2.py:139][0m 561152 total steps have happened
[32m[20221213 12:24:18 @agent_ppo2.py:115][0m #------------------------ Iteration 274 --------------------------#
[32m[20221213 12:24:18 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |           0.0470 |           8.3045 |           9.8201 |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |           0.0054 |           8.1112 |           9.7178 |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |          -0.0159 |           8.0016 |           9.9461 |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |          -0.0270 |           7.9210 |          10.0310 |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |          -0.0342 |           7.8499 |          10.0513 |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |          -0.0409 |           7.7824 |          10.1044 |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |          -0.0392 |           7.7516 |          10.1066 |
[32m[20221213 12:24:19 @agent_ppo2.py:179][0m |          -0.0428 |           7.6766 |          10.0839 |
[32m[20221213 12:24:20 @agent_ppo2.py:179][0m |          -0.0501 |           7.6273 |          10.1358 |
[32m[20221213 12:24:20 @agent_ppo2.py:179][0m |          -0.0529 |           7.5830 |          10.1624 |
[32m[20221213 12:24:20 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:24:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.00
[32m[20221213 12:24:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 176.04
[32m[20221213 12:24:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 189.44
[32m[20221213 12:24:20 @agent_ppo2.py:137][0m Total time:       8.39 min
[32m[20221213 12:24:20 @agent_ppo2.py:139][0m 563200 total steps have happened
[32m[20221213 12:24:20 @agent_ppo2.py:115][0m #------------------------ Iteration 275 --------------------------#
[32m[20221213 12:24:20 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:20 @agent_ppo2.py:179][0m |           0.0401 |           8.7646 |           9.9891 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |           0.0338 |           8.4725 |           9.4669 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |          -0.0116 |           8.3676 |           9.8999 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |          -0.0279 |           8.3329 |          10.0394 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |          -0.0343 |           8.2943 |          10.1226 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |          -0.0352 |           8.2794 |          10.1052 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |          -0.0433 |           8.2791 |          10.1508 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |          -0.0443 |           8.2189 |          10.1808 |
[32m[20221213 12:24:21 @agent_ppo2.py:179][0m |          -0.0466 |           8.2501 |          10.1757 |
[32m[20221213 12:24:22 @agent_ppo2.py:179][0m |          -0.0429 |           8.1991 |          10.1617 |
[32m[20221213 12:24:22 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:24:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.68
[32m[20221213 12:24:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 173.23
[32m[20221213 12:24:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.22
[32m[20221213 12:24:22 @agent_ppo2.py:137][0m Total time:       8.42 min
[32m[20221213 12:24:22 @agent_ppo2.py:139][0m 565248 total steps have happened
[32m[20221213 12:24:22 @agent_ppo2.py:115][0m #------------------------ Iteration 276 --------------------------#
[32m[20221213 12:24:22 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:22 @agent_ppo2.py:179][0m |           0.0407 |           8.2424 |          10.0644 |
[32m[20221213 12:24:22 @agent_ppo2.py:179][0m |           0.0060 |           7.8824 |          10.0810 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0102 |           8.2773 |          10.2387 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0264 |           7.8015 |          10.2431 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0323 |           7.7084 |          10.2957 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0349 |           7.6824 |          10.2979 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0344 |           7.7734 |          10.3450 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0412 |           7.6466 |          10.3466 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0450 |           7.6169 |          10.3469 |
[32m[20221213 12:24:23 @agent_ppo2.py:179][0m |          -0.0409 |           7.5795 |          10.3518 |
[32m[20221213 12:24:23 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:24:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 160.06
[32m[20221213 12:24:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 170.34
[32m[20221213 12:24:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 184.77
[32m[20221213 12:24:24 @agent_ppo2.py:137][0m Total time:       8.45 min
[32m[20221213 12:24:24 @agent_ppo2.py:139][0m 567296 total steps have happened
[32m[20221213 12:24:24 @agent_ppo2.py:115][0m #------------------------ Iteration 277 --------------------------#
[32m[20221213 12:24:24 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:24 @agent_ppo2.py:179][0m |           0.0365 |           8.1816 |           9.9050 |
[32m[20221213 12:24:24 @agent_ppo2.py:179][0m |           0.0232 |           8.0407 |           9.8003 |
[32m[20221213 12:24:24 @agent_ppo2.py:179][0m |          -0.0113 |           8.0204 |          10.0744 |
[32m[20221213 12:24:25 @agent_ppo2.py:179][0m |          -0.0207 |           7.9664 |          10.1643 |
[32m[20221213 12:24:25 @agent_ppo2.py:179][0m |          -0.0222 |           8.4441 |          10.2705 |
[32m[20221213 12:24:25 @agent_ppo2.py:179][0m |          -0.0311 |           7.9159 |          10.2738 |
[32m[20221213 12:24:25 @agent_ppo2.py:179][0m |          -0.0356 |           7.8913 |          10.3017 |
[32m[20221213 12:24:25 @agent_ppo2.py:179][0m |          -0.0415 |           7.8866 |          10.3482 |
[32m[20221213 12:24:25 @agent_ppo2.py:179][0m |          -0.0427 |           7.8926 |          10.3620 |
[32m[20221213 12:24:25 @agent_ppo2.py:179][0m |          -0.0360 |           7.8636 |          10.2750 |
[32m[20221213 12:24:25 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:24:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.66
[32m[20221213 12:24:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 187.35
[32m[20221213 12:24:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.92
[32m[20221213 12:24:26 @agent_ppo2.py:137][0m Total time:       8.48 min
[32m[20221213 12:24:26 @agent_ppo2.py:139][0m 569344 total steps have happened
[32m[20221213 12:24:26 @agent_ppo2.py:115][0m #------------------------ Iteration 278 --------------------------#
[32m[20221213 12:24:26 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:26 @agent_ppo2.py:179][0m |           0.0289 |           8.2248 |          10.2760 |
[32m[20221213 12:24:26 @agent_ppo2.py:179][0m |           0.0121 |           7.9832 |          10.0670 |
[32m[20221213 12:24:26 @agent_ppo2.py:179][0m |          -0.0131 |           7.7956 |          10.2889 |
[32m[20221213 12:24:26 @agent_ppo2.py:179][0m |          -0.0149 |           7.6994 |          10.3089 |
[32m[20221213 12:24:27 @agent_ppo2.py:179][0m |          -0.0253 |           7.5928 |          10.3754 |
[32m[20221213 12:24:27 @agent_ppo2.py:179][0m |          -0.0306 |           7.4531 |          10.3959 |
[32m[20221213 12:24:27 @agent_ppo2.py:179][0m |          -0.0344 |           7.3396 |          10.4321 |
[32m[20221213 12:24:27 @agent_ppo2.py:179][0m |          -0.0392 |           7.2300 |          10.4809 |
[32m[20221213 12:24:27 @agent_ppo2.py:179][0m |          -0.0345 |           7.2305 |          10.4595 |
[32m[20221213 12:24:27 @agent_ppo2.py:179][0m |          -0.0344 |           7.1017 |          10.4550 |
[32m[20221213 12:24:27 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:24:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.03
[32m[20221213 12:24:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 181.83
[32m[20221213 12:24:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 201.69
[32m[20221213 12:24:28 @agent_ppo2.py:137][0m Total time:       8.51 min
[32m[20221213 12:24:28 @agent_ppo2.py:139][0m 571392 total steps have happened
[32m[20221213 12:24:28 @agent_ppo2.py:115][0m #------------------------ Iteration 279 --------------------------#
[32m[20221213 12:24:28 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:28 @agent_ppo2.py:179][0m |           0.0517 |           8.3259 |          10.2337 |
[32m[20221213 12:24:28 @agent_ppo2.py:179][0m |           0.0220 |           7.9139 |           9.9652 |
[32m[20221213 12:24:28 @agent_ppo2.py:179][0m |          -0.0125 |           7.8971 |          10.4058 |
[32m[20221213 12:24:28 @agent_ppo2.py:179][0m |          -0.0257 |           7.7900 |          10.5068 |
[32m[20221213 12:24:28 @agent_ppo2.py:179][0m |          -0.0299 |           7.7622 |          10.5338 |
[32m[20221213 12:24:29 @agent_ppo2.py:179][0m |          -0.0305 |           7.6664 |          10.5556 |
[32m[20221213 12:24:29 @agent_ppo2.py:179][0m |          -0.0352 |           7.7276 |          10.5647 |
[32m[20221213 12:24:29 @agent_ppo2.py:179][0m |          -0.0399 |           7.6464 |          10.6378 |
[32m[20221213 12:24:29 @agent_ppo2.py:179][0m |          -0.0371 |           7.5255 |          10.5893 |
[32m[20221213 12:24:29 @agent_ppo2.py:179][0m |          -0.0415 |           7.5174 |          10.6443 |
[32m[20221213 12:24:29 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:24:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 173.03
[32m[20221213 12:24:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.78
[32m[20221213 12:24:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 194.28
[32m[20221213 12:24:29 @agent_ppo2.py:137][0m Total time:       8.54 min
[32m[20221213 12:24:29 @agent_ppo2.py:139][0m 573440 total steps have happened
[32m[20221213 12:24:29 @agent_ppo2.py:115][0m #------------------------ Iteration 280 --------------------------#
[32m[20221213 12:24:30 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:24:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:30 @agent_ppo2.py:179][0m |           0.0312 |           8.3205 |          10.6818 |
[32m[20221213 12:24:30 @agent_ppo2.py:179][0m |           0.0097 |           7.8193 |          10.4703 |
[32m[20221213 12:24:30 @agent_ppo2.py:179][0m |          -0.0102 |           7.6879 |          10.5326 |
[32m[20221213 12:24:30 @agent_ppo2.py:179][0m |          -0.0258 |           7.6619 |          10.6598 |
[32m[20221213 12:24:30 @agent_ppo2.py:179][0m |          -0.0325 |           7.5878 |          10.6921 |
[32m[20221213 12:24:31 @agent_ppo2.py:179][0m |          -0.0346 |           7.5410 |          10.6868 |
[32m[20221213 12:24:31 @agent_ppo2.py:179][0m |          -0.0415 |           7.5285 |          10.7180 |
[32m[20221213 12:24:31 @agent_ppo2.py:179][0m |          -0.0454 |           7.4621 |          10.7556 |
[32m[20221213 12:24:31 @agent_ppo2.py:179][0m |          -0.0463 |           7.4598 |          10.7851 |
[32m[20221213 12:24:31 @agent_ppo2.py:179][0m |          -0.0490 |           7.4229 |          10.7773 |
[32m[20221213 12:24:31 @agent_ppo2.py:124][0m Policy update time: 1.52 s
[32m[20221213 12:24:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 160.99
[32m[20221213 12:24:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 178.53
[32m[20221213 12:24:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 203.57
[32m[20221213 12:24:31 @agent_ppo2.py:137][0m Total time:       8.58 min
[32m[20221213 12:24:31 @agent_ppo2.py:139][0m 575488 total steps have happened
[32m[20221213 12:24:31 @agent_ppo2.py:115][0m #------------------------ Iteration 281 --------------------------#
[32m[20221213 12:24:32 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:32 @agent_ppo2.py:179][0m |           0.0376 |           8.2034 |          10.6546 |
[32m[20221213 12:24:32 @agent_ppo2.py:179][0m |           0.0054 |           7.7305 |          10.6494 |
[32m[20221213 12:24:32 @agent_ppo2.py:179][0m |          -0.0168 |           7.6145 |          10.7641 |
[32m[20221213 12:24:32 @agent_ppo2.py:179][0m |          -0.0276 |           7.5346 |          10.8341 |
[32m[20221213 12:24:32 @agent_ppo2.py:179][0m |          -0.0298 |           7.5718 |          10.8486 |
[32m[20221213 12:24:32 @agent_ppo2.py:179][0m |          -0.0352 |           7.4454 |          10.8582 |
[32m[20221213 12:24:33 @agent_ppo2.py:179][0m |          -0.0368 |           7.5924 |          10.8919 |
[32m[20221213 12:24:33 @agent_ppo2.py:179][0m |          -0.0288 |           7.4179 |          10.7923 |
[32m[20221213 12:24:33 @agent_ppo2.py:179][0m |          -0.0433 |           7.3833 |          10.8779 |
[32m[20221213 12:24:33 @agent_ppo2.py:179][0m |          -0.0514 |           7.3462 |          10.9344 |
[32m[20221213 12:24:33 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:24:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.27
[32m[20221213 12:24:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.01
[32m[20221213 12:24:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 211.04
[32m[20221213 12:24:33 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 211.04
[32m[20221213 12:24:33 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 211.04
[32m[20221213 12:24:33 @agent_ppo2.py:137][0m Total time:       8.61 min
[32m[20221213 12:24:33 @agent_ppo2.py:139][0m 577536 total steps have happened
[32m[20221213 12:24:33 @agent_ppo2.py:115][0m #------------------------ Iteration 282 --------------------------#
[32m[20221213 12:24:33 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:34 @agent_ppo2.py:179][0m |           0.0400 |           8.5963 |          10.0841 |
[32m[20221213 12:24:34 @agent_ppo2.py:179][0m |           0.0341 |           8.4343 |           9.7107 |
[32m[20221213 12:24:34 @agent_ppo2.py:179][0m |          -0.0070 |           8.3892 |          10.2853 |
[32m[20221213 12:24:34 @agent_ppo2.py:179][0m |          -0.0226 |           8.3895 |          10.5259 |
[32m[20221213 12:24:34 @agent_ppo2.py:179][0m |          -0.0261 |           8.3301 |          10.5483 |
[32m[20221213 12:24:34 @agent_ppo2.py:179][0m |          -0.0263 |           8.3149 |          10.5462 |
[32m[20221213 12:24:34 @agent_ppo2.py:179][0m |          -0.0256 |           9.0373 |          10.6090 |
[32m[20221213 12:24:35 @agent_ppo2.py:179][0m |          -0.0392 |           8.3367 |          10.6458 |
[32m[20221213 12:24:35 @agent_ppo2.py:179][0m |          -0.0387 |           8.2992 |          10.6409 |
[32m[20221213 12:24:35 @agent_ppo2.py:179][0m |          -0.0404 |           8.2666 |          10.6481 |
[32m[20221213 12:24:35 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:24:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.82
[32m[20221213 12:24:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.79
[32m[20221213 12:24:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 211.84
[32m[20221213 12:24:35 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 211.84
[32m[20221213 12:24:35 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 211.84
[32m[20221213 12:24:35 @agent_ppo2.py:137][0m Total time:       8.64 min
[32m[20221213 12:24:35 @agent_ppo2.py:139][0m 579584 total steps have happened
[32m[20221213 12:24:35 @agent_ppo2.py:115][0m #------------------------ Iteration 283 --------------------------#
[32m[20221213 12:24:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:24:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:35 @agent_ppo2.py:179][0m |           0.0447 |           8.4567 |          10.4788 |
[32m[20221213 12:24:36 @agent_ppo2.py:179][0m |           0.0446 |           8.3826 |          10.2636 |
[32m[20221213 12:24:36 @agent_ppo2.py:179][0m |          -0.0028 |           8.4477 |          10.5893 |
[32m[20221213 12:24:36 @agent_ppo2.py:179][0m |          -0.0181 |           8.2524 |          10.6931 |
[32m[20221213 12:24:36 @agent_ppo2.py:179][0m |          -0.0275 |           8.1650 |          10.7685 |
[32m[20221213 12:24:36 @agent_ppo2.py:179][0m |          -0.0271 |           8.3560 |          10.8252 |
[32m[20221213 12:24:36 @agent_ppo2.py:179][0m |          -0.0239 |           9.0770 |          10.8251 |
[32m[20221213 12:24:36 @agent_ppo2.py:179][0m |          -0.0350 |           8.1387 |          10.8555 |
[32m[20221213 12:24:37 @agent_ppo2.py:179][0m |          -0.0362 |           8.1266 |          10.8536 |
[32m[20221213 12:24:37 @agent_ppo2.py:179][0m |          -0.0405 |           8.0561 |          10.8814 |
[32m[20221213 12:24:37 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:24:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.87
[32m[20221213 12:24:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 179.85
[32m[20221213 12:24:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 199.97
[32m[20221213 12:24:37 @agent_ppo2.py:137][0m Total time:       8.67 min
[32m[20221213 12:24:37 @agent_ppo2.py:139][0m 581632 total steps have happened
[32m[20221213 12:24:37 @agent_ppo2.py:115][0m #------------------------ Iteration 284 --------------------------#
[32m[20221213 12:24:37 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:37 @agent_ppo2.py:179][0m |           0.0486 |           8.4507 |          10.3330 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |           0.0154 |           8.2258 |          10.1900 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |          -0.0056 |           8.1120 |          10.5305 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |          -0.0229 |           8.0476 |          10.6553 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |          -0.0289 |           7.9643 |          10.7496 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |          -0.0337 |           8.0137 |          10.7549 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |          -0.0402 |           7.9174 |          10.8209 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |          -0.0360 |           7.8720 |          10.8017 |
[32m[20221213 12:24:38 @agent_ppo2.py:179][0m |          -0.0425 |           7.8195 |          10.8458 |
[32m[20221213 12:24:39 @agent_ppo2.py:179][0m |          -0.0242 |           8.7632 |          10.8845 |
[32m[20221213 12:24:39 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:24:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.16
[32m[20221213 12:24:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.65
[32m[20221213 12:24:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 214.73
[32m[20221213 12:24:39 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 214.73
[32m[20221213 12:24:39 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 214.73
[32m[20221213 12:24:39 @agent_ppo2.py:137][0m Total time:       8.70 min
[32m[20221213 12:24:39 @agent_ppo2.py:139][0m 583680 total steps have happened
[32m[20221213 12:24:39 @agent_ppo2.py:115][0m #------------------------ Iteration 285 --------------------------#
[32m[20221213 12:24:39 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:24:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:39 @agent_ppo2.py:179][0m |           0.0608 |           9.0288 |          10.1522 |
[32m[20221213 12:24:39 @agent_ppo2.py:179][0m |           0.0521 |           9.6576 |           9.9723 |
[32m[20221213 12:24:40 @agent_ppo2.py:179][0m |           0.0165 |           8.7163 |          10.4189 |
[32m[20221213 12:24:40 @agent_ppo2.py:179][0m |          -0.0104 |           8.6832 |          10.7264 |
[32m[20221213 12:24:40 @agent_ppo2.py:179][0m |          -0.0273 |           8.6496 |          10.8692 |
[32m[20221213 12:24:40 @agent_ppo2.py:179][0m |          -0.0262 |           8.6070 |          10.9134 |
[32m[20221213 12:24:40 @agent_ppo2.py:179][0m |          -0.0297 |           8.6378 |          10.9637 |
[32m[20221213 12:24:40 @agent_ppo2.py:179][0m |          -0.0323 |           8.5953 |          10.9455 |
[32m[20221213 12:24:40 @agent_ppo2.py:179][0m |          -0.0402 |           8.6062 |          11.0189 |
[32m[20221213 12:24:41 @agent_ppo2.py:179][0m |          -0.0365 |           8.5432 |          11.0141 |
[32m[20221213 12:24:41 @agent_ppo2.py:124][0m Policy update time: 1.56 s
[32m[20221213 12:24:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 181.22
[32m[20221213 12:24:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.89
[32m[20221213 12:24:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 204.69
[32m[20221213 12:24:41 @agent_ppo2.py:137][0m Total time:       8.74 min
[32m[20221213 12:24:41 @agent_ppo2.py:139][0m 585728 total steps have happened
[32m[20221213 12:24:41 @agent_ppo2.py:115][0m #------------------------ Iteration 286 --------------------------#
[32m[20221213 12:24:41 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:41 @agent_ppo2.py:179][0m |           0.0553 |           9.0719 |          10.3254 |
[32m[20221213 12:24:41 @agent_ppo2.py:179][0m |           0.0571 |           9.0197 |           9.8088 |
[32m[20221213 12:24:42 @agent_ppo2.py:179][0m |           0.0126 |           9.7888 |          10.5569 |
[32m[20221213 12:24:42 @agent_ppo2.py:179][0m |           0.0069 |           9.8196 |          10.3880 |
[32m[20221213 12:24:42 @agent_ppo2.py:179][0m |          -0.0197 |           8.8706 |          10.7087 |
[32m[20221213 12:24:42 @agent_ppo2.py:179][0m |          -0.0193 |           8.9864 |          10.7547 |
[32m[20221213 12:24:42 @agent_ppo2.py:179][0m |          -0.0336 |           8.7878 |          10.8354 |
[32m[20221213 12:24:42 @agent_ppo2.py:179][0m |          -0.0349 |           8.7734 |          10.8370 |
[32m[20221213 12:24:42 @agent_ppo2.py:179][0m |          -0.0333 |           8.7594 |          10.8383 |
[32m[20221213 12:24:43 @agent_ppo2.py:179][0m |          -0.0405 |           8.7289 |          10.8821 |
[32m[20221213 12:24:43 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:24:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.45
[32m[20221213 12:24:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 186.10
[32m[20221213 12:24:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 151.53
[32m[20221213 12:24:43 @agent_ppo2.py:137][0m Total time:       8.77 min
[32m[20221213 12:24:43 @agent_ppo2.py:139][0m 587776 total steps have happened
[32m[20221213 12:24:43 @agent_ppo2.py:115][0m #------------------------ Iteration 287 --------------------------#
[32m[20221213 12:24:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:24:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:43 @agent_ppo2.py:179][0m |           0.0498 |           8.9860 |          10.1088 |
[32m[20221213 12:24:43 @agent_ppo2.py:179][0m |           0.0464 |           8.8905 |           9.6635 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |           0.0164 |           8.7862 |           9.9831 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |          -0.0117 |           8.8772 |          10.3603 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |          -0.0213 |           8.7272 |          10.4428 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |          -0.0330 |           8.6961 |          10.5389 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |          -0.0363 |           8.6928 |          10.5596 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |          -0.0320 |           8.6750 |          10.5174 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |          -0.0308 |           8.7035 |          10.5392 |
[32m[20221213 12:24:44 @agent_ppo2.py:179][0m |          -0.0408 |           8.6255 |          10.5798 |
[32m[20221213 12:24:44 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:24:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.73
[32m[20221213 12:24:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 202.14
[32m[20221213 12:24:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 215.95
[32m[20221213 12:24:45 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 215.95
[32m[20221213 12:24:45 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 215.95
[32m[20221213 12:24:45 @agent_ppo2.py:137][0m Total time:       8.80 min
[32m[20221213 12:24:45 @agent_ppo2.py:139][0m 589824 total steps have happened
[32m[20221213 12:24:45 @agent_ppo2.py:115][0m #------------------------ Iteration 288 --------------------------#
[32m[20221213 12:24:45 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:45 @agent_ppo2.py:179][0m |           0.0419 |           9.2957 |          10.2751 |
[32m[20221213 12:24:45 @agent_ppo2.py:179][0m |           0.0293 |           9.0967 |           9.9572 |
[32m[20221213 12:24:45 @agent_ppo2.py:179][0m |           0.0348 |          11.1868 |          10.3521 |
[32m[20221213 12:24:46 @agent_ppo2.py:179][0m |          -0.0032 |           9.0827 |          10.3015 |
[32m[20221213 12:24:46 @agent_ppo2.py:179][0m |          -0.0230 |           8.9618 |          10.5709 |
[32m[20221213 12:24:46 @agent_ppo2.py:179][0m |          -0.0289 |           8.9513 |          10.5869 |
[32m[20221213 12:24:46 @agent_ppo2.py:179][0m |          -0.0299 |           8.9297 |          10.6020 |
[32m[20221213 12:24:46 @agent_ppo2.py:179][0m |          -0.0377 |           8.9452 |          10.6481 |
[32m[20221213 12:24:46 @agent_ppo2.py:179][0m |          -0.0341 |           9.3023 |          10.6883 |
[32m[20221213 12:24:46 @agent_ppo2.py:179][0m |          -0.0339 |           8.8915 |          10.5590 |
[32m[20221213 12:24:46 @agent_ppo2.py:124][0m Policy update time: 1.52 s
[32m[20221213 12:24:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.42
[32m[20221213 12:24:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.09
[32m[20221213 12:24:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 202.19
[32m[20221213 12:24:47 @agent_ppo2.py:137][0m Total time:       8.83 min
[32m[20221213 12:24:47 @agent_ppo2.py:139][0m 591872 total steps have happened
[32m[20221213 12:24:47 @agent_ppo2.py:115][0m #------------------------ Iteration 289 --------------------------#
[32m[20221213 12:24:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:24:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:47 @agent_ppo2.py:179][0m |           0.0280 |           8.9870 |          10.5698 |
[32m[20221213 12:24:47 @agent_ppo2.py:179][0m |           0.0061 |           8.7600 |          10.5273 |
[32m[20221213 12:24:47 @agent_ppo2.py:179][0m |          -0.0089 |           8.6442 |          10.5877 |
[32m[20221213 12:24:47 @agent_ppo2.py:179][0m |          -0.0258 |           8.5416 |          10.7123 |
[32m[20221213 12:24:48 @agent_ppo2.py:179][0m |          -0.0220 |           8.9386 |          10.7620 |
[32m[20221213 12:24:48 @agent_ppo2.py:179][0m |          -0.0361 |           8.4389 |          10.7968 |
[32m[20221213 12:24:48 @agent_ppo2.py:179][0m |          -0.0359 |           8.3836 |          10.8127 |
[32m[20221213 12:24:48 @agent_ppo2.py:179][0m |          -0.0366 |           8.2477 |          10.8092 |
[32m[20221213 12:24:48 @agent_ppo2.py:179][0m |          -0.0347 |           8.1890 |          10.8282 |
[32m[20221213 12:24:48 @agent_ppo2.py:179][0m |          -0.0433 |           8.1752 |          10.8779 |
[32m[20221213 12:24:48 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:24:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.28
[32m[20221213 12:24:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 197.53
[32m[20221213 12:24:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 204.02
[32m[20221213 12:24:48 @agent_ppo2.py:137][0m Total time:       8.86 min
[32m[20221213 12:24:48 @agent_ppo2.py:139][0m 593920 total steps have happened
[32m[20221213 12:24:48 @agent_ppo2.py:115][0m #------------------------ Iteration 290 --------------------------#
[32m[20221213 12:24:49 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:24:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:49 @agent_ppo2.py:179][0m |           0.0285 |           9.2929 |          10.3265 |
[32m[20221213 12:24:49 @agent_ppo2.py:179][0m |           0.0238 |           9.0152 |           9.9902 |
[32m[20221213 12:24:49 @agent_ppo2.py:179][0m |           0.0030 |           8.9584 |          10.2091 |
[32m[20221213 12:24:49 @agent_ppo2.py:179][0m |           0.0146 |           8.8904 |           9.9836 |
[32m[20221213 12:24:49 @agent_ppo2.py:179][0m |          -0.0195 |           8.8645 |          10.3367 |
[32m[20221213 12:24:50 @agent_ppo2.py:179][0m |          -0.0323 |           8.8324 |          10.4225 |
[32m[20221213 12:24:50 @agent_ppo2.py:179][0m |          -0.0341 |           8.7974 |          10.4399 |
[32m[20221213 12:24:50 @agent_ppo2.py:179][0m |          -0.0363 |           8.7927 |          10.4765 |
[32m[20221213 12:24:50 @agent_ppo2.py:179][0m |          -0.0379 |           8.7710 |          10.4813 |
[32m[20221213 12:24:50 @agent_ppo2.py:179][0m |          -0.0417 |           8.7428 |          10.5105 |
[32m[20221213 12:24:50 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:24:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.54
[32m[20221213 12:24:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.61
[32m[20221213 12:24:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 197.19
[32m[20221213 12:24:50 @agent_ppo2.py:137][0m Total time:       8.89 min
[32m[20221213 12:24:50 @agent_ppo2.py:139][0m 595968 total steps have happened
[32m[20221213 12:24:50 @agent_ppo2.py:115][0m #------------------------ Iteration 291 --------------------------#
[32m[20221213 12:24:51 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:24:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:51 @agent_ppo2.py:179][0m |           0.0470 |           9.0358 |          10.0626 |
[32m[20221213 12:24:51 @agent_ppo2.py:179][0m |           0.0346 |           8.6412 |           9.7538 |
[32m[20221213 12:24:51 @agent_ppo2.py:179][0m |           0.0021 |           8.5247 |          10.1445 |
[32m[20221213 12:24:51 @agent_ppo2.py:179][0m |          -0.0250 |           8.4768 |          10.3321 |
[32m[20221213 12:24:51 @agent_ppo2.py:179][0m |          -0.0289 |           8.4084 |          10.4766 |
[32m[20221213 12:24:51 @agent_ppo2.py:179][0m |          -0.0347 |           8.3500 |          10.5065 |
[32m[20221213 12:24:52 @agent_ppo2.py:179][0m |          -0.0387 |           8.2869 |          10.5518 |
[32m[20221213 12:24:52 @agent_ppo2.py:179][0m |          -0.0402 |           8.2690 |          10.5483 |
[32m[20221213 12:24:52 @agent_ppo2.py:179][0m |          -0.0417 |           8.2751 |          10.5846 |
[32m[20221213 12:24:52 @agent_ppo2.py:179][0m |          -0.0383 |           8.2080 |          10.5568 |
[32m[20221213 12:24:52 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:24:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.47
[32m[20221213 12:24:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 196.47
[32m[20221213 12:24:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 211.02
[32m[20221213 12:24:52 @agent_ppo2.py:137][0m Total time:       8.93 min
[32m[20221213 12:24:52 @agent_ppo2.py:139][0m 598016 total steps have happened
[32m[20221213 12:24:52 @agent_ppo2.py:115][0m #------------------------ Iteration 292 --------------------------#
[32m[20221213 12:24:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:24:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:53 @agent_ppo2.py:179][0m |           0.0375 |           9.3522 |          10.2477 |
[32m[20221213 12:24:53 @agent_ppo2.py:179][0m |           0.0216 |           9.1772 |           9.9133 |
[32m[20221213 12:24:53 @agent_ppo2.py:179][0m |          -0.0123 |           9.0814 |          10.3871 |
[32m[20221213 12:24:53 @agent_ppo2.py:179][0m |          -0.0127 |           9.5213 |          10.4992 |
[32m[20221213 12:24:53 @agent_ppo2.py:179][0m |          -0.0136 |           9.0086 |          10.2836 |
[32m[20221213 12:24:53 @agent_ppo2.py:179][0m |          -0.0293 |           8.9878 |          10.5398 |
[32m[20221213 12:24:54 @agent_ppo2.py:179][0m |          -0.0316 |           8.9467 |          10.5067 |
[32m[20221213 12:24:54 @agent_ppo2.py:179][0m |          -0.0291 |           9.0462 |          10.5966 |
[32m[20221213 12:24:54 @agent_ppo2.py:179][0m |          -0.0322 |           8.9588 |          10.5032 |
[32m[20221213 12:24:54 @agent_ppo2.py:179][0m |          -0.0385 |           8.8685 |          10.6122 |
[32m[20221213 12:24:54 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:24:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 187.90
[32m[20221213 12:24:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 199.35
[32m[20221213 12:24:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 201.19
[32m[20221213 12:24:54 @agent_ppo2.py:137][0m Total time:       8.96 min
[32m[20221213 12:24:54 @agent_ppo2.py:139][0m 600064 total steps have happened
[32m[20221213 12:24:54 @agent_ppo2.py:115][0m #------------------------ Iteration 293 --------------------------#
[32m[20221213 12:24:54 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:55 @agent_ppo2.py:179][0m |           0.0279 |           9.5311 |          10.3955 |
[32m[20221213 12:24:55 @agent_ppo2.py:179][0m |           0.0601 |           8.9670 |           9.4722 |
[32m[20221213 12:24:55 @agent_ppo2.py:179][0m |           0.0035 |           8.8245 |          10.0402 |
[32m[20221213 12:24:55 @agent_ppo2.py:179][0m |          -0.0170 |           8.7098 |          10.3044 |
[32m[20221213 12:24:55 @agent_ppo2.py:179][0m |          -0.0241 |           8.6694 |          10.4392 |
[32m[20221213 12:24:55 @agent_ppo2.py:179][0m |          -0.0294 |           8.5919 |          10.4759 |
[32m[20221213 12:24:55 @agent_ppo2.py:179][0m |          -0.0340 |           8.5108 |          10.5209 |
[32m[20221213 12:24:56 @agent_ppo2.py:179][0m |          -0.0324 |           8.5791 |          10.5567 |
[32m[20221213 12:24:56 @agent_ppo2.py:179][0m |          -0.0349 |           8.4941 |          10.5378 |
[32m[20221213 12:24:56 @agent_ppo2.py:179][0m |          -0.0263 |           8.5117 |          10.5040 |
[32m[20221213 12:24:56 @agent_ppo2.py:124][0m Policy update time: 1.53 s
[32m[20221213 12:24:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.72
[32m[20221213 12:24:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 201.80
[32m[20221213 12:24:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.52
[32m[20221213 12:24:56 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 221.52
[32m[20221213 12:24:56 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 221.52
[32m[20221213 12:24:56 @agent_ppo2.py:137][0m Total time:       8.99 min
[32m[20221213 12:24:56 @agent_ppo2.py:139][0m 602112 total steps have happened
[32m[20221213 12:24:56 @agent_ppo2.py:115][0m #------------------------ Iteration 294 --------------------------#
[32m[20221213 12:24:56 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:24:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:57 @agent_ppo2.py:179][0m |           0.0363 |           9.3919 |          10.4032 |
[32m[20221213 12:24:57 @agent_ppo2.py:179][0m |           0.0383 |           9.4928 |          10.1333 |
[32m[20221213 12:24:57 @agent_ppo2.py:179][0m |           0.0279 |           8.9545 |          10.0010 |
[32m[20221213 12:24:57 @agent_ppo2.py:179][0m |          -0.0249 |           8.8240 |          10.4800 |
[32m[20221213 12:24:57 @agent_ppo2.py:179][0m |          -0.0262 |           8.7435 |          10.5137 |
[32m[20221213 12:24:57 @agent_ppo2.py:179][0m |          -0.0329 |           8.6961 |          10.5509 |
[32m[20221213 12:24:57 @agent_ppo2.py:179][0m |          -0.0383 |           8.6341 |          10.6368 |
[32m[20221213 12:24:58 @agent_ppo2.py:179][0m |          -0.0385 |           8.5521 |          10.6045 |
[32m[20221213 12:24:58 @agent_ppo2.py:179][0m |          -0.0386 |           8.7226 |          10.6287 |
[32m[20221213 12:24:58 @agent_ppo2.py:179][0m |          -0.0462 |           8.5783 |          10.6682 |
[32m[20221213 12:24:58 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:24:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.74
[32m[20221213 12:24:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 201.67
[32m[20221213 12:24:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 210.96
[32m[20221213 12:24:58 @agent_ppo2.py:137][0m Total time:       9.02 min
[32m[20221213 12:24:58 @agent_ppo2.py:139][0m 604160 total steps have happened
[32m[20221213 12:24:58 @agent_ppo2.py:115][0m #------------------------ Iteration 295 --------------------------#
[32m[20221213 12:24:58 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:24:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:24:59 @agent_ppo2.py:179][0m |           0.0226 |          10.3483 |          10.2951 |
[32m[20221213 12:24:59 @agent_ppo2.py:179][0m |           0.0044 |          10.1748 |          10.2036 |
[32m[20221213 12:24:59 @agent_ppo2.py:179][0m |          -0.0190 |          10.0653 |          10.3198 |
[32m[20221213 12:24:59 @agent_ppo2.py:179][0m |          -0.0312 |          10.0602 |          10.4000 |
[32m[20221213 12:24:59 @agent_ppo2.py:179][0m |          -0.0344 |          10.0733 |          10.3937 |
[32m[20221213 12:24:59 @agent_ppo2.py:179][0m |          -0.0415 |          10.0040 |          10.4103 |
[32m[20221213 12:25:00 @agent_ppo2.py:179][0m |          -0.0350 |          10.2329 |          10.4041 |
[32m[20221213 12:25:00 @agent_ppo2.py:179][0m |          -0.0401 |           9.9413 |          10.4379 |
[32m[20221213 12:25:00 @agent_ppo2.py:179][0m |          -0.0390 |          10.6763 |          10.4100 |
[32m[20221213 12:25:00 @agent_ppo2.py:179][0m |          -0.0394 |          10.0484 |          10.4037 |
[32m[20221213 12:25:00 @agent_ppo2.py:124][0m Policy update time: 1.90 s
[32m[20221213 12:25:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.39
[32m[20221213 12:25:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 189.69
[32m[20221213 12:25:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 218.81
[32m[20221213 12:25:00 @agent_ppo2.py:137][0m Total time:       9.06 min
[32m[20221213 12:25:00 @agent_ppo2.py:139][0m 606208 total steps have happened
[32m[20221213 12:25:00 @agent_ppo2.py:115][0m #------------------------ Iteration 296 --------------------------#
[32m[20221213 12:25:01 @agent_ppo2.py:121][0m Sampling time: 0.38 s by 5 slaves
[32m[20221213 12:25:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:01 @agent_ppo2.py:179][0m |           0.0456 |          10.0119 |          10.2079 |
[32m[20221213 12:25:01 @agent_ppo2.py:179][0m |           0.0070 |           9.8201 |          10.3459 |
[32m[20221213 12:25:02 @agent_ppo2.py:179][0m |          -0.0089 |          10.0361 |          10.4903 |
[32m[20221213 12:25:02 @agent_ppo2.py:179][0m |          -0.0242 |           9.6762 |          10.5755 |
[32m[20221213 12:25:02 @agent_ppo2.py:179][0m |          -0.0315 |           9.6291 |          10.6159 |
[32m[20221213 12:25:02 @agent_ppo2.py:179][0m |          -0.0320 |           9.5822 |          10.6754 |
[32m[20221213 12:25:02 @agent_ppo2.py:179][0m |          -0.0308 |           9.7366 |          10.6874 |
[32m[20221213 12:25:02 @agent_ppo2.py:179][0m |          -0.0251 |          10.4070 |          10.6784 |
[32m[20221213 12:25:03 @agent_ppo2.py:179][0m |          -0.0359 |           9.5935 |          10.7126 |
[32m[20221213 12:25:03 @agent_ppo2.py:179][0m |          -0.0357 |           9.5452 |          10.6617 |
[32m[20221213 12:25:03 @agent_ppo2.py:124][0m Policy update time: 1.89 s
[32m[20221213 12:25:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 195.81
[32m[20221213 12:25:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 200.48
[32m[20221213 12:25:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 206.20
[32m[20221213 12:25:03 @agent_ppo2.py:137][0m Total time:       9.11 min
[32m[20221213 12:25:03 @agent_ppo2.py:139][0m 608256 total steps have happened
[32m[20221213 12:25:03 @agent_ppo2.py:115][0m #------------------------ Iteration 297 --------------------------#
[32m[20221213 12:25:03 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:25:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:04 @agent_ppo2.py:179][0m |           0.0440 |           9.8221 |          10.5261 |
[32m[20221213 12:25:04 @agent_ppo2.py:179][0m |           0.0507 |           9.3680 |           9.6546 |
[32m[20221213 12:25:04 @agent_ppo2.py:179][0m |           0.0015 |           9.1886 |          10.5611 |
[32m[20221213 12:25:04 @agent_ppo2.py:179][0m |          -0.0158 |           9.0491 |          10.6542 |
[32m[20221213 12:25:04 @agent_ppo2.py:179][0m |          -0.0215 |           9.3009 |          10.7519 |
[32m[20221213 12:25:04 @agent_ppo2.py:179][0m |          -0.0298 |           8.9010 |          10.7956 |
[32m[20221213 12:25:05 @agent_ppo2.py:179][0m |          -0.0288 |           8.9873 |          10.8082 |
[32m[20221213 12:25:05 @agent_ppo2.py:179][0m |          -0.0349 |           8.6812 |          10.8308 |
[32m[20221213 12:25:05 @agent_ppo2.py:179][0m |          -0.0402 |           8.6138 |          10.8567 |
[32m[20221213 12:25:05 @agent_ppo2.py:179][0m |          -0.0419 |           8.5343 |          10.8499 |
[32m[20221213 12:25:05 @agent_ppo2.py:124][0m Policy update time: 1.77 s
[32m[20221213 12:25:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.88
[32m[20221213 12:25:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 207.23
[32m[20221213 12:25:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 215.53
[32m[20221213 12:25:05 @agent_ppo2.py:137][0m Total time:       9.14 min
[32m[20221213 12:25:05 @agent_ppo2.py:139][0m 610304 total steps have happened
[32m[20221213 12:25:05 @agent_ppo2.py:115][0m #------------------------ Iteration 298 --------------------------#
[32m[20221213 12:25:06 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:25:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:06 @agent_ppo2.py:179][0m |           0.0575 |          10.9606 |          10.0771 |
[32m[20221213 12:25:06 @agent_ppo2.py:179][0m |           0.0614 |          10.6276 |           9.4370 |
[32m[20221213 12:25:06 @agent_ppo2.py:179][0m |           0.0227 |          10.5710 |           9.9854 |
[32m[20221213 12:25:06 @agent_ppo2.py:179][0m |          -0.0196 |          10.4747 |          10.3389 |
[32m[20221213 12:25:07 @agent_ppo2.py:179][0m |          -0.0307 |          10.4149 |          10.4846 |
[32m[20221213 12:25:07 @agent_ppo2.py:179][0m |          -0.0404 |          10.3504 |          10.5517 |
[32m[20221213 12:25:07 @agent_ppo2.py:179][0m |          -0.0410 |          10.3420 |          10.5698 |
[32m[20221213 12:25:07 @agent_ppo2.py:179][0m |          -0.0453 |          10.3590 |          10.5486 |
[32m[20221213 12:25:07 @agent_ppo2.py:179][0m |          -0.0446 |          10.3301 |          10.6085 |
[32m[20221213 12:25:07 @agent_ppo2.py:179][0m |          -0.0501 |          10.3152 |          10.6172 |
[32m[20221213 12:25:07 @agent_ppo2.py:124][0m Policy update time: 1.73 s
[32m[20221213 12:25:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 179.61
[32m[20221213 12:25:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 202.57
[32m[20221213 12:25:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 200.95
[32m[20221213 12:25:08 @agent_ppo2.py:137][0m Total time:       9.18 min
[32m[20221213 12:25:08 @agent_ppo2.py:139][0m 612352 total steps have happened
[32m[20221213 12:25:08 @agent_ppo2.py:115][0m #------------------------ Iteration 299 --------------------------#
[32m[20221213 12:25:08 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:08 @agent_ppo2.py:179][0m |           0.0443 |          10.0380 |          10.2966 |
[32m[20221213 12:25:08 @agent_ppo2.py:179][0m |           0.0485 |           9.6860 |           9.6566 |
[32m[20221213 12:25:08 @agent_ppo2.py:179][0m |           0.0094 |           9.5699 |          10.0912 |
[32m[20221213 12:25:09 @agent_ppo2.py:179][0m |          -0.0200 |           9.4928 |          10.4286 |
[32m[20221213 12:25:09 @agent_ppo2.py:179][0m |          -0.0258 |           9.4685 |          10.4556 |
[32m[20221213 12:25:09 @agent_ppo2.py:179][0m |          -0.0180 |           9.5237 |          10.4575 |
[32m[20221213 12:25:09 @agent_ppo2.py:179][0m |          -0.0225 |           9.6187 |          10.5331 |
[32m[20221213 12:25:09 @agent_ppo2.py:179][0m |          -0.0366 |           9.2618 |          10.5685 |
[32m[20221213 12:25:09 @agent_ppo2.py:179][0m |          -0.0389 |           9.2205 |          10.6161 |
[32m[20221213 12:25:09 @agent_ppo2.py:179][0m |          -0.0425 |           9.1930 |          10.6475 |
[32m[20221213 12:25:09 @agent_ppo2.py:124][0m Policy update time: 1.67 s
[32m[20221213 12:25:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 208.35
[32m[20221213 12:25:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 219.41
[32m[20221213 12:25:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 197.42
[32m[20221213 12:25:10 @agent_ppo2.py:137][0m Total time:       9.22 min
[32m[20221213 12:25:10 @agent_ppo2.py:139][0m 614400 total steps have happened
[32m[20221213 12:25:10 @agent_ppo2.py:115][0m #------------------------ Iteration 300 --------------------------#
[32m[20221213 12:25:10 @agent_ppo2.py:121][0m Sampling time: 0.28 s by 5 slaves
[32m[20221213 12:25:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:10 @agent_ppo2.py:179][0m |           0.0445 |          10.9246 |          10.1178 |
[32m[20221213 12:25:10 @agent_ppo2.py:179][0m |           0.0052 |          10.6913 |          10.1837 |
[32m[20221213 12:25:11 @agent_ppo2.py:179][0m |          -0.0023 |          10.6173 |          10.2916 |
[32m[20221213 12:25:11 @agent_ppo2.py:179][0m |          -0.0177 |          10.5952 |          10.3843 |
[32m[20221213 12:25:11 @agent_ppo2.py:179][0m |          -0.0280 |          10.5552 |          10.4529 |
[32m[20221213 12:25:11 @agent_ppo2.py:179][0m |          -0.0286 |          10.5283 |          10.4847 |
[32m[20221213 12:25:11 @agent_ppo2.py:179][0m |          -0.0350 |          10.4998 |          10.4882 |
[32m[20221213 12:25:11 @agent_ppo2.py:179][0m |          -0.0363 |          10.5347 |          10.5451 |
[32m[20221213 12:25:12 @agent_ppo2.py:179][0m |          -0.0274 |          10.6448 |          10.5094 |
[32m[20221213 12:25:12 @agent_ppo2.py:179][0m |          -0.0349 |          10.5017 |          10.5326 |
[32m[20221213 12:25:12 @agent_ppo2.py:124][0m Policy update time: 1.71 s
[32m[20221213 12:25:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.66
[32m[20221213 12:25:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 199.74
[32m[20221213 12:25:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 222.66
[32m[20221213 12:25:12 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 222.66
[32m[20221213 12:25:12 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 222.66
[32m[20221213 12:25:12 @agent_ppo2.py:137][0m Total time:       9.25 min
[32m[20221213 12:25:12 @agent_ppo2.py:139][0m 616448 total steps have happened
[32m[20221213 12:25:12 @agent_ppo2.py:115][0m #------------------------ Iteration 301 --------------------------#
[32m[20221213 12:25:12 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:12 @agent_ppo2.py:179][0m |           0.0827 |          10.4215 |           9.7455 |
[32m[20221213 12:25:13 @agent_ppo2.py:179][0m |           0.0381 |          10.1694 |           9.2675 |
[32m[20221213 12:25:13 @agent_ppo2.py:179][0m |          -0.0004 |          10.0347 |           9.9758 |
[32m[20221213 12:25:13 @agent_ppo2.py:179][0m |          -0.0182 |           9.9928 |          10.3024 |
[32m[20221213 12:25:13 @agent_ppo2.py:179][0m |          -0.0286 |           9.9079 |          10.4088 |
[32m[20221213 12:25:13 @agent_ppo2.py:179][0m |          -0.0346 |           9.8851 |          10.4817 |
[32m[20221213 12:25:13 @agent_ppo2.py:179][0m |          -0.0346 |           9.8655 |          10.4982 |
[32m[20221213 12:25:14 @agent_ppo2.py:179][0m |          -0.0333 |          10.0294 |          10.5061 |
[32m[20221213 12:25:14 @agent_ppo2.py:179][0m |          -0.0339 |           9.8037 |          10.4697 |
[32m[20221213 12:25:14 @agent_ppo2.py:179][0m |          -0.0443 |           9.7373 |          10.5473 |
[32m[20221213 12:25:14 @agent_ppo2.py:124][0m Policy update time: 1.73 s
[32m[20221213 12:25:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 194.96
[32m[20221213 12:25:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 208.72
[32m[20221213 12:25:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 225.05
[32m[20221213 12:25:14 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 225.05
[32m[20221213 12:25:14 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 225.05
[32m[20221213 12:25:14 @agent_ppo2.py:137][0m Total time:       9.29 min
[32m[20221213 12:25:14 @agent_ppo2.py:139][0m 618496 total steps have happened
[32m[20221213 12:25:14 @agent_ppo2.py:115][0m #------------------------ Iteration 302 --------------------------#
[32m[20221213 12:25:14 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:15 @agent_ppo2.py:179][0m |           0.0878 |           9.9771 |          10.0189 |
[32m[20221213 12:25:15 @agent_ppo2.py:179][0m |           0.0527 |           9.5224 |           8.7344 |
[32m[20221213 12:25:15 @agent_ppo2.py:179][0m |          -0.0134 |           9.3410 |           8.7963 |
[32m[20221213 12:25:15 @agent_ppo2.py:179][0m |          -0.0314 |           9.3116 |           9.0025 |
[32m[20221213 12:25:15 @agent_ppo2.py:179][0m |          -0.0287 |          10.5628 |           9.0976 |
[32m[20221213 12:25:15 @agent_ppo2.py:179][0m |          -0.0446 |           9.5044 |           9.1814 |
[32m[20221213 12:25:16 @agent_ppo2.py:179][0m |          -0.0385 |          10.5728 |           9.1765 |
[32m[20221213 12:25:16 @agent_ppo2.py:179][0m |          -0.0530 |           9.1215 |           9.2097 |
[32m[20221213 12:25:16 @agent_ppo2.py:179][0m |          -0.0564 |           9.0444 |           9.2410 |
[32m[20221213 12:25:16 @agent_ppo2.py:179][0m |          -0.0605 |           9.0119 |           9.2665 |
[32m[20221213 12:25:16 @agent_ppo2.py:124][0m Policy update time: 1.72 s
[32m[20221213 12:25:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.09
[32m[20221213 12:25:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 214.60
[32m[20221213 12:25:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 214.71
[32m[20221213 12:25:16 @agent_ppo2.py:137][0m Total time:       9.33 min
[32m[20221213 12:25:16 @agent_ppo2.py:139][0m 620544 total steps have happened
[32m[20221213 12:25:16 @agent_ppo2.py:115][0m #------------------------ Iteration 303 --------------------------#
[32m[20221213 12:25:17 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:17 @agent_ppo2.py:179][0m |           0.0372 |          10.7268 |          10.7766 |
[32m[20221213 12:25:17 @agent_ppo2.py:179][0m |           0.0151 |          10.3951 |          10.5354 |
[32m[20221213 12:25:17 @agent_ppo2.py:179][0m |          -0.0180 |           9.8379 |          10.8498 |
[32m[20221213 12:25:17 @agent_ppo2.py:179][0m |          -0.0255 |           9.5588 |          10.9395 |
[32m[20221213 12:25:17 @agent_ppo2.py:179][0m |          -0.0319 |           9.3812 |          10.9911 |
[32m[20221213 12:25:18 @agent_ppo2.py:179][0m |          -0.0363 |           9.2586 |          11.0073 |
[32m[20221213 12:25:18 @agent_ppo2.py:179][0m |          -0.0376 |           9.1771 |          11.0226 |
[32m[20221213 12:25:18 @agent_ppo2.py:179][0m |          -0.0378 |           9.1156 |          11.0389 |
[32m[20221213 12:25:18 @agent_ppo2.py:179][0m |          -0.0426 |           9.1306 |          11.0664 |
[32m[20221213 12:25:18 @agent_ppo2.py:179][0m |          -0.0383 |           9.0471 |          11.0575 |
[32m[20221213 12:25:18 @agent_ppo2.py:124][0m Policy update time: 1.72 s
[32m[20221213 12:25:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 209.04
[32m[20221213 12:25:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 217.16
[32m[20221213 12:25:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 223.75
[32m[20221213 12:25:19 @agent_ppo2.py:137][0m Total time:       9.36 min
[32m[20221213 12:25:19 @agent_ppo2.py:139][0m 622592 total steps have happened
[32m[20221213 12:25:19 @agent_ppo2.py:115][0m #------------------------ Iteration 304 --------------------------#
[32m[20221213 12:25:19 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:19 @agent_ppo2.py:179][0m |           0.0606 |          12.8079 |          10.4686 |
[32m[20221213 12:25:19 @agent_ppo2.py:179][0m |           0.0258 |          11.6365 |          10.2297 |
[32m[20221213 12:25:19 @agent_ppo2.py:179][0m |          -0.0092 |          11.3532 |          10.6623 |
[32m[20221213 12:25:20 @agent_ppo2.py:179][0m |          -0.0187 |          11.8883 |          10.8255 |
[32m[20221213 12:25:20 @agent_ppo2.py:179][0m |          -0.0322 |          11.2402 |          10.7975 |
[32m[20221213 12:25:20 @agent_ppo2.py:179][0m |          -0.0371 |          11.0532 |          10.8416 |
[32m[20221213 12:25:20 @agent_ppo2.py:179][0m |          -0.0387 |          11.1004 |          10.9286 |
[32m[20221213 12:25:20 @agent_ppo2.py:179][0m |          -0.0441 |          10.8778 |          10.9299 |
[32m[20221213 12:25:20 @agent_ppo2.py:179][0m |          -0.0515 |          10.8769 |          10.9655 |
[32m[20221213 12:25:20 @agent_ppo2.py:179][0m |          -0.0310 |          11.3930 |          10.9184 |
[32m[20221213 12:25:20 @agent_ppo2.py:124][0m Policy update time: 1.68 s
[32m[20221213 12:25:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 205.27
[32m[20221213 12:25:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 211.05
[32m[20221213 12:25:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 208.92
[32m[20221213 12:25:21 @agent_ppo2.py:137][0m Total time:       9.40 min
[32m[20221213 12:25:21 @agent_ppo2.py:139][0m 624640 total steps have happened
[32m[20221213 12:25:21 @agent_ppo2.py:115][0m #------------------------ Iteration 305 --------------------------#
[32m[20221213 12:25:21 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:21 @agent_ppo2.py:179][0m |           0.0525 |          11.4093 |          10.7900 |
[32m[20221213 12:25:21 @agent_ppo2.py:179][0m |           0.0282 |          10.5004 |          10.3462 |
[32m[20221213 12:25:22 @agent_ppo2.py:179][0m |          -0.0114 |          10.3939 |          10.9799 |
[32m[20221213 12:25:22 @agent_ppo2.py:179][0m |          -0.0261 |          10.3350 |          11.1663 |
[32m[20221213 12:25:22 @agent_ppo2.py:179][0m |          -0.0339 |          10.3112 |          11.1799 |
[32m[20221213 12:25:22 @agent_ppo2.py:179][0m |          -0.0205 |          10.9530 |          11.2103 |
[32m[20221213 12:25:22 @agent_ppo2.py:179][0m |          -0.0327 |          10.2374 |          11.1655 |
[32m[20221213 12:25:22 @agent_ppo2.py:179][0m |          -0.0372 |          10.1419 |          11.2146 |
[32m[20221213 12:25:22 @agent_ppo2.py:179][0m |          -0.0378 |          10.2396 |          11.2762 |
[32m[20221213 12:25:23 @agent_ppo2.py:179][0m |          -0.0364 |          10.5066 |          11.3038 |
[32m[20221213 12:25:23 @agent_ppo2.py:124][0m Policy update time: 1.70 s
[32m[20221213 12:25:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 205.71
[32m[20221213 12:25:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 211.75
[32m[20221213 12:25:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.54
[32m[20221213 12:25:23 @agent_ppo2.py:137][0m Total time:       9.44 min
[32m[20221213 12:25:23 @agent_ppo2.py:139][0m 626688 total steps have happened
[32m[20221213 12:25:23 @agent_ppo2.py:115][0m #------------------------ Iteration 306 --------------------------#
[32m[20221213 12:25:23 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:23 @agent_ppo2.py:179][0m |           0.0466 |          13.5022 |          10.7724 |
[32m[20221213 12:25:24 @agent_ppo2.py:179][0m |           0.0652 |          12.8196 |           9.7502 |
[32m[20221213 12:25:24 @agent_ppo2.py:179][0m |           0.0051 |          12.6697 |          10.8034 |
[32m[20221213 12:25:24 @agent_ppo2.py:179][0m |          -0.0152 |          12.4915 |          10.9618 |
[32m[20221213 12:25:24 @agent_ppo2.py:179][0m |          -0.0308 |          12.4418 |          11.1195 |
[32m[20221213 12:25:24 @agent_ppo2.py:179][0m |          -0.0362 |          12.4165 |          11.1464 |
[32m[20221213 12:25:24 @agent_ppo2.py:179][0m |          -0.0357 |          12.1913 |          11.1765 |
[32m[20221213 12:25:25 @agent_ppo2.py:179][0m |          -0.0415 |          12.1774 |          11.2611 |
[32m[20221213 12:25:25 @agent_ppo2.py:179][0m |          -0.0393 |          12.2551 |          11.2683 |
[32m[20221213 12:25:25 @agent_ppo2.py:179][0m |          -0.0457 |          11.9910 |          11.2494 |
[32m[20221213 12:25:25 @agent_ppo2.py:124][0m Policy update time: 1.75 s
[32m[20221213 12:25:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 205.64
[32m[20221213 12:25:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 214.50
[32m[20221213 12:25:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 238.03
[32m[20221213 12:25:25 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 238.03
[32m[20221213 12:25:25 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 238.03
[32m[20221213 12:25:25 @agent_ppo2.py:137][0m Total time:       9.47 min
[32m[20221213 12:25:25 @agent_ppo2.py:139][0m 628736 total steps have happened
[32m[20221213 12:25:25 @agent_ppo2.py:115][0m #------------------------ Iteration 307 --------------------------#
[32m[20221213 12:25:25 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:26 @agent_ppo2.py:179][0m |           0.0346 |          12.1621 |          10.9568 |
[32m[20221213 12:25:26 @agent_ppo2.py:179][0m |           0.0078 |          10.8921 |          10.8516 |
[32m[20221213 12:25:26 @agent_ppo2.py:179][0m |          -0.0174 |          10.8028 |          11.0603 |
[32m[20221213 12:25:26 @agent_ppo2.py:179][0m |          -0.0290 |          10.7608 |          11.1056 |
[32m[20221213 12:25:26 @agent_ppo2.py:179][0m |          -0.0178 |          12.0747 |          11.1071 |
[32m[20221213 12:25:26 @agent_ppo2.py:179][0m |          -0.0270 |          10.6645 |          11.1184 |
[32m[20221213 12:25:27 @agent_ppo2.py:179][0m |          -0.0329 |          10.6570 |          11.1805 |
[32m[20221213 12:25:27 @agent_ppo2.py:179][0m |          -0.0389 |          10.5849 |          11.1641 |
[32m[20221213 12:25:27 @agent_ppo2.py:179][0m |          -0.0427 |          10.5696 |          11.2103 |
[32m[20221213 12:25:27 @agent_ppo2.py:179][0m |          -0.0323 |          11.5269 |          11.1983 |
[32m[20221213 12:25:27 @agent_ppo2.py:124][0m Policy update time: 1.71 s
[32m[20221213 12:25:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 161.27
[32m[20221213 12:25:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 205.71
[32m[20221213 12:25:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 110.05
[32m[20221213 12:25:27 @agent_ppo2.py:137][0m Total time:       9.51 min
[32m[20221213 12:25:27 @agent_ppo2.py:139][0m 630784 total steps have happened
[32m[20221213 12:25:27 @agent_ppo2.py:115][0m #------------------------ Iteration 308 --------------------------#
[32m[20221213 12:25:28 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:28 @agent_ppo2.py:179][0m |           0.0471 |          11.5913 |          10.9976 |
[32m[20221213 12:25:28 @agent_ppo2.py:179][0m |           0.0094 |          11.2298 |          10.7810 |
[32m[20221213 12:25:28 @agent_ppo2.py:179][0m |          -0.0187 |          11.1098 |          11.1296 |
[32m[20221213 12:25:28 @agent_ppo2.py:179][0m |          -0.0219 |          11.7001 |          11.2723 |
[32m[20221213 12:25:29 @agent_ppo2.py:179][0m |          -0.0350 |          11.0106 |          11.3023 |
[32m[20221213 12:25:29 @agent_ppo2.py:179][0m |          -0.0337 |          10.9718 |          11.3801 |
[32m[20221213 12:25:29 @agent_ppo2.py:179][0m |          -0.0327 |          10.8264 |          11.2947 |
[32m[20221213 12:25:29 @agent_ppo2.py:179][0m |          -0.0422 |          10.8054 |          11.3778 |
[32m[20221213 12:25:29 @agent_ppo2.py:179][0m |          -0.0454 |          10.7698 |          11.4249 |
[32m[20221213 12:25:29 @agent_ppo2.py:179][0m |          -0.0400 |          10.7154 |          11.3735 |
[32m[20221213 12:25:29 @agent_ppo2.py:124][0m Policy update time: 1.71 s
[32m[20221213 12:25:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 206.28
[32m[20221213 12:25:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 213.70
[32m[20221213 12:25:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 230.59
[32m[20221213 12:25:30 @agent_ppo2.py:137][0m Total time:       9.55 min
[32m[20221213 12:25:30 @agent_ppo2.py:139][0m 632832 total steps have happened
[32m[20221213 12:25:30 @agent_ppo2.py:115][0m #------------------------ Iteration 309 --------------------------#
[32m[20221213 12:25:30 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:25:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:30 @agent_ppo2.py:179][0m |           0.0687 |          12.6641 |          11.1596 |
[32m[20221213 12:25:30 @agent_ppo2.py:179][0m |           0.0166 |          12.3214 |          10.9745 |
[32m[20221213 12:25:30 @agent_ppo2.py:179][0m |          -0.0155 |          12.1825 |          11.3059 |
[32m[20221213 12:25:30 @agent_ppo2.py:179][0m |          -0.0283 |          12.1379 |          11.4354 |
[32m[20221213 12:25:30 @agent_ppo2.py:179][0m |          -0.0356 |          12.0627 |          11.4699 |
[32m[20221213 12:25:31 @agent_ppo2.py:179][0m |          -0.0420 |          12.0134 |          11.5499 |
[32m[20221213 12:25:31 @agent_ppo2.py:179][0m |          -0.0402 |          11.9935 |          11.5599 |
[32m[20221213 12:25:31 @agent_ppo2.py:179][0m |          -0.0458 |          11.9372 |          11.5721 |
[32m[20221213 12:25:31 @agent_ppo2.py:179][0m |          -0.0451 |          11.9383 |          11.5678 |
[32m[20221213 12:25:31 @agent_ppo2.py:179][0m |          -0.0437 |          11.9337 |          11.5531 |
[32m[20221213 12:25:31 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:25:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 202.36
[32m[20221213 12:25:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 215.46
[32m[20221213 12:25:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 235.94
[32m[20221213 12:25:31 @agent_ppo2.py:137][0m Total time:       9.58 min
[32m[20221213 12:25:31 @agent_ppo2.py:139][0m 634880 total steps have happened
[32m[20221213 12:25:31 @agent_ppo2.py:115][0m #------------------------ Iteration 310 --------------------------#
[32m[20221213 12:25:32 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:32 @agent_ppo2.py:179][0m |           0.0683 |          10.5869 |          11.1208 |
[32m[20221213 12:25:32 @agent_ppo2.py:179][0m |           0.0772 |          10.3938 |          10.2525 |
[32m[20221213 12:25:32 @agent_ppo2.py:179][0m |           0.0080 |          10.3864 |          11.2133 |
[32m[20221213 12:25:32 @agent_ppo2.py:179][0m |          -0.0114 |          10.3243 |          11.3853 |
[32m[20221213 12:25:32 @agent_ppo2.py:179][0m |          -0.0204 |          10.2884 |          11.5009 |
[32m[20221213 12:25:33 @agent_ppo2.py:179][0m |          -0.0229 |          10.2472 |          11.5305 |
[32m[20221213 12:25:33 @agent_ppo2.py:179][0m |          -0.0232 |          10.2368 |          11.5528 |
[32m[20221213 12:25:33 @agent_ppo2.py:179][0m |          -0.0221 |          10.2438 |          11.5549 |
[32m[20221213 12:25:33 @agent_ppo2.py:179][0m |          -0.0309 |          10.1885 |          11.5986 |
[32m[20221213 12:25:33 @agent_ppo2.py:179][0m |          -0.0276 |          10.1908 |          11.6210 |
[32m[20221213 12:25:33 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:25:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.27
[32m[20221213 12:25:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 218.35
[32m[20221213 12:25:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 215.56
[32m[20221213 12:25:33 @agent_ppo2.py:137][0m Total time:       9.61 min
[32m[20221213 12:25:33 @agent_ppo2.py:139][0m 636928 total steps have happened
[32m[20221213 12:25:33 @agent_ppo2.py:115][0m #------------------------ Iteration 311 --------------------------#
[32m[20221213 12:25:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:25:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:34 @agent_ppo2.py:179][0m |           0.0335 |          12.1613 |          11.1195 |
[32m[20221213 12:25:34 @agent_ppo2.py:179][0m |           0.0154 |          11.9458 |          10.8099 |
[32m[20221213 12:25:34 @agent_ppo2.py:179][0m |          -0.0052 |          13.3799 |          11.2178 |
[32m[20221213 12:25:34 @agent_ppo2.py:179][0m |          -0.0252 |          11.8341 |          11.2844 |
[32m[20221213 12:25:34 @agent_ppo2.py:179][0m |          -0.0240 |          11.6715 |          11.2508 |
[32m[20221213 12:25:34 @agent_ppo2.py:179][0m |          -0.0301 |          11.6920 |          11.2976 |
[32m[20221213 12:25:34 @agent_ppo2.py:179][0m |          -0.0368 |          11.5175 |          11.3252 |
[32m[20221213 12:25:35 @agent_ppo2.py:179][0m |          -0.0385 |          11.4876 |          11.3595 |
[32m[20221213 12:25:35 @agent_ppo2.py:179][0m |          -0.0326 |          11.5225 |          11.3347 |
[32m[20221213 12:25:35 @agent_ppo2.py:179][0m |          -0.0325 |          11.3882 |          11.3424 |
[32m[20221213 12:25:35 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:25:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 207.07
[32m[20221213 12:25:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 216.90
[32m[20221213 12:25:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 233.85
[32m[20221213 12:25:35 @agent_ppo2.py:137][0m Total time:       9.64 min
[32m[20221213 12:25:35 @agent_ppo2.py:139][0m 638976 total steps have happened
[32m[20221213 12:25:35 @agent_ppo2.py:115][0m #------------------------ Iteration 312 --------------------------#
[32m[20221213 12:25:35 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:25:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:36 @agent_ppo2.py:179][0m |           0.0441 |          10.6771 |          11.1764 |
[32m[20221213 12:25:36 @agent_ppo2.py:179][0m |           0.0316 |          10.2385 |          10.8755 |
[32m[20221213 12:25:36 @agent_ppo2.py:179][0m |          -0.0087 |           9.9885 |          11.2725 |
[32m[20221213 12:25:36 @agent_ppo2.py:179][0m |          -0.0193 |           9.8230 |          11.3126 |
[32m[20221213 12:25:36 @agent_ppo2.py:179][0m |          -0.0202 |           9.6423 |          11.3568 |
[32m[20221213 12:25:36 @agent_ppo2.py:179][0m |          -0.0300 |           9.5720 |          11.4016 |
[32m[20221213 12:25:36 @agent_ppo2.py:179][0m |          -0.0259 |           9.4796 |          11.4317 |
[32m[20221213 12:25:37 @agent_ppo2.py:179][0m |          -0.0305 |           9.4131 |          11.3775 |
[32m[20221213 12:25:37 @agent_ppo2.py:179][0m |          -0.0338 |           9.3626 |          11.4242 |
[32m[20221213 12:25:37 @agent_ppo2.py:179][0m |          -0.0365 |           9.3157 |          11.4452 |
[32m[20221213 12:25:37 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:25:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 190.69
[32m[20221213 12:25:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 225.80
[32m[20221213 12:25:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 225.30
[32m[20221213 12:25:37 @agent_ppo2.py:137][0m Total time:       9.67 min
[32m[20221213 12:25:37 @agent_ppo2.py:139][0m 641024 total steps have happened
[32m[20221213 12:25:37 @agent_ppo2.py:115][0m #------------------------ Iteration 313 --------------------------#
[32m[20221213 12:25:37 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:25:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:37 @agent_ppo2.py:179][0m |           0.0425 |          12.1310 |          11.0187 |
[32m[20221213 12:25:38 @agent_ppo2.py:179][0m |           0.0077 |          11.8154 |          10.9420 |
[32m[20221213 12:25:38 @agent_ppo2.py:179][0m |          -0.0129 |          11.6874 |          11.2555 |
[32m[20221213 12:25:38 @agent_ppo2.py:179][0m |          -0.0244 |          11.6676 |          11.3517 |
[32m[20221213 12:25:38 @agent_ppo2.py:179][0m |          -0.0304 |          11.6144 |          11.4381 |
[32m[20221213 12:25:38 @agent_ppo2.py:179][0m |          -0.0240 |          11.5577 |          11.3844 |
[32m[20221213 12:25:38 @agent_ppo2.py:179][0m |          -0.0213 |          13.4199 |          11.4676 |
[32m[20221213 12:25:38 @agent_ppo2.py:179][0m |          -0.0405 |          11.5728 |          11.4923 |
[32m[20221213 12:25:39 @agent_ppo2.py:179][0m |          -0.0361 |          11.4822 |          11.4993 |
[32m[20221213 12:25:39 @agent_ppo2.py:179][0m |          -0.0404 |          11.4549 |          11.5078 |
[32m[20221213 12:25:39 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:25:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 208.56
[32m[20221213 12:25:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 219.00
[32m[20221213 12:25:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 217.97
[32m[20221213 12:25:39 @agent_ppo2.py:137][0m Total time:       9.70 min
[32m[20221213 12:25:39 @agent_ppo2.py:139][0m 643072 total steps have happened
[32m[20221213 12:25:39 @agent_ppo2.py:115][0m #------------------------ Iteration 314 --------------------------#
[32m[20221213 12:25:39 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:25:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:39 @agent_ppo2.py:179][0m |           0.0373 |          11.7860 |          11.3163 |
[32m[20221213 12:25:39 @agent_ppo2.py:179][0m |           0.0249 |          11.5895 |          10.9849 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |           0.0006 |          11.5346 |          11.2687 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |          -0.0171 |          11.4699 |          11.5030 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |          -0.0228 |          11.4516 |          11.5981 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |          -0.0276 |          11.4186 |          11.6353 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |          -0.0302 |          11.3863 |          11.7169 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |          -0.0304 |          11.3613 |          11.7206 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |          -0.0344 |          11.3752 |          11.7722 |
[32m[20221213 12:25:40 @agent_ppo2.py:179][0m |          -0.0322 |          11.3315 |          11.7729 |
[32m[20221213 12:25:40 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:25:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 219.57
[32m[20221213 12:25:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 221.72
[32m[20221213 12:25:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 229.37
[32m[20221213 12:25:41 @agent_ppo2.py:137][0m Total time:       9.73 min
[32m[20221213 12:25:41 @agent_ppo2.py:139][0m 645120 total steps have happened
[32m[20221213 12:25:41 @agent_ppo2.py:115][0m #------------------------ Iteration 315 --------------------------#
[32m[20221213 12:25:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:25:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:41 @agent_ppo2.py:179][0m |           0.0388 |          11.9094 |          11.2717 |
[32m[20221213 12:25:41 @agent_ppo2.py:179][0m |           0.0197 |          11.8740 |          11.0137 |
[32m[20221213 12:25:41 @agent_ppo2.py:179][0m |          -0.0026 |          11.7407 |          11.2657 |
[32m[20221213 12:25:42 @agent_ppo2.py:179][0m |          -0.0212 |          11.6761 |          11.4253 |
[32m[20221213 12:25:42 @agent_ppo2.py:179][0m |          -0.0246 |          11.6280 |          11.4688 |
[32m[20221213 12:25:42 @agent_ppo2.py:179][0m |          -0.0367 |          11.6174 |          11.5293 |
[32m[20221213 12:25:42 @agent_ppo2.py:179][0m |          -0.0355 |          11.6174 |          11.5850 |
[32m[20221213 12:25:42 @agent_ppo2.py:179][0m |          -0.0369 |          11.5569 |          11.5859 |
[32m[20221213 12:25:42 @agent_ppo2.py:179][0m |          -0.0418 |          11.5306 |          11.5938 |
[32m[20221213 12:25:42 @agent_ppo2.py:179][0m |          -0.0389 |          11.5086 |          11.5860 |
[32m[20221213 12:25:42 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:25:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 201.52
[32m[20221213 12:25:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 217.11
[32m[20221213 12:25:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 238.68
[32m[20221213 12:25:43 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 238.68
[32m[20221213 12:25:43 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 238.68
[32m[20221213 12:25:43 @agent_ppo2.py:137][0m Total time:       9.76 min
[32m[20221213 12:25:43 @agent_ppo2.py:139][0m 647168 total steps have happened
[32m[20221213 12:25:43 @agent_ppo2.py:115][0m #------------------------ Iteration 316 --------------------------#
[32m[20221213 12:25:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:25:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:43 @agent_ppo2.py:179][0m |           0.0937 |          11.9219 |          10.6864 |
[32m[20221213 12:25:43 @agent_ppo2.py:179][0m |           0.0311 |          11.6953 |          10.8263 |
[32m[20221213 12:25:43 @agent_ppo2.py:179][0m |          -0.0053 |          11.6015 |          11.2034 |
[32m[20221213 12:25:43 @agent_ppo2.py:179][0m |          -0.0107 |          13.0067 |          11.3824 |
[32m[20221213 12:25:43 @agent_ppo2.py:179][0m |          -0.0249 |          11.5359 |          11.3990 |
[32m[20221213 12:25:44 @agent_ppo2.py:179][0m |          -0.0337 |          11.4360 |          11.5000 |
[32m[20221213 12:25:44 @agent_ppo2.py:179][0m |          -0.0390 |          11.4393 |          11.5191 |
[32m[20221213 12:25:44 @agent_ppo2.py:179][0m |          -0.0345 |          11.3391 |          11.4671 |
[32m[20221213 12:25:44 @agent_ppo2.py:179][0m |          -0.0407 |          11.3189 |          11.4962 |
[32m[20221213 12:25:44 @agent_ppo2.py:179][0m |          -0.0438 |          11.2951 |          11.5284 |
[32m[20221213 12:25:44 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:25:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.53
[32m[20221213 12:25:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 201.98
[32m[20221213 12:25:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 219.38
[32m[20221213 12:25:44 @agent_ppo2.py:137][0m Total time:       9.79 min
[32m[20221213 12:25:44 @agent_ppo2.py:139][0m 649216 total steps have happened
[32m[20221213 12:25:44 @agent_ppo2.py:115][0m #------------------------ Iteration 317 --------------------------#
[32m[20221213 12:25:45 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:25:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:45 @agent_ppo2.py:179][0m |           0.0529 |          12.1838 |          11.0130 |
[32m[20221213 12:25:45 @agent_ppo2.py:179][0m |           0.0172 |          11.9418 |          10.9790 |
[32m[20221213 12:25:45 @agent_ppo2.py:179][0m |          -0.0024 |          11.8283 |          11.1418 |
[32m[20221213 12:25:45 @agent_ppo2.py:179][0m |          -0.0197 |          11.7664 |          11.3772 |
[32m[20221213 12:25:45 @agent_ppo2.py:179][0m |          -0.0170 |          12.8938 |          11.4644 |
[32m[20221213 12:25:46 @agent_ppo2.py:179][0m |          -0.0317 |          11.7076 |          11.5409 |
[32m[20221213 12:25:46 @agent_ppo2.py:179][0m |          -0.0266 |          11.7281 |          11.5438 |
[32m[20221213 12:25:46 @agent_ppo2.py:179][0m |          -0.0350 |          11.6178 |          11.5653 |
[32m[20221213 12:25:46 @agent_ppo2.py:179][0m |          -0.0364 |          11.5415 |          11.5823 |
[32m[20221213 12:25:46 @agent_ppo2.py:179][0m |          -0.0326 |          11.8849 |          11.6377 |
[32m[20221213 12:25:46 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:25:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 211.69
[32m[20221213 12:25:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 221.93
[32m[20221213 12:25:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 224.33
[32m[20221213 12:25:46 @agent_ppo2.py:137][0m Total time:       9.83 min
[32m[20221213 12:25:46 @agent_ppo2.py:139][0m 651264 total steps have happened
[32m[20221213 12:25:46 @agent_ppo2.py:115][0m #------------------------ Iteration 318 --------------------------#
[32m[20221213 12:25:46 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:25:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:47 @agent_ppo2.py:179][0m |           0.0490 |          12.1822 |          11.1546 |
[32m[20221213 12:25:47 @agent_ppo2.py:179][0m |           0.0392 |          12.0343 |          10.5500 |
[32m[20221213 12:25:47 @agent_ppo2.py:179][0m |           0.0103 |          12.1884 |          11.2887 |
[32m[20221213 12:25:47 @agent_ppo2.py:179][0m |          -0.0062 |          11.9495 |          11.4840 |
[32m[20221213 12:25:47 @agent_ppo2.py:179][0m |          -0.0140 |          12.6918 |          11.7082 |
[32m[20221213 12:25:47 @agent_ppo2.py:179][0m |          -0.0288 |          11.8559 |          11.7557 |
[32m[20221213 12:25:48 @agent_ppo2.py:179][0m |          -0.0304 |          11.8368 |          11.8048 |
[32m[20221213 12:25:48 @agent_ppo2.py:179][0m |          -0.0327 |          11.7932 |          11.8504 |
[32m[20221213 12:25:48 @agent_ppo2.py:179][0m |          -0.0360 |          11.8126 |          11.8396 |
[32m[20221213 12:25:48 @agent_ppo2.py:179][0m |          -0.0347 |          11.7792 |          11.8142 |
[32m[20221213 12:25:48 @agent_ppo2.py:124][0m Policy update time: 1.55 s
[32m[20221213 12:25:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 214.51
[32m[20221213 12:25:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 233.65
[32m[20221213 12:25:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 235.65
[32m[20221213 12:25:48 @agent_ppo2.py:137][0m Total time:       9.86 min
[32m[20221213 12:25:48 @agent_ppo2.py:139][0m 653312 total steps have happened
[32m[20221213 12:25:48 @agent_ppo2.py:115][0m #------------------------ Iteration 319 --------------------------#
[32m[20221213 12:25:48 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:25:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:49 @agent_ppo2.py:179][0m |           0.0529 |          12.4563 |          11.1819 |
[32m[20221213 12:25:49 @agent_ppo2.py:179][0m |           0.0230 |          12.1683 |          10.7140 |
[32m[20221213 12:25:49 @agent_ppo2.py:179][0m |          -0.0033 |          12.0734 |          11.2598 |
[32m[20221213 12:25:49 @agent_ppo2.py:179][0m |          -0.0219 |          11.9632 |          11.3785 |
[32m[20221213 12:25:49 @agent_ppo2.py:179][0m |          -0.0266 |          11.9182 |          11.4929 |
[32m[20221213 12:25:49 @agent_ppo2.py:179][0m |          -0.0312 |          11.8347 |          11.5092 |
[32m[20221213 12:25:50 @agent_ppo2.py:179][0m |          -0.0298 |          11.9349 |          11.5349 |
[32m[20221213 12:25:50 @agent_ppo2.py:179][0m |          -0.0347 |          11.7539 |          11.5555 |
[32m[20221213 12:25:50 @agent_ppo2.py:179][0m |          -0.0347 |          11.7702 |          11.5678 |
[32m[20221213 12:25:50 @agent_ppo2.py:179][0m |          -0.0334 |          12.0414 |          11.5615 |
[32m[20221213 12:25:50 @agent_ppo2.py:124][0m Policy update time: 1.56 s
[32m[20221213 12:25:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 206.73
[32m[20221213 12:25:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 219.21
[32m[20221213 12:25:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 220.15
[32m[20221213 12:25:50 @agent_ppo2.py:137][0m Total time:       9.89 min
[32m[20221213 12:25:50 @agent_ppo2.py:139][0m 655360 total steps have happened
[32m[20221213 12:25:50 @agent_ppo2.py:115][0m #------------------------ Iteration 320 --------------------------#
[32m[20221213 12:25:51 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:25:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:51 @agent_ppo2.py:179][0m |           0.0491 |          12.2845 |          11.2705 |
[32m[20221213 12:25:51 @agent_ppo2.py:179][0m |           0.0083 |          11.9231 |          11.1423 |
[32m[20221213 12:25:51 @agent_ppo2.py:179][0m |          -0.0050 |          11.9519 |          11.4952 |
[32m[20221213 12:25:51 @agent_ppo2.py:179][0m |          -0.0210 |          11.7323 |          11.5693 |
[32m[20221213 12:25:51 @agent_ppo2.py:179][0m |          -0.0300 |          11.6986 |          11.6128 |
[32m[20221213 12:25:52 @agent_ppo2.py:179][0m |          -0.0335 |          11.6712 |          11.6445 |
[32m[20221213 12:25:52 @agent_ppo2.py:179][0m |          -0.0351 |          11.6556 |          11.6612 |
[32m[20221213 12:25:52 @agent_ppo2.py:179][0m |          -0.0375 |          11.6434 |          11.6672 |
[32m[20221213 12:25:52 @agent_ppo2.py:179][0m |          -0.0302 |          11.8005 |          11.6961 |
[32m[20221213 12:25:52 @agent_ppo2.py:179][0m |          -0.0327 |          11.6683 |          11.6030 |
[32m[20221213 12:25:52 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:25:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 208.46
[32m[20221213 12:25:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 226.44
[32m[20221213 12:25:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 225.26
[32m[20221213 12:25:52 @agent_ppo2.py:137][0m Total time:       9.93 min
[32m[20221213 12:25:52 @agent_ppo2.py:139][0m 657408 total steps have happened
[32m[20221213 12:25:52 @agent_ppo2.py:115][0m #------------------------ Iteration 321 --------------------------#
[32m[20221213 12:25:53 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:25:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:53 @agent_ppo2.py:179][0m |           0.0595 |          12.5683 |          10.8300 |
[32m[20221213 12:25:53 @agent_ppo2.py:179][0m |           0.0139 |          12.2935 |          10.9296 |
[32m[20221213 12:25:53 @agent_ppo2.py:179][0m |          -0.0062 |          12.6714 |          11.3512 |
[32m[20221213 12:25:53 @agent_ppo2.py:179][0m |          -0.0153 |          12.2089 |          11.4200 |
[32m[20221213 12:25:53 @agent_ppo2.py:179][0m |          -0.0267 |          12.1612 |          11.5365 |
[32m[20221213 12:25:54 @agent_ppo2.py:179][0m |          -0.0260 |          12.3128 |          11.5451 |
[32m[20221213 12:25:54 @agent_ppo2.py:179][0m |          -0.0325 |          12.1527 |          11.5424 |
[32m[20221213 12:25:54 @agent_ppo2.py:179][0m |          -0.0318 |          12.1298 |          11.5620 |
[32m[20221213 12:25:54 @agent_ppo2.py:179][0m |          -0.0367 |          12.0937 |          11.5788 |
[32m[20221213 12:25:54 @agent_ppo2.py:179][0m |          -0.0402 |          12.0477 |          11.6016 |
[32m[20221213 12:25:54 @agent_ppo2.py:124][0m Policy update time: 1.57 s
[32m[20221213 12:25:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 211.83
[32m[20221213 12:25:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 224.86
[32m[20221213 12:25:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 243.66
[32m[20221213 12:25:54 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 243.66
[32m[20221213 12:25:54 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 243.66
[32m[20221213 12:25:54 @agent_ppo2.py:137][0m Total time:       9.96 min
[32m[20221213 12:25:54 @agent_ppo2.py:139][0m 659456 total steps have happened
[32m[20221213 12:25:54 @agent_ppo2.py:115][0m #------------------------ Iteration 322 --------------------------#
[32m[20221213 12:25:55 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:25:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:55 @agent_ppo2.py:179][0m |           0.0548 |          11.2932 |          11.3434 |
[32m[20221213 12:25:55 @agent_ppo2.py:179][0m |           0.0188 |          10.1013 |          11.1991 |
[32m[20221213 12:25:55 @agent_ppo2.py:179][0m |          -0.0167 |           9.7952 |          11.4770 |
[32m[20221213 12:25:55 @agent_ppo2.py:179][0m |          -0.0265 |           9.6405 |          11.6102 |
[32m[20221213 12:25:55 @agent_ppo2.py:179][0m |          -0.0237 |           9.5915 |          11.6733 |
[32m[20221213 12:25:55 @agent_ppo2.py:179][0m |          -0.0367 |           9.5402 |          11.6515 |
[32m[20221213 12:25:56 @agent_ppo2.py:179][0m |          -0.0283 |           9.5216 |          11.6858 |
[32m[20221213 12:25:56 @agent_ppo2.py:179][0m |          -0.0398 |           9.4901 |          11.6810 |
[32m[20221213 12:25:56 @agent_ppo2.py:179][0m |          -0.0327 |           9.4485 |          11.6718 |
[32m[20221213 12:25:56 @agent_ppo2.py:179][0m |          -0.0438 |           9.4218 |          11.7082 |
[32m[20221213 12:25:56 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:25:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 198.63
[32m[20221213 12:25:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 209.08
[32m[20221213 12:25:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 243.33
[32m[20221213 12:25:56 @agent_ppo2.py:137][0m Total time:       9.99 min
[32m[20221213 12:25:56 @agent_ppo2.py:139][0m 661504 total steps have happened
[32m[20221213 12:25:56 @agent_ppo2.py:115][0m #------------------------ Iteration 323 --------------------------#
[32m[20221213 12:25:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:25:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:57 @agent_ppo2.py:179][0m |           0.0466 |          13.6912 |          11.3740 |
[32m[20221213 12:25:57 @agent_ppo2.py:179][0m |           0.0071 |          12.1399 |          11.1958 |
[32m[20221213 12:25:57 @agent_ppo2.py:179][0m |          -0.0195 |          11.8673 |          11.4965 |
[32m[20221213 12:25:57 @agent_ppo2.py:179][0m |          -0.0290 |          11.6771 |          11.6731 |
[32m[20221213 12:25:57 @agent_ppo2.py:179][0m |          -0.0286 |          12.3010 |          11.7179 |
[32m[20221213 12:25:57 @agent_ppo2.py:179][0m |          -0.0383 |          11.6057 |          11.7626 |
[32m[20221213 12:25:57 @agent_ppo2.py:179][0m |          -0.0389 |          11.7221 |          11.8158 |
[32m[20221213 12:25:58 @agent_ppo2.py:179][0m |          -0.0445 |          11.2282 |          11.7908 |
[32m[20221213 12:25:58 @agent_ppo2.py:179][0m |          -0.0452 |          11.0968 |          11.8215 |
[32m[20221213 12:25:58 @agent_ppo2.py:179][0m |          -0.0410 |          11.9793 |          11.8766 |
[32m[20221213 12:25:58 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:25:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 210.49
[32m[20221213 12:25:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 226.17
[32m[20221213 12:25:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 236.06
[32m[20221213 12:25:58 @agent_ppo2.py:137][0m Total time:      10.02 min
[32m[20221213 12:25:58 @agent_ppo2.py:139][0m 663552 total steps have happened
[32m[20221213 12:25:58 @agent_ppo2.py:115][0m #------------------------ Iteration 324 --------------------------#
[32m[20221213 12:25:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:25:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:25:58 @agent_ppo2.py:179][0m |           0.0595 |          13.3504 |          11.2533 |
[32m[20221213 12:25:59 @agent_ppo2.py:179][0m |           0.0335 |          12.8017 |          10.8268 |
[32m[20221213 12:25:59 @agent_ppo2.py:179][0m |          -0.0029 |          12.5890 |          11.3351 |
[32m[20221213 12:25:59 @agent_ppo2.py:179][0m |          -0.0225 |          12.4381 |          11.6314 |
[32m[20221213 12:25:59 @agent_ppo2.py:179][0m |          -0.0351 |          12.2224 |          11.6950 |
[32m[20221213 12:25:59 @agent_ppo2.py:179][0m |          -0.0369 |          12.1446 |          11.7775 |
[32m[20221213 12:25:59 @agent_ppo2.py:179][0m |          -0.0405 |          12.0038 |          11.7825 |
[32m[20221213 12:25:59 @agent_ppo2.py:179][0m |          -0.0447 |          11.8884 |          11.8181 |
[32m[20221213 12:26:00 @agent_ppo2.py:179][0m |          -0.0442 |          11.8532 |          11.8066 |
[32m[20221213 12:26:00 @agent_ppo2.py:179][0m |          -0.0474 |          11.7594 |          11.8216 |
[32m[20221213 12:26:00 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:26:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 221.35
[32m[20221213 12:26:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 229.40
[32m[20221213 12:26:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 232.27
[32m[20221213 12:26:00 @agent_ppo2.py:137][0m Total time:      10.05 min
[32m[20221213 12:26:00 @agent_ppo2.py:139][0m 665600 total steps have happened
[32m[20221213 12:26:00 @agent_ppo2.py:115][0m #------------------------ Iteration 325 --------------------------#
[32m[20221213 12:26:00 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:00 @agent_ppo2.py:179][0m |           0.0374 |          11.5403 |          11.4427 |
[32m[20221213 12:26:00 @agent_ppo2.py:179][0m |           0.0181 |          10.8240 |          11.2634 |
[32m[20221213 12:26:01 @agent_ppo2.py:179][0m |          -0.0040 |          10.7236 |          11.5513 |
[32m[20221213 12:26:01 @agent_ppo2.py:179][0m |          -0.0195 |          10.5095 |          11.6176 |
[32m[20221213 12:26:01 @agent_ppo2.py:179][0m |          -0.0281 |          10.4237 |          11.6581 |
[32m[20221213 12:26:01 @agent_ppo2.py:179][0m |          -0.0327 |          10.4832 |          11.7331 |
[32m[20221213 12:26:01 @agent_ppo2.py:179][0m |          -0.0319 |          10.2877 |          11.7286 |
[32m[20221213 12:26:01 @agent_ppo2.py:179][0m |          -0.0377 |          10.3040 |          11.7104 |
[32m[20221213 12:26:01 @agent_ppo2.py:179][0m |          -0.0369 |          10.2234 |          11.7581 |
[32m[20221213 12:26:02 @agent_ppo2.py:179][0m |          -0.0433 |          10.1575 |          11.7871 |
[32m[20221213 12:26:02 @agent_ppo2.py:124][0m Policy update time: 1.56 s
[32m[20221213 12:26:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.92
[32m[20221213 12:26:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 229.77
[32m[20221213 12:26:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.06
[32m[20221213 12:26:02 @agent_ppo2.py:137][0m Total time:      10.09 min
[32m[20221213 12:26:02 @agent_ppo2.py:139][0m 667648 total steps have happened
[32m[20221213 12:26:02 @agent_ppo2.py:115][0m #------------------------ Iteration 326 --------------------------#
[32m[20221213 12:26:02 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:02 @agent_ppo2.py:179][0m |           0.0501 |          14.6012 |          11.4888 |
[32m[20221213 12:26:02 @agent_ppo2.py:179][0m |           0.0080 |          14.2212 |          11.4084 |
[32m[20221213 12:26:03 @agent_ppo2.py:179][0m |          -0.0095 |          14.1056 |          11.6132 |
[32m[20221213 12:26:03 @agent_ppo2.py:179][0m |          -0.0259 |          14.0493 |          11.6966 |
[32m[20221213 12:26:03 @agent_ppo2.py:179][0m |          -0.0348 |          13.9966 |          11.7998 |
[32m[20221213 12:26:03 @agent_ppo2.py:179][0m |          -0.0359 |          13.9358 |          11.8126 |
[32m[20221213 12:26:03 @agent_ppo2.py:179][0m |          -0.0403 |          14.0132 |          11.8433 |
[32m[20221213 12:26:03 @agent_ppo2.py:179][0m |          -0.0441 |          13.8658 |          11.8657 |
[32m[20221213 12:26:03 @agent_ppo2.py:179][0m |          -0.0442 |          13.8921 |          11.8703 |
[32m[20221213 12:26:04 @agent_ppo2.py:179][0m |          -0.0529 |          13.8485 |          11.9199 |
[32m[20221213 12:26:04 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:26:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 203.71
[32m[20221213 12:26:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 225.09
[32m[20221213 12:26:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 219.83
[32m[20221213 12:26:04 @agent_ppo2.py:137][0m Total time:      10.12 min
[32m[20221213 12:26:04 @agent_ppo2.py:139][0m 669696 total steps have happened
[32m[20221213 12:26:04 @agent_ppo2.py:115][0m #------------------------ Iteration 327 --------------------------#
[32m[20221213 12:26:04 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:04 @agent_ppo2.py:179][0m |           0.0361 |          13.1075 |          11.5931 |
[32m[20221213 12:26:04 @agent_ppo2.py:179][0m |           0.0046 |          12.7067 |          11.5304 |
[32m[20221213 12:26:05 @agent_ppo2.py:179][0m |          -0.0134 |          13.0618 |          11.7399 |
[32m[20221213 12:26:05 @agent_ppo2.py:179][0m |          -0.0244 |          12.5443 |          11.7316 |
[32m[20221213 12:26:05 @agent_ppo2.py:179][0m |          -0.0352 |          12.4773 |          11.8209 |
[32m[20221213 12:26:05 @agent_ppo2.py:179][0m |          -0.0378 |          12.8534 |          11.8686 |
[32m[20221213 12:26:05 @agent_ppo2.py:179][0m |          -0.0439 |          12.4069 |          11.8873 |
[32m[20221213 12:26:05 @agent_ppo2.py:179][0m |          -0.0402 |          12.3555 |          11.9056 |
[32m[20221213 12:26:06 @agent_ppo2.py:179][0m |          -0.0452 |          12.2890 |          11.8764 |
[32m[20221213 12:26:06 @agent_ppo2.py:179][0m |          -0.0438 |          12.2529 |          11.8880 |
[32m[20221213 12:26:06 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:26:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 206.23
[32m[20221213 12:26:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 217.68
[32m[20221213 12:26:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 251.79
[32m[20221213 12:26:06 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 251.79
[32m[20221213 12:26:06 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 251.79
[32m[20221213 12:26:06 @agent_ppo2.py:137][0m Total time:      10.15 min
[32m[20221213 12:26:06 @agent_ppo2.py:139][0m 671744 total steps have happened
[32m[20221213 12:26:06 @agent_ppo2.py:115][0m #------------------------ Iteration 328 --------------------------#
[32m[20221213 12:26:06 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:06 @agent_ppo2.py:179][0m |           0.0440 |          13.5162 |          11.5590 |
[32m[20221213 12:26:06 @agent_ppo2.py:179][0m |           0.0218 |          13.3727 |          11.4918 |
[32m[20221213 12:26:07 @agent_ppo2.py:179][0m |          -0.0084 |          13.2314 |          11.6148 |
[32m[20221213 12:26:07 @agent_ppo2.py:179][0m |          -0.0270 |          13.1502 |          11.8042 |
[32m[20221213 12:26:07 @agent_ppo2.py:179][0m |          -0.0286 |          13.8607 |          11.8395 |
[32m[20221213 12:26:07 @agent_ppo2.py:179][0m |          -0.0361 |          13.0430 |          11.8503 |
[32m[20221213 12:26:07 @agent_ppo2.py:179][0m |          -0.0380 |          13.0425 |          11.8687 |
[32m[20221213 12:26:07 @agent_ppo2.py:179][0m |          -0.0407 |          12.9640 |          11.8619 |
[32m[20221213 12:26:07 @agent_ppo2.py:179][0m |          -0.0428 |          12.9338 |          11.8698 |
[32m[20221213 12:26:08 @agent_ppo2.py:179][0m |          -0.0443 |          12.8996 |          11.9121 |
[32m[20221213 12:26:08 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:26:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 198.59
[32m[20221213 12:26:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 219.52
[32m[20221213 12:26:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 248.29
[32m[20221213 12:26:08 @agent_ppo2.py:137][0m Total time:      10.18 min
[32m[20221213 12:26:08 @agent_ppo2.py:139][0m 673792 total steps have happened
[32m[20221213 12:26:08 @agent_ppo2.py:115][0m #------------------------ Iteration 329 --------------------------#
[32m[20221213 12:26:08 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:08 @agent_ppo2.py:179][0m |           0.0536 |          13.4943 |          11.6943 |
[32m[20221213 12:26:08 @agent_ppo2.py:179][0m |           0.0909 |          13.2919 |          10.2261 |
[32m[20221213 12:26:08 @agent_ppo2.py:179][0m |           0.0056 |          13.2005 |          11.5703 |
[32m[20221213 12:26:09 @agent_ppo2.py:179][0m |          -0.0093 |          13.2900 |          11.8542 |
[32m[20221213 12:26:09 @agent_ppo2.py:179][0m |          -0.0256 |          13.0909 |          11.9824 |
[32m[20221213 12:26:09 @agent_ppo2.py:179][0m |          -0.0263 |          13.2496 |          12.0685 |
[32m[20221213 12:26:09 @agent_ppo2.py:179][0m |          -0.0327 |          13.0232 |          12.0873 |
[32m[20221213 12:26:09 @agent_ppo2.py:179][0m |          -0.0369 |          13.0424 |          12.1013 |
[32m[20221213 12:26:09 @agent_ppo2.py:179][0m |          -0.0352 |          13.2285 |          12.1141 |
[32m[20221213 12:26:09 @agent_ppo2.py:179][0m |          -0.0394 |          13.0109 |          12.1285 |
[32m[20221213 12:26:09 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:26:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 212.04
[32m[20221213 12:26:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 231.69
[32m[20221213 12:26:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 241.11
[32m[20221213 12:26:10 @agent_ppo2.py:137][0m Total time:      10.21 min
[32m[20221213 12:26:10 @agent_ppo2.py:139][0m 675840 total steps have happened
[32m[20221213 12:26:10 @agent_ppo2.py:115][0m #------------------------ Iteration 330 --------------------------#
[32m[20221213 12:26:10 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:26:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:10 @agent_ppo2.py:179][0m |           0.0358 |          12.8286 |          11.5220 |
[32m[20221213 12:26:10 @agent_ppo2.py:179][0m |           0.0125 |          12.6518 |          11.4123 |
[32m[20221213 12:26:10 @agent_ppo2.py:179][0m |          -0.0108 |          12.5553 |          11.6656 |
[32m[20221213 12:26:10 @agent_ppo2.py:179][0m |          -0.0208 |          12.4783 |          11.7850 |
[32m[20221213 12:26:11 @agent_ppo2.py:179][0m |          -0.0280 |          12.4587 |          11.9015 |
[32m[20221213 12:26:11 @agent_ppo2.py:179][0m |          -0.0299 |          12.3620 |          11.9150 |
[32m[20221213 12:26:11 @agent_ppo2.py:179][0m |          -0.0291 |          12.3431 |          11.9184 |
[32m[20221213 12:26:11 @agent_ppo2.py:179][0m |          -0.0342 |          12.2527 |          11.9238 |
[32m[20221213 12:26:11 @agent_ppo2.py:179][0m |          -0.0384 |          12.1984 |          11.9817 |
[32m[20221213 12:26:11 @agent_ppo2.py:179][0m |          -0.0232 |          14.0437 |          11.9432 |
[32m[20221213 12:26:11 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:26:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 220.29
[32m[20221213 12:26:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 241.89
[32m[20221213 12:26:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 242.78
[32m[20221213 12:26:11 @agent_ppo2.py:137][0m Total time:      10.24 min
[32m[20221213 12:26:11 @agent_ppo2.py:139][0m 677888 total steps have happened
[32m[20221213 12:26:11 @agent_ppo2.py:115][0m #------------------------ Iteration 331 --------------------------#
[32m[20221213 12:26:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:12 @agent_ppo2.py:179][0m |           0.0628 |          13.2476 |          11.5443 |
[32m[20221213 12:26:12 @agent_ppo2.py:179][0m |           0.0274 |          12.6415 |          10.8299 |
[32m[20221213 12:26:12 @agent_ppo2.py:179][0m |          -0.0027 |          12.3824 |          11.5399 |
[32m[20221213 12:26:12 @agent_ppo2.py:179][0m |          -0.0188 |          12.1460 |          11.7431 |
[32m[20221213 12:26:12 @agent_ppo2.py:179][0m |          -0.0200 |          12.3202 |          11.8735 |
[32m[20221213 12:26:12 @agent_ppo2.py:179][0m |          -0.0279 |          11.8784 |          11.8932 |
[32m[20221213 12:26:13 @agent_ppo2.py:179][0m |          -0.0285 |          11.9300 |          11.9351 |
[32m[20221213 12:26:13 @agent_ppo2.py:179][0m |          -0.0372 |          11.7299 |          12.0076 |
[32m[20221213 12:26:13 @agent_ppo2.py:179][0m |          -0.0402 |          11.6379 |          12.0232 |
[32m[20221213 12:26:13 @agent_ppo2.py:179][0m |          -0.0426 |          11.5260 |          12.0702 |
[32m[20221213 12:26:13 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:26:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 207.45
[32m[20221213 12:26:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 214.46
[32m[20221213 12:26:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 226.61
[32m[20221213 12:26:13 @agent_ppo2.py:137][0m Total time:      10.27 min
[32m[20221213 12:26:13 @agent_ppo2.py:139][0m 679936 total steps have happened
[32m[20221213 12:26:13 @agent_ppo2.py:115][0m #------------------------ Iteration 332 --------------------------#
[32m[20221213 12:26:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |           0.0948 |          12.2640 |          11.0509 |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |           0.0474 |          11.4112 |          10.4579 |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |          -0.0018 |          11.1421 |          11.3972 |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |          -0.0174 |          11.0289 |          11.6502 |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |          -0.0194 |          11.1616 |          11.7512 |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |          -0.0243 |          10.9335 |          11.7844 |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |          -0.0237 |          11.6294 |          11.8798 |
[32m[20221213 12:26:14 @agent_ppo2.py:179][0m |          -0.0251 |          10.8733 |          11.7597 |
[32m[20221213 12:26:15 @agent_ppo2.py:179][0m |          -0.0294 |          10.8290 |          11.8689 |
[32m[20221213 12:26:15 @agent_ppo2.py:179][0m |          -0.0361 |          10.8048 |          11.9180 |
[32m[20221213 12:26:15 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:26:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.44
[32m[20221213 12:26:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 234.25
[32m[20221213 12:26:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.19
[32m[20221213 12:26:15 @agent_ppo2.py:137][0m Total time:      10.30 min
[32m[20221213 12:26:15 @agent_ppo2.py:139][0m 681984 total steps have happened
[32m[20221213 12:26:15 @agent_ppo2.py:115][0m #------------------------ Iteration 333 --------------------------#
[32m[20221213 12:26:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:15 @agent_ppo2.py:179][0m |           0.0421 |          15.7648 |          11.6095 |
[32m[20221213 12:26:15 @agent_ppo2.py:179][0m |           0.0112 |          14.5865 |          11.3807 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0199 |          14.4218 |          11.6050 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0298 |          14.3227 |          11.7328 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0327 |          14.3570 |          11.7847 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0396 |          14.2198 |          11.8178 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0391 |          14.1794 |          11.8052 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0400 |          14.1291 |          11.8864 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0385 |          14.2006 |          11.8614 |
[32m[20221213 12:26:16 @agent_ppo2.py:179][0m |          -0.0339 |          14.2420 |          11.7948 |
[32m[20221213 12:26:16 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:26:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 213.68
[32m[20221213 12:26:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 218.57
[32m[20221213 12:26:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 208.52
[32m[20221213 12:26:17 @agent_ppo2.py:137][0m Total time:      10.33 min
[32m[20221213 12:26:17 @agent_ppo2.py:139][0m 684032 total steps have happened
[32m[20221213 12:26:17 @agent_ppo2.py:115][0m #------------------------ Iteration 334 --------------------------#
[32m[20221213 12:26:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:17 @agent_ppo2.py:179][0m |           0.0581 |          14.1437 |          11.0973 |
[32m[20221213 12:26:17 @agent_ppo2.py:179][0m |           0.0297 |          13.5565 |          10.8401 |
[32m[20221213 12:26:17 @agent_ppo2.py:179][0m |          -0.0073 |          13.4210 |          11.2068 |
[32m[20221213 12:26:17 @agent_ppo2.py:179][0m |          -0.0196 |          13.5929 |          11.3668 |
[32m[20221213 12:26:18 @agent_ppo2.py:179][0m |          -0.0187 |          13.9120 |          11.3798 |
[32m[20221213 12:26:18 @agent_ppo2.py:179][0m |          -0.0144 |          13.2668 |          11.2593 |
[32m[20221213 12:26:18 @agent_ppo2.py:179][0m |          -0.0289 |          13.3135 |          11.4631 |
[32m[20221213 12:26:18 @agent_ppo2.py:179][0m |          -0.0265 |          14.3877 |          11.5221 |
[32m[20221213 12:26:18 @agent_ppo2.py:179][0m |          -0.0372 |          13.5147 |          11.5603 |
[32m[20221213 12:26:18 @agent_ppo2.py:179][0m |          -0.0288 |          13.1480 |          11.3832 |
[32m[20221213 12:26:18 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:26:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 213.96
[32m[20221213 12:26:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 224.85
[32m[20221213 12:26:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 246.59
[32m[20221213 12:26:18 @agent_ppo2.py:137][0m Total time:      10.36 min
[32m[20221213 12:26:18 @agent_ppo2.py:139][0m 686080 total steps have happened
[32m[20221213 12:26:18 @agent_ppo2.py:115][0m #------------------------ Iteration 335 --------------------------#
[32m[20221213 12:26:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:19 @agent_ppo2.py:179][0m |           0.0446 |          14.7668 |          11.4780 |
[32m[20221213 12:26:19 @agent_ppo2.py:179][0m |           0.0138 |          13.4112 |          11.2019 |
[32m[20221213 12:26:19 @agent_ppo2.py:179][0m |          -0.0074 |          13.2725 |          11.3676 |
[32m[20221213 12:26:19 @agent_ppo2.py:179][0m |          -0.0215 |          13.1643 |          11.6497 |
[32m[20221213 12:26:19 @agent_ppo2.py:179][0m |          -0.0253 |          13.1248 |          11.6913 |
[32m[20221213 12:26:19 @agent_ppo2.py:179][0m |          -0.0330 |          13.0657 |          11.7395 |
[32m[20221213 12:26:20 @agent_ppo2.py:179][0m |          -0.0240 |          14.8958 |          11.7787 |
[32m[20221213 12:26:20 @agent_ppo2.py:179][0m |          -0.0394 |          12.9825 |          11.7947 |
[32m[20221213 12:26:20 @agent_ppo2.py:179][0m |          -0.0425 |          12.9142 |          11.8439 |
[32m[20221213 12:26:20 @agent_ppo2.py:179][0m |          -0.0446 |          12.8884 |          11.8581 |
[32m[20221213 12:26:20 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:26:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 207.41
[32m[20221213 12:26:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 215.79
[32m[20221213 12:26:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 240.16
[32m[20221213 12:26:20 @agent_ppo2.py:137][0m Total time:      10.39 min
[32m[20221213 12:26:20 @agent_ppo2.py:139][0m 688128 total steps have happened
[32m[20221213 12:26:20 @agent_ppo2.py:115][0m #------------------------ Iteration 336 --------------------------#
[32m[20221213 12:26:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |           0.0228 |          13.7623 |          11.6998 |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |           0.0111 |          13.6323 |          11.4579 |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |           0.0290 |          14.4535 |          11.4354 |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |          -0.0103 |          13.5477 |          11.5370 |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |          -0.0246 |          13.4779 |          11.7442 |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |          -0.0276 |          14.1021 |          11.8318 |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |          -0.0305 |          13.6865 |          11.8347 |
[32m[20221213 12:26:21 @agent_ppo2.py:179][0m |          -0.0386 |          13.4137 |          11.8684 |
[32m[20221213 12:26:22 @agent_ppo2.py:179][0m |          -0.0293 |          14.3197 |          11.8899 |
[32m[20221213 12:26:22 @agent_ppo2.py:179][0m |          -0.0388 |          13.3721 |          11.8748 |
[32m[20221213 12:26:22 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:26:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 213.89
[32m[20221213 12:26:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 232.63
[32m[20221213 12:26:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 246.34
[32m[20221213 12:26:22 @agent_ppo2.py:137][0m Total time:      10.42 min
[32m[20221213 12:26:22 @agent_ppo2.py:139][0m 690176 total steps have happened
[32m[20221213 12:26:22 @agent_ppo2.py:115][0m #------------------------ Iteration 337 --------------------------#
[32m[20221213 12:26:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:22 @agent_ppo2.py:179][0m |           0.0226 |          13.0036 |          11.4086 |
[32m[20221213 12:26:22 @agent_ppo2.py:179][0m |           0.0292 |          12.7825 |          11.0733 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0077 |          12.7539 |          11.3451 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0205 |          12.6731 |          11.5531 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0258 |          12.6567 |          11.6132 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0275 |          12.7473 |          11.6303 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0311 |          12.5947 |          11.6295 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0368 |          12.5365 |          11.6627 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0362 |          12.4931 |          11.6609 |
[32m[20221213 12:26:23 @agent_ppo2.py:179][0m |          -0.0374 |          12.4775 |          11.6838 |
[32m[20221213 12:26:23 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:26:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 222.85
[32m[20221213 12:26:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 227.88
[32m[20221213 12:26:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 236.81
[32m[20221213 12:26:24 @agent_ppo2.py:137][0m Total time:      10.45 min
[32m[20221213 12:26:24 @agent_ppo2.py:139][0m 692224 total steps have happened
[32m[20221213 12:26:24 @agent_ppo2.py:115][0m #------------------------ Iteration 338 --------------------------#
[32m[20221213 12:26:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:24 @agent_ppo2.py:179][0m |           0.0604 |          13.0253 |          10.9536 |
[32m[20221213 12:26:24 @agent_ppo2.py:179][0m |           0.0363 |          12.4916 |          10.8458 |
[32m[20221213 12:26:24 @agent_ppo2.py:179][0m |          -0.0126 |          12.1374 |          11.2038 |
[32m[20221213 12:26:24 @agent_ppo2.py:179][0m |          -0.0189 |          11.9307 |          11.2744 |
[32m[20221213 12:26:25 @agent_ppo2.py:179][0m |          -0.0239 |          11.7785 |          11.3493 |
[32m[20221213 12:26:25 @agent_ppo2.py:179][0m |          -0.0290 |          11.7087 |          11.3775 |
[32m[20221213 12:26:25 @agent_ppo2.py:179][0m |          -0.0267 |          11.8437 |          11.4510 |
[32m[20221213 12:26:25 @agent_ppo2.py:179][0m |          -0.0318 |          11.4024 |          11.4426 |
[32m[20221213 12:26:25 @agent_ppo2.py:179][0m |          -0.0400 |          11.3409 |          11.4350 |
[32m[20221213 12:26:25 @agent_ppo2.py:179][0m |          -0.0359 |          11.2217 |          11.4409 |
[32m[20221213 12:26:25 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:26:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 217.19
[32m[20221213 12:26:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 229.44
[32m[20221213 12:26:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.08
[32m[20221213 12:26:25 @agent_ppo2.py:137][0m Total time:      10.48 min
[32m[20221213 12:26:25 @agent_ppo2.py:139][0m 694272 total steps have happened
[32m[20221213 12:26:25 @agent_ppo2.py:115][0m #------------------------ Iteration 339 --------------------------#
[32m[20221213 12:26:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:26 @agent_ppo2.py:179][0m |           0.0223 |          15.3223 |          11.3657 |
[32m[20221213 12:26:26 @agent_ppo2.py:179][0m |          -0.0043 |          14.7204 |          11.2802 |
[32m[20221213 12:26:26 @agent_ppo2.py:179][0m |          -0.0152 |          14.5461 |          11.3571 |
[32m[20221213 12:26:26 @agent_ppo2.py:179][0m |          -0.0246 |          15.5389 |          11.4348 |
[32m[20221213 12:26:26 @agent_ppo2.py:179][0m |          -0.0384 |          14.5019 |          11.4460 |
[32m[20221213 12:26:26 @agent_ppo2.py:179][0m |          -0.0380 |          15.1458 |          11.4741 |
[32m[20221213 12:26:27 @agent_ppo2.py:179][0m |          -0.0490 |          14.2532 |          11.4878 |
[32m[20221213 12:26:27 @agent_ppo2.py:179][0m |          -0.0445 |          14.5259 |          11.4853 |
[32m[20221213 12:26:27 @agent_ppo2.py:179][0m |          -0.0475 |          14.1703 |          11.4617 |
[32m[20221213 12:26:27 @agent_ppo2.py:179][0m |          -0.0564 |          14.0953 |          11.4866 |
[32m[20221213 12:26:27 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:26:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 214.43
[32m[20221213 12:26:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 236.34
[32m[20221213 12:26:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 213.83
[32m[20221213 12:26:27 @agent_ppo2.py:137][0m Total time:      10.51 min
[32m[20221213 12:26:27 @agent_ppo2.py:139][0m 696320 total steps have happened
[32m[20221213 12:26:27 @agent_ppo2.py:115][0m #------------------------ Iteration 340 --------------------------#
[32m[20221213 12:26:27 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:26:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:28 @agent_ppo2.py:179][0m |           0.0465 |          14.7331 |          11.2025 |
[32m[20221213 12:26:28 @agent_ppo2.py:179][0m |           0.0256 |          14.5284 |          11.1060 |
[32m[20221213 12:26:28 @agent_ppo2.py:179][0m |          -0.0035 |          14.7442 |          11.3693 |
[32m[20221213 12:26:28 @agent_ppo2.py:179][0m |          -0.0097 |          15.6285 |          11.4137 |
[32m[20221213 12:26:28 @agent_ppo2.py:179][0m |          -0.0297 |          14.2654 |          11.4654 |
[32m[20221213 12:26:28 @agent_ppo2.py:179][0m |          -0.0213 |          14.1473 |          11.3675 |
[32m[20221213 12:26:28 @agent_ppo2.py:179][0m |          -0.0291 |          14.0810 |          11.4686 |
[32m[20221213 12:26:29 @agent_ppo2.py:179][0m |          -0.0360 |          14.0261 |          11.4864 |
[32m[20221213 12:26:29 @agent_ppo2.py:179][0m |          -0.0380 |          14.3296 |          11.5247 |
[32m[20221213 12:26:29 @agent_ppo2.py:179][0m |          -0.0418 |          14.0033 |          11.5411 |
[32m[20221213 12:26:29 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:26:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 210.70
[32m[20221213 12:26:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 228.21
[32m[20221213 12:26:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 212.59
[32m[20221213 12:26:29 @agent_ppo2.py:137][0m Total time:      10.54 min
[32m[20221213 12:26:29 @agent_ppo2.py:139][0m 698368 total steps have happened
[32m[20221213 12:26:29 @agent_ppo2.py:115][0m #------------------------ Iteration 341 --------------------------#
[32m[20221213 12:26:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:29 @agent_ppo2.py:179][0m |           0.0489 |          14.6881 |          11.2693 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |           0.0098 |          14.0704 |          11.3185 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |          -0.0143 |          13.8004 |          11.5114 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |          -0.0233 |          13.7318 |          11.5428 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |          -0.0371 |          13.5458 |          11.6132 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |          -0.0375 |          13.5098 |          11.6754 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |          -0.0370 |          13.3370 |          11.6252 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |          -0.0331 |          14.1616 |          11.6745 |
[32m[20221213 12:26:30 @agent_ppo2.py:179][0m |          -0.0229 |          13.3591 |          11.4903 |
[32m[20221213 12:26:31 @agent_ppo2.py:179][0m |          -0.0373 |          13.1604 |          11.5120 |
[32m[20221213 12:26:31 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:26:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 208.06
[32m[20221213 12:26:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 216.90
[32m[20221213 12:26:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.64
[32m[20221213 12:26:31 @agent_ppo2.py:137][0m Total time:      10.57 min
[32m[20221213 12:26:31 @agent_ppo2.py:139][0m 700416 total steps have happened
[32m[20221213 12:26:31 @agent_ppo2.py:115][0m #------------------------ Iteration 342 --------------------------#
[32m[20221213 12:26:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:31 @agent_ppo2.py:179][0m |           0.0304 |          14.4616 |          11.4543 |
[32m[20221213 12:26:31 @agent_ppo2.py:179][0m |          -0.0022 |          13.9365 |          11.3962 |
[32m[20221213 12:26:31 @agent_ppo2.py:179][0m |          -0.0169 |          13.7033 |          11.4556 |
[32m[20221213 12:26:32 @agent_ppo2.py:179][0m |          -0.0292 |          13.6390 |          11.6019 |
[32m[20221213 12:26:32 @agent_ppo2.py:179][0m |          -0.0345 |          13.5457 |          11.6359 |
[32m[20221213 12:26:32 @agent_ppo2.py:179][0m |          -0.0386 |          13.4810 |          11.6795 |
[32m[20221213 12:26:32 @agent_ppo2.py:179][0m |          -0.0328 |          13.8398 |          11.6564 |
[32m[20221213 12:26:32 @agent_ppo2.py:179][0m |          -0.0418 |          13.5042 |          11.6700 |
[32m[20221213 12:26:32 @agent_ppo2.py:179][0m |          -0.0465 |          13.3852 |          11.7114 |
[32m[20221213 12:26:32 @agent_ppo2.py:179][0m |          -0.0402 |          13.4402 |          11.6911 |
[32m[20221213 12:26:32 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:26:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 211.71
[32m[20221213 12:26:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 217.17
[32m[20221213 12:26:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 241.80
[32m[20221213 12:26:33 @agent_ppo2.py:137][0m Total time:      10.60 min
[32m[20221213 12:26:33 @agent_ppo2.py:139][0m 702464 total steps have happened
[32m[20221213 12:26:33 @agent_ppo2.py:115][0m #------------------------ Iteration 343 --------------------------#
[32m[20221213 12:26:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:33 @agent_ppo2.py:179][0m |           0.0648 |          14.2057 |          11.0584 |
[32m[20221213 12:26:33 @agent_ppo2.py:179][0m |           0.0521 |          14.0212 |          10.5490 |
[32m[20221213 12:26:33 @agent_ppo2.py:179][0m |           0.0006 |          14.3440 |          11.1670 |
[32m[20221213 12:26:33 @agent_ppo2.py:179][0m |          -0.0181 |          13.8646 |          11.3973 |
[32m[20221213 12:26:34 @agent_ppo2.py:179][0m |          -0.0256 |          13.8417 |          11.5221 |
[32m[20221213 12:26:34 @agent_ppo2.py:179][0m |          -0.0316 |          13.7146 |          11.5659 |
[32m[20221213 12:26:34 @agent_ppo2.py:179][0m |          -0.0396 |          13.7076 |          11.6498 |
[32m[20221213 12:26:34 @agent_ppo2.py:179][0m |          -0.0223 |          15.6484 |          11.6336 |
[32m[20221213 12:26:34 @agent_ppo2.py:179][0m |          -0.0165 |          13.7462 |          11.3058 |
[32m[20221213 12:26:34 @agent_ppo2.py:179][0m |          -0.0363 |          13.6645 |          11.5601 |
[32m[20221213 12:26:34 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:26:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 213.63
[32m[20221213 12:26:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 244.74
[32m[20221213 12:26:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 219.95
[32m[20221213 12:26:34 @agent_ppo2.py:137][0m Total time:      10.63 min
[32m[20221213 12:26:34 @agent_ppo2.py:139][0m 704512 total steps have happened
[32m[20221213 12:26:34 @agent_ppo2.py:115][0m #------------------------ Iteration 344 --------------------------#
[32m[20221213 12:26:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:35 @agent_ppo2.py:179][0m |           0.0529 |          13.6274 |          11.0042 |
[32m[20221213 12:26:35 @agent_ppo2.py:179][0m |           0.0199 |          13.8402 |          11.0921 |
[32m[20221213 12:26:35 @agent_ppo2.py:179][0m |          -0.0000 |          13.3829 |          11.2967 |
[32m[20221213 12:26:35 @agent_ppo2.py:179][0m |          -0.0200 |          13.1858 |          11.4908 |
[32m[20221213 12:26:35 @agent_ppo2.py:179][0m |          -0.0287 |          13.1435 |          11.6291 |
[32m[20221213 12:26:36 @agent_ppo2.py:179][0m |          -0.0318 |          13.0545 |          11.6484 |
[32m[20221213 12:26:36 @agent_ppo2.py:179][0m |          -0.0236 |          13.8711 |          11.6356 |
[32m[20221213 12:26:36 @agent_ppo2.py:179][0m |          -0.0208 |          12.9502 |          11.4433 |
[32m[20221213 12:26:36 @agent_ppo2.py:179][0m |          -0.0267 |          13.2767 |          11.6089 |
[32m[20221213 12:26:36 @agent_ppo2.py:179][0m |          -0.0368 |          12.8677 |          11.6786 |
[32m[20221213 12:26:36 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:26:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 219.12
[32m[20221213 12:26:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 233.77
[32m[20221213 12:26:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 217.82
[32m[20221213 12:26:36 @agent_ppo2.py:137][0m Total time:      10.66 min
[32m[20221213 12:26:36 @agent_ppo2.py:139][0m 706560 total steps have happened
[32m[20221213 12:26:36 @agent_ppo2.py:115][0m #------------------------ Iteration 345 --------------------------#
[32m[20221213 12:26:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:37 @agent_ppo2.py:179][0m |           0.0139 |          14.5935 |          11.5026 |
[32m[20221213 12:26:37 @agent_ppo2.py:179][0m |           0.0082 |          14.3385 |          11.3485 |
[32m[20221213 12:26:37 @agent_ppo2.py:179][0m |          -0.0163 |          14.0734 |          11.4669 |
[32m[20221213 12:26:37 @agent_ppo2.py:179][0m |          -0.0251 |          14.0159 |          11.6053 |
[32m[20221213 12:26:37 @agent_ppo2.py:179][0m |          -0.0237 |          14.0208 |          11.6024 |
[32m[20221213 12:26:37 @agent_ppo2.py:179][0m |          -0.0303 |          13.9288 |          11.5492 |
[32m[20221213 12:26:38 @agent_ppo2.py:179][0m |          -0.0259 |          14.4694 |          11.6008 |
[32m[20221213 12:26:38 @agent_ppo2.py:179][0m |          -0.0341 |          13.8609 |          11.6202 |
[32m[20221213 12:26:38 @agent_ppo2.py:179][0m |          -0.0401 |          13.8155 |          11.6934 |
[32m[20221213 12:26:38 @agent_ppo2.py:179][0m |          -0.0430 |          13.8010 |          11.6520 |
[32m[20221213 12:26:38 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:26:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 223.99
[32m[20221213 12:26:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 240.31
[32m[20221213 12:26:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 228.14
[32m[20221213 12:26:38 @agent_ppo2.py:137][0m Total time:      10.69 min
[32m[20221213 12:26:38 @agent_ppo2.py:139][0m 708608 total steps have happened
[32m[20221213 12:26:38 @agent_ppo2.py:115][0m #------------------------ Iteration 346 --------------------------#
[32m[20221213 12:26:38 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:39 @agent_ppo2.py:179][0m |           0.0361 |          13.7662 |          11.2701 |
[32m[20221213 12:26:39 @agent_ppo2.py:179][0m |           0.0229 |          13.5917 |          10.9359 |
[32m[20221213 12:26:39 @agent_ppo2.py:179][0m |           0.0041 |          13.5435 |          11.0673 |
[32m[20221213 12:26:39 @agent_ppo2.py:179][0m |          -0.0037 |          14.1928 |          11.3080 |
[32m[20221213 12:26:39 @agent_ppo2.py:179][0m |          -0.0232 |          13.5448 |          11.5251 |
[32m[20221213 12:26:39 @agent_ppo2.py:179][0m |          -0.0279 |          13.4521 |          11.6651 |
[32m[20221213 12:26:39 @agent_ppo2.py:179][0m |          -0.0206 |          14.1648 |          11.6550 |
[32m[20221213 12:26:40 @agent_ppo2.py:179][0m |          -0.0273 |          13.3784 |          11.6413 |
[32m[20221213 12:26:40 @agent_ppo2.py:179][0m |          -0.0315 |          13.3449 |          11.6942 |
[32m[20221213 12:26:40 @agent_ppo2.py:179][0m |          -0.0333 |          13.3359 |          11.7168 |
[32m[20221213 12:26:40 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:26:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 227.94
[32m[20221213 12:26:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 241.27
[32m[20221213 12:26:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.81
[32m[20221213 12:26:40 @agent_ppo2.py:137][0m Total time:      10.72 min
[32m[20221213 12:26:40 @agent_ppo2.py:139][0m 710656 total steps have happened
[32m[20221213 12:26:40 @agent_ppo2.py:115][0m #------------------------ Iteration 347 --------------------------#
[32m[20221213 12:26:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |           0.0360 |          13.3494 |          11.0832 |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |           0.0048 |          12.9427 |          11.2281 |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |          -0.0052 |          12.7616 |          11.2534 |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |          -0.0200 |          12.5045 |          11.4365 |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |          -0.0213 |          13.4466 |          11.5334 |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |          -0.0322 |          12.2571 |          11.5733 |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |          -0.0350 |          12.1283 |          11.6078 |
[32m[20221213 12:26:41 @agent_ppo2.py:179][0m |          -0.0328 |          12.0770 |          11.5619 |
[32m[20221213 12:26:42 @agent_ppo2.py:179][0m |          -0.0352 |          11.9901 |          11.5962 |
[32m[20221213 12:26:42 @agent_ppo2.py:179][0m |          -0.0352 |          11.9021 |          11.6029 |
[32m[20221213 12:26:42 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:26:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 228.69
[32m[20221213 12:26:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 243.95
[32m[20221213 12:26:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 228.72
[32m[20221213 12:26:42 @agent_ppo2.py:137][0m Total time:      10.75 min
[32m[20221213 12:26:42 @agent_ppo2.py:139][0m 712704 total steps have happened
[32m[20221213 12:26:42 @agent_ppo2.py:115][0m #------------------------ Iteration 348 --------------------------#
[32m[20221213 12:26:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:42 @agent_ppo2.py:179][0m |           0.0556 |          13.3106 |          11.6379 |
[32m[20221213 12:26:42 @agent_ppo2.py:179][0m |           0.0318 |          12.4092 |          11.5789 |
[32m[20221213 12:26:43 @agent_ppo2.py:179][0m |          -0.0069 |          12.2636 |          11.7945 |
[32m[20221213 12:26:43 @agent_ppo2.py:179][0m |          -0.0186 |          12.0572 |          11.8933 |
[32m[20221213 12:26:43 @agent_ppo2.py:179][0m |          -0.0138 |          12.0521 |          11.9134 |
[32m[20221213 12:26:43 @agent_ppo2.py:179][0m |          -0.0238 |          11.8135 |          11.8926 |
[32m[20221213 12:26:43 @agent_ppo2.py:179][0m |          -0.0194 |          11.7530 |          11.8319 |
[32m[20221213 12:26:43 @agent_ppo2.py:179][0m |          -0.0286 |          11.7134 |          11.9369 |
[32m[20221213 12:26:43 @agent_ppo2.py:179][0m |          -0.0367 |          11.5923 |          11.9259 |
[32m[20221213 12:26:44 @agent_ppo2.py:179][0m |          -0.0331 |          11.4921 |          11.9232 |
[32m[20221213 12:26:44 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:26:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 174.30
[32m[20221213 12:26:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 226.69
[32m[20221213 12:26:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 207.39
[32m[20221213 12:26:44 @agent_ppo2.py:137][0m Total time:      10.78 min
[32m[20221213 12:26:44 @agent_ppo2.py:139][0m 714752 total steps have happened
[32m[20221213 12:26:44 @agent_ppo2.py:115][0m #------------------------ Iteration 349 --------------------------#
[32m[20221213 12:26:44 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:44 @agent_ppo2.py:179][0m |           0.0397 |          15.7879 |          11.5965 |
[32m[20221213 12:26:44 @agent_ppo2.py:179][0m |           0.0208 |          13.1242 |          11.0965 |
[32m[20221213 12:26:44 @agent_ppo2.py:179][0m |          -0.0135 |          12.8507 |          11.6282 |
[32m[20221213 12:26:45 @agent_ppo2.py:179][0m |          -0.0249 |          12.5791 |          11.7409 |
[32m[20221213 12:26:45 @agent_ppo2.py:179][0m |          -0.0267 |          12.4552 |          11.7753 |
[32m[20221213 12:26:45 @agent_ppo2.py:179][0m |          -0.0302 |          12.3161 |          11.8246 |
[32m[20221213 12:26:45 @agent_ppo2.py:179][0m |          -0.0397 |          12.1772 |          11.8464 |
[32m[20221213 12:26:45 @agent_ppo2.py:179][0m |          -0.0380 |          12.0891 |          11.8675 |
[32m[20221213 12:26:45 @agent_ppo2.py:179][0m |          -0.0433 |          11.9783 |          11.8930 |
[32m[20221213 12:26:45 @agent_ppo2.py:179][0m |          -0.0374 |          12.5315 |          11.9014 |
[32m[20221213 12:26:45 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:26:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 220.78
[32m[20221213 12:26:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 242.18
[32m[20221213 12:26:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 114.98
[32m[20221213 12:26:46 @agent_ppo2.py:137][0m Total time:      10.81 min
[32m[20221213 12:26:46 @agent_ppo2.py:139][0m 716800 total steps have happened
[32m[20221213 12:26:46 @agent_ppo2.py:115][0m #------------------------ Iteration 350 --------------------------#
[32m[20221213 12:26:46 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:26:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:46 @agent_ppo2.py:179][0m |           0.0587 |          15.2630 |          11.3881 |
[32m[20221213 12:26:46 @agent_ppo2.py:179][0m |           0.0154 |          15.0842 |          11.5938 |
[32m[20221213 12:26:46 @agent_ppo2.py:179][0m |          -0.0059 |          15.1116 |          11.7633 |
[32m[20221213 12:26:47 @agent_ppo2.py:179][0m |          -0.0184 |          14.8895 |          11.9237 |
[32m[20221213 12:26:47 @agent_ppo2.py:179][0m |          -0.0252 |          14.8361 |          11.9838 |
[32m[20221213 12:26:47 @agent_ppo2.py:179][0m |          -0.0300 |          14.8382 |          12.0295 |
[32m[20221213 12:26:47 @agent_ppo2.py:179][0m |          -0.0346 |          14.7905 |          12.0583 |
[32m[20221213 12:26:47 @agent_ppo2.py:179][0m |          -0.0377 |          14.7672 |          12.0353 |
[32m[20221213 12:26:47 @agent_ppo2.py:179][0m |          -0.0295 |          15.3961 |          12.0699 |
[32m[20221213 12:26:47 @agent_ppo2.py:179][0m |          -0.0355 |          14.7935 |          12.0687 |
[32m[20221213 12:26:47 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:26:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.65
[32m[20221213 12:26:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 241.35
[32m[20221213 12:26:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 227.05
[32m[20221213 12:26:47 @agent_ppo2.py:137][0m Total time:      10.85 min
[32m[20221213 12:26:47 @agent_ppo2.py:139][0m 718848 total steps have happened
[32m[20221213 12:26:47 @agent_ppo2.py:115][0m #------------------------ Iteration 351 --------------------------#
[32m[20221213 12:26:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:48 @agent_ppo2.py:179][0m |           0.0439 |          13.0950 |          11.5666 |
[32m[20221213 12:26:48 @agent_ppo2.py:179][0m |           0.0652 |          12.7800 |          10.1008 |
[32m[20221213 12:26:48 @agent_ppo2.py:179][0m |           0.0090 |          12.3782 |          10.8457 |
[32m[20221213 12:26:48 @agent_ppo2.py:179][0m |          -0.0114 |          12.2320 |          11.5251 |
[32m[20221213 12:26:48 @agent_ppo2.py:179][0m |          -0.0224 |          12.1827 |          11.7129 |
[32m[20221213 12:26:49 @agent_ppo2.py:179][0m |          -0.0268 |          12.1012 |          11.7937 |
[32m[20221213 12:26:49 @agent_ppo2.py:179][0m |          -0.0321 |          12.0566 |          11.8478 |
[32m[20221213 12:26:49 @agent_ppo2.py:179][0m |          -0.0323 |          12.0063 |          11.9159 |
[32m[20221213 12:26:49 @agent_ppo2.py:179][0m |          -0.0331 |          12.2469 |          11.9520 |
[32m[20221213 12:26:49 @agent_ppo2.py:179][0m |          -0.0278 |          11.9229 |          11.7637 |
[32m[20221213 12:26:49 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:26:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.71
[32m[20221213 12:26:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 251.15
[32m[20221213 12:26:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 259.81
[32m[20221213 12:26:49 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 259.81
[32m[20221213 12:26:49 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 259.81
[32m[20221213 12:26:49 @agent_ppo2.py:137][0m Total time:      10.88 min
[32m[20221213 12:26:49 @agent_ppo2.py:139][0m 720896 total steps have happened
[32m[20221213 12:26:49 @agent_ppo2.py:115][0m #------------------------ Iteration 352 --------------------------#
[32m[20221213 12:26:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:50 @agent_ppo2.py:179][0m |           0.0400 |          15.3973 |          11.6478 |
[32m[20221213 12:26:50 @agent_ppo2.py:179][0m |           0.0261 |          14.9362 |          11.0851 |
[32m[20221213 12:26:50 @agent_ppo2.py:179][0m |          -0.0064 |          14.6784 |          11.5388 |
[32m[20221213 12:26:50 @agent_ppo2.py:179][0m |          -0.0262 |          14.4992 |          11.6866 |
[32m[20221213 12:26:50 @agent_ppo2.py:179][0m |          -0.0306 |          14.3237 |          11.7438 |
[32m[20221213 12:26:50 @agent_ppo2.py:179][0m |          -0.0321 |          14.2478 |          11.7528 |
[32m[20221213 12:26:50 @agent_ppo2.py:179][0m |          -0.0386 |          14.1255 |          11.7857 |
[32m[20221213 12:26:51 @agent_ppo2.py:179][0m |          -0.0431 |          13.9762 |          11.8358 |
[32m[20221213 12:26:51 @agent_ppo2.py:179][0m |          -0.0386 |          14.6490 |          11.8167 |
[32m[20221213 12:26:51 @agent_ppo2.py:179][0m |          -0.0435 |          13.8543 |          11.8398 |
[32m[20221213 12:26:51 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:26:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 217.74
[32m[20221213 12:26:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 229.68
[32m[20221213 12:26:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 247.68
[32m[20221213 12:26:51 @agent_ppo2.py:137][0m Total time:      10.90 min
[32m[20221213 12:26:51 @agent_ppo2.py:139][0m 722944 total steps have happened
[32m[20221213 12:26:51 @agent_ppo2.py:115][0m #------------------------ Iteration 353 --------------------------#
[32m[20221213 12:26:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:51 @agent_ppo2.py:179][0m |           0.0583 |          15.6105 |          11.4763 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |           0.0376 |          15.4063 |          10.9394 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |          -0.0082 |          15.2851 |          11.6339 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |          -0.0194 |          15.2020 |          11.8437 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |          -0.0268 |          15.0922 |          11.9129 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |          -0.0297 |          15.0220 |          11.9700 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |          -0.0340 |          15.0005 |          12.0122 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |          -0.0304 |          14.9427 |          11.9764 |
[32m[20221213 12:26:52 @agent_ppo2.py:179][0m |          -0.0337 |          14.9505 |          12.0272 |
[32m[20221213 12:26:53 @agent_ppo2.py:179][0m |          -0.0315 |          14.9500 |          11.9876 |
[32m[20221213 12:26:53 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:26:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.59
[32m[20221213 12:26:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 252.37
[32m[20221213 12:26:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 245.96
[32m[20221213 12:26:53 @agent_ppo2.py:137][0m Total time:      10.93 min
[32m[20221213 12:26:53 @agent_ppo2.py:139][0m 724992 total steps have happened
[32m[20221213 12:26:53 @agent_ppo2.py:115][0m #------------------------ Iteration 354 --------------------------#
[32m[20221213 12:26:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:53 @agent_ppo2.py:179][0m |           0.0594 |          16.6634 |          11.6238 |
[32m[20221213 12:26:53 @agent_ppo2.py:179][0m |           0.0355 |          16.5887 |          11.3222 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |           0.0088 |          16.0240 |          11.4892 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |          -0.0219 |          15.8866 |          11.8057 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |          -0.0295 |          15.8218 |          11.9359 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |          -0.0281 |          17.6246 |          12.0010 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |          -0.0376 |          15.6912 |          12.0091 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |          -0.0397 |          15.5706 |          11.9915 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |          -0.0454 |          15.4960 |          12.0363 |
[32m[20221213 12:26:54 @agent_ppo2.py:179][0m |          -0.0484 |          15.4197 |          12.0510 |
[32m[20221213 12:26:54 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:26:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 212.10
[32m[20221213 12:26:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 238.64
[32m[20221213 12:26:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 232.69
[32m[20221213 12:26:55 @agent_ppo2.py:137][0m Total time:      10.96 min
[32m[20221213 12:26:55 @agent_ppo2.py:139][0m 727040 total steps have happened
[32m[20221213 12:26:55 @agent_ppo2.py:115][0m #------------------------ Iteration 355 --------------------------#
[32m[20221213 12:26:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:26:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:55 @agent_ppo2.py:179][0m |           0.0883 |          15.6753 |          11.3563 |
[32m[20221213 12:26:55 @agent_ppo2.py:179][0m |           0.0541 |          15.4923 |          10.7993 |
[32m[20221213 12:26:55 @agent_ppo2.py:179][0m |           0.0137 |          15.4097 |          11.3776 |
[32m[20221213 12:26:55 @agent_ppo2.py:179][0m |          -0.0118 |          15.4008 |          11.7407 |
[32m[20221213 12:26:56 @agent_ppo2.py:179][0m |          -0.0222 |          15.3443 |          11.9324 |
[32m[20221213 12:26:56 @agent_ppo2.py:179][0m |          -0.0267 |          15.3564 |          12.0421 |
[32m[20221213 12:26:56 @agent_ppo2.py:179][0m |          -0.0321 |          15.3241 |          12.0790 |
[32m[20221213 12:26:56 @agent_ppo2.py:179][0m |          -0.0327 |          15.3252 |          12.1435 |
[32m[20221213 12:26:56 @agent_ppo2.py:179][0m |          -0.0318 |          15.2723 |          12.1119 |
[32m[20221213 12:26:56 @agent_ppo2.py:179][0m |          -0.0350 |          15.2583 |          12.0905 |
[32m[20221213 12:26:56 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:26:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 235.45
[32m[20221213 12:26:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 247.22
[32m[20221213 12:26:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.80
[32m[20221213 12:26:56 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.80
[32m[20221213 12:26:56 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.80
[32m[20221213 12:26:56 @agent_ppo2.py:137][0m Total time:      10.99 min
[32m[20221213 12:26:56 @agent_ppo2.py:139][0m 729088 total steps have happened
[32m[20221213 12:26:56 @agent_ppo2.py:115][0m #------------------------ Iteration 356 --------------------------#
[32m[20221213 12:26:57 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:57 @agent_ppo2.py:179][0m |           0.0358 |          15.5556 |          11.5900 |
[32m[20221213 12:26:57 @agent_ppo2.py:179][0m |           0.0115 |          15.3982 |          11.4922 |
[32m[20221213 12:26:57 @agent_ppo2.py:179][0m |          -0.0060 |          15.3025 |          11.8560 |
[32m[20221213 12:26:57 @agent_ppo2.py:179][0m |          -0.0196 |          15.2561 |          11.9905 |
[32m[20221213 12:26:57 @agent_ppo2.py:179][0m |          -0.0261 |          15.2353 |          12.0770 |
[32m[20221213 12:26:57 @agent_ppo2.py:179][0m |          -0.0295 |          15.2139 |          12.1544 |
[32m[20221213 12:26:58 @agent_ppo2.py:179][0m |          -0.0275 |          15.1982 |          12.1266 |
[32m[20221213 12:26:58 @agent_ppo2.py:179][0m |          -0.0297 |          15.1858 |          12.1753 |
[32m[20221213 12:26:58 @agent_ppo2.py:179][0m |          -0.0303 |          15.1680 |          12.1353 |
[32m[20221213 12:26:58 @agent_ppo2.py:179][0m |          -0.0312 |          15.1859 |          12.1435 |
[32m[20221213 12:26:58 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:26:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 245.86
[32m[20221213 12:26:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 256.46
[32m[20221213 12:26:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 251.25
[32m[20221213 12:26:58 @agent_ppo2.py:137][0m Total time:      11.03 min
[32m[20221213 12:26:58 @agent_ppo2.py:139][0m 731136 total steps have happened
[32m[20221213 12:26:58 @agent_ppo2.py:115][0m #------------------------ Iteration 357 --------------------------#
[32m[20221213 12:26:58 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:26:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:26:59 @agent_ppo2.py:179][0m |           0.0460 |          16.1287 |          12.0383 |
[32m[20221213 12:26:59 @agent_ppo2.py:179][0m |           0.0224 |          15.9690 |          11.8716 |
[32m[20221213 12:26:59 @agent_ppo2.py:179][0m |          -0.0040 |          16.0057 |          12.1907 |
[32m[20221213 12:26:59 @agent_ppo2.py:179][0m |          -0.0202 |          15.9123 |          12.3098 |
[32m[20221213 12:26:59 @agent_ppo2.py:179][0m |          -0.0174 |          15.8754 |          12.3384 |
[32m[20221213 12:26:59 @agent_ppo2.py:179][0m |          -0.0260 |          15.8373 |          12.4033 |
[32m[20221213 12:26:59 @agent_ppo2.py:179][0m |          -0.0279 |          15.7978 |          12.4313 |
[32m[20221213 12:27:00 @agent_ppo2.py:179][0m |          -0.0257 |          16.2478 |          12.4081 |
[32m[20221213 12:27:00 @agent_ppo2.py:179][0m |          -0.0323 |          15.7828 |          12.4698 |
[32m[20221213 12:27:00 @agent_ppo2.py:179][0m |          -0.0253 |          16.4388 |          12.4629 |
[32m[20221213 12:27:00 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:27:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.84
[32m[20221213 12:27:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.64
[32m[20221213 12:27:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 249.10
[32m[20221213 12:27:00 @agent_ppo2.py:137][0m Total time:      11.06 min
[32m[20221213 12:27:00 @agent_ppo2.py:139][0m 733184 total steps have happened
[32m[20221213 12:27:00 @agent_ppo2.py:115][0m #------------------------ Iteration 358 --------------------------#
[32m[20221213 12:27:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:01 @agent_ppo2.py:179][0m |           0.0740 |          16.1741 |          11.2227 |
[32m[20221213 12:27:01 @agent_ppo2.py:179][0m |           0.0295 |          15.4593 |          11.2095 |
[32m[20221213 12:27:01 @agent_ppo2.py:179][0m |          -0.0057 |          15.3152 |          11.8650 |
[32m[20221213 12:27:01 @agent_ppo2.py:179][0m |          -0.0173 |          15.2328 |          12.0033 |
[32m[20221213 12:27:01 @agent_ppo2.py:179][0m |          -0.0273 |          15.1754 |          12.1634 |
[32m[20221213 12:27:01 @agent_ppo2.py:179][0m |          -0.0329 |          15.1359 |          12.1927 |
[32m[20221213 12:27:02 @agent_ppo2.py:179][0m |          -0.0234 |          15.9946 |          12.1871 |
[32m[20221213 12:27:02 @agent_ppo2.py:179][0m |          -0.0319 |          15.1136 |          12.2084 |
[32m[20221213 12:27:02 @agent_ppo2.py:179][0m |          -0.0391 |          14.9278 |          12.2683 |
[32m[20221213 12:27:02 @agent_ppo2.py:179][0m |          -0.0381 |          14.9042 |          12.2598 |
[32m[20221213 12:27:02 @agent_ppo2.py:124][0m Policy update time: 1.62 s
[32m[20221213 12:27:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.47
[32m[20221213 12:27:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 254.89
[32m[20221213 12:27:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 235.75
[32m[20221213 12:27:02 @agent_ppo2.py:137][0m Total time:      11.09 min
[32m[20221213 12:27:02 @agent_ppo2.py:139][0m 735232 total steps have happened
[32m[20221213 12:27:02 @agent_ppo2.py:115][0m #------------------------ Iteration 359 --------------------------#
[32m[20221213 12:27:02 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:03 @agent_ppo2.py:179][0m |           0.0275 |          15.8914 |          11.8392 |
[32m[20221213 12:27:03 @agent_ppo2.py:179][0m |           0.0510 |          15.7350 |          11.1212 |
[32m[20221213 12:27:03 @agent_ppo2.py:179][0m |           0.0414 |          16.8430 |          11.0273 |
[32m[20221213 12:27:03 @agent_ppo2.py:179][0m |          -0.0097 |          15.5657 |          11.6181 |
[32m[20221213 12:27:03 @agent_ppo2.py:179][0m |          -0.0206 |          15.5157 |          11.8404 |
[32m[20221213 12:27:03 @agent_ppo2.py:179][0m |          -0.0196 |          17.0780 |          11.9922 |
[32m[20221213 12:27:04 @agent_ppo2.py:179][0m |          -0.0311 |          15.5272 |          12.0301 |
[32m[20221213 12:27:04 @agent_ppo2.py:179][0m |          -0.0364 |          15.4893 |          12.0655 |
[32m[20221213 12:27:04 @agent_ppo2.py:179][0m |          -0.0346 |          15.4538 |          12.1186 |
[32m[20221213 12:27:04 @agent_ppo2.py:179][0m |          -0.0329 |          15.4994 |          12.1119 |
[32m[20221213 12:27:04 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:27:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 228.17
[32m[20221213 12:27:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 235.04
[32m[20221213 12:27:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.96
[32m[20221213 12:27:04 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 262.96
[32m[20221213 12:27:04 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 262.96
[32m[20221213 12:27:04 @agent_ppo2.py:137][0m Total time:      11.12 min
[32m[20221213 12:27:04 @agent_ppo2.py:139][0m 737280 total steps have happened
[32m[20221213 12:27:04 @agent_ppo2.py:115][0m #------------------------ Iteration 360 --------------------------#
[32m[20221213 12:27:04 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:27:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:05 @agent_ppo2.py:179][0m |           0.0569 |          15.6040 |          11.6948 |
[32m[20221213 12:27:05 @agent_ppo2.py:179][0m |           0.0197 |          15.7314 |          11.8403 |
[32m[20221213 12:27:05 @agent_ppo2.py:179][0m |           0.0027 |          15.3481 |          11.8229 |
[32m[20221213 12:27:05 @agent_ppo2.py:179][0m |          -0.0164 |          15.4441 |          12.1270 |
[32m[20221213 12:27:05 @agent_ppo2.py:179][0m |          -0.0211 |          15.5638 |          12.2123 |
[32m[20221213 12:27:05 @agent_ppo2.py:179][0m |          -0.0324 |          15.2142 |          12.2615 |
[32m[20221213 12:27:06 @agent_ppo2.py:179][0m |          -0.0342 |          15.1662 |          12.2413 |
[32m[20221213 12:27:06 @agent_ppo2.py:179][0m |          -0.0333 |          15.1755 |          12.2439 |
[32m[20221213 12:27:06 @agent_ppo2.py:179][0m |          -0.0346 |          15.1165 |          12.2815 |
[32m[20221213 12:27:06 @agent_ppo2.py:179][0m |          -0.0346 |          15.0855 |          12.2462 |
[32m[20221213 12:27:06 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:27:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.65
[32m[20221213 12:27:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 234.89
[32m[20221213 12:27:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 251.67
[32m[20221213 12:27:06 @agent_ppo2.py:137][0m Total time:      11.16 min
[32m[20221213 12:27:06 @agent_ppo2.py:139][0m 739328 total steps have happened
[32m[20221213 12:27:06 @agent_ppo2.py:115][0m #------------------------ Iteration 361 --------------------------#
[32m[20221213 12:27:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |           0.0609 |          16.5745 |          12.1770 |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |           0.0417 |          15.7737 |          11.7444 |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |           0.0026 |          15.9747 |          12.0812 |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |          -0.0155 |          15.5797 |          12.2367 |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |          -0.0144 |          16.5390 |          12.2555 |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |          -0.0282 |          15.5098 |          12.3365 |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |          -0.0282 |          15.4466 |          12.3320 |
[32m[20221213 12:27:07 @agent_ppo2.py:179][0m |          -0.0326 |          15.4155 |          12.3537 |
[32m[20221213 12:27:08 @agent_ppo2.py:179][0m |          -0.0337 |          15.4011 |          12.3518 |
[32m[20221213 12:27:08 @agent_ppo2.py:179][0m |          -0.0341 |          15.3954 |          12.3434 |
[32m[20221213 12:27:08 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:27:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 221.52
[32m[20221213 12:27:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 238.36
[32m[20221213 12:27:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 127.59
[32m[20221213 12:27:08 @agent_ppo2.py:137][0m Total time:      11.19 min
[32m[20221213 12:27:08 @agent_ppo2.py:139][0m 741376 total steps have happened
[32m[20221213 12:27:08 @agent_ppo2.py:115][0m #------------------------ Iteration 362 --------------------------#
[32m[20221213 12:27:08 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:08 @agent_ppo2.py:179][0m |           0.0443 |          15.5257 |          11.8205 |
[32m[20221213 12:27:09 @agent_ppo2.py:179][0m |           0.0294 |          15.1862 |          11.5462 |
[32m[20221213 12:27:09 @agent_ppo2.py:179][0m |           0.0019 |          15.0549 |          11.7923 |
[32m[20221213 12:27:09 @agent_ppo2.py:179][0m |          -0.0186 |          14.9344 |          11.9608 |
[32m[20221213 12:27:09 @agent_ppo2.py:179][0m |          -0.0268 |          14.8657 |          12.0746 |
[32m[20221213 12:27:09 @agent_ppo2.py:179][0m |          -0.0319 |          14.7771 |          12.1151 |
[32m[20221213 12:27:09 @agent_ppo2.py:179][0m |          -0.0311 |          14.7342 |          12.1109 |
[32m[20221213 12:27:09 @agent_ppo2.py:179][0m |          -0.0270 |          14.6701 |          12.0594 |
[32m[20221213 12:27:10 @agent_ppo2.py:179][0m |          -0.0363 |          14.6289 |          12.1308 |
[32m[20221213 12:27:10 @agent_ppo2.py:179][0m |          -0.0338 |          14.5570 |          12.1147 |
[32m[20221213 12:27:10 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:27:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.72
[32m[20221213 12:27:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 246.80
[32m[20221213 12:27:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 144.81
[32m[20221213 12:27:10 @agent_ppo2.py:137][0m Total time:      11.22 min
[32m[20221213 12:27:10 @agent_ppo2.py:139][0m 743424 total steps have happened
[32m[20221213 12:27:10 @agent_ppo2.py:115][0m #------------------------ Iteration 363 --------------------------#
[32m[20221213 12:27:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:10 @agent_ppo2.py:179][0m |           0.0703 |          16.3784 |          11.3251 |
[32m[20221213 12:27:10 @agent_ppo2.py:179][0m |           0.0180 |          15.9963 |          11.5535 |
[32m[20221213 12:27:11 @agent_ppo2.py:179][0m |          -0.0043 |          16.5358 |          11.8735 |
[32m[20221213 12:27:11 @agent_ppo2.py:179][0m |          -0.0172 |          15.9997 |          11.9848 |
[32m[20221213 12:27:11 @agent_ppo2.py:179][0m |          -0.0208 |          15.7320 |          11.8832 |
[32m[20221213 12:27:11 @agent_ppo2.py:179][0m |          -0.0263 |          15.6960 |          11.9815 |
[32m[20221213 12:27:11 @agent_ppo2.py:179][0m |          -0.0165 |          19.0130 |          12.1044 |
[32m[20221213 12:27:11 @agent_ppo2.py:179][0m |          -0.0357 |          15.7290 |          12.0805 |
[32m[20221213 12:27:11 @agent_ppo2.py:179][0m |          -0.0382 |          15.6169 |          12.1059 |
[32m[20221213 12:27:12 @agent_ppo2.py:179][0m |          -0.0340 |          16.8623 |          12.1671 |
[32m[20221213 12:27:12 @agent_ppo2.py:124][0m Policy update time: 1.52 s
[32m[20221213 12:27:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 236.64
[32m[20221213 12:27:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 248.07
[32m[20221213 12:27:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 250.47
[32m[20221213 12:27:12 @agent_ppo2.py:137][0m Total time:      11.25 min
[32m[20221213 12:27:12 @agent_ppo2.py:139][0m 745472 total steps have happened
[32m[20221213 12:27:12 @agent_ppo2.py:115][0m #------------------------ Iteration 364 --------------------------#
[32m[20221213 12:27:12 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:12 @agent_ppo2.py:179][0m |           0.0331 |          15.7172 |          11.8054 |
[32m[20221213 12:27:12 @agent_ppo2.py:179][0m |           0.0179 |          15.6028 |          11.5498 |
[32m[20221213 12:27:13 @agent_ppo2.py:179][0m |          -0.0076 |          15.5254 |          11.8843 |
[32m[20221213 12:27:13 @agent_ppo2.py:179][0m |          -0.0159 |          15.4875 |          11.9893 |
[32m[20221213 12:27:13 @agent_ppo2.py:179][0m |          -0.0282 |          15.4994 |          12.0810 |
[32m[20221213 12:27:13 @agent_ppo2.py:179][0m |          -0.0299 |          15.6367 |          12.1645 |
[32m[20221213 12:27:13 @agent_ppo2.py:179][0m |          -0.0328 |          15.4061 |          12.1323 |
[32m[20221213 12:27:13 @agent_ppo2.py:179][0m |          -0.0308 |          15.3735 |          12.1306 |
[32m[20221213 12:27:13 @agent_ppo2.py:179][0m |          -0.0323 |          15.4449 |          12.1709 |
[32m[20221213 12:27:14 @agent_ppo2.py:179][0m |          -0.0320 |          15.3350 |          12.1178 |
[32m[20221213 12:27:14 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:27:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.75
[32m[20221213 12:27:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 252.27
[32m[20221213 12:27:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.56
[32m[20221213 12:27:14 @agent_ppo2.py:137][0m Total time:      11.28 min
[32m[20221213 12:27:14 @agent_ppo2.py:139][0m 747520 total steps have happened
[32m[20221213 12:27:14 @agent_ppo2.py:115][0m #------------------------ Iteration 365 --------------------------#
[32m[20221213 12:27:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:14 @agent_ppo2.py:179][0m |           0.0267 |          14.5395 |          12.0296 |
[32m[20221213 12:27:14 @agent_ppo2.py:179][0m |          -0.0016 |          13.9447 |          11.9388 |
[32m[20221213 12:27:14 @agent_ppo2.py:179][0m |          -0.0135 |          13.7743 |          12.0501 |
[32m[20221213 12:27:15 @agent_ppo2.py:179][0m |          -0.0195 |          13.8393 |          12.1970 |
[32m[20221213 12:27:15 @agent_ppo2.py:179][0m |          -0.0274 |          13.6959 |          12.1689 |
[32m[20221213 12:27:15 @agent_ppo2.py:179][0m |          -0.0297 |          13.5023 |          12.2221 |
[32m[20221213 12:27:15 @agent_ppo2.py:179][0m |          -0.0351 |          13.5202 |          12.2554 |
[32m[20221213 12:27:15 @agent_ppo2.py:179][0m |          -0.0362 |          13.4521 |          12.2745 |
[32m[20221213 12:27:15 @agent_ppo2.py:179][0m |          -0.0253 |          13.5909 |          12.2457 |
[32m[20221213 12:27:15 @agent_ppo2.py:179][0m |          -0.0337 |          13.3285 |          12.2449 |
[32m[20221213 12:27:15 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:27:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.13
[32m[20221213 12:27:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 255.54
[32m[20221213 12:27:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.24
[32m[20221213 12:27:16 @agent_ppo2.py:137][0m Total time:      11.31 min
[32m[20221213 12:27:16 @agent_ppo2.py:139][0m 749568 total steps have happened
[32m[20221213 12:27:16 @agent_ppo2.py:115][0m #------------------------ Iteration 366 --------------------------#
[32m[20221213 12:27:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:16 @agent_ppo2.py:179][0m |           0.0560 |          14.7628 |          11.3759 |
[32m[20221213 12:27:16 @agent_ppo2.py:179][0m |           0.0305 |          14.2808 |           9.3635 |
[32m[20221213 12:27:16 @agent_ppo2.py:179][0m |          -0.0147 |          14.0600 |           9.1075 |
[32m[20221213 12:27:16 @agent_ppo2.py:179][0m |          -0.0265 |          13.9629 |           9.3057 |
[32m[20221213 12:27:17 @agent_ppo2.py:179][0m |          -0.0363 |          13.8611 |           9.3754 |
[32m[20221213 12:27:17 @agent_ppo2.py:179][0m |          -0.0492 |          13.8397 |           9.4883 |
[32m[20221213 12:27:17 @agent_ppo2.py:179][0m |          -0.0430 |          14.9664 |           9.5049 |
[32m[20221213 12:27:17 @agent_ppo2.py:179][0m |          -0.0512 |          13.7760 |           9.4914 |
[32m[20221213 12:27:17 @agent_ppo2.py:179][0m |          -0.0527 |          13.6985 |           9.5112 |
[32m[20221213 12:27:17 @agent_ppo2.py:179][0m |          -0.0572 |          14.1677 |           9.5692 |
[32m[20221213 12:27:17 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:27:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.07
[32m[20221213 12:27:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 220.44
[32m[20221213 12:27:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 268.57
[32m[20221213 12:27:17 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 268.57
[32m[20221213 12:27:17 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 268.57
[32m[20221213 12:27:17 @agent_ppo2.py:137][0m Total time:      11.35 min
[32m[20221213 12:27:17 @agent_ppo2.py:139][0m 751616 total steps have happened
[32m[20221213 12:27:17 @agent_ppo2.py:115][0m #------------------------ Iteration 367 --------------------------#
[32m[20221213 12:27:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:18 @agent_ppo2.py:179][0m |           0.0844 |          16.2981 |          11.3121 |
[32m[20221213 12:27:18 @agent_ppo2.py:179][0m |           0.0866 |          16.7525 |          10.5289 |
[32m[20221213 12:27:18 @agent_ppo2.py:179][0m |           0.0089 |          15.9256 |          11.4337 |
[32m[20221213 12:27:18 @agent_ppo2.py:179][0m |          -0.0130 |          15.8277 |          11.7808 |
[32m[20221213 12:27:18 @agent_ppo2.py:179][0m |          -0.0209 |          15.7671 |          11.9578 |
[32m[20221213 12:27:19 @agent_ppo2.py:179][0m |          -0.0288 |          15.7150 |          12.0501 |
[32m[20221213 12:27:19 @agent_ppo2.py:179][0m |          -0.0279 |          15.7077 |          12.0704 |
[32m[20221213 12:27:19 @agent_ppo2.py:179][0m |          -0.0328 |          15.6282 |          12.1013 |
[32m[20221213 12:27:19 @agent_ppo2.py:179][0m |          -0.0323 |          15.6473 |          12.1149 |
[32m[20221213 12:27:19 @agent_ppo2.py:179][0m |          -0.0396 |          15.5644 |          12.1600 |
[32m[20221213 12:27:19 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:27:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.55
[32m[20221213 12:27:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 251.91
[32m[20221213 12:27:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 257.73
[32m[20221213 12:27:19 @agent_ppo2.py:137][0m Total time:      11.38 min
[32m[20221213 12:27:19 @agent_ppo2.py:139][0m 753664 total steps have happened
[32m[20221213 12:27:19 @agent_ppo2.py:115][0m #------------------------ Iteration 368 --------------------------#
[32m[20221213 12:27:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:20 @agent_ppo2.py:179][0m |           0.0619 |          16.0503 |          11.6361 |
[32m[20221213 12:27:20 @agent_ppo2.py:179][0m |           0.0532 |          15.8639 |          11.0852 |
[32m[20221213 12:27:20 @agent_ppo2.py:179][0m |           0.0179 |          15.7507 |          11.3134 |
[32m[20221213 12:27:20 @agent_ppo2.py:179][0m |          -0.0088 |          15.7600 |          12.0718 |
[32m[20221213 12:27:20 @agent_ppo2.py:179][0m |          -0.0182 |          15.7067 |          12.2594 |
[32m[20221213 12:27:20 @agent_ppo2.py:179][0m |          -0.0247 |          15.6471 |          12.3440 |
[32m[20221213 12:27:21 @agent_ppo2.py:179][0m |          -0.0221 |          17.1492 |          12.4117 |
[32m[20221213 12:27:21 @agent_ppo2.py:179][0m |          -0.0318 |          15.6181 |          12.4679 |
[32m[20221213 12:27:21 @agent_ppo2.py:179][0m |          -0.0342 |          15.5569 |          12.4551 |
[32m[20221213 12:27:21 @agent_ppo2.py:179][0m |          -0.0278 |          16.6602 |          12.4458 |
[32m[20221213 12:27:21 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:27:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.66
[32m[20221213 12:27:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 246.36
[32m[20221213 12:27:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 238.66
[32m[20221213 12:27:21 @agent_ppo2.py:137][0m Total time:      11.41 min
[32m[20221213 12:27:21 @agent_ppo2.py:139][0m 755712 total steps have happened
[32m[20221213 12:27:21 @agent_ppo2.py:115][0m #------------------------ Iteration 369 --------------------------#
[32m[20221213 12:27:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:22 @agent_ppo2.py:179][0m |           0.0585 |          14.8225 |          11.6451 |
[32m[20221213 12:27:22 @agent_ppo2.py:179][0m |           0.0296 |          14.3831 |          11.7567 |
[32m[20221213 12:27:22 @agent_ppo2.py:179][0m |          -0.0077 |          14.2382 |          12.0691 |
[32m[20221213 12:27:22 @agent_ppo2.py:179][0m |          -0.0105 |          14.2138 |          12.1220 |
[32m[20221213 12:27:22 @agent_ppo2.py:179][0m |          -0.0181 |          14.1589 |          12.1753 |
[32m[20221213 12:27:22 @agent_ppo2.py:179][0m |          -0.0208 |          14.0368 |          12.1860 |
[32m[20221213 12:27:22 @agent_ppo2.py:179][0m |          -0.0234 |          14.0413 |          12.1369 |
[32m[20221213 12:27:23 @agent_ppo2.py:179][0m |          -0.0279 |          13.9594 |          12.1999 |
[32m[20221213 12:27:23 @agent_ppo2.py:179][0m |          -0.0311 |          13.9594 |          12.2175 |
[32m[20221213 12:27:23 @agent_ppo2.py:179][0m |          -0.0276 |          13.8889 |          12.1245 |
[32m[20221213 12:27:23 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:27:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 194.57
[32m[20221213 12:27:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 249.08
[32m[20221213 12:27:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 203.19
[32m[20221213 12:27:23 @agent_ppo2.py:137][0m Total time:      11.44 min
[32m[20221213 12:27:23 @agent_ppo2.py:139][0m 757760 total steps have happened
[32m[20221213 12:27:23 @agent_ppo2.py:115][0m #------------------------ Iteration 370 --------------------------#
[32m[20221213 12:27:23 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:27:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:23 @agent_ppo2.py:179][0m |           0.0262 |          15.9269 |          12.1829 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |           0.0141 |          15.7456 |          12.1708 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |          -0.0073 |          15.6823 |          12.3462 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |          -0.0189 |          15.6257 |          12.4240 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |          -0.0275 |          15.6613 |          12.4995 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |          -0.0301 |          15.6028 |          12.5039 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |          -0.0283 |          15.5407 |          12.5052 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |          -0.0337 |          15.5414 |          12.5409 |
[32m[20221213 12:27:24 @agent_ppo2.py:179][0m |          -0.0318 |          15.5593 |          12.5244 |
[32m[20221213 12:27:25 @agent_ppo2.py:179][0m |          -0.0339 |          15.5044 |          12.5322 |
[32m[20221213 12:27:25 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:27:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.56
[32m[20221213 12:27:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 249.34
[32m[20221213 12:27:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 240.27
[32m[20221213 12:27:25 @agent_ppo2.py:137][0m Total time:      11.47 min
[32m[20221213 12:27:25 @agent_ppo2.py:139][0m 759808 total steps have happened
[32m[20221213 12:27:25 @agent_ppo2.py:115][0m #------------------------ Iteration 371 --------------------------#
[32m[20221213 12:27:25 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:25 @agent_ppo2.py:179][0m |           0.0392 |          15.5513 |          12.0385 |
[32m[20221213 12:27:25 @agent_ppo2.py:179][0m |           0.0276 |          15.4184 |          11.3032 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |           0.0060 |          16.8544 |          11.9652 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |          -0.0144 |          15.3102 |          12.1632 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |          -0.0118 |          16.6014 |          12.2780 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |          -0.0229 |          15.5255 |          12.3585 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |          -0.0247 |          15.2045 |          12.3487 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |          -0.0212 |          15.1954 |          12.3015 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |          -0.0282 |          15.1611 |          12.3752 |
[32m[20221213 12:27:26 @agent_ppo2.py:179][0m |          -0.0287 |          15.1519 |          12.3872 |
[32m[20221213 12:27:26 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:27:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 252.92
[32m[20221213 12:27:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.30
[32m[20221213 12:27:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 239.42
[32m[20221213 12:27:27 @agent_ppo2.py:137][0m Total time:      11.50 min
[32m[20221213 12:27:27 @agent_ppo2.py:139][0m 761856 total steps have happened
[32m[20221213 12:27:27 @agent_ppo2.py:115][0m #------------------------ Iteration 372 --------------------------#
[32m[20221213 12:27:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:27 @agent_ppo2.py:179][0m |           0.0244 |          15.3618 |          11.9755 |
[32m[20221213 12:27:27 @agent_ppo2.py:179][0m |           0.0195 |          15.2745 |          11.8244 |
[32m[20221213 12:27:27 @agent_ppo2.py:179][0m |          -0.0040 |          15.4680 |          11.9916 |
[32m[20221213 12:27:28 @agent_ppo2.py:179][0m |          -0.0290 |          13.6905 |          12.1025 |
[32m[20221213 12:27:28 @agent_ppo2.py:179][0m |          -0.0246 |          14.0822 |          12.1461 |
[32m[20221213 12:27:28 @agent_ppo2.py:179][0m |          -0.0387 |          13.0710 |          12.1758 |
[32m[20221213 12:27:28 @agent_ppo2.py:179][0m |          -0.0407 |          12.7940 |          12.1977 |
[32m[20221213 12:27:28 @agent_ppo2.py:179][0m |          -0.0404 |          12.6605 |          12.2039 |
[32m[20221213 12:27:28 @agent_ppo2.py:179][0m |          -0.0431 |          12.4859 |          12.2233 |
[32m[20221213 12:27:28 @agent_ppo2.py:179][0m |          -0.0411 |          12.4202 |          12.2149 |
[32m[20221213 12:27:28 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:27:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 225.30
[32m[20221213 12:27:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 239.03
[32m[20221213 12:27:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.41
[32m[20221213 12:27:29 @agent_ppo2.py:137][0m Total time:      11.53 min
[32m[20221213 12:27:29 @agent_ppo2.py:139][0m 763904 total steps have happened
[32m[20221213 12:27:29 @agent_ppo2.py:115][0m #------------------------ Iteration 373 --------------------------#
[32m[20221213 12:27:29 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:29 @agent_ppo2.py:179][0m |           0.0756 |          19.3508 |          11.4760 |
[32m[20221213 12:27:29 @agent_ppo2.py:179][0m |           0.0486 |          16.9853 |          11.1092 |
[32m[20221213 12:27:29 @agent_ppo2.py:179][0m |          -0.0020 |          16.4398 |          11.9975 |
[32m[20221213 12:27:29 @agent_ppo2.py:179][0m |          -0.0183 |          16.3237 |          12.1544 |
[32m[20221213 12:27:30 @agent_ppo2.py:179][0m |          -0.0162 |          16.2629 |          12.1574 |
[32m[20221213 12:27:30 @agent_ppo2.py:179][0m |          -0.0227 |          17.0248 |          12.2368 |
[32m[20221213 12:27:30 @agent_ppo2.py:179][0m |          -0.0222 |          18.5225 |          12.2805 |
[32m[20221213 12:27:30 @agent_ppo2.py:179][0m |          -0.0358 |          16.1382 |          12.3208 |
[32m[20221213 12:27:30 @agent_ppo2.py:179][0m |          -0.0333 |          16.0782 |          12.2799 |
[32m[20221213 12:27:30 @agent_ppo2.py:179][0m |          -0.0377 |          16.8220 |          12.3411 |
[32m[20221213 12:27:30 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:27:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 231.48
[32m[20221213 12:27:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 245.41
[32m[20221213 12:27:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 253.46
[32m[20221213 12:27:30 @agent_ppo2.py:137][0m Total time:      11.56 min
[32m[20221213 12:27:30 @agent_ppo2.py:139][0m 765952 total steps have happened
[32m[20221213 12:27:30 @agent_ppo2.py:115][0m #------------------------ Iteration 374 --------------------------#
[32m[20221213 12:27:31 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:31 @agent_ppo2.py:179][0m |           0.0361 |          15.7710 |          11.7744 |
[32m[20221213 12:27:31 @agent_ppo2.py:179][0m |           0.0152 |          15.1842 |          11.3968 |
[32m[20221213 12:27:31 @agent_ppo2.py:179][0m |          -0.0117 |          14.8762 |          11.8323 |
[32m[20221213 12:27:31 @agent_ppo2.py:179][0m |          -0.0216 |          14.5468 |          11.9939 |
[32m[20221213 12:27:32 @agent_ppo2.py:179][0m |          -0.0314 |          14.6599 |          12.0972 |
[32m[20221213 12:27:32 @agent_ppo2.py:179][0m |          -0.0356 |          14.5088 |          12.1435 |
[32m[20221213 12:27:32 @agent_ppo2.py:179][0m |          -0.0334 |          14.1762 |          12.1448 |
[32m[20221213 12:27:32 @agent_ppo2.py:179][0m |          -0.0162 |          16.9506 |          12.1873 |
[32m[20221213 12:27:32 @agent_ppo2.py:179][0m |          -0.0076 |          14.0181 |          11.6485 |
[32m[20221213 12:27:32 @agent_ppo2.py:179][0m |          -0.0283 |          14.0759 |          12.0714 |
[32m[20221213 12:27:32 @agent_ppo2.py:124][0m Policy update time: 1.83 s
[32m[20221213 12:27:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 246.48
[32m[20221213 12:27:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 254.98
[32m[20221213 12:27:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 245.61
[32m[20221213 12:27:33 @agent_ppo2.py:137][0m Total time:      11.60 min
[32m[20221213 12:27:33 @agent_ppo2.py:139][0m 768000 total steps have happened
[32m[20221213 12:27:33 @agent_ppo2.py:115][0m #------------------------ Iteration 375 --------------------------#
[32m[20221213 12:27:33 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:33 @agent_ppo2.py:179][0m |           0.1006 |          12.1435 |          10.4446 |
[32m[20221213 12:27:33 @agent_ppo2.py:179][0m |          -0.0065 |          11.5816 |           4.7626 |
[32m[20221213 12:27:34 @agent_ppo2.py:179][0m |          -0.0321 |          11.4636 |           4.9128 |
[32m[20221213 12:27:34 @agent_ppo2.py:179][0m |          -0.0412 |          11.4072 |           5.0597 |
[32m[20221213 12:27:34 @agent_ppo2.py:179][0m |          -0.0474 |          11.3472 |           5.1023 |
[32m[20221213 12:27:34 @agent_ppo2.py:179][0m |          -0.0449 |          11.3142 |           5.0815 |
[32m[20221213 12:27:34 @agent_ppo2.py:179][0m |          -0.0497 |          11.2540 |           5.1058 |
[32m[20221213 12:27:34 @agent_ppo2.py:179][0m |          -0.0594 |          11.2307 |           5.1271 |
[32m[20221213 12:27:34 @agent_ppo2.py:179][0m |          -0.0588 |          11.1688 |           5.1196 |
[32m[20221213 12:27:35 @agent_ppo2.py:179][0m |          -0.0583 |          11.1344 |           5.1228 |
[32m[20221213 12:27:35 @agent_ppo2.py:124][0m Policy update time: 1.57 s
[32m[20221213 12:27:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.27
[32m[20221213 12:27:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 222.87
[32m[20221213 12:27:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 240.20
[32m[20221213 12:27:35 @agent_ppo2.py:137][0m Total time:      11.63 min
[32m[20221213 12:27:35 @agent_ppo2.py:139][0m 770048 total steps have happened
[32m[20221213 12:27:35 @agent_ppo2.py:115][0m #------------------------ Iteration 376 --------------------------#
[32m[20221213 12:27:35 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:35 @agent_ppo2.py:179][0m |           0.0267 |          16.7700 |          12.4039 |
[32m[20221213 12:27:35 @agent_ppo2.py:179][0m |           0.0157 |          16.4548 |          12.0111 |
[32m[20221213 12:27:36 @agent_ppo2.py:179][0m |          -0.0118 |          16.3081 |          12.3169 |
[32m[20221213 12:27:36 @agent_ppo2.py:179][0m |          -0.0260 |          16.2061 |          12.5760 |
[32m[20221213 12:27:36 @agent_ppo2.py:179][0m |          -0.0315 |          16.1341 |          12.6434 |
[32m[20221213 12:27:36 @agent_ppo2.py:179][0m |          -0.0308 |          16.0864 |          12.6768 |
[32m[20221213 12:27:36 @agent_ppo2.py:179][0m |          -0.0329 |          16.0302 |          12.6017 |
[32m[20221213 12:27:36 @agent_ppo2.py:179][0m |          -0.0327 |          16.0037 |          12.6262 |
[32m[20221213 12:27:36 @agent_ppo2.py:179][0m |          -0.0389 |          15.9587 |          12.6702 |
[32m[20221213 12:27:37 @agent_ppo2.py:179][0m |          -0.0425 |          15.9243 |          12.6645 |
[32m[20221213 12:27:37 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:27:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.58
[32m[20221213 12:27:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 249.49
[32m[20221213 12:27:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 242.35
[32m[20221213 12:27:37 @agent_ppo2.py:137][0m Total time:      11.67 min
[32m[20221213 12:27:37 @agent_ppo2.py:139][0m 772096 total steps have happened
[32m[20221213 12:27:37 @agent_ppo2.py:115][0m #------------------------ Iteration 377 --------------------------#
[32m[20221213 12:27:37 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:37 @agent_ppo2.py:179][0m |           0.0636 |          17.3398 |          12.0016 |
[32m[20221213 12:27:37 @agent_ppo2.py:179][0m |           0.0313 |          16.3625 |          11.5335 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |           0.0112 |          16.9972 |          12.1163 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |          -0.0066 |          16.0287 |          12.2617 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |          -0.0232 |          15.8715 |          12.4628 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |          -0.0208 |          18.1713 |          12.5270 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |          -0.0327 |          16.1091 |          12.5817 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |          -0.0356 |          15.6962 |          12.5565 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |          -0.0348 |          15.8964 |          12.6000 |
[32m[20221213 12:27:38 @agent_ppo2.py:179][0m |          -0.0247 |          15.7890 |          12.3655 |
[32m[20221213 12:27:38 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:27:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 240.60
[32m[20221213 12:27:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 250.50
[32m[20221213 12:27:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 217.68
[32m[20221213 12:27:39 @agent_ppo2.py:137][0m Total time:      11.70 min
[32m[20221213 12:27:39 @agent_ppo2.py:139][0m 774144 total steps have happened
[32m[20221213 12:27:39 @agent_ppo2.py:115][0m #------------------------ Iteration 378 --------------------------#
[32m[20221213 12:27:39 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:39 @agent_ppo2.py:179][0m |           0.0756 |          16.8877 |          11.9995 |
[32m[20221213 12:27:39 @agent_ppo2.py:179][0m |           0.0214 |          16.5277 |          12.2142 |
[32m[20221213 12:27:39 @agent_ppo2.py:179][0m |          -0.0102 |          16.4868 |          12.4745 |
[32m[20221213 12:27:39 @agent_ppo2.py:179][0m |          -0.0131 |          16.4327 |          12.5544 |
[32m[20221213 12:27:40 @agent_ppo2.py:179][0m |          -0.0254 |          16.3539 |          12.6396 |
[32m[20221213 12:27:40 @agent_ppo2.py:179][0m |          -0.0321 |          16.3157 |          12.7111 |
[32m[20221213 12:27:40 @agent_ppo2.py:179][0m |          -0.0337 |          16.2924 |          12.6688 |
[32m[20221213 12:27:40 @agent_ppo2.py:179][0m |          -0.0335 |          16.2654 |          12.7118 |
[32m[20221213 12:27:40 @agent_ppo2.py:179][0m |          -0.0386 |          16.2275 |          12.7178 |
[32m[20221213 12:27:40 @agent_ppo2.py:179][0m |          -0.0398 |          16.2778 |          12.7307 |
[32m[20221213 12:27:40 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:27:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.68
[32m[20221213 12:27:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 257.64
[32m[20221213 12:27:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.32
[32m[20221213 12:27:41 @agent_ppo2.py:137][0m Total time:      11.73 min
[32m[20221213 12:27:41 @agent_ppo2.py:139][0m 776192 total steps have happened
[32m[20221213 12:27:41 @agent_ppo2.py:115][0m #------------------------ Iteration 379 --------------------------#
[32m[20221213 12:27:41 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:27:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:41 @agent_ppo2.py:179][0m |           0.0584 |          16.5313 |          11.7496 |
[32m[20221213 12:27:41 @agent_ppo2.py:179][0m |           0.0238 |          16.3190 |          11.7531 |
[32m[20221213 12:27:41 @agent_ppo2.py:179][0m |           0.0063 |          16.2008 |          11.8537 |
[32m[20221213 12:27:41 @agent_ppo2.py:179][0m |          -0.0131 |          16.1112 |          12.0916 |
[32m[20221213 12:27:42 @agent_ppo2.py:179][0m |          -0.0197 |          16.6184 |          12.3368 |
[32m[20221213 12:27:42 @agent_ppo2.py:179][0m |          -0.0168 |          17.8854 |          12.3873 |
[32m[20221213 12:27:42 @agent_ppo2.py:179][0m |          -0.0298 |          16.1019 |          12.3991 |
[32m[20221213 12:27:42 @agent_ppo2.py:179][0m |          -0.0279 |          16.2000 |          12.4222 |
[32m[20221213 12:27:42 @agent_ppo2.py:179][0m |          -0.0295 |          16.7580 |          12.4358 |
[32m[20221213 12:27:42 @agent_ppo2.py:179][0m |          -0.0361 |          15.8562 |          12.4497 |
[32m[20221213 12:27:42 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:27:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 246.62
[32m[20221213 12:27:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 255.10
[32m[20221213 12:27:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 240.45
[32m[20221213 12:27:43 @agent_ppo2.py:137][0m Total time:      11.76 min
[32m[20221213 12:27:43 @agent_ppo2.py:139][0m 778240 total steps have happened
[32m[20221213 12:27:43 @agent_ppo2.py:115][0m #------------------------ Iteration 380 --------------------------#
[32m[20221213 12:27:43 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:27:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:43 @agent_ppo2.py:179][0m |           0.0290 |          16.9079 |          12.3996 |
[32m[20221213 12:27:43 @agent_ppo2.py:179][0m |           0.0063 |          16.6244 |          12.4037 |
[32m[20221213 12:27:43 @agent_ppo2.py:179][0m |          -0.0142 |          16.5428 |          12.5721 |
[32m[20221213 12:27:43 @agent_ppo2.py:179][0m |          -0.0243 |          16.4465 |          12.6826 |
[32m[20221213 12:27:44 @agent_ppo2.py:179][0m |          -0.0289 |          16.3452 |          12.7105 |
[32m[20221213 12:27:44 @agent_ppo2.py:179][0m |          -0.0344 |          16.3682 |          12.8002 |
[32m[20221213 12:27:44 @agent_ppo2.py:179][0m |          -0.0350 |          16.2873 |          12.7933 |
[32m[20221213 12:27:44 @agent_ppo2.py:179][0m |          -0.0217 |          17.8343 |          12.7496 |
[32m[20221213 12:27:44 @agent_ppo2.py:179][0m |          -0.0236 |          16.2178 |          12.4060 |
[32m[20221213 12:27:44 @agent_ppo2.py:179][0m |          -0.0357 |          16.1683 |          12.6943 |
[32m[20221213 12:27:44 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:27:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.54
[32m[20221213 12:27:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 257.43
[32m[20221213 12:27:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 224.08
[32m[20221213 12:27:44 @agent_ppo2.py:137][0m Total time:      11.79 min
[32m[20221213 12:27:44 @agent_ppo2.py:139][0m 780288 total steps have happened
[32m[20221213 12:27:44 @agent_ppo2.py:115][0m #------------------------ Iteration 381 --------------------------#
[32m[20221213 12:27:45 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:45 @agent_ppo2.py:179][0m |           0.0588 |          16.6865 |          11.8556 |
[32m[20221213 12:27:45 @agent_ppo2.py:179][0m |           0.0213 |          16.2965 |          11.8835 |
[32m[20221213 12:27:45 @agent_ppo2.py:179][0m |           0.0069 |          16.0723 |          12.0349 |
[32m[20221213 12:27:45 @agent_ppo2.py:179][0m |          -0.0167 |          15.8233 |          12.3671 |
[32m[20221213 12:27:45 @agent_ppo2.py:179][0m |          -0.0241 |          16.0389 |          12.5457 |
[32m[20221213 12:27:46 @agent_ppo2.py:179][0m |          -0.0177 |          15.5300 |          12.3769 |
[32m[20221213 12:27:46 @agent_ppo2.py:179][0m |          -0.0327 |          15.3476 |          12.5381 |
[32m[20221213 12:27:46 @agent_ppo2.py:179][0m |          -0.0398 |          15.2445 |          12.6194 |
[32m[20221213 12:27:46 @agent_ppo2.py:179][0m |          -0.0396 |          15.1552 |          12.6025 |
[32m[20221213 12:27:46 @agent_ppo2.py:179][0m |          -0.0450 |          15.0622 |          12.6276 |
[32m[20221213 12:27:46 @agent_ppo2.py:124][0m Policy update time: 1.65 s
[32m[20221213 12:27:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 227.98
[32m[20221213 12:27:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 253.32
[32m[20221213 12:27:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 63.22
[32m[20221213 12:27:47 @agent_ppo2.py:137][0m Total time:      11.83 min
[32m[20221213 12:27:47 @agent_ppo2.py:139][0m 782336 total steps have happened
[32m[20221213 12:27:47 @agent_ppo2.py:115][0m #------------------------ Iteration 382 --------------------------#
[32m[20221213 12:27:47 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:47 @agent_ppo2.py:179][0m |           0.0708 |          15.5080 |          11.6962 |
[32m[20221213 12:27:47 @agent_ppo2.py:179][0m |           0.0274 |          14.1507 |          11.8205 |
[32m[20221213 12:27:47 @agent_ppo2.py:179][0m |          -0.0129 |          13.7466 |          12.4279 |
[32m[20221213 12:27:47 @agent_ppo2.py:179][0m |          -0.0209 |          13.4657 |          12.5316 |
[32m[20221213 12:27:47 @agent_ppo2.py:179][0m |          -0.0285 |          13.2842 |          12.5717 |
[32m[20221213 12:27:48 @agent_ppo2.py:179][0m |          -0.0272 |          13.9417 |          12.5996 |
[32m[20221213 12:27:48 @agent_ppo2.py:179][0m |          -0.0360 |          13.0947 |          12.6223 |
[32m[20221213 12:27:48 @agent_ppo2.py:179][0m |          -0.0397 |          13.0052 |          12.6297 |
[32m[20221213 12:27:48 @agent_ppo2.py:179][0m |          -0.0412 |          12.9804 |          12.6534 |
[32m[20221213 12:27:48 @agent_ppo2.py:179][0m |          -0.0437 |          12.9044 |          12.6685 |
[32m[20221213 12:27:48 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:27:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 219.37
[32m[20221213 12:27:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 234.16
[32m[20221213 12:27:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 242.26
[32m[20221213 12:27:48 @agent_ppo2.py:137][0m Total time:      11.86 min
[32m[20221213 12:27:48 @agent_ppo2.py:139][0m 784384 total steps have happened
[32m[20221213 12:27:48 @agent_ppo2.py:115][0m #------------------------ Iteration 383 --------------------------#
[32m[20221213 12:27:49 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:49 @agent_ppo2.py:179][0m |           0.0477 |          23.9917 |          12.2324 |
[32m[20221213 12:27:49 @agent_ppo2.py:179][0m |           0.0215 |          20.8176 |          11.7775 |
[32m[20221213 12:27:49 @agent_ppo2.py:179][0m |          -0.0169 |          20.2413 |          12.0100 |
[32m[20221213 12:27:49 @agent_ppo2.py:179][0m |          -0.0234 |          19.8043 |          12.1861 |
[32m[20221213 12:27:49 @agent_ppo2.py:179][0m |          -0.0348 |          19.8261 |          12.2128 |
[32m[20221213 12:27:49 @agent_ppo2.py:179][0m |          -0.0409 |          19.4452 |          12.2048 |
[32m[20221213 12:27:49 @agent_ppo2.py:179][0m |          -0.0447 |          19.1942 |          12.2510 |
[32m[20221213 12:27:50 @agent_ppo2.py:179][0m |          -0.0449 |          19.5139 |          12.2367 |
[32m[20221213 12:27:50 @agent_ppo2.py:179][0m |          -0.0536 |          18.9091 |          12.2377 |
[32m[20221213 12:27:50 @agent_ppo2.py:179][0m |          -0.0541 |          18.7148 |          12.2122 |
[32m[20221213 12:27:50 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:27:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.81
[32m[20221213 12:27:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 249.99
[32m[20221213 12:27:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 247.91
[32m[20221213 12:27:50 @agent_ppo2.py:137][0m Total time:      11.89 min
[32m[20221213 12:27:50 @agent_ppo2.py:139][0m 786432 total steps have happened
[32m[20221213 12:27:50 @agent_ppo2.py:115][0m #------------------------ Iteration 384 --------------------------#
[32m[20221213 12:27:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |           0.0502 |          18.2050 |          12.3994 |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |           0.0065 |          17.2898 |          12.2983 |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |          -0.0150 |          16.9630 |          12.6273 |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |          -0.0255 |          17.1465 |          12.7412 |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |          -0.0289 |          16.6157 |          12.7517 |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |          -0.0412 |          16.5002 |          12.8524 |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |          -0.0370 |          16.9455 |          12.8807 |
[32m[20221213 12:27:51 @agent_ppo2.py:179][0m |          -0.0442 |          16.3446 |          12.8611 |
[32m[20221213 12:27:52 @agent_ppo2.py:179][0m |          -0.0400 |          16.4106 |          12.8331 |
[32m[20221213 12:27:52 @agent_ppo2.py:179][0m |          -0.0499 |          16.1737 |          12.8972 |
[32m[20221213 12:27:52 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:27:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.64
[32m[20221213 12:27:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 246.16
[32m[20221213 12:27:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.17
[32m[20221213 12:27:52 @agent_ppo2.py:137][0m Total time:      11.92 min
[32m[20221213 12:27:52 @agent_ppo2.py:139][0m 788480 total steps have happened
[32m[20221213 12:27:52 @agent_ppo2.py:115][0m #------------------------ Iteration 385 --------------------------#
[32m[20221213 12:27:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:52 @agent_ppo2.py:179][0m |           0.0637 |          18.9563 |          12.0463 |
[32m[20221213 12:27:52 @agent_ppo2.py:179][0m |           0.0231 |          16.5128 |          11.6870 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0049 |          16.2856 |          12.1956 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0110 |          16.5767 |          12.3630 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0166 |          16.0837 |          12.2538 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0284 |          16.1572 |          12.4745 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0230 |          16.7954 |          12.4539 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0366 |          15.9719 |          12.5051 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0391 |          15.9541 |          12.5199 |
[32m[20221213 12:27:53 @agent_ppo2.py:179][0m |          -0.0406 |          15.9165 |          12.5442 |
[32m[20221213 12:27:53 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:27:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.16
[32m[20221213 12:27:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 237.12
[32m[20221213 12:27:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 237.37
[32m[20221213 12:27:54 @agent_ppo2.py:137][0m Total time:      11.95 min
[32m[20221213 12:27:54 @agent_ppo2.py:139][0m 790528 total steps have happened
[32m[20221213 12:27:54 @agent_ppo2.py:115][0m #------------------------ Iteration 386 --------------------------#
[32m[20221213 12:27:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:54 @agent_ppo2.py:179][0m |           0.0402 |          15.8711 |          12.3210 |
[32m[20221213 12:27:54 @agent_ppo2.py:179][0m |          -0.0006 |          15.4496 |          12.3252 |
[32m[20221213 12:27:54 @agent_ppo2.py:179][0m |          -0.0191 |          15.2784 |          12.5470 |
[32m[20221213 12:27:54 @agent_ppo2.py:179][0m |          -0.0311 |          15.1327 |          12.5916 |
[32m[20221213 12:27:55 @agent_ppo2.py:179][0m |          -0.0291 |          15.3516 |          12.6289 |
[32m[20221213 12:27:55 @agent_ppo2.py:179][0m |          -0.0242 |          14.8458 |          12.4900 |
[32m[20221213 12:27:55 @agent_ppo2.py:179][0m |          -0.0357 |          14.8570 |          12.5811 |
[32m[20221213 12:27:55 @agent_ppo2.py:179][0m |          -0.0358 |          14.6696 |          12.6362 |
[32m[20221213 12:27:55 @agent_ppo2.py:179][0m |          -0.0441 |          14.6193 |          12.6065 |
[32m[20221213 12:27:55 @agent_ppo2.py:179][0m |          -0.0297 |          15.6746 |          12.5969 |
[32m[20221213 12:27:55 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:27:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 225.77
[32m[20221213 12:27:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 248.80
[32m[20221213 12:27:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 253.85
[32m[20221213 12:27:55 @agent_ppo2.py:137][0m Total time:      11.98 min
[32m[20221213 12:27:55 @agent_ppo2.py:139][0m 792576 total steps have happened
[32m[20221213 12:27:55 @agent_ppo2.py:115][0m #------------------------ Iteration 387 --------------------------#
[32m[20221213 12:27:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:56 @agent_ppo2.py:179][0m |           0.0792 |          16.3325 |          11.6406 |
[32m[20221213 12:27:56 @agent_ppo2.py:179][0m |           0.0897 |          15.9105 |          10.4584 |
[32m[20221213 12:27:56 @agent_ppo2.py:179][0m |           0.0338 |          15.7393 |          11.1393 |
[32m[20221213 12:27:56 @agent_ppo2.py:179][0m |          -0.0025 |          15.5832 |          11.8565 |
[32m[20221213 12:27:56 @agent_ppo2.py:179][0m |          -0.0180 |          15.4852 |          12.1677 |
[32m[20221213 12:27:56 @agent_ppo2.py:179][0m |          -0.0263 |          15.3806 |          12.3396 |
[32m[20221213 12:27:57 @agent_ppo2.py:179][0m |          -0.0328 |          15.3373 |          12.4742 |
[32m[20221213 12:27:57 @agent_ppo2.py:179][0m |          -0.0334 |          15.2560 |          12.4920 |
[32m[20221213 12:27:57 @agent_ppo2.py:179][0m |          -0.0378 |          15.2028 |          12.5301 |
[32m[20221213 12:27:57 @agent_ppo2.py:179][0m |          -0.0396 |          15.1716 |          12.5898 |
[32m[20221213 12:27:57 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:27:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 222.02
[32m[20221213 12:27:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 236.02
[32m[20221213 12:27:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.63
[32m[20221213 12:27:57 @agent_ppo2.py:137][0m Total time:      12.01 min
[32m[20221213 12:27:57 @agent_ppo2.py:139][0m 794624 total steps have happened
[32m[20221213 12:27:57 @agent_ppo2.py:115][0m #------------------------ Iteration 388 --------------------------#
[32m[20221213 12:27:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:27:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:58 @agent_ppo2.py:179][0m |           0.0294 |          15.1337 |          12.4797 |
[32m[20221213 12:27:58 @agent_ppo2.py:179][0m |           0.0113 |          13.6693 |          12.2479 |
[32m[20221213 12:27:58 @agent_ppo2.py:179][0m |          -0.0196 |          13.1925 |          12.5633 |
[32m[20221213 12:27:58 @agent_ppo2.py:179][0m |          -0.0309 |          12.9836 |          12.6766 |
[32m[20221213 12:27:58 @agent_ppo2.py:179][0m |          -0.0345 |          12.8077 |          12.7202 |
[32m[20221213 12:27:58 @agent_ppo2.py:179][0m |          -0.0344 |          13.0266 |          12.7291 |
[32m[20221213 12:27:58 @agent_ppo2.py:179][0m |          -0.0367 |          12.6624 |          12.7234 |
[32m[20221213 12:27:59 @agent_ppo2.py:179][0m |          -0.0384 |          12.6191 |          12.7361 |
[32m[20221213 12:27:59 @agent_ppo2.py:179][0m |          -0.0403 |          12.5736 |          12.6774 |
[32m[20221213 12:27:59 @agent_ppo2.py:179][0m |          -0.0417 |          12.5141 |          12.6877 |
[32m[20221213 12:27:59 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:27:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 248.03
[32m[20221213 12:27:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.67
[32m[20221213 12:27:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 246.71
[32m[20221213 12:27:59 @agent_ppo2.py:137][0m Total time:      12.04 min
[32m[20221213 12:27:59 @agent_ppo2.py:139][0m 796672 total steps have happened
[32m[20221213 12:27:59 @agent_ppo2.py:115][0m #------------------------ Iteration 389 --------------------------#
[32m[20221213 12:27:59 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:27:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:27:59 @agent_ppo2.py:179][0m |           0.0311 |          16.2570 |          12.3605 |
[32m[20221213 12:28:00 @agent_ppo2.py:179][0m |           0.0144 |          15.8028 |          12.2186 |
[32m[20221213 12:28:00 @agent_ppo2.py:179][0m |          -0.0060 |          15.6970 |          12.5148 |
[32m[20221213 12:28:00 @agent_ppo2.py:179][0m |          -0.0080 |          15.7607 |          12.2593 |
[32m[20221213 12:28:00 @agent_ppo2.py:179][0m |          -0.0144 |          15.6720 |          12.5546 |
[32m[20221213 12:28:00 @agent_ppo2.py:179][0m |          -0.0262 |          15.5102 |          12.6432 |
[32m[20221213 12:28:00 @agent_ppo2.py:179][0m |          -0.0322 |          15.4895 |          12.7135 |
[32m[20221213 12:28:00 @agent_ppo2.py:179][0m |          -0.0348 |          15.4293 |          12.7693 |
[32m[20221213 12:28:01 @agent_ppo2.py:179][0m |          -0.0321 |          15.4020 |          12.7176 |
[32m[20221213 12:28:01 @agent_ppo2.py:179][0m |          -0.0357 |          15.3853 |          12.7386 |
[32m[20221213 12:28:01 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:28:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.93
[32m[20221213 12:28:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 254.65
[32m[20221213 12:28:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 248.69
[32m[20221213 12:28:01 @agent_ppo2.py:137][0m Total time:      12.07 min
[32m[20221213 12:28:01 @agent_ppo2.py:139][0m 798720 total steps have happened
[32m[20221213 12:28:01 @agent_ppo2.py:115][0m #------------------------ Iteration 390 --------------------------#
[32m[20221213 12:28:01 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:28:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:01 @agent_ppo2.py:179][0m |           0.0615 |          17.8766 |          11.3769 |
[32m[20221213 12:28:01 @agent_ppo2.py:179][0m |           0.1023 |          17.0262 |           9.0602 |
[32m[20221213 12:28:02 @agent_ppo2.py:179][0m |           0.0484 |          16.8301 |          10.8129 |
[32m[20221213 12:28:02 @agent_ppo2.py:179][0m |           0.0147 |          16.6375 |          11.6054 |
[32m[20221213 12:28:02 @agent_ppo2.py:179][0m |          -0.0093 |          16.5707 |          12.1518 |
[32m[20221213 12:28:02 @agent_ppo2.py:179][0m |          -0.0165 |          16.5508 |          12.3735 |
[32m[20221213 12:28:02 @agent_ppo2.py:179][0m |          -0.0201 |          16.5776 |          12.4240 |
[32m[20221213 12:28:02 @agent_ppo2.py:179][0m |          -0.0270 |          16.4657 |          12.5466 |
[32m[20221213 12:28:03 @agent_ppo2.py:179][0m |          -0.0316 |          16.4142 |          12.6211 |
[32m[20221213 12:28:03 @agent_ppo2.py:179][0m |          -0.0345 |          16.4171 |          12.6238 |
[32m[20221213 12:28:03 @agent_ppo2.py:124][0m Policy update time: 1.58 s
[32m[20221213 12:28:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.90
[32m[20221213 12:28:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 263.95
[32m[20221213 12:28:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 246.20
[32m[20221213 12:28:03 @agent_ppo2.py:137][0m Total time:      12.10 min
[32m[20221213 12:28:03 @agent_ppo2.py:139][0m 800768 total steps have happened
[32m[20221213 12:28:03 @agent_ppo2.py:115][0m #------------------------ Iteration 391 --------------------------#
[32m[20221213 12:28:03 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:28:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:03 @agent_ppo2.py:179][0m |           0.1029 |          18.9255 |          12.1305 |
[32m[20221213 12:28:04 @agent_ppo2.py:179][0m |           0.0391 |          17.6785 |          11.6848 |
[32m[20221213 12:28:04 @agent_ppo2.py:179][0m |           0.0176 |          17.2554 |          11.5846 |
[32m[20221213 12:28:04 @agent_ppo2.py:179][0m |          -0.0200 |          17.2166 |          12.2704 |
[32m[20221213 12:28:04 @agent_ppo2.py:179][0m |          -0.0323 |          17.0008 |          12.4003 |
[32m[20221213 12:28:04 @agent_ppo2.py:179][0m |          -0.0342 |          16.8323 |          12.4799 |
[32m[20221213 12:28:04 @agent_ppo2.py:179][0m |          -0.0308 |          17.8479 |          12.4898 |
[32m[20221213 12:28:04 @agent_ppo2.py:179][0m |          -0.0421 |          16.8930 |          12.5123 |
[32m[20221213 12:28:05 @agent_ppo2.py:179][0m |          -0.0495 |          16.6242 |          12.5211 |
[32m[20221213 12:28:05 @agent_ppo2.py:179][0m |          -0.0472 |          16.5872 |          12.5300 |
[32m[20221213 12:28:05 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:28:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.86
[32m[20221213 12:28:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 246.71
[32m[20221213 12:28:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 251.04
[32m[20221213 12:28:05 @agent_ppo2.py:137][0m Total time:      12.14 min
[32m[20221213 12:28:05 @agent_ppo2.py:139][0m 802816 total steps have happened
[32m[20221213 12:28:05 @agent_ppo2.py:115][0m #------------------------ Iteration 392 --------------------------#
[32m[20221213 12:28:05 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:28:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:05 @agent_ppo2.py:179][0m |           0.0307 |          17.9375 |          12.4504 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0077 |          17.4356 |          12.4457 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0232 |          17.2330 |          12.5927 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0303 |          17.1805 |          12.6181 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0318 |          17.0444 |          12.6162 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0404 |          16.9239 |          12.6346 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0434 |          16.8888 |          12.6591 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0434 |          16.7792 |          12.6597 |
[32m[20221213 12:28:06 @agent_ppo2.py:179][0m |          -0.0433 |          16.7422 |          12.6586 |
[32m[20221213 12:28:07 @agent_ppo2.py:179][0m |          -0.0461 |          16.7441 |          12.5964 |
[32m[20221213 12:28:07 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:28:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.45
[32m[20221213 12:28:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 253.51
[32m[20221213 12:28:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 229.34
[32m[20221213 12:28:07 @agent_ppo2.py:137][0m Total time:      12.17 min
[32m[20221213 12:28:07 @agent_ppo2.py:139][0m 804864 total steps have happened
[32m[20221213 12:28:07 @agent_ppo2.py:115][0m #------------------------ Iteration 393 --------------------------#
[32m[20221213 12:28:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:07 @agent_ppo2.py:179][0m |           0.0396 |          16.8523 |          12.3031 |
[32m[20221213 12:28:07 @agent_ppo2.py:179][0m |           0.0032 |          16.2574 |          12.1900 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0196 |          16.0167 |          12.4274 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0343 |          15.8566 |          12.4361 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0348 |          15.7835 |          12.4437 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0427 |          15.5970 |          12.5183 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0484 |          15.5163 |          12.5085 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0427 |          15.3701 |          12.4653 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0483 |          15.2977 |          12.4674 |
[32m[20221213 12:28:08 @agent_ppo2.py:179][0m |          -0.0524 |          15.2220 |          12.4412 |
[32m[20221213 12:28:08 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:28:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 203.58
[32m[20221213 12:28:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 228.24
[32m[20221213 12:28:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 244.90
[32m[20221213 12:28:09 @agent_ppo2.py:137][0m Total time:      12.20 min
[32m[20221213 12:28:09 @agent_ppo2.py:139][0m 806912 total steps have happened
[32m[20221213 12:28:09 @agent_ppo2.py:115][0m #------------------------ Iteration 394 --------------------------#
[32m[20221213 12:28:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:09 @agent_ppo2.py:179][0m |           0.0528 |          17.9359 |          12.4201 |
[32m[20221213 12:28:09 @agent_ppo2.py:179][0m |           0.0623 |          17.5186 |          11.6928 |
[32m[20221213 12:28:09 @agent_ppo2.py:179][0m |           0.0005 |          17.3347 |          12.1627 |
[32m[20221213 12:28:10 @agent_ppo2.py:179][0m |          -0.0067 |          18.7892 |          12.3851 |
[32m[20221213 12:28:10 @agent_ppo2.py:179][0m |          -0.0275 |          17.1844 |          12.4778 |
[32m[20221213 12:28:10 @agent_ppo2.py:179][0m |          -0.0318 |          17.1060 |          12.5590 |
[32m[20221213 12:28:10 @agent_ppo2.py:179][0m |          -0.0355 |          17.0634 |          12.5850 |
[32m[20221213 12:28:10 @agent_ppo2.py:179][0m |          -0.0422 |          17.0574 |          12.6579 |
[32m[20221213 12:28:10 @agent_ppo2.py:179][0m |          -0.0320 |          17.8729 |          12.6243 |
[32m[20221213 12:28:10 @agent_ppo2.py:179][0m |          -0.0089 |          17.8711 |          12.2136 |
[32m[20221213 12:28:10 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:28:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.02
[32m[20221213 12:28:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 251.44
[32m[20221213 12:28:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 264.46
[32m[20221213 12:28:11 @agent_ppo2.py:137][0m Total time:      12.23 min
[32m[20221213 12:28:11 @agent_ppo2.py:139][0m 808960 total steps have happened
[32m[20221213 12:28:11 @agent_ppo2.py:115][0m #------------------------ Iteration 395 --------------------------#
[32m[20221213 12:28:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:11 @agent_ppo2.py:179][0m |           0.0472 |          14.5141 |          12.5691 |
[32m[20221213 12:28:11 @agent_ppo2.py:179][0m |           0.0287 |          13.6242 |          11.9392 |
[32m[20221213 12:28:11 @agent_ppo2.py:179][0m |          -0.0082 |          13.4259 |          12.4622 |
[32m[20221213 12:28:11 @agent_ppo2.py:179][0m |          -0.0228 |          13.2791 |          12.6219 |
[32m[20221213 12:28:12 @agent_ppo2.py:179][0m |          -0.0326 |          13.2230 |          12.6863 |
[32m[20221213 12:28:12 @agent_ppo2.py:179][0m |          -0.0337 |          13.0905 |          12.6886 |
[32m[20221213 12:28:12 @agent_ppo2.py:179][0m |          -0.0355 |          13.0658 |          12.7372 |
[32m[20221213 12:28:12 @agent_ppo2.py:179][0m |          -0.0307 |          13.1858 |          12.6830 |
[32m[20221213 12:28:12 @agent_ppo2.py:179][0m |          -0.0392 |          12.9015 |          12.7033 |
[32m[20221213 12:28:12 @agent_ppo2.py:179][0m |          -0.0415 |          12.7087 |          12.7219 |
[32m[20221213 12:28:12 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:28:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.34
[32m[20221213 12:28:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 257.69
[32m[20221213 12:28:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 239.60
[32m[20221213 12:28:12 @agent_ppo2.py:137][0m Total time:      12.26 min
[32m[20221213 12:28:12 @agent_ppo2.py:139][0m 811008 total steps have happened
[32m[20221213 12:28:12 @agent_ppo2.py:115][0m #------------------------ Iteration 396 --------------------------#
[32m[20221213 12:28:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:13 @agent_ppo2.py:179][0m |           0.0443 |          17.1759 |          12.4004 |
[32m[20221213 12:28:13 @agent_ppo2.py:179][0m |           0.0329 |          18.1269 |          12.0933 |
[32m[20221213 12:28:13 @agent_ppo2.py:179][0m |           0.0076 |          16.3641 |          12.1945 |
[32m[20221213 12:28:13 @agent_ppo2.py:179][0m |          -0.0135 |          16.2681 |          12.5797 |
[32m[20221213 12:28:13 @agent_ppo2.py:179][0m |          -0.0231 |          16.2072 |          12.6651 |
[32m[20221213 12:28:14 @agent_ppo2.py:179][0m |          -0.0259 |          16.1791 |          12.7436 |
[32m[20221213 12:28:14 @agent_ppo2.py:179][0m |          -0.0250 |          16.6993 |          12.7879 |
[32m[20221213 12:28:14 @agent_ppo2.py:179][0m |          -0.0348 |          16.0856 |          12.8348 |
[32m[20221213 12:28:14 @agent_ppo2.py:179][0m |          -0.0275 |          18.1218 |          12.8622 |
[32m[20221213 12:28:14 @agent_ppo2.py:179][0m |          -0.0100 |          16.1123 |          12.4644 |
[32m[20221213 12:28:14 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:28:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 233.84
[32m[20221213 12:28:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 248.19
[32m[20221213 12:28:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.50
[32m[20221213 12:28:14 @agent_ppo2.py:137][0m Total time:      12.29 min
[32m[20221213 12:28:14 @agent_ppo2.py:139][0m 813056 total steps have happened
[32m[20221213 12:28:14 @agent_ppo2.py:115][0m #------------------------ Iteration 397 --------------------------#
[32m[20221213 12:28:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:15 @agent_ppo2.py:179][0m |           0.0434 |          16.1572 |          12.1657 |
[32m[20221213 12:28:15 @agent_ppo2.py:179][0m |           0.0152 |          15.7775 |          12.1251 |
[32m[20221213 12:28:15 @agent_ppo2.py:179][0m |          -0.0078 |          15.6054 |          12.2988 |
[32m[20221213 12:28:15 @agent_ppo2.py:179][0m |          -0.0116 |          15.6396 |          12.4055 |
[32m[20221213 12:28:15 @agent_ppo2.py:179][0m |          -0.0259 |          15.6631 |          12.5222 |
[32m[20221213 12:28:15 @agent_ppo2.py:179][0m |          -0.0333 |          15.4918 |          12.5858 |
[32m[20221213 12:28:15 @agent_ppo2.py:179][0m |          -0.0325 |          15.4095 |          12.5305 |
[32m[20221213 12:28:16 @agent_ppo2.py:179][0m |          -0.0393 |          15.3846 |          12.6015 |
[32m[20221213 12:28:16 @agent_ppo2.py:179][0m |          -0.0385 |          15.3515 |          12.5960 |
[32m[20221213 12:28:16 @agent_ppo2.py:179][0m |          -0.0378 |          15.2876 |          12.5563 |
[32m[20221213 12:28:16 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:28:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.05
[32m[20221213 12:28:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 248.22
[32m[20221213 12:28:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.64
[32m[20221213 12:28:16 @agent_ppo2.py:137][0m Total time:      12.32 min
[32m[20221213 12:28:16 @agent_ppo2.py:139][0m 815104 total steps have happened
[32m[20221213 12:28:16 @agent_ppo2.py:115][0m #------------------------ Iteration 398 --------------------------#
[32m[20221213 12:28:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:16 @agent_ppo2.py:179][0m |           0.0439 |          17.7089 |          12.4821 |
[32m[20221213 12:28:17 @agent_ppo2.py:179][0m |           0.0145 |          16.7588 |          12.2688 |
[32m[20221213 12:28:17 @agent_ppo2.py:179][0m |          -0.0135 |          16.4941 |          12.5487 |
[32m[20221213 12:28:17 @agent_ppo2.py:179][0m |          -0.0337 |          16.2993 |          12.6799 |
[32m[20221213 12:28:17 @agent_ppo2.py:179][0m |          -0.0415 |          16.1155 |          12.7125 |
[32m[20221213 12:28:17 @agent_ppo2.py:179][0m |          -0.0457 |          15.9764 |          12.7148 |
[32m[20221213 12:28:17 @agent_ppo2.py:179][0m |          -0.0396 |          16.4691 |          12.7240 |
[32m[20221213 12:28:17 @agent_ppo2.py:179][0m |          -0.0506 |          15.7555 |          12.7053 |
[32m[20221213 12:28:18 @agent_ppo2.py:179][0m |          -0.0491 |          15.6562 |          12.7499 |
[32m[20221213 12:28:18 @agent_ppo2.py:179][0m |          -0.0515 |          15.5694 |          12.7501 |
[32m[20221213 12:28:18 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:28:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 217.01
[32m[20221213 12:28:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 235.64
[32m[20221213 12:28:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 248.22
[32m[20221213 12:28:18 @agent_ppo2.py:137][0m Total time:      12.35 min
[32m[20221213 12:28:18 @agent_ppo2.py:139][0m 817152 total steps have happened
[32m[20221213 12:28:18 @agent_ppo2.py:115][0m #------------------------ Iteration 399 --------------------------#
[32m[20221213 12:28:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:18 @agent_ppo2.py:179][0m |           0.0341 |          15.3572 |          12.7161 |
[32m[20221213 12:28:18 @agent_ppo2.py:179][0m |          -0.0025 |          14.9542 |          12.6771 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0080 |          14.8853 |          12.6903 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0258 |          14.6952 |          12.9683 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0279 |          14.6819 |          12.9742 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0376 |          14.5540 |          12.9401 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0369 |          14.4915 |          12.9461 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0402 |          14.4211 |          12.9531 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0321 |          15.9317 |          12.9061 |
[32m[20221213 12:28:19 @agent_ppo2.py:179][0m |          -0.0429 |          14.3617 |          12.8753 |
[32m[20221213 12:28:19 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:28:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 179.52
[32m[20221213 12:28:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 240.53
[32m[20221213 12:28:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 267.22
[32m[20221213 12:28:20 @agent_ppo2.py:137][0m Total time:      12.38 min
[32m[20221213 12:28:20 @agent_ppo2.py:139][0m 819200 total steps have happened
[32m[20221213 12:28:20 @agent_ppo2.py:115][0m #------------------------ Iteration 400 --------------------------#
[32m[20221213 12:28:20 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:28:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:20 @agent_ppo2.py:179][0m |           0.0375 |          16.4414 |          12.3115 |
[32m[20221213 12:28:20 @agent_ppo2.py:179][0m |           0.0106 |          15.9439 |          12.1144 |
[32m[20221213 12:28:20 @agent_ppo2.py:179][0m |          -0.0059 |          17.8338 |          12.4417 |
[32m[20221213 12:28:21 @agent_ppo2.py:179][0m |          -0.0311 |          15.6388 |          12.5713 |
[32m[20221213 12:28:21 @agent_ppo2.py:179][0m |          -0.0335 |          15.4422 |          12.5960 |
[32m[20221213 12:28:21 @agent_ppo2.py:179][0m |          -0.0342 |          15.3145 |          12.5855 |
[32m[20221213 12:28:21 @agent_ppo2.py:179][0m |          -0.0355 |          15.3460 |          12.5952 |
[32m[20221213 12:28:21 @agent_ppo2.py:179][0m |          -0.0365 |          15.1553 |          12.5661 |
[32m[20221213 12:28:21 @agent_ppo2.py:179][0m |          -0.0349 |          15.0798 |          12.5788 |
[32m[20221213 12:28:21 @agent_ppo2.py:179][0m |          -0.0415 |          15.0005 |          12.5479 |
[32m[20221213 12:28:21 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:28:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.11
[32m[20221213 12:28:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 252.19
[32m[20221213 12:28:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 251.26
[32m[20221213 12:28:22 @agent_ppo2.py:137][0m Total time:      12.41 min
[32m[20221213 12:28:22 @agent_ppo2.py:139][0m 821248 total steps have happened
[32m[20221213 12:28:22 @agent_ppo2.py:115][0m #------------------------ Iteration 401 --------------------------#
[32m[20221213 12:28:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:22 @agent_ppo2.py:179][0m |           0.1755 |          19.3492 |          11.9040 |
[32m[20221213 12:28:22 @agent_ppo2.py:179][0m |           0.0377 |          17.6223 |          11.2983 |
[32m[20221213 12:28:22 @agent_ppo2.py:179][0m |          -0.0031 |          17.2745 |          12.1294 |
[32m[20221213 12:28:22 @agent_ppo2.py:179][0m |          -0.0198 |          17.1216 |          12.4274 |
[32m[20221213 12:28:23 @agent_ppo2.py:179][0m |          -0.0258 |          16.8908 |          12.5298 |
[32m[20221213 12:28:23 @agent_ppo2.py:179][0m |          -0.0363 |          16.7797 |          12.6300 |
[32m[20221213 12:28:23 @agent_ppo2.py:179][0m |          -0.0398 |          16.6357 |          12.6386 |
[32m[20221213 12:28:23 @agent_ppo2.py:179][0m |          -0.0434 |          16.5650 |          12.6541 |
[32m[20221213 12:28:23 @agent_ppo2.py:179][0m |          -0.0424 |          16.5014 |          12.6238 |
[32m[20221213 12:28:23 @agent_ppo2.py:179][0m |          -0.0439 |          16.3736 |          12.6201 |
[32m[20221213 12:28:23 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:28:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.36
[32m[20221213 12:28:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 252.70
[32m[20221213 12:28:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.52
[32m[20221213 12:28:23 @agent_ppo2.py:137][0m Total time:      12.44 min
[32m[20221213 12:28:23 @agent_ppo2.py:139][0m 823296 total steps have happened
[32m[20221213 12:28:23 @agent_ppo2.py:115][0m #------------------------ Iteration 402 --------------------------#
[32m[20221213 12:28:24 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:24 @agent_ppo2.py:179][0m |           0.0458 |          17.4817 |          12.7171 |
[32m[20221213 12:28:24 @agent_ppo2.py:179][0m |           0.0124 |          17.0135 |          12.5721 |
[32m[20221213 12:28:24 @agent_ppo2.py:179][0m |          -0.0089 |          16.7933 |          12.8556 |
[32m[20221213 12:28:24 @agent_ppo2.py:179][0m |          -0.0259 |          16.6214 |          13.0102 |
[32m[20221213 12:28:24 @agent_ppo2.py:179][0m |          -0.0297 |          16.5160 |          13.1247 |
[32m[20221213 12:28:24 @agent_ppo2.py:179][0m |          -0.0353 |          16.4617 |          13.1594 |
[32m[20221213 12:28:25 @agent_ppo2.py:179][0m |          -0.0388 |          16.3557 |          13.1715 |
[32m[20221213 12:28:25 @agent_ppo2.py:179][0m |          -0.0380 |          16.3157 |          13.1801 |
[32m[20221213 12:28:25 @agent_ppo2.py:179][0m |          -0.0388 |          16.4451 |          13.1729 |
[32m[20221213 12:28:25 @agent_ppo2.py:179][0m |          -0.0432 |          16.2436 |          13.1973 |
[32m[20221213 12:28:25 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:28:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.28
[32m[20221213 12:28:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 246.37
[32m[20221213 12:28:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 283.33
[32m[20221213 12:28:25 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 283.33
[32m[20221213 12:28:25 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 283.33
[32m[20221213 12:28:25 @agent_ppo2.py:137][0m Total time:      12.47 min
[32m[20221213 12:28:25 @agent_ppo2.py:139][0m 825344 total steps have happened
[32m[20221213 12:28:25 @agent_ppo2.py:115][0m #------------------------ Iteration 403 --------------------------#
[32m[20221213 12:28:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:26 @agent_ppo2.py:179][0m |           0.0762 |          16.9626 |          12.3867 |
[32m[20221213 12:28:26 @agent_ppo2.py:179][0m |           0.0636 |          16.6325 |          11.8987 |
[32m[20221213 12:28:26 @agent_ppo2.py:179][0m |           0.0021 |          16.3095 |          12.3454 |
[32m[20221213 12:28:26 @agent_ppo2.py:179][0m |          -0.0227 |          16.1920 |          12.6031 |
[32m[20221213 12:28:26 @agent_ppo2.py:179][0m |          -0.0311 |          16.0197 |          12.6589 |
[32m[20221213 12:28:26 @agent_ppo2.py:179][0m |          -0.0319 |          15.9332 |          12.7174 |
[32m[20221213 12:28:26 @agent_ppo2.py:179][0m |          -0.0358 |          15.8968 |          12.7065 |
[32m[20221213 12:28:27 @agent_ppo2.py:179][0m |          -0.0382 |          15.8282 |          12.7134 |
[32m[20221213 12:28:27 @agent_ppo2.py:179][0m |          -0.0431 |          15.7762 |          12.7415 |
[32m[20221213 12:28:27 @agent_ppo2.py:179][0m |          -0.0408 |          15.8624 |          12.7093 |
[32m[20221213 12:28:27 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:28:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 218.11
[32m[20221213 12:28:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 241.75
[32m[20221213 12:28:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.79
[32m[20221213 12:28:27 @agent_ppo2.py:137][0m Total time:      12.51 min
[32m[20221213 12:28:27 @agent_ppo2.py:139][0m 827392 total steps have happened
[32m[20221213 12:28:27 @agent_ppo2.py:115][0m #------------------------ Iteration 404 --------------------------#
[32m[20221213 12:28:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:27 @agent_ppo2.py:179][0m |           0.0401 |          18.3289 |          12.4710 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |           0.0111 |          17.6151 |          12.3564 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |          -0.0180 |          17.4144 |          12.5776 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |          -0.0297 |          17.3210 |          12.6245 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |          -0.0297 |          17.3056 |          12.6367 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |          -0.0315 |          18.0498 |          12.6435 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |          -0.0405 |          17.0832 |          12.6694 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |          -0.0383 |          17.5206 |          12.6316 |
[32m[20221213 12:28:28 @agent_ppo2.py:179][0m |          -0.0470 |          16.9702 |          12.6512 |
[32m[20221213 12:28:29 @agent_ppo2.py:179][0m |          -0.0323 |          18.3107 |          12.5983 |
[32m[20221213 12:28:29 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:28:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 242.01
[32m[20221213 12:28:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 248.40
[32m[20221213 12:28:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.08
[32m[20221213 12:28:29 @agent_ppo2.py:137][0m Total time:      12.53 min
[32m[20221213 12:28:29 @agent_ppo2.py:139][0m 829440 total steps have happened
[32m[20221213 12:28:29 @agent_ppo2.py:115][0m #------------------------ Iteration 405 --------------------------#
[32m[20221213 12:28:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:29 @agent_ppo2.py:179][0m |           0.0819 |          17.6398 |          11.6967 |
[32m[20221213 12:28:29 @agent_ppo2.py:179][0m |           0.0373 |          17.1888 |          11.7889 |
[32m[20221213 12:28:29 @agent_ppo2.py:179][0m |          -0.0008 |          16.9782 |          12.5798 |
[32m[20221213 12:28:30 @agent_ppo2.py:179][0m |           0.0006 |          17.6132 |          12.6472 |
[32m[20221213 12:28:30 @agent_ppo2.py:179][0m |          -0.0048 |          18.0492 |          12.6231 |
[32m[20221213 12:28:30 @agent_ppo2.py:179][0m |          -0.0146 |          16.8199 |          12.6124 |
[32m[20221213 12:28:30 @agent_ppo2.py:179][0m |          -0.0223 |          16.7583 |          12.7835 |
[32m[20221213 12:28:30 @agent_ppo2.py:179][0m |          -0.0266 |          16.6325 |          12.7336 |
[32m[20221213 12:28:30 @agent_ppo2.py:179][0m |          -0.0399 |          16.4464 |          12.8568 |
[32m[20221213 12:28:30 @agent_ppo2.py:179][0m |          -0.0347 |          16.6488 |          12.8843 |
[32m[20221213 12:28:30 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:28:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.14
[32m[20221213 12:28:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 250.47
[32m[20221213 12:28:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 244.97
[32m[20221213 12:28:31 @agent_ppo2.py:137][0m Total time:      12.56 min
[32m[20221213 12:28:31 @agent_ppo2.py:139][0m 831488 total steps have happened
[32m[20221213 12:28:31 @agent_ppo2.py:115][0m #------------------------ Iteration 406 --------------------------#
[32m[20221213 12:28:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:31 @agent_ppo2.py:179][0m |           0.0551 |          16.9543 |          12.4572 |
[32m[20221213 12:28:31 @agent_ppo2.py:179][0m |           0.0693 |          16.5186 |          11.3669 |
[32m[20221213 12:28:31 @agent_ppo2.py:179][0m |           0.0074 |          16.2986 |          12.2952 |
[32m[20221213 12:28:31 @agent_ppo2.py:179][0m |          -0.0046 |          16.8827 |          12.7321 |
[32m[20221213 12:28:32 @agent_ppo2.py:179][0m |          -0.0190 |          15.9610 |          12.8733 |
[32m[20221213 12:28:32 @agent_ppo2.py:179][0m |          -0.0124 |          18.0146 |          12.9665 |
[32m[20221213 12:28:32 @agent_ppo2.py:179][0m |          -0.0277 |          15.7171 |          12.9588 |
[32m[20221213 12:28:32 @agent_ppo2.py:179][0m |          -0.0314 |          15.5900 |          12.9913 |
[32m[20221213 12:28:32 @agent_ppo2.py:179][0m |          -0.0337 |          15.5085 |          13.0542 |
[32m[20221213 12:28:32 @agent_ppo2.py:179][0m |          -0.0354 |          15.4294 |          13.0352 |
[32m[20221213 12:28:32 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:28:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.71
[32m[20221213 12:28:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 266.46
[32m[20221213 12:28:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 204.71
[32m[20221213 12:28:32 @agent_ppo2.py:137][0m Total time:      12.60 min
[32m[20221213 12:28:32 @agent_ppo2.py:139][0m 833536 total steps have happened
[32m[20221213 12:28:32 @agent_ppo2.py:115][0m #------------------------ Iteration 407 --------------------------#
[32m[20221213 12:28:33 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:33 @agent_ppo2.py:179][0m |           0.0465 |          17.6790 |          12.2624 |
[32m[20221213 12:28:33 @agent_ppo2.py:179][0m |           0.0211 |          17.0317 |          11.9110 |
[32m[20221213 12:28:33 @agent_ppo2.py:179][0m |          -0.0107 |          16.8324 |          12.5842 |
[32m[20221213 12:28:33 @agent_ppo2.py:179][0m |          -0.0168 |          16.6622 |          12.6629 |
[32m[20221213 12:28:33 @agent_ppo2.py:179][0m |          -0.0202 |          17.4948 |          12.7580 |
[32m[20221213 12:28:34 @agent_ppo2.py:179][0m |          -0.0294 |          16.4136 |          12.8118 |
[32m[20221213 12:28:34 @agent_ppo2.py:179][0m |          -0.0370 |          16.3974 |          12.8368 |
[32m[20221213 12:28:34 @agent_ppo2.py:179][0m |          -0.0364 |          16.2880 |          12.8041 |
[32m[20221213 12:28:34 @agent_ppo2.py:179][0m |          -0.0416 |          16.3028 |          12.8484 |
[32m[20221213 12:28:34 @agent_ppo2.py:179][0m |          -0.0365 |          16.3085 |          12.8756 |
[32m[20221213 12:28:34 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:28:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.83
[32m[20221213 12:28:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 268.92
[32m[20221213 12:28:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 238.94
[32m[20221213 12:28:34 @agent_ppo2.py:137][0m Total time:      12.63 min
[32m[20221213 12:28:34 @agent_ppo2.py:139][0m 835584 total steps have happened
[32m[20221213 12:28:34 @agent_ppo2.py:115][0m #------------------------ Iteration 408 --------------------------#
[32m[20221213 12:28:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:35 @agent_ppo2.py:179][0m |           0.0458 |          17.1894 |          12.2174 |
[32m[20221213 12:28:35 @agent_ppo2.py:179][0m |           0.0137 |          16.9931 |          12.1543 |
[32m[20221213 12:28:35 @agent_ppo2.py:179][0m |          -0.0155 |          16.6286 |          12.3723 |
[32m[20221213 12:28:35 @agent_ppo2.py:179][0m |          -0.0244 |          16.4864 |          12.4647 |
[32m[20221213 12:28:35 @agent_ppo2.py:179][0m |          -0.0342 |          16.3713 |          12.5865 |
[32m[20221213 12:28:35 @agent_ppo2.py:179][0m |          -0.0261 |          17.5736 |          12.5499 |
[32m[20221213 12:28:36 @agent_ppo2.py:179][0m |          -0.0394 |          16.2352 |          12.5665 |
[32m[20221213 12:28:36 @agent_ppo2.py:179][0m |          -0.0365 |          16.1942 |          12.6145 |
[32m[20221213 12:28:36 @agent_ppo2.py:179][0m |          -0.0345 |          16.3213 |          12.6324 |
[32m[20221213 12:28:36 @agent_ppo2.py:179][0m |          -0.0393 |          16.0910 |          12.6035 |
[32m[20221213 12:28:36 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:28:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 233.37
[32m[20221213 12:28:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 254.14
[32m[20221213 12:28:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 238.54
[32m[20221213 12:28:36 @agent_ppo2.py:137][0m Total time:      12.66 min
[32m[20221213 12:28:36 @agent_ppo2.py:139][0m 837632 total steps have happened
[32m[20221213 12:28:36 @agent_ppo2.py:115][0m #------------------------ Iteration 409 --------------------------#
[32m[20221213 12:28:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:37 @agent_ppo2.py:179][0m |           0.0440 |          14.1773 |          12.1790 |
[32m[20221213 12:28:37 @agent_ppo2.py:179][0m |           0.0501 |          13.5983 |          10.7412 |
[32m[20221213 12:28:37 @agent_ppo2.py:179][0m |          -0.0072 |          13.5016 |           9.8872 |
[32m[20221213 12:28:37 @agent_ppo2.py:179][0m |          -0.0297 |          13.1732 |          10.1602 |
[32m[20221213 12:28:37 @agent_ppo2.py:179][0m |          -0.0355 |          13.1200 |          10.1960 |
[32m[20221213 12:28:37 @agent_ppo2.py:179][0m |          -0.0310 |          14.8023 |          10.2186 |
[32m[20221213 12:28:37 @agent_ppo2.py:179][0m |          -0.0448 |          12.9759 |          10.2039 |
[32m[20221213 12:28:38 @agent_ppo2.py:179][0m |          -0.0444 |          12.7692 |          10.1762 |
[32m[20221213 12:28:38 @agent_ppo2.py:179][0m |          -0.0534 |          12.6319 |          10.2231 |
[32m[20221213 12:28:38 @agent_ppo2.py:179][0m |          -0.0513 |          12.7620 |          10.2121 |
[32m[20221213 12:28:38 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:28:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 187.90
[32m[20221213 12:28:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 244.61
[32m[20221213 12:28:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 257.92
[32m[20221213 12:28:38 @agent_ppo2.py:137][0m Total time:      12.69 min
[32m[20221213 12:28:38 @agent_ppo2.py:139][0m 839680 total steps have happened
[32m[20221213 12:28:38 @agent_ppo2.py:115][0m #------------------------ Iteration 410 --------------------------#
[32m[20221213 12:28:38 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:28:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:38 @agent_ppo2.py:179][0m |           0.0324 |          15.3638 |          12.6688 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |           0.0163 |          14.7966 |          12.6687 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |          -0.0056 |          14.6191 |          12.7425 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |          -0.0061 |          14.6323 |          12.6987 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |          -0.0160 |          14.4615 |          12.6800 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |          -0.0231 |          14.3522 |          12.8223 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |          -0.0339 |          14.9048 |          12.8234 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |          -0.0323 |          14.5563 |          12.8684 |
[32m[20221213 12:28:39 @agent_ppo2.py:179][0m |          -0.0397 |          14.1693 |          12.9041 |
[32m[20221213 12:28:40 @agent_ppo2.py:179][0m |          -0.0394 |          14.1316 |          12.8996 |
[32m[20221213 12:28:40 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:28:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.10
[32m[20221213 12:28:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 240.56
[32m[20221213 12:28:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 256.91
[32m[20221213 12:28:40 @agent_ppo2.py:137][0m Total time:      12.72 min
[32m[20221213 12:28:40 @agent_ppo2.py:139][0m 841728 total steps have happened
[32m[20221213 12:28:40 @agent_ppo2.py:115][0m #------------------------ Iteration 411 --------------------------#
[32m[20221213 12:28:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:40 @agent_ppo2.py:179][0m |           0.0335 |          17.5238 |          12.7317 |
[32m[20221213 12:28:40 @agent_ppo2.py:179][0m |           0.0064 |          16.7369 |          12.4716 |
[32m[20221213 12:28:40 @agent_ppo2.py:179][0m |          -0.0169 |          16.3017 |          12.8351 |
[32m[20221213 12:28:41 @agent_ppo2.py:179][0m |          -0.0297 |          15.9403 |          12.9521 |
[32m[20221213 12:28:41 @agent_ppo2.py:179][0m |          -0.0320 |          15.6766 |          12.9369 |
[32m[20221213 12:28:41 @agent_ppo2.py:179][0m |          -0.0420 |          15.4897 |          13.0135 |
[32m[20221213 12:28:41 @agent_ppo2.py:179][0m |          -0.0464 |          15.2759 |          12.9910 |
[32m[20221213 12:28:41 @agent_ppo2.py:179][0m |          -0.0506 |          15.3612 |          12.9672 |
[32m[20221213 12:28:41 @agent_ppo2.py:179][0m |          -0.0490 |          15.1114 |          13.0085 |
[32m[20221213 12:28:41 @agent_ppo2.py:179][0m |          -0.0446 |          14.9402 |          12.9457 |
[32m[20221213 12:28:41 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:28:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.48
[32m[20221213 12:28:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 255.54
[32m[20221213 12:28:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 255.87
[32m[20221213 12:28:42 @agent_ppo2.py:137][0m Total time:      12.75 min
[32m[20221213 12:28:42 @agent_ppo2.py:139][0m 843776 total steps have happened
[32m[20221213 12:28:42 @agent_ppo2.py:115][0m #------------------------ Iteration 412 --------------------------#
[32m[20221213 12:28:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:42 @agent_ppo2.py:179][0m |           0.0547 |          17.7576 |          12.1650 |
[32m[20221213 12:28:42 @agent_ppo2.py:179][0m |           0.0396 |          16.9714 |          11.2557 |
[32m[20221213 12:28:42 @agent_ppo2.py:179][0m |          -0.0018 |          16.7577 |          12.3349 |
[32m[20221213 12:28:42 @agent_ppo2.py:179][0m |          -0.0239 |          16.6290 |          12.6061 |
[32m[20221213 12:28:43 @agent_ppo2.py:179][0m |          -0.0285 |          16.4854 |          12.6499 |
[32m[20221213 12:28:43 @agent_ppo2.py:179][0m |          -0.0317 |          16.6845 |          12.6382 |
[32m[20221213 12:28:43 @agent_ppo2.py:179][0m |          -0.0386 |          16.3304 |          12.6839 |
[32m[20221213 12:28:43 @agent_ppo2.py:179][0m |          -0.0392 |          16.2642 |          12.6994 |
[32m[20221213 12:28:43 @agent_ppo2.py:179][0m |          -0.0410 |          16.2216 |          12.6920 |
[32m[20221213 12:28:43 @agent_ppo2.py:179][0m |          -0.0417 |          16.1155 |          12.6497 |
[32m[20221213 12:28:43 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:28:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.38
[32m[20221213 12:28:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 272.34
[32m[20221213 12:28:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 245.05
[32m[20221213 12:28:43 @agent_ppo2.py:137][0m Total time:      12.78 min
[32m[20221213 12:28:43 @agent_ppo2.py:139][0m 845824 total steps have happened
[32m[20221213 12:28:43 @agent_ppo2.py:115][0m #------------------------ Iteration 413 --------------------------#
[32m[20221213 12:28:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:28:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:44 @agent_ppo2.py:179][0m |           0.0610 |          19.2630 |          12.3566 |
[32m[20221213 12:28:44 @agent_ppo2.py:179][0m |           0.0356 |          16.7293 |          11.6719 |
[32m[20221213 12:28:44 @agent_ppo2.py:179][0m |          -0.0100 |          16.4380 |          12.3732 |
[32m[20221213 12:28:44 @agent_ppo2.py:179][0m |          -0.0264 |          16.3145 |          12.6495 |
[32m[20221213 12:28:44 @agent_ppo2.py:179][0m |          -0.0227 |          18.1375 |          12.6802 |
[32m[20221213 12:28:44 @agent_ppo2.py:179][0m |          -0.0378 |          16.2015 |          12.7675 |
[32m[20221213 12:28:45 @agent_ppo2.py:179][0m |          -0.0396 |          16.0507 |          12.7237 |
[32m[20221213 12:28:45 @agent_ppo2.py:179][0m |          -0.0452 |          15.9450 |          12.7394 |
[32m[20221213 12:28:45 @agent_ppo2.py:179][0m |          -0.0483 |          15.9222 |          12.7923 |
[32m[20221213 12:28:45 @agent_ppo2.py:179][0m |          -0.0464 |          15.8690 |          12.7305 |
[32m[20221213 12:28:45 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:28:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 235.93
[32m[20221213 12:28:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 242.69
[32m[20221213 12:28:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 232.09
[32m[20221213 12:28:45 @agent_ppo2.py:137][0m Total time:      12.81 min
[32m[20221213 12:28:45 @agent_ppo2.py:139][0m 847872 total steps have happened
[32m[20221213 12:28:45 @agent_ppo2.py:115][0m #------------------------ Iteration 414 --------------------------#
[32m[20221213 12:28:45 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:46 @agent_ppo2.py:179][0m |           0.1945 |          15.1205 |          12.2137 |
[32m[20221213 12:28:46 @agent_ppo2.py:179][0m |           0.0212 |          13.7127 |          11.9968 |
[32m[20221213 12:28:46 @agent_ppo2.py:179][0m |          -0.0158 |          13.3247 |          12.4013 |
[32m[20221213 12:28:46 @agent_ppo2.py:179][0m |          -0.0290 |          12.8464 |          12.4117 |
[32m[20221213 12:28:46 @agent_ppo2.py:179][0m |          -0.0378 |          12.6156 |          12.5161 |
[32m[20221213 12:28:46 @agent_ppo2.py:179][0m |          -0.0384 |          12.7330 |          12.5499 |
[32m[20221213 12:28:46 @agent_ppo2.py:179][0m |          -0.0453 |          12.2481 |          12.5241 |
[32m[20221213 12:28:47 @agent_ppo2.py:179][0m |          -0.0464 |          12.1545 |          12.5307 |
[32m[20221213 12:28:47 @agent_ppo2.py:179][0m |          -0.0492 |          12.0388 |          12.5199 |
[32m[20221213 12:28:47 @agent_ppo2.py:179][0m |          -0.0510 |          11.8817 |          12.5293 |
[32m[20221213 12:28:47 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:28:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 204.48
[32m[20221213 12:28:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 219.93
[32m[20221213 12:28:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 259.47
[32m[20221213 12:28:47 @agent_ppo2.py:137][0m Total time:      12.84 min
[32m[20221213 12:28:47 @agent_ppo2.py:139][0m 849920 total steps have happened
[32m[20221213 12:28:47 @agent_ppo2.py:115][0m #------------------------ Iteration 415 --------------------------#
[32m[20221213 12:28:47 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |           0.0397 |          17.9988 |          12.8262 |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |           0.0112 |          17.4530 |          12.1830 |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |          -0.0161 |          17.1941 |          12.2049 |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |          -0.0220 |          18.2521 |          12.3585 |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |          -0.0329 |          16.9343 |          12.3719 |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |          -0.0374 |          16.8479 |          12.4486 |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |          -0.0425 |          16.6528 |          12.4314 |
[32m[20221213 12:28:48 @agent_ppo2.py:179][0m |          -0.0390 |          16.5702 |          12.4307 |
[32m[20221213 12:28:49 @agent_ppo2.py:179][0m |          -0.0420 |          16.5422 |          12.4330 |
[32m[20221213 12:28:49 @agent_ppo2.py:179][0m |          -0.0443 |          16.4750 |          12.4302 |
[32m[20221213 12:28:49 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:28:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.61
[32m[20221213 12:28:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 269.10
[32m[20221213 12:28:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 265.80
[32m[20221213 12:28:49 @agent_ppo2.py:137][0m Total time:      12.87 min
[32m[20221213 12:28:49 @agent_ppo2.py:139][0m 851968 total steps have happened
[32m[20221213 12:28:49 @agent_ppo2.py:115][0m #------------------------ Iteration 416 --------------------------#
[32m[20221213 12:28:49 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:49 @agent_ppo2.py:179][0m |           0.0638 |          17.3032 |          12.1703 |
[32m[20221213 12:28:49 @agent_ppo2.py:179][0m |           0.0243 |          16.9195 |          12.0950 |
[32m[20221213 12:28:50 @agent_ppo2.py:179][0m |          -0.0015 |          16.7083 |          12.5540 |
[32m[20221213 12:28:50 @agent_ppo2.py:179][0m |          -0.0066 |          19.0768 |          12.6650 |
[32m[20221213 12:28:50 @agent_ppo2.py:179][0m |          -0.0236 |          16.3741 |          12.8098 |
[32m[20221213 12:28:50 @agent_ppo2.py:179][0m |          -0.0289 |          16.2711 |          12.8398 |
[32m[20221213 12:28:50 @agent_ppo2.py:179][0m |          -0.0337 |          16.1330 |          12.9047 |
[32m[20221213 12:28:50 @agent_ppo2.py:179][0m |          -0.0339 |          16.0691 |          12.8991 |
[32m[20221213 12:28:50 @agent_ppo2.py:179][0m |          -0.0390 |          16.0554 |          12.9036 |
[32m[20221213 12:28:51 @agent_ppo2.py:179][0m |          -0.0372 |          15.9646 |          12.8729 |
[32m[20221213 12:28:51 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:28:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.54
[32m[20221213 12:28:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.29
[32m[20221213 12:28:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 256.30
[32m[20221213 12:28:51 @agent_ppo2.py:137][0m Total time:      12.90 min
[32m[20221213 12:28:51 @agent_ppo2.py:139][0m 854016 total steps have happened
[32m[20221213 12:28:51 @agent_ppo2.py:115][0m #------------------------ Iteration 417 --------------------------#
[32m[20221213 12:28:51 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:51 @agent_ppo2.py:179][0m |           0.0545 |          16.2945 |          12.4544 |
[32m[20221213 12:28:51 @agent_ppo2.py:179][0m |           0.0314 |          15.0823 |          12.0973 |
[32m[20221213 12:28:51 @agent_ppo2.py:179][0m |          -0.0056 |          14.2704 |          12.4059 |
[32m[20221213 12:28:52 @agent_ppo2.py:179][0m |          -0.0170 |          14.1236 |          12.5750 |
[32m[20221213 12:28:52 @agent_ppo2.py:179][0m |          -0.0251 |          13.9257 |          12.6648 |
[32m[20221213 12:28:52 @agent_ppo2.py:179][0m |          -0.0239 |          14.3920 |          12.6554 |
[32m[20221213 12:28:52 @agent_ppo2.py:179][0m |          -0.0318 |          13.7734 |          12.6836 |
[32m[20221213 12:28:52 @agent_ppo2.py:179][0m |          -0.0289 |          13.7283 |          12.6403 |
[32m[20221213 12:28:52 @agent_ppo2.py:179][0m |          -0.0386 |          13.7020 |          12.6307 |
[32m[20221213 12:28:52 @agent_ppo2.py:179][0m |          -0.0342 |          13.8111 |          12.6657 |
[32m[20221213 12:28:52 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:28:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 240.91
[32m[20221213 12:28:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 250.55
[32m[20221213 12:28:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 259.46
[32m[20221213 12:28:53 @agent_ppo2.py:137][0m Total time:      12.93 min
[32m[20221213 12:28:53 @agent_ppo2.py:139][0m 856064 total steps have happened
[32m[20221213 12:28:53 @agent_ppo2.py:115][0m #------------------------ Iteration 418 --------------------------#
[32m[20221213 12:28:53 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:53 @agent_ppo2.py:179][0m |           0.0263 |          20.4674 |          12.7068 |
[32m[20221213 12:28:53 @agent_ppo2.py:179][0m |           0.0030 |          19.4145 |          12.2613 |
[32m[20221213 12:28:53 @agent_ppo2.py:179][0m |          -0.0124 |          19.0657 |          12.2782 |
[32m[20221213 12:28:53 @agent_ppo2.py:179][0m |          -0.0280 |          18.8576 |          12.5334 |
[32m[20221213 12:28:54 @agent_ppo2.py:179][0m |          -0.0429 |          18.7269 |          12.6761 |
[32m[20221213 12:28:54 @agent_ppo2.py:179][0m |          -0.0492 |          18.6771 |          12.6763 |
[32m[20221213 12:28:54 @agent_ppo2.py:179][0m |          -0.0390 |          18.6279 |          12.6618 |
[32m[20221213 12:28:54 @agent_ppo2.py:179][0m |          -0.0475 |          18.5224 |          12.6509 |
[32m[20221213 12:28:54 @agent_ppo2.py:179][0m |          -0.0444 |          18.8607 |          12.6727 |
[32m[20221213 12:28:54 @agent_ppo2.py:179][0m |          -0.0411 |          19.9214 |          12.6580 |
[32m[20221213 12:28:54 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:28:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.64
[32m[20221213 12:28:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 252.36
[32m[20221213 12:28:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 257.96
[32m[20221213 12:28:54 @agent_ppo2.py:137][0m Total time:      12.96 min
[32m[20221213 12:28:54 @agent_ppo2.py:139][0m 858112 total steps have happened
[32m[20221213 12:28:54 @agent_ppo2.py:115][0m #------------------------ Iteration 419 --------------------------#
[32m[20221213 12:28:55 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:55 @agent_ppo2.py:179][0m |           0.0614 |          18.0788 |          12.2621 |
[32m[20221213 12:28:55 @agent_ppo2.py:179][0m |           0.0196 |          17.0836 |          12.0015 |
[32m[20221213 12:28:55 @agent_ppo2.py:179][0m |           0.0015 |          19.2546 |          12.3480 |
[32m[20221213 12:28:55 @agent_ppo2.py:179][0m |          -0.0272 |          16.7163 |          12.5004 |
[32m[20221213 12:28:55 @agent_ppo2.py:179][0m |          -0.0290 |          16.7961 |          12.6501 |
[32m[20221213 12:28:56 @agent_ppo2.py:179][0m |          -0.0245 |          17.2237 |          12.6190 |
[32m[20221213 12:28:56 @agent_ppo2.py:179][0m |          -0.0230 |          16.4573 |          12.3977 |
[32m[20221213 12:28:56 @agent_ppo2.py:179][0m |          -0.0378 |          16.3142 |          12.4897 |
[32m[20221213 12:28:56 @agent_ppo2.py:179][0m |          -0.0440 |          16.2421 |          12.5680 |
[32m[20221213 12:28:56 @agent_ppo2.py:179][0m |          -0.0480 |          16.1985 |          12.6278 |
[32m[20221213 12:28:56 @agent_ppo2.py:124][0m Policy update time: 1.69 s
[32m[20221213 12:28:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.79
[32m[20221213 12:28:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.81
[32m[20221213 12:28:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.98
[32m[20221213 12:28:56 @agent_ppo2.py:137][0m Total time:      13.00 min
[32m[20221213 12:28:56 @agent_ppo2.py:139][0m 860160 total steps have happened
[32m[20221213 12:28:56 @agent_ppo2.py:115][0m #------------------------ Iteration 420 --------------------------#
[32m[20221213 12:28:57 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:28:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:57 @agent_ppo2.py:179][0m |           0.0294 |          18.3555 |          12.6137 |
[32m[20221213 12:28:57 @agent_ppo2.py:179][0m |          -0.0095 |          17.9359 |          12.6598 |
[32m[20221213 12:28:57 @agent_ppo2.py:179][0m |          -0.0232 |          17.7358 |          12.8435 |
[32m[20221213 12:28:57 @agent_ppo2.py:179][0m |          -0.0269 |          17.6467 |          12.7498 |
[32m[20221213 12:28:58 @agent_ppo2.py:179][0m |          -0.0333 |          17.5607 |          12.8229 |
[32m[20221213 12:28:58 @agent_ppo2.py:179][0m |          -0.0391 |          17.4974 |          12.8542 |
[32m[20221213 12:28:58 @agent_ppo2.py:179][0m |          -0.0400 |          17.4432 |          12.8600 |
[32m[20221213 12:28:58 @agent_ppo2.py:179][0m |          -0.0401 |          17.3771 |          12.8908 |
[32m[20221213 12:28:58 @agent_ppo2.py:179][0m |          -0.0419 |          17.5946 |          12.8850 |
[32m[20221213 12:28:58 @agent_ppo2.py:179][0m |          -0.0398 |          17.3375 |          12.8212 |
[32m[20221213 12:28:58 @agent_ppo2.py:124][0m Policy update time: 1.56 s
[32m[20221213 12:28:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.20
[32m[20221213 12:28:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.71
[32m[20221213 12:28:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.80
[32m[20221213 12:28:59 @agent_ppo2.py:137][0m Total time:      13.03 min
[32m[20221213 12:28:59 @agent_ppo2.py:139][0m 862208 total steps have happened
[32m[20221213 12:28:59 @agent_ppo2.py:115][0m #------------------------ Iteration 421 --------------------------#
[32m[20221213 12:28:59 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:28:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:28:59 @agent_ppo2.py:179][0m |           0.0563 |          19.5066 |          12.1182 |
[32m[20221213 12:28:59 @agent_ppo2.py:179][0m |           0.0500 |          18.7511 |          11.2641 |
[32m[20221213 12:28:59 @agent_ppo2.py:179][0m |           0.0009 |          18.4383 |          12.1013 |
[32m[20221213 12:28:59 @agent_ppo2.py:179][0m |          -0.0212 |          18.1674 |          12.4910 |
[32m[20221213 12:29:00 @agent_ppo2.py:179][0m |          -0.0265 |          18.1155 |          12.5815 |
[32m[20221213 12:29:00 @agent_ppo2.py:179][0m |          -0.0329 |          17.9282 |          12.5722 |
[32m[20221213 12:29:00 @agent_ppo2.py:179][0m |          -0.0307 |          19.8946 |          12.6494 |
[32m[20221213 12:29:00 @agent_ppo2.py:179][0m |          -0.0402 |          17.6740 |          12.6527 |
[32m[20221213 12:29:00 @agent_ppo2.py:179][0m |          -0.0415 |          17.6222 |          12.6481 |
[32m[20221213 12:29:00 @agent_ppo2.py:179][0m |          -0.0403 |          17.6122 |          12.6564 |
[32m[20221213 12:29:00 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:29:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.96
[32m[20221213 12:29:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 249.31
[32m[20221213 12:29:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.75
[32m[20221213 12:29:01 @agent_ppo2.py:137][0m Total time:      13.06 min
[32m[20221213 12:29:01 @agent_ppo2.py:139][0m 864256 total steps have happened
[32m[20221213 12:29:01 @agent_ppo2.py:115][0m #------------------------ Iteration 422 --------------------------#
[32m[20221213 12:29:01 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:01 @agent_ppo2.py:179][0m |           0.0622 |          18.4399 |          12.4237 |
[32m[20221213 12:29:01 @agent_ppo2.py:179][0m |           0.0430 |          17.7626 |          12.2458 |
[32m[20221213 12:29:01 @agent_ppo2.py:179][0m |          -0.0025 |          17.2009 |          12.6389 |
[32m[20221213 12:29:01 @agent_ppo2.py:179][0m |          -0.0221 |          17.3754 |          12.7187 |
[32m[20221213 12:29:02 @agent_ppo2.py:179][0m |          -0.0326 |          16.7687 |          12.8629 |
[32m[20221213 12:29:02 @agent_ppo2.py:179][0m |          -0.0416 |          16.6000 |          12.8679 |
[32m[20221213 12:29:02 @agent_ppo2.py:179][0m |          -0.0211 |          20.0529 |          12.8853 |
[32m[20221213 12:29:02 @agent_ppo2.py:179][0m |          -0.0246 |          18.6747 |          12.7999 |
[32m[20221213 12:29:02 @agent_ppo2.py:179][0m |          -0.0292 |          16.2918 |          12.6837 |
[32m[20221213 12:29:02 @agent_ppo2.py:179][0m |          -0.0434 |          16.2641 |          12.7794 |
[32m[20221213 12:29:02 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:29:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.92
[32m[20221213 12:29:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.71
[32m[20221213 12:29:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 242.84
[32m[20221213 12:29:03 @agent_ppo2.py:137][0m Total time:      13.10 min
[32m[20221213 12:29:03 @agent_ppo2.py:139][0m 866304 total steps have happened
[32m[20221213 12:29:03 @agent_ppo2.py:115][0m #------------------------ Iteration 423 --------------------------#
[32m[20221213 12:29:03 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:03 @agent_ppo2.py:179][0m |           0.0452 |          17.4769 |          12.4837 |
[32m[20221213 12:29:03 @agent_ppo2.py:179][0m |           0.0114 |          16.1322 |          12.4332 |
[32m[20221213 12:29:03 @agent_ppo2.py:179][0m |          -0.0229 |          15.6499 |          12.8404 |
[32m[20221213 12:29:03 @agent_ppo2.py:179][0m |          -0.0316 |          15.1961 |          12.9173 |
[32m[20221213 12:29:04 @agent_ppo2.py:179][0m |          -0.0372 |          14.9777 |          12.9450 |
[32m[20221213 12:29:04 @agent_ppo2.py:179][0m |          -0.0367 |          14.6563 |          12.9370 |
[32m[20221213 12:29:04 @agent_ppo2.py:179][0m |          -0.0368 |          14.5943 |          12.8737 |
[32m[20221213 12:29:04 @agent_ppo2.py:179][0m |          -0.0480 |          14.4773 |          12.9487 |
[32m[20221213 12:29:04 @agent_ppo2.py:179][0m |          -0.0485 |          14.1390 |          12.9009 |
[32m[20221213 12:29:04 @agent_ppo2.py:179][0m |          -0.0430 |          14.3268 |          12.9146 |
[32m[20221213 12:29:04 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:29:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.72
[32m[20221213 12:29:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 263.38
[32m[20221213 12:29:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 252.71
[32m[20221213 12:29:05 @agent_ppo2.py:137][0m Total time:      13.13 min
[32m[20221213 12:29:05 @agent_ppo2.py:139][0m 868352 total steps have happened
[32m[20221213 12:29:05 @agent_ppo2.py:115][0m #------------------------ Iteration 424 --------------------------#
[32m[20221213 12:29:05 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:05 @agent_ppo2.py:179][0m |           0.0457 |          19.4838 |          12.4525 |
[32m[20221213 12:29:05 @agent_ppo2.py:179][0m |           0.0094 |          17.7736 |          11.6438 |
[32m[20221213 12:29:05 @agent_ppo2.py:179][0m |          -0.0140 |          17.6464 |          11.4741 |
[32m[20221213 12:29:05 @agent_ppo2.py:179][0m |          -0.0416 |          16.9049 |          11.5794 |
[32m[20221213 12:29:06 @agent_ppo2.py:179][0m |          -0.0444 |          16.6142 |          11.5672 |
[32m[20221213 12:29:06 @agent_ppo2.py:179][0m |          -0.0370 |          18.2238 |          11.5281 |
[32m[20221213 12:29:06 @agent_ppo2.py:179][0m |          -0.0587 |          16.4081 |          11.4692 |
[32m[20221213 12:29:06 @agent_ppo2.py:179][0m |          -0.0636 |          16.3581 |          11.5310 |
[32m[20221213 12:29:06 @agent_ppo2.py:179][0m |          -0.0618 |          16.3195 |          11.4297 |
[32m[20221213 12:29:07 @agent_ppo2.py:179][0m |          -0.0525 |          17.4457 |          11.4043 |
[32m[20221213 12:29:07 @agent_ppo2.py:124][0m Policy update time: 1.80 s
[32m[20221213 12:29:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.35
[32m[20221213 12:29:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 226.04
[32m[20221213 12:29:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 246.19
[32m[20221213 12:29:07 @agent_ppo2.py:137][0m Total time:      13.17 min
[32m[20221213 12:29:07 @agent_ppo2.py:139][0m 870400 total steps have happened
[32m[20221213 12:29:07 @agent_ppo2.py:115][0m #------------------------ Iteration 425 --------------------------#
[32m[20221213 12:29:07 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:29:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:07 @agent_ppo2.py:179][0m |           0.0424 |          19.8128 |          12.6896 |
[32m[20221213 12:29:07 @agent_ppo2.py:179][0m |           0.0308 |          18.6446 |          12.0691 |
[32m[20221213 12:29:08 @agent_ppo2.py:179][0m |          -0.0049 |          18.1714 |          12.6524 |
[32m[20221213 12:29:08 @agent_ppo2.py:179][0m |          -0.0185 |          17.9897 |          12.8098 |
[32m[20221213 12:29:08 @agent_ppo2.py:179][0m |          -0.0258 |          17.7854 |          12.9096 |
[32m[20221213 12:29:08 @agent_ppo2.py:179][0m |          -0.0317 |          17.7055 |          12.8853 |
[32m[20221213 12:29:08 @agent_ppo2.py:179][0m |          -0.0372 |          17.6143 |          12.9888 |
[32m[20221213 12:29:08 @agent_ppo2.py:179][0m |          -0.0351 |          17.6856 |          12.9429 |
[32m[20221213 12:29:09 @agent_ppo2.py:179][0m |          -0.0409 |          17.4666 |          12.9497 |
[32m[20221213 12:29:09 @agent_ppo2.py:179][0m |          -0.0462 |          17.4404 |          12.9116 |
[32m[20221213 12:29:09 @agent_ppo2.py:124][0m Policy update time: 1.65 s
[32m[20221213 12:29:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 240.65
[32m[20221213 12:29:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.86
[32m[20221213 12:29:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.63
[32m[20221213 12:29:09 @agent_ppo2.py:137][0m Total time:      13.20 min
[32m[20221213 12:29:09 @agent_ppo2.py:139][0m 872448 total steps have happened
[32m[20221213 12:29:09 @agent_ppo2.py:115][0m #------------------------ Iteration 426 --------------------------#
[32m[20221213 12:29:09 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:09 @agent_ppo2.py:179][0m |           0.0335 |          18.8612 |          13.1587 |
[32m[20221213 12:29:10 @agent_ppo2.py:179][0m |           0.0239 |          18.3669 |          12.8620 |
[32m[20221213 12:29:10 @agent_ppo2.py:179][0m |           0.0166 |          19.7722 |          12.7076 |
[32m[20221213 12:29:10 @agent_ppo2.py:179][0m |          -0.0240 |          17.9785 |          13.1589 |
[32m[20221213 12:29:10 @agent_ppo2.py:179][0m |          -0.0345 |          17.9510 |          13.2022 |
[32m[20221213 12:29:10 @agent_ppo2.py:179][0m |          -0.0374 |          17.8078 |          13.2712 |
[32m[20221213 12:29:10 @agent_ppo2.py:179][0m |          -0.0299 |          19.4298 |          13.2767 |
[32m[20221213 12:29:10 @agent_ppo2.py:179][0m |          -0.0371 |          17.7087 |          13.2766 |
[32m[20221213 12:29:11 @agent_ppo2.py:179][0m |          -0.0427 |          17.6155 |          13.2253 |
[32m[20221213 12:29:11 @agent_ppo2.py:179][0m |          -0.0425 |          17.5498 |          13.1982 |
[32m[20221213 12:29:11 @agent_ppo2.py:124][0m Policy update time: 1.64 s
[32m[20221213 12:29:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.49
[32m[20221213 12:29:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 255.56
[32m[20221213 12:29:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 242.63
[32m[20221213 12:29:11 @agent_ppo2.py:137][0m Total time:      13.24 min
[32m[20221213 12:29:11 @agent_ppo2.py:139][0m 874496 total steps have happened
[32m[20221213 12:29:11 @agent_ppo2.py:115][0m #------------------------ Iteration 427 --------------------------#
[32m[20221213 12:29:11 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:11 @agent_ppo2.py:179][0m |           0.0546 |          19.3288 |          12.5981 |
[32m[20221213 12:29:12 @agent_ppo2.py:179][0m |           0.0318 |          18.7952 |          12.4253 |
[32m[20221213 12:29:12 @agent_ppo2.py:179][0m |          -0.0007 |          18.4271 |          12.6352 |
[32m[20221213 12:29:12 @agent_ppo2.py:179][0m |          -0.0290 |          17.9714 |          12.8275 |
[32m[20221213 12:29:12 @agent_ppo2.py:179][0m |          -0.0381 |          17.7583 |          12.8933 |
[32m[20221213 12:29:12 @agent_ppo2.py:179][0m |          -0.0420 |          17.9165 |          12.9529 |
[32m[20221213 12:29:12 @agent_ppo2.py:179][0m |          -0.0523 |          17.6898 |          12.9766 |
[32m[20221213 12:29:12 @agent_ppo2.py:179][0m |          -0.0478 |          17.4975 |          12.9666 |
[32m[20221213 12:29:13 @agent_ppo2.py:179][0m |          -0.0525 |          17.4230 |          12.9098 |
[32m[20221213 12:29:13 @agent_ppo2.py:179][0m |          -0.0523 |          17.3268 |          12.8726 |
[32m[20221213 12:29:13 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:29:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 211.86
[32m[20221213 12:29:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 237.10
[32m[20221213 12:29:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 264.07
[32m[20221213 12:29:13 @agent_ppo2.py:137][0m Total time:      13.27 min
[32m[20221213 12:29:13 @agent_ppo2.py:139][0m 876544 total steps have happened
[32m[20221213 12:29:13 @agent_ppo2.py:115][0m #------------------------ Iteration 428 --------------------------#
[32m[20221213 12:29:13 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:29:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:13 @agent_ppo2.py:179][0m |           0.0332 |          17.9105 |          12.7143 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0035 |          17.1064 |          12.7632 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0246 |          16.8927 |          12.8954 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0287 |          16.6163 |          12.8548 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0383 |          16.4880 |          12.8677 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0420 |          16.3051 |          12.8342 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0461 |          16.1736 |          12.8317 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0494 |          16.1113 |          12.9177 |
[32m[20221213 12:29:14 @agent_ppo2.py:179][0m |          -0.0519 |          16.0459 |          12.8398 |
[32m[20221213 12:29:15 @agent_ppo2.py:179][0m |          -0.0542 |          15.9544 |          12.8122 |
[32m[20221213 12:29:15 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:29:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 198.93
[32m[20221213 12:29:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 253.38
[32m[20221213 12:29:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 252.40
[32m[20221213 12:29:15 @agent_ppo2.py:137][0m Total time:      13.30 min
[32m[20221213 12:29:15 @agent_ppo2.py:139][0m 878592 total steps have happened
[32m[20221213 12:29:15 @agent_ppo2.py:115][0m #------------------------ Iteration 429 --------------------------#
[32m[20221213 12:29:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:15 @agent_ppo2.py:179][0m |           0.0532 |          18.3006 |          12.4211 |
[32m[20221213 12:29:15 @agent_ppo2.py:179][0m |           0.0344 |          17.7194 |          12.1917 |
[32m[20221213 12:29:16 @agent_ppo2.py:179][0m |          -0.0069 |          17.4913 |          12.4888 |
[32m[20221213 12:29:16 @agent_ppo2.py:179][0m |          -0.0207 |          17.3807 |          12.6196 |
[32m[20221213 12:29:16 @agent_ppo2.py:179][0m |          -0.0337 |          17.2127 |          12.6439 |
[32m[20221213 12:29:16 @agent_ppo2.py:179][0m |          -0.0375 |          17.1706 |          12.6342 |
[32m[20221213 12:29:16 @agent_ppo2.py:179][0m |          -0.0404 |          17.1214 |          12.6387 |
[32m[20221213 12:29:16 @agent_ppo2.py:179][0m |          -0.0464 |          17.0167 |          12.6741 |
[32m[20221213 12:29:16 @agent_ppo2.py:179][0m |          -0.0412 |          17.5080 |          12.6604 |
[32m[20221213 12:29:17 @agent_ppo2.py:179][0m |          -0.0477 |          16.9113 |          12.6175 |
[32m[20221213 12:29:17 @agent_ppo2.py:124][0m Policy update time: 1.56 s
[32m[20221213 12:29:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 216.98
[32m[20221213 12:29:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 229.00
[32m[20221213 12:29:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 250.40
[32m[20221213 12:29:17 @agent_ppo2.py:137][0m Total time:      13.33 min
[32m[20221213 12:29:17 @agent_ppo2.py:139][0m 880640 total steps have happened
[32m[20221213 12:29:17 @agent_ppo2.py:115][0m #------------------------ Iteration 430 --------------------------#
[32m[20221213 12:29:17 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:29:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:17 @agent_ppo2.py:179][0m |           0.0493 |          13.0673 |          12.4463 |
[32m[20221213 12:29:17 @agent_ppo2.py:179][0m |           0.0264 |          12.5000 |          12.3773 |
[32m[20221213 12:29:18 @agent_ppo2.py:179][0m |           0.0193 |          13.3243 |          11.8771 |
[32m[20221213 12:29:18 @agent_ppo2.py:179][0m |          -0.0119 |          11.7513 |          11.4654 |
[32m[20221213 12:29:18 @agent_ppo2.py:179][0m |          -0.0305 |          11.5796 |          11.3455 |
[32m[20221213 12:29:18 @agent_ppo2.py:179][0m |          -0.0404 |          11.3502 |          11.3053 |
[32m[20221213 12:29:18 @agent_ppo2.py:179][0m |          -0.0483 |          11.2154 |          11.2348 |
[32m[20221213 12:29:18 @agent_ppo2.py:179][0m |          -0.0503 |          11.1279 |          11.2912 |
[32m[20221213 12:29:18 @agent_ppo2.py:179][0m |          -0.0531 |          10.9345 |          11.2095 |
[32m[20221213 12:29:19 @agent_ppo2.py:179][0m |          -0.0547 |          10.8306 |          11.1967 |
[32m[20221213 12:29:19 @agent_ppo2.py:124][0m Policy update time: 1.57 s
[32m[20221213 12:29:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 209.93
[32m[20221213 12:29:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.13
[32m[20221213 12:29:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 256.80
[32m[20221213 12:29:19 @agent_ppo2.py:137][0m Total time:      13.37 min
[32m[20221213 12:29:19 @agent_ppo2.py:139][0m 882688 total steps have happened
[32m[20221213 12:29:19 @agent_ppo2.py:115][0m #------------------------ Iteration 431 --------------------------#
[32m[20221213 12:29:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:19 @agent_ppo2.py:179][0m |           0.0397 |          18.1428 |          12.3902 |
[32m[20221213 12:29:19 @agent_ppo2.py:179][0m |           0.0039 |          17.5240 |          12.4533 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0074 |          17.2769 |          12.6341 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0259 |          17.0832 |          12.7860 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0280 |          17.5560 |          12.8241 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0327 |          16.8920 |          12.7664 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0440 |          16.7862 |          12.7700 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0427 |          16.6761 |          12.7556 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0388 |          16.6108 |          12.7834 |
[32m[20221213 12:29:20 @agent_ppo2.py:179][0m |          -0.0425 |          16.5448 |          12.7207 |
[32m[20221213 12:29:20 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:29:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.12
[32m[20221213 12:29:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 253.54
[32m[20221213 12:29:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 255.06
[32m[20221213 12:29:21 @agent_ppo2.py:137][0m Total time:      13.40 min
[32m[20221213 12:29:21 @agent_ppo2.py:139][0m 884736 total steps have happened
[32m[20221213 12:29:21 @agent_ppo2.py:115][0m #------------------------ Iteration 432 --------------------------#
[32m[20221213 12:29:21 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:21 @agent_ppo2.py:179][0m |           0.0408 |          15.2734 |          12.2062 |
[32m[20221213 12:29:21 @agent_ppo2.py:179][0m |           0.0306 |          14.4905 |          10.5958 |
[32m[20221213 12:29:21 @agent_ppo2.py:179][0m |          -0.0207 |          14.0646 |          10.2266 |
[32m[20221213 12:29:22 @agent_ppo2.py:179][0m |          -0.0314 |          14.1943 |          10.2788 |
[32m[20221213 12:29:22 @agent_ppo2.py:179][0m |          -0.0430 |          13.5874 |          10.2628 |
[32m[20221213 12:29:22 @agent_ppo2.py:179][0m |          -0.0501 |          13.3410 |          10.2537 |
[32m[20221213 12:29:22 @agent_ppo2.py:179][0m |          -0.0472 |          13.3136 |          10.2689 |
[32m[20221213 12:29:22 @agent_ppo2.py:179][0m |          -0.0591 |          13.0034 |          10.2839 |
[32m[20221213 12:29:22 @agent_ppo2.py:179][0m |          -0.0497 |          13.4910 |          10.2310 |
[32m[20221213 12:29:22 @agent_ppo2.py:179][0m |          -0.0564 |          12.8590 |          10.2165 |
[32m[20221213 12:29:22 @agent_ppo2.py:124][0m Policy update time: 1.57 s
[32m[20221213 12:29:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.64
[32m[20221213 12:29:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 246.55
[32m[20221213 12:29:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.50
[32m[20221213 12:29:23 @agent_ppo2.py:137][0m Total time:      13.43 min
[32m[20221213 12:29:23 @agent_ppo2.py:139][0m 886784 total steps have happened
[32m[20221213 12:29:23 @agent_ppo2.py:115][0m #------------------------ Iteration 433 --------------------------#
[32m[20221213 12:29:23 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:23 @agent_ppo2.py:179][0m |           0.0405 |          18.1065 |          12.5810 |
[32m[20221213 12:29:23 @agent_ppo2.py:179][0m |           0.0151 |          15.8712 |          12.3558 |
[32m[20221213 12:29:23 @agent_ppo2.py:179][0m |          -0.0162 |          15.5104 |          12.6759 |
[32m[20221213 12:29:24 @agent_ppo2.py:179][0m |          -0.0261 |          15.3113 |          12.7649 |
[32m[20221213 12:29:24 @agent_ppo2.py:179][0m |          -0.0224 |          15.2574 |          12.6584 |
[32m[20221213 12:29:24 @agent_ppo2.py:179][0m |          -0.0357 |          15.2969 |          12.7258 |
[32m[20221213 12:29:24 @agent_ppo2.py:179][0m |          -0.0398 |          14.9863 |          12.7519 |
[32m[20221213 12:29:24 @agent_ppo2.py:179][0m |          -0.0376 |          15.5033 |          12.7036 |
[32m[20221213 12:29:24 @agent_ppo2.py:179][0m |          -0.0441 |          14.8818 |          12.7039 |
[32m[20221213 12:29:24 @agent_ppo2.py:179][0m |          -0.0437 |          14.7628 |          12.6080 |
[32m[20221213 12:29:24 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:29:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 235.02
[32m[20221213 12:29:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 244.92
[32m[20221213 12:29:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.18
[32m[20221213 12:29:25 @agent_ppo2.py:137][0m Total time:      13.46 min
[32m[20221213 12:29:25 @agent_ppo2.py:139][0m 888832 total steps have happened
[32m[20221213 12:29:25 @agent_ppo2.py:115][0m #------------------------ Iteration 434 --------------------------#
[32m[20221213 12:29:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:25 @agent_ppo2.py:179][0m |           0.0502 |          18.4280 |          12.3395 |
[32m[20221213 12:29:25 @agent_ppo2.py:179][0m |           0.0294 |          17.8969 |          12.0904 |
[32m[20221213 12:29:25 @agent_ppo2.py:179][0m |           0.0022 |          17.7245 |          12.5218 |
[32m[20221213 12:29:25 @agent_ppo2.py:179][0m |          -0.0189 |          17.5397 |          12.5685 |
[32m[20221213 12:29:25 @agent_ppo2.py:179][0m |          -0.0340 |          17.4631 |          12.6419 |
[32m[20221213 12:29:26 @agent_ppo2.py:179][0m |          -0.0366 |          17.3755 |          12.6593 |
[32m[20221213 12:29:26 @agent_ppo2.py:179][0m |          -0.0413 |          17.2578 |          12.6296 |
[32m[20221213 12:29:26 @agent_ppo2.py:179][0m |          -0.0417 |          17.1998 |          12.5684 |
[32m[20221213 12:29:26 @agent_ppo2.py:179][0m |          -0.0344 |          19.0913 |          12.5394 |
[32m[20221213 12:29:26 @agent_ppo2.py:179][0m |          -0.0472 |          17.1506 |          12.5445 |
[32m[20221213 12:29:26 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:29:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 225.96
[32m[20221213 12:29:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.62
[32m[20221213 12:29:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 264.25
[32m[20221213 12:29:26 @agent_ppo2.py:137][0m Total time:      13.49 min
[32m[20221213 12:29:26 @agent_ppo2.py:139][0m 890880 total steps have happened
[32m[20221213 12:29:26 @agent_ppo2.py:115][0m #------------------------ Iteration 435 --------------------------#
[32m[20221213 12:29:27 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:27 @agent_ppo2.py:179][0m |           0.0159 |           8.9899 |           5.1546 |
[32m[20221213 12:29:27 @agent_ppo2.py:179][0m |          -0.0053 |           8.2115 |           3.0568 |
[32m[20221213 12:29:27 @agent_ppo2.py:179][0m |          -0.0266 |           8.0965 |           3.2135 |
[32m[20221213 12:29:27 @agent_ppo2.py:179][0m |          -0.0393 |           8.0306 |           3.2328 |
[32m[20221213 12:29:27 @agent_ppo2.py:179][0m |          -0.0345 |           7.9564 |           3.2372 |
[32m[20221213 12:29:27 @agent_ppo2.py:179][0m |          -0.0426 |           7.9691 |           3.2842 |
[32m[20221213 12:29:27 @agent_ppo2.py:179][0m |          -0.0489 |           7.8744 |           3.2770 |
[32m[20221213 12:29:28 @agent_ppo2.py:179][0m |          -0.0515 |           7.8420 |           3.2801 |
[32m[20221213 12:29:28 @agent_ppo2.py:179][0m |          -0.0516 |           7.8104 |           3.2487 |
[32m[20221213 12:29:28 @agent_ppo2.py:179][0m |          -0.0476 |           7.8393 |           3.2313 |
[32m[20221213 12:29:28 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:29:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 33.63
[32m[20221213 12:29:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.84
[32m[20221213 12:29:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 4.12
[32m[20221213 12:29:28 @agent_ppo2.py:137][0m Total time:      13.52 min
[32m[20221213 12:29:28 @agent_ppo2.py:139][0m 892928 total steps have happened
[32m[20221213 12:29:28 @agent_ppo2.py:115][0m #------------------------ Iteration 436 --------------------------#
[32m[20221213 12:29:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:29 @agent_ppo2.py:179][0m |           0.0906 |          16.4964 |          12.0257 |
[32m[20221213 12:29:29 @agent_ppo2.py:179][0m |           0.0710 |          15.9051 |          11.5298 |
[32m[20221213 12:29:29 @agent_ppo2.py:179][0m |           0.0264 |          15.6797 |          12.1570 |
[32m[20221213 12:29:29 @agent_ppo2.py:179][0m |          -0.0012 |          16.7856 |          12.6531 |
[32m[20221213 12:29:29 @agent_ppo2.py:179][0m |          -0.0081 |          15.5270 |          12.3840 |
[32m[20221213 12:29:29 @agent_ppo2.py:179][0m |          -0.0229 |          15.8315 |          12.7621 |
[32m[20221213 12:29:29 @agent_ppo2.py:179][0m |          -0.0356 |          15.3552 |          12.6941 |
[32m[20221213 12:29:30 @agent_ppo2.py:179][0m |          -0.0397 |          15.3333 |          12.6576 |
[32m[20221213 12:29:30 @agent_ppo2.py:179][0m |          -0.0361 |          15.2777 |          12.6109 |
[32m[20221213 12:29:30 @agent_ppo2.py:179][0m |          -0.0368 |          15.4503 |          12.5101 |
[32m[20221213 12:29:30 @agent_ppo2.py:124][0m Policy update time: 1.63 s
[32m[20221213 12:29:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 248.54
[32m[20221213 12:29:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 256.86
[32m[20221213 12:29:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 278.01
[32m[20221213 12:29:30 @agent_ppo2.py:137][0m Total time:      13.56 min
[32m[20221213 12:29:30 @agent_ppo2.py:139][0m 894976 total steps have happened
[32m[20221213 12:29:30 @agent_ppo2.py:115][0m #------------------------ Iteration 437 --------------------------#
[32m[20221213 12:29:30 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |           0.0882 |          16.1766 |          11.8856 |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |           0.0280 |          15.8105 |          11.9546 |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |          -0.0108 |          15.6988 |          12.4273 |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |          -0.0227 |          15.5766 |          12.5971 |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |          -0.0239 |          15.5141 |          12.5441 |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |          -0.0355 |          15.4696 |          12.5800 |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |          -0.0398 |          15.4260 |          12.6345 |
[32m[20221213 12:29:31 @agent_ppo2.py:179][0m |          -0.0385 |          15.3866 |          12.6421 |
[32m[20221213 12:29:32 @agent_ppo2.py:179][0m |          -0.0346 |          15.5971 |          12.6270 |
[32m[20221213 12:29:32 @agent_ppo2.py:179][0m |          -0.0422 |          15.3170 |          12.6093 |
[32m[20221213 12:29:32 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:29:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.58
[32m[20221213 12:29:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 249.37
[32m[20221213 12:29:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 255.72
[32m[20221213 12:29:32 @agent_ppo2.py:137][0m Total time:      13.59 min
[32m[20221213 12:29:32 @agent_ppo2.py:139][0m 897024 total steps have happened
[32m[20221213 12:29:32 @agent_ppo2.py:115][0m #------------------------ Iteration 438 --------------------------#
[32m[20221213 12:29:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:32 @agent_ppo2.py:179][0m |           0.0879 |          11.4360 |          12.0239 |
[32m[20221213 12:29:32 @agent_ppo2.py:179][0m |           0.0152 |          10.5249 |           7.2254 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0205 |          10.1922 |           7.0731 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0428 |          10.0270 |           7.1076 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0520 |           9.8109 |           7.0918 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0577 |           9.6379 |           7.0785 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0586 |           9.4992 |           7.0422 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0649 |           9.3940 |           6.9611 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0640 |           9.5560 |           7.0104 |
[32m[20221213 12:29:33 @agent_ppo2.py:179][0m |          -0.0690 |           9.1332 |           6.9654 |
[32m[20221213 12:29:33 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:29:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.29
[32m[20221213 12:29:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 220.86
[32m[20221213 12:29:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.47
[32m[20221213 12:29:34 @agent_ppo2.py:137][0m Total time:      13.62 min
[32m[20221213 12:29:34 @agent_ppo2.py:139][0m 899072 total steps have happened
[32m[20221213 12:29:34 @agent_ppo2.py:115][0m #------------------------ Iteration 439 --------------------------#
[32m[20221213 12:29:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:34 @agent_ppo2.py:179][0m |           0.0306 |          16.5941 |          12.0878 |
[32m[20221213 12:29:34 @agent_ppo2.py:179][0m |           0.0116 |          16.3682 |          12.0557 |
[32m[20221213 12:29:34 @agent_ppo2.py:179][0m |          -0.0168 |          16.0816 |          12.2750 |
[32m[20221213 12:29:34 @agent_ppo2.py:179][0m |          -0.0262 |          15.9824 |          12.3915 |
[32m[20221213 12:29:35 @agent_ppo2.py:179][0m |          -0.0315 |          15.9423 |          12.3981 |
[32m[20221213 12:29:35 @agent_ppo2.py:179][0m |          -0.0345 |          15.7984 |          12.4652 |
[32m[20221213 12:29:35 @agent_ppo2.py:179][0m |          -0.0356 |          15.8118 |          12.4264 |
[32m[20221213 12:29:35 @agent_ppo2.py:179][0m |          -0.0400 |          15.7204 |          12.4693 |
[32m[20221213 12:29:35 @agent_ppo2.py:179][0m |          -0.0432 |          15.6536 |          12.3535 |
[32m[20221213 12:29:35 @agent_ppo2.py:179][0m |          -0.0410 |          15.6452 |          12.3406 |
[32m[20221213 12:29:35 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:29:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 231.35
[32m[20221213 12:29:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 242.28
[32m[20221213 12:29:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 250.81
[32m[20221213 12:29:35 @agent_ppo2.py:137][0m Total time:      13.64 min
[32m[20221213 12:29:35 @agent_ppo2.py:139][0m 901120 total steps have happened
[32m[20221213 12:29:35 @agent_ppo2.py:115][0m #------------------------ Iteration 440 --------------------------#
[32m[20221213 12:29:36 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:29:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:36 @agent_ppo2.py:179][0m |           0.0584 |          16.1945 |          11.5579 |
[32m[20221213 12:29:36 @agent_ppo2.py:179][0m |           0.0317 |          15.9456 |          11.2381 |
[32m[20221213 12:29:36 @agent_ppo2.py:179][0m |           0.0155 |          15.8775 |          11.6726 |
[32m[20221213 12:29:36 @agent_ppo2.py:179][0m |          -0.0054 |          15.8403 |          11.9267 |
[32m[20221213 12:29:36 @agent_ppo2.py:179][0m |          -0.0121 |          15.8162 |          12.0358 |
[32m[20221213 12:29:37 @agent_ppo2.py:179][0m |          -0.0279 |          15.7920 |          12.1915 |
[32m[20221213 12:29:37 @agent_ppo2.py:179][0m |          -0.0288 |          15.7492 |          12.2705 |
[32m[20221213 12:29:37 @agent_ppo2.py:179][0m |          -0.0316 |          15.7926 |          12.1894 |
[32m[20221213 12:29:37 @agent_ppo2.py:179][0m |          -0.0206 |          18.0848 |          12.1699 |
[32m[20221213 12:29:37 @agent_ppo2.py:179][0m |          -0.0335 |          15.7541 |          12.1492 |
[32m[20221213 12:29:37 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:29:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.64
[32m[20221213 12:29:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 254.57
[32m[20221213 12:29:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 129.28
[32m[20221213 12:29:37 @agent_ppo2.py:137][0m Total time:      13.68 min
[32m[20221213 12:29:37 @agent_ppo2.py:139][0m 903168 total steps have happened
[32m[20221213 12:29:37 @agent_ppo2.py:115][0m #------------------------ Iteration 441 --------------------------#
[32m[20221213 12:29:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:38 @agent_ppo2.py:179][0m |           0.0644 |          18.5899 |          11.9308 |
[32m[20221213 12:29:38 @agent_ppo2.py:179][0m |           0.0217 |          16.1720 |          11.5606 |
[32m[20221213 12:29:38 @agent_ppo2.py:179][0m |          -0.0020 |          16.6720 |          11.9890 |
[32m[20221213 12:29:38 @agent_ppo2.py:179][0m |          -0.0114 |          17.3995 |          12.2707 |
[32m[20221213 12:29:38 @agent_ppo2.py:179][0m |          -0.0280 |          15.9527 |          12.3117 |
[32m[20221213 12:29:38 @agent_ppo2.py:179][0m |          -0.0267 |          15.8840 |          12.3044 |
[32m[20221213 12:29:38 @agent_ppo2.py:179][0m |          -0.0350 |          15.8811 |          12.3466 |
[32m[20221213 12:29:39 @agent_ppo2.py:179][0m |          -0.0329 |          15.8310 |          12.2547 |
[32m[20221213 12:29:39 @agent_ppo2.py:179][0m |          -0.0354 |          15.8159 |          12.2172 |
[32m[20221213 12:29:39 @agent_ppo2.py:179][0m |          -0.0340 |          15.9537 |          12.1308 |
[32m[20221213 12:29:39 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:29:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.88
[32m[20221213 12:29:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.40
[32m[20221213 12:29:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 267.45
[32m[20221213 12:29:39 @agent_ppo2.py:137][0m Total time:      13.70 min
[32m[20221213 12:29:39 @agent_ppo2.py:139][0m 905216 total steps have happened
[32m[20221213 12:29:39 @agent_ppo2.py:115][0m #------------------------ Iteration 442 --------------------------#
[32m[20221213 12:29:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:39 @agent_ppo2.py:179][0m |           0.0437 |          14.5389 |          11.9154 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |           0.0298 |          14.2856 |          11.8457 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |          -0.0052 |          14.2222 |          12.0955 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |          -0.0057 |          14.8058 |          11.9953 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |           0.0513 |          14.1395 |          11.2742 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |          -0.0169 |          14.0937 |           9.4814 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |          -0.0364 |          14.0823 |           9.1679 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |          -0.0512 |          14.0507 |           9.2142 |
[32m[20221213 12:29:40 @agent_ppo2.py:179][0m |          -0.0502 |          14.0251 |           9.1080 |
[32m[20221213 12:29:41 @agent_ppo2.py:179][0m |          -0.0564 |          14.0600 |           9.0977 |
[32m[20221213 12:29:41 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:29:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.95
[32m[20221213 12:29:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 246.40
[32m[20221213 12:29:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 259.91
[32m[20221213 12:29:41 @agent_ppo2.py:137][0m Total time:      13.73 min
[32m[20221213 12:29:41 @agent_ppo2.py:139][0m 907264 total steps have happened
[32m[20221213 12:29:41 @agent_ppo2.py:115][0m #------------------------ Iteration 443 --------------------------#
[32m[20221213 12:29:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:41 @agent_ppo2.py:179][0m |           0.0388 |          16.6530 |          12.0101 |
[32m[20221213 12:29:41 @agent_ppo2.py:179][0m |           0.0172 |          16.1115 |          11.9713 |
[32m[20221213 12:29:41 @agent_ppo2.py:179][0m |          -0.0106 |          15.8829 |          12.1396 |
[32m[20221213 12:29:42 @agent_ppo2.py:179][0m |          -0.0220 |          15.7354 |          12.1812 |
[32m[20221213 12:29:42 @agent_ppo2.py:179][0m |          -0.0238 |          16.6414 |          12.1442 |
[32m[20221213 12:29:42 @agent_ppo2.py:179][0m |          -0.0330 |          15.8179 |          12.0724 |
[32m[20221213 12:29:42 @agent_ppo2.py:179][0m |          -0.0370 |          15.5299 |          11.9752 |
[32m[20221213 12:29:42 @agent_ppo2.py:179][0m |          -0.0260 |          16.7848 |          11.8801 |
[32m[20221213 12:29:42 @agent_ppo2.py:179][0m |          -0.0424 |          15.5356 |          11.8397 |
[32m[20221213 12:29:42 @agent_ppo2.py:179][0m |          -0.0400 |          15.4158 |          11.7185 |
[32m[20221213 12:29:42 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:29:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.48
[32m[20221213 12:29:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 259.94
[32m[20221213 12:29:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.42
[32m[20221213 12:29:43 @agent_ppo2.py:137][0m Total time:      13.77 min
[32m[20221213 12:29:43 @agent_ppo2.py:139][0m 909312 total steps have happened
[32m[20221213 12:29:43 @agent_ppo2.py:115][0m #------------------------ Iteration 444 --------------------------#
[32m[20221213 12:29:43 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:29:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:43 @agent_ppo2.py:179][0m |           0.0411 |          17.0908 |          11.5214 |
[32m[20221213 12:29:43 @agent_ppo2.py:179][0m |           0.0186 |          16.8849 |          11.3977 |
[32m[20221213 12:29:43 @agent_ppo2.py:179][0m |          -0.0031 |          16.5319 |          11.5758 |
[32m[20221213 12:29:43 @agent_ppo2.py:179][0m |          -0.0145 |          16.4804 |          11.6578 |
[32m[20221213 12:29:44 @agent_ppo2.py:179][0m |          -0.0214 |          16.3870 |          11.6576 |
[32m[20221213 12:29:44 @agent_ppo2.py:179][0m |          -0.0310 |          16.2801 |          11.5547 |
[32m[20221213 12:29:44 @agent_ppo2.py:179][0m |          -0.0367 |          16.2362 |          11.6361 |
[32m[20221213 12:29:44 @agent_ppo2.py:179][0m |          -0.0211 |          18.5409 |          11.5699 |
[32m[20221213 12:29:44 @agent_ppo2.py:179][0m |          -0.0189 |          16.1323 |          11.4144 |
[32m[20221213 12:29:44 @agent_ppo2.py:179][0m |          -0.0343 |          16.0780 |          11.3920 |
[32m[20221213 12:29:44 @agent_ppo2.py:124][0m Policy update time: 1.55 s
[32m[20221213 12:29:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 245.63
[32m[20221213 12:29:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.50
[32m[20221213 12:29:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.18
[32m[20221213 12:29:45 @agent_ppo2.py:137][0m Total time:      13.80 min
[32m[20221213 12:29:45 @agent_ppo2.py:139][0m 911360 total steps have happened
[32m[20221213 12:29:45 @agent_ppo2.py:115][0m #------------------------ Iteration 445 --------------------------#
[32m[20221213 12:29:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:45 @agent_ppo2.py:179][0m |           0.0644 |          16.5831 |          11.3286 |
[32m[20221213 12:29:45 @agent_ppo2.py:179][0m |           0.0624 |          15.7845 |          10.7128 |
[32m[20221213 12:29:45 @agent_ppo2.py:179][0m |           0.0059 |          15.5600 |          11.4721 |
[32m[20221213 12:29:46 @agent_ppo2.py:179][0m |          -0.0087 |          15.3955 |          11.7440 |
[32m[20221213 12:29:46 @agent_ppo2.py:179][0m |          -0.0234 |          15.2424 |          11.8463 |
[32m[20221213 12:29:46 @agent_ppo2.py:179][0m |          -0.0256 |          15.1077 |          11.8638 |
[32m[20221213 12:29:46 @agent_ppo2.py:179][0m |          -0.0222 |          16.3055 |          11.9143 |
[32m[20221213 12:29:46 @agent_ppo2.py:179][0m |          -0.0393 |          14.7952 |          11.8472 |
[32m[20221213 12:29:46 @agent_ppo2.py:179][0m |          -0.0435 |          14.7122 |          11.7862 |
[32m[20221213 12:29:46 @agent_ppo2.py:179][0m |          -0.0422 |          14.5538 |          11.7447 |
[32m[20221213 12:29:46 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:29:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.91
[32m[20221213 12:29:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 251.71
[32m[20221213 12:29:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 271.50
[32m[20221213 12:29:47 @agent_ppo2.py:137][0m Total time:      13.83 min
[32m[20221213 12:29:47 @agent_ppo2.py:139][0m 913408 total steps have happened
[32m[20221213 12:29:47 @agent_ppo2.py:115][0m #------------------------ Iteration 446 --------------------------#
[32m[20221213 12:29:47 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:47 @agent_ppo2.py:179][0m |           0.0760 |          17.1635 |          11.2844 |
[32m[20221213 12:29:47 @agent_ppo2.py:179][0m |           0.0918 |          15.4093 |           9.9712 |
[32m[20221213 12:29:47 @agent_ppo2.py:179][0m |           0.0130 |          15.2737 |          10.1201 |
[32m[20221213 12:29:47 @agent_ppo2.py:179][0m |          -0.0170 |          15.1831 |          10.2099 |
[32m[20221213 12:29:48 @agent_ppo2.py:179][0m |          -0.0223 |          16.3927 |          10.3005 |
[32m[20221213 12:29:48 @agent_ppo2.py:179][0m |          -0.0337 |          15.0596 |          10.2192 |
[32m[20221213 12:29:48 @agent_ppo2.py:179][0m |          -0.0357 |          15.0192 |          10.2208 |
[32m[20221213 12:29:48 @agent_ppo2.py:179][0m |          -0.0461 |          14.9738 |          10.1698 |
[32m[20221213 12:29:48 @agent_ppo2.py:179][0m |          -0.0423 |          14.9881 |          10.0639 |
[32m[20221213 12:29:48 @agent_ppo2.py:179][0m |          -0.0449 |          14.8814 |          10.0394 |
[32m[20221213 12:29:48 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:29:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 220.80
[32m[20221213 12:29:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 274.64
[32m[20221213 12:29:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 274.93
[32m[20221213 12:29:48 @agent_ppo2.py:137][0m Total time:      13.86 min
[32m[20221213 12:29:48 @agent_ppo2.py:139][0m 915456 total steps have happened
[32m[20221213 12:29:48 @agent_ppo2.py:115][0m #------------------------ Iteration 447 --------------------------#
[32m[20221213 12:29:49 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:49 @agent_ppo2.py:179][0m |           0.0462 |          16.7673 |          11.3974 |
[32m[20221213 12:29:49 @agent_ppo2.py:179][0m |           0.0198 |          14.9416 |          11.5208 |
[32m[20221213 12:29:49 @agent_ppo2.py:179][0m |          -0.0139 |          14.5183 |          11.6405 |
[32m[20221213 12:29:49 @agent_ppo2.py:179][0m |          -0.0281 |          14.0267 |          11.7049 |
[32m[20221213 12:29:49 @agent_ppo2.py:179][0m |          -0.0285 |          13.8528 |          11.7311 |
[32m[20221213 12:29:50 @agent_ppo2.py:179][0m |          -0.0425 |          13.4572 |          11.6140 |
[32m[20221213 12:29:50 @agent_ppo2.py:179][0m |          -0.0430 |          13.2158 |          11.6133 |
[32m[20221213 12:29:50 @agent_ppo2.py:179][0m |          -0.0434 |          13.0592 |          11.5836 |
[32m[20221213 12:29:50 @agent_ppo2.py:179][0m |          -0.0475 |          12.9276 |          11.4528 |
[32m[20221213 12:29:50 @agent_ppo2.py:179][0m |          -0.0513 |          12.8336 |          11.4452 |
[32m[20221213 12:29:50 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:29:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.75
[32m[20221213 12:29:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 274.62
[32m[20221213 12:29:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.33
[32m[20221213 12:29:50 @agent_ppo2.py:137][0m Total time:      13.89 min
[32m[20221213 12:29:50 @agent_ppo2.py:139][0m 917504 total steps have happened
[32m[20221213 12:29:50 @agent_ppo2.py:115][0m #------------------------ Iteration 448 --------------------------#
[32m[20221213 12:29:51 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:51 @agent_ppo2.py:179][0m |           0.0626 |          17.5326 |          11.3871 |
[32m[20221213 12:29:51 @agent_ppo2.py:179][0m |           0.0318 |          15.7098 |          11.3728 |
[32m[20221213 12:29:51 @agent_ppo2.py:179][0m |          -0.0065 |          15.2608 |          11.5005 |
[32m[20221213 12:29:51 @agent_ppo2.py:179][0m |          -0.0123 |          15.0590 |          11.5947 |
[32m[20221213 12:29:51 @agent_ppo2.py:179][0m |          -0.0303 |          14.7846 |          11.6098 |
[32m[20221213 12:29:51 @agent_ppo2.py:179][0m |          -0.0088 |          14.6885 |          11.2336 |
[32m[20221213 12:29:52 @agent_ppo2.py:179][0m |          -0.0253 |          15.7760 |           9.8079 |
[32m[20221213 12:29:52 @agent_ppo2.py:179][0m |          -0.0500 |          14.3008 |           9.5029 |
[32m[20221213 12:29:52 @agent_ppo2.py:179][0m |          -0.0633 |          14.2506 |           9.4519 |
[32m[20221213 12:29:52 @agent_ppo2.py:179][0m |          -0.0690 |          14.1444 |           9.3564 |
[32m[20221213 12:29:52 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:29:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 170.94
[32m[20221213 12:29:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 241.14
[32m[20221213 12:29:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 246.46
[32m[20221213 12:29:52 @agent_ppo2.py:137][0m Total time:      13.92 min
[32m[20221213 12:29:52 @agent_ppo2.py:139][0m 919552 total steps have happened
[32m[20221213 12:29:52 @agent_ppo2.py:115][0m #------------------------ Iteration 449 --------------------------#
[32m[20221213 12:29:52 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:29:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:53 @agent_ppo2.py:179][0m |           0.0601 |          17.0126 |          11.3330 |
[32m[20221213 12:29:53 @agent_ppo2.py:179][0m |           0.0426 |          15.1271 |          11.2344 |
[32m[20221213 12:29:53 @agent_ppo2.py:179][0m |          -0.0071 |          14.6804 |          11.4874 |
[32m[20221213 12:29:53 @agent_ppo2.py:179][0m |          -0.0232 |          14.5049 |          11.5509 |
[32m[20221213 12:29:53 @agent_ppo2.py:179][0m |          -0.0249 |          16.8066 |          11.6254 |
[32m[20221213 12:29:53 @agent_ppo2.py:179][0m |          -0.0403 |          14.1057 |          11.5960 |
[32m[20221213 12:29:53 @agent_ppo2.py:179][0m |          -0.0414 |          13.9509 |          11.4732 |
[32m[20221213 12:29:54 @agent_ppo2.py:179][0m |          -0.0460 |          13.8928 |          11.3856 |
[32m[20221213 12:29:54 @agent_ppo2.py:179][0m |          -0.0435 |          13.7126 |          11.3516 |
[32m[20221213 12:29:54 @agent_ppo2.py:179][0m |          -0.0466 |          13.6406 |          11.2954 |
[32m[20221213 12:29:54 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:29:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 245.78
[32m[20221213 12:29:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 281.53
[32m[20221213 12:29:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.04
[32m[20221213 12:29:54 @agent_ppo2.py:137][0m Total time:      13.95 min
[32m[20221213 12:29:54 @agent_ppo2.py:139][0m 921600 total steps have happened
[32m[20221213 12:29:54 @agent_ppo2.py:115][0m #------------------------ Iteration 450 --------------------------#
[32m[20221213 12:29:54 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:29:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:54 @agent_ppo2.py:179][0m |           0.0509 |          17.3142 |          10.9431 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |           0.0264 |          16.2818 |          10.9539 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |           0.0648 |          15.9180 |          10.9848 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |          -0.0064 |          15.7437 |          11.0150 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |          -0.0290 |          15.4869 |          10.9392 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |          -0.0292 |          15.8267 |          10.8201 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |          -0.0339 |          15.2556 |          10.7022 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |          -0.0291 |          15.1729 |          10.6658 |
[32m[20221213 12:29:55 @agent_ppo2.py:179][0m |          -0.0327 |          15.0772 |          10.4912 |
[32m[20221213 12:29:56 @agent_ppo2.py:179][0m |          -0.0486 |          15.0056 |          10.3804 |
[32m[20221213 12:29:56 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:29:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 196.51
[32m[20221213 12:29:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 259.79
[32m[20221213 12:29:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 249.45
[32m[20221213 12:29:56 @agent_ppo2.py:137][0m Total time:      13.98 min
[32m[20221213 12:29:56 @agent_ppo2.py:139][0m 923648 total steps have happened
[32m[20221213 12:29:56 @agent_ppo2.py:115][0m #------------------------ Iteration 451 --------------------------#
[32m[20221213 12:29:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:56 @agent_ppo2.py:179][0m |           0.0468 |          17.7091 |          10.9263 |
[32m[20221213 12:29:56 @agent_ppo2.py:179][0m |           0.0820 |          16.0663 |          10.5905 |
[32m[20221213 12:29:56 @agent_ppo2.py:179][0m |           0.0270 |          15.7580 |           8.5029 |
[32m[20221213 12:29:57 @agent_ppo2.py:179][0m |          -0.0152 |          15.4863 |           8.4365 |
[32m[20221213 12:29:57 @agent_ppo2.py:179][0m |          -0.0295 |          15.3633 |           8.3755 |
[32m[20221213 12:29:57 @agent_ppo2.py:179][0m |          -0.0360 |          15.3820 |           8.3891 |
[32m[20221213 12:29:57 @agent_ppo2.py:179][0m |          -0.0492 |          15.1661 |           8.3350 |
[32m[20221213 12:29:57 @agent_ppo2.py:179][0m |          -0.0520 |          15.0253 |           8.3233 |
[32m[20221213 12:29:57 @agent_ppo2.py:179][0m |          -0.0540 |          15.0477 |           8.2887 |
[32m[20221213 12:29:57 @agent_ppo2.py:179][0m |          -0.0592 |          14.9332 |           8.2422 |
[32m[20221213 12:29:57 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:29:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.74
[32m[20221213 12:29:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 248.24
[32m[20221213 12:29:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 252.36
[32m[20221213 12:29:58 @agent_ppo2.py:137][0m Total time:      14.01 min
[32m[20221213 12:29:58 @agent_ppo2.py:139][0m 925696 total steps have happened
[32m[20221213 12:29:58 @agent_ppo2.py:115][0m #------------------------ Iteration 452 --------------------------#
[32m[20221213 12:29:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:29:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:29:58 @agent_ppo2.py:179][0m |           0.1117 |          18.5403 |          10.0986 |
[32m[20221213 12:29:58 @agent_ppo2.py:179][0m |           0.0250 |          17.9641 |          10.2459 |
[32m[20221213 12:29:58 @agent_ppo2.py:179][0m |          -0.0019 |          17.7359 |          10.1328 |
[32m[20221213 12:29:58 @agent_ppo2.py:179][0m |          -0.0203 |          17.5683 |          10.0667 |
[32m[20221213 12:29:59 @agent_ppo2.py:179][0m |          -0.0301 |          17.4850 |          10.0464 |
[32m[20221213 12:29:59 @agent_ppo2.py:179][0m |          -0.0364 |          17.3645 |           9.9987 |
[32m[20221213 12:29:59 @agent_ppo2.py:179][0m |          -0.0411 |          17.3004 |           9.9057 |
[32m[20221213 12:29:59 @agent_ppo2.py:179][0m |          -0.0373 |          17.1909 |           9.8279 |
[32m[20221213 12:29:59 @agent_ppo2.py:179][0m |          -0.0450 |          17.1275 |           9.7640 |
[32m[20221213 12:29:59 @agent_ppo2.py:179][0m |          -0.0452 |          17.0596 |           9.7370 |
[32m[20221213 12:29:59 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:29:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 242.99
[32m[20221213 12:29:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 251.39
[32m[20221213 12:29:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 264.58
[32m[20221213 12:29:59 @agent_ppo2.py:137][0m Total time:      14.04 min
[32m[20221213 12:29:59 @agent_ppo2.py:139][0m 927744 total steps have happened
[32m[20221213 12:29:59 @agent_ppo2.py:115][0m #------------------------ Iteration 453 --------------------------#
[32m[20221213 12:30:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:00 @agent_ppo2.py:179][0m |           0.0725 |          18.3193 |          10.0424 |
[32m[20221213 12:30:00 @agent_ppo2.py:179][0m |           0.0314 |          17.9138 |          10.1771 |
[32m[20221213 12:30:00 @agent_ppo2.py:179][0m |           0.0013 |          17.8158 |          10.1706 |
[32m[20221213 12:30:00 @agent_ppo2.py:179][0m |          -0.0176 |          17.6763 |          10.2775 |
[32m[20221213 12:30:00 @agent_ppo2.py:179][0m |          -0.0220 |          17.5604 |          10.2339 |
[32m[20221213 12:30:00 @agent_ppo2.py:179][0m |          -0.0238 |          17.5614 |          10.2517 |
[32m[20221213 12:30:01 @agent_ppo2.py:179][0m |          -0.0317 |          17.4284 |          10.0131 |
[32m[20221213 12:30:01 @agent_ppo2.py:179][0m |          -0.0330 |          17.5468 |          10.0922 |
[32m[20221213 12:30:01 @agent_ppo2.py:179][0m |          -0.0365 |          17.3357 |          10.0862 |
[32m[20221213 12:30:01 @agent_ppo2.py:179][0m |          -0.0397 |          17.3279 |           9.9870 |
[32m[20221213 12:30:01 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:30:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 256.22
[32m[20221213 12:30:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.59
[32m[20221213 12:30:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 270.27
[32m[20221213 12:30:01 @agent_ppo2.py:137][0m Total time:      14.08 min
[32m[20221213 12:30:01 @agent_ppo2.py:139][0m 929792 total steps have happened
[32m[20221213 12:30:01 @agent_ppo2.py:115][0m #------------------------ Iteration 454 --------------------------#
[32m[20221213 12:30:01 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:02 @agent_ppo2.py:179][0m |           0.0737 |          16.2125 |          10.4464 |
[32m[20221213 12:30:02 @agent_ppo2.py:179][0m |           0.0396 |          15.1207 |          10.5531 |
[32m[20221213 12:30:02 @agent_ppo2.py:179][0m |           0.0086 |          14.7949 |          10.4470 |
[32m[20221213 12:30:02 @agent_ppo2.py:179][0m |           0.0004 |          14.6762 |           9.8909 |
[32m[20221213 12:30:02 @agent_ppo2.py:179][0m |          -0.0210 |          14.5702 |           8.7768 |
[32m[20221213 12:30:02 @agent_ppo2.py:179][0m |          -0.0300 |          14.7774 |           8.6000 |
[32m[20221213 12:30:02 @agent_ppo2.py:179][0m |          -0.0409 |          14.3492 |           8.5439 |
[32m[20221213 12:30:03 @agent_ppo2.py:179][0m |          -0.0407 |          14.3070 |           8.4738 |
[32m[20221213 12:30:03 @agent_ppo2.py:179][0m |          -0.0482 |          14.2283 |           8.3910 |
[32m[20221213 12:30:03 @agent_ppo2.py:179][0m |          -0.0520 |          14.1663 |           8.3639 |
[32m[20221213 12:30:03 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:30:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.58
[32m[20221213 12:30:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 252.99
[32m[20221213 12:30:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 263.33
[32m[20221213 12:30:03 @agent_ppo2.py:137][0m Total time:      14.11 min
[32m[20221213 12:30:03 @agent_ppo2.py:139][0m 931840 total steps have happened
[32m[20221213 12:30:03 @agent_ppo2.py:115][0m #------------------------ Iteration 455 --------------------------#
[32m[20221213 12:30:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:03 @agent_ppo2.py:179][0m |           0.0813 |          18.7183 |           9.7827 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |           0.0277 |          17.8475 |          10.1940 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |          -0.0113 |          17.3874 |          10.2548 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |          -0.0128 |          17.1670 |          10.2132 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |          -0.0265 |          16.9229 |          10.1705 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |          -0.0337 |          16.7659 |           9.9023 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |          -0.0413 |          16.5957 |           9.7253 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |          -0.0456 |          16.5519 |           9.6306 |
[32m[20221213 12:30:04 @agent_ppo2.py:179][0m |          -0.0441 |          16.4886 |           9.5068 |
[32m[20221213 12:30:05 @agent_ppo2.py:179][0m |          -0.0456 |          16.3289 |           9.3920 |
[32m[20221213 12:30:05 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:30:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.63
[32m[20221213 12:30:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 271.90
[32m[20221213 12:30:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 279.72
[32m[20221213 12:30:05 @agent_ppo2.py:137][0m Total time:      14.13 min
[32m[20221213 12:30:05 @agent_ppo2.py:139][0m 933888 total steps have happened
[32m[20221213 12:30:05 @agent_ppo2.py:115][0m #------------------------ Iteration 456 --------------------------#
[32m[20221213 12:30:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:05 @agent_ppo2.py:179][0m |           0.1023 |          15.9740 |           9.0554 |
[32m[20221213 12:30:05 @agent_ppo2.py:179][0m |           0.0558 |          15.6652 |           9.1056 |
[32m[20221213 12:30:05 @agent_ppo2.py:179][0m |           0.0160 |          15.2611 |           9.2923 |
[32m[20221213 12:30:06 @agent_ppo2.py:179][0m |          -0.0077 |          15.0419 |           9.3393 |
[32m[20221213 12:30:06 @agent_ppo2.py:179][0m |          -0.0297 |          14.8591 |           9.3125 |
[32m[20221213 12:30:06 @agent_ppo2.py:179][0m |          -0.0362 |          14.6888 |           9.2436 |
[32m[20221213 12:30:06 @agent_ppo2.py:179][0m |          -0.0420 |          14.6037 |           9.2377 |
[32m[20221213 12:30:06 @agent_ppo2.py:179][0m |          -0.0446 |          14.4956 |           9.0510 |
[32m[20221213 12:30:06 @agent_ppo2.py:179][0m |          -0.0554 |          14.3985 |           8.9880 |
[32m[20221213 12:30:06 @agent_ppo2.py:179][0m |          -0.0567 |          14.3222 |           8.8871 |
[32m[20221213 12:30:06 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:30:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 216.56
[32m[20221213 12:30:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 252.43
[32m[20221213 12:30:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 253.82
[32m[20221213 12:30:07 @agent_ppo2.py:137][0m Total time:      14.16 min
[32m[20221213 12:30:07 @agent_ppo2.py:139][0m 935936 total steps have happened
[32m[20221213 12:30:07 @agent_ppo2.py:115][0m #------------------------ Iteration 457 --------------------------#
[32m[20221213 12:30:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:07 @agent_ppo2.py:179][0m |           0.0537 |          18.5888 |           8.9529 |
[32m[20221213 12:30:07 @agent_ppo2.py:179][0m |           0.0889 |          17.5787 |           9.5984 |
[32m[20221213 12:30:07 @agent_ppo2.py:179][0m |           0.0534 |          18.0517 |           9.7698 |
[32m[20221213 12:30:07 @agent_ppo2.py:179][0m |           0.0117 |          17.1829 |           9.6557 |
[32m[20221213 12:30:08 @agent_ppo2.py:179][0m |          -0.0114 |          17.1021 |           9.7447 |
[32m[20221213 12:30:08 @agent_ppo2.py:179][0m |          -0.0297 |          16.9657 |           9.5797 |
[32m[20221213 12:30:08 @agent_ppo2.py:179][0m |          -0.0299 |          16.9687 |           9.6931 |
[32m[20221213 12:30:08 @agent_ppo2.py:179][0m |          -0.0363 |          16.8993 |           9.6836 |
[32m[20221213 12:30:08 @agent_ppo2.py:179][0m |          -0.0378 |          16.8257 |           9.4356 |
[32m[20221213 12:30:08 @agent_ppo2.py:179][0m |          -0.0428 |          16.7863 |           9.4039 |
[32m[20221213 12:30:08 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:30:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.41
[32m[20221213 12:30:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 261.16
[32m[20221213 12:30:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.20
[32m[20221213 12:30:08 @agent_ppo2.py:137][0m Total time:      14.19 min
[32m[20221213 12:30:08 @agent_ppo2.py:139][0m 937984 total steps have happened
[32m[20221213 12:30:08 @agent_ppo2.py:115][0m #------------------------ Iteration 458 --------------------------#
[32m[20221213 12:30:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:09 @agent_ppo2.py:179][0m |           0.0596 |          18.7066 |          10.2919 |
[32m[20221213 12:30:09 @agent_ppo2.py:179][0m |           0.0195 |          18.2244 |          10.4996 |
[32m[20221213 12:30:09 @agent_ppo2.py:179][0m |          -0.0118 |          17.8883 |          10.5036 |
[32m[20221213 12:30:09 @agent_ppo2.py:179][0m |          -0.0209 |          17.8150 |          10.6882 |
[32m[20221213 12:30:09 @agent_ppo2.py:179][0m |          -0.0268 |          17.7609 |          10.3613 |
[32m[20221213 12:30:09 @agent_ppo2.py:179][0m |          -0.0293 |          17.6598 |          10.4230 |
[32m[20221213 12:30:10 @agent_ppo2.py:179][0m |          -0.0348 |          17.6342 |          10.4031 |
[32m[20221213 12:30:10 @agent_ppo2.py:179][0m |          -0.0305 |          18.1830 |          10.3500 |
[32m[20221213 12:30:10 @agent_ppo2.py:179][0m |          -0.0376 |          17.4731 |          10.2488 |
[32m[20221213 12:30:10 @agent_ppo2.py:179][0m |          -0.0393 |          17.4529 |          10.1730 |
[32m[20221213 12:30:10 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:30:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.29
[32m[20221213 12:30:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 268.15
[32m[20221213 12:30:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 255.85
[32m[20221213 12:30:10 @agent_ppo2.py:137][0m Total time:      14.22 min
[32m[20221213 12:30:10 @agent_ppo2.py:139][0m 940032 total steps have happened
[32m[20221213 12:30:10 @agent_ppo2.py:115][0m #------------------------ Iteration 459 --------------------------#
[32m[20221213 12:30:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |           0.0457 |          16.5008 |           9.9171 |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |           0.0140 |          15.7930 |          10.0451 |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |          -0.0058 |          16.1325 |           9.9847 |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |          -0.0219 |          15.4208 |          10.0744 |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |          -0.0280 |          15.5676 |           9.9738 |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |          -0.0322 |          15.2570 |           9.8429 |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |          -0.0352 |          15.2094 |           9.7528 |
[32m[20221213 12:30:11 @agent_ppo2.py:179][0m |          -0.0411 |          15.1314 |           9.6827 |
[32m[20221213 12:30:12 @agent_ppo2.py:179][0m |          -0.0430 |          15.0478 |           9.6829 |
[32m[20221213 12:30:12 @agent_ppo2.py:179][0m |          -0.0436 |          14.9857 |           9.7057 |
[32m[20221213 12:30:12 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:30:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 223.42
[32m[20221213 12:30:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.31
[32m[20221213 12:30:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.37
[32m[20221213 12:30:12 @agent_ppo2.py:137][0m Total time:      14.25 min
[32m[20221213 12:30:12 @agent_ppo2.py:139][0m 942080 total steps have happened
[32m[20221213 12:30:12 @agent_ppo2.py:115][0m #------------------------ Iteration 460 --------------------------#
[32m[20221213 12:30:12 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:30:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:12 @agent_ppo2.py:179][0m |           0.0648 |          18.0851 |          10.0206 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |           0.0151 |          17.3887 |          10.0583 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0071 |          19.7486 |          10.1224 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0274 |          17.1064 |          10.0292 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0354 |          16.7732 |          10.0561 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0443 |          16.6889 |          10.0473 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0489 |          16.6305 |           9.8035 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0614 |          16.5309 |           9.7697 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0439 |          16.4246 |           9.4915 |
[32m[20221213 12:30:13 @agent_ppo2.py:179][0m |          -0.0287 |          18.8179 |           9.4534 |
[32m[20221213 12:30:13 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:30:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 212.91
[32m[20221213 12:30:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 231.94
[32m[20221213 12:30:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.82
[32m[20221213 12:30:14 @agent_ppo2.py:137][0m Total time:      14.28 min
[32m[20221213 12:30:14 @agent_ppo2.py:139][0m 944128 total steps have happened
[32m[20221213 12:30:14 @agent_ppo2.py:115][0m #------------------------ Iteration 461 --------------------------#
[32m[20221213 12:30:14 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:14 @agent_ppo2.py:179][0m |           0.0763 |          12.9881 |           7.6314 |
[32m[20221213 12:30:14 @agent_ppo2.py:179][0m |           0.0044 |          11.4677 |           5.0239 |
[32m[20221213 12:30:14 @agent_ppo2.py:179][0m |          -0.0371 |          10.9853 |           5.1160 |
[32m[20221213 12:30:14 @agent_ppo2.py:179][0m |          -0.0415 |          10.6540 |           4.9795 |
[32m[20221213 12:30:15 @agent_ppo2.py:179][0m |          -0.0534 |          10.3167 |           5.0445 |
[32m[20221213 12:30:15 @agent_ppo2.py:179][0m |          -0.0628 |          10.0776 |           4.8642 |
[32m[20221213 12:30:15 @agent_ppo2.py:179][0m |          -0.0654 |           9.8993 |           4.7001 |
[32m[20221213 12:30:15 @agent_ppo2.py:179][0m |          -0.0695 |           9.8182 |           4.5134 |
[32m[20221213 12:30:15 @agent_ppo2.py:179][0m |          -0.0731 |           9.6444 |           4.4379 |
[32m[20221213 12:30:15 @agent_ppo2.py:179][0m |          -0.0752 |           9.4281 |           4.4577 |
[32m[20221213 12:30:15 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:30:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 128.43
[32m[20221213 12:30:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 238.26
[32m[20221213 12:30:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 256.38
[32m[20221213 12:30:15 @agent_ppo2.py:137][0m Total time:      14.31 min
[32m[20221213 12:30:15 @agent_ppo2.py:139][0m 946176 total steps have happened
[32m[20221213 12:30:15 @agent_ppo2.py:115][0m #------------------------ Iteration 462 --------------------------#
[32m[20221213 12:30:16 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:16 @agent_ppo2.py:179][0m |           0.0968 |          19.0711 |           8.0857 |
[32m[20221213 12:30:16 @agent_ppo2.py:179][0m |           0.0582 |          18.1705 |           8.5718 |
[32m[20221213 12:30:16 @agent_ppo2.py:179][0m |           0.0033 |          17.8574 |           8.9223 |
[32m[20221213 12:30:16 @agent_ppo2.py:179][0m |          -0.0100 |          17.7132 |           8.3977 |
[32m[20221213 12:30:16 @agent_ppo2.py:179][0m |          -0.0152 |          17.4045 |           8.3761 |
[32m[20221213 12:30:17 @agent_ppo2.py:179][0m |          -0.0315 |          17.2633 |           8.2417 |
[32m[20221213 12:30:17 @agent_ppo2.py:179][0m |          -0.0354 |          17.2020 |           7.9019 |
[32m[20221213 12:30:17 @agent_ppo2.py:179][0m |          -0.0443 |          17.0663 |           7.9020 |
[32m[20221213 12:30:17 @agent_ppo2.py:179][0m |          -0.0445 |          16.9935 |           7.6292 |
[32m[20221213 12:30:17 @agent_ppo2.py:179][0m |          -0.0430 |          17.0966 |           7.3647 |
[32m[20221213 12:30:17 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:30:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.05
[32m[20221213 12:30:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 270.66
[32m[20221213 12:30:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.19
[32m[20221213 12:30:17 @agent_ppo2.py:137][0m Total time:      14.34 min
[32m[20221213 12:30:17 @agent_ppo2.py:139][0m 948224 total steps have happened
[32m[20221213 12:30:17 @agent_ppo2.py:115][0m #------------------------ Iteration 463 --------------------------#
[32m[20221213 12:30:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:18 @agent_ppo2.py:179][0m |           0.0379 |          16.8018 |           7.9428 |
[32m[20221213 12:30:18 @agent_ppo2.py:179][0m |           0.0205 |          15.7610 |           8.6414 |
[32m[20221213 12:30:18 @agent_ppo2.py:179][0m |          -0.0090 |          15.3897 |           8.4380 |
[32m[20221213 12:30:18 @agent_ppo2.py:179][0m |          -0.0258 |          15.2009 |           8.0951 |
[32m[20221213 12:30:18 @agent_ppo2.py:179][0m |          -0.0307 |          15.0693 |           7.8727 |
[32m[20221213 12:30:18 @agent_ppo2.py:179][0m |          -0.0340 |          15.1073 |           7.7773 |
[32m[20221213 12:30:19 @agent_ppo2.py:179][0m |          -0.0387 |          14.8127 |           7.5328 |
[32m[20221213 12:30:19 @agent_ppo2.py:179][0m |          -0.0380 |          14.6714 |           7.1888 |
[32m[20221213 12:30:19 @agent_ppo2.py:179][0m |          -0.0399 |          14.5743 |           7.1905 |
[32m[20221213 12:30:19 @agent_ppo2.py:179][0m |          -0.0472 |          14.4696 |           7.1474 |
[32m[20221213 12:30:19 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:30:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.92
[32m[20221213 12:30:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 267.38
[32m[20221213 12:30:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 5.07
[32m[20221213 12:30:19 @agent_ppo2.py:137][0m Total time:      14.37 min
[32m[20221213 12:30:19 @agent_ppo2.py:139][0m 950272 total steps have happened
[32m[20221213 12:30:19 @agent_ppo2.py:115][0m #------------------------ Iteration 464 --------------------------#
[32m[20221213 12:30:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.0332 |           5.7639 |           7.5832 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1281 |           5.9869 |           7.8289 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1424 |           5.3912 |           0.4926 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1433 |           5.4189 |           0.6771 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1391 |           5.3645 |           1.0571 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1452 |           5.5849 |           1.1507 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1381 |           5.3653 |           1.5436 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1461 |           5.6935 |           0.9304 |
[32m[20221213 12:30:20 @agent_ppo2.py:179][0m |           0.1339 |           5.3506 |           1.4202 |
[32m[20221213 12:30:21 @agent_ppo2.py:179][0m |           0.1348 |           5.3629 |           2.4913 |
[32m[20221213 12:30:21 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:30:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.61
[32m[20221213 12:30:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.64
[32m[20221213 12:30:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.92
[32m[20221213 12:30:21 @agent_ppo2.py:137][0m Total time:      14.40 min
[32m[20221213 12:30:21 @agent_ppo2.py:139][0m 952320 total steps have happened
[32m[20221213 12:30:21 @agent_ppo2.py:115][0m #------------------------ Iteration 465 --------------------------#
[32m[20221213 12:30:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:21 @agent_ppo2.py:179][0m |           0.0518 |          18.2523 |           8.7069 |
[32m[20221213 12:30:21 @agent_ppo2.py:179][0m |           0.0481 |          17.6964 |           9.0300 |
[32m[20221213 12:30:21 @agent_ppo2.py:179][0m |           0.0427 |          17.5543 |           9.3384 |
[32m[20221213 12:30:22 @agent_ppo2.py:179][0m |           0.0125 |          17.3137 |           8.9734 |
[32m[20221213 12:30:22 @agent_ppo2.py:179][0m |          -0.0143 |          17.2045 |           9.1961 |
[32m[20221213 12:30:22 @agent_ppo2.py:179][0m |          -0.0256 |          17.0992 |           9.3101 |
[32m[20221213 12:30:22 @agent_ppo2.py:179][0m |          -0.0356 |          17.0289 |           9.2717 |
[32m[20221213 12:30:22 @agent_ppo2.py:179][0m |          -0.0333 |          17.9097 |           9.0989 |
[32m[20221213 12:30:22 @agent_ppo2.py:179][0m |          -0.0430 |          16.9884 |           8.7856 |
[32m[20221213 12:30:22 @agent_ppo2.py:179][0m |          -0.0458 |          16.8812 |           8.9637 |
[32m[20221213 12:30:22 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:30:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.74
[32m[20221213 12:30:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 254.49
[32m[20221213 12:30:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 279.21
[32m[20221213 12:30:23 @agent_ppo2.py:137][0m Total time:      14.43 min
[32m[20221213 12:30:23 @agent_ppo2.py:139][0m 954368 total steps have happened
[32m[20221213 12:30:23 @agent_ppo2.py:115][0m #------------------------ Iteration 466 --------------------------#
[32m[20221213 12:30:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:23 @agent_ppo2.py:179][0m |           0.0405 |          16.4142 |           8.6803 |
[32m[20221213 12:30:23 @agent_ppo2.py:179][0m |           0.0285 |          16.1973 |           8.6056 |
[32m[20221213 12:30:23 @agent_ppo2.py:179][0m |          -0.0100 |          15.4947 |           8.6822 |
[32m[20221213 12:30:23 @agent_ppo2.py:179][0m |          -0.0238 |          15.2414 |           8.5696 |
[32m[20221213 12:30:23 @agent_ppo2.py:179][0m |          -0.0345 |          15.1715 |           8.7015 |
[32m[20221213 12:30:24 @agent_ppo2.py:179][0m |          -0.0349 |          15.0622 |           8.7289 |
[32m[20221213 12:30:24 @agent_ppo2.py:179][0m |          -0.0429 |          14.9912 |           8.4810 |
[32m[20221213 12:30:24 @agent_ppo2.py:179][0m |          -0.0402 |          14.9179 |           8.3113 |
[32m[20221213 12:30:24 @agent_ppo2.py:179][0m |          -0.0491 |          14.7824 |           8.0762 |
[32m[20221213 12:30:24 @agent_ppo2.py:179][0m |          -0.0511 |          14.7027 |           7.7105 |
[32m[20221213 12:30:24 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:30:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 201.63
[32m[20221213 12:30:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 249.06
[32m[20221213 12:30:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 272.87
[32m[20221213 12:30:24 @agent_ppo2.py:137][0m Total time:      14.46 min
[32m[20221213 12:30:24 @agent_ppo2.py:139][0m 956416 total steps have happened
[32m[20221213 12:30:24 @agent_ppo2.py:115][0m #------------------------ Iteration 467 --------------------------#
[32m[20221213 12:30:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:25 @agent_ppo2.py:179][0m |           0.0609 |          17.2493 |           8.1413 |
[32m[20221213 12:30:25 @agent_ppo2.py:179][0m |           0.0164 |          16.9292 |           8.5448 |
[32m[20221213 12:30:25 @agent_ppo2.py:179][0m |          -0.0088 |          16.1775 |           8.2007 |
[32m[20221213 12:30:25 @agent_ppo2.py:179][0m |          -0.0270 |          16.0224 |           8.2392 |
[32m[20221213 12:30:25 @agent_ppo2.py:179][0m |          -0.0319 |          15.9032 |           8.1693 |
[32m[20221213 12:30:25 @agent_ppo2.py:179][0m |          -0.0275 |          17.6511 |           8.1040 |
[32m[20221213 12:30:26 @agent_ppo2.py:179][0m |          -0.0433 |          15.7032 |           7.9151 |
[32m[20221213 12:30:26 @agent_ppo2.py:179][0m |          -0.0449 |          15.6155 |           7.8532 |
[32m[20221213 12:30:26 @agent_ppo2.py:179][0m |          -0.0477 |          15.5042 |           7.7208 |
[32m[20221213 12:30:26 @agent_ppo2.py:179][0m |          -0.0480 |          15.6060 |           7.5599 |
[32m[20221213 12:30:26 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:30:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 219.51
[32m[20221213 12:30:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 250.54
[32m[20221213 12:30:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 284.28
[32m[20221213 12:30:26 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 284.28
[32m[20221213 12:30:26 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 284.28
[32m[20221213 12:30:26 @agent_ppo2.py:137][0m Total time:      14.49 min
[32m[20221213 12:30:26 @agent_ppo2.py:139][0m 958464 total steps have happened
[32m[20221213 12:30:26 @agent_ppo2.py:115][0m #------------------------ Iteration 468 --------------------------#
[32m[20221213 12:30:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |           0.0478 |          17.3389 |           7.8660 |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |           0.1788 |          16.5852 |           8.7940 |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |           0.0090 |          16.2558 |           8.8138 |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |          -0.0135 |          16.0920 |           8.7507 |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |          -0.0234 |          15.8985 |           8.7742 |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |          -0.0374 |          15.7450 |           8.6533 |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |          -0.0376 |          15.5226 |           8.3608 |
[32m[20221213 12:30:27 @agent_ppo2.py:179][0m |          -0.0428 |          15.4034 |           8.3360 |
[32m[20221213 12:30:28 @agent_ppo2.py:179][0m |          -0.0483 |          15.2984 |           8.1468 |
[32m[20221213 12:30:28 @agent_ppo2.py:179][0m |          -0.0279 |          17.0234 |           7.9443 |
[32m[20221213 12:30:28 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:30:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 221.69
[32m[20221213 12:30:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.41
[32m[20221213 12:30:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 268.10
[32m[20221213 12:30:28 @agent_ppo2.py:137][0m Total time:      14.52 min
[32m[20221213 12:30:28 @agent_ppo2.py:139][0m 960512 total steps have happened
[32m[20221213 12:30:28 @agent_ppo2.py:115][0m #------------------------ Iteration 469 --------------------------#
[32m[20221213 12:30:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:28 @agent_ppo2.py:179][0m |           0.0674 |          17.1815 |           8.7503 |
[32m[20221213 12:30:28 @agent_ppo2.py:179][0m |           0.0403 |          16.3265 |           8.7769 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |           0.0065 |          15.8805 |           9.0207 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |          -0.0179 |          15.5313 |           8.7101 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |          -0.0317 |          15.2414 |           8.5830 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |          -0.0345 |          15.0372 |           8.6371 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |          -0.0338 |          16.1928 |           8.6603 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |          -0.0411 |          14.6678 |           8.4518 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |          -0.0466 |          14.4662 |           8.3617 |
[32m[20221213 12:30:29 @agent_ppo2.py:179][0m |          -0.0464 |          14.2394 |           8.3952 |
[32m[20221213 12:30:29 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:30:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.05
[32m[20221213 12:30:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 248.59
[32m[20221213 12:30:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 278.48
[32m[20221213 12:30:30 @agent_ppo2.py:137][0m Total time:      14.55 min
[32m[20221213 12:30:30 @agent_ppo2.py:139][0m 962560 total steps have happened
[32m[20221213 12:30:30 @agent_ppo2.py:115][0m #------------------------ Iteration 470 --------------------------#
[32m[20221213 12:30:30 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:30:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:30 @agent_ppo2.py:179][0m |           0.0547 |          17.7193 |           8.6117 |
[32m[20221213 12:30:30 @agent_ppo2.py:179][0m |           0.0110 |          17.0462 |           8.7011 |
[32m[20221213 12:30:30 @agent_ppo2.py:179][0m |          -0.0146 |          16.7582 |           8.6908 |
[32m[20221213 12:30:30 @agent_ppo2.py:179][0m |          -0.0259 |          16.6229 |           8.8379 |
[32m[20221213 12:30:31 @agent_ppo2.py:179][0m |          -0.0338 |          16.4822 |           8.8609 |
[32m[20221213 12:30:31 @agent_ppo2.py:179][0m |          -0.0262 |          16.3794 |           9.0301 |
[32m[20221213 12:30:31 @agent_ppo2.py:179][0m |          -0.0395 |          16.2500 |           8.9350 |
[32m[20221213 12:30:31 @agent_ppo2.py:179][0m |          -0.0376 |          16.7105 |           8.7315 |
[32m[20221213 12:30:31 @agent_ppo2.py:179][0m |          -0.0498 |          16.0769 |           8.6467 |
[32m[20221213 12:30:31 @agent_ppo2.py:179][0m |          -0.0437 |          16.3595 |           8.5855 |
[32m[20221213 12:30:31 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:30:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.16
[32m[20221213 12:30:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 266.84
[32m[20221213 12:30:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.13
[32m[20221213 12:30:31 @agent_ppo2.py:137][0m Total time:      14.58 min
[32m[20221213 12:30:31 @agent_ppo2.py:139][0m 964608 total steps have happened
[32m[20221213 12:30:31 @agent_ppo2.py:115][0m #------------------------ Iteration 471 --------------------------#
[32m[20221213 12:30:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:32 @agent_ppo2.py:179][0m |           0.1257 |          18.4835 |           9.0852 |
[32m[20221213 12:30:32 @agent_ppo2.py:179][0m |           0.0257 |          17.9737 |           9.2019 |
[32m[20221213 12:30:32 @agent_ppo2.py:179][0m |          -0.0008 |          18.6336 |           8.9540 |
[32m[20221213 12:30:32 @agent_ppo2.py:179][0m |          -0.0189 |          17.5022 |           9.2026 |
[32m[20221213 12:30:32 @agent_ppo2.py:179][0m |          -0.0275 |          17.3471 |           9.2090 |
[32m[20221213 12:30:33 @agent_ppo2.py:179][0m |          -0.0355 |          17.1896 |           9.0603 |
[32m[20221213 12:30:33 @agent_ppo2.py:179][0m |          -0.0410 |          17.1153 |           8.8438 |
[32m[20221213 12:30:33 @agent_ppo2.py:179][0m |          -0.0449 |          17.0050 |           8.9116 |
[32m[20221213 12:30:33 @agent_ppo2.py:179][0m |          -0.0401 |          16.9192 |           8.7149 |
[32m[20221213 12:30:33 @agent_ppo2.py:179][0m |          -0.0433 |          16.8502 |           8.6385 |
[32m[20221213 12:30:33 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:30:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.55
[32m[20221213 12:30:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 254.89
[32m[20221213 12:30:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 288.36
[32m[20221213 12:30:33 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 288.36
[32m[20221213 12:30:33 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 288.36
[32m[20221213 12:30:33 @agent_ppo2.py:137][0m Total time:      14.61 min
[32m[20221213 12:30:33 @agent_ppo2.py:139][0m 966656 total steps have happened
[32m[20221213 12:30:33 @agent_ppo2.py:115][0m #------------------------ Iteration 472 --------------------------#
[32m[20221213 12:30:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:34 @agent_ppo2.py:179][0m |           0.0602 |          17.5298 |           9.1103 |
[32m[20221213 12:30:34 @agent_ppo2.py:179][0m |           0.0372 |          16.8143 |           8.8161 |
[32m[20221213 12:30:34 @agent_ppo2.py:179][0m |           0.0153 |          18.9233 |           8.9703 |
[32m[20221213 12:30:34 @agent_ppo2.py:179][0m |          -0.0007 |          16.1981 |           8.9395 |
[32m[20221213 12:30:34 @agent_ppo2.py:179][0m |          -0.0126 |          15.9324 |           9.0535 |
[32m[20221213 12:30:34 @agent_ppo2.py:179][0m |          -0.0281 |          15.7613 |           8.9698 |
[32m[20221213 12:30:34 @agent_ppo2.py:179][0m |          -0.0341 |          15.5814 |           8.8700 |
[32m[20221213 12:30:35 @agent_ppo2.py:179][0m |          -0.0394 |          15.4537 |           8.7365 |
[32m[20221213 12:30:35 @agent_ppo2.py:179][0m |          -0.0391 |          15.2938 |           8.6751 |
[32m[20221213 12:30:35 @agent_ppo2.py:179][0m |          -0.0328 |          16.4402 |           8.5754 |
[32m[20221213 12:30:35 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:30:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 259.06
[32m[20221213 12:30:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 270.35
[32m[20221213 12:30:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 188.27
[32m[20221213 12:30:35 @agent_ppo2.py:137][0m Total time:      14.64 min
[32m[20221213 12:30:35 @agent_ppo2.py:139][0m 968704 total steps have happened
[32m[20221213 12:30:35 @agent_ppo2.py:115][0m #------------------------ Iteration 473 --------------------------#
[32m[20221213 12:30:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:35 @agent_ppo2.py:179][0m |           0.0416 |          18.0707 |           9.1148 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |           0.0145 |          17.2474 |           9.0422 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |          -0.0119 |          16.9238 |           8.9820 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |          -0.0225 |          16.7230 |           8.8284 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |          -0.0350 |          16.5886 |           8.7789 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |          -0.0349 |          16.5013 |           8.5678 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |          -0.0411 |          16.3669 |           8.7919 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |          -0.0411 |          16.3111 |           8.4892 |
[32m[20221213 12:30:36 @agent_ppo2.py:179][0m |          -0.0303 |          17.1373 |           8.4814 |
[32m[20221213 12:30:37 @agent_ppo2.py:179][0m |          -0.0160 |          16.2389 |           8.5635 |
[32m[20221213 12:30:37 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:30:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.20
[32m[20221213 12:30:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.72
[32m[20221213 12:30:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.25
[32m[20221213 12:30:37 @agent_ppo2.py:137][0m Total time:      14.67 min
[32m[20221213 12:30:37 @agent_ppo2.py:139][0m 970752 total steps have happened
[32m[20221213 12:30:37 @agent_ppo2.py:115][0m #------------------------ Iteration 474 --------------------------#
[32m[20221213 12:30:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:37 @agent_ppo2.py:179][0m |           0.0573 |          17.9524 |           8.2738 |
[32m[20221213 12:30:37 @agent_ppo2.py:179][0m |           0.0349 |          17.8233 |           8.5532 |
[32m[20221213 12:30:37 @agent_ppo2.py:179][0m |          -0.0122 |          17.1333 |           8.6908 |
[32m[20221213 12:30:38 @agent_ppo2.py:179][0m |          -0.0205 |          16.9462 |           8.7538 |
[32m[20221213 12:30:38 @agent_ppo2.py:179][0m |          -0.0314 |          16.8692 |           8.5474 |
[32m[20221213 12:30:38 @agent_ppo2.py:179][0m |          -0.0365 |          16.7223 |           8.3878 |
[32m[20221213 12:30:38 @agent_ppo2.py:179][0m |          -0.0385 |          16.6018 |           8.4377 |
[32m[20221213 12:30:38 @agent_ppo2.py:179][0m |          -0.0410 |          16.5336 |           8.3966 |
[32m[20221213 12:30:38 @agent_ppo2.py:179][0m |          -0.0437 |          16.4582 |           8.2148 |
[32m[20221213 12:30:38 @agent_ppo2.py:179][0m |          -0.0429 |          16.4360 |           8.1102 |
[32m[20221213 12:30:38 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:30:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.62
[32m[20221213 12:30:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.07
[32m[20221213 12:30:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 268.12
[32m[20221213 12:30:39 @agent_ppo2.py:137][0m Total time:      14.70 min
[32m[20221213 12:30:39 @agent_ppo2.py:139][0m 972800 total steps have happened
[32m[20221213 12:30:39 @agent_ppo2.py:115][0m #------------------------ Iteration 475 --------------------------#
[32m[20221213 12:30:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:39 @agent_ppo2.py:179][0m |           0.0552 |          18.0523 |           8.5042 |
[32m[20221213 12:30:39 @agent_ppo2.py:179][0m |           0.0331 |          17.6091 |           8.5960 |
[32m[20221213 12:30:39 @agent_ppo2.py:179][0m |           0.0012 |          17.2042 |           8.7177 |
[32m[20221213 12:30:39 @agent_ppo2.py:179][0m |          -0.0172 |          16.9941 |           8.7530 |
[32m[20221213 12:30:40 @agent_ppo2.py:179][0m |          -0.0271 |          16.8246 |           8.5169 |
[32m[20221213 12:30:40 @agent_ppo2.py:179][0m |          -0.0208 |          18.7115 |           8.4968 |
[32m[20221213 12:30:40 @agent_ppo2.py:179][0m |          -0.0265 |          17.1227 |           8.3503 |
[32m[20221213 12:30:40 @agent_ppo2.py:179][0m |          -0.0370 |          16.4650 |           8.0532 |
[32m[20221213 12:30:40 @agent_ppo2.py:179][0m |          -0.0351 |          16.2791 |           8.1482 |
[32m[20221213 12:30:40 @agent_ppo2.py:179][0m |          -0.0409 |          16.2381 |           7.9103 |
[32m[20221213 12:30:40 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:30:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 225.10
[32m[20221213 12:30:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 253.48
[32m[20221213 12:30:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.60
[32m[20221213 12:30:40 @agent_ppo2.py:137][0m Total time:      14.73 min
[32m[20221213 12:30:40 @agent_ppo2.py:139][0m 974848 total steps have happened
[32m[20221213 12:30:40 @agent_ppo2.py:115][0m #------------------------ Iteration 476 --------------------------#
[32m[20221213 12:30:41 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:41 @agent_ppo2.py:179][0m |           0.0571 |          18.5949 |           7.6812 |
[32m[20221213 12:30:41 @agent_ppo2.py:179][0m |           0.0558 |          18.1207 |           8.1908 |
[32m[20221213 12:30:41 @agent_ppo2.py:179][0m |           0.0120 |          18.1356 |           8.1326 |
[32m[20221213 12:30:41 @agent_ppo2.py:179][0m |          -0.0091 |          17.6051 |           8.2295 |
[32m[20221213 12:30:41 @agent_ppo2.py:179][0m |          -0.0245 |          17.4729 |           8.1416 |
[32m[20221213 12:30:42 @agent_ppo2.py:179][0m |          -0.0269 |          17.4213 |           7.9674 |
[32m[20221213 12:30:42 @agent_ppo2.py:179][0m |          -0.0288 |          18.4447 |           8.0237 |
[32m[20221213 12:30:42 @agent_ppo2.py:179][0m |          -0.0354 |          17.1867 |           7.9680 |
[32m[20221213 12:30:42 @agent_ppo2.py:179][0m |          -0.0437 |          17.1013 |           7.8009 |
[32m[20221213 12:30:42 @agent_ppo2.py:179][0m |          -0.0362 |          16.9920 |           7.6292 |
[32m[20221213 12:30:42 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:30:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 253.99
[32m[20221213 12:30:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 269.00
[32m[20221213 12:30:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 273.56
[32m[20221213 12:30:42 @agent_ppo2.py:137][0m Total time:      14.76 min
[32m[20221213 12:30:42 @agent_ppo2.py:139][0m 976896 total steps have happened
[32m[20221213 12:30:42 @agent_ppo2.py:115][0m #------------------------ Iteration 477 --------------------------#
[32m[20221213 12:30:43 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:30:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:43 @agent_ppo2.py:179][0m |           0.0571 |          17.1040 |           8.1974 |
[32m[20221213 12:30:43 @agent_ppo2.py:179][0m |           0.0224 |          15.6575 |           8.3180 |
[32m[20221213 12:30:43 @agent_ppo2.py:179][0m |           0.0007 |          15.5347 |           7.9723 |
[32m[20221213 12:30:43 @agent_ppo2.py:179][0m |          -0.0057 |          15.4187 |           7.8625 |
[32m[20221213 12:30:43 @agent_ppo2.py:179][0m |          -0.0184 |          15.4842 |           7.7963 |
[32m[20221213 12:30:43 @agent_ppo2.py:179][0m |          -0.0306 |          15.2840 |           7.8890 |
[32m[20221213 12:30:44 @agent_ppo2.py:179][0m |          -0.0287 |          15.2613 |           7.7674 |
[32m[20221213 12:30:44 @agent_ppo2.py:179][0m |          -0.0235 |          16.7292 |           7.6586 |
[32m[20221213 12:30:44 @agent_ppo2.py:179][0m |          -0.0234 |          15.1246 |           7.5254 |
[32m[20221213 12:30:44 @agent_ppo2.py:179][0m |          -0.0236 |          15.1192 |           7.4307 |
[32m[20221213 12:30:44 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:30:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 203.84
[32m[20221213 12:30:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 259.59
[32m[20221213 12:30:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.37
[32m[20221213 12:30:44 @agent_ppo2.py:137][0m Total time:      14.79 min
[32m[20221213 12:30:44 @agent_ppo2.py:139][0m 978944 total steps have happened
[32m[20221213 12:30:44 @agent_ppo2.py:115][0m #------------------------ Iteration 478 --------------------------#
[32m[20221213 12:30:44 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:45 @agent_ppo2.py:179][0m |           0.0561 |          17.4567 |           7.8658 |
[32m[20221213 12:30:45 @agent_ppo2.py:179][0m |           0.0350 |          16.5247 |           8.6639 |
[32m[20221213 12:30:45 @agent_ppo2.py:179][0m |           0.0018 |          16.0496 |           8.0972 |
[32m[20221213 12:30:45 @agent_ppo2.py:179][0m |          -0.0138 |          15.7883 |           8.0793 |
[32m[20221213 12:30:45 @agent_ppo2.py:179][0m |          -0.0307 |          15.6101 |           8.0293 |
[32m[20221213 12:30:45 @agent_ppo2.py:179][0m |          -0.0319 |          15.7486 |           7.8081 |
[32m[20221213 12:30:45 @agent_ppo2.py:179][0m |          -0.0433 |          15.4139 |           7.5714 |
[32m[20221213 12:30:46 @agent_ppo2.py:179][0m |          -0.0451 |          15.3602 |           7.4578 |
[32m[20221213 12:30:46 @agent_ppo2.py:179][0m |          -0.0488 |          15.2099 |           7.3747 |
[32m[20221213 12:30:46 @agent_ppo2.py:179][0m |          -0.0501 |          15.1472 |           7.3111 |
[32m[20221213 12:30:46 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:30:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 210.23
[32m[20221213 12:30:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 241.86
[32m[20221213 12:30:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 276.60
[32m[20221213 12:30:46 @agent_ppo2.py:137][0m Total time:      14.82 min
[32m[20221213 12:30:46 @agent_ppo2.py:139][0m 980992 total steps have happened
[32m[20221213 12:30:46 @agent_ppo2.py:115][0m #------------------------ Iteration 479 --------------------------#
[32m[20221213 12:30:46 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |           0.0508 |          19.4339 |           8.4440 |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |           0.0408 |          18.9766 |           8.7968 |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |          -0.0032 |          18.7048 |           8.8029 |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |          -0.0122 |          18.6327 |           8.8530 |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |          -0.0165 |          19.0506 |           8.6566 |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |          -0.0263 |          18.3466 |           8.5671 |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |          -0.0336 |          18.2620 |           8.4237 |
[32m[20221213 12:30:47 @agent_ppo2.py:179][0m |          -0.0304 |          18.2843 |           8.2878 |
[32m[20221213 12:30:48 @agent_ppo2.py:179][0m |          -0.0215 |          18.6389 |           8.0629 |
[32m[20221213 12:30:48 @agent_ppo2.py:179][0m |          -0.0374 |          18.0060 |           7.8566 |
[32m[20221213 12:30:48 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:30:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.84
[32m[20221213 12:30:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 266.54
[32m[20221213 12:30:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 269.39
[32m[20221213 12:30:48 @agent_ppo2.py:137][0m Total time:      14.85 min
[32m[20221213 12:30:48 @agent_ppo2.py:139][0m 983040 total steps have happened
[32m[20221213 12:30:48 @agent_ppo2.py:115][0m #------------------------ Iteration 480 --------------------------#
[32m[20221213 12:30:48 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:30:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:48 @agent_ppo2.py:179][0m |           0.0502 |          19.3065 |           7.8569 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |           0.0528 |          21.3964 |           8.6279 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |          -0.0103 |          18.6095 |           8.5329 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |          -0.0238 |          18.3969 |           8.4960 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |          -0.0196 |          20.0607 |           8.5206 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |          -0.0380 |          18.1300 |           8.3544 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |          -0.0386 |          18.0285 |           8.1153 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |          -0.0374 |          18.0440 |           8.0232 |
[32m[20221213 12:30:49 @agent_ppo2.py:179][0m |          -0.0377 |          18.1544 |           8.0161 |
[32m[20221213 12:30:50 @agent_ppo2.py:179][0m |          -0.0418 |          17.7043 |           7.9446 |
[32m[20221213 12:30:50 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:30:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 248.03
[32m[20221213 12:30:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 261.21
[32m[20221213 12:30:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.06
[32m[20221213 12:30:50 @agent_ppo2.py:137][0m Total time:      14.88 min
[32m[20221213 12:30:50 @agent_ppo2.py:139][0m 985088 total steps have happened
[32m[20221213 12:30:50 @agent_ppo2.py:115][0m #------------------------ Iteration 481 --------------------------#
[32m[20221213 12:30:50 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:50 @agent_ppo2.py:179][0m |           0.0418 |          18.7414 |           8.1296 |
[32m[20221213 12:30:50 @agent_ppo2.py:179][0m |           0.0361 |          18.4625 |           8.5045 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |           0.0036 |          18.2306 |           8.6261 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |          -0.0150 |          18.1369 |           8.7031 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |          -0.0206 |          18.0749 |           8.6238 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |          -0.0337 |          17.9524 |           8.6896 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |          -0.0368 |          17.9143 |           8.6753 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |          -0.0391 |          17.8605 |           8.4727 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |          -0.0373 |          17.8000 |           8.3490 |
[32m[20221213 12:30:51 @agent_ppo2.py:179][0m |          -0.0418 |          17.7636 |           8.4639 |
[32m[20221213 12:30:51 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:30:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 233.51
[32m[20221213 12:30:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 263.19
[32m[20221213 12:30:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 258.33
[32m[20221213 12:30:52 @agent_ppo2.py:137][0m Total time:      14.92 min
[32m[20221213 12:30:52 @agent_ppo2.py:139][0m 987136 total steps have happened
[32m[20221213 12:30:52 @agent_ppo2.py:115][0m #------------------------ Iteration 482 --------------------------#
[32m[20221213 12:30:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:52 @agent_ppo2.py:179][0m |           0.0541 |          18.3210 |           8.2025 |
[32m[20221213 12:30:52 @agent_ppo2.py:179][0m |           0.0319 |          18.8932 |           8.5499 |
[32m[20221213 12:30:52 @agent_ppo2.py:179][0m |           0.0071 |          17.5064 |           8.1982 |
[32m[20221213 12:30:52 @agent_ppo2.py:179][0m |          -0.0094 |          17.3462 |           8.1356 |
[32m[20221213 12:30:53 @agent_ppo2.py:179][0m |          -0.0153 |          19.3978 |           7.9614 |
[32m[20221213 12:30:53 @agent_ppo2.py:179][0m |          -0.0327 |          16.8084 |           7.8635 |
[32m[20221213 12:30:53 @agent_ppo2.py:179][0m |          -0.0358 |          16.5572 |           7.9314 |
[32m[20221213 12:30:53 @agent_ppo2.py:179][0m |          -0.0388 |          16.4054 |           7.7916 |
[32m[20221213 12:30:53 @agent_ppo2.py:179][0m |          -0.0358 |          16.2718 |           7.5636 |
[32m[20221213 12:30:53 @agent_ppo2.py:179][0m |          -0.0309 |          16.1940 |           7.4080 |
[32m[20221213 12:30:53 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:30:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.96
[32m[20221213 12:30:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 265.20
[32m[20221213 12:30:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 243.38
[32m[20221213 12:30:53 @agent_ppo2.py:137][0m Total time:      14.95 min
[32m[20221213 12:30:53 @agent_ppo2.py:139][0m 989184 total steps have happened
[32m[20221213 12:30:53 @agent_ppo2.py:115][0m #------------------------ Iteration 483 --------------------------#
[32m[20221213 12:30:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:54 @agent_ppo2.py:179][0m |           0.0447 |          16.2733 |           8.3006 |
[32m[20221213 12:30:54 @agent_ppo2.py:179][0m |           0.0255 |          14.8929 |           7.7920 |
[32m[20221213 12:30:54 @agent_ppo2.py:179][0m |          -0.0053 |          14.2202 |           6.4566 |
[32m[20221213 12:30:54 @agent_ppo2.py:179][0m |          -0.0285 |          13.7753 |           6.2627 |
[32m[20221213 12:30:54 @agent_ppo2.py:179][0m |          -0.0326 |          13.6677 |           6.0633 |
[32m[20221213 12:30:55 @agent_ppo2.py:179][0m |          -0.0453 |          13.2264 |           5.8072 |
[32m[20221213 12:30:55 @agent_ppo2.py:179][0m |          -0.0496 |          12.9907 |           5.7052 |
[32m[20221213 12:30:55 @agent_ppo2.py:179][0m |          -0.0474 |          12.7476 |           5.6982 |
[32m[20221213 12:30:55 @agent_ppo2.py:179][0m |          -0.0529 |          12.7007 |           5.4399 |
[32m[20221213 12:30:55 @agent_ppo2.py:179][0m |          -0.0609 |          12.4040 |           5.2765 |
[32m[20221213 12:30:55 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:30:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 204.92
[32m[20221213 12:30:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 269.32
[32m[20221213 12:30:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.26
[32m[20221213 12:30:55 @agent_ppo2.py:137][0m Total time:      14.98 min
[32m[20221213 12:30:55 @agent_ppo2.py:139][0m 991232 total steps have happened
[32m[20221213 12:30:55 @agent_ppo2.py:115][0m #------------------------ Iteration 484 --------------------------#
[32m[20221213 12:30:56 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:30:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:56 @agent_ppo2.py:179][0m |           0.0765 |          23.5170 |           7.3321 |
[32m[20221213 12:30:56 @agent_ppo2.py:179][0m |           0.0435 |          19.1620 |           7.4119 |
[32m[20221213 12:30:56 @agent_ppo2.py:179][0m |           0.0085 |          18.7197 |           7.5655 |
[32m[20221213 12:30:56 @agent_ppo2.py:179][0m |          -0.0125 |          18.5018 |           7.2879 |
[32m[20221213 12:30:56 @agent_ppo2.py:179][0m |          -0.0241 |          18.3413 |           7.2591 |
[32m[20221213 12:30:56 @agent_ppo2.py:179][0m |          -0.0295 |          18.3043 |           7.1275 |
[32m[20221213 12:30:57 @agent_ppo2.py:179][0m |          -0.0255 |          20.6645 |           6.9424 |
[32m[20221213 12:30:57 @agent_ppo2.py:179][0m |          -0.0473 |          18.0951 |           6.7516 |
[32m[20221213 12:30:57 @agent_ppo2.py:179][0m |          -0.0411 |          17.9833 |           6.7228 |
[32m[20221213 12:30:57 @agent_ppo2.py:179][0m |          -0.0462 |          17.9128 |           6.5510 |
[32m[20221213 12:30:57 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:30:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.46
[32m[20221213 12:30:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 262.84
[32m[20221213 12:30:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 251.51
[32m[20221213 12:30:57 @agent_ppo2.py:137][0m Total time:      15.01 min
[32m[20221213 12:30:57 @agent_ppo2.py:139][0m 993280 total steps have happened
[32m[20221213 12:30:57 @agent_ppo2.py:115][0m #------------------------ Iteration 485 --------------------------#
[32m[20221213 12:30:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |           0.0681 |          19.2057 |           7.4962 |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |           0.0305 |          18.8440 |           8.0194 |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |          -0.0072 |          18.3493 |           7.7095 |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |          -0.0153 |          18.2533 |           7.8206 |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |          -0.0290 |          18.0258 |           7.6560 |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |          -0.0230 |          18.8116 |           7.7177 |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |          -0.0192 |          17.8516 |           8.0116 |
[32m[20221213 12:30:58 @agent_ppo2.py:179][0m |          -0.0372 |          17.7853 |           7.8188 |
[32m[20221213 12:30:59 @agent_ppo2.py:179][0m |          -0.0397 |          17.7394 |           7.7399 |
[32m[20221213 12:30:59 @agent_ppo2.py:179][0m |          -0.0397 |          17.6592 |           7.6606 |
[32m[20221213 12:30:59 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:30:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 245.56
[32m[20221213 12:30:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 278.52
[32m[20221213 12:30:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 264.70
[32m[20221213 12:30:59 @agent_ppo2.py:137][0m Total time:      15.04 min
[32m[20221213 12:30:59 @agent_ppo2.py:139][0m 995328 total steps have happened
[32m[20221213 12:30:59 @agent_ppo2.py:115][0m #------------------------ Iteration 486 --------------------------#
[32m[20221213 12:30:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:30:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:30:59 @agent_ppo2.py:179][0m |           0.0657 |          18.6827 |           8.3831 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |           0.0239 |          18.0920 |           8.1820 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |          -0.0010 |          17.7691 |           8.0073 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |          -0.0239 |          17.4942 |           7.5709 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |          -0.0310 |          17.2959 |           7.3738 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |          -0.0341 |          17.1477 |           7.2078 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |          -0.0266 |          17.7976 |           6.9773 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |          -0.0353 |          16.7305 |           7.0275 |
[32m[20221213 12:31:00 @agent_ppo2.py:179][0m |          -0.0421 |          16.6057 |           6.7730 |
[32m[20221213 12:31:01 @agent_ppo2.py:179][0m |          -0.0369 |          17.1004 |           6.6858 |
[32m[20221213 12:31:01 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:31:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 273.01
[32m[20221213 12:31:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 284.58
[32m[20221213 12:31:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.05
[32m[20221213 12:31:01 @agent_ppo2.py:137][0m Total time:      15.07 min
[32m[20221213 12:31:01 @agent_ppo2.py:139][0m 997376 total steps have happened
[32m[20221213 12:31:01 @agent_ppo2.py:115][0m #------------------------ Iteration 487 --------------------------#
[32m[20221213 12:31:01 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:01 @agent_ppo2.py:179][0m |           0.0363 |          19.3333 |           7.2651 |
[32m[20221213 12:31:01 @agent_ppo2.py:179][0m |           0.0306 |          18.4426 |           7.6086 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |           0.0014 |          18.2454 |           7.4246 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |          -0.0240 |          17.6370 |           7.2239 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |          -0.0331 |          17.4289 |           7.0253 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |          -0.0417 |          17.1558 |           7.1082 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |          -0.0404 |          17.0964 |           7.0073 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |          -0.0493 |          16.8022 |           6.8142 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |          -0.0422 |          18.9286 |           6.6042 |
[32m[20221213 12:31:02 @agent_ppo2.py:179][0m |          -0.0509 |          16.9377 |           6.5309 |
[32m[20221213 12:31:02 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:31:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 233.76
[32m[20221213 12:31:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 255.31
[32m[20221213 12:31:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.15
[32m[20221213 12:31:03 @agent_ppo2.py:137][0m Total time:      15.10 min
[32m[20221213 12:31:03 @agent_ppo2.py:139][0m 999424 total steps have happened
[32m[20221213 12:31:03 @agent_ppo2.py:115][0m #------------------------ Iteration 488 --------------------------#
[32m[20221213 12:31:03 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:03 @agent_ppo2.py:179][0m |           0.0784 |          18.4670 |           7.0041 |
[32m[20221213 12:31:03 @agent_ppo2.py:179][0m |           0.0483 |          18.0791 |           7.6564 |
[32m[20221213 12:31:03 @agent_ppo2.py:179][0m |           0.0007 |          17.4589 |           7.5114 |
[32m[20221213 12:31:04 @agent_ppo2.py:179][0m |           0.0021 |          17.2807 |           7.2740 |
[32m[20221213 12:31:04 @agent_ppo2.py:179][0m |          -0.0184 |          17.1018 |           7.2501 |
[32m[20221213 12:31:04 @agent_ppo2.py:179][0m |          -0.0288 |          16.9654 |           7.0368 |
[32m[20221213 12:31:04 @agent_ppo2.py:179][0m |          -0.0262 |          17.6241 |           6.8865 |
[32m[20221213 12:31:04 @agent_ppo2.py:179][0m |          -0.0340 |          16.7726 |           6.8776 |
[32m[20221213 12:31:04 @agent_ppo2.py:179][0m |          -0.0386 |          16.7618 |           6.5983 |
[32m[20221213 12:31:04 @agent_ppo2.py:179][0m |          -0.0355 |          16.8929 |           6.4116 |
[32m[20221213 12:31:04 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:31:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 256.19
[32m[20221213 12:31:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 265.34
[32m[20221213 12:31:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.39
[32m[20221213 12:31:05 @agent_ppo2.py:137][0m Total time:      15.13 min
[32m[20221213 12:31:05 @agent_ppo2.py:139][0m 1001472 total steps have happened
[32m[20221213 12:31:05 @agent_ppo2.py:115][0m #------------------------ Iteration 489 --------------------------#
[32m[20221213 12:31:05 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:05 @agent_ppo2.py:179][0m |           0.0778 |          20.1753 |           7.4755 |
[32m[20221213 12:31:05 @agent_ppo2.py:179][0m |           0.0360 |          21.6961 |           7.5173 |
[32m[20221213 12:31:05 @agent_ppo2.py:179][0m |           0.0211 |          19.0974 |           7.7099 |
[32m[20221213 12:31:05 @agent_ppo2.py:179][0m |          -0.0031 |          18.8684 |           7.7281 |
[32m[20221213 12:31:06 @agent_ppo2.py:179][0m |          -0.0237 |          18.7303 |           7.6062 |
[32m[20221213 12:31:06 @agent_ppo2.py:179][0m |          -0.0229 |          18.6679 |           7.4835 |
[32m[20221213 12:31:06 @agent_ppo2.py:179][0m |          -0.0296 |          18.5824 |           7.3029 |
[32m[20221213 12:31:06 @agent_ppo2.py:179][0m |          -0.0393 |          18.5169 |           7.3355 |
[32m[20221213 12:31:06 @agent_ppo2.py:179][0m |          -0.0437 |          18.3702 |           7.1321 |
[32m[20221213 12:31:06 @agent_ppo2.py:179][0m |          -0.0394 |          18.4837 |           6.9774 |
[32m[20221213 12:31:06 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:31:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.30
[32m[20221213 12:31:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 279.59
[32m[20221213 12:31:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 300.15
[32m[20221213 12:31:06 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 300.15
[32m[20221213 12:31:06 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 300.15
[32m[20221213 12:31:06 @agent_ppo2.py:137][0m Total time:      15.16 min
[32m[20221213 12:31:06 @agent_ppo2.py:139][0m 1003520 total steps have happened
[32m[20221213 12:31:06 @agent_ppo2.py:115][0m #------------------------ Iteration 490 --------------------------#
[32m[20221213 12:31:07 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:31:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:07 @agent_ppo2.py:179][0m |           0.0821 |          19.3383 |           7.2760 |
[32m[20221213 12:31:07 @agent_ppo2.py:179][0m |           0.0487 |          18.7077 |           7.6350 |
[32m[20221213 12:31:07 @agent_ppo2.py:179][0m |           0.0037 |          18.2525 |           7.2385 |
[32m[20221213 12:31:07 @agent_ppo2.py:179][0m |          -0.0136 |          18.0315 |           7.3347 |
[32m[20221213 12:31:07 @agent_ppo2.py:179][0m |          -0.0207 |          17.7100 |           7.2546 |
[32m[20221213 12:31:08 @agent_ppo2.py:179][0m |          -0.0248 |          17.5139 |           7.1533 |
[32m[20221213 12:31:08 @agent_ppo2.py:179][0m |          -0.0345 |          17.3186 |           6.6699 |
[32m[20221213 12:31:08 @agent_ppo2.py:179][0m |          -0.0339 |          17.1959 |           6.7304 |
[32m[20221213 12:31:08 @agent_ppo2.py:179][0m |          -0.0386 |          17.0499 |           6.5688 |
[32m[20221213 12:31:08 @agent_ppo2.py:179][0m |          -0.0400 |          16.8484 |           6.3537 |
[32m[20221213 12:31:08 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:31:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.35
[32m[20221213 12:31:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 283.78
[32m[20221213 12:31:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 296.65
[32m[20221213 12:31:08 @agent_ppo2.py:137][0m Total time:      15.19 min
[32m[20221213 12:31:08 @agent_ppo2.py:139][0m 1005568 total steps have happened
[32m[20221213 12:31:08 @agent_ppo2.py:115][0m #------------------------ Iteration 491 --------------------------#
[32m[20221213 12:31:09 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:09 @agent_ppo2.py:179][0m |           0.0656 |          20.0049 |           6.3774 |
[32m[20221213 12:31:09 @agent_ppo2.py:179][0m |           0.0449 |          19.0959 |           6.7461 |
[32m[20221213 12:31:09 @agent_ppo2.py:179][0m |           0.0039 |          19.5106 |           6.3222 |
[32m[20221213 12:31:09 @agent_ppo2.py:179][0m |          -0.0223 |          18.1774 |           5.8425 |
[32m[20221213 12:31:09 @agent_ppo2.py:179][0m |          -0.0235 |          18.7915 |           5.8304 |
[32m[20221213 12:31:10 @agent_ppo2.py:179][0m |          -0.0380 |          17.7506 |           5.6768 |
[32m[20221213 12:31:10 @agent_ppo2.py:179][0m |          -0.0425 |          17.6072 |           5.7276 |
[32m[20221213 12:31:10 @agent_ppo2.py:179][0m |          -0.0444 |          17.4498 |           5.4874 |
[32m[20221213 12:31:10 @agent_ppo2.py:179][0m |          -0.0462 |          17.3269 |           5.3749 |
[32m[20221213 12:31:10 @agent_ppo2.py:179][0m |          -0.0482 |          17.1806 |           5.1741 |
[32m[20221213 12:31:10 @agent_ppo2.py:124][0m Policy update time: 1.62 s
[32m[20221213 12:31:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.70
[32m[20221213 12:31:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.45
[32m[20221213 12:31:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 281.69
[32m[20221213 12:31:10 @agent_ppo2.py:137][0m Total time:      15.23 min
[32m[20221213 12:31:10 @agent_ppo2.py:139][0m 1007616 total steps have happened
[32m[20221213 12:31:10 @agent_ppo2.py:115][0m #------------------------ Iteration 492 --------------------------#
[32m[20221213 12:31:11 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:11 @agent_ppo2.py:179][0m |           0.0876 |          17.4955 |           6.7352 |
[32m[20221213 12:31:11 @agent_ppo2.py:179][0m |           0.0573 |          16.1428 |           7.2882 |
[32m[20221213 12:31:11 @agent_ppo2.py:179][0m |          -0.0001 |          15.6100 |           7.0403 |
[32m[20221213 12:31:11 @agent_ppo2.py:179][0m |          -0.0175 |          15.3602 |           7.2427 |
[32m[20221213 12:31:11 @agent_ppo2.py:179][0m |          -0.0335 |          15.0535 |           6.8292 |
[32m[20221213 12:31:12 @agent_ppo2.py:179][0m |          -0.0403 |          14.7947 |           6.7584 |
[32m[20221213 12:31:12 @agent_ppo2.py:179][0m |          -0.0404 |          14.6154 |           6.7102 |
[32m[20221213 12:31:12 @agent_ppo2.py:179][0m |          -0.0420 |          14.5582 |           6.5719 |
[32m[20221213 12:31:12 @agent_ppo2.py:179][0m |          -0.0471 |          14.3323 |           6.4111 |
[32m[20221213 12:31:12 @agent_ppo2.py:179][0m |          -0.0500 |          14.1503 |           6.2569 |
[32m[20221213 12:31:12 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:31:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.51
[32m[20221213 12:31:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 275.23
[32m[20221213 12:31:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 193.44
[32m[20221213 12:31:12 @agent_ppo2.py:137][0m Total time:      15.26 min
[32m[20221213 12:31:12 @agent_ppo2.py:139][0m 1009664 total steps have happened
[32m[20221213 12:31:12 @agent_ppo2.py:115][0m #------------------------ Iteration 493 --------------------------#
[32m[20221213 12:31:12 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:13 @agent_ppo2.py:179][0m |           0.0859 |          21.0690 |           5.9912 |
[32m[20221213 12:31:13 @agent_ppo2.py:179][0m |           0.0609 |          20.4643 |           7.0823 |
[32m[20221213 12:31:13 @agent_ppo2.py:179][0m |           0.0117 |          19.9229 |           6.3646 |
[32m[20221213 12:31:13 @agent_ppo2.py:179][0m |          -0.0108 |          19.6721 |           5.9794 |
[32m[20221213 12:31:13 @agent_ppo2.py:179][0m |          -0.0167 |          20.0373 |           5.8294 |
[32m[20221213 12:31:13 @agent_ppo2.py:179][0m |          -0.0295 |          19.4097 |           5.5902 |
[32m[20221213 12:31:14 @agent_ppo2.py:179][0m |          -0.0324 |          19.2994 |           5.2532 |
[32m[20221213 12:31:14 @agent_ppo2.py:179][0m |          -0.0397 |          19.1905 |           4.7802 |
[32m[20221213 12:31:14 @agent_ppo2.py:179][0m |          -0.0386 |          19.1077 |           4.8146 |
[32m[20221213 12:31:14 @agent_ppo2.py:179][0m |          -0.0426 |          19.0770 |           4.5115 |
[32m[20221213 12:31:14 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:31:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.55
[32m[20221213 12:31:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 274.92
[32m[20221213 12:31:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.66
[32m[20221213 12:31:14 @agent_ppo2.py:137][0m Total time:      15.29 min
[32m[20221213 12:31:14 @agent_ppo2.py:139][0m 1011712 total steps have happened
[32m[20221213 12:31:14 @agent_ppo2.py:115][0m #------------------------ Iteration 494 --------------------------#
[32m[20221213 12:31:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:15 @agent_ppo2.py:179][0m |           0.0776 |          18.3185 |           5.8778 |
[32m[20221213 12:31:15 @agent_ppo2.py:179][0m |           0.1319 |          17.6409 |           6.9399 |
[32m[20221213 12:31:15 @agent_ppo2.py:179][0m |           0.0016 |          17.3171 |           6.0375 |
[32m[20221213 12:31:15 @agent_ppo2.py:179][0m |          -0.0153 |          17.1354 |           5.7363 |
[32m[20221213 12:31:15 @agent_ppo2.py:179][0m |          -0.0219 |          17.0724 |           5.3946 |
[32m[20221213 12:31:15 @agent_ppo2.py:179][0m |          -0.0331 |          16.8925 |           5.1718 |
[32m[20221213 12:31:15 @agent_ppo2.py:179][0m |          -0.0328 |          17.7410 |           4.8353 |
[32m[20221213 12:31:16 @agent_ppo2.py:179][0m |          -0.0433 |          16.7453 |           4.5554 |
[32m[20221213 12:31:16 @agent_ppo2.py:179][0m |          -0.0355 |          17.1460 |           4.2616 |
[32m[20221213 12:31:16 @agent_ppo2.py:179][0m |          -0.0472 |          16.5913 |           4.1432 |
[32m[20221213 12:31:16 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:31:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.74
[32m[20221213 12:31:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 265.41
[32m[20221213 12:31:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 304.50
[32m[20221213 12:31:16 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 304.50
[32m[20221213 12:31:16 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 304.50
[32m[20221213 12:31:16 @agent_ppo2.py:137][0m Total time:      15.32 min
[32m[20221213 12:31:16 @agent_ppo2.py:139][0m 1013760 total steps have happened
[32m[20221213 12:31:16 @agent_ppo2.py:115][0m #------------------------ Iteration 495 --------------------------#
[32m[20221213 12:31:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:16 @agent_ppo2.py:179][0m |           0.0528 |          21.7971 |           4.1134 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |           0.0487 |          20.9930 |           5.4783 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |          -0.0054 |          20.6919 |           5.3235 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |          -0.0187 |          20.5304 |           5.0589 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |          -0.0332 |          20.3960 |           4.9691 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |          -0.0343 |          20.3058 |           4.7985 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |          -0.0318 |          20.3414 |           4.5003 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |          -0.0420 |          20.2205 |           4.4013 |
[32m[20221213 12:31:17 @agent_ppo2.py:179][0m |          -0.0410 |          20.0274 |           4.0904 |
[32m[20221213 12:31:18 @agent_ppo2.py:179][0m |          -0.0436 |          20.0295 |           4.0126 |
[32m[20221213 12:31:18 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:31:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 242.58
[32m[20221213 12:31:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 266.64
[32m[20221213 12:31:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 265.99
[32m[20221213 12:31:18 @agent_ppo2.py:137][0m Total time:      15.35 min
[32m[20221213 12:31:18 @agent_ppo2.py:139][0m 1015808 total steps have happened
[32m[20221213 12:31:18 @agent_ppo2.py:115][0m #------------------------ Iteration 496 --------------------------#
[32m[20221213 12:31:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:18 @agent_ppo2.py:179][0m |           0.0939 |          21.6382 |           5.5323 |
[32m[20221213 12:31:18 @agent_ppo2.py:179][0m |           0.0499 |          20.8715 |           6.2565 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |           0.0145 |          20.4623 |           6.0564 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |          -0.0189 |          20.2532 |           5.5278 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |          -0.0283 |          19.9170 |           5.1216 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |          -0.0406 |          19.8194 |           5.1293 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |          -0.0359 |          19.6525 |           4.9937 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |          -0.0450 |          19.4009 |           4.9776 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |          -0.0444 |          19.3253 |           4.7245 |
[32m[20221213 12:31:19 @agent_ppo2.py:179][0m |          -0.0427 |          19.3349 |           4.7162 |
[32m[20221213 12:31:19 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:31:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.60
[32m[20221213 12:31:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 272.00
[32m[20221213 12:31:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.54
[32m[20221213 12:31:20 @agent_ppo2.py:137][0m Total time:      15.38 min
[32m[20221213 12:31:20 @agent_ppo2.py:139][0m 1017856 total steps have happened
[32m[20221213 12:31:20 @agent_ppo2.py:115][0m #------------------------ Iteration 497 --------------------------#
[32m[20221213 12:31:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:20 @agent_ppo2.py:179][0m |           0.0580 |          20.1123 |           4.4583 |
[32m[20221213 12:31:20 @agent_ppo2.py:179][0m |           0.0471 |          19.4587 |           5.5402 |
[32m[20221213 12:31:20 @agent_ppo2.py:179][0m |           0.0252 |          21.7136 |           5.8023 |
[32m[20221213 12:31:20 @agent_ppo2.py:179][0m |          -0.0140 |          18.9378 |           5.1438 |
[32m[20221213 12:31:21 @agent_ppo2.py:179][0m |          -0.0260 |          18.7727 |           5.1391 |
[32m[20221213 12:31:21 @agent_ppo2.py:179][0m |          -0.0244 |          20.1261 |           5.0197 |
[32m[20221213 12:31:21 @agent_ppo2.py:179][0m |          -0.0365 |          18.4461 |           4.7291 |
[32m[20221213 12:31:21 @agent_ppo2.py:179][0m |          -0.0445 |          18.2847 |           4.5267 |
[32m[20221213 12:31:21 @agent_ppo2.py:179][0m |          -0.0447 |          18.2882 |           4.3149 |
[32m[20221213 12:31:21 @agent_ppo2.py:179][0m |          -0.0368 |          19.5908 |           4.2033 |
[32m[20221213 12:31:21 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:31:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 264.43
[32m[20221213 12:31:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 281.46
[32m[20221213 12:31:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 285.59
[32m[20221213 12:31:21 @agent_ppo2.py:137][0m Total time:      15.41 min
[32m[20221213 12:31:21 @agent_ppo2.py:139][0m 1019904 total steps have happened
[32m[20221213 12:31:21 @agent_ppo2.py:115][0m #------------------------ Iteration 498 --------------------------#
[32m[20221213 12:31:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:22 @agent_ppo2.py:179][0m |           0.0589 |          20.4721 |           5.0845 |
[32m[20221213 12:31:22 @agent_ppo2.py:179][0m |           0.0279 |          20.1513 |           5.9596 |
[32m[20221213 12:31:22 @agent_ppo2.py:179][0m |          -0.0017 |          19.5677 |           5.6979 |
[32m[20221213 12:31:22 @agent_ppo2.py:179][0m |          -0.0181 |          19.3501 |           5.7749 |
[32m[20221213 12:31:22 @agent_ppo2.py:179][0m |          -0.0312 |          19.1084 |           5.3035 |
[32m[20221213 12:31:22 @agent_ppo2.py:179][0m |          -0.0261 |          20.5886 |           5.3607 |
[32m[20221213 12:31:23 @agent_ppo2.py:179][0m |          -0.0374 |          18.9269 |           5.0114 |
[32m[20221213 12:31:23 @agent_ppo2.py:179][0m |          -0.0368 |          19.2767 |           4.9066 |
[32m[20221213 12:31:23 @agent_ppo2.py:179][0m |          -0.0371 |          19.1700 |           4.7829 |
[32m[20221213 12:31:23 @agent_ppo2.py:179][0m |          -0.0426 |          18.6187 |           4.7647 |
[32m[20221213 12:31:23 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:31:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 268.22
[32m[20221213 12:31:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 282.81
[32m[20221213 12:31:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 273.86
[32m[20221213 12:31:23 @agent_ppo2.py:137][0m Total time:      15.44 min
[32m[20221213 12:31:23 @agent_ppo2.py:139][0m 1021952 total steps have happened
[32m[20221213 12:31:23 @agent_ppo2.py:115][0m #------------------------ Iteration 499 --------------------------#
[32m[20221213 12:31:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |           0.0581 |          20.1613 |           5.1834 |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |           0.0268 |          19.5158 |           5.7880 |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |          -0.0021 |          19.3130 |           5.5215 |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |          -0.0088 |          19.1701 |           5.3212 |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |          -0.0243 |          19.0013 |           5.3461 |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |          -0.0376 |          18.9571 |           5.1030 |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |          -0.0355 |          19.6944 |           4.7130 |
[32m[20221213 12:31:24 @agent_ppo2.py:179][0m |          -0.0439 |          18.8151 |           4.7759 |
[32m[20221213 12:31:25 @agent_ppo2.py:179][0m |          -0.0359 |          19.3131 |           4.8293 |
[32m[20221213 12:31:25 @agent_ppo2.py:179][0m |          -0.0349 |          20.1638 |           4.4320 |
[32m[20221213 12:31:25 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:31:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.98
[32m[20221213 12:31:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 279.33
[32m[20221213 12:31:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 288.83
[32m[20221213 12:31:25 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 304.50
[32m[20221213 12:31:25 @agent_ppo2.py:137][0m Total time:      15.47 min
[32m[20221213 12:31:25 @agent_ppo2.py:139][0m 1024000 total steps have happened
[32m[20221213 12:31:25 @agent_ppo2.py:115][0m #------------------------ Iteration 500 --------------------------#
[32m[20221213 12:31:25 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:31:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:25 @agent_ppo2.py:179][0m |           0.0536 |          19.4507 |           5.9851 |
[32m[20221213 12:31:26 @agent_ppo2.py:179][0m |           0.0937 |          18.8929 |           6.8034 |
[32m[20221213 12:31:26 @agent_ppo2.py:179][0m |           0.0126 |          18.6342 |           6.6851 |
[32m[20221213 12:31:26 @agent_ppo2.py:179][0m |          -0.0121 |          18.3912 |           6.6543 |
[32m[20221213 12:31:26 @agent_ppo2.py:179][0m |          -0.0272 |          18.2727 |           6.3822 |
[32m[20221213 12:31:26 @agent_ppo2.py:179][0m |          -0.0278 |          18.5034 |           6.2037 |
[32m[20221213 12:31:26 @agent_ppo2.py:179][0m |          -0.0317 |          18.2838 |           5.9159 |
[32m[20221213 12:31:26 @agent_ppo2.py:179][0m |          -0.0373 |          17.9399 |           6.1039 |
[32m[20221213 12:31:27 @agent_ppo2.py:179][0m |          -0.0336 |          17.9079 |           5.7785 |
[32m[20221213 12:31:27 @agent_ppo2.py:179][0m |          -0.0397 |          18.2252 |           5.8597 |
[32m[20221213 12:31:27 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:31:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 256.64
[32m[20221213 12:31:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 267.13
[32m[20221213 12:31:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 279.69
[32m[20221213 12:31:27 @agent_ppo2.py:137][0m Total time:      15.50 min
[32m[20221213 12:31:27 @agent_ppo2.py:139][0m 1026048 total steps have happened
[32m[20221213 12:31:27 @agent_ppo2.py:115][0m #------------------------ Iteration 501 --------------------------#
[32m[20221213 12:31:27 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:27 @agent_ppo2.py:179][0m |           0.0483 |          19.1664 |           6.1688 |
[32m[20221213 12:31:27 @agent_ppo2.py:179][0m |           0.0178 |          18.1270 |           6.6587 |
[32m[20221213 12:31:28 @agent_ppo2.py:179][0m |          -0.0120 |          17.6563 |           6.2736 |
[32m[20221213 12:31:28 @agent_ppo2.py:179][0m |          -0.0324 |          17.3078 |           5.9591 |
[32m[20221213 12:31:28 @agent_ppo2.py:179][0m |          -0.0364 |          17.0903 |           5.9151 |
[32m[20221213 12:31:28 @agent_ppo2.py:179][0m |          -0.0422 |          16.9612 |           5.6637 |
[32m[20221213 12:31:28 @agent_ppo2.py:179][0m |          -0.0428 |          16.8272 |           5.5202 |
[32m[20221213 12:31:28 @agent_ppo2.py:179][0m |          -0.0439 |          16.7409 |           5.5826 |
[32m[20221213 12:31:28 @agent_ppo2.py:179][0m |          -0.0480 |          16.5753 |           5.2027 |
[32m[20221213 12:31:29 @agent_ppo2.py:179][0m |          -0.0498 |          16.4832 |           5.1632 |
[32m[20221213 12:31:29 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:31:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.43
[32m[20221213 12:31:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 281.96
[32m[20221213 12:31:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 252.43
[32m[20221213 12:31:29 @agent_ppo2.py:137][0m Total time:      15.53 min
[32m[20221213 12:31:29 @agent_ppo2.py:139][0m 1028096 total steps have happened
[32m[20221213 12:31:29 @agent_ppo2.py:115][0m #------------------------ Iteration 502 --------------------------#
[32m[20221213 12:31:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:29 @agent_ppo2.py:179][0m |           0.0532 |          20.7695 |           6.0800 |
[32m[20221213 12:31:29 @agent_ppo2.py:179][0m |           0.0290 |          21.1427 |           5.1281 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |           0.0312 |          19.7124 |           6.4699 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |          -0.0096 |          19.5394 |           5.8047 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |          -0.0336 |          19.4181 |           5.7038 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |          -0.0359 |          19.2587 |           5.6002 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |          -0.0404 |          19.1084 |           5.4089 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |          -0.0422 |          19.0221 |           5.3622 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |          -0.0470 |          18.9379 |           5.2420 |
[32m[20221213 12:31:30 @agent_ppo2.py:179][0m |          -0.0435 |          18.8373 |           4.9701 |
[32m[20221213 12:31:30 @agent_ppo2.py:124][0m Policy update time: 1.49 s
[32m[20221213 12:31:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 265.98
[32m[20221213 12:31:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 277.81
[32m[20221213 12:31:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 250.20
[32m[20221213 12:31:31 @agent_ppo2.py:137][0m Total time:      15.57 min
[32m[20221213 12:31:31 @agent_ppo2.py:139][0m 1030144 total steps have happened
[32m[20221213 12:31:31 @agent_ppo2.py:115][0m #------------------------ Iteration 503 --------------------------#
[32m[20221213 12:31:31 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:31 @agent_ppo2.py:179][0m |           0.0459 |          21.5830 |           5.1659 |
[32m[20221213 12:31:31 @agent_ppo2.py:179][0m |           0.0115 |          18.9236 |           5.5269 |
[32m[20221213 12:31:31 @agent_ppo2.py:179][0m |          -0.0093 |          18.3903 |           5.4625 |
[32m[20221213 12:31:32 @agent_ppo2.py:179][0m |          -0.0299 |          18.0331 |           5.2888 |
[32m[20221213 12:31:32 @agent_ppo2.py:179][0m |          -0.0388 |          17.7924 |           5.1141 |
[32m[20221213 12:31:32 @agent_ppo2.py:179][0m |          -0.0398 |          17.8535 |           4.9961 |
[32m[20221213 12:31:32 @agent_ppo2.py:179][0m |          -0.0466 |          17.2315 |           4.7641 |
[32m[20221213 12:31:32 @agent_ppo2.py:179][0m |          -0.0497 |          17.1664 |           4.7610 |
[32m[20221213 12:31:32 @agent_ppo2.py:179][0m |          -0.0530 |          17.0219 |           4.4008 |
[32m[20221213 12:31:32 @agent_ppo2.py:179][0m |          -0.0563 |          16.8565 |           4.3415 |
[32m[20221213 12:31:32 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:31:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.47
[32m[20221213 12:31:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 261.15
[32m[20221213 12:31:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 265.74
[32m[20221213 12:31:33 @agent_ppo2.py:137][0m Total time:      15.60 min
[32m[20221213 12:31:33 @agent_ppo2.py:139][0m 1032192 total steps have happened
[32m[20221213 12:31:33 @agent_ppo2.py:115][0m #------------------------ Iteration 504 --------------------------#
[32m[20221213 12:31:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:33 @agent_ppo2.py:179][0m |           0.0714 |          21.0086 |           6.1672 |
[32m[20221213 12:31:33 @agent_ppo2.py:179][0m |           0.0352 |          19.8406 |           6.3914 |
[32m[20221213 12:31:33 @agent_ppo2.py:179][0m |           0.0074 |          19.3142 |           5.7228 |
[32m[20221213 12:31:33 @agent_ppo2.py:179][0m |          -0.0187 |          19.1602 |           5.9187 |
[32m[20221213 12:31:34 @agent_ppo2.py:179][0m |          -0.0253 |          18.9892 |           5.5055 |
[32m[20221213 12:31:34 @agent_ppo2.py:179][0m |          -0.0347 |          18.8266 |           5.5274 |
[32m[20221213 12:31:34 @agent_ppo2.py:179][0m |          -0.0281 |          18.8937 |           5.4957 |
[32m[20221213 12:31:34 @agent_ppo2.py:179][0m |          -0.0185 |          18.8191 |           5.6451 |
[32m[20221213 12:31:34 @agent_ppo2.py:179][0m |          -0.0380 |          18.5599 |           5.2520 |
[32m[20221213 12:31:34 @agent_ppo2.py:179][0m |          -0.0370 |          18.4212 |           5.2123 |
[32m[20221213 12:31:34 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:31:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 262.06
[32m[20221213 12:31:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 279.39
[32m[20221213 12:31:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 267.63
[32m[20221213 12:31:34 @agent_ppo2.py:137][0m Total time:      15.63 min
[32m[20221213 12:31:34 @agent_ppo2.py:139][0m 1034240 total steps have happened
[32m[20221213 12:31:34 @agent_ppo2.py:115][0m #------------------------ Iteration 505 --------------------------#
[32m[20221213 12:31:35 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:31:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:35 @agent_ppo2.py:179][0m |           0.0650 |          21.2658 |           5.0826 |
[32m[20221213 12:31:35 @agent_ppo2.py:179][0m |           0.0496 |          20.5318 |           5.2854 |
[32m[20221213 12:31:35 @agent_ppo2.py:179][0m |          -0.0057 |          20.3379 |           5.2499 |
[32m[20221213 12:31:36 @agent_ppo2.py:179][0m |          -0.0269 |          20.1023 |           4.8861 |
[32m[20221213 12:31:36 @agent_ppo2.py:179][0m |          -0.0230 |          20.6203 |           4.9303 |
[32m[20221213 12:31:36 @agent_ppo2.py:179][0m |          -0.0197 |          19.9170 |           4.7809 |
[32m[20221213 12:31:36 @agent_ppo2.py:179][0m |          -0.0382 |          19.7834 |           4.6807 |
[32m[20221213 12:31:36 @agent_ppo2.py:179][0m |          -0.0376 |          19.7621 |           4.1923 |
[32m[20221213 12:31:36 @agent_ppo2.py:179][0m |          -0.0424 |          19.7409 |           4.1535 |
[32m[20221213 12:31:36 @agent_ppo2.py:179][0m |          -0.0439 |          19.6548 |           3.9338 |
[32m[20221213 12:31:36 @agent_ppo2.py:124][0m Policy update time: 1.73 s
[32m[20221213 12:31:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 231.69
[32m[20221213 12:31:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 272.12
[32m[20221213 12:31:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 255.22
[32m[20221213 12:31:37 @agent_ppo2.py:137][0m Total time:      15.67 min
[32m[20221213 12:31:37 @agent_ppo2.py:139][0m 1036288 total steps have happened
[32m[20221213 12:31:37 @agent_ppo2.py:115][0m #------------------------ Iteration 506 --------------------------#
[32m[20221213 12:31:37 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:31:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:37 @agent_ppo2.py:179][0m |           0.0501 |          18.9078 |           4.4816 |
[32m[20221213 12:31:37 @agent_ppo2.py:179][0m |           0.0139 |          18.3657 |           4.6029 |
[32m[20221213 12:31:37 @agent_ppo2.py:179][0m |          -0.0115 |          18.1931 |           4.4752 |
[32m[20221213 12:31:38 @agent_ppo2.py:179][0m |          -0.0182 |          17.9983 |           4.2201 |
[32m[20221213 12:31:38 @agent_ppo2.py:179][0m |          -0.0320 |          17.8519 |           4.1874 |
[32m[20221213 12:31:38 @agent_ppo2.py:179][0m |          -0.0337 |          18.0730 |           3.9276 |
[32m[20221213 12:31:38 @agent_ppo2.py:179][0m |          -0.0349 |          17.7759 |           3.8169 |
[32m[20221213 12:31:38 @agent_ppo2.py:179][0m |          -0.0386 |          17.6373 |           3.7158 |
[32m[20221213 12:31:38 @agent_ppo2.py:179][0m |          -0.0407 |          17.5475 |           3.7144 |
[32m[20221213 12:31:38 @agent_ppo2.py:179][0m |          -0.0393 |          18.8092 |           3.4501 |
[32m[20221213 12:31:38 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:31:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.33
[32m[20221213 12:31:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 253.07
[32m[20221213 12:31:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 265.52
[32m[20221213 12:31:39 @agent_ppo2.py:137][0m Total time:      15.70 min
[32m[20221213 12:31:39 @agent_ppo2.py:139][0m 1038336 total steps have happened
[32m[20221213 12:31:39 @agent_ppo2.py:115][0m #------------------------ Iteration 507 --------------------------#
[32m[20221213 12:31:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:39 @agent_ppo2.py:179][0m |           0.0543 |          20.1917 |           4.4220 |
[32m[20221213 12:31:39 @agent_ppo2.py:179][0m |           0.0298 |          19.7577 |           4.6627 |
[32m[20221213 12:31:39 @agent_ppo2.py:179][0m |           0.0064 |          19.4193 |           5.1330 |
[32m[20221213 12:31:39 @agent_ppo2.py:179][0m |          -0.0092 |          20.3332 |           4.9881 |
[32m[20221213 12:31:39 @agent_ppo2.py:179][0m |          -0.0299 |          19.1312 |           4.6637 |
[32m[20221213 12:31:40 @agent_ppo2.py:179][0m |          -0.0296 |          18.9992 |           4.9201 |
[32m[20221213 12:31:40 @agent_ppo2.py:179][0m |          -0.0325 |          18.9261 |           4.6332 |
[32m[20221213 12:31:40 @agent_ppo2.py:179][0m |          -0.0326 |          18.8486 |           4.5113 |
[32m[20221213 12:31:40 @agent_ppo2.py:179][0m |          -0.0415 |          18.8078 |           4.2028 |
[32m[20221213 12:31:40 @agent_ppo2.py:179][0m |          -0.0403 |          18.6697 |           4.2290 |
[32m[20221213 12:31:40 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:31:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 245.29
[32m[20221213 12:31:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.54
[32m[20221213 12:31:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 241.34
[32m[20221213 12:31:40 @agent_ppo2.py:137][0m Total time:      15.73 min
[32m[20221213 12:31:40 @agent_ppo2.py:139][0m 1040384 total steps have happened
[32m[20221213 12:31:40 @agent_ppo2.py:115][0m #------------------------ Iteration 508 --------------------------#
[32m[20221213 12:31:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:41 @agent_ppo2.py:179][0m |           0.0540 |          19.8123 |           5.7625 |
[32m[20221213 12:31:41 @agent_ppo2.py:179][0m |           0.0583 |          20.0805 |           6.4123 |
[32m[20221213 12:31:41 @agent_ppo2.py:179][0m |           0.0052 |          18.4767 |           6.1226 |
[32m[20221213 12:31:41 @agent_ppo2.py:179][0m |          -0.0167 |          18.0771 |           5.8123 |
[32m[20221213 12:31:41 @agent_ppo2.py:179][0m |          -0.0271 |          17.8001 |           5.4331 |
[32m[20221213 12:31:41 @agent_ppo2.py:179][0m |          -0.0310 |          17.6337 |           5.3310 |
[32m[20221213 12:31:42 @agent_ppo2.py:179][0m |          -0.0318 |          17.8094 |           5.1240 |
[32m[20221213 12:31:42 @agent_ppo2.py:179][0m |          -0.0374 |          17.3629 |           4.9458 |
[32m[20221213 12:31:42 @agent_ppo2.py:179][0m |          -0.0413 |          17.2766 |           4.7788 |
[32m[20221213 12:31:42 @agent_ppo2.py:179][0m |          -0.0346 |          17.1254 |           4.7978 |
[32m[20221213 12:31:42 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:31:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.07
[32m[20221213 12:31:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 279.34
[32m[20221213 12:31:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 276.68
[32m[20221213 12:31:42 @agent_ppo2.py:137][0m Total time:      15.76 min
[32m[20221213 12:31:42 @agent_ppo2.py:139][0m 1042432 total steps have happened
[32m[20221213 12:31:42 @agent_ppo2.py:115][0m #------------------------ Iteration 509 --------------------------#
[32m[20221213 12:31:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |           0.0634 |          20.1835 |           5.6224 |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |           0.0415 |          19.3336 |           6.5258 |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |          -0.0064 |          19.0752 |           5.6445 |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |          -0.0180 |          18.8924 |           5.4128 |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |          -0.0297 |          18.7508 |           5.1847 |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |          -0.0278 |          20.5017 |           4.9754 |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |          -0.0424 |          18.6008 |           4.6649 |
[32m[20221213 12:31:43 @agent_ppo2.py:179][0m |          -0.0406 |          18.5120 |           4.4919 |
[32m[20221213 12:31:44 @agent_ppo2.py:179][0m |          -0.0417 |          18.4261 |           4.3132 |
[32m[20221213 12:31:44 @agent_ppo2.py:179][0m |          -0.0426 |          18.4370 |           4.0311 |
[32m[20221213 12:31:44 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:31:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.82
[32m[20221213 12:31:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 267.46
[32m[20221213 12:31:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.41
[32m[20221213 12:31:44 @agent_ppo2.py:137][0m Total time:      15.79 min
[32m[20221213 12:31:44 @agent_ppo2.py:139][0m 1044480 total steps have happened
[32m[20221213 12:31:44 @agent_ppo2.py:115][0m #------------------------ Iteration 510 --------------------------#
[32m[20221213 12:31:44 @agent_ppo2.py:121][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 12:31:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:44 @agent_ppo2.py:179][0m |           0.0542 |          19.8647 |           5.2010 |
[32m[20221213 12:31:45 @agent_ppo2.py:179][0m |           0.0420 |          19.1692 |           5.9489 |
[32m[20221213 12:31:45 @agent_ppo2.py:179][0m |          -0.0019 |          18.8743 |           5.6133 |
[32m[20221213 12:31:45 @agent_ppo2.py:179][0m |          -0.0138 |          18.7791 |           5.5280 |
[32m[20221213 12:31:45 @agent_ppo2.py:179][0m |          -0.0138 |          19.9137 |           5.2667 |
[32m[20221213 12:31:45 @agent_ppo2.py:179][0m |          -0.0040 |          19.6346 |           5.5290 |
[32m[20221213 12:31:45 @agent_ppo2.py:179][0m |          -0.0304 |          18.2787 |           5.2462 |
[32m[20221213 12:31:45 @agent_ppo2.py:179][0m |          -0.0379 |          18.1641 |           5.0371 |
[32m[20221213 12:31:46 @agent_ppo2.py:179][0m |          -0.0386 |          18.1059 |           4.7697 |
[32m[20221213 12:31:46 @agent_ppo2.py:179][0m |          -0.0420 |          17.9669 |           4.5766 |
[32m[20221213 12:31:46 @agent_ppo2.py:124][0m Policy update time: 1.55 s
[32m[20221213 12:31:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.70
[32m[20221213 12:31:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 261.57
[32m[20221213 12:31:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 269.69
[32m[20221213 12:31:46 @agent_ppo2.py:137][0m Total time:      15.82 min
[32m[20221213 12:31:46 @agent_ppo2.py:139][0m 1046528 total steps have happened
[32m[20221213 12:31:46 @agent_ppo2.py:115][0m #------------------------ Iteration 511 --------------------------#
[32m[20221213 12:31:46 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:46 @agent_ppo2.py:179][0m |           0.0558 |          19.6908 |           5.4217 |
[32m[20221213 12:31:47 @agent_ppo2.py:179][0m |           0.0441 |          19.3242 |           6.2366 |
[32m[20221213 12:31:47 @agent_ppo2.py:179][0m |           0.0036 |          19.0629 |           6.2277 |
[32m[20221213 12:31:47 @agent_ppo2.py:179][0m |          -0.0061 |          19.9931 |           6.2335 |
[32m[20221213 12:31:47 @agent_ppo2.py:179][0m |          -0.0251 |          18.8609 |           5.8360 |
[32m[20221213 12:31:47 @agent_ppo2.py:179][0m |          -0.0330 |          18.7936 |           5.5047 |
[32m[20221213 12:31:47 @agent_ppo2.py:179][0m |          -0.0343 |          18.7116 |           5.4017 |
[32m[20221213 12:31:47 @agent_ppo2.py:179][0m |          -0.0341 |          18.8959 |           5.0913 |
[32m[20221213 12:31:48 @agent_ppo2.py:179][0m |          -0.0404 |          18.6513 |           4.7764 |
[32m[20221213 12:31:48 @agent_ppo2.py:179][0m |          -0.0403 |          18.5722 |           4.7327 |
[32m[20221213 12:31:48 @agent_ppo2.py:124][0m Policy update time: 1.54 s
[32m[20221213 12:31:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 242.10
[32m[20221213 12:31:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 265.00
[32m[20221213 12:31:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 278.71
[32m[20221213 12:31:48 @agent_ppo2.py:137][0m Total time:      15.85 min
[32m[20221213 12:31:48 @agent_ppo2.py:139][0m 1048576 total steps have happened
[32m[20221213 12:31:48 @agent_ppo2.py:115][0m #------------------------ Iteration 512 --------------------------#
[32m[20221213 12:31:48 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:48 @agent_ppo2.py:179][0m |           0.0645 |          19.1612 |           5.9095 |
[32m[20221213 12:31:49 @agent_ppo2.py:179][0m |           0.0125 |          18.4832 |           5.9705 |
[32m[20221213 12:31:49 @agent_ppo2.py:179][0m |          -0.0070 |          18.3317 |           5.6017 |
[32m[20221213 12:31:49 @agent_ppo2.py:179][0m |          -0.0245 |          18.2203 |           5.5666 |
[32m[20221213 12:31:49 @agent_ppo2.py:179][0m |          -0.0266 |          18.0676 |           5.5027 |
[32m[20221213 12:31:49 @agent_ppo2.py:179][0m |          -0.0323 |          17.9723 |           5.1675 |
[32m[20221213 12:31:49 @agent_ppo2.py:179][0m |          -0.0407 |          17.8314 |           5.2038 |
[32m[20221213 12:31:49 @agent_ppo2.py:179][0m |          -0.0402 |          17.8407 |           4.7003 |
[32m[20221213 12:31:50 @agent_ppo2.py:179][0m |          -0.0422 |          17.8692 |           4.7656 |
[32m[20221213 12:31:50 @agent_ppo2.py:179][0m |          -0.0463 |          17.6119 |           4.2744 |
[32m[20221213 12:31:50 @agent_ppo2.py:124][0m Policy update time: 1.51 s
[32m[20221213 12:31:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.27
[32m[20221213 12:31:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 244.27
[32m[20221213 12:31:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 258.88
[32m[20221213 12:31:50 @agent_ppo2.py:137][0m Total time:      15.89 min
[32m[20221213 12:31:50 @agent_ppo2.py:139][0m 1050624 total steps have happened
[32m[20221213 12:31:50 @agent_ppo2.py:115][0m #------------------------ Iteration 513 --------------------------#
[32m[20221213 12:31:50 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:50 @agent_ppo2.py:179][0m |           0.0527 |          19.7791 |           4.8380 |
[32m[20221213 12:31:50 @agent_ppo2.py:179][0m |           0.0602 |          19.1280 |           5.2777 |
[32m[20221213 12:31:51 @agent_ppo2.py:179][0m |           0.0583 |          19.0273 |           5.1733 |
[32m[20221213 12:31:51 @agent_ppo2.py:179][0m |           0.0059 |          18.9614 |           5.3159 |
[32m[20221213 12:31:51 @agent_ppo2.py:179][0m |          -0.0038 |          18.9725 |           4.9808 |
[32m[20221213 12:31:51 @agent_ppo2.py:179][0m |          -0.0157 |          18.9115 |           4.9299 |
[32m[20221213 12:31:51 @agent_ppo2.py:179][0m |          -0.0284 |          18.8723 |           4.3832 |
[32m[20221213 12:31:51 @agent_ppo2.py:179][0m |          -0.0204 |          20.6652 |           4.2615 |
[32m[20221213 12:31:52 @agent_ppo2.py:179][0m |          -0.0269 |          19.2644 |           4.1099 |
[32m[20221213 12:31:52 @agent_ppo2.py:179][0m |          -0.0381 |          18.6993 |           3.9702 |
[32m[20221213 12:31:52 @agent_ppo2.py:124][0m Policy update time: 1.59 s
[32m[20221213 12:31:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.65
[32m[20221213 12:31:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 270.15
[32m[20221213 12:31:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 290.99
[32m[20221213 12:31:52 @agent_ppo2.py:137][0m Total time:      15.92 min
[32m[20221213 12:31:52 @agent_ppo2.py:139][0m 1052672 total steps have happened
[32m[20221213 12:31:52 @agent_ppo2.py:115][0m #------------------------ Iteration 514 --------------------------#
[32m[20221213 12:31:52 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:31:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:52 @agent_ppo2.py:179][0m |           0.0679 |          20.1086 |           5.1061 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |           0.0337 |          19.4220 |           5.2707 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |          -0.0050 |          19.0120 |           5.0772 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |          -0.0218 |          18.7917 |           4.5464 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |          -0.0296 |          18.6051 |           4.3335 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |          -0.0337 |          18.4904 |           4.0818 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |          -0.0382 |          18.3355 |           3.6560 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |          -0.0345 |          18.3039 |           3.7093 |
[32m[20221213 12:31:53 @agent_ppo2.py:179][0m |          -0.0405 |          18.1607 |           3.4020 |
[32m[20221213 12:31:54 @agent_ppo2.py:179][0m |          -0.0433 |          18.0833 |           3.1767 |
[32m[20221213 12:31:54 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:31:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.11
[32m[20221213 12:31:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 281.04
[32m[20221213 12:31:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 214.59
[32m[20221213 12:31:54 @agent_ppo2.py:137][0m Total time:      15.95 min
[32m[20221213 12:31:54 @agent_ppo2.py:139][0m 1054720 total steps have happened
[32m[20221213 12:31:54 @agent_ppo2.py:115][0m #------------------------ Iteration 515 --------------------------#
[32m[20221213 12:31:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:54 @agent_ppo2.py:179][0m |           0.0573 |          19.3347 |           3.5351 |
[32m[20221213 12:31:54 @agent_ppo2.py:179][0m |           0.0289 |          18.8431 |           4.6125 |
[32m[20221213 12:31:54 @agent_ppo2.py:179][0m |          -0.0005 |          18.9548 |           3.6361 |
[32m[20221213 12:31:55 @agent_ppo2.py:179][0m |          -0.0159 |          18.4769 |           3.7958 |
[32m[20221213 12:31:55 @agent_ppo2.py:179][0m |          -0.0243 |          18.3502 |           3.5204 |
[32m[20221213 12:31:55 @agent_ppo2.py:179][0m |          -0.0282 |          18.2473 |           3.3863 |
[32m[20221213 12:31:55 @agent_ppo2.py:179][0m |          -0.0341 |          18.2067 |           2.9834 |
[32m[20221213 12:31:55 @agent_ppo2.py:179][0m |          -0.0311 |          18.2372 |           2.7764 |
[32m[20221213 12:31:55 @agent_ppo2.py:179][0m |          -0.0368 |          17.9435 |           2.4092 |
[32m[20221213 12:31:55 @agent_ppo2.py:179][0m |          -0.0280 |          17.9427 |           2.3867 |
[32m[20221213 12:31:55 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:31:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 268.83
[32m[20221213 12:31:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 284.17
[32m[20221213 12:31:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.64
[32m[20221213 12:31:56 @agent_ppo2.py:137][0m Total time:      15.98 min
[32m[20221213 12:31:56 @agent_ppo2.py:139][0m 1056768 total steps have happened
[32m[20221213 12:31:56 @agent_ppo2.py:115][0m #------------------------ Iteration 516 --------------------------#
[32m[20221213 12:31:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:56 @agent_ppo2.py:179][0m |           0.0744 |          18.7475 |           3.7799 |
[32m[20221213 12:31:56 @agent_ppo2.py:179][0m |           0.0519 |          17.7997 |           4.1425 |
[32m[20221213 12:31:56 @agent_ppo2.py:179][0m |           0.0056 |          17.4467 |           4.4006 |
[32m[20221213 12:31:56 @agent_ppo2.py:179][0m |          -0.0176 |          17.2037 |           3.6576 |
[32m[20221213 12:31:56 @agent_ppo2.py:179][0m |          -0.0314 |          16.8666 |           3.5965 |
[32m[20221213 12:31:57 @agent_ppo2.py:179][0m |          -0.0362 |          16.6974 |           3.2661 |
[32m[20221213 12:31:57 @agent_ppo2.py:179][0m |          -0.0386 |          16.5633 |           3.0675 |
[32m[20221213 12:31:57 @agent_ppo2.py:179][0m |          -0.0362 |          16.4046 |           3.1196 |
[32m[20221213 12:31:57 @agent_ppo2.py:179][0m |          -0.0384 |          16.1811 |           2.8642 |
[32m[20221213 12:31:57 @agent_ppo2.py:179][0m |          -0.0429 |          16.0349 |           2.6399 |
[32m[20221213 12:31:57 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:31:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.27
[32m[20221213 12:31:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 262.31
[32m[20221213 12:31:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 273.14
[32m[20221213 12:31:57 @agent_ppo2.py:137][0m Total time:      16.01 min
[32m[20221213 12:31:57 @agent_ppo2.py:139][0m 1058816 total steps have happened
[32m[20221213 12:31:57 @agent_ppo2.py:115][0m #------------------------ Iteration 517 --------------------------#
[32m[20221213 12:31:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:31:58 @agent_ppo2.py:179][0m |           0.0772 |          18.4278 |           4.2979 |
[32m[20221213 12:31:58 @agent_ppo2.py:179][0m |           0.0411 |          17.8459 |           5.6866 |
[32m[20221213 12:31:58 @agent_ppo2.py:179][0m |           0.0057 |          17.4036 |           4.2807 |
[32m[20221213 12:31:58 @agent_ppo2.py:179][0m |          -0.0160 |          17.0419 |           3.8657 |
[32m[20221213 12:31:58 @agent_ppo2.py:179][0m |          -0.0177 |          16.6946 |           3.8348 |
[32m[20221213 12:31:58 @agent_ppo2.py:179][0m |          -0.0234 |          16.5359 |           3.4660 |
[32m[20221213 12:31:59 @agent_ppo2.py:179][0m |          -0.0282 |          16.3463 |           3.0632 |
[32m[20221213 12:31:59 @agent_ppo2.py:179][0m |          -0.0347 |          16.2529 |           2.9547 |
[32m[20221213 12:31:59 @agent_ppo2.py:179][0m |          -0.0362 |          16.0912 |           2.5952 |
[32m[20221213 12:31:59 @agent_ppo2.py:179][0m |          -0.0349 |          16.5689 |           2.2683 |
[32m[20221213 12:31:59 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:31:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 269.32
[32m[20221213 12:31:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 274.69
[32m[20221213 12:31:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 281.29
[32m[20221213 12:31:59 @agent_ppo2.py:137][0m Total time:      16.04 min
[32m[20221213 12:31:59 @agent_ppo2.py:139][0m 1060864 total steps have happened
[32m[20221213 12:31:59 @agent_ppo2.py:115][0m #------------------------ Iteration 518 --------------------------#
[32m[20221213 12:31:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:31:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |           0.0841 |          19.8054 |           4.8378 |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |           0.0468 |          19.1775 |           4.7977 |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |           0.0082 |          18.8381 |           4.3305 |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |          -0.0088 |          18.6431 |           3.8989 |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |          -0.0164 |          18.5765 |           3.4053 |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |          -0.0137 |          20.5971 |           3.4275 |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |          -0.0312 |          18.4561 |           3.1265 |
[32m[20221213 12:32:00 @agent_ppo2.py:179][0m |          -0.0333 |          18.0863 |           2.9862 |
[32m[20221213 12:32:01 @agent_ppo2.py:179][0m |          -0.0335 |          18.0631 |           2.6885 |
[32m[20221213 12:32:01 @agent_ppo2.py:179][0m |          -0.0342 |          17.9119 |           2.7155 |
[32m[20221213 12:32:01 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:32:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.92
[32m[20221213 12:32:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.09
[32m[20221213 12:32:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.07
[32m[20221213 12:32:01 @agent_ppo2.py:137][0m Total time:      16.07 min
[32m[20221213 12:32:01 @agent_ppo2.py:139][0m 1062912 total steps have happened
[32m[20221213 12:32:01 @agent_ppo2.py:115][0m #------------------------ Iteration 519 --------------------------#
[32m[20221213 12:32:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:01 @agent_ppo2.py:179][0m |           0.0472 |          21.6087 |           2.7902 |
[32m[20221213 12:32:01 @agent_ppo2.py:179][0m |           0.0264 |          20.6239 |           3.9728 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0002 |          20.2690 |           2.9656 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0256 |          20.1434 |           2.3543 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0334 |          20.0383 |           2.5054 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0333 |          19.9242 |           2.3708 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0325 |          19.8878 |           2.3572 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0398 |          19.7962 |           2.6000 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0394 |          19.6918 |           1.9152 |
[32m[20221213 12:32:02 @agent_ppo2.py:179][0m |          -0.0407 |          19.6349 |           1.9842 |
[32m[20221213 12:32:02 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:32:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.42
[32m[20221213 12:32:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.91
[32m[20221213 12:32:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.09
[32m[20221213 12:32:03 @agent_ppo2.py:137][0m Total time:      16.10 min
[32m[20221213 12:32:03 @agent_ppo2.py:139][0m 1064960 total steps have happened
[32m[20221213 12:32:03 @agent_ppo2.py:115][0m #------------------------ Iteration 520 --------------------------#
[32m[20221213 12:32:03 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:32:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:03 @agent_ppo2.py:179][0m |           0.0615 |          21.5541 |           4.1147 |
[32m[20221213 12:32:03 @agent_ppo2.py:179][0m |           0.0484 |          19.8481 |           4.4265 |
[32m[20221213 12:32:03 @agent_ppo2.py:179][0m |          -0.0036 |          19.7871 |           3.6492 |
[32m[20221213 12:32:03 @agent_ppo2.py:179][0m |          -0.0255 |          18.9567 |           3.5254 |
[32m[20221213 12:32:04 @agent_ppo2.py:179][0m |          -0.0330 |          18.9048 |           3.0493 |
[32m[20221213 12:32:04 @agent_ppo2.py:179][0m |          -0.0287 |          18.3933 |           2.9510 |
[32m[20221213 12:32:04 @agent_ppo2.py:179][0m |          -0.0444 |          18.1821 |           2.7137 |
[32m[20221213 12:32:04 @agent_ppo2.py:179][0m |          -0.0492 |          18.0952 |           2.5540 |
[32m[20221213 12:32:04 @agent_ppo2.py:179][0m |          -0.0518 |          17.8702 |           2.2546 |
[32m[20221213 12:32:04 @agent_ppo2.py:179][0m |          -0.0634 |          17.7553 |           1.8514 |
[32m[20221213 12:32:04 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:32:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 222.12
[32m[20221213 12:32:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 234.06
[32m[20221213 12:32:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 231.42
[32m[20221213 12:32:04 @agent_ppo2.py:137][0m Total time:      16.13 min
[32m[20221213 12:32:04 @agent_ppo2.py:139][0m 1067008 total steps have happened
[32m[20221213 12:32:04 @agent_ppo2.py:115][0m #------------------------ Iteration 521 --------------------------#
[32m[20221213 12:32:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:05 @agent_ppo2.py:179][0m |           0.0603 |          19.6049 |           1.2734 |
[32m[20221213 12:32:05 @agent_ppo2.py:179][0m |           0.0637 |          18.1721 |           2.3740 |
[32m[20221213 12:32:05 @agent_ppo2.py:179][0m |           0.0210 |          18.5976 |           1.5318 |
[32m[20221213 12:32:05 @agent_ppo2.py:179][0m |          -0.0163 |          16.9370 |           1.5746 |
[32m[20221213 12:32:05 @agent_ppo2.py:179][0m |          -0.0272 |          16.2979 |           1.5399 |
[32m[20221213 12:32:05 @agent_ppo2.py:179][0m |          -0.0299 |          16.1218 |           1.0194 |
[32m[20221213 12:32:05 @agent_ppo2.py:179][0m |          -0.0402 |          15.7044 |           0.9219 |
[32m[20221213 12:32:06 @agent_ppo2.py:179][0m |          -0.0445 |          15.4811 |           0.7998 |
[32m[20221213 12:32:06 @agent_ppo2.py:179][0m |          -0.0450 |          15.3706 |           0.2176 |
[32m[20221213 12:32:06 @agent_ppo2.py:179][0m |          -0.0492 |          15.0631 |           0.0692 |
[32m[20221213 12:32:06 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:32:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.12
[32m[20221213 12:32:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 269.97
[32m[20221213 12:32:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 240.77
[32m[20221213 12:32:06 @agent_ppo2.py:137][0m Total time:      16.16 min
[32m[20221213 12:32:06 @agent_ppo2.py:139][0m 1069056 total steps have happened
[32m[20221213 12:32:06 @agent_ppo2.py:115][0m #------------------------ Iteration 522 --------------------------#
[32m[20221213 12:32:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:06 @agent_ppo2.py:179][0m |           0.0656 |          22.1095 |           2.0907 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |           0.0521 |          21.1766 |           3.4082 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |          -0.0008 |          20.5820 |           3.1223 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |          -0.0235 |          20.3818 |           2.7272 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |          -0.0163 |          23.8493 |           2.1062 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |          -0.0219 |          22.0926 |           2.0775 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |          -0.0410 |          19.8559 |           1.9745 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |          -0.0395 |          20.0665 |           1.5904 |
[32m[20221213 12:32:07 @agent_ppo2.py:179][0m |          -0.0443 |          19.5457 |           1.5450 |
[32m[20221213 12:32:08 @agent_ppo2.py:179][0m |          -0.0476 |          19.4289 |           1.0966 |
[32m[20221213 12:32:08 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:32:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.19
[32m[20221213 12:32:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.41
[32m[20221213 12:32:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 275.62
[32m[20221213 12:32:08 @agent_ppo2.py:137][0m Total time:      16.18 min
[32m[20221213 12:32:08 @agent_ppo2.py:139][0m 1071104 total steps have happened
[32m[20221213 12:32:08 @agent_ppo2.py:115][0m #------------------------ Iteration 523 --------------------------#
[32m[20221213 12:32:08 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:32:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:08 @agent_ppo2.py:179][0m |           0.0373 |          15.7509 |           1.7537 |
[32m[20221213 12:32:08 @agent_ppo2.py:179][0m |           0.0354 |          14.5945 |           2.7185 |
[32m[20221213 12:32:09 @agent_ppo2.py:179][0m |           0.0199 |          14.0700 |           1.5597 |
[32m[20221213 12:32:09 @agent_ppo2.py:179][0m |          -0.0120 |          13.6672 |           2.2009 |
[32m[20221213 12:32:09 @agent_ppo2.py:179][0m |          -0.0213 |          13.3184 |           1.7207 |
[32m[20221213 12:32:09 @agent_ppo2.py:179][0m |          -0.0305 |          13.0571 |           1.5520 |
[32m[20221213 12:32:09 @agent_ppo2.py:179][0m |          -0.0370 |          12.8300 |           1.4901 |
[32m[20221213 12:32:09 @agent_ppo2.py:179][0m |          -0.0309 |          13.1810 |           1.3714 |
[32m[20221213 12:32:09 @agent_ppo2.py:179][0m |          -0.0296 |          13.6029 |           1.1410 |
[32m[20221213 12:32:10 @agent_ppo2.py:179][0m |          -0.0432 |          12.2867 |           0.8414 |
[32m[20221213 12:32:10 @agent_ppo2.py:124][0m Policy update time: 1.62 s
[32m[20221213 12:32:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.13
[32m[20221213 12:32:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 266.96
[32m[20221213 12:32:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 241.60
[32m[20221213 12:32:10 @agent_ppo2.py:137][0m Total time:      16.22 min
[32m[20221213 12:32:10 @agent_ppo2.py:139][0m 1073152 total steps have happened
[32m[20221213 12:32:10 @agent_ppo2.py:115][0m #------------------------ Iteration 524 --------------------------#
[32m[20221213 12:32:10 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:10 @agent_ppo2.py:179][0m |           0.0355 |          20.4715 |           0.8609 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |           0.0104 |          19.7129 |           1.5850 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |          -0.0065 |          19.5339 |           1.0490 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |          -0.0171 |          19.3984 |           1.3326 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |          -0.0317 |          19.2544 |           0.9411 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |          -0.0315 |          19.2094 |           0.6838 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |          -0.0353 |          19.0409 |           0.6358 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |          -0.0371 |          18.9470 |           0.4080 |
[32m[20221213 12:32:11 @agent_ppo2.py:179][0m |          -0.0420 |          18.9573 |           0.0178 |
[32m[20221213 12:32:12 @agent_ppo2.py:179][0m |          -0.0445 |          18.8640 |           0.0723 |
[32m[20221213 12:32:12 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:32:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 270.24
[32m[20221213 12:32:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.42
[32m[20221213 12:32:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 283.74
[32m[20221213 12:32:12 @agent_ppo2.py:137][0m Total time:      16.25 min
[32m[20221213 12:32:12 @agent_ppo2.py:139][0m 1075200 total steps have happened
[32m[20221213 12:32:12 @agent_ppo2.py:115][0m #------------------------ Iteration 525 --------------------------#
[32m[20221213 12:32:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:12 @agent_ppo2.py:179][0m |           0.0904 |          20.4739 |           3.0318 |
[32m[20221213 12:32:12 @agent_ppo2.py:179][0m |           0.0574 |          20.0833 |           4.6843 |
[32m[20221213 12:32:12 @agent_ppo2.py:179][0m |           0.0128 |          19.8709 |           3.4747 |
[32m[20221213 12:32:13 @agent_ppo2.py:179][0m |          -0.0089 |          19.7160 |           2.7729 |
[32m[20221213 12:32:13 @agent_ppo2.py:179][0m |          -0.0212 |          19.6169 |           2.5539 |
[32m[20221213 12:32:13 @agent_ppo2.py:179][0m |          -0.0251 |          19.5374 |           2.2289 |
[32m[20221213 12:32:13 @agent_ppo2.py:179][0m |          -0.0324 |          19.4640 |           1.9742 |
[32m[20221213 12:32:13 @agent_ppo2.py:179][0m |          -0.0356 |          19.4415 |           1.7437 |
[32m[20221213 12:32:13 @agent_ppo2.py:179][0m |          -0.0394 |          19.3683 |           1.4137 |
[32m[20221213 12:32:13 @agent_ppo2.py:179][0m |          -0.0340 |          19.3762 |           1.3147 |
[32m[20221213 12:32:13 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:32:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.00
[32m[20221213 12:32:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.26
[32m[20221213 12:32:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 234.96
[32m[20221213 12:32:14 @agent_ppo2.py:137][0m Total time:      16.28 min
[32m[20221213 12:32:14 @agent_ppo2.py:139][0m 1077248 total steps have happened
[32m[20221213 12:32:14 @agent_ppo2.py:115][0m #------------------------ Iteration 526 --------------------------#
[32m[20221213 12:32:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:14 @agent_ppo2.py:179][0m |           0.0378 |          19.4863 |           1.4351 |
[32m[20221213 12:32:14 @agent_ppo2.py:179][0m |           0.0259 |          18.4606 |           2.5173 |
[32m[20221213 12:32:14 @agent_ppo2.py:179][0m |          -0.0004 |          18.0735 |           2.5136 |
[32m[20221213 12:32:14 @agent_ppo2.py:179][0m |          -0.0226 |          17.8228 |           1.9109 |
[32m[20221213 12:32:15 @agent_ppo2.py:179][0m |          -0.0281 |          17.5462 |           1.7306 |
[32m[20221213 12:32:15 @agent_ppo2.py:179][0m |          -0.0338 |          17.3098 |           1.7730 |
[32m[20221213 12:32:15 @agent_ppo2.py:179][0m |          -0.0382 |          17.1219 |           0.9788 |
[32m[20221213 12:32:15 @agent_ppo2.py:179][0m |          -0.0401 |          17.0767 |           1.0942 |
[32m[20221213 12:32:15 @agent_ppo2.py:179][0m |          -0.0364 |          17.2270 |           0.5629 |
[32m[20221213 12:32:15 @agent_ppo2.py:179][0m |          -0.0445 |          16.7956 |           0.3559 |
[32m[20221213 12:32:15 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:32:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.53
[32m[20221213 12:32:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 280.35
[32m[20221213 12:32:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 284.59
[32m[20221213 12:32:15 @agent_ppo2.py:137][0m Total time:      16.31 min
[32m[20221213 12:32:15 @agent_ppo2.py:139][0m 1079296 total steps have happened
[32m[20221213 12:32:15 @agent_ppo2.py:115][0m #------------------------ Iteration 527 --------------------------#
[32m[20221213 12:32:16 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:32:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:16 @agent_ppo2.py:179][0m |           0.0813 |          22.1236 |           2.2985 |
[32m[20221213 12:32:16 @agent_ppo2.py:179][0m |           0.0377 |          20.7513 |           2.5879 |
[32m[20221213 12:32:16 @agent_ppo2.py:179][0m |           0.0146 |          21.0369 |           1.3887 |
[32m[20221213 12:32:16 @agent_ppo2.py:179][0m |          -0.0110 |          20.3260 |           0.5777 |
[32m[20221213 12:32:17 @agent_ppo2.py:179][0m |          -0.0172 |          19.9447 |           0.8529 |
[32m[20221213 12:32:17 @agent_ppo2.py:179][0m |          -0.0306 |          19.8261 |           0.8852 |
[32m[20221213 12:32:17 @agent_ppo2.py:179][0m |          -0.0353 |          19.6127 |           0.6645 |
[32m[20221213 12:32:17 @agent_ppo2.py:179][0m |          -0.0460 |          19.5123 |           0.3643 |
[32m[20221213 12:32:17 @agent_ppo2.py:179][0m |          -0.0450 |          19.4743 |          -0.2617 |
[32m[20221213 12:32:17 @agent_ppo2.py:179][0m |          -0.0461 |          19.3361 |          -0.7134 |
[32m[20221213 12:32:17 @agent_ppo2.py:124][0m Policy update time: 1.60 s
[32m[20221213 12:32:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 236.41
[32m[20221213 12:32:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 263.51
[32m[20221213 12:32:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 283.94
[32m[20221213 12:32:17 @agent_ppo2.py:137][0m Total time:      16.35 min
[32m[20221213 12:32:17 @agent_ppo2.py:139][0m 1081344 total steps have happened
[32m[20221213 12:32:17 @agent_ppo2.py:115][0m #------------------------ Iteration 528 --------------------------#
[32m[20221213 12:32:18 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:32:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:18 @agent_ppo2.py:179][0m |           0.0590 |          21.2049 |           0.4775 |
[32m[20221213 12:32:18 @agent_ppo2.py:179][0m |           0.0222 |          20.6522 |           0.6264 |
[32m[20221213 12:32:18 @agent_ppo2.py:179][0m |           0.0110 |          20.7837 |          -0.1103 |
[32m[20221213 12:32:18 @agent_ppo2.py:179][0m |          -0.0143 |          20.2365 |          -0.8069 |
[32m[20221213 12:32:19 @agent_ppo2.py:179][0m |          -0.0275 |          20.1174 |          -1.1989 |
[32m[20221213 12:32:19 @agent_ppo2.py:179][0m |          -0.0321 |          20.0307 |          -1.5681 |
[32m[20221213 12:32:19 @agent_ppo2.py:179][0m |          -0.0250 |          19.9863 |          -1.9163 |
[32m[20221213 12:32:19 @agent_ppo2.py:179][0m |          -0.0395 |          19.9539 |          -2.1504 |
[32m[20221213 12:32:19 @agent_ppo2.py:179][0m |          -0.0394 |          19.9186 |          -2.7324 |
[32m[20221213 12:32:19 @agent_ppo2.py:179][0m |          -0.0404 |          19.8467 |          -2.7220 |
[32m[20221213 12:32:19 @agent_ppo2.py:124][0m Policy update time: 1.50 s
[32m[20221213 12:32:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 269.35
[32m[20221213 12:32:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 290.01
[32m[20221213 12:32:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 279.17
[32m[20221213 12:32:19 @agent_ppo2.py:137][0m Total time:      16.38 min
[32m[20221213 12:32:19 @agent_ppo2.py:139][0m 1083392 total steps have happened
[32m[20221213 12:32:19 @agent_ppo2.py:115][0m #------------------------ Iteration 529 --------------------------#
[32m[20221213 12:32:20 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:20 @agent_ppo2.py:179][0m |           0.0745 |          20.2247 |          -0.3148 |
[32m[20221213 12:32:20 @agent_ppo2.py:179][0m |           0.0343 |          19.5039 |           0.3853 |
[32m[20221213 12:32:20 @agent_ppo2.py:179][0m |           0.0128 |          19.1550 |          -0.2786 |
[32m[20221213 12:32:20 @agent_ppo2.py:179][0m |          -0.0161 |          18.6504 |          -0.5151 |
[32m[20221213 12:32:21 @agent_ppo2.py:179][0m |          -0.0199 |          18.3667 |          -0.7248 |
[32m[20221213 12:32:21 @agent_ppo2.py:179][0m |          -0.0291 |          18.1478 |          -0.8764 |
[32m[20221213 12:32:21 @agent_ppo2.py:179][0m |          -0.0365 |          17.9459 |          -1.6050 |
[32m[20221213 12:32:21 @agent_ppo2.py:179][0m |          -0.0380 |          17.7328 |          -1.6699 |
[32m[20221213 12:32:21 @agent_ppo2.py:179][0m |          -0.0412 |          17.5831 |          -2.0364 |
[32m[20221213 12:32:21 @agent_ppo2.py:179][0m |          -0.0395 |          17.4139 |          -2.4992 |
[32m[20221213 12:32:21 @agent_ppo2.py:124][0m Policy update time: 1.74 s
[32m[20221213 12:32:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 258.55
[32m[20221213 12:32:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 287.48
[32m[20221213 12:32:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 272.20
[32m[20221213 12:32:22 @agent_ppo2.py:137][0m Total time:      16.41 min
[32m[20221213 12:32:22 @agent_ppo2.py:139][0m 1085440 total steps have happened
[32m[20221213 12:32:22 @agent_ppo2.py:115][0m #------------------------ Iteration 530 --------------------------#
[32m[20221213 12:32:22 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:32:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:22 @agent_ppo2.py:179][0m |           0.0553 |          18.7237 |          -1.1232 |
[32m[20221213 12:32:22 @agent_ppo2.py:179][0m |           0.0218 |          17.5883 |          -0.4079 |
[32m[20221213 12:32:22 @agent_ppo2.py:179][0m |           0.0018 |          17.1239 |          -0.8203 |
[32m[20221213 12:32:22 @agent_ppo2.py:179][0m |          -0.0200 |          16.7734 |          -1.1821 |
[32m[20221213 12:32:23 @agent_ppo2.py:179][0m |          -0.0260 |          16.5365 |          -0.9231 |
[32m[20221213 12:32:23 @agent_ppo2.py:179][0m |          -0.0330 |          16.4551 |          -1.6903 |
[32m[20221213 12:32:23 @agent_ppo2.py:179][0m |          -0.0403 |          16.1380 |          -2.3625 |
[32m[20221213 12:32:23 @agent_ppo2.py:179][0m |          -0.0287 |          17.3898 |          -2.5640 |
[32m[20221213 12:32:23 @agent_ppo2.py:179][0m |          -0.0441 |          15.9220 |          -3.1445 |
[32m[20221213 12:32:23 @agent_ppo2.py:179][0m |          -0.0403 |          15.7022 |          -3.3851 |
[32m[20221213 12:32:23 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:32:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.29
[32m[20221213 12:32:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 263.11
[32m[20221213 12:32:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 276.13
[32m[20221213 12:32:23 @agent_ppo2.py:137][0m Total time:      16.45 min
[32m[20221213 12:32:23 @agent_ppo2.py:139][0m 1087488 total steps have happened
[32m[20221213 12:32:23 @agent_ppo2.py:115][0m #------------------------ Iteration 531 --------------------------#
[32m[20221213 12:32:24 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:24 @agent_ppo2.py:179][0m |           0.0352 |          22.2022 |           0.1900 |
[32m[20221213 12:32:24 @agent_ppo2.py:179][0m |           0.0246 |          21.9799 |           0.5392 |
[32m[20221213 12:32:24 @agent_ppo2.py:179][0m |           0.0160 |          21.0357 |           1.8481 |
[32m[20221213 12:32:24 @agent_ppo2.py:179][0m |          -0.0051 |          20.8717 |           0.5487 |
[32m[20221213 12:32:25 @agent_ppo2.py:179][0m |          -0.0257 |          20.7349 |          -0.0802 |
[32m[20221213 12:32:25 @agent_ppo2.py:179][0m |          -0.0277 |          20.6347 |          -0.4475 |
[32m[20221213 12:32:25 @agent_ppo2.py:179][0m |          -0.0350 |          20.5431 |          -0.7883 |
[32m[20221213 12:32:25 @agent_ppo2.py:179][0m |          -0.0340 |          20.8290 |          -1.2439 |
[32m[20221213 12:32:25 @agent_ppo2.py:179][0m |          -0.0416 |          20.4300 |          -1.4752 |
[32m[20221213 12:32:25 @agent_ppo2.py:179][0m |          -0.0345 |          20.8123 |          -1.5908 |
[32m[20221213 12:32:25 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:32:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 264.78
[32m[20221213 12:32:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.16
[32m[20221213 12:32:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 257.02
[32m[20221213 12:32:26 @agent_ppo2.py:137][0m Total time:      16.48 min
[32m[20221213 12:32:26 @agent_ppo2.py:139][0m 1089536 total steps have happened
[32m[20221213 12:32:26 @agent_ppo2.py:115][0m #------------------------ Iteration 532 --------------------------#
[32m[20221213 12:32:26 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:26 @agent_ppo2.py:179][0m |           0.1004 |          23.9589 |           0.7354 |
[32m[20221213 12:32:26 @agent_ppo2.py:179][0m |           0.1560 |          21.4825 |           3.6513 |
[32m[20221213 12:32:26 @agent_ppo2.py:179][0m |           0.0412 |          21.2166 |           2.6751 |
[32m[20221213 12:32:26 @agent_ppo2.py:179][0m |           0.0110 |          20.9230 |           1.3435 |
[32m[20221213 12:32:27 @agent_ppo2.py:179][0m |           0.0004 |          21.6470 |           1.1865 |
[32m[20221213 12:32:27 @agent_ppo2.py:179][0m |          -0.0085 |          20.6958 |           1.1389 |
[32m[20221213 12:32:27 @agent_ppo2.py:179][0m |          -0.0252 |          20.6042 |           0.8676 |
[32m[20221213 12:32:27 @agent_ppo2.py:179][0m |          -0.0288 |          20.5770 |           0.5716 |
[32m[20221213 12:32:27 @agent_ppo2.py:179][0m |          -0.0316 |          20.6134 |           0.2981 |
[32m[20221213 12:32:27 @agent_ppo2.py:179][0m |          -0.0372 |          20.3502 |           0.2013 |
[32m[20221213 12:32:27 @agent_ppo2.py:124][0m Policy update time: 1.56 s
[32m[20221213 12:32:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 267.59
[32m[20221213 12:32:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 280.99
[32m[20221213 12:32:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 295.68
[32m[20221213 12:32:28 @agent_ppo2.py:137][0m Total time:      16.51 min
[32m[20221213 12:32:28 @agent_ppo2.py:139][0m 1091584 total steps have happened
[32m[20221213 12:32:28 @agent_ppo2.py:115][0m #------------------------ Iteration 533 --------------------------#
[32m[20221213 12:32:28 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:28 @agent_ppo2.py:179][0m |           0.0676 |          21.6504 |           0.9899 |
[32m[20221213 12:32:28 @agent_ppo2.py:179][0m |           0.0342 |          20.9276 |           2.2740 |
[32m[20221213 12:32:28 @agent_ppo2.py:179][0m |           0.0010 |          20.6220 |           1.6164 |
[32m[20221213 12:32:28 @agent_ppo2.py:179][0m |          -0.0059 |          21.3761 |           1.0358 |
[32m[20221213 12:32:28 @agent_ppo2.py:179][0m |          -0.0151 |          20.2537 |           1.0714 |
[32m[20221213 12:32:29 @agent_ppo2.py:179][0m |          -0.0214 |          21.1900 |           0.5193 |
[32m[20221213 12:32:29 @agent_ppo2.py:179][0m |          -0.0351 |          19.9989 |           0.2684 |
[32m[20221213 12:32:29 @agent_ppo2.py:179][0m |          -0.0418 |          19.8465 |           0.1564 |
[32m[20221213 12:32:29 @agent_ppo2.py:179][0m |          -0.0440 |          19.7466 |           0.0286 |
[32m[20221213 12:32:29 @agent_ppo2.py:179][0m |          -0.0434 |          19.6485 |          -0.4038 |
[32m[20221213 12:32:29 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:32:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 258.72
[32m[20221213 12:32:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 278.90
[32m[20221213 12:32:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 280.58
[32m[20221213 12:32:29 @agent_ppo2.py:137][0m Total time:      16.54 min
[32m[20221213 12:32:29 @agent_ppo2.py:139][0m 1093632 total steps have happened
[32m[20221213 12:32:29 @agent_ppo2.py:115][0m #------------------------ Iteration 534 --------------------------#
[32m[20221213 12:32:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:30 @agent_ppo2.py:179][0m |           0.0617 |          21.1347 |           0.4308 |
[32m[20221213 12:32:30 @agent_ppo2.py:179][0m |           0.0408 |          19.9953 |           1.8100 |
[32m[20221213 12:32:30 @agent_ppo2.py:179][0m |           0.0022 |          19.6097 |           1.6501 |
[32m[20221213 12:32:30 @agent_ppo2.py:179][0m |          -0.0107 |          19.4564 |           1.3705 |
[32m[20221213 12:32:30 @agent_ppo2.py:179][0m |          -0.0245 |          18.9948 |           0.8799 |
[32m[20221213 12:32:30 @agent_ppo2.py:179][0m |          -0.0344 |          18.7934 |           0.8593 |
[32m[20221213 12:32:30 @agent_ppo2.py:179][0m |          -0.0378 |          18.6356 |           0.5320 |
[32m[20221213 12:32:31 @agent_ppo2.py:179][0m |          -0.0425 |          18.5845 |           0.1582 |
[32m[20221213 12:32:31 @agent_ppo2.py:179][0m |          -0.0423 |          18.4631 |          -0.1277 |
[32m[20221213 12:32:31 @agent_ppo2.py:179][0m |          -0.0419 |          18.3287 |          -0.6170 |
[32m[20221213 12:32:31 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:32:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 258.37
[32m[20221213 12:32:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 266.72
[32m[20221213 12:32:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 256.54
[32m[20221213 12:32:31 @agent_ppo2.py:137][0m Total time:      16.57 min
[32m[20221213 12:32:31 @agent_ppo2.py:139][0m 1095680 total steps have happened
[32m[20221213 12:32:31 @agent_ppo2.py:115][0m #------------------------ Iteration 535 --------------------------#
[32m[20221213 12:32:31 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:31 @agent_ppo2.py:179][0m |           0.0824 |          19.8434 |           2.7168 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |           0.0674 |          19.0075 |           2.4369 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |           0.0040 |          20.8941 |           1.8782 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |          -0.0271 |          18.4230 |           1.3808 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |          -0.0364 |          18.2530 |           1.1021 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |          -0.0458 |          18.0738 |           0.8836 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |          -0.0485 |          18.2559 |           0.5564 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |          -0.0572 |          17.8480 |           0.3262 |
[32m[20221213 12:32:32 @agent_ppo2.py:179][0m |          -0.0514 |          17.8305 |           0.1063 |
[32m[20221213 12:32:33 @agent_ppo2.py:179][0m |          -0.0471 |          17.9787 |          -0.4647 |
[32m[20221213 12:32:33 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:32:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 197.61
[32m[20221213 12:32:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 271.18
[32m[20221213 12:32:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.21
[32m[20221213 12:32:33 @agent_ppo2.py:137][0m Total time:      16.60 min
[32m[20221213 12:32:33 @agent_ppo2.py:139][0m 1097728 total steps have happened
[32m[20221213 12:32:33 @agent_ppo2.py:115][0m #------------------------ Iteration 536 --------------------------#
[32m[20221213 12:32:33 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:33 @agent_ppo2.py:179][0m |           0.1203 |          21.5112 |           1.3425 |
[32m[20221213 12:32:33 @agent_ppo2.py:179][0m |           0.0826 |          21.1412 |           2.7704 |
[32m[20221213 12:32:34 @agent_ppo2.py:179][0m |           0.0492 |          20.7149 |           3.0271 |
[32m[20221213 12:32:34 @agent_ppo2.py:179][0m |           0.0233 |          20.6589 |           1.5835 |
[32m[20221213 12:32:34 @agent_ppo2.py:179][0m |           0.0013 |          20.5093 |           1.0143 |
[32m[20221213 12:32:34 @agent_ppo2.py:179][0m |          -0.0105 |          20.6881 |           0.5431 |
[32m[20221213 12:32:34 @agent_ppo2.py:179][0m |          -0.0232 |          20.3448 |           0.2444 |
[32m[20221213 12:32:34 @agent_ppo2.py:179][0m |          -0.0140 |          20.2689 |           0.1449 |
[32m[20221213 12:32:34 @agent_ppo2.py:179][0m |          -0.0180 |          20.1925 |          -0.2497 |
[32m[20221213 12:32:35 @agent_ppo2.py:179][0m |          -0.0239 |          20.1179 |          -0.3947 |
[32m[20221213 12:32:35 @agent_ppo2.py:124][0m Policy update time: 1.62 s
[32m[20221213 12:32:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 265.54
[32m[20221213 12:32:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 287.48
[32m[20221213 12:32:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.56
[32m[20221213 12:32:35 @agent_ppo2.py:137][0m Total time:      16.64 min
[32m[20221213 12:32:35 @agent_ppo2.py:139][0m 1099776 total steps have happened
[32m[20221213 12:32:35 @agent_ppo2.py:115][0m #------------------------ Iteration 537 --------------------------#
[32m[20221213 12:32:35 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:35 @agent_ppo2.py:179][0m |           0.0325 |          21.7032 |          -0.4646 |
[32m[20221213 12:32:35 @agent_ppo2.py:179][0m |           0.0016 |          21.2500 |           0.3735 |
[32m[20221213 12:32:36 @agent_ppo2.py:179][0m |          -0.0177 |          21.0162 |          -0.0194 |
[32m[20221213 12:32:36 @agent_ppo2.py:179][0m |          -0.0233 |          20.9048 |          -0.3422 |
[32m[20221213 12:32:36 @agent_ppo2.py:179][0m |          -0.0325 |          20.7733 |          -0.9539 |
[32m[20221213 12:32:36 @agent_ppo2.py:179][0m |          -0.0328 |          20.7644 |          -0.8394 |
[32m[20221213 12:32:36 @agent_ppo2.py:179][0m |          -0.0353 |          20.6446 |          -1.1763 |
[32m[20221213 12:32:36 @agent_ppo2.py:179][0m |          -0.0343 |          21.3398 |          -1.1999 |
[32m[20221213 12:32:36 @agent_ppo2.py:179][0m |          -0.0370 |          20.5510 |          -1.3915 |
[32m[20221213 12:32:37 @agent_ppo2.py:179][0m |          -0.0369 |          20.5062 |          -1.4743 |
[32m[20221213 12:32:37 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:32:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 274.09
[32m[20221213 12:32:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 282.14
[32m[20221213 12:32:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 271.87
[32m[20221213 12:32:37 @agent_ppo2.py:137][0m Total time:      16.67 min
[32m[20221213 12:32:37 @agent_ppo2.py:139][0m 1101824 total steps have happened
[32m[20221213 12:32:37 @agent_ppo2.py:115][0m #------------------------ Iteration 538 --------------------------#
[32m[20221213 12:32:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:37 @agent_ppo2.py:179][0m |           0.0341 |          20.5564 |           0.6028 |
[32m[20221213 12:32:37 @agent_ppo2.py:179][0m |           0.0273 |          20.1242 |           1.1316 |
[32m[20221213 12:32:37 @agent_ppo2.py:179][0m |           0.0085 |          19.8859 |           0.6942 |
[32m[20221213 12:32:38 @agent_ppo2.py:179][0m |          -0.0034 |          19.7524 |           0.2453 |
[32m[20221213 12:32:38 @agent_ppo2.py:179][0m |          -0.0199 |          19.6529 |          -0.3950 |
[32m[20221213 12:32:38 @agent_ppo2.py:179][0m |          -0.0272 |          19.5573 |          -0.8731 |
[32m[20221213 12:32:38 @agent_ppo2.py:179][0m |          -0.0177 |          21.5971 |          -0.8408 |
[32m[20221213 12:32:38 @agent_ppo2.py:179][0m |          -0.0377 |          19.5208 |          -1.4942 |
[32m[20221213 12:32:38 @agent_ppo2.py:179][0m |          -0.0363 |          19.3617 |          -1.6106 |
[32m[20221213 12:32:38 @agent_ppo2.py:179][0m |          -0.0352 |          19.3109 |          -2.1323 |
[32m[20221213 12:32:38 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:32:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.42
[32m[20221213 12:32:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 275.41
[32m[20221213 12:32:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 274.43
[32m[20221213 12:32:39 @agent_ppo2.py:137][0m Total time:      16.70 min
[32m[20221213 12:32:39 @agent_ppo2.py:139][0m 1103872 total steps have happened
[32m[20221213 12:32:39 @agent_ppo2.py:115][0m #------------------------ Iteration 539 --------------------------#
[32m[20221213 12:32:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:39 @agent_ppo2.py:179][0m |           0.0577 |          20.8379 |          -0.8350 |
[32m[20221213 12:32:39 @agent_ppo2.py:179][0m |           0.0344 |          20.5713 |           0.2784 |
[32m[20221213 12:32:39 @agent_ppo2.py:179][0m |          -0.0018 |          20.3395 |          -0.5180 |
[32m[20221213 12:32:39 @agent_ppo2.py:179][0m |          -0.0138 |          20.8127 |          -0.8629 |
[32m[20221213 12:32:40 @agent_ppo2.py:179][0m |          -0.0249 |          20.2710 |          -1.7964 |
[32m[20221213 12:32:40 @agent_ppo2.py:179][0m |          -0.0300 |          20.0773 |          -1.8104 |
[32m[20221213 12:32:40 @agent_ppo2.py:179][0m |          -0.0334 |          19.9811 |          -2.0945 |
[32m[20221213 12:32:40 @agent_ppo2.py:179][0m |          -0.0372 |          19.8695 |          -2.9207 |
[32m[20221213 12:32:40 @agent_ppo2.py:179][0m |          -0.0406 |          19.8198 |          -3.0263 |
[32m[20221213 12:32:40 @agent_ppo2.py:179][0m |          -0.0392 |          19.7674 |          -3.3525 |
[32m[20221213 12:32:40 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:32:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.84
[32m[20221213 12:32:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 295.45
[32m[20221213 12:32:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 231.81
[32m[20221213 12:32:40 @agent_ppo2.py:137][0m Total time:      16.73 min
[32m[20221213 12:32:40 @agent_ppo2.py:139][0m 1105920 total steps have happened
[32m[20221213 12:32:40 @agent_ppo2.py:115][0m #------------------------ Iteration 540 --------------------------#
[32m[20221213 12:32:41 @agent_ppo2.py:121][0m Sampling time: 0.27 s by 5 slaves
[32m[20221213 12:32:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:41 @agent_ppo2.py:179][0m |           0.0675 |          20.6879 |          -0.8086 |
[32m[20221213 12:32:41 @agent_ppo2.py:179][0m |           0.0180 |          19.6188 |          -0.5860 |
[32m[20221213 12:32:41 @agent_ppo2.py:179][0m |          -0.0108 |          19.1225 |          -1.2177 |
[32m[20221213 12:32:41 @agent_ppo2.py:179][0m |          -0.0263 |          18.7840 |          -1.5588 |
[32m[20221213 12:32:42 @agent_ppo2.py:179][0m |          -0.0310 |          18.5733 |          -1.7771 |
[32m[20221213 12:32:42 @agent_ppo2.py:179][0m |          -0.0390 |          18.2277 |          -1.9639 |
[32m[20221213 12:32:42 @agent_ppo2.py:179][0m |          -0.0453 |          17.9935 |          -2.4973 |
[32m[20221213 12:32:42 @agent_ppo2.py:179][0m |          -0.0319 |          19.9470 |          -2.4447 |
[32m[20221213 12:32:42 @agent_ppo2.py:179][0m |          -0.0480 |          17.9691 |          -3.0460 |
[32m[20221213 12:32:42 @agent_ppo2.py:179][0m |          -0.0542 |          17.5430 |          -3.3980 |
[32m[20221213 12:32:42 @agent_ppo2.py:124][0m Policy update time: 1.60 s
[32m[20221213 12:32:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 228.29
[32m[20221213 12:32:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 274.32
[32m[20221213 12:32:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 281.41
[32m[20221213 12:32:43 @agent_ppo2.py:137][0m Total time:      16.76 min
[32m[20221213 12:32:43 @agent_ppo2.py:139][0m 1107968 total steps have happened
[32m[20221213 12:32:43 @agent_ppo2.py:115][0m #------------------------ Iteration 541 --------------------------#
[32m[20221213 12:32:43 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:43 @agent_ppo2.py:179][0m |           0.0613 |          20.5884 |          -2.2769 |
[32m[20221213 12:32:43 @agent_ppo2.py:179][0m |           0.0982 |          19.4638 |           0.4362 |
[32m[20221213 12:32:43 @agent_ppo2.py:179][0m |           0.0266 |          18.6193 |          -1.5173 |
[32m[20221213 12:32:43 @agent_ppo2.py:179][0m |          -0.0006 |          18.5479 |          -1.8288 |
[32m[20221213 12:32:44 @agent_ppo2.py:179][0m |          -0.0257 |          17.6009 |          -2.6809 |
[32m[20221213 12:32:44 @agent_ppo2.py:179][0m |          -0.0336 |          17.1223 |          -2.3714 |
[32m[20221213 12:32:44 @agent_ppo2.py:179][0m |          -0.0382 |          16.7550 |          -2.6628 |
[32m[20221213 12:32:44 @agent_ppo2.py:179][0m |          -0.0414 |          16.5034 |          -2.9608 |
[32m[20221213 12:32:44 @agent_ppo2.py:179][0m |          -0.0429 |          16.3073 |          -3.4436 |
[32m[20221213 12:32:44 @agent_ppo2.py:179][0m |          -0.0442 |          16.0482 |          -3.7650 |
[32m[20221213 12:32:44 @agent_ppo2.py:124][0m Policy update time: 1.54 s
[32m[20221213 12:32:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 267.56
[32m[20221213 12:32:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 288.75
[32m[20221213 12:32:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 288.47
[32m[20221213 12:32:44 @agent_ppo2.py:137][0m Total time:      16.80 min
[32m[20221213 12:32:44 @agent_ppo2.py:139][0m 1110016 total steps have happened
[32m[20221213 12:32:44 @agent_ppo2.py:115][0m #------------------------ Iteration 542 --------------------------#
[32m[20221213 12:32:45 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:45 @agent_ppo2.py:179][0m |           0.0400 |          19.4314 |          -2.3893 |
[32m[20221213 12:32:45 @agent_ppo2.py:179][0m |          -0.0062 |          18.0431 |          -2.1135 |
[32m[20221213 12:32:45 @agent_ppo2.py:179][0m |          -0.0257 |          17.4984 |          -2.5311 |
[32m[20221213 12:32:45 @agent_ppo2.py:179][0m |          -0.0439 |          17.1025 |          -2.8303 |
[32m[20221213 12:32:46 @agent_ppo2.py:179][0m |          -0.0497 |          16.7109 |          -3.2768 |
[32m[20221213 12:32:46 @agent_ppo2.py:179][0m |          -0.0480 |          16.7710 |          -3.5245 |
[32m[20221213 12:32:46 @agent_ppo2.py:179][0m |          -0.0595 |          16.2426 |          -4.4005 |
[32m[20221213 12:32:46 @agent_ppo2.py:179][0m |          -0.0591 |          15.9648 |          -4.6906 |
[32m[20221213 12:32:46 @agent_ppo2.py:179][0m |          -0.0651 |          15.7624 |          -5.1653 |
[32m[20221213 12:32:46 @agent_ppo2.py:179][0m |          -0.0590 |          15.7964 |          -5.3121 |
[32m[20221213 12:32:46 @agent_ppo2.py:124][0m Policy update time: 1.61 s
[32m[20221213 12:32:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.18
[32m[20221213 12:32:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 236.13
[32m[20221213 12:32:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 270.04
[32m[20221213 12:32:47 @agent_ppo2.py:137][0m Total time:      16.83 min
[32m[20221213 12:32:47 @agent_ppo2.py:139][0m 1112064 total steps have happened
[32m[20221213 12:32:47 @agent_ppo2.py:115][0m #------------------------ Iteration 543 --------------------------#
[32m[20221213 12:32:47 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:47 @agent_ppo2.py:179][0m |           0.0714 |          22.8027 |          -4.4415 |
[32m[20221213 12:32:47 @agent_ppo2.py:179][0m |           0.0478 |          21.4547 |          -2.2152 |
[32m[20221213 12:32:47 @agent_ppo2.py:179][0m |           0.0015 |          20.8506 |          -4.1764 |
[32m[20221213 12:32:47 @agent_ppo2.py:179][0m |          -0.0112 |          20.4469 |          -4.3626 |
[32m[20221213 12:32:48 @agent_ppo2.py:179][0m |          -0.0283 |          20.1780 |          -5.1550 |
[32m[20221213 12:32:48 @agent_ppo2.py:179][0m |          -0.0363 |          20.6354 |          -5.7523 |
[32m[20221213 12:32:48 @agent_ppo2.py:179][0m |          -0.0330 |          21.3324 |          -6.5226 |
[32m[20221213 12:32:48 @agent_ppo2.py:179][0m |          -0.0515 |          19.4594 |          -6.7816 |
[32m[20221213 12:32:48 @agent_ppo2.py:179][0m |          -0.0493 |          19.2749 |          -7.4932 |
[32m[20221213 12:32:48 @agent_ppo2.py:179][0m |          -0.0562 |          19.1017 |          -8.0187 |
[32m[20221213 12:32:48 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:32:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.56
[32m[20221213 12:32:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 268.37
[32m[20221213 12:32:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 48.12
[32m[20221213 12:32:48 @agent_ppo2.py:137][0m Total time:      16.86 min
[32m[20221213 12:32:48 @agent_ppo2.py:139][0m 1114112 total steps have happened
[32m[20221213 12:32:48 @agent_ppo2.py:115][0m #------------------------ Iteration 544 --------------------------#
[32m[20221213 12:32:49 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:32:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:49 @agent_ppo2.py:179][0m |           0.0772 |          23.4151 |          -4.9150 |
[32m[20221213 12:32:49 @agent_ppo2.py:179][0m |           0.1026 |          22.1124 |           0.5065 |
[32m[20221213 12:32:49 @agent_ppo2.py:179][0m |           0.0400 |          21.7830 |          -3.2445 |
[32m[20221213 12:32:49 @agent_ppo2.py:179][0m |           0.0125 |          21.5209 |          -4.9963 |
[32m[20221213 12:32:49 @agent_ppo2.py:179][0m |           0.0053 |          21.9266 |          -6.2853 |
[32m[20221213 12:32:50 @agent_ppo2.py:179][0m |          -0.0139 |          21.2818 |          -6.6286 |
[32m[20221213 12:32:50 @agent_ppo2.py:179][0m |          -0.0239 |          21.1369 |          -7.2726 |
[32m[20221213 12:32:50 @agent_ppo2.py:179][0m |          -0.0244 |          23.0852 |          -7.5619 |
[32m[20221213 12:32:50 @agent_ppo2.py:179][0m |          -0.0307 |          20.8842 |          -7.7768 |
[32m[20221213 12:32:50 @agent_ppo2.py:179][0m |          -0.0289 |          21.3424 |          -8.8293 |
[32m[20221213 12:32:50 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:32:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 284.10
[32m[20221213 12:32:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 295.32
[32m[20221213 12:32:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 247.00
[32m[20221213 12:32:50 @agent_ppo2.py:137][0m Total time:      16.89 min
[32m[20221213 12:32:50 @agent_ppo2.py:139][0m 1116160 total steps have happened
[32m[20221213 12:32:50 @agent_ppo2.py:115][0m #------------------------ Iteration 545 --------------------------#
[32m[20221213 12:32:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:51 @agent_ppo2.py:179][0m |           0.0593 |          21.1121 |          -7.8408 |
[32m[20221213 12:32:51 @agent_ppo2.py:179][0m |           0.0344 |          20.4050 |          -6.8193 |
[32m[20221213 12:32:51 @agent_ppo2.py:179][0m |           0.0005 |          20.0773 |          -7.7255 |
[32m[20221213 12:32:51 @agent_ppo2.py:179][0m |          -0.0170 |          19.8060 |          -8.7778 |
[32m[20221213 12:32:51 @agent_ppo2.py:179][0m |          -0.0253 |          19.5975 |          -9.1454 |
[32m[20221213 12:32:51 @agent_ppo2.py:179][0m |          -0.0342 |          19.3681 |          -9.6955 |
[32m[20221213 12:32:52 @agent_ppo2.py:179][0m |          -0.0387 |          19.1754 |          -9.4856 |
[32m[20221213 12:32:52 @agent_ppo2.py:179][0m |          -0.0340 |          20.5329 |         -10.1522 |
[32m[20221213 12:32:52 @agent_ppo2.py:179][0m |          -0.0404 |          18.9839 |         -10.4298 |
[32m[20221213 12:32:52 @agent_ppo2.py:179][0m |          -0.0429 |          18.8102 |         -10.8059 |
[32m[20221213 12:32:52 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:32:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.63
[32m[20221213 12:32:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 282.87
[32m[20221213 12:32:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 295.63
[32m[20221213 12:32:52 @agent_ppo2.py:137][0m Total time:      16.92 min
[32m[20221213 12:32:52 @agent_ppo2.py:139][0m 1118208 total steps have happened
[32m[20221213 12:32:52 @agent_ppo2.py:115][0m #------------------------ Iteration 546 --------------------------#
[32m[20221213 12:32:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |           0.0475 |          16.8491 |          -7.6208 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |           0.0109 |          15.2231 |          -6.6227 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |          -0.0180 |          14.6136 |          -7.0420 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |          -0.0341 |          14.2054 |          -7.3449 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |          -0.0413 |          13.8412 |          -8.1318 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |          -0.0453 |          13.5248 |          -8.3604 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |          -0.0471 |          13.3086 |          -8.3355 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |          -0.0488 |          13.1121 |          -8.8004 |
[32m[20221213 12:32:53 @agent_ppo2.py:179][0m |          -0.0537 |          12.9463 |          -9.1231 |
[32m[20221213 12:32:54 @agent_ppo2.py:179][0m |          -0.0532 |          12.8488 |          -9.4263 |
[32m[20221213 12:32:54 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:32:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.46
[32m[20221213 12:32:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 278.22
[32m[20221213 12:32:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.33
[32m[20221213 12:32:54 @agent_ppo2.py:137][0m Total time:      16.95 min
[32m[20221213 12:32:54 @agent_ppo2.py:139][0m 1120256 total steps have happened
[32m[20221213 12:32:54 @agent_ppo2.py:115][0m #------------------------ Iteration 547 --------------------------#
[32m[20221213 12:32:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:54 @agent_ppo2.py:179][0m |           0.0913 |          23.5498 |          -8.4502 |
[32m[20221213 12:32:54 @agent_ppo2.py:179][0m |           0.0521 |          22.6232 |          -5.7370 |
[32m[20221213 12:32:54 @agent_ppo2.py:179][0m |           0.0319 |          23.8264 |          -6.3569 |
[32m[20221213 12:32:55 @agent_ppo2.py:179][0m |           0.0175 |          22.2424 |          -5.9792 |
[32m[20221213 12:32:55 @agent_ppo2.py:179][0m |          -0.0122 |          22.0427 |          -7.3920 |
[32m[20221213 12:32:55 @agent_ppo2.py:179][0m |          -0.0215 |          21.9552 |          -8.4312 |
[32m[20221213 12:32:55 @agent_ppo2.py:179][0m |          -0.0308 |          21.8963 |          -9.4396 |
[32m[20221213 12:32:55 @agent_ppo2.py:179][0m |          -0.0317 |          22.0701 |          -9.4646 |
[32m[20221213 12:32:55 @agent_ppo2.py:179][0m |          -0.0413 |          21.7281 |          -9.6019 |
[32m[20221213 12:32:55 @agent_ppo2.py:179][0m |          -0.0438 |          21.6784 |         -10.2687 |
[32m[20221213 12:32:55 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:32:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 273.45
[32m[20221213 12:32:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 291.27
[32m[20221213 12:32:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 201.87
[32m[20221213 12:32:55 @agent_ppo2.py:137][0m Total time:      16.98 min
[32m[20221213 12:32:55 @agent_ppo2.py:139][0m 1122304 total steps have happened
[32m[20221213 12:32:55 @agent_ppo2.py:115][0m #------------------------ Iteration 548 --------------------------#
[32m[20221213 12:32:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:56 @agent_ppo2.py:179][0m |           0.0739 |          21.4776 |          -7.1200 |
[32m[20221213 12:32:56 @agent_ppo2.py:179][0m |           0.0454 |          20.3382 |          -5.4829 |
[32m[20221213 12:32:56 @agent_ppo2.py:179][0m |           0.0012 |          19.7273 |          -6.6817 |
[32m[20221213 12:32:56 @agent_ppo2.py:179][0m |          -0.0181 |          19.4097 |          -7.6061 |
[32m[20221213 12:32:56 @agent_ppo2.py:179][0m |          -0.0280 |          19.0453 |          -7.8299 |
[32m[20221213 12:32:57 @agent_ppo2.py:179][0m |          -0.0357 |          18.6679 |          -8.5404 |
[32m[20221213 12:32:57 @agent_ppo2.py:179][0m |          -0.0388 |          18.5245 |          -9.0371 |
[32m[20221213 12:32:57 @agent_ppo2.py:179][0m |          -0.0477 |          18.1939 |          -9.7543 |
[32m[20221213 12:32:57 @agent_ppo2.py:179][0m |          -0.0433 |          18.0341 |         -10.1049 |
[32m[20221213 12:32:57 @agent_ppo2.py:179][0m |          -0.0469 |          17.7957 |         -10.3082 |
[32m[20221213 12:32:57 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:32:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.87
[32m[20221213 12:32:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 277.73
[32m[20221213 12:32:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 314.86
[32m[20221213 12:32:57 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 314.86
[32m[20221213 12:32:57 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 314.86
[32m[20221213 12:32:57 @agent_ppo2.py:137][0m Total time:      17.01 min
[32m[20221213 12:32:57 @agent_ppo2.py:139][0m 1124352 total steps have happened
[32m[20221213 12:32:57 @agent_ppo2.py:115][0m #------------------------ Iteration 549 --------------------------#
[32m[20221213 12:32:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:32:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |           0.0556 |          23.2869 |          -7.5778 |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |           0.0522 |          22.7080 |          -5.5130 |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |           0.0185 |          21.2766 |          -6.1237 |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |          -0.0122 |          20.8276 |          -7.5366 |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |          -0.0239 |          20.5245 |          -8.1706 |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |          -0.0301 |          21.8865 |          -8.6800 |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |          -0.0285 |          21.1294 |          -8.6633 |
[32m[20221213 12:32:58 @agent_ppo2.py:179][0m |          -0.0440 |          19.9010 |          -9.1937 |
[32m[20221213 12:32:59 @agent_ppo2.py:179][0m |          -0.0518 |          19.7255 |          -9.7571 |
[32m[20221213 12:32:59 @agent_ppo2.py:179][0m |          -0.0414 |          20.6750 |         -10.3432 |
[32m[20221213 12:32:59 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:32:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 236.53
[32m[20221213 12:32:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 282.37
[32m[20221213 12:32:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 263.08
[32m[20221213 12:32:59 @agent_ppo2.py:137][0m Total time:      17.04 min
[32m[20221213 12:32:59 @agent_ppo2.py:139][0m 1126400 total steps have happened
[32m[20221213 12:32:59 @agent_ppo2.py:115][0m #------------------------ Iteration 550 --------------------------#
[32m[20221213 12:32:59 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:32:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:32:59 @agent_ppo2.py:179][0m |           0.0930 |          21.7767 |          -5.7690 |
[32m[20221213 12:32:59 @agent_ppo2.py:179][0m |           0.0491 |          19.4740 |          -5.4098 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0004 |          18.9767 |          -6.6621 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0270 |          18.8030 |          -8.0268 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0428 |          18.5005 |          -8.4985 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0476 |          18.3210 |          -8.9905 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0550 |          18.1515 |          -9.3298 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0554 |          17.9674 |          -9.7595 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0593 |          17.9469 |          -9.7206 |
[32m[20221213 12:33:00 @agent_ppo2.py:179][0m |          -0.0562 |          17.7544 |         -10.5138 |
[32m[20221213 12:33:00 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 196.37
[32m[20221213 12:33:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 281.12
[32m[20221213 12:33:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 315.33
[32m[20221213 12:33:01 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 315.33
[32m[20221213 12:33:01 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 315.33
[32m[20221213 12:33:01 @agent_ppo2.py:137][0m Total time:      17.06 min
[32m[20221213 12:33:01 @agent_ppo2.py:139][0m 1128448 total steps have happened
[32m[20221213 12:33:01 @agent_ppo2.py:115][0m #------------------------ Iteration 551 --------------------------#
[32m[20221213 12:33:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:01 @agent_ppo2.py:179][0m |           0.0551 |          19.4046 |         -10.9181 |
[32m[20221213 12:33:01 @agent_ppo2.py:179][0m |           0.0310 |          17.9130 |          -8.5047 |
[32m[20221213 12:33:01 @agent_ppo2.py:179][0m |          -0.0067 |          17.2904 |         -10.4285 |
[32m[20221213 12:33:01 @agent_ppo2.py:179][0m |          -0.0130 |          17.8431 |         -10.9664 |
[32m[20221213 12:33:02 @agent_ppo2.py:179][0m |          -0.0390 |          16.7787 |         -12.0591 |
[32m[20221213 12:33:02 @agent_ppo2.py:179][0m |          -0.0459 |          16.4282 |         -12.8205 |
[32m[20221213 12:33:02 @agent_ppo2.py:179][0m |          -0.0534 |          16.1715 |         -13.4858 |
[32m[20221213 12:33:02 @agent_ppo2.py:179][0m |          -0.0503 |          15.9917 |         -13.9933 |
[32m[20221213 12:33:02 @agent_ppo2.py:179][0m |          -0.0583 |          15.8103 |         -15.0246 |
[32m[20221213 12:33:02 @agent_ppo2.py:179][0m |          -0.0600 |          15.7036 |         -15.5206 |
[32m[20221213 12:33:02 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 235.06
[32m[20221213 12:33:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 257.02
[32m[20221213 12:33:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.06
[32m[20221213 12:33:02 @agent_ppo2.py:137][0m Total time:      17.09 min
[32m[20221213 12:33:02 @agent_ppo2.py:139][0m 1130496 total steps have happened
[32m[20221213 12:33:02 @agent_ppo2.py:115][0m #------------------------ Iteration 552 --------------------------#
[32m[20221213 12:33:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:03 @agent_ppo2.py:179][0m |           0.1552 |          21.7844 |         -12.7067 |
[32m[20221213 12:33:03 @agent_ppo2.py:179][0m |           0.0524 |          20.5556 |         -10.6318 |
[32m[20221213 12:33:03 @agent_ppo2.py:179][0m |           0.0065 |          20.4638 |         -11.5459 |
[32m[20221213 12:33:03 @agent_ppo2.py:179][0m |          -0.0217 |          20.0380 |         -13.9115 |
[32m[20221213 12:33:03 @agent_ppo2.py:179][0m |          -0.0323 |          19.5841 |         -14.6556 |
[32m[20221213 12:33:03 @agent_ppo2.py:179][0m |          -0.0399 |          19.4215 |         -14.8470 |
[32m[20221213 12:33:03 @agent_ppo2.py:179][0m |          -0.0446 |          19.4263 |         -14.9025 |
[32m[20221213 12:33:04 @agent_ppo2.py:179][0m |          -0.0400 |          19.1021 |         -15.5032 |
[32m[20221213 12:33:04 @agent_ppo2.py:179][0m |          -0.0493 |          18.9914 |         -15.7953 |
[32m[20221213 12:33:04 @agent_ppo2.py:179][0m |          -0.0480 |          19.5951 |         -15.9978 |
[32m[20221213 12:33:04 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:33:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.17
[32m[20221213 12:33:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 297.35
[32m[20221213 12:33:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 302.09
[32m[20221213 12:33:04 @agent_ppo2.py:137][0m Total time:      17.12 min
[32m[20221213 12:33:04 @agent_ppo2.py:139][0m 1132544 total steps have happened
[32m[20221213 12:33:04 @agent_ppo2.py:115][0m #------------------------ Iteration 553 --------------------------#
[32m[20221213 12:33:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:04 @agent_ppo2.py:179][0m |           0.0722 |          18.4556 |          -9.1440 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0037 |          17.2864 |          -6.5862 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0311 |          16.9698 |          -8.2122 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0432 |          16.6441 |          -9.2292 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0505 |          16.4024 |          -9.7842 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0550 |          16.1658 |         -10.4947 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0622 |          16.0018 |         -11.3424 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0628 |          15.9214 |         -11.6787 |
[32m[20221213 12:33:05 @agent_ppo2.py:179][0m |          -0.0645 |          15.6650 |         -12.0771 |
[32m[20221213 12:33:06 @agent_ppo2.py:179][0m |          -0.0660 |          15.6141 |         -12.6237 |
[32m[20221213 12:33:06 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:33:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.54
[32m[20221213 12:33:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 273.45
[32m[20221213 12:33:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 272.55
[32m[20221213 12:33:06 @agent_ppo2.py:137][0m Total time:      17.15 min
[32m[20221213 12:33:06 @agent_ppo2.py:139][0m 1134592 total steps have happened
[32m[20221213 12:33:06 @agent_ppo2.py:115][0m #------------------------ Iteration 554 --------------------------#
[32m[20221213 12:33:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:06 @agent_ppo2.py:179][0m |           0.0931 |          22.4639 |         -13.4341 |
[32m[20221213 12:33:06 @agent_ppo2.py:179][0m |           0.0686 |          21.3406 |         -11.4224 |
[32m[20221213 12:33:06 @agent_ppo2.py:179][0m |           0.0095 |          20.6384 |         -13.7883 |
[32m[20221213 12:33:07 @agent_ppo2.py:179][0m |          -0.0177 |          20.3892 |         -14.9229 |
[32m[20221213 12:33:07 @agent_ppo2.py:179][0m |          -0.0220 |          20.2775 |         -15.8401 |
[32m[20221213 12:33:07 @agent_ppo2.py:179][0m |          -0.0323 |          20.1762 |         -15.7122 |
[32m[20221213 12:33:07 @agent_ppo2.py:179][0m |          -0.0302 |          21.6590 |         -16.7705 |
[32m[20221213 12:33:07 @agent_ppo2.py:179][0m |          -0.0168 |          19.5820 |         -15.4259 |
[32m[20221213 12:33:07 @agent_ppo2.py:179][0m |          -0.0440 |          19.5159 |         -17.2502 |
[32m[20221213 12:33:07 @agent_ppo2.py:179][0m |          -0.0430 |          19.5326 |         -17.8618 |
[32m[20221213 12:33:07 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.57
[32m[20221213 12:33:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 288.30
[32m[20221213 12:33:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 275.50
[32m[20221213 12:33:07 @agent_ppo2.py:137][0m Total time:      17.18 min
[32m[20221213 12:33:07 @agent_ppo2.py:139][0m 1136640 total steps have happened
[32m[20221213 12:33:07 @agent_ppo2.py:115][0m #------------------------ Iteration 555 --------------------------#
[32m[20221213 12:33:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:08 @agent_ppo2.py:179][0m |           0.0816 |          22.1935 |         -12.3971 |
[32m[20221213 12:33:08 @agent_ppo2.py:179][0m |           0.0294 |          21.1320 |          -9.7831 |
[32m[20221213 12:33:08 @agent_ppo2.py:179][0m |          -0.0110 |          20.6127 |         -13.1524 |
[32m[20221213 12:33:08 @agent_ppo2.py:179][0m |          -0.0314 |          20.3459 |         -15.1827 |
[32m[20221213 12:33:08 @agent_ppo2.py:179][0m |          -0.0354 |          20.1971 |         -14.6793 |
[32m[20221213 12:33:08 @agent_ppo2.py:179][0m |          -0.0418 |          19.7492 |         -15.8940 |
[32m[20221213 12:33:09 @agent_ppo2.py:179][0m |          -0.0551 |          19.5133 |         -16.5110 |
[32m[20221213 12:33:09 @agent_ppo2.py:179][0m |          -0.0584 |          19.4069 |         -17.2336 |
[32m[20221213 12:33:09 @agent_ppo2.py:179][0m |          -0.0603 |          19.2078 |         -17.3793 |
[32m[20221213 12:33:09 @agent_ppo2.py:179][0m |          -0.0654 |          19.1496 |         -18.2704 |
[32m[20221213 12:33:09 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:33:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 202.07
[32m[20221213 12:33:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 274.79
[32m[20221213 12:33:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 275.00
[32m[20221213 12:33:09 @agent_ppo2.py:137][0m Total time:      17.21 min
[32m[20221213 12:33:09 @agent_ppo2.py:139][0m 1138688 total steps have happened
[32m[20221213 12:33:09 @agent_ppo2.py:115][0m #------------------------ Iteration 556 --------------------------#
[32m[20221213 12:33:09 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:33:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:10 @agent_ppo2.py:179][0m |           0.0644 |          18.8514 |         -18.6161 |
[32m[20221213 12:33:10 @agent_ppo2.py:179][0m |           0.0585 |          17.2221 |         -14.3537 |
[32m[20221213 12:33:10 @agent_ppo2.py:179][0m |           0.0202 |          16.8730 |         -13.4980 |
[32m[20221213 12:33:10 @agent_ppo2.py:179][0m |          -0.0250 |          16.6158 |         -14.6266 |
[32m[20221213 12:33:10 @agent_ppo2.py:179][0m |          -0.0388 |          16.3983 |         -16.6856 |
[32m[20221213 12:33:10 @agent_ppo2.py:179][0m |          -0.0433 |          16.2381 |         -17.8966 |
[32m[20221213 12:33:10 @agent_ppo2.py:179][0m |          -0.0455 |          16.0229 |         -18.0202 |
[32m[20221213 12:33:11 @agent_ppo2.py:179][0m |          -0.0506 |          15.9728 |         -19.0200 |
[32m[20221213 12:33:11 @agent_ppo2.py:179][0m |          -0.0580 |          15.8794 |         -20.0358 |
[32m[20221213 12:33:11 @agent_ppo2.py:179][0m |          -0.0521 |          15.6892 |         -20.4181 |
[32m[20221213 12:33:11 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:33:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 211.91
[32m[20221213 12:33:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.04
[32m[20221213 12:33:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.64
[32m[20221213 12:33:11 @agent_ppo2.py:137][0m Total time:      17.24 min
[32m[20221213 12:33:11 @agent_ppo2.py:139][0m 1140736 total steps have happened
[32m[20221213 12:33:11 @agent_ppo2.py:115][0m #------------------------ Iteration 557 --------------------------#
[32m[20221213 12:33:11 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:33:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:11 @agent_ppo2.py:179][0m |           0.0774 |          20.2048 |         -15.4168 |
[32m[20221213 12:33:12 @agent_ppo2.py:179][0m |           0.0321 |          19.3840 |         -14.9323 |
[32m[20221213 12:33:12 @agent_ppo2.py:179][0m |           0.0106 |          19.7535 |         -15.3951 |
[32m[20221213 12:33:12 @agent_ppo2.py:179][0m |          -0.0163 |          19.0394 |         -17.1493 |
[32m[20221213 12:33:12 @agent_ppo2.py:179][0m |          -0.0298 |          18.5949 |         -17.7911 |
[32m[20221213 12:33:12 @agent_ppo2.py:179][0m |          -0.0377 |          18.5428 |         -18.4523 |
[32m[20221213 12:33:12 @agent_ppo2.py:179][0m |          -0.0420 |          18.4215 |         -19.0772 |
[32m[20221213 12:33:12 @agent_ppo2.py:179][0m |          -0.0382 |          18.7365 |         -19.6097 |
[32m[20221213 12:33:13 @agent_ppo2.py:179][0m |          -0.0483 |          18.3436 |         -19.8873 |
[32m[20221213 12:33:13 @agent_ppo2.py:179][0m |          -0.0513 |          18.1983 |         -19.7712 |
[32m[20221213 12:33:13 @agent_ppo2.py:124][0m Policy update time: 1.48 s
[32m[20221213 12:33:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.45
[32m[20221213 12:33:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 285.08
[32m[20221213 12:33:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.91
[32m[20221213 12:33:13 @agent_ppo2.py:137][0m Total time:      17.27 min
[32m[20221213 12:33:13 @agent_ppo2.py:139][0m 1142784 total steps have happened
[32m[20221213 12:33:13 @agent_ppo2.py:115][0m #------------------------ Iteration 558 --------------------------#
[32m[20221213 12:33:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:13 @agent_ppo2.py:179][0m |           0.0686 |          16.7001 |         -16.5477 |
[32m[20221213 12:33:13 @agent_ppo2.py:179][0m |           0.0346 |          15.8862 |          -8.3945 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |           0.0167 |          16.5853 |         -10.8955 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |           0.0052 |          17.5616 |         -11.1664 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |          -0.0308 |          14.9328 |         -12.3305 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |          -0.0387 |          14.7071 |         -13.2295 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |          -0.0500 |          14.6542 |         -13.5011 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |          -0.0498 |          14.5291 |         -13.9731 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |          -0.0514 |          14.4447 |         -14.5785 |
[32m[20221213 12:33:14 @agent_ppo2.py:179][0m |          -0.0565 |          14.3697 |         -14.7682 |
[32m[20221213 12:33:14 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:33:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 166.48
[32m[20221213 12:33:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 269.85
[32m[20221213 12:33:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 294.29
[32m[20221213 12:33:15 @agent_ppo2.py:137][0m Total time:      17.30 min
[32m[20221213 12:33:15 @agent_ppo2.py:139][0m 1144832 total steps have happened
[32m[20221213 12:33:15 @agent_ppo2.py:115][0m #------------------------ Iteration 559 --------------------------#
[32m[20221213 12:33:15 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:33:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:15 @agent_ppo2.py:179][0m |           0.0613 |          18.7501 |         -20.6333 |
[32m[20221213 12:33:15 @agent_ppo2.py:179][0m |           0.0514 |          16.3475 |         -16.1461 |
[32m[20221213 12:33:15 @agent_ppo2.py:179][0m |           0.0015 |          15.2134 |         -19.9926 |
[32m[20221213 12:33:15 @agent_ppo2.py:179][0m |          -0.0208 |          14.3860 |         -20.8893 |
[32m[20221213 12:33:16 @agent_ppo2.py:179][0m |          -0.0271 |          13.8415 |         -21.8397 |
[32m[20221213 12:33:16 @agent_ppo2.py:179][0m |          -0.0279 |          14.1208 |         -22.8396 |
[32m[20221213 12:33:16 @agent_ppo2.py:179][0m |          -0.0274 |          13.0007 |         -21.6534 |
[32m[20221213 12:33:16 @agent_ppo2.py:179][0m |          -0.0382 |          12.6557 |         -23.2860 |
[32m[20221213 12:33:16 @agent_ppo2.py:179][0m |          -0.0420 |          12.4614 |         -23.7189 |
[32m[20221213 12:33:16 @agent_ppo2.py:179][0m |          -0.0434 |          12.1475 |         -24.4865 |
[32m[20221213 12:33:16 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 284.57
[32m[20221213 12:33:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 296.56
[32m[20221213 12:33:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 293.15
[32m[20221213 12:33:16 @agent_ppo2.py:137][0m Total time:      17.33 min
[32m[20221213 12:33:16 @agent_ppo2.py:139][0m 1146880 total steps have happened
[32m[20221213 12:33:16 @agent_ppo2.py:115][0m #------------------------ Iteration 560 --------------------------#
[32m[20221213 12:33:17 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:33:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:17 @agent_ppo2.py:179][0m |           0.0567 |          24.2562 |         -20.2009 |
[32m[20221213 12:33:17 @agent_ppo2.py:179][0m |           0.0335 |          23.8826 |         -19.0624 |
[32m[20221213 12:33:17 @agent_ppo2.py:179][0m |          -0.0015 |          22.3902 |         -19.8258 |
[32m[20221213 12:33:17 @agent_ppo2.py:179][0m |          -0.0169 |          22.1354 |         -21.7296 |
[32m[20221213 12:33:17 @agent_ppo2.py:179][0m |          -0.0151 |          22.1231 |         -21.5209 |
[32m[20221213 12:33:17 @agent_ppo2.py:179][0m |          -0.0336 |          21.8146 |         -23.4840 |
[32m[20221213 12:33:18 @agent_ppo2.py:179][0m |          -0.0305 |          23.8482 |         -23.7549 |
[32m[20221213 12:33:18 @agent_ppo2.py:179][0m |          -0.0441 |          21.5476 |         -24.4142 |
[32m[20221213 12:33:18 @agent_ppo2.py:179][0m |          -0.0395 |          21.4586 |         -24.8298 |
[32m[20221213 12:33:18 @agent_ppo2.py:179][0m |          -0.0468 |          21.3503 |         -25.1573 |
[32m[20221213 12:33:18 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:33:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.36
[32m[20221213 12:33:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 305.46
[32m[20221213 12:33:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.77
[32m[20221213 12:33:18 @agent_ppo2.py:137][0m Total time:      17.36 min
[32m[20221213 12:33:18 @agent_ppo2.py:139][0m 1148928 total steps have happened
[32m[20221213 12:33:18 @agent_ppo2.py:115][0m #------------------------ Iteration 561 --------------------------#
[32m[20221213 12:33:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |           0.0429 |           8.5867 |         -11.9338 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |           0.0231 |           7.8594 |         -10.2456 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |           0.0138 |           7.5691 |          -8.9815 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |           0.0226 |           7.4632 |          -5.3377 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |          -0.0100 |           7.3682 |          -6.6214 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |          -0.0342 |           7.2914 |         -13.4246 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |          -0.0496 |           7.2329 |         -15.2966 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |          -0.0507 |           7.1819 |         -14.9454 |
[32m[20221213 12:33:19 @agent_ppo2.py:179][0m |          -0.0494 |           7.1453 |         -14.7190 |
[32m[20221213 12:33:20 @agent_ppo2.py:179][0m |          -0.0636 |           7.1383 |         -15.6052 |
[32m[20221213 12:33:20 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 67.69
[32m[20221213 12:33:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 244.34
[32m[20221213 12:33:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 237.20
[32m[20221213 12:33:20 @agent_ppo2.py:137][0m Total time:      17.38 min
[32m[20221213 12:33:20 @agent_ppo2.py:139][0m 1150976 total steps have happened
[32m[20221213 12:33:20 @agent_ppo2.py:115][0m #------------------------ Iteration 562 --------------------------#
[32m[20221213 12:33:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:20 @agent_ppo2.py:179][0m |           0.1081 |          21.5259 |         -18.8665 |
[32m[20221213 12:33:20 @agent_ppo2.py:179][0m |           0.0749 |          19.9897 |         -14.9210 |
[32m[20221213 12:33:20 @agent_ppo2.py:179][0m |           0.0165 |          19.3312 |         -17.6830 |
[32m[20221213 12:33:21 @agent_ppo2.py:179][0m |          -0.0100 |          18.8566 |         -19.8123 |
[32m[20221213 12:33:21 @agent_ppo2.py:179][0m |          -0.0273 |          18.5236 |         -21.2400 |
[32m[20221213 12:33:21 @agent_ppo2.py:179][0m |          -0.0300 |          18.3186 |         -22.0818 |
[32m[20221213 12:33:21 @agent_ppo2.py:179][0m |          -0.0364 |          18.0620 |         -22.7089 |
[32m[20221213 12:33:21 @agent_ppo2.py:179][0m |          -0.0457 |          17.8179 |         -23.7114 |
[32m[20221213 12:33:21 @agent_ppo2.py:179][0m |          -0.0543 |          17.6568 |         -24.6412 |
[32m[20221213 12:33:21 @agent_ppo2.py:179][0m |          -0.0553 |          17.5423 |         -24.8509 |
[32m[20221213 12:33:21 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.42
[32m[20221213 12:33:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 268.98
[32m[20221213 12:33:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.57
[32m[20221213 12:33:21 @agent_ppo2.py:137][0m Total time:      17.41 min
[32m[20221213 12:33:21 @agent_ppo2.py:139][0m 1153024 total steps have happened
[32m[20221213 12:33:21 @agent_ppo2.py:115][0m #------------------------ Iteration 563 --------------------------#
[32m[20221213 12:33:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:22 @agent_ppo2.py:179][0m |           0.0793 |          15.7157 |         -19.1894 |
[32m[20221213 12:33:22 @agent_ppo2.py:179][0m |           0.0534 |          14.4940 |         -16.2528 |
[32m[20221213 12:33:22 @agent_ppo2.py:179][0m |           0.0042 |          13.9410 |         -20.1384 |
[32m[20221213 12:33:22 @agent_ppo2.py:179][0m |          -0.0155 |          13.6651 |         -21.2431 |
[32m[20221213 12:33:22 @agent_ppo2.py:179][0m |          -0.0211 |          13.4172 |         -22.8262 |
[32m[20221213 12:33:22 @agent_ppo2.py:179][0m |          -0.0256 |          13.2166 |         -23.3282 |
[32m[20221213 12:33:23 @agent_ppo2.py:179][0m |          -0.0303 |          13.7337 |         -23.6290 |
[32m[20221213 12:33:23 @agent_ppo2.py:179][0m |          -0.0368 |          12.9316 |         -24.8299 |
[32m[20221213 12:33:23 @agent_ppo2.py:179][0m |          -0.0386 |          12.8970 |         -25.5005 |
[32m[20221213 12:33:23 @agent_ppo2.py:179][0m |          -0.0416 |          12.6903 |         -25.9974 |
[32m[20221213 12:33:23 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:33:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 180.25
[32m[20221213 12:33:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 278.82
[32m[20221213 12:33:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 309.57
[32m[20221213 12:33:23 @agent_ppo2.py:137][0m Total time:      17.44 min
[32m[20221213 12:33:23 @agent_ppo2.py:139][0m 1155072 total steps have happened
[32m[20221213 12:33:23 @agent_ppo2.py:115][0m #------------------------ Iteration 564 --------------------------#
[32m[20221213 12:33:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |           0.0570 |           8.1832 |         -23.8720 |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |           0.1143 |           7.4222 |         -17.9639 |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |           0.0428 |           7.0828 |         -19.7517 |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |           0.0224 |           6.8597 |         -18.8997 |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |          -0.0016 |           6.6777 |         -20.9326 |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |           0.0029 |           6.4982 |         -21.1342 |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |          -0.0146 |           6.3379 |         -23.8156 |
[32m[20221213 12:33:24 @agent_ppo2.py:179][0m |          -0.0152 |           6.3133 |         -24.5613 |
[32m[20221213 12:33:25 @agent_ppo2.py:179][0m |          -0.0202 |           6.1611 |         -25.6009 |
[32m[20221213 12:33:25 @agent_ppo2.py:179][0m |          -0.0188 |           6.0909 |         -22.8290 |
[32m[20221213 12:33:25 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:33:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.15
[32m[20221213 12:33:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 234.70
[32m[20221213 12:33:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.59
[32m[20221213 12:33:25 @agent_ppo2.py:137][0m Total time:      17.47 min
[32m[20221213 12:33:25 @agent_ppo2.py:139][0m 1157120 total steps have happened
[32m[20221213 12:33:25 @agent_ppo2.py:115][0m #------------------------ Iteration 565 --------------------------#
[32m[20221213 12:33:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:25 @agent_ppo2.py:179][0m |           0.0600 |          15.8081 |         -22.7751 |
[32m[20221213 12:33:25 @agent_ppo2.py:179][0m |           0.0239 |          14.5151 |         -23.9657 |
[32m[20221213 12:33:25 @agent_ppo2.py:179][0m |          -0.0070 |          14.1585 |         -25.4984 |
[32m[20221213 12:33:26 @agent_ppo2.py:179][0m |          -0.0199 |          13.8282 |         -25.8079 |
[32m[20221213 12:33:26 @agent_ppo2.py:179][0m |          -0.0155 |          15.9475 |         -26.7204 |
[32m[20221213 12:33:26 @agent_ppo2.py:179][0m |          -0.0321 |          13.5575 |         -28.2739 |
[32m[20221213 12:33:26 @agent_ppo2.py:179][0m |          -0.0400 |          13.2873 |         -28.3970 |
[32m[20221213 12:33:26 @agent_ppo2.py:179][0m |          -0.0329 |          13.6546 |         -29.4870 |
[32m[20221213 12:33:26 @agent_ppo2.py:179][0m |          -0.0408 |          13.0467 |         -29.0214 |
[32m[20221213 12:33:26 @agent_ppo2.py:179][0m |          -0.0492 |          12.9690 |         -31.3919 |
[32m[20221213 12:33:26 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 171.75
[32m[20221213 12:33:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 285.86
[32m[20221213 12:33:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.06
[32m[20221213 12:33:26 @agent_ppo2.py:137][0m Total time:      17.50 min
[32m[20221213 12:33:26 @agent_ppo2.py:139][0m 1159168 total steps have happened
[32m[20221213 12:33:27 @agent_ppo2.py:115][0m #------------------------ Iteration 566 --------------------------#
[32m[20221213 12:33:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:27 @agent_ppo2.py:179][0m |           0.0761 |          22.8447 |         -25.4173 |
[32m[20221213 12:33:27 @agent_ppo2.py:179][0m |           0.0973 |          23.3542 |         -17.2378 |
[32m[20221213 12:33:27 @agent_ppo2.py:179][0m |           0.0362 |          20.5314 |         -20.0529 |
[32m[20221213 12:33:27 @agent_ppo2.py:179][0m |          -0.0059 |          20.2776 |         -25.1665 |
[32m[20221213 12:33:27 @agent_ppo2.py:179][0m |          -0.0248 |          20.1705 |         -27.0636 |
[32m[20221213 12:33:28 @agent_ppo2.py:179][0m |          -0.0279 |          20.0249 |         -27.4627 |
[32m[20221213 12:33:28 @agent_ppo2.py:179][0m |          -0.0358 |          19.9134 |         -28.4210 |
[32m[20221213 12:33:28 @agent_ppo2.py:179][0m |          -0.0369 |          19.8240 |         -28.3523 |
[32m[20221213 12:33:28 @agent_ppo2.py:179][0m |          -0.0392 |          19.8142 |         -29.1665 |
[32m[20221213 12:33:28 @agent_ppo2.py:179][0m |          -0.0355 |          19.8467 |         -29.8591 |
[32m[20221213 12:33:28 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.45
[32m[20221213 12:33:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 289.50
[32m[20221213 12:33:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.76
[32m[20221213 12:33:28 @agent_ppo2.py:137][0m Total time:      17.53 min
[32m[20221213 12:33:28 @agent_ppo2.py:139][0m 1161216 total steps have happened
[32m[20221213 12:33:28 @agent_ppo2.py:115][0m #------------------------ Iteration 567 --------------------------#
[32m[20221213 12:33:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |           0.0813 |          22.8881 |         -25.1177 |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |           0.0499 |          22.0247 |         -23.0678 |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |           0.0240 |          21.4736 |         -22.8985 |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |          -0.0121 |          21.1525 |         -25.4837 |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |          -0.0274 |          20.9523 |         -26.6012 |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |          -0.0343 |          20.7796 |         -27.7235 |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |          -0.0371 |          20.8223 |         -28.8269 |
[32m[20221213 12:33:29 @agent_ppo2.py:179][0m |          -0.0360 |          21.7645 |         -29.9699 |
[32m[20221213 12:33:30 @agent_ppo2.py:179][0m |          -0.0414 |          20.5881 |         -30.3121 |
[32m[20221213 12:33:30 @agent_ppo2.py:179][0m |          -0.0456 |          20.1645 |         -30.6793 |
[32m[20221213 12:33:30 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:33:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 252.40
[32m[20221213 12:33:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 283.91
[32m[20221213 12:33:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 294.11
[32m[20221213 12:33:30 @agent_ppo2.py:137][0m Total time:      17.55 min
[32m[20221213 12:33:30 @agent_ppo2.py:139][0m 1163264 total steps have happened
[32m[20221213 12:33:30 @agent_ppo2.py:115][0m #------------------------ Iteration 568 --------------------------#
[32m[20221213 12:33:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:30 @agent_ppo2.py:179][0m |           0.0650 |          21.7568 |         -24.9062 |
[32m[20221213 12:33:30 @agent_ppo2.py:179][0m |           0.0237 |          21.3353 |         -24.5370 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0023 |          21.0771 |         -26.3943 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0191 |          20.8901 |         -27.5468 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0071 |          23.4567 |         -28.5124 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0201 |          22.6264 |         -28.8938 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0402 |          20.5952 |         -29.9225 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0417 |          20.4358 |         -30.6574 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0391 |          20.3234 |         -30.7671 |
[32m[20221213 12:33:31 @agent_ppo2.py:179][0m |          -0.0423 |          20.3030 |         -31.4037 |
[32m[20221213 12:33:31 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 281.72
[32m[20221213 12:33:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 298.12
[32m[20221213 12:33:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 273.71
[32m[20221213 12:33:32 @agent_ppo2.py:137][0m Total time:      17.58 min
[32m[20221213 12:33:32 @agent_ppo2.py:139][0m 1165312 total steps have happened
[32m[20221213 12:33:32 @agent_ppo2.py:115][0m #------------------------ Iteration 569 --------------------------#
[32m[20221213 12:33:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:32 @agent_ppo2.py:179][0m |           0.0653 |          21.6496 |         -26.3561 |
[32m[20221213 12:33:32 @agent_ppo2.py:179][0m |           0.0316 |          19.0223 |         -25.9058 |
[32m[20221213 12:33:32 @agent_ppo2.py:179][0m |          -0.0080 |          17.4622 |         -28.6076 |
[32m[20221213 12:33:32 @agent_ppo2.py:179][0m |          -0.0215 |          17.1067 |         -28.4493 |
[32m[20221213 12:33:33 @agent_ppo2.py:179][0m |          -0.0291 |          18.0472 |         -30.1926 |
[32m[20221213 12:33:33 @agent_ppo2.py:179][0m |          -0.0338 |          16.6690 |         -30.8810 |
[32m[20221213 12:33:33 @agent_ppo2.py:179][0m |          -0.0416 |          16.4976 |         -31.3858 |
[32m[20221213 12:33:33 @agent_ppo2.py:179][0m |          -0.0446 |          16.3043 |         -32.9112 |
[32m[20221213 12:33:33 @agent_ppo2.py:179][0m |          -0.0413 |          16.1499 |         -32.7245 |
[32m[20221213 12:33:33 @agent_ppo2.py:179][0m |          -0.0449 |          16.1409 |         -33.1461 |
[32m[20221213 12:33:33 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 199.35
[32m[20221213 12:33:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 263.85
[32m[20221213 12:33:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.31
[32m[20221213 12:33:33 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 320.31
[32m[20221213 12:33:33 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 320.31
[32m[20221213 12:33:33 @agent_ppo2.py:137][0m Total time:      17.61 min
[32m[20221213 12:33:33 @agent_ppo2.py:139][0m 1167360 total steps have happened
[32m[20221213 12:33:33 @agent_ppo2.py:115][0m #------------------------ Iteration 570 --------------------------#
[32m[20221213 12:33:34 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:33:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:34 @agent_ppo2.py:179][0m |           0.0850 |          21.2209 |         -29.0290 |
[32m[20221213 12:33:34 @agent_ppo2.py:179][0m |           0.0840 |          20.1360 |         -23.1475 |
[32m[20221213 12:33:34 @agent_ppo2.py:179][0m |           0.0184 |          20.8944 |         -26.4993 |
[32m[20221213 12:33:34 @agent_ppo2.py:179][0m |          -0.0088 |          19.6901 |         -27.9012 |
[32m[20221213 12:33:34 @agent_ppo2.py:179][0m |          -0.0270 |          19.2094 |         -29.2629 |
[32m[20221213 12:33:34 @agent_ppo2.py:179][0m |          -0.0359 |          18.9111 |         -29.4852 |
[32m[20221213 12:33:35 @agent_ppo2.py:179][0m |          -0.0439 |          18.7476 |         -30.6755 |
[32m[20221213 12:33:35 @agent_ppo2.py:179][0m |          -0.0447 |          18.5869 |         -31.6224 |
[32m[20221213 12:33:35 @agent_ppo2.py:179][0m |          -0.0355 |          19.0839 |         -32.0391 |
[32m[20221213 12:33:35 @agent_ppo2.py:179][0m |          -0.0374 |          18.3869 |         -32.4117 |
[32m[20221213 12:33:35 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:33:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.21
[32m[20221213 12:33:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 285.26
[32m[20221213 12:33:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 233.20
[32m[20221213 12:33:35 @agent_ppo2.py:137][0m Total time:      17.64 min
[32m[20221213 12:33:35 @agent_ppo2.py:139][0m 1169408 total steps have happened
[32m[20221213 12:33:35 @agent_ppo2.py:115][0m #------------------------ Iteration 571 --------------------------#
[32m[20221213 12:33:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:35 @agent_ppo2.py:179][0m |           0.0869 |          22.6583 |         -30.7350 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |           0.0719 |          21.9146 |         -24.3846 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |           0.0169 |          21.2239 |         -28.6432 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |          -0.0158 |          20.7368 |         -31.6882 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |          -0.0302 |          20.5687 |         -33.1231 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |          -0.0386 |          20.3312 |         -34.4305 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |          -0.0432 |          20.0211 |         -35.6828 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |          -0.0496 |          19.8960 |         -36.6349 |
[32m[20221213 12:33:36 @agent_ppo2.py:179][0m |          -0.0507 |          19.7100 |         -37.6918 |
[32m[20221213 12:33:37 @agent_ppo2.py:179][0m |          -0.0459 |          19.5747 |         -37.2681 |
[32m[20221213 12:33:37 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.61
[32m[20221213 12:33:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 294.92
[32m[20221213 12:33:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 318.10
[32m[20221213 12:33:37 @agent_ppo2.py:137][0m Total time:      17.67 min
[32m[20221213 12:33:37 @agent_ppo2.py:139][0m 1171456 total steps have happened
[32m[20221213 12:33:37 @agent_ppo2.py:115][0m #------------------------ Iteration 572 --------------------------#
[32m[20221213 12:33:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:37 @agent_ppo2.py:179][0m |           0.0559 |          24.1069 |         -32.2682 |
[32m[20221213 12:33:37 @agent_ppo2.py:179][0m |           0.0234 |          22.8374 |         -29.4207 |
[32m[20221213 12:33:37 @agent_ppo2.py:179][0m |          -0.0058 |          22.0954 |         -32.1658 |
[32m[20221213 12:33:38 @agent_ppo2.py:179][0m |          -0.0132 |          21.8349 |         -33.5307 |
[32m[20221213 12:33:38 @agent_ppo2.py:179][0m |          -0.0277 |          21.0622 |         -35.0225 |
[32m[20221213 12:33:38 @agent_ppo2.py:179][0m |          -0.0395 |          20.7661 |         -35.9115 |
[32m[20221213 12:33:38 @agent_ppo2.py:179][0m |          -0.0433 |          20.6020 |         -37.3477 |
[32m[20221213 12:33:38 @agent_ppo2.py:179][0m |          -0.0466 |          20.3627 |         -38.4005 |
[32m[20221213 12:33:38 @agent_ppo2.py:179][0m |          -0.0440 |          20.5945 |         -39.1413 |
[32m[20221213 12:33:38 @agent_ppo2.py:179][0m |          -0.0491 |          19.9326 |         -39.9902 |
[32m[20221213 12:33:38 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:33:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 264.01
[32m[20221213 12:33:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 299.69
[32m[20221213 12:33:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.55
[32m[20221213 12:33:38 @agent_ppo2.py:137][0m Total time:      17.70 min
[32m[20221213 12:33:38 @agent_ppo2.py:139][0m 1173504 total steps have happened
[32m[20221213 12:33:38 @agent_ppo2.py:115][0m #------------------------ Iteration 573 --------------------------#
[32m[20221213 12:33:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:39 @agent_ppo2.py:179][0m |           0.0691 |          24.4767 |         -33.9384 |
[32m[20221213 12:33:39 @agent_ppo2.py:179][0m |           0.0486 |          22.8338 |         -27.5906 |
[32m[20221213 12:33:39 @agent_ppo2.py:179][0m |           0.0051 |          22.2533 |         -31.3972 |
[32m[20221213 12:33:39 @agent_ppo2.py:179][0m |          -0.0238 |          21.8495 |         -32.1377 |
[32m[20221213 12:33:39 @agent_ppo2.py:179][0m |          -0.0337 |          21.7713 |         -33.0991 |
[32m[20221213 12:33:40 @agent_ppo2.py:179][0m |          -0.0437 |          21.4792 |         -33.5553 |
[32m[20221213 12:33:40 @agent_ppo2.py:179][0m |          -0.0423 |          21.2688 |         -35.0992 |
[32m[20221213 12:33:40 @agent_ppo2.py:179][0m |          -0.0444 |          21.0816 |         -35.7205 |
[32m[20221213 12:33:40 @agent_ppo2.py:179][0m |          -0.0468 |          20.9961 |         -36.9361 |
[32m[20221213 12:33:40 @agent_ppo2.py:179][0m |          -0.0503 |          21.2886 |         -37.3806 |
[32m[20221213 12:33:40 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.11
[32m[20221213 12:33:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 270.74
[32m[20221213 12:33:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 275.57
[32m[20221213 12:33:40 @agent_ppo2.py:137][0m Total time:      17.72 min
[32m[20221213 12:33:40 @agent_ppo2.py:139][0m 1175552 total steps have happened
[32m[20221213 12:33:40 @agent_ppo2.py:115][0m #------------------------ Iteration 574 --------------------------#
[32m[20221213 12:33:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |           0.0731 |          14.3986 |         -34.8583 |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |           0.0539 |          13.5545 |         -27.0364 |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |           0.0017 |          13.2143 |         -32.9045 |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |          -0.0246 |          12.9859 |         -38.7315 |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |          -0.0259 |          12.8113 |         -39.3040 |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |          -0.0337 |          12.7537 |         -40.2470 |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |          -0.0313 |          12.5218 |         -41.0085 |
[32m[20221213 12:33:41 @agent_ppo2.py:179][0m |          -0.0327 |          12.5276 |         -41.4702 |
[32m[20221213 12:33:42 @agent_ppo2.py:179][0m |          -0.0357 |          12.3907 |         -41.3157 |
[32m[20221213 12:33:42 @agent_ppo2.py:179][0m |          -0.0291 |          12.2654 |         -38.1590 |
[32m[20221213 12:33:42 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.20
[32m[20221213 12:33:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 283.88
[32m[20221213 12:33:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 292.38
[32m[20221213 12:33:42 @agent_ppo2.py:137][0m Total time:      17.75 min
[32m[20221213 12:33:42 @agent_ppo2.py:139][0m 1177600 total steps have happened
[32m[20221213 12:33:42 @agent_ppo2.py:115][0m #------------------------ Iteration 575 --------------------------#
[32m[20221213 12:33:42 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:33:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:42 @agent_ppo2.py:179][0m |           0.0608 |          17.3881 |         -36.0256 |
[32m[20221213 12:33:42 @agent_ppo2.py:179][0m |           0.0391 |          16.4504 |         -34.6432 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |           0.0085 |          18.1926 |         -35.6464 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |          -0.0236 |          15.9928 |         -38.2199 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |          -0.0243 |          15.7758 |         -38.8154 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |          -0.0369 |          15.6136 |         -40.5547 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |          -0.0313 |          15.4316 |         -40.3850 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |          -0.0384 |          15.3660 |         -41.9005 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |          -0.0408 |          15.2392 |         -41.7485 |
[32m[20221213 12:33:43 @agent_ppo2.py:179][0m |          -0.0421 |          15.1501 |         -43.0923 |
[32m[20221213 12:33:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:33:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.19
[32m[20221213 12:33:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 300.84
[32m[20221213 12:33:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 278.54
[32m[20221213 12:33:44 @agent_ppo2.py:137][0m Total time:      17.78 min
[32m[20221213 12:33:44 @agent_ppo2.py:139][0m 1179648 total steps have happened
[32m[20221213 12:33:44 @agent_ppo2.py:115][0m #------------------------ Iteration 576 --------------------------#
[32m[20221213 12:33:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:44 @agent_ppo2.py:179][0m |           0.0589 |          21.7677 |         -34.1317 |
[32m[20221213 12:33:44 @agent_ppo2.py:179][0m |           0.0268 |          21.2499 |         -33.6587 |
[32m[20221213 12:33:44 @agent_ppo2.py:179][0m |           0.0074 |          20.4335 |         -33.9489 |
[32m[20221213 12:33:44 @agent_ppo2.py:179][0m |          -0.0225 |          20.0872 |         -37.4165 |
[32m[20221213 12:33:44 @agent_ppo2.py:179][0m |          -0.0363 |          19.9491 |         -38.8173 |
[32m[20221213 12:33:45 @agent_ppo2.py:179][0m |          -0.0338 |          19.7062 |         -38.9548 |
[32m[20221213 12:33:45 @agent_ppo2.py:179][0m |          -0.0382 |          19.6732 |         -40.2628 |
[32m[20221213 12:33:45 @agent_ppo2.py:179][0m |          -0.0364 |          20.1868 |         -41.8110 |
[32m[20221213 12:33:45 @agent_ppo2.py:179][0m |          -0.0448 |          19.7102 |         -42.7082 |
[32m[20221213 12:33:45 @agent_ppo2.py:179][0m |          -0.0471 |          19.1728 |         -42.6334 |
[32m[20221213 12:33:45 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:33:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.69
[32m[20221213 12:33:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 295.23
[32m[20221213 12:33:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 298.06
[32m[20221213 12:33:45 @agent_ppo2.py:137][0m Total time:      17.81 min
[32m[20221213 12:33:45 @agent_ppo2.py:139][0m 1181696 total steps have happened
[32m[20221213 12:33:45 @agent_ppo2.py:115][0m #------------------------ Iteration 577 --------------------------#
[32m[20221213 12:33:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:46 @agent_ppo2.py:179][0m |           0.0784 |          19.5972 |         -38.7957 |
[32m[20221213 12:33:46 @agent_ppo2.py:179][0m |           0.0417 |          18.0454 |         -33.7560 |
[32m[20221213 12:33:46 @agent_ppo2.py:179][0m |          -0.0106 |          17.2881 |         -37.7844 |
[32m[20221213 12:33:46 @agent_ppo2.py:179][0m |          -0.0285 |          16.8369 |         -39.4819 |
[32m[20221213 12:33:46 @agent_ppo2.py:179][0m |          -0.0358 |          16.3519 |         -41.6139 |
[32m[20221213 12:33:46 @agent_ppo2.py:179][0m |          -0.0346 |          16.4812 |         -42.3953 |
[32m[20221213 12:33:46 @agent_ppo2.py:179][0m |          -0.0502 |          15.9718 |         -43.2452 |
[32m[20221213 12:33:47 @agent_ppo2.py:179][0m |          -0.0500 |          15.6358 |         -43.9332 |
[32m[20221213 12:33:47 @agent_ppo2.py:179][0m |          -0.0458 |          16.6555 |         -44.9594 |
[32m[20221213 12:33:47 @agent_ppo2.py:179][0m |          -0.0561 |          15.3035 |         -46.2056 |
[32m[20221213 12:33:47 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:33:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 202.70
[32m[20221213 12:33:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 274.26
[32m[20221213 12:33:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 297.34
[32m[20221213 12:33:47 @agent_ppo2.py:137][0m Total time:      17.84 min
[32m[20221213 12:33:47 @agent_ppo2.py:139][0m 1183744 total steps have happened
[32m[20221213 12:33:47 @agent_ppo2.py:115][0m #------------------------ Iteration 578 --------------------------#
[32m[20221213 12:33:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:47 @agent_ppo2.py:179][0m |           0.0462 |          12.7802 |         -38.5198 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |           0.0589 |          11.7510 |         -34.7784 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |           0.0117 |          11.3799 |         -39.1623 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |          -0.0135 |          11.1069 |         -42.9037 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |          -0.0234 |          10.8894 |         -42.9117 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |          -0.0336 |          10.8011 |         -42.8691 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |          -0.0267 |          11.1496 |         -44.3821 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |          -0.0303 |          10.5469 |         -44.9890 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |          -0.0286 |          10.4798 |         -45.5590 |
[32m[20221213 12:33:48 @agent_ppo2.py:179][0m |          -0.0197 |          11.2612 |         -44.3810 |
[32m[20221213 12:33:48 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:33:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.67
[32m[20221213 12:33:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 299.40
[32m[20221213 12:33:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 286.44
[32m[20221213 12:33:49 @agent_ppo2.py:137][0m Total time:      17.86 min
[32m[20221213 12:33:49 @agent_ppo2.py:139][0m 1185792 total steps have happened
[32m[20221213 12:33:49 @agent_ppo2.py:115][0m #------------------------ Iteration 579 --------------------------#
[32m[20221213 12:33:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:49 @agent_ppo2.py:179][0m |           0.0756 |          20.9674 |         -37.1735 |
[32m[20221213 12:33:49 @agent_ppo2.py:179][0m |           0.0415 |          20.1384 |         -35.1591 |
[32m[20221213 12:33:49 @agent_ppo2.py:179][0m |           0.0100 |          19.5851 |         -38.0283 |
[32m[20221213 12:33:49 @agent_ppo2.py:179][0m |          -0.0192 |          19.1296 |         -43.3367 |
[32m[20221213 12:33:50 @agent_ppo2.py:179][0m |          -0.0320 |          18.7884 |         -47.0049 |
[32m[20221213 12:33:50 @agent_ppo2.py:179][0m |          -0.0347 |          18.7002 |         -48.6840 |
[32m[20221213 12:33:50 @agent_ppo2.py:179][0m |          -0.0398 |          18.3940 |         -49.8026 |
[32m[20221213 12:33:50 @agent_ppo2.py:179][0m |          -0.0467 |          18.2801 |         -51.1876 |
[32m[20221213 12:33:50 @agent_ppo2.py:179][0m |          -0.0495 |          18.1405 |         -51.3096 |
[32m[20221213 12:33:50 @agent_ppo2.py:179][0m |          -0.0506 |          18.0188 |         -52.8283 |
[32m[20221213 12:33:50 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 277.38
[32m[20221213 12:33:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.26
[32m[20221213 12:33:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.41
[32m[20221213 12:33:50 @agent_ppo2.py:137][0m Total time:      17.89 min
[32m[20221213 12:33:50 @agent_ppo2.py:139][0m 1187840 total steps have happened
[32m[20221213 12:33:50 @agent_ppo2.py:115][0m #------------------------ Iteration 580 --------------------------#
[32m[20221213 12:33:51 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:33:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:51 @agent_ppo2.py:179][0m |           0.0532 |          12.8701 |         -48.7861 |
[32m[20221213 12:33:51 @agent_ppo2.py:179][0m |           0.0331 |          11.7329 |         -45.6612 |
[32m[20221213 12:33:51 @agent_ppo2.py:179][0m |          -0.0100 |          11.3736 |         -49.0427 |
[32m[20221213 12:33:51 @agent_ppo2.py:179][0m |          -0.0180 |          11.1455 |         -50.6144 |
[32m[20221213 12:33:51 @agent_ppo2.py:179][0m |          -0.0247 |          10.9828 |         -50.3823 |
[32m[20221213 12:33:51 @agent_ppo2.py:179][0m |          -0.0274 |          10.8023 |         -52.1850 |
[32m[20221213 12:33:51 @agent_ppo2.py:179][0m |          -0.0316 |          10.6579 |         -52.0628 |
[32m[20221213 12:33:52 @agent_ppo2.py:179][0m |          -0.0360 |          10.6094 |         -52.9983 |
[32m[20221213 12:33:52 @agent_ppo2.py:179][0m |          -0.0417 |          10.4403 |         -54.0156 |
[32m[20221213 12:33:52 @agent_ppo2.py:179][0m |          -0.0381 |          10.3988 |         -54.0094 |
[32m[20221213 12:33:52 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:33:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.99
[32m[20221213 12:33:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 290.86
[32m[20221213 12:33:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 310.97
[32m[20221213 12:33:52 @agent_ppo2.py:137][0m Total time:      17.92 min
[32m[20221213 12:33:52 @agent_ppo2.py:139][0m 1189888 total steps have happened
[32m[20221213 12:33:52 @agent_ppo2.py:115][0m #------------------------ Iteration 581 --------------------------#
[32m[20221213 12:33:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:52 @agent_ppo2.py:179][0m |           0.0558 |          18.9659 |         -44.6400 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |           0.0580 |          17.7877 |         -36.2555 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0007 |          17.1337 |         -42.4376 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0091 |          16.7842 |         -43.7964 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0363 |          16.6430 |         -46.0280 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0338 |          16.3642 |         -46.8319 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0431 |          16.2364 |         -49.1745 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0383 |          17.4788 |         -49.6463 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0403 |          16.0446 |         -49.8771 |
[32m[20221213 12:33:53 @agent_ppo2.py:179][0m |          -0.0493 |          15.8633 |         -50.5504 |
[32m[20221213 12:33:53 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:33:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.86
[32m[20221213 12:33:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.25
[32m[20221213 12:33:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.76
[32m[20221213 12:33:54 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 326.76
[32m[20221213 12:33:54 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 326.76
[32m[20221213 12:33:54 @agent_ppo2.py:137][0m Total time:      17.95 min
[32m[20221213 12:33:54 @agent_ppo2.py:139][0m 1191936 total steps have happened
[32m[20221213 12:33:54 @agent_ppo2.py:115][0m #------------------------ Iteration 582 --------------------------#
[32m[20221213 12:33:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:54 @agent_ppo2.py:179][0m |           0.0592 |          28.6148 |         -52.8123 |
[32m[20221213 12:33:54 @agent_ppo2.py:179][0m |           0.0416 |          26.6407 |         -42.1152 |
[32m[20221213 12:33:54 @agent_ppo2.py:179][0m |           0.0108 |          25.9648 |         -46.1717 |
[32m[20221213 12:33:54 @agent_ppo2.py:179][0m |          -0.0163 |          25.4797 |         -48.3689 |
[32m[20221213 12:33:55 @agent_ppo2.py:179][0m |          -0.0308 |          25.1041 |         -50.6530 |
[32m[20221213 12:33:55 @agent_ppo2.py:179][0m |          -0.0413 |          24.8191 |         -52.5875 |
[32m[20221213 12:33:55 @agent_ppo2.py:179][0m |          -0.0381 |          26.0604 |         -53.3205 |
[32m[20221213 12:33:55 @agent_ppo2.py:179][0m |          -0.0441 |          24.3588 |         -53.4862 |
[32m[20221213 12:33:55 @agent_ppo2.py:179][0m |          -0.0521 |          24.2861 |         -54.5235 |
[32m[20221213 12:33:55 @agent_ppo2.py:179][0m |          -0.0533 |          24.0194 |         -55.4772 |
[32m[20221213 12:33:55 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:33:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 256.32
[32m[20221213 12:33:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 293.76
[32m[20221213 12:33:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.98
[32m[20221213 12:33:55 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 328.98
[32m[20221213 12:33:55 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 328.98
[32m[20221213 12:33:55 @agent_ppo2.py:137][0m Total time:      17.98 min
[32m[20221213 12:33:55 @agent_ppo2.py:139][0m 1193984 total steps have happened
[32m[20221213 12:33:55 @agent_ppo2.py:115][0m #------------------------ Iteration 583 --------------------------#
[32m[20221213 12:33:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:56 @agent_ppo2.py:179][0m |           0.0514 |          25.4737 |         -41.6493 |
[32m[20221213 12:33:56 @agent_ppo2.py:179][0m |           0.0205 |          24.7307 |         -36.0209 |
[32m[20221213 12:33:56 @agent_ppo2.py:179][0m |          -0.0002 |          24.2431 |         -41.0288 |
[32m[20221213 12:33:56 @agent_ppo2.py:179][0m |          -0.0057 |          23.9208 |         -39.5762 |
[32m[20221213 12:33:56 @agent_ppo2.py:179][0m |          -0.0254 |          23.6171 |         -43.0785 |
[32m[20221213 12:33:56 @agent_ppo2.py:179][0m |          -0.0327 |          23.4391 |         -44.4978 |
[32m[20221213 12:33:57 @agent_ppo2.py:179][0m |          -0.0335 |          23.1459 |         -45.1564 |
[32m[20221213 12:33:57 @agent_ppo2.py:179][0m |          -0.0437 |          23.0429 |         -47.2733 |
[32m[20221213 12:33:57 @agent_ppo2.py:179][0m |          -0.0434 |          22.8393 |         -47.8594 |
[32m[20221213 12:33:57 @agent_ppo2.py:179][0m |          -0.0476 |          22.7595 |         -48.0777 |
[32m[20221213 12:33:57 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:33:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 267.69
[32m[20221213 12:33:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 306.23
[32m[20221213 12:33:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.45
[32m[20221213 12:33:57 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 340.45
[32m[20221213 12:33:57 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 340.45
[32m[20221213 12:33:57 @agent_ppo2.py:137][0m Total time:      18.01 min
[32m[20221213 12:33:57 @agent_ppo2.py:139][0m 1196032 total steps have happened
[32m[20221213 12:33:57 @agent_ppo2.py:115][0m #------------------------ Iteration 584 --------------------------#
[32m[20221213 12:33:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |           0.0421 |          11.3855 |         -41.3233 |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |           0.0490 |          10.2972 |         -24.9897 |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |          -0.0083 |           9.7674 |         -18.3169 |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |          -0.0342 |           9.3769 |         -20.3268 |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |          -0.0431 |           9.1918 |         -21.7033 |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |          -0.0422 |           9.4777 |         -22.0422 |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |          -0.0597 |           8.8875 |         -22.8647 |
[32m[20221213 12:33:58 @agent_ppo2.py:179][0m |          -0.0574 |           8.7065 |         -23.8194 |
[32m[20221213 12:33:59 @agent_ppo2.py:179][0m |          -0.0644 |           8.5830 |         -24.6715 |
[32m[20221213 12:33:59 @agent_ppo2.py:179][0m |          -0.0566 |           8.6002 |         -25.1680 |
[32m[20221213 12:33:59 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:33:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.53
[32m[20221213 12:33:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 269.48
[32m[20221213 12:33:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.12
[32m[20221213 12:33:59 @agent_ppo2.py:137][0m Total time:      18.04 min
[32m[20221213 12:33:59 @agent_ppo2.py:139][0m 1198080 total steps have happened
[32m[20221213 12:33:59 @agent_ppo2.py:115][0m #------------------------ Iteration 585 --------------------------#
[32m[20221213 12:33:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:33:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:33:59 @agent_ppo2.py:179][0m |           0.1215 |          26.3976 |         -45.4130 |
[32m[20221213 12:33:59 @agent_ppo2.py:179][0m |           0.0947 |          24.1890 |         -30.4031 |
[32m[20221213 12:33:59 @agent_ppo2.py:179][0m |           0.0207 |          23.2206 |         -35.5739 |
[32m[20221213 12:34:00 @agent_ppo2.py:179][0m |          -0.0058 |          22.5898 |         -43.2382 |
[32m[20221213 12:34:00 @agent_ppo2.py:179][0m |          -0.0272 |          22.0402 |         -46.1687 |
[32m[20221213 12:34:00 @agent_ppo2.py:179][0m |          -0.0342 |          21.6533 |         -49.1828 |
[32m[20221213 12:34:00 @agent_ppo2.py:179][0m |          -0.0363 |          21.4244 |         -49.9225 |
[32m[20221213 12:34:00 @agent_ppo2.py:179][0m |          -0.0404 |          21.1914 |         -51.7210 |
[32m[20221213 12:34:00 @agent_ppo2.py:179][0m |          -0.0469 |          20.9187 |         -54.5401 |
[32m[20221213 12:34:00 @agent_ppo2.py:179][0m |          -0.0516 |          20.6452 |         -55.3309 |
[32m[20221213 12:34:00 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:34:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 275.37
[32m[20221213 12:34:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.43
[32m[20221213 12:34:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 308.56
[32m[20221213 12:34:01 @agent_ppo2.py:137][0m Total time:      18.06 min
[32m[20221213 12:34:01 @agent_ppo2.py:139][0m 1200128 total steps have happened
[32m[20221213 12:34:01 @agent_ppo2.py:115][0m #------------------------ Iteration 586 --------------------------#
[32m[20221213 12:34:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:01 @agent_ppo2.py:179][0m |           0.0492 |          24.1797 |         -48.2644 |
[32m[20221213 12:34:01 @agent_ppo2.py:179][0m |           0.0278 |          22.3834 |         -45.5076 |
[32m[20221213 12:34:01 @agent_ppo2.py:179][0m |           0.0026 |          21.7351 |         -46.0459 |
[32m[20221213 12:34:01 @agent_ppo2.py:179][0m |          -0.0184 |          21.4847 |         -50.0494 |
[32m[20221213 12:34:01 @agent_ppo2.py:179][0m |          -0.0307 |          20.8896 |         -52.2687 |
[32m[20221213 12:34:02 @agent_ppo2.py:179][0m |          -0.0310 |          20.7696 |         -53.4795 |
[32m[20221213 12:34:02 @agent_ppo2.py:179][0m |          -0.0402 |          20.4075 |         -55.1201 |
[32m[20221213 12:34:02 @agent_ppo2.py:179][0m |          -0.0415 |          20.1141 |         -56.2062 |
[32m[20221213 12:34:02 @agent_ppo2.py:179][0m |          -0.0398 |          20.1078 |         -56.9577 |
[32m[20221213 12:34:02 @agent_ppo2.py:179][0m |          -0.0503 |          19.8616 |         -58.1729 |
[32m[20221213 12:34:02 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 285.85
[32m[20221213 12:34:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 316.50
[32m[20221213 12:34:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.03
[32m[20221213 12:34:02 @agent_ppo2.py:137][0m Total time:      18.09 min
[32m[20221213 12:34:02 @agent_ppo2.py:139][0m 1202176 total steps have happened
[32m[20221213 12:34:02 @agent_ppo2.py:115][0m #------------------------ Iteration 587 --------------------------#
[32m[20221213 12:34:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |           0.0747 |          26.5315 |         -46.8700 |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |           0.0701 |          24.9802 |         -36.0463 |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |           0.0335 |          24.4482 |         -36.9257 |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |           0.0059 |          24.3482 |         -43.7478 |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |          -0.0195 |          23.7685 |         -47.1311 |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |          -0.0218 |          23.5439 |         -51.6892 |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |          -0.0330 |          23.3591 |         -52.1911 |
[32m[20221213 12:34:03 @agent_ppo2.py:179][0m |          -0.0397 |          23.1605 |         -53.7904 |
[32m[20221213 12:34:04 @agent_ppo2.py:179][0m |          -0.0403 |          23.0604 |         -53.9043 |
[32m[20221213 12:34:04 @agent_ppo2.py:179][0m |          -0.0342 |          25.3308 |         -55.6808 |
[32m[20221213 12:34:04 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.83
[32m[20221213 12:34:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 307.39
[32m[20221213 12:34:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.76
[32m[20221213 12:34:04 @agent_ppo2.py:137][0m Total time:      18.12 min
[32m[20221213 12:34:04 @agent_ppo2.py:139][0m 1204224 total steps have happened
[32m[20221213 12:34:04 @agent_ppo2.py:115][0m #------------------------ Iteration 588 --------------------------#
[32m[20221213 12:34:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:04 @agent_ppo2.py:179][0m |           0.1269 |          26.8177 |         -36.5273 |
[32m[20221213 12:34:04 @agent_ppo2.py:179][0m |           0.0824 |          25.1715 |         -27.3616 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |           0.0372 |          23.3891 |         -33.6639 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |          -0.0009 |          23.0036 |         -41.3661 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |          -0.0117 |          22.7442 |         -45.6072 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |          -0.0193 |          22.5512 |         -47.0935 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |          -0.0267 |          22.3172 |         -47.9277 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |          -0.0247 |          22.3765 |         -48.0992 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |          -0.0316 |          22.0705 |         -49.8053 |
[32m[20221213 12:34:05 @agent_ppo2.py:179][0m |          -0.0250 |          24.7029 |         -51.1131 |
[32m[20221213 12:34:05 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:34:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 306.24
[32m[20221213 12:34:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.03
[32m[20221213 12:34:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 273.52
[32m[20221213 12:34:06 @agent_ppo2.py:137][0m Total time:      18.15 min
[32m[20221213 12:34:06 @agent_ppo2.py:139][0m 1206272 total steps have happened
[32m[20221213 12:34:06 @agent_ppo2.py:115][0m #------------------------ Iteration 589 --------------------------#
[32m[20221213 12:34:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:06 @agent_ppo2.py:179][0m |           0.0821 |          26.0524 |         -43.1403 |
[32m[20221213 12:34:06 @agent_ppo2.py:179][0m |           0.0627 |          23.8367 |         -30.9451 |
[32m[20221213 12:34:06 @agent_ppo2.py:179][0m |           0.0088 |          23.1277 |         -41.2117 |
[32m[20221213 12:34:06 @agent_ppo2.py:179][0m |           0.0009 |          25.9692 |         -45.0126 |
[32m[20221213 12:34:07 @agent_ppo2.py:179][0m |          -0.0231 |          22.2968 |         -45.7909 |
[32m[20221213 12:34:07 @agent_ppo2.py:179][0m |          -0.0348 |          22.3011 |         -48.1634 |
[32m[20221213 12:34:07 @agent_ppo2.py:179][0m |          -0.0402 |          21.8509 |         -48.2101 |
[32m[20221213 12:34:07 @agent_ppo2.py:179][0m |          -0.0447 |          21.5630 |         -50.9642 |
[32m[20221213 12:34:07 @agent_ppo2.py:179][0m |          -0.0458 |          21.3327 |         -50.8064 |
[32m[20221213 12:34:07 @agent_ppo2.py:179][0m |          -0.0518 |          21.1710 |         -53.3152 |
[32m[20221213 12:34:07 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:34:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.44
[32m[20221213 12:34:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 310.61
[32m[20221213 12:34:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 297.73
[32m[20221213 12:34:07 @agent_ppo2.py:137][0m Total time:      18.18 min
[32m[20221213 12:34:07 @agent_ppo2.py:139][0m 1208320 total steps have happened
[32m[20221213 12:34:07 @agent_ppo2.py:115][0m #------------------------ Iteration 590 --------------------------#
[32m[20221213 12:34:08 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:34:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:08 @agent_ppo2.py:179][0m |           0.0843 |          18.8826 |         -44.3262 |
[32m[20221213 12:34:08 @agent_ppo2.py:179][0m |           0.0660 |          17.7031 |         -38.6006 |
[32m[20221213 12:34:08 @agent_ppo2.py:179][0m |           0.0815 |          19.4285 |         -32.0138 |
[32m[20221213 12:34:08 @agent_ppo2.py:179][0m |          -0.0058 |          16.9544 |         -27.1905 |
[32m[20221213 12:34:08 @agent_ppo2.py:179][0m |          -0.0378 |          16.4662 |         -29.8766 |
[32m[20221213 12:34:08 @agent_ppo2.py:179][0m |          -0.0473 |          16.2190 |         -31.6734 |
[32m[20221213 12:34:09 @agent_ppo2.py:179][0m |          -0.0476 |          15.9785 |         -32.0879 |
[32m[20221213 12:34:09 @agent_ppo2.py:179][0m |          -0.0597 |          15.9193 |         -34.8917 |
[32m[20221213 12:34:09 @agent_ppo2.py:179][0m |          -0.0638 |          15.6946 |         -35.0495 |
[32m[20221213 12:34:09 @agent_ppo2.py:179][0m |          -0.0674 |          15.4381 |         -36.0260 |
[32m[20221213 12:34:09 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:34:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 174.09
[32m[20221213 12:34:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 287.70
[32m[20221213 12:34:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 324.05
[32m[20221213 12:34:09 @agent_ppo2.py:137][0m Total time:      18.21 min
[32m[20221213 12:34:09 @agent_ppo2.py:139][0m 1210368 total steps have happened
[32m[20221213 12:34:09 @agent_ppo2.py:115][0m #------------------------ Iteration 591 --------------------------#
[32m[20221213 12:34:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |           0.0863 |          26.0041 |         -50.1406 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |           0.0618 |          24.5431 |         -36.4120 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |           0.0111 |          23.9007 |         -47.0289 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |          -0.0082 |          23.3835 |         -52.5621 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |          -0.0218 |          23.0968 |         -55.4507 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |          -0.0272 |          23.1281 |         -57.2298 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |          -0.0254 |          23.4253 |         -58.7063 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |          -0.0394 |          22.1912 |         -59.5544 |
[32m[20221213 12:34:10 @agent_ppo2.py:179][0m |          -0.0402 |          22.0022 |         -61.2364 |
[32m[20221213 12:34:11 @agent_ppo2.py:179][0m |          -0.0396 |          22.1751 |         -62.3125 |
[32m[20221213 12:34:11 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:34:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 285.15
[32m[20221213 12:34:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 310.18
[32m[20221213 12:34:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.70
[32m[20221213 12:34:11 @agent_ppo2.py:137][0m Total time:      18.23 min
[32m[20221213 12:34:11 @agent_ppo2.py:139][0m 1212416 total steps have happened
[32m[20221213 12:34:11 @agent_ppo2.py:115][0m #------------------------ Iteration 592 --------------------------#
[32m[20221213 12:34:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:11 @agent_ppo2.py:179][0m |           0.0981 |          30.1343 |         -47.6661 |
[32m[20221213 12:34:11 @agent_ppo2.py:179][0m |           0.1016 |          27.9052 |         -33.7476 |
[32m[20221213 12:34:11 @agent_ppo2.py:179][0m |           0.0419 |          29.4959 |         -37.1849 |
[32m[20221213 12:34:12 @agent_ppo2.py:179][0m |           0.0033 |          26.7017 |         -44.8725 |
[32m[20221213 12:34:12 @agent_ppo2.py:179][0m |          -0.0164 |          26.4085 |         -49.6996 |
[32m[20221213 12:34:12 @agent_ppo2.py:179][0m |          -0.0333 |          26.0634 |         -52.2613 |
[32m[20221213 12:34:12 @agent_ppo2.py:179][0m |          -0.0431 |          25.8305 |         -54.5335 |
[32m[20221213 12:34:12 @agent_ppo2.py:179][0m |          -0.0407 |          25.7206 |         -55.3735 |
[32m[20221213 12:34:12 @agent_ppo2.py:179][0m |          -0.0441 |          25.4949 |         -57.5095 |
[32m[20221213 12:34:12 @agent_ppo2.py:179][0m |          -0.0461 |          25.3833 |         -59.4915 |
[32m[20221213 12:34:12 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:34:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 264.96
[32m[20221213 12:34:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 307.27
[32m[20221213 12:34:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.65
[32m[20221213 12:34:12 @agent_ppo2.py:137][0m Total time:      18.26 min
[32m[20221213 12:34:12 @agent_ppo2.py:139][0m 1214464 total steps have happened
[32m[20221213 12:34:12 @agent_ppo2.py:115][0m #------------------------ Iteration 593 --------------------------#
[32m[20221213 12:34:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:13 @agent_ppo2.py:179][0m |           0.0607 |          27.3414 |         -49.0658 |
[32m[20221213 12:34:13 @agent_ppo2.py:179][0m |           0.0448 |          27.8696 |         -42.9455 |
[32m[20221213 12:34:13 @agent_ppo2.py:179][0m |           0.0213 |          26.0968 |         -45.2629 |
[32m[20221213 12:34:13 @agent_ppo2.py:179][0m |          -0.0027 |          25.7763 |         -45.4966 |
[32m[20221213 12:34:13 @agent_ppo2.py:179][0m |          -0.0160 |          25.5215 |         -51.3629 |
[32m[20221213 12:34:13 @agent_ppo2.py:179][0m |          -0.0229 |          25.2588 |         -53.6524 |
[32m[20221213 12:34:14 @agent_ppo2.py:179][0m |          -0.0327 |          25.1112 |         -57.1768 |
[32m[20221213 12:34:14 @agent_ppo2.py:179][0m |          -0.0399 |          24.9531 |         -58.9044 |
[32m[20221213 12:34:14 @agent_ppo2.py:179][0m |          -0.0324 |          25.7648 |         -58.9476 |
[32m[20221213 12:34:14 @agent_ppo2.py:179][0m |          -0.0437 |          24.7528 |         -60.0736 |
[32m[20221213 12:34:14 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 311.13
[32m[20221213 12:34:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.28
[32m[20221213 12:34:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.86
[32m[20221213 12:34:14 @agent_ppo2.py:137][0m Total time:      18.29 min
[32m[20221213 12:34:14 @agent_ppo2.py:139][0m 1216512 total steps have happened
[32m[20221213 12:34:14 @agent_ppo2.py:115][0m #------------------------ Iteration 594 --------------------------#
[32m[20221213 12:34:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |           0.0739 |          13.4170 |         -56.2296 |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |           0.0868 |          11.9780 |         -31.2351 |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |           0.0090 |          11.5783 |         -17.2147 |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |          -0.0096 |          12.6129 |         -19.6631 |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |          -0.0267 |          11.1327 |         -20.8318 |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |          -0.0403 |          10.8956 |         -22.5796 |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |          -0.0361 |          10.9218 |         -23.3681 |
[32m[20221213 12:34:15 @agent_ppo2.py:179][0m |          -0.0525 |          10.6139 |         -24.3340 |
[32m[20221213 12:34:16 @agent_ppo2.py:179][0m |          -0.0561 |          10.5060 |         -25.3258 |
[32m[20221213 12:34:16 @agent_ppo2.py:179][0m |          -0.0609 |          10.3804 |         -26.0161 |
[32m[20221213 12:34:16 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:34:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.85
[32m[20221213 12:34:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.55
[32m[20221213 12:34:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 317.45
[32m[20221213 12:34:16 @agent_ppo2.py:137][0m Total time:      18.32 min
[32m[20221213 12:34:16 @agent_ppo2.py:139][0m 1218560 total steps have happened
[32m[20221213 12:34:16 @agent_ppo2.py:115][0m #------------------------ Iteration 595 --------------------------#
[32m[20221213 12:34:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:16 @agent_ppo2.py:179][0m |           0.1048 |          22.2838 |         -54.6918 |
[32m[20221213 12:34:16 @agent_ppo2.py:179][0m |           0.0763 |          20.9794 |         -49.3357 |
[32m[20221213 12:34:16 @agent_ppo2.py:179][0m |           0.0346 |          20.4430 |         -53.4281 |
[32m[20221213 12:34:17 @agent_ppo2.py:179][0m |           0.0063 |          20.1223 |         -58.9763 |
[32m[20221213 12:34:17 @agent_ppo2.py:179][0m |          -0.0180 |          19.8762 |         -62.1023 |
[32m[20221213 12:34:17 @agent_ppo2.py:179][0m |          -0.0200 |          19.6956 |         -65.6845 |
[32m[20221213 12:34:17 @agent_ppo2.py:179][0m |          -0.0154 |          19.3520 |         -65.3134 |
[32m[20221213 12:34:17 @agent_ppo2.py:179][0m |          -0.0306 |          19.0929 |         -68.1128 |
[32m[20221213 12:34:17 @agent_ppo2.py:179][0m |          -0.0392 |          18.9636 |         -71.0476 |
[32m[20221213 12:34:17 @agent_ppo2.py:179][0m |          -0.0427 |          18.7686 |         -71.7430 |
[32m[20221213 12:34:17 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 258.30
[32m[20221213 12:34:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 327.28
[32m[20221213 12:34:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.24
[32m[20221213 12:34:18 @agent_ppo2.py:137][0m Total time:      18.35 min
[32m[20221213 12:34:18 @agent_ppo2.py:139][0m 1220608 total steps have happened
[32m[20221213 12:34:18 @agent_ppo2.py:115][0m #------------------------ Iteration 596 --------------------------#
[32m[20221213 12:34:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:18 @agent_ppo2.py:179][0m |           0.0954 |          26.1573 |         -54.9118 |
[32m[20221213 12:34:18 @agent_ppo2.py:179][0m |           0.0412 |          25.4192 |         -49.1057 |
[32m[20221213 12:34:18 @agent_ppo2.py:179][0m |           0.0035 |          24.9746 |         -57.9975 |
[32m[20221213 12:34:18 @agent_ppo2.py:179][0m |          -0.0154 |          24.6117 |         -60.5079 |
[32m[20221213 12:34:18 @agent_ppo2.py:179][0m |          -0.0158 |          24.2498 |         -61.7267 |
[32m[20221213 12:34:19 @agent_ppo2.py:179][0m |          -0.0239 |          24.0064 |         -62.3566 |
[32m[20221213 12:34:19 @agent_ppo2.py:179][0m |          -0.0318 |          23.8247 |         -64.5426 |
[32m[20221213 12:34:19 @agent_ppo2.py:179][0m |          -0.0337 |          24.1648 |         -67.3292 |
[32m[20221213 12:34:19 @agent_ppo2.py:179][0m |          -0.0441 |          23.6178 |         -68.1577 |
[32m[20221213 12:34:19 @agent_ppo2.py:179][0m |          -0.0479 |          23.3541 |         -69.3225 |
[32m[20221213 12:34:19 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.03
[32m[20221213 12:34:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.31
[32m[20221213 12:34:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.17
[32m[20221213 12:34:19 @agent_ppo2.py:137][0m Total time:      18.38 min
[32m[20221213 12:34:19 @agent_ppo2.py:139][0m 1222656 total steps have happened
[32m[20221213 12:34:19 @agent_ppo2.py:115][0m #------------------------ Iteration 597 --------------------------#
[32m[20221213 12:34:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:20 @agent_ppo2.py:179][0m |           0.0576 |          23.4670 |         -59.7627 |
[32m[20221213 12:34:20 @agent_ppo2.py:179][0m |           0.0219 |          22.3860 |         -59.9477 |
[32m[20221213 12:34:20 @agent_ppo2.py:179][0m |           0.0009 |          21.8819 |         -63.2356 |
[32m[20221213 12:34:20 @agent_ppo2.py:179][0m |          -0.0105 |          21.5609 |         -67.7687 |
[32m[20221213 12:34:20 @agent_ppo2.py:179][0m |          -0.0249 |          21.4185 |         -69.4483 |
[32m[20221213 12:34:20 @agent_ppo2.py:179][0m |          -0.0217 |          24.3393 |         -71.3191 |
[32m[20221213 12:34:20 @agent_ppo2.py:179][0m |          -0.0327 |          21.0339 |         -73.0392 |
[32m[20221213 12:34:21 @agent_ppo2.py:179][0m |          -0.0393 |          20.8596 |         -74.8588 |
[32m[20221213 12:34:21 @agent_ppo2.py:179][0m |          -0.0359 |          20.8072 |         -76.2321 |
[32m[20221213 12:34:21 @agent_ppo2.py:179][0m |          -0.0446 |          20.5888 |         -77.9329 |
[32m[20221213 12:34:21 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.92
[32m[20221213 12:34:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.06
[32m[20221213 12:34:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.18
[32m[20221213 12:34:21 @agent_ppo2.py:137][0m Total time:      18.40 min
[32m[20221213 12:34:21 @agent_ppo2.py:139][0m 1224704 total steps have happened
[32m[20221213 12:34:21 @agent_ppo2.py:115][0m #------------------------ Iteration 598 --------------------------#
[32m[20221213 12:34:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:21 @agent_ppo2.py:179][0m |           0.1209 |          20.7003 |         -58.4498 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |           0.0664 |          18.9789 |         -57.7051 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |           0.0251 |          18.4358 |         -63.9053 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |           0.0060 |          20.6289 |         -69.2154 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |           0.0016 |          19.4365 |         -70.9112 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |          -0.0103 |          17.7426 |         -71.5047 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |          -0.0280 |          17.4788 |         -72.7912 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |          -0.0368 |          17.3851 |         -75.1950 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |          -0.0354 |          17.2341 |         -77.6314 |
[32m[20221213 12:34:22 @agent_ppo2.py:179][0m |          -0.0378 |          17.1343 |         -80.1425 |
[32m[20221213 12:34:22 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:34:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.50
[32m[20221213 12:34:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 294.81
[32m[20221213 12:34:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.81
[32m[20221213 12:34:23 @agent_ppo2.py:137][0m Total time:      18.43 min
[32m[20221213 12:34:23 @agent_ppo2.py:139][0m 1226752 total steps have happened
[32m[20221213 12:34:23 @agent_ppo2.py:115][0m #------------------------ Iteration 599 --------------------------#
[32m[20221213 12:34:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:23 @agent_ppo2.py:179][0m |           0.0532 |          25.3662 |         -72.5276 |
[32m[20221213 12:34:23 @agent_ppo2.py:179][0m |           0.0241 |          22.8673 |         -68.9036 |
[32m[20221213 12:34:23 @agent_ppo2.py:179][0m |          -0.0122 |          22.3215 |         -72.4569 |
[32m[20221213 12:34:23 @agent_ppo2.py:179][0m |          -0.0164 |          21.9471 |         -74.1450 |
[32m[20221213 12:34:24 @agent_ppo2.py:179][0m |          -0.0231 |          21.7217 |         -74.8005 |
[32m[20221213 12:34:24 @agent_ppo2.py:179][0m |          -0.0309 |          21.7532 |         -76.5364 |
[32m[20221213 12:34:24 @agent_ppo2.py:179][0m |          -0.0392 |          21.4203 |         -79.1431 |
[32m[20221213 12:34:24 @agent_ppo2.py:179][0m |          -0.0458 |          21.4259 |         -80.2639 |
[32m[20221213 12:34:24 @agent_ppo2.py:179][0m |          -0.0357 |          21.3084 |         -79.6603 |
[32m[20221213 12:34:24 @agent_ppo2.py:179][0m |          -0.0442 |          21.2337 |         -82.2249 |
[32m[20221213 12:34:24 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.19
[32m[20221213 12:34:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 310.56
[32m[20221213 12:34:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 268.35
[32m[20221213 12:34:24 @agent_ppo2.py:137][0m Total time:      18.46 min
[32m[20221213 12:34:24 @agent_ppo2.py:139][0m 1228800 total steps have happened
[32m[20221213 12:34:24 @agent_ppo2.py:115][0m #------------------------ Iteration 600 --------------------------#
[32m[20221213 12:34:25 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:34:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:25 @agent_ppo2.py:179][0m |           0.0608 |          24.6211 |         -66.1651 |
[32m[20221213 12:34:25 @agent_ppo2.py:179][0m |           0.0322 |          24.0666 |         -60.7093 |
[32m[20221213 12:34:25 @agent_ppo2.py:179][0m |          -0.0102 |          23.0526 |         -67.4910 |
[32m[20221213 12:34:25 @agent_ppo2.py:179][0m |          -0.0251 |          22.5378 |         -69.9215 |
[32m[20221213 12:34:25 @agent_ppo2.py:179][0m |          -0.0288 |          22.3234 |         -70.8470 |
[32m[20221213 12:34:25 @agent_ppo2.py:179][0m |          -0.0380 |          22.0253 |         -73.0606 |
[32m[20221213 12:34:26 @agent_ppo2.py:179][0m |          -0.0434 |          21.7977 |         -74.7843 |
[32m[20221213 12:34:26 @agent_ppo2.py:179][0m |          -0.0418 |          21.6489 |         -75.3886 |
[32m[20221213 12:34:26 @agent_ppo2.py:179][0m |          -0.0416 |          21.4993 |         -78.0300 |
[32m[20221213 12:34:26 @agent_ppo2.py:179][0m |          -0.0408 |          21.3848 |         -78.0705 |
[32m[20221213 12:34:26 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.18
[32m[20221213 12:34:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.18
[32m[20221213 12:34:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 302.32
[32m[20221213 12:34:26 @agent_ppo2.py:137][0m Total time:      18.49 min
[32m[20221213 12:34:26 @agent_ppo2.py:139][0m 1230848 total steps have happened
[32m[20221213 12:34:26 @agent_ppo2.py:115][0m #------------------------ Iteration 601 --------------------------#
[32m[20221213 12:34:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:26 @agent_ppo2.py:179][0m |           0.0580 |          25.8255 |         -71.5999 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |           0.0278 |          24.2907 |         -67.2209 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |          -0.0063 |          23.5351 |         -70.9059 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |          -0.0253 |          23.0064 |         -73.2291 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |          -0.0348 |          22.6234 |         -75.6470 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |          -0.0384 |          22.2464 |         -78.0979 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |          -0.0448 |          22.0683 |         -79.6292 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |          -0.0324 |          23.3633 |         -80.8918 |
[32m[20221213 12:34:27 @agent_ppo2.py:179][0m |          -0.0321 |          21.4574 |         -77.9223 |
[32m[20221213 12:34:28 @agent_ppo2.py:179][0m |          -0.0343 |          25.0457 |         -84.0203 |
[32m[20221213 12:34:28 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 284.17
[32m[20221213 12:34:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.25
[32m[20221213 12:34:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.09
[32m[20221213 12:34:28 @agent_ppo2.py:137][0m Total time:      18.52 min
[32m[20221213 12:34:28 @agent_ppo2.py:139][0m 1232896 total steps have happened
[32m[20221213 12:34:28 @agent_ppo2.py:115][0m #------------------------ Iteration 602 --------------------------#
[32m[20221213 12:34:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:28 @agent_ppo2.py:179][0m |           0.0959 |          27.7328 |         -62.1659 |
[32m[20221213 12:34:28 @agent_ppo2.py:179][0m |           0.0635 |          26.5332 |         -51.9499 |
[32m[20221213 12:34:28 @agent_ppo2.py:179][0m |           0.0156 |          26.0952 |         -65.3378 |
[32m[20221213 12:34:29 @agent_ppo2.py:179][0m |           0.0180 |          27.0427 |         -65.8978 |
[32m[20221213 12:34:29 @agent_ppo2.py:179][0m |          -0.0134 |          25.5717 |         -70.0362 |
[32m[20221213 12:34:29 @agent_ppo2.py:179][0m |          -0.0159 |          28.6077 |         -73.2526 |
[32m[20221213 12:34:29 @agent_ppo2.py:179][0m |          -0.0212 |          25.3593 |         -69.8782 |
[32m[20221213 12:34:29 @agent_ppo2.py:179][0m |          -0.0307 |          25.2702 |         -73.7669 |
[32m[20221213 12:34:29 @agent_ppo2.py:179][0m |          -0.0359 |          25.1568 |         -76.4892 |
[32m[20221213 12:34:29 @agent_ppo2.py:179][0m |          -0.0367 |          25.0313 |         -75.7162 |
[32m[20221213 12:34:29 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:34:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.67
[32m[20221213 12:34:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.44
[32m[20221213 12:34:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 321.71
[32m[20221213 12:34:29 @agent_ppo2.py:137][0m Total time:      18.55 min
[32m[20221213 12:34:29 @agent_ppo2.py:139][0m 1234944 total steps have happened
[32m[20221213 12:34:29 @agent_ppo2.py:115][0m #------------------------ Iteration 603 --------------------------#
[32m[20221213 12:34:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:30 @agent_ppo2.py:179][0m |           0.0606 |          23.9182 |         -64.6849 |
[32m[20221213 12:34:30 @agent_ppo2.py:179][0m |           0.0814 |          22.6740 |         -45.5536 |
[32m[20221213 12:34:30 @agent_ppo2.py:179][0m |           0.0441 |          21.6960 |         -45.3226 |
[32m[20221213 12:34:30 @agent_ppo2.py:179][0m |           0.0018 |          21.1623 |         -57.5472 |
[32m[20221213 12:34:30 @agent_ppo2.py:179][0m |          -0.0021 |          22.5423 |         -62.3024 |
[32m[20221213 12:34:30 @agent_ppo2.py:179][0m |          -0.0205 |          20.4764 |         -63.5662 |
[32m[20221213 12:34:31 @agent_ppo2.py:179][0m |          -0.0285 |          20.2347 |         -66.6188 |
[32m[20221213 12:34:31 @agent_ppo2.py:179][0m |          -0.0368 |          20.0370 |         -68.7517 |
[32m[20221213 12:34:31 @agent_ppo2.py:179][0m |          -0.0243 |          21.9751 |         -70.9100 |
[32m[20221213 12:34:31 @agent_ppo2.py:179][0m |          -0.0399 |          19.7852 |         -71.2685 |
[32m[20221213 12:34:31 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 307.54
[32m[20221213 12:34:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.21
[32m[20221213 12:34:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.55
[32m[20221213 12:34:31 @agent_ppo2.py:137][0m Total time:      18.57 min
[32m[20221213 12:34:31 @agent_ppo2.py:139][0m 1236992 total steps have happened
[32m[20221213 12:34:31 @agent_ppo2.py:115][0m #------------------------ Iteration 604 --------------------------#
[32m[20221213 12:34:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |           0.0652 |          15.3366 |         -53.6114 |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |           0.0445 |          14.0346 |         -50.1115 |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |           0.0275 |          13.5830 |         -42.4060 |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |           0.0022 |          13.2832 |         -26.5541 |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |          -0.0280 |          13.1394 |         -27.0954 |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |          -0.0363 |          12.9856 |         -29.5643 |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |          -0.0484 |          12.9380 |         -30.5675 |
[32m[20221213 12:34:32 @agent_ppo2.py:179][0m |          -0.0583 |          12.7731 |         -32.9895 |
[32m[20221213 12:34:33 @agent_ppo2.py:179][0m |          -0.0552 |          13.0359 |         -33.2959 |
[32m[20221213 12:34:33 @agent_ppo2.py:179][0m |          -0.0580 |          12.6690 |         -34.7926 |
[32m[20221213 12:34:33 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:34:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.57
[32m[20221213 12:34:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.62
[32m[20221213 12:34:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 294.23
[32m[20221213 12:34:33 @agent_ppo2.py:137][0m Total time:      18.60 min
[32m[20221213 12:34:33 @agent_ppo2.py:139][0m 1239040 total steps have happened
[32m[20221213 12:34:33 @agent_ppo2.py:115][0m #------------------------ Iteration 605 --------------------------#
[32m[20221213 12:34:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:33 @agent_ppo2.py:179][0m |           0.0859 |          22.3306 |         -58.4263 |
[32m[20221213 12:34:33 @agent_ppo2.py:179][0m |           0.0585 |          20.6130 |         -53.3355 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |           0.0123 |          22.7055 |         -62.6361 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |          -0.0216 |          19.7280 |         -65.5721 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |          -0.0343 |          19.3461 |         -68.8660 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |          -0.0394 |          19.1204 |         -69.7354 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |          -0.0394 |          18.9516 |         -69.4679 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |          -0.0444 |          18.7414 |         -71.2059 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |          -0.0508 |          18.5991 |         -71.7977 |
[32m[20221213 12:34:34 @agent_ppo2.py:179][0m |          -0.0508 |          18.4388 |         -75.2260 |
[32m[20221213 12:34:34 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.60
[32m[20221213 12:34:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.27
[32m[20221213 12:34:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.60
[32m[20221213 12:34:35 @agent_ppo2.py:137][0m Total time:      18.63 min
[32m[20221213 12:34:35 @agent_ppo2.py:139][0m 1241088 total steps have happened
[32m[20221213 12:34:35 @agent_ppo2.py:115][0m #------------------------ Iteration 606 --------------------------#
[32m[20221213 12:34:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:35 @agent_ppo2.py:179][0m |           0.0765 |          24.2528 |         -65.1039 |
[32m[20221213 12:34:35 @agent_ppo2.py:179][0m |           0.1203 |          25.7783 |         -40.6195 |
[32m[20221213 12:34:35 @agent_ppo2.py:179][0m |           0.0554 |          22.7546 |         -51.2354 |
[32m[20221213 12:34:35 @agent_ppo2.py:179][0m |           0.0040 |          22.4249 |         -67.1841 |
[32m[20221213 12:34:35 @agent_ppo2.py:179][0m |          -0.0121 |          22.2214 |         -71.8669 |
[32m[20221213 12:34:36 @agent_ppo2.py:179][0m |          -0.0185 |          22.0647 |         -74.7717 |
[32m[20221213 12:34:36 @agent_ppo2.py:179][0m |          -0.0288 |          21.9337 |         -77.2033 |
[32m[20221213 12:34:36 @agent_ppo2.py:179][0m |          -0.0396 |          21.8801 |         -79.8727 |
[32m[20221213 12:34:36 @agent_ppo2.py:179][0m |          -0.0383 |          21.7460 |         -81.6693 |
[32m[20221213 12:34:36 @agent_ppo2.py:179][0m |          -0.0325 |          21.6509 |         -79.3796 |
[32m[20221213 12:34:36 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 278.55
[32m[20221213 12:34:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 316.36
[32m[20221213 12:34:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.46
[32m[20221213 12:34:36 @agent_ppo2.py:137][0m Total time:      18.66 min
[32m[20221213 12:34:36 @agent_ppo2.py:139][0m 1243136 total steps have happened
[32m[20221213 12:34:36 @agent_ppo2.py:115][0m #------------------------ Iteration 607 --------------------------#
[32m[20221213 12:34:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:37 @agent_ppo2.py:179][0m |           0.1172 |          26.1577 |         -63.2459 |
[32m[20221213 12:34:37 @agent_ppo2.py:179][0m |           0.0873 |          24.5079 |         -40.2073 |
[32m[20221213 12:34:37 @agent_ppo2.py:179][0m |           0.0183 |          23.8510 |         -55.4361 |
[32m[20221213 12:34:37 @agent_ppo2.py:179][0m |          -0.0155 |          23.5563 |         -62.8609 |
[32m[20221213 12:34:37 @agent_ppo2.py:179][0m |          -0.0197 |          25.9855 |         -67.2730 |
[32m[20221213 12:34:37 @agent_ppo2.py:179][0m |          -0.0327 |          23.1266 |         -70.5355 |
[32m[20221213 12:34:37 @agent_ppo2.py:179][0m |          -0.0414 |          22.8862 |         -71.7498 |
[32m[20221213 12:34:38 @agent_ppo2.py:179][0m |          -0.0421 |          22.7364 |         -74.0938 |
[32m[20221213 12:34:38 @agent_ppo2.py:179][0m |          -0.0426 |          22.6805 |         -73.0566 |
[32m[20221213 12:34:38 @agent_ppo2.py:179][0m |          -0.0485 |          22.5217 |         -74.4658 |
[32m[20221213 12:34:38 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 270.71
[32m[20221213 12:34:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.53
[32m[20221213 12:34:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.46
[32m[20221213 12:34:38 @agent_ppo2.py:137][0m Total time:      18.69 min
[32m[20221213 12:34:38 @agent_ppo2.py:139][0m 1245184 total steps have happened
[32m[20221213 12:34:38 @agent_ppo2.py:115][0m #------------------------ Iteration 608 --------------------------#
[32m[20221213 12:34:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:38 @agent_ppo2.py:179][0m |           0.0639 |          29.0243 |         -59.1512 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |           0.0634 |          27.9800 |         -44.3124 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |           0.0329 |          27.5941 |         -48.3498 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |           0.0059 |          27.3513 |         -54.7285 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |          -0.0247 |          27.0853 |         -62.9509 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |          -0.0179 |          27.6641 |         -64.8974 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |          -0.0318 |          26.6665 |         -67.7597 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |          -0.0377 |          26.5561 |         -70.1606 |
[32m[20221213 12:34:39 @agent_ppo2.py:179][0m |          -0.0441 |          26.4414 |         -71.6707 |
[32m[20221213 12:34:40 @agent_ppo2.py:179][0m |          -0.0401 |          26.2657 |         -71.8039 |
[32m[20221213 12:34:40 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:34:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.00
[32m[20221213 12:34:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 309.01
[32m[20221213 12:34:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.10
[32m[20221213 12:34:40 @agent_ppo2.py:137][0m Total time:      18.72 min
[32m[20221213 12:34:40 @agent_ppo2.py:139][0m 1247232 total steps have happened
[32m[20221213 12:34:40 @agent_ppo2.py:115][0m #------------------------ Iteration 609 --------------------------#
[32m[20221213 12:34:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:40 @agent_ppo2.py:179][0m |           0.0451 |           9.9040 |         -68.0865 |
[32m[20221213 12:34:40 @agent_ppo2.py:179][0m |           0.0421 |           9.1482 |         -57.9688 |
[32m[20221213 12:34:40 @agent_ppo2.py:179][0m |           0.0084 |           9.0187 |         -68.5539 |
[32m[20221213 12:34:41 @agent_ppo2.py:179][0m |          -0.0079 |           8.8843 |         -68.4175 |
[32m[20221213 12:34:41 @agent_ppo2.py:179][0m |          -0.0102 |           8.8256 |         -71.5219 |
[32m[20221213 12:34:41 @agent_ppo2.py:179][0m |          -0.0199 |           8.6950 |         -72.4836 |
[32m[20221213 12:34:41 @agent_ppo2.py:179][0m |          -0.0229 |           8.6357 |         -72.6707 |
[32m[20221213 12:34:41 @agent_ppo2.py:179][0m |          -0.0203 |           8.6080 |         -71.3004 |
[32m[20221213 12:34:41 @agent_ppo2.py:179][0m |          -0.0253 |           8.5375 |         -72.6910 |
[32m[20221213 12:34:41 @agent_ppo2.py:179][0m |          -0.0213 |           8.5792 |         -74.4994 |
[32m[20221213 12:34:41 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:34:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 81.45
[32m[20221213 12:34:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 230.83
[32m[20221213 12:34:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 312.13
[32m[20221213 12:34:41 @agent_ppo2.py:137][0m Total time:      18.74 min
[32m[20221213 12:34:41 @agent_ppo2.py:139][0m 1249280 total steps have happened
[32m[20221213 12:34:41 @agent_ppo2.py:115][0m #------------------------ Iteration 610 --------------------------#
[32m[20221213 12:34:42 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:34:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:42 @agent_ppo2.py:179][0m |           0.0974 |          28.5430 |         -45.3167 |
[32m[20221213 12:34:42 @agent_ppo2.py:179][0m |           0.0876 |          26.6152 |         -23.1741 |
[32m[20221213 12:34:42 @agent_ppo2.py:179][0m |           0.0422 |          26.5196 |         -35.3551 |
[32m[20221213 12:34:42 @agent_ppo2.py:179][0m |           0.0208 |          25.9844 |         -43.0356 |
[32m[20221213 12:34:42 @agent_ppo2.py:179][0m |           0.0032 |          26.4498 |         -50.5015 |
[32m[20221213 12:34:42 @agent_ppo2.py:179][0m |          -0.0111 |          25.7820 |         -56.2814 |
[32m[20221213 12:34:43 @agent_ppo2.py:179][0m |          -0.0198 |          25.6257 |         -59.3050 |
[32m[20221213 12:34:43 @agent_ppo2.py:179][0m |          -0.0217 |          25.4945 |         -58.8312 |
[32m[20221213 12:34:43 @agent_ppo2.py:179][0m |          -0.0239 |          25.5542 |         -60.9337 |
[32m[20221213 12:34:43 @agent_ppo2.py:179][0m |          -0.0316 |          25.2963 |         -63.0512 |
[32m[20221213 12:34:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:34:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 291.35
[32m[20221213 12:34:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 310.52
[32m[20221213 12:34:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.86
[32m[20221213 12:34:43 @agent_ppo2.py:137][0m Total time:      18.77 min
[32m[20221213 12:34:43 @agent_ppo2.py:139][0m 1251328 total steps have happened
[32m[20221213 12:34:43 @agent_ppo2.py:115][0m #------------------------ Iteration 611 --------------------------#
[32m[20221213 12:34:43 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:34:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |           0.0859 |          29.6173 |         -53.4611 |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |           0.0385 |          27.6753 |         -48.6540 |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |          -0.0058 |          27.1921 |         -51.0163 |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |          -0.0284 |          26.9853 |         -53.3108 |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |          -0.0254 |          26.8200 |         -53.5111 |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |          -0.0368 |          26.5542 |         -54.0576 |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |          -0.0493 |          26.3833 |         -56.0176 |
[32m[20221213 12:34:44 @agent_ppo2.py:179][0m |          -0.0479 |          26.1528 |         -56.6571 |
[32m[20221213 12:34:45 @agent_ppo2.py:179][0m |          -0.0496 |          26.0896 |         -58.1237 |
[32m[20221213 12:34:45 @agent_ppo2.py:179][0m |          -0.0585 |          26.0400 |         -59.5087 |
[32m[20221213 12:34:45 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:34:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.83
[32m[20221213 12:34:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 280.34
[32m[20221213 12:34:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.41
[32m[20221213 12:34:45 @agent_ppo2.py:137][0m Total time:      18.80 min
[32m[20221213 12:34:45 @agent_ppo2.py:139][0m 1253376 total steps have happened
[32m[20221213 12:34:45 @agent_ppo2.py:115][0m #------------------------ Iteration 612 --------------------------#
[32m[20221213 12:34:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:45 @agent_ppo2.py:179][0m |           0.1191 |          27.1746 |         -48.9437 |
[32m[20221213 12:34:45 @agent_ppo2.py:179][0m |           0.1100 |          26.1196 |         -29.7890 |
[32m[20221213 12:34:45 @agent_ppo2.py:179][0m |           0.1605 |          25.5009 |         -26.9977 |
[32m[20221213 12:34:46 @agent_ppo2.py:179][0m |           0.0407 |          25.2399 |         -30.3833 |
[32m[20221213 12:34:46 @agent_ppo2.py:179][0m |           0.0165 |          24.9937 |         -39.2463 |
[32m[20221213 12:34:46 @agent_ppo2.py:179][0m |           0.0017 |          24.7965 |         -43.0823 |
[32m[20221213 12:34:46 @agent_ppo2.py:179][0m |          -0.0094 |          24.6964 |         -46.3731 |
[32m[20221213 12:34:46 @agent_ppo2.py:179][0m |          -0.0205 |          24.4692 |         -48.4910 |
[32m[20221213 12:34:46 @agent_ppo2.py:179][0m |          -0.0214 |          26.2682 |         -50.7646 |
[32m[20221213 12:34:46 @agent_ppo2.py:179][0m |          -0.0291 |          24.3745 |         -51.9984 |
[32m[20221213 12:34:46 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:34:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 283.79
[32m[20221213 12:34:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.25
[32m[20221213 12:34:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.77
[32m[20221213 12:34:47 @agent_ppo2.py:137][0m Total time:      18.83 min
[32m[20221213 12:34:47 @agent_ppo2.py:139][0m 1255424 total steps have happened
[32m[20221213 12:34:47 @agent_ppo2.py:115][0m #------------------------ Iteration 613 --------------------------#
[32m[20221213 12:34:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:47 @agent_ppo2.py:179][0m |           0.0536 |          27.2448 |         -43.5128 |
[32m[20221213 12:34:47 @agent_ppo2.py:179][0m |           0.0177 |          25.6236 |         -43.7907 |
[32m[20221213 12:34:47 @agent_ppo2.py:179][0m |          -0.0141 |          25.0670 |         -46.1180 |
[32m[20221213 12:34:47 @agent_ppo2.py:179][0m |          -0.0178 |          24.6466 |         -47.5299 |
[32m[20221213 12:34:47 @agent_ppo2.py:179][0m |          -0.0270 |          24.3534 |         -46.6981 |
[32m[20221213 12:34:48 @agent_ppo2.py:179][0m |          -0.0303 |          24.9735 |         -47.8989 |
[32m[20221213 12:34:48 @agent_ppo2.py:179][0m |          -0.0429 |          23.9620 |         -49.8121 |
[32m[20221213 12:34:48 @agent_ppo2.py:179][0m |          -0.0404 |          23.6222 |         -49.9684 |
[32m[20221213 12:34:48 @agent_ppo2.py:179][0m |          -0.0321 |          23.4823 |         -50.1260 |
[32m[20221213 12:34:48 @agent_ppo2.py:179][0m |          -0.0387 |          23.2122 |         -51.3136 |
[32m[20221213 12:34:48 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:34:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 278.69
[32m[20221213 12:34:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 311.77
[32m[20221213 12:34:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.81
[32m[20221213 12:34:48 @agent_ppo2.py:137][0m Total time:      18.86 min
[32m[20221213 12:34:48 @agent_ppo2.py:139][0m 1257472 total steps have happened
[32m[20221213 12:34:48 @agent_ppo2.py:115][0m #------------------------ Iteration 614 --------------------------#
[32m[20221213 12:34:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:49 @agent_ppo2.py:179][0m |           0.0697 |          27.5783 |         -44.9628 |
[32m[20221213 12:34:49 @agent_ppo2.py:179][0m |           0.0432 |          26.4565 |         -41.1521 |
[32m[20221213 12:34:49 @agent_ppo2.py:179][0m |           0.0160 |          26.2549 |         -45.8719 |
[32m[20221213 12:34:49 @agent_ppo2.py:179][0m |           0.0027 |          28.9237 |         -51.3214 |
[32m[20221213 12:34:49 @agent_ppo2.py:179][0m |          -0.0245 |          25.4931 |         -53.7380 |
[32m[20221213 12:34:49 @agent_ppo2.py:179][0m |          -0.0316 |          25.1079 |         -55.4967 |
[32m[20221213 12:34:49 @agent_ppo2.py:179][0m |          -0.0359 |          24.9735 |         -55.6095 |
[32m[20221213 12:34:50 @agent_ppo2.py:179][0m |          -0.0383 |          24.7987 |         -57.2730 |
[32m[20221213 12:34:50 @agent_ppo2.py:179][0m |          -0.0264 |          24.7196 |         -57.0574 |
[32m[20221213 12:34:50 @agent_ppo2.py:179][0m |          -0.0431 |          24.5787 |         -58.9547 |
[32m[20221213 12:34:50 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.90
[32m[20221213 12:34:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 298.07
[32m[20221213 12:34:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 56.40
[32m[20221213 12:34:50 @agent_ppo2.py:137][0m Total time:      18.89 min
[32m[20221213 12:34:50 @agent_ppo2.py:139][0m 1259520 total steps have happened
[32m[20221213 12:34:50 @agent_ppo2.py:115][0m #------------------------ Iteration 615 --------------------------#
[32m[20221213 12:34:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:50 @agent_ppo2.py:179][0m |           0.0621 |          27.1004 |         -50.1572 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |           0.0275 |          26.2787 |         -47.5452 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0012 |          25.7255 |         -52.6367 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0122 |          25.4894 |         -54.8803 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0291 |          25.3552 |         -58.3637 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0314 |          25.0859 |         -60.5498 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0324 |          24.9232 |         -62.2836 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0366 |          24.9833 |         -64.2901 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0346 |          24.6787 |         -63.7888 |
[32m[20221213 12:34:51 @agent_ppo2.py:179][0m |          -0.0233 |          25.6231 |         -64.3557 |
[32m[20221213 12:34:51 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.72
[32m[20221213 12:34:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.26
[32m[20221213 12:34:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.45
[32m[20221213 12:34:52 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 346.45
[32m[20221213 12:34:52 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 346.45
[32m[20221213 12:34:52 @agent_ppo2.py:137][0m Total time:      18.92 min
[32m[20221213 12:34:52 @agent_ppo2.py:139][0m 1261568 total steps have happened
[32m[20221213 12:34:52 @agent_ppo2.py:115][0m #------------------------ Iteration 616 --------------------------#
[32m[20221213 12:34:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:52 @agent_ppo2.py:179][0m |           0.1181 |          28.2314 |         -46.3334 |
[32m[20221213 12:34:52 @agent_ppo2.py:179][0m |           0.1439 |          25.1679 |         -28.2898 |
[32m[20221213 12:34:52 @agent_ppo2.py:179][0m |           0.0738 |          24.8028 |         -31.7210 |
[32m[20221213 12:34:52 @agent_ppo2.py:179][0m |           0.0268 |          24.0948 |         -36.6468 |
[32m[20221213 12:34:53 @agent_ppo2.py:179][0m |          -0.0021 |          23.7279 |         -42.4898 |
[32m[20221213 12:34:53 @agent_ppo2.py:179][0m |          -0.0093 |          23.5163 |         -44.8995 |
[32m[20221213 12:34:53 @agent_ppo2.py:179][0m |          -0.0164 |          23.3251 |         -46.2684 |
[32m[20221213 12:34:53 @agent_ppo2.py:179][0m |          -0.0187 |          23.1477 |         -47.9676 |
[32m[20221213 12:34:53 @agent_ppo2.py:179][0m |          -0.0166 |          22.8849 |         -47.5117 |
[32m[20221213 12:34:53 @agent_ppo2.py:179][0m |          -0.0364 |          22.5920 |         -49.3019 |
[32m[20221213 12:34:53 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 273.08
[32m[20221213 12:34:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 318.85
[32m[20221213 12:34:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 308.95
[32m[20221213 12:34:53 @agent_ppo2.py:137][0m Total time:      18.95 min
[32m[20221213 12:34:53 @agent_ppo2.py:139][0m 1263616 total steps have happened
[32m[20221213 12:34:53 @agent_ppo2.py:115][0m #------------------------ Iteration 617 --------------------------#
[32m[20221213 12:34:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:54 @agent_ppo2.py:179][0m |           0.1073 |          27.8459 |         -43.9639 |
[32m[20221213 12:34:54 @agent_ppo2.py:179][0m |           0.0387 |          26.4579 |         -36.7901 |
[32m[20221213 12:34:54 @agent_ppo2.py:179][0m |          -0.0020 |          26.0410 |         -47.2725 |
[32m[20221213 12:34:54 @agent_ppo2.py:179][0m |          -0.0136 |          25.6615 |         -49.9460 |
[32m[20221213 12:34:54 @agent_ppo2.py:179][0m |          -0.0195 |          25.4355 |         -51.6386 |
[32m[20221213 12:34:54 @agent_ppo2.py:179][0m |          -0.0142 |          28.0923 |         -52.8125 |
[32m[20221213 12:34:55 @agent_ppo2.py:179][0m |          -0.0276 |          25.1575 |         -53.2601 |
[32m[20221213 12:34:55 @agent_ppo2.py:179][0m |          -0.0358 |          24.8403 |         -54.2778 |
[32m[20221213 12:34:55 @agent_ppo2.py:179][0m |          -0.0390 |          24.9715 |         -55.6644 |
[32m[20221213 12:34:55 @agent_ppo2.py:179][0m |          -0.0408 |          24.6306 |         -56.4190 |
[32m[20221213 12:34:55 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:34:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.78
[32m[20221213 12:34:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.29
[32m[20221213 12:34:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.44
[32m[20221213 12:34:55 @agent_ppo2.py:137][0m Total time:      18.97 min
[32m[20221213 12:34:55 @agent_ppo2.py:139][0m 1265664 total steps have happened
[32m[20221213 12:34:55 @agent_ppo2.py:115][0m #------------------------ Iteration 618 --------------------------#
[32m[20221213 12:34:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |           0.0773 |          24.1636 |         -48.7954 |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |           0.0426 |          23.5585 |         -46.4073 |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |           0.0115 |          21.4100 |         -47.3800 |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |          -0.0187 |          20.9895 |         -48.7668 |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |          -0.0287 |          20.7201 |         -49.8541 |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |          -0.0361 |          20.4520 |         -51.5336 |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |          -0.0399 |          20.3613 |         -53.8233 |
[32m[20221213 12:34:56 @agent_ppo2.py:179][0m |          -0.0426 |          20.2465 |         -54.2157 |
[32m[20221213 12:34:57 @agent_ppo2.py:179][0m |          -0.0438 |          20.2998 |         -55.8710 |
[32m[20221213 12:34:57 @agent_ppo2.py:179][0m |          -0.0446 |          19.9220 |         -55.7364 |
[32m[20221213 12:34:57 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:34:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.01
[32m[20221213 12:34:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.70
[32m[20221213 12:34:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 308.74
[32m[20221213 12:34:57 @agent_ppo2.py:137][0m Total time:      19.00 min
[32m[20221213 12:34:57 @agent_ppo2.py:139][0m 1267712 total steps have happened
[32m[20221213 12:34:57 @agent_ppo2.py:115][0m #------------------------ Iteration 619 --------------------------#
[32m[20221213 12:34:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:34:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:57 @agent_ppo2.py:179][0m |           0.0722 |          24.6999 |         -50.0447 |
[32m[20221213 12:34:57 @agent_ppo2.py:179][0m |           0.0411 |          23.5613 |         -46.5072 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0045 |          23.0824 |         -54.4819 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0238 |          22.5665 |         -56.1722 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0283 |          22.4119 |         -57.7369 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0326 |          22.0302 |         -58.5297 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0377 |          21.9032 |         -59.7730 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0340 |          21.6999 |         -61.5010 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0439 |          21.6690 |         -62.1440 |
[32m[20221213 12:34:58 @agent_ppo2.py:179][0m |          -0.0364 |          21.9975 |         -61.5853 |
[32m[20221213 12:34:58 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:34:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.06
[32m[20221213 12:34:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 309.43
[32m[20221213 12:34:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 300.65
[32m[20221213 12:34:59 @agent_ppo2.py:137][0m Total time:      19.03 min
[32m[20221213 12:34:59 @agent_ppo2.py:139][0m 1269760 total steps have happened
[32m[20221213 12:34:59 @agent_ppo2.py:115][0m #------------------------ Iteration 620 --------------------------#
[32m[20221213 12:34:59 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:34:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:34:59 @agent_ppo2.py:179][0m |           0.0621 |          26.4705 |         -52.9407 |
[32m[20221213 12:34:59 @agent_ppo2.py:179][0m |           0.0468 |          24.8429 |         -43.4193 |
[32m[20221213 12:34:59 @agent_ppo2.py:179][0m |           0.0146 |          24.3118 |         -48.5274 |
[32m[20221213 12:34:59 @agent_ppo2.py:179][0m |          -0.0136 |          23.9918 |         -50.3182 |
[32m[20221213 12:34:59 @agent_ppo2.py:179][0m |          -0.0301 |          23.7552 |         -52.3062 |
[32m[20221213 12:35:00 @agent_ppo2.py:179][0m |          -0.0333 |          23.5682 |         -53.4035 |
[32m[20221213 12:35:00 @agent_ppo2.py:179][0m |          -0.0315 |          24.0196 |         -54.3930 |
[32m[20221213 12:35:00 @agent_ppo2.py:179][0m |          -0.0362 |          23.2602 |         -54.8489 |
[32m[20221213 12:35:00 @agent_ppo2.py:179][0m |          -0.0443 |          23.1935 |         -56.0821 |
[32m[20221213 12:35:00 @agent_ppo2.py:179][0m |          -0.0405 |          22.9746 |         -56.9463 |
[32m[20221213 12:35:00 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:35:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.00
[32m[20221213 12:35:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 281.98
[32m[20221213 12:35:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 248.49
[32m[20221213 12:35:00 @agent_ppo2.py:137][0m Total time:      19.06 min
[32m[20221213 12:35:00 @agent_ppo2.py:139][0m 1271808 total steps have happened
[32m[20221213 12:35:00 @agent_ppo2.py:115][0m #------------------------ Iteration 621 --------------------------#
[32m[20221213 12:35:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:01 @agent_ppo2.py:179][0m |           0.0649 |          17.8986 |         -53.0439 |
[32m[20221213 12:35:01 @agent_ppo2.py:179][0m |           0.0453 |          16.4693 |         -46.3198 |
[32m[20221213 12:35:01 @agent_ppo2.py:179][0m |           0.0003 |          15.7844 |         -54.7721 |
[32m[20221213 12:35:01 @agent_ppo2.py:179][0m |          -0.0161 |          15.3877 |         -57.1159 |
[32m[20221213 12:35:01 @agent_ppo2.py:179][0m |          -0.0175 |          14.9076 |         -58.8467 |
[32m[20221213 12:35:01 @agent_ppo2.py:179][0m |          -0.0252 |          14.5228 |         -60.9066 |
[32m[20221213 12:35:01 @agent_ppo2.py:179][0m |          -0.0338 |          14.3769 |         -61.2720 |
[32m[20221213 12:35:02 @agent_ppo2.py:179][0m |          -0.0383 |          14.1029 |         -63.7583 |
[32m[20221213 12:35:02 @agent_ppo2.py:179][0m |          -0.0356 |          13.9156 |         -63.4961 |
[32m[20221213 12:35:02 @agent_ppo2.py:179][0m |          -0.0281 |          13.9730 |         -63.1901 |
[32m[20221213 12:35:02 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 186.61
[32m[20221213 12:35:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 307.87
[32m[20221213 12:35:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 305.09
[32m[20221213 12:35:02 @agent_ppo2.py:137][0m Total time:      19.09 min
[32m[20221213 12:35:02 @agent_ppo2.py:139][0m 1273856 total steps have happened
[32m[20221213 12:35:02 @agent_ppo2.py:115][0m #------------------------ Iteration 622 --------------------------#
[32m[20221213 12:35:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:02 @agent_ppo2.py:179][0m |           0.0796 |          30.6304 |         -50.0008 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |           0.0266 |          29.2189 |         -49.0097 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |          -0.0048 |          28.6698 |         -50.8108 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |          -0.0254 |          28.3049 |         -53.1632 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |          -0.0335 |          28.0083 |         -55.5187 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |          -0.0402 |          27.8442 |         -55.3920 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |          -0.0398 |          27.6090 |         -55.7598 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |          -0.0475 |          27.3673 |         -57.7356 |
[32m[20221213 12:35:03 @agent_ppo2.py:179][0m |          -0.0449 |          27.3024 |         -58.6263 |
[32m[20221213 12:35:04 @agent_ppo2.py:179][0m |          -0.0432 |          27.2130 |         -58.3228 |
[32m[20221213 12:35:04 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:35:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.19
[32m[20221213 12:35:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 311.65
[32m[20221213 12:35:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.02
[32m[20221213 12:35:04 @agent_ppo2.py:137][0m Total time:      19.12 min
[32m[20221213 12:35:04 @agent_ppo2.py:139][0m 1275904 total steps have happened
[32m[20221213 12:35:04 @agent_ppo2.py:115][0m #------------------------ Iteration 623 --------------------------#
[32m[20221213 12:35:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:04 @agent_ppo2.py:179][0m |           0.1226 |          24.1428 |         -45.0007 |
[32m[20221213 12:35:04 @agent_ppo2.py:179][0m |           0.1103 |          23.1594 |         -31.4410 |
[32m[20221213 12:35:04 @agent_ppo2.py:179][0m |           0.0104 |          22.7300 |         -47.8525 |
[32m[20221213 12:35:05 @agent_ppo2.py:179][0m |          -0.0118 |          22.3612 |         -51.4585 |
[32m[20221213 12:35:05 @agent_ppo2.py:179][0m |          -0.0249 |          22.1331 |         -53.7420 |
[32m[20221213 12:35:05 @agent_ppo2.py:179][0m |          -0.0316 |          21.9676 |         -54.9171 |
[32m[20221213 12:35:05 @agent_ppo2.py:179][0m |          -0.0365 |          21.7639 |         -55.8962 |
[32m[20221213 12:35:05 @agent_ppo2.py:179][0m |          -0.0402 |          21.6408 |         -57.4306 |
[32m[20221213 12:35:05 @agent_ppo2.py:179][0m |          -0.0419 |          21.5982 |         -57.7107 |
[32m[20221213 12:35:05 @agent_ppo2.py:179][0m |          -0.0427 |          21.5612 |         -59.4712 |
[32m[20221213 12:35:05 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:35:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.84
[32m[20221213 12:35:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.88
[32m[20221213 12:35:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 325.51
[32m[20221213 12:35:05 @agent_ppo2.py:137][0m Total time:      19.15 min
[32m[20221213 12:35:05 @agent_ppo2.py:139][0m 1277952 total steps have happened
[32m[20221213 12:35:05 @agent_ppo2.py:115][0m #------------------------ Iteration 624 --------------------------#
[32m[20221213 12:35:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:06 @agent_ppo2.py:179][0m |           0.0388 |          13.7777 |         -55.8689 |
[32m[20221213 12:35:06 @agent_ppo2.py:179][0m |           0.0086 |          12.3835 |         -55.9952 |
[32m[20221213 12:35:06 @agent_ppo2.py:179][0m |          -0.0007 |          11.7432 |         -57.0176 |
[32m[20221213 12:35:06 @agent_ppo2.py:179][0m |          -0.0174 |          11.4479 |         -57.4369 |
[32m[20221213 12:35:06 @agent_ppo2.py:179][0m |          -0.0141 |          11.1919 |         -57.7900 |
[32m[20221213 12:35:06 @agent_ppo2.py:179][0m |          -0.0211 |          11.0233 |         -58.4353 |
[32m[20221213 12:35:07 @agent_ppo2.py:179][0m |          -0.0290 |          10.8246 |         -59.4716 |
[32m[20221213 12:35:07 @agent_ppo2.py:179][0m |          -0.0347 |          10.6728 |         -60.6227 |
[32m[20221213 12:35:07 @agent_ppo2.py:179][0m |          -0.0281 |          10.5545 |         -59.8560 |
[32m[20221213 12:35:07 @agent_ppo2.py:179][0m |          -0.0299 |          10.4823 |         -61.1230 |
[32m[20221213 12:35:07 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:35:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.60
[32m[20221213 12:35:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.22
[32m[20221213 12:35:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.40
[32m[20221213 12:35:07 @agent_ppo2.py:137][0m Total time:      19.17 min
[32m[20221213 12:35:07 @agent_ppo2.py:139][0m 1280000 total steps have happened
[32m[20221213 12:35:07 @agent_ppo2.py:115][0m #------------------------ Iteration 625 --------------------------#
[32m[20221213 12:35:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |           0.0534 |          28.2408 |         -52.3745 |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |           0.0210 |          26.2498 |         -52.6689 |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |           0.0038 |          25.7460 |         -51.8000 |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |          -0.0229 |          25.4967 |         -53.8827 |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |          -0.0228 |          25.8093 |         -54.3635 |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |          -0.0347 |          25.0970 |         -55.7932 |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |          -0.0478 |          24.7086 |         -59.1317 |
[32m[20221213 12:35:08 @agent_ppo2.py:179][0m |          -0.0467 |          24.6381 |         -59.1231 |
[32m[20221213 12:35:09 @agent_ppo2.py:179][0m |          -0.0524 |          24.4723 |         -60.0594 |
[32m[20221213 12:35:09 @agent_ppo2.py:179][0m |          -0.0540 |          24.3347 |         -60.8725 |
[32m[20221213 12:35:09 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:35:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.30
[32m[20221213 12:35:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 305.67
[32m[20221213 12:35:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.61
[32m[20221213 12:35:09 @agent_ppo2.py:137][0m Total time:      19.20 min
[32m[20221213 12:35:09 @agent_ppo2.py:139][0m 1282048 total steps have happened
[32m[20221213 12:35:09 @agent_ppo2.py:115][0m #------------------------ Iteration 626 --------------------------#
[32m[20221213 12:35:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:09 @agent_ppo2.py:179][0m |           0.0815 |          21.3273 |         -46.0668 |
[32m[20221213 12:35:09 @agent_ppo2.py:179][0m |           0.0491 |          20.3552 |         -40.2744 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |           0.0118 |          19.9023 |         -46.0966 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |          -0.0053 |          19.5872 |         -48.2906 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |          -0.0217 |          19.3623 |         -50.4960 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |          -0.0305 |          19.2466 |         -52.3698 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |          -0.0293 |          19.1724 |         -53.2068 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |          -0.0394 |          18.8560 |         -54.9499 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |          -0.0354 |          19.0617 |         -54.5702 |
[32m[20221213 12:35:10 @agent_ppo2.py:179][0m |          -0.0395 |          18.6897 |         -55.1107 |
[32m[20221213 12:35:10 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.78
[32m[20221213 12:35:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 297.41
[32m[20221213 12:35:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.35
[32m[20221213 12:35:11 @agent_ppo2.py:137][0m Total time:      19.23 min
[32m[20221213 12:35:11 @agent_ppo2.py:139][0m 1284096 total steps have happened
[32m[20221213 12:35:11 @agent_ppo2.py:115][0m #------------------------ Iteration 627 --------------------------#
[32m[20221213 12:35:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:11 @agent_ppo2.py:179][0m |           0.0557 |          21.7416 |         -44.8655 |
[32m[20221213 12:35:11 @agent_ppo2.py:179][0m |           0.0466 |          20.3201 |         -40.3999 |
[32m[20221213 12:35:11 @agent_ppo2.py:179][0m |          -0.0017 |          19.6995 |         -45.7813 |
[32m[20221213 12:35:11 @agent_ppo2.py:179][0m |          -0.0229 |          19.2602 |         -49.0454 |
[32m[20221213 12:35:12 @agent_ppo2.py:179][0m |          -0.0342 |          18.8683 |         -50.5809 |
[32m[20221213 12:35:12 @agent_ppo2.py:179][0m |          -0.0415 |          18.5779 |         -50.5854 |
[32m[20221213 12:35:12 @agent_ppo2.py:179][0m |          -0.0442 |          18.2792 |         -52.3532 |
[32m[20221213 12:35:12 @agent_ppo2.py:179][0m |          -0.0459 |          18.0844 |         -53.6619 |
[32m[20221213 12:35:12 @agent_ppo2.py:179][0m |          -0.0493 |          17.8364 |         -53.5168 |
[32m[20221213 12:35:12 @agent_ppo2.py:179][0m |          -0.0401 |          19.9565 |         -54.0273 |
[32m[20221213 12:35:12 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:35:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.68
[32m[20221213 12:35:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 304.26
[32m[20221213 12:35:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.87
[32m[20221213 12:35:12 @agent_ppo2.py:137][0m Total time:      19.26 min
[32m[20221213 12:35:12 @agent_ppo2.py:139][0m 1286144 total steps have happened
[32m[20221213 12:35:12 @agent_ppo2.py:115][0m #------------------------ Iteration 628 --------------------------#
[32m[20221213 12:35:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:13 @agent_ppo2.py:179][0m |           0.0724 |          27.6984 |         -47.5333 |
[32m[20221213 12:35:13 @agent_ppo2.py:179][0m |           0.0232 |          25.9412 |         -43.2772 |
[32m[20221213 12:35:13 @agent_ppo2.py:179][0m |          -0.0098 |          25.0893 |         -48.7738 |
[32m[20221213 12:35:13 @agent_ppo2.py:179][0m |          -0.0207 |          24.4345 |         -52.2711 |
[32m[20221213 12:35:13 @agent_ppo2.py:179][0m |          -0.0286 |          24.1855 |         -53.7276 |
[32m[20221213 12:35:14 @agent_ppo2.py:179][0m |          -0.0409 |          23.7905 |         -54.7039 |
[32m[20221213 12:35:14 @agent_ppo2.py:179][0m |          -0.0411 |          23.6149 |         -54.9662 |
[32m[20221213 12:35:14 @agent_ppo2.py:179][0m |          -0.0336 |          26.8843 |         -56.4417 |
[32m[20221213 12:35:14 @agent_ppo2.py:179][0m |          -0.0406 |          23.2106 |         -57.3713 |
[32m[20221213 12:35:14 @agent_ppo2.py:179][0m |          -0.0454 |          22.8776 |         -56.7236 |
[32m[20221213 12:35:14 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:35:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 281.81
[32m[20221213 12:35:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 294.49
[32m[20221213 12:35:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.78
[32m[20221213 12:35:14 @agent_ppo2.py:137][0m Total time:      19.29 min
[32m[20221213 12:35:14 @agent_ppo2.py:139][0m 1288192 total steps have happened
[32m[20221213 12:35:14 @agent_ppo2.py:115][0m #------------------------ Iteration 629 --------------------------#
[32m[20221213 12:35:15 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:35:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:15 @agent_ppo2.py:179][0m |           0.0988 |          27.0919 |         -42.6212 |
[32m[20221213 12:35:15 @agent_ppo2.py:179][0m |           0.1073 |          27.9026 |         -23.3766 |
[32m[20221213 12:35:15 @agent_ppo2.py:179][0m |           0.0710 |          25.8114 |         -24.1111 |
[32m[20221213 12:35:15 @agent_ppo2.py:179][0m |           0.0553 |          25.5967 |         -31.1292 |
[32m[20221213 12:35:15 @agent_ppo2.py:179][0m |           0.0376 |          29.4548 |         -39.5414 |
[32m[20221213 12:35:15 @agent_ppo2.py:179][0m |           0.0078 |          25.4489 |         -43.8765 |
[32m[20221213 12:35:15 @agent_ppo2.py:179][0m |          -0.0059 |          25.1950 |         -47.5200 |
[32m[20221213 12:35:16 @agent_ppo2.py:179][0m |          -0.0098 |          25.1788 |         -48.3240 |
[32m[20221213 12:35:16 @agent_ppo2.py:179][0m |          -0.0176 |          25.1236 |         -50.2321 |
[32m[20221213 12:35:16 @agent_ppo2.py:179][0m |          -0.0274 |          25.0344 |         -52.0273 |
[32m[20221213 12:35:16 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:35:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.85
[32m[20221213 12:35:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.42
[32m[20221213 12:35:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.14
[32m[20221213 12:35:16 @agent_ppo2.py:137][0m Total time:      19.32 min
[32m[20221213 12:35:16 @agent_ppo2.py:139][0m 1290240 total steps have happened
[32m[20221213 12:35:16 @agent_ppo2.py:115][0m #------------------------ Iteration 630 --------------------------#
[32m[20221213 12:35:16 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:35:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |           0.1530 |          29.0682 |         -49.6505 |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |           0.0886 |          27.1358 |         -33.8083 |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |           0.0426 |          26.7549 |         -37.6763 |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |           0.0097 |          26.6031 |         -46.9811 |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |           0.0045 |          26.5167 |         -48.5761 |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |          -0.0090 |          26.4083 |         -51.0616 |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |          -0.0244 |          26.3640 |         -54.1388 |
[32m[20221213 12:35:17 @agent_ppo2.py:179][0m |          -0.0255 |          26.2416 |         -55.3297 |
[32m[20221213 12:35:18 @agent_ppo2.py:179][0m |          -0.0309 |          26.2580 |         -55.8778 |
[32m[20221213 12:35:18 @agent_ppo2.py:179][0m |          -0.0303 |          26.1880 |         -57.3262 |
[32m[20221213 12:35:18 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:35:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.03
[32m[20221213 12:35:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.64
[32m[20221213 12:35:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.05
[32m[20221213 12:35:18 @agent_ppo2.py:137][0m Total time:      19.35 min
[32m[20221213 12:35:18 @agent_ppo2.py:139][0m 1292288 total steps have happened
[32m[20221213 12:35:18 @agent_ppo2.py:115][0m #------------------------ Iteration 631 --------------------------#
[32m[20221213 12:35:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:18 @agent_ppo2.py:179][0m |           0.0666 |          30.9633 |         -46.4166 |
[32m[20221213 12:35:18 @agent_ppo2.py:179][0m |           0.0824 |          27.8763 |         -30.9553 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |           0.0204 |          28.6845 |         -38.7683 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |          -0.0219 |          27.0067 |         -43.4638 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |          -0.0262 |          26.6959 |         -44.4358 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |          -0.0383 |          26.5659 |         -46.0282 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |          -0.0382 |          26.4024 |         -45.5229 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |          -0.0395 |          26.8817 |         -47.4204 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |          -0.0460 |          26.1394 |         -48.5152 |
[32m[20221213 12:35:19 @agent_ppo2.py:179][0m |          -0.0495 |          26.0192 |         -50.0743 |
[32m[20221213 12:35:19 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:35:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 281.38
[32m[20221213 12:35:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 299.19
[32m[20221213 12:35:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 278.13
[32m[20221213 12:35:20 @agent_ppo2.py:137][0m Total time:      19.38 min
[32m[20221213 12:35:20 @agent_ppo2.py:139][0m 1294336 total steps have happened
[32m[20221213 12:35:20 @agent_ppo2.py:115][0m #------------------------ Iteration 632 --------------------------#
[32m[20221213 12:35:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:20 @agent_ppo2.py:179][0m |           0.0590 |          26.9984 |         -39.3522 |
[32m[20221213 12:35:20 @agent_ppo2.py:179][0m |           0.0295 |          25.2496 |         -38.0952 |
[32m[20221213 12:35:20 @agent_ppo2.py:179][0m |           0.0268 |          24.5109 |         -38.4033 |
[32m[20221213 12:35:20 @agent_ppo2.py:179][0m |           0.0102 |          24.0403 |         -37.5877 |
[32m[20221213 12:35:20 @agent_ppo2.py:179][0m |          -0.0172 |          23.6314 |         -42.4809 |
[32m[20221213 12:35:21 @agent_ppo2.py:179][0m |          -0.0310 |          23.2466 |         -45.2401 |
[32m[20221213 12:35:21 @agent_ppo2.py:179][0m |          -0.0312 |          22.9886 |         -47.3182 |
[32m[20221213 12:35:21 @agent_ppo2.py:179][0m |          -0.0346 |          22.7656 |         -47.9092 |
[32m[20221213 12:35:21 @agent_ppo2.py:179][0m |          -0.0280 |          22.5388 |         -46.8562 |
[32m[20221213 12:35:21 @agent_ppo2.py:179][0m |          -0.0278 |          23.8397 |         -49.0349 |
[32m[20221213 12:35:21 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.64
[32m[20221213 12:35:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 319.41
[32m[20221213 12:35:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.55
[32m[20221213 12:35:21 @agent_ppo2.py:137][0m Total time:      19.41 min
[32m[20221213 12:35:21 @agent_ppo2.py:139][0m 1296384 total steps have happened
[32m[20221213 12:35:21 @agent_ppo2.py:115][0m #------------------------ Iteration 633 --------------------------#
[32m[20221213 12:35:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.0529 |           7.8256 |         -37.6892 |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.1437 |           7.3654 |         -11.0147 |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.1892 |           7.1685 |          -2.6536 |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.0970 |           7.1173 |          -4.4039 |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.0801 |           7.1313 |          -9.9156 |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.0803 |           7.1125 |          -7.9571 |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.0778 |           7.1043 |          -9.3604 |
[32m[20221213 12:35:22 @agent_ppo2.py:179][0m |           0.0730 |           7.1145 |         -10.8637 |
[32m[20221213 12:35:23 @agent_ppo2.py:179][0m |           0.0764 |           7.0822 |          -9.0470 |
[32m[20221213 12:35:23 @agent_ppo2.py:179][0m |           0.0667 |           7.0816 |         -10.8791 |
[32m[20221213 12:35:23 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:35:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.68
[32m[20221213 12:35:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 43.88
[32m[20221213 12:35:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.78
[32m[20221213 12:35:23 @agent_ppo2.py:137][0m Total time:      19.44 min
[32m[20221213 12:35:23 @agent_ppo2.py:139][0m 1298432 total steps have happened
[32m[20221213 12:35:23 @agent_ppo2.py:115][0m #------------------------ Iteration 634 --------------------------#
[32m[20221213 12:35:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:23 @agent_ppo2.py:179][0m |           0.0614 |          21.3226 |         -31.5474 |
[32m[20221213 12:35:23 @agent_ppo2.py:179][0m |           0.0658 |          18.1319 |         -26.5895 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |           0.0010 |          17.2694 |         -27.4953 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |          -0.0239 |          16.6779 |         -31.2249 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |          -0.0315 |          16.4356 |         -33.1254 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |          -0.0449 |          16.1207 |         -34.2242 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |          -0.0499 |          15.9935 |         -35.6839 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |          -0.0464 |          15.8268 |         -36.1066 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |          -0.0526 |          15.6653 |         -36.5996 |
[32m[20221213 12:35:24 @agent_ppo2.py:179][0m |          -0.0567 |          15.5453 |         -37.8782 |
[32m[20221213 12:35:24 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:35:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.35
[32m[20221213 12:35:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 216.39
[32m[20221213 12:35:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 284.69
[32m[20221213 12:35:25 @agent_ppo2.py:137][0m Total time:      19.46 min
[32m[20221213 12:35:25 @agent_ppo2.py:139][0m 1300480 total steps have happened
[32m[20221213 12:35:25 @agent_ppo2.py:115][0m #------------------------ Iteration 635 --------------------------#
[32m[20221213 12:35:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:25 @agent_ppo2.py:179][0m |           0.0596 |          20.4960 |         -37.2666 |
[32m[20221213 12:35:25 @agent_ppo2.py:179][0m |           0.0328 |          19.5568 |         -35.3660 |
[32m[20221213 12:35:25 @agent_ppo2.py:179][0m |           0.0096 |          20.1624 |         -35.1940 |
[32m[20221213 12:35:25 @agent_ppo2.py:179][0m |          -0.0066 |          18.6318 |         -36.4765 |
[32m[20221213 12:35:26 @agent_ppo2.py:179][0m |          -0.0177 |          18.3982 |         -37.7190 |
[32m[20221213 12:35:26 @agent_ppo2.py:179][0m |          -0.0298 |          18.1963 |         -39.1847 |
[32m[20221213 12:35:26 @agent_ppo2.py:179][0m |          -0.0415 |          18.0331 |         -39.8823 |
[32m[20221213 12:35:26 @agent_ppo2.py:179][0m |          -0.0313 |          17.9218 |         -39.0098 |
[32m[20221213 12:35:26 @agent_ppo2.py:179][0m |          -0.0393 |          17.7653 |         -40.0540 |
[32m[20221213 12:35:26 @agent_ppo2.py:179][0m |          -0.0442 |          17.7371 |         -42.3454 |
[32m[20221213 12:35:26 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 207.12
[32m[20221213 12:35:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.11
[32m[20221213 12:35:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.17
[32m[20221213 12:35:26 @agent_ppo2.py:137][0m Total time:      19.49 min
[32m[20221213 12:35:26 @agent_ppo2.py:139][0m 1302528 total steps have happened
[32m[20221213 12:35:26 @agent_ppo2.py:115][0m #------------------------ Iteration 636 --------------------------#
[32m[20221213 12:35:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:27 @agent_ppo2.py:179][0m |           0.0874 |          15.2729 |         -39.5445 |
[32m[20221213 12:35:27 @agent_ppo2.py:179][0m |           0.0390 |          10.7491 |         -34.9552 |
[32m[20221213 12:35:27 @agent_ppo2.py:179][0m |           0.0351 |          10.3172 |         -32.7497 |
[32m[20221213 12:35:27 @agent_ppo2.py:179][0m |          -0.0018 |          10.1185 |         -33.4793 |
[32m[20221213 12:35:27 @agent_ppo2.py:179][0m |          -0.0215 |          10.0482 |         -39.0356 |
[32m[20221213 12:35:27 @agent_ppo2.py:179][0m |          -0.0249 |           9.8504 |         -39.5364 |
[32m[20221213 12:35:27 @agent_ppo2.py:179][0m |          -0.0295 |           9.8188 |         -36.2592 |
[32m[20221213 12:35:28 @agent_ppo2.py:179][0m |          -0.0208 |           9.7246 |         -36.8756 |
[32m[20221213 12:35:28 @agent_ppo2.py:179][0m |          -0.0269 |           9.6030 |         -38.8082 |
[32m[20221213 12:35:28 @agent_ppo2.py:179][0m |          -0.0307 |           9.5396 |         -39.9432 |
[32m[20221213 12:35:28 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:35:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.92
[32m[20221213 12:35:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 262.82
[32m[20221213 12:35:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.13
[32m[20221213 12:35:28 @agent_ppo2.py:137][0m Total time:      19.52 min
[32m[20221213 12:35:28 @agent_ppo2.py:139][0m 1304576 total steps have happened
[32m[20221213 12:35:28 @agent_ppo2.py:115][0m #------------------------ Iteration 637 --------------------------#
[32m[20221213 12:35:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:28 @agent_ppo2.py:179][0m |           0.0772 |          31.1630 |         -32.6489 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |           0.0398 |          28.3316 |         -26.6801 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |          -0.0035 |          27.5503 |         -31.7060 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |          -0.0196 |          27.1723 |         -32.8545 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |          -0.0318 |          26.8306 |         -35.6408 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |          -0.0366 |          26.7157 |         -36.4587 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |          -0.0423 |          26.5301 |         -37.2143 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |          -0.0349 |          27.7561 |         -38.2845 |
[32m[20221213 12:35:29 @agent_ppo2.py:179][0m |          -0.0469 |          26.2083 |         -39.0461 |
[32m[20221213 12:35:30 @agent_ppo2.py:179][0m |          -0.0385 |          28.0990 |         -39.3278 |
[32m[20221213 12:35:30 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:35:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 273.45
[32m[20221213 12:35:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 310.33
[32m[20221213 12:35:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.68
[32m[20221213 12:35:30 @agent_ppo2.py:137][0m Total time:      19.55 min
[32m[20221213 12:35:30 @agent_ppo2.py:139][0m 1306624 total steps have happened
[32m[20221213 12:35:30 @agent_ppo2.py:115][0m #------------------------ Iteration 638 --------------------------#
[32m[20221213 12:35:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:30 @agent_ppo2.py:179][0m |           0.0568 |          20.7343 |         -39.2889 |
[32m[20221213 12:35:30 @agent_ppo2.py:179][0m |           0.0468 |          19.4001 |         -33.8704 |
[32m[20221213 12:35:30 @agent_ppo2.py:179][0m |           0.0121 |          19.5316 |         -37.2476 |
[32m[20221213 12:35:31 @agent_ppo2.py:179][0m |           0.0116 |          18.8162 |         -36.9439 |
[32m[20221213 12:35:31 @agent_ppo2.py:179][0m |          -0.0211 |          18.5252 |         -41.5848 |
[32m[20221213 12:35:31 @agent_ppo2.py:179][0m |          -0.0353 |          18.3618 |         -42.5392 |
[32m[20221213 12:35:31 @agent_ppo2.py:179][0m |          -0.0289 |          18.2707 |         -42.2228 |
[32m[20221213 12:35:31 @agent_ppo2.py:179][0m |          -0.0329 |          18.1247 |         -43.4709 |
[32m[20221213 12:35:31 @agent_ppo2.py:179][0m |          -0.0356 |          20.2136 |         -43.7928 |
[32m[20221213 12:35:31 @agent_ppo2.py:179][0m |          -0.0458 |          18.0666 |         -44.6670 |
[32m[20221213 12:35:31 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:35:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.80
[32m[20221213 12:35:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 289.04
[32m[20221213 12:35:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.27
[32m[20221213 12:35:31 @agent_ppo2.py:137][0m Total time:      19.58 min
[32m[20221213 12:35:31 @agent_ppo2.py:139][0m 1308672 total steps have happened
[32m[20221213 12:35:31 @agent_ppo2.py:115][0m #------------------------ Iteration 639 --------------------------#
[32m[20221213 12:35:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:32 @agent_ppo2.py:179][0m |           0.0580 |          15.7038 |         -40.4401 |
[32m[20221213 12:35:32 @agent_ppo2.py:179][0m |           0.0355 |          14.5400 |         -37.5392 |
[32m[20221213 12:35:32 @agent_ppo2.py:179][0m |           0.0003 |          14.1085 |         -39.1813 |
[32m[20221213 12:35:32 @agent_ppo2.py:179][0m |          -0.0114 |          14.0331 |         -39.8053 |
[32m[20221213 12:35:32 @agent_ppo2.py:179][0m |          -0.0312 |          13.7350 |         -41.7384 |
[32m[20221213 12:35:33 @agent_ppo2.py:179][0m |          -0.0302 |          13.5236 |         -41.9525 |
[32m[20221213 12:35:33 @agent_ppo2.py:179][0m |          -0.0328 |          13.4082 |         -44.5206 |
[32m[20221213 12:35:33 @agent_ppo2.py:179][0m |          -0.0353 |          13.2987 |         -45.2300 |
[32m[20221213 12:35:33 @agent_ppo2.py:179][0m |          -0.0316 |          13.2151 |         -44.0439 |
[32m[20221213 12:35:33 @agent_ppo2.py:179][0m |          -0.0429 |          13.1463 |         -45.3836 |
[32m[20221213 12:35:33 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.84
[32m[20221213 12:35:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 271.08
[32m[20221213 12:35:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.04
[32m[20221213 12:35:33 @agent_ppo2.py:137][0m Total time:      19.61 min
[32m[20221213 12:35:33 @agent_ppo2.py:139][0m 1310720 total steps have happened
[32m[20221213 12:35:33 @agent_ppo2.py:115][0m #------------------------ Iteration 640 --------------------------#
[32m[20221213 12:35:33 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:35:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |           0.0399 |          10.3989 |         -43.4480 |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |           0.0206 |           9.8367 |         -40.3488 |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |           0.0089 |           9.5695 |         -41.9687 |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |          -0.0074 |           9.4682 |         -37.8098 |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |          -0.0215 |           9.3684 |         -43.0535 |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |          -0.0121 |           9.2559 |         -43.5270 |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |          -0.0137 |           9.0506 |         -42.6322 |
[32m[20221213 12:35:34 @agent_ppo2.py:179][0m |          -0.0122 |           8.9494 |         -40.6905 |
[32m[20221213 12:35:35 @agent_ppo2.py:179][0m |           0.0027 |           9.0053 |         -33.1650 |
[32m[20221213 12:35:35 @agent_ppo2.py:179][0m |           0.0018 |           8.8028 |         -37.3594 |
[32m[20221213 12:35:35 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:35:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.93
[32m[20221213 12:35:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 268.07
[32m[20221213 12:35:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.67
[32m[20221213 12:35:35 @agent_ppo2.py:137][0m Total time:      19.64 min
[32m[20221213 12:35:35 @agent_ppo2.py:139][0m 1312768 total steps have happened
[32m[20221213 12:35:35 @agent_ppo2.py:115][0m #------------------------ Iteration 641 --------------------------#
[32m[20221213 12:35:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:35 @agent_ppo2.py:179][0m |           0.0547 |          13.1955 |         -38.2722 |
[32m[20221213 12:35:35 @agent_ppo2.py:179][0m |           0.0418 |          12.2482 |         -38.6469 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |           0.0140 |          11.3669 |         -37.5082 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |           0.0001 |          11.0478 |         -39.2695 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |          -0.0178 |          10.7084 |         -40.0380 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |          -0.0157 |          11.1812 |         -40.4766 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |          -0.0204 |          10.6388 |         -40.7096 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |          -0.0300 |          10.1432 |         -40.6269 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |          -0.0305 |          10.0643 |         -41.5438 |
[32m[20221213 12:35:36 @agent_ppo2.py:179][0m |          -0.0328 |           9.9619 |         -42.6349 |
[32m[20221213 12:35:36 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:35:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 153.62
[32m[20221213 12:35:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 318.57
[32m[20221213 12:35:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.25
[32m[20221213 12:35:37 @agent_ppo2.py:137][0m Total time:      19.66 min
[32m[20221213 12:35:37 @agent_ppo2.py:139][0m 1314816 total steps have happened
[32m[20221213 12:35:37 @agent_ppo2.py:115][0m #------------------------ Iteration 642 --------------------------#
[32m[20221213 12:35:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:37 @agent_ppo2.py:179][0m |           0.0759 |          27.2695 |         -38.5189 |
[32m[20221213 12:35:37 @agent_ppo2.py:179][0m |           0.0442 |          25.6106 |         -31.8404 |
[32m[20221213 12:35:37 @agent_ppo2.py:179][0m |           0.0149 |          24.6124 |         -33.5791 |
[32m[20221213 12:35:37 @agent_ppo2.py:179][0m |          -0.0127 |          24.1133 |         -36.9138 |
[32m[20221213 12:35:37 @agent_ppo2.py:179][0m |          -0.0325 |          23.6039 |         -40.1711 |
[32m[20221213 12:35:38 @agent_ppo2.py:179][0m |          -0.0406 |          23.2507 |         -41.8383 |
[32m[20221213 12:35:38 @agent_ppo2.py:179][0m |          -0.0319 |          22.9356 |         -42.2947 |
[32m[20221213 12:35:38 @agent_ppo2.py:179][0m |          -0.0430 |          22.6970 |         -43.3817 |
[32m[20221213 12:35:38 @agent_ppo2.py:179][0m |          -0.0379 |          25.8698 |         -44.9361 |
[32m[20221213 12:35:38 @agent_ppo2.py:179][0m |          -0.0548 |          22.3134 |         -46.4086 |
[32m[20221213 12:35:38 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:35:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.28
[32m[20221213 12:35:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 309.72
[32m[20221213 12:35:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 296.99
[32m[20221213 12:35:38 @agent_ppo2.py:137][0m Total time:      19.69 min
[32m[20221213 12:35:38 @agent_ppo2.py:139][0m 1316864 total steps have happened
[32m[20221213 12:35:38 @agent_ppo2.py:115][0m #------------------------ Iteration 643 --------------------------#
[32m[20221213 12:35:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:39 @agent_ppo2.py:179][0m |           0.0545 |          23.2290 |         -40.7937 |
[32m[20221213 12:35:39 @agent_ppo2.py:179][0m |           0.0488 |          22.1629 |         -38.1497 |
[32m[20221213 12:35:39 @agent_ppo2.py:179][0m |          -0.0030 |          21.7296 |         -41.3837 |
[32m[20221213 12:35:39 @agent_ppo2.py:179][0m |          -0.0151 |          21.4517 |         -43.3715 |
[32m[20221213 12:35:39 @agent_ppo2.py:179][0m |          -0.0207 |          21.2239 |         -44.8870 |
[32m[20221213 12:35:39 @agent_ppo2.py:179][0m |          -0.0280 |          21.0644 |         -47.2119 |
[32m[20221213 12:35:39 @agent_ppo2.py:179][0m |          -0.0312 |          21.1039 |         -47.1285 |
[32m[20221213 12:35:40 @agent_ppo2.py:179][0m |          -0.0330 |          20.8078 |         -47.8576 |
[32m[20221213 12:35:40 @agent_ppo2.py:179][0m |          -0.0317 |          20.7099 |         -48.5253 |
[32m[20221213 12:35:40 @agent_ppo2.py:179][0m |          -0.0367 |          20.6521 |         -49.0208 |
[32m[20221213 12:35:40 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.32
[32m[20221213 12:35:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 306.49
[32m[20221213 12:35:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 312.83
[32m[20221213 12:35:40 @agent_ppo2.py:137][0m Total time:      19.72 min
[32m[20221213 12:35:40 @agent_ppo2.py:139][0m 1318912 total steps have happened
[32m[20221213 12:35:40 @agent_ppo2.py:115][0m #------------------------ Iteration 644 --------------------------#
[32m[20221213 12:35:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:40 @agent_ppo2.py:179][0m |           0.0656 |          25.3127 |         -39.4424 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |           0.0369 |          22.7407 |         -34.6661 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |           0.0121 |          23.9284 |         -36.1382 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |          -0.0194 |          21.9695 |         -40.2632 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |          -0.0244 |          21.6391 |         -41.7598 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |          -0.0339 |          21.4234 |         -42.4985 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |          -0.0329 |          21.4863 |         -43.4875 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |          -0.0429 |          21.2431 |         -44.3462 |
[32m[20221213 12:35:41 @agent_ppo2.py:179][0m |          -0.0366 |          21.1285 |         -45.7932 |
[32m[20221213 12:35:42 @agent_ppo2.py:179][0m |          -0.0364 |          21.0119 |         -45.6273 |
[32m[20221213 12:35:42 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:35:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.97
[32m[20221213 12:35:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.51
[32m[20221213 12:35:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.07
[32m[20221213 12:35:42 @agent_ppo2.py:137][0m Total time:      19.75 min
[32m[20221213 12:35:42 @agent_ppo2.py:139][0m 1320960 total steps have happened
[32m[20221213 12:35:42 @agent_ppo2.py:115][0m #------------------------ Iteration 645 --------------------------#
[32m[20221213 12:35:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:42 @agent_ppo2.py:179][0m |           0.0613 |          16.8353 |         -41.8193 |
[32m[20221213 12:35:42 @agent_ppo2.py:179][0m |           0.0586 |          15.1838 |         -38.7441 |
[32m[20221213 12:35:42 @agent_ppo2.py:179][0m |           0.0191 |          14.6857 |         -40.5702 |
[32m[20221213 12:35:43 @agent_ppo2.py:179][0m |          -0.0049 |          14.4375 |         -42.6658 |
[32m[20221213 12:35:43 @agent_ppo2.py:179][0m |          -0.0211 |          14.2708 |         -44.0502 |
[32m[20221213 12:35:43 @agent_ppo2.py:179][0m |          -0.0261 |          14.0780 |         -45.5311 |
[32m[20221213 12:35:43 @agent_ppo2.py:179][0m |          -0.0281 |          13.9252 |         -45.9497 |
[32m[20221213 12:35:43 @agent_ppo2.py:179][0m |          -0.0354 |          13.8177 |         -45.9489 |
[32m[20221213 12:35:43 @agent_ppo2.py:179][0m |          -0.0399 |          13.9073 |         -47.6470 |
[32m[20221213 12:35:43 @agent_ppo2.py:179][0m |          -0.0304 |          13.7584 |         -47.8636 |
[32m[20221213 12:35:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:35:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.39
[32m[20221213 12:35:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 292.52
[32m[20221213 12:35:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.86
[32m[20221213 12:35:43 @agent_ppo2.py:137][0m Total time:      19.78 min
[32m[20221213 12:35:43 @agent_ppo2.py:139][0m 1323008 total steps have happened
[32m[20221213 12:35:43 @agent_ppo2.py:115][0m #------------------------ Iteration 646 --------------------------#
[32m[20221213 12:35:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:44 @agent_ppo2.py:179][0m |           0.0725 |          28.3172 |         -39.9823 |
[32m[20221213 12:35:44 @agent_ppo2.py:179][0m |           0.0833 |          27.4106 |         -24.6673 |
[32m[20221213 12:35:44 @agent_ppo2.py:179][0m |           0.0533 |          26.7805 |         -21.6232 |
[32m[20221213 12:35:44 @agent_ppo2.py:179][0m |           0.0137 |          26.4629 |         -33.4367 |
[32m[20221213 12:35:44 @agent_ppo2.py:179][0m |           0.0048 |          27.1477 |         -35.1553 |
[32m[20221213 12:35:44 @agent_ppo2.py:179][0m |          -0.0157 |          25.9740 |         -38.7285 |
[32m[20221213 12:35:45 @agent_ppo2.py:179][0m |          -0.0213 |          25.8694 |         -39.9954 |
[32m[20221213 12:35:45 @agent_ppo2.py:179][0m |          -0.0301 |          26.5709 |         -42.6772 |
[32m[20221213 12:35:45 @agent_ppo2.py:179][0m |          -0.0347 |          25.6767 |         -43.8650 |
[32m[20221213 12:35:45 @agent_ppo2.py:179][0m |          -0.0410 |          25.5557 |         -45.0936 |
[32m[20221213 12:35:45 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:35:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 298.73
[32m[20221213 12:35:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 304.91
[32m[20221213 12:35:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.97
[32m[20221213 12:35:45 @agent_ppo2.py:137][0m Total time:      19.81 min
[32m[20221213 12:35:45 @agent_ppo2.py:139][0m 1325056 total steps have happened
[32m[20221213 12:35:45 @agent_ppo2.py:115][0m #------------------------ Iteration 647 --------------------------#
[32m[20221213 12:35:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |           0.0438 |          21.3295 |         -38.5888 |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |           0.0484 |          21.5363 |         -35.0513 |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |           0.0104 |          19.6348 |         -36.4638 |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |          -0.0155 |          19.3154 |         -37.1882 |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |          -0.0269 |          19.2026 |         -38.0149 |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |          -0.0206 |          22.1531 |         -39.0344 |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |          -0.0205 |          20.5313 |         -40.2580 |
[32m[20221213 12:35:46 @agent_ppo2.py:179][0m |          -0.0218 |          18.8016 |         -38.7745 |
[32m[20221213 12:35:47 @agent_ppo2.py:179][0m |          -0.0339 |          18.6554 |         -39.6275 |
[32m[20221213 12:35:47 @agent_ppo2.py:179][0m |          -0.0365 |          18.6231 |         -40.5522 |
[32m[20221213 12:35:47 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:35:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.40
[32m[20221213 12:35:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 299.51
[32m[20221213 12:35:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.04
[32m[20221213 12:35:47 @agent_ppo2.py:137][0m Total time:      19.84 min
[32m[20221213 12:35:47 @agent_ppo2.py:139][0m 1327104 total steps have happened
[32m[20221213 12:35:47 @agent_ppo2.py:115][0m #------------------------ Iteration 648 --------------------------#
[32m[20221213 12:35:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:47 @agent_ppo2.py:179][0m |           0.0449 |          20.8637 |         -37.8908 |
[32m[20221213 12:35:47 @agent_ppo2.py:179][0m |           0.0538 |          19.0365 |         -32.6783 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |           0.0062 |          18.2671 |         -34.9921 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |          -0.0148 |          17.6793 |         -36.6441 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |          -0.0268 |          17.4085 |         -38.8595 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |          -0.0353 |          17.2556 |         -39.0099 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |          -0.0357 |          16.7628 |         -40.7895 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |          -0.0382 |          16.5882 |         -40.4877 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |          -0.0324 |          16.8763 |         -41.0223 |
[32m[20221213 12:35:48 @agent_ppo2.py:179][0m |          -0.0398 |          16.4478 |         -42.5199 |
[32m[20221213 12:35:48 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 168.62
[32m[20221213 12:35:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 297.20
[32m[20221213 12:35:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.20
[32m[20221213 12:35:49 @agent_ppo2.py:137][0m Total time:      19.86 min
[32m[20221213 12:35:49 @agent_ppo2.py:139][0m 1329152 total steps have happened
[32m[20221213 12:35:49 @agent_ppo2.py:115][0m #------------------------ Iteration 649 --------------------------#
[32m[20221213 12:35:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:49 @agent_ppo2.py:179][0m |           0.0928 |          25.2997 |         -34.8439 |
[32m[20221213 12:35:49 @agent_ppo2.py:179][0m |           0.0287 |          22.2494 |         -33.7850 |
[32m[20221213 12:35:49 @agent_ppo2.py:179][0m |          -0.0046 |          21.5513 |         -36.3712 |
[32m[20221213 12:35:49 @agent_ppo2.py:179][0m |          -0.0165 |          21.1495 |         -36.7212 |
[32m[20221213 12:35:49 @agent_ppo2.py:179][0m |          -0.0241 |          20.9733 |         -39.3298 |
[32m[20221213 12:35:50 @agent_ppo2.py:179][0m |          -0.0241 |          20.5939 |         -39.0701 |
[32m[20221213 12:35:50 @agent_ppo2.py:179][0m |          -0.0204 |          23.4564 |         -40.6083 |
[32m[20221213 12:35:50 @agent_ppo2.py:179][0m |          -0.0425 |          20.2730 |         -41.5585 |
[32m[20221213 12:35:50 @agent_ppo2.py:179][0m |          -0.0464 |          20.1249 |         -42.8861 |
[32m[20221213 12:35:50 @agent_ppo2.py:179][0m |          -0.0461 |          20.0208 |         -43.2114 |
[32m[20221213 12:35:50 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.90
[32m[20221213 12:35:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.76
[32m[20221213 12:35:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 324.77
[32m[20221213 12:35:50 @agent_ppo2.py:137][0m Total time:      19.89 min
[32m[20221213 12:35:50 @agent_ppo2.py:139][0m 1331200 total steps have happened
[32m[20221213 12:35:50 @agent_ppo2.py:115][0m #------------------------ Iteration 650 --------------------------#
[32m[20221213 12:35:50 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:35:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:51 @agent_ppo2.py:179][0m |           0.0734 |          29.8591 |         -39.6689 |
[32m[20221213 12:35:51 @agent_ppo2.py:179][0m |           0.0274 |          26.2334 |         -36.6211 |
[32m[20221213 12:35:51 @agent_ppo2.py:179][0m |          -0.0067 |          25.5012 |         -41.1186 |
[32m[20221213 12:35:51 @agent_ppo2.py:179][0m |          -0.0304 |          24.7594 |         -44.2544 |
[32m[20221213 12:35:51 @agent_ppo2.py:179][0m |          -0.0373 |          24.2857 |         -43.6033 |
[32m[20221213 12:35:51 @agent_ppo2.py:179][0m |          -0.0398 |          24.7402 |         -44.7569 |
[32m[20221213 12:35:51 @agent_ppo2.py:179][0m |          -0.0500 |          23.6352 |         -45.8335 |
[32m[20221213 12:35:52 @agent_ppo2.py:179][0m |          -0.0554 |          23.4319 |         -47.0546 |
[32m[20221213 12:35:52 @agent_ppo2.py:179][0m |          -0.0516 |          23.2491 |         -46.7997 |
[32m[20221213 12:35:52 @agent_ppo2.py:179][0m |          -0.0518 |          22.9433 |         -46.3979 |
[32m[20221213 12:35:52 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 274.12
[32m[20221213 12:35:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 312.93
[32m[20221213 12:35:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 318.77
[32m[20221213 12:35:52 @agent_ppo2.py:137][0m Total time:      19.92 min
[32m[20221213 12:35:52 @agent_ppo2.py:139][0m 1333248 total steps have happened
[32m[20221213 12:35:52 @agent_ppo2.py:115][0m #------------------------ Iteration 651 --------------------------#
[32m[20221213 12:35:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:52 @agent_ppo2.py:179][0m |           0.0697 |          15.3389 |         -43.4266 |
[32m[20221213 12:35:52 @agent_ppo2.py:179][0m |           0.0549 |          14.7466 |         -40.8772 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |           0.0347 |          14.6404 |         -42.6884 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |           0.0151 |          14.9348 |         -38.5387 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |          -0.0101 |          14.2556 |         -42.5806 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |          -0.0092 |          13.9577 |         -46.3398 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |          -0.0233 |          13.8734 |         -48.5353 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |          -0.0301 |          13.7915 |         -48.0287 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |          -0.0309 |          13.7375 |         -49.9069 |
[32m[20221213 12:35:53 @agent_ppo2.py:179][0m |          -0.0157 |          16.7947 |         -51.0767 |
[32m[20221213 12:35:53 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:35:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 147.26
[32m[20221213 12:35:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 305.69
[32m[20221213 12:35:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.03
[32m[20221213 12:35:54 @agent_ppo2.py:137][0m Total time:      19.95 min
[32m[20221213 12:35:54 @agent_ppo2.py:139][0m 1335296 total steps have happened
[32m[20221213 12:35:54 @agent_ppo2.py:115][0m #------------------------ Iteration 652 --------------------------#
[32m[20221213 12:35:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:54 @agent_ppo2.py:179][0m |           0.0687 |          26.7670 |         -47.3106 |
[32m[20221213 12:35:54 @agent_ppo2.py:179][0m |           0.0963 |          25.2547 |         -31.5513 |
[32m[20221213 12:35:54 @agent_ppo2.py:179][0m |           0.0438 |          24.6234 |         -39.5521 |
[32m[20221213 12:35:54 @agent_ppo2.py:179][0m |           0.0095 |          24.6456 |         -41.9751 |
[32m[20221213 12:35:55 @agent_ppo2.py:179][0m |          -0.0071 |          24.2890 |         -42.9032 |
[32m[20221213 12:35:55 @agent_ppo2.py:179][0m |          -0.0261 |          23.7788 |         -47.3912 |
[32m[20221213 12:35:55 @agent_ppo2.py:179][0m |          -0.0207 |          24.2711 |         -48.4839 |
[32m[20221213 12:35:55 @agent_ppo2.py:179][0m |           0.0006 |          24.8152 |         -46.1996 |
[32m[20221213 12:35:55 @agent_ppo2.py:179][0m |          -0.0164 |          23.3881 |         -46.5142 |
[32m[20221213 12:35:55 @agent_ppo2.py:179][0m |          -0.0316 |          23.1537 |         -49.5070 |
[32m[20221213 12:35:55 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:35:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.94
[32m[20221213 12:35:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.47
[32m[20221213 12:35:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 72.60
[32m[20221213 12:35:55 @agent_ppo2.py:137][0m Total time:      19.98 min
[32m[20221213 12:35:55 @agent_ppo2.py:139][0m 1337344 total steps have happened
[32m[20221213 12:35:55 @agent_ppo2.py:115][0m #------------------------ Iteration 653 --------------------------#
[32m[20221213 12:35:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:56 @agent_ppo2.py:179][0m |           0.0820 |          26.4472 |         -43.9547 |
[32m[20221213 12:35:56 @agent_ppo2.py:179][0m |           0.0253 |          24.0089 |         -43.9112 |
[32m[20221213 12:35:56 @agent_ppo2.py:179][0m |          -0.0004 |          24.5932 |         -46.1879 |
[32m[20221213 12:35:56 @agent_ppo2.py:179][0m |          -0.0276 |          23.4106 |         -47.3372 |
[32m[20221213 12:35:56 @agent_ppo2.py:179][0m |          -0.0288 |          23.1554 |         -48.8822 |
[32m[20221213 12:35:56 @agent_ppo2.py:179][0m |          -0.0377 |          23.1449 |         -48.5941 |
[32m[20221213 12:35:56 @agent_ppo2.py:179][0m |          -0.0364 |          23.0190 |         -49.9631 |
[32m[20221213 12:35:57 @agent_ppo2.py:179][0m |          -0.0399 |          22.8783 |         -50.7954 |
[32m[20221213 12:35:57 @agent_ppo2.py:179][0m |          -0.0373 |          22.7124 |         -50.7803 |
[32m[20221213 12:35:57 @agent_ppo2.py:179][0m |          -0.0454 |          22.6327 |         -52.4037 |
[32m[20221213 12:35:57 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:35:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.07
[32m[20221213 12:35:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.91
[32m[20221213 12:35:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 342.25
[32m[20221213 12:35:57 @agent_ppo2.py:137][0m Total time:      20.00 min
[32m[20221213 12:35:57 @agent_ppo2.py:139][0m 1339392 total steps have happened
[32m[20221213 12:35:57 @agent_ppo2.py:115][0m #------------------------ Iteration 654 --------------------------#
[32m[20221213 12:35:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:57 @agent_ppo2.py:179][0m |           0.0522 |          28.4474 |         -40.5620 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |           0.0440 |          27.4103 |         -33.3284 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |           0.0107 |          27.1363 |         -40.0578 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |          -0.0085 |          26.9635 |         -40.8616 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |          -0.0230 |          26.7504 |         -44.2501 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |          -0.0214 |          27.0431 |         -45.9884 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |          -0.0313 |          26.4559 |         -46.0854 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |          -0.0295 |          26.3313 |         -46.7873 |
[32m[20221213 12:35:58 @agent_ppo2.py:179][0m |          -0.0229 |          27.9433 |         -47.5979 |
[32m[20221213 12:35:59 @agent_ppo2.py:179][0m |          -0.0358 |          26.0985 |         -48.1657 |
[32m[20221213 12:35:59 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:35:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.29
[32m[20221213 12:35:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.26
[32m[20221213 12:35:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.83
[32m[20221213 12:35:59 @agent_ppo2.py:137][0m Total time:      20.03 min
[32m[20221213 12:35:59 @agent_ppo2.py:139][0m 1341440 total steps have happened
[32m[20221213 12:35:59 @agent_ppo2.py:115][0m #------------------------ Iteration 655 --------------------------#
[32m[20221213 12:35:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:35:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:35:59 @agent_ppo2.py:179][0m |           0.0609 |          29.8280 |         -45.5723 |
[32m[20221213 12:35:59 @agent_ppo2.py:179][0m |           0.0321 |          25.7479 |         -40.7311 |
[32m[20221213 12:35:59 @agent_ppo2.py:179][0m |           0.0228 |          25.1690 |         -38.1522 |
[32m[20221213 12:36:00 @agent_ppo2.py:179][0m |          -0.0199 |          24.9052 |         -43.0291 |
[32m[20221213 12:36:00 @agent_ppo2.py:179][0m |          -0.0280 |          24.6165 |         -45.1286 |
[32m[20221213 12:36:00 @agent_ppo2.py:179][0m |          -0.0328 |          24.4740 |         -46.6422 |
[32m[20221213 12:36:00 @agent_ppo2.py:179][0m |          -0.0293 |          24.2504 |         -46.6918 |
[32m[20221213 12:36:00 @agent_ppo2.py:179][0m |          -0.0377 |          24.2057 |         -50.2846 |
[32m[20221213 12:36:00 @agent_ppo2.py:179][0m |          -0.0440 |          23.9334 |         -50.4925 |
[32m[20221213 12:36:00 @agent_ppo2.py:179][0m |          -0.0427 |          24.5571 |         -50.6952 |
[32m[20221213 12:36:00 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.03
[32m[20221213 12:36:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.52
[32m[20221213 12:36:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 250.10
[32m[20221213 12:36:00 @agent_ppo2.py:137][0m Total time:      20.06 min
[32m[20221213 12:36:00 @agent_ppo2.py:139][0m 1343488 total steps have happened
[32m[20221213 12:36:00 @agent_ppo2.py:115][0m #------------------------ Iteration 656 --------------------------#
[32m[20221213 12:36:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:01 @agent_ppo2.py:179][0m |           0.0818 |          20.5690 |         -45.6417 |
[32m[20221213 12:36:01 @agent_ppo2.py:179][0m |           0.0450 |          19.4810 |         -36.6201 |
[32m[20221213 12:36:01 @agent_ppo2.py:179][0m |          -0.0054 |          18.9797 |         -47.6125 |
[32m[20221213 12:36:01 @agent_ppo2.py:179][0m |          -0.0104 |          18.6035 |         -48.1129 |
[32m[20221213 12:36:01 @agent_ppo2.py:179][0m |          -0.0236 |          19.4058 |         -48.9796 |
[32m[20221213 12:36:01 @agent_ppo2.py:179][0m |          -0.0329 |          18.6617 |         -51.5394 |
[32m[20221213 12:36:02 @agent_ppo2.py:179][0m |          -0.0354 |          17.8791 |         -50.3792 |
[32m[20221213 12:36:02 @agent_ppo2.py:179][0m |          -0.0400 |          17.7914 |         -51.3948 |
[32m[20221213 12:36:02 @agent_ppo2.py:179][0m |          -0.0504 |          17.5371 |         -53.5673 |
[32m[20221213 12:36:02 @agent_ppo2.py:179][0m |          -0.0511 |          17.5723 |         -53.7947 |
[32m[20221213 12:36:02 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:36:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 192.92
[32m[20221213 12:36:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.18
[32m[20221213 12:36:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.44
[32m[20221213 12:36:02 @agent_ppo2.py:137][0m Total time:      20.09 min
[32m[20221213 12:36:02 @agent_ppo2.py:139][0m 1345536 total steps have happened
[32m[20221213 12:36:02 @agent_ppo2.py:115][0m #------------------------ Iteration 657 --------------------------#
[32m[20221213 12:36:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |           0.0648 |          30.6972 |         -47.1946 |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |           0.0555 |          29.5737 |         -38.6762 |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |           0.0186 |          29.0014 |         -43.1408 |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |          -0.0098 |          28.6059 |         -46.3713 |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |          -0.0261 |          28.3301 |         -48.7043 |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |          -0.0361 |          28.0707 |         -49.3534 |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |          -0.0450 |          27.8670 |         -51.3242 |
[32m[20221213 12:36:03 @agent_ppo2.py:179][0m |          -0.0419 |          27.5568 |         -52.0103 |
[32m[20221213 12:36:04 @agent_ppo2.py:179][0m |          -0.0466 |          27.4479 |         -53.2318 |
[32m[20221213 12:36:04 @agent_ppo2.py:179][0m |          -0.0498 |          27.2613 |         -53.2229 |
[32m[20221213 12:36:04 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:36:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.51
[32m[20221213 12:36:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 308.00
[32m[20221213 12:36:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.76
[32m[20221213 12:36:04 @agent_ppo2.py:137][0m Total time:      20.12 min
[32m[20221213 12:36:04 @agent_ppo2.py:139][0m 1347584 total steps have happened
[32m[20221213 12:36:04 @agent_ppo2.py:115][0m #------------------------ Iteration 658 --------------------------#
[32m[20221213 12:36:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:04 @agent_ppo2.py:179][0m |           0.1147 |          25.9906 |         -44.3034 |
[32m[20221213 12:36:04 @agent_ppo2.py:179][0m |           0.0657 |          24.7972 |         -38.6582 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |           0.0347 |          24.1781 |         -39.5416 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |           0.0184 |          25.2317 |         -41.6500 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |          -0.0053 |          24.8553 |         -44.7690 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |          -0.0114 |          23.2510 |         -44.7914 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |          -0.0253 |          23.1533 |         -46.9884 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |          -0.0210 |          25.9357 |         -48.7123 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |          -0.0376 |          23.8170 |         -50.7862 |
[32m[20221213 12:36:05 @agent_ppo2.py:179][0m |          -0.0399 |          22.4786 |         -52.0950 |
[32m[20221213 12:36:05 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:36:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.53
[32m[20221213 12:36:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.34
[32m[20221213 12:36:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 207.77
[32m[20221213 12:36:06 @agent_ppo2.py:137][0m Total time:      20.15 min
[32m[20221213 12:36:06 @agent_ppo2.py:139][0m 1349632 total steps have happened
[32m[20221213 12:36:06 @agent_ppo2.py:115][0m #------------------------ Iteration 659 --------------------------#
[32m[20221213 12:36:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:06 @agent_ppo2.py:179][0m |           0.0558 |          30.0767 |         -50.6929 |
[32m[20221213 12:36:06 @agent_ppo2.py:179][0m |           0.0261 |          28.4563 |         -44.8325 |
[32m[20221213 12:36:06 @agent_ppo2.py:179][0m |          -0.0048 |          27.6276 |         -49.2326 |
[32m[20221213 12:36:06 @agent_ppo2.py:179][0m |          -0.0258 |          27.0955 |         -52.5244 |
[32m[20221213 12:36:07 @agent_ppo2.py:179][0m |          -0.0302 |          27.1799 |         -54.4048 |
[32m[20221213 12:36:07 @agent_ppo2.py:179][0m |          -0.0389 |          26.3794 |         -55.1070 |
[32m[20221213 12:36:07 @agent_ppo2.py:179][0m |          -0.0427 |          26.1533 |         -54.8938 |
[32m[20221213 12:36:07 @agent_ppo2.py:179][0m |          -0.0345 |          27.9197 |         -55.7969 |
[32m[20221213 12:36:07 @agent_ppo2.py:179][0m |          -0.0421 |          25.9343 |         -55.9931 |
[32m[20221213 12:36:07 @agent_ppo2.py:179][0m |          -0.0422 |          26.7627 |         -58.2622 |
[32m[20221213 12:36:07 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:36:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.65
[32m[20221213 12:36:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.05
[32m[20221213 12:36:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.16
[32m[20221213 12:36:07 @agent_ppo2.py:137][0m Total time:      20.18 min
[32m[20221213 12:36:07 @agent_ppo2.py:139][0m 1351680 total steps have happened
[32m[20221213 12:36:07 @agent_ppo2.py:115][0m #------------------------ Iteration 660 --------------------------#
[32m[20221213 12:36:08 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:36:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:08 @agent_ppo2.py:179][0m |           0.0890 |          30.3728 |         -45.0286 |
[32m[20221213 12:36:08 @agent_ppo2.py:179][0m |           0.0642 |          28.4621 |         -35.4122 |
[32m[20221213 12:36:08 @agent_ppo2.py:179][0m |           0.0002 |          27.6409 |         -44.2839 |
[32m[20221213 12:36:08 @agent_ppo2.py:179][0m |          -0.0154 |          27.8321 |         -48.0966 |
[32m[20221213 12:36:08 @agent_ppo2.py:179][0m |          -0.0308 |          26.5415 |         -50.6609 |
[32m[20221213 12:36:08 @agent_ppo2.py:179][0m |          -0.0382 |          26.1043 |         -51.9361 |
[32m[20221213 12:36:09 @agent_ppo2.py:179][0m |          -0.0430 |          25.7891 |         -53.2963 |
[32m[20221213 12:36:09 @agent_ppo2.py:179][0m |          -0.0468 |          25.4720 |         -53.6808 |
[32m[20221213 12:36:09 @agent_ppo2.py:179][0m |          -0.0454 |          25.4552 |         -55.1378 |
[32m[20221213 12:36:09 @agent_ppo2.py:179][0m |          -0.0485 |          25.1537 |         -56.5178 |
[32m[20221213 12:36:09 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:36:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 283.76
[32m[20221213 12:36:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 312.22
[32m[20221213 12:36:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.30
[32m[20221213 12:36:09 @agent_ppo2.py:137][0m Total time:      20.21 min
[32m[20221213 12:36:09 @agent_ppo2.py:139][0m 1353728 total steps have happened
[32m[20221213 12:36:09 @agent_ppo2.py:115][0m #------------------------ Iteration 661 --------------------------#
[32m[20221213 12:36:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:09 @agent_ppo2.py:179][0m |           0.0865 |          17.9053 |         -47.3707 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |           0.0365 |          16.3479 |         -38.8685 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |          -0.0100 |          15.7597 |         -47.8006 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |          -0.0146 |          15.4188 |         -49.5262 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |          -0.0321 |          14.6557 |         -50.2935 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |          -0.0395 |          14.3051 |         -51.2846 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |          -0.0392 |          13.9333 |         -52.7741 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |          -0.0428 |          13.7187 |         -52.8046 |
[32m[20221213 12:36:10 @agent_ppo2.py:179][0m |          -0.0432 |          13.3604 |         -53.8647 |
[32m[20221213 12:36:11 @agent_ppo2.py:179][0m |          -0.0439 |          13.2191 |         -54.6801 |
[32m[20221213 12:36:11 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:36:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.23
[32m[20221213 12:36:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.33
[32m[20221213 12:36:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 275.63
[32m[20221213 12:36:11 @agent_ppo2.py:137][0m Total time:      20.23 min
[32m[20221213 12:36:11 @agent_ppo2.py:139][0m 1355776 total steps have happened
[32m[20221213 12:36:11 @agent_ppo2.py:115][0m #------------------------ Iteration 662 --------------------------#
[32m[20221213 12:36:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:11 @agent_ppo2.py:179][0m |           0.0559 |          25.6393 |         -52.3069 |
[32m[20221213 12:36:11 @agent_ppo2.py:179][0m |           0.0572 |          23.5321 |         -40.1450 |
[32m[20221213 12:36:11 @agent_ppo2.py:179][0m |           0.0035 |          22.5167 |         -45.3815 |
[32m[20221213 12:36:12 @agent_ppo2.py:179][0m |          -0.0180 |          21.8803 |         -49.6313 |
[32m[20221213 12:36:12 @agent_ppo2.py:179][0m |          -0.0322 |          21.5131 |         -53.0259 |
[32m[20221213 12:36:12 @agent_ppo2.py:179][0m |          -0.0325 |          21.3686 |         -55.5556 |
[32m[20221213 12:36:12 @agent_ppo2.py:179][0m |          -0.0433 |          20.8098 |         -56.8528 |
[32m[20221213 12:36:12 @agent_ppo2.py:179][0m |          -0.0463 |          20.5018 |         -59.0256 |
[32m[20221213 12:36:12 @agent_ppo2.py:179][0m |          -0.0486 |          20.3225 |         -59.5561 |
[32m[20221213 12:36:12 @agent_ppo2.py:179][0m |          -0.0472 |          19.9518 |         -60.0478 |
[32m[20221213 12:36:12 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:36:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 307.62
[32m[20221213 12:36:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.43
[32m[20221213 12:36:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.53
[32m[20221213 12:36:12 @agent_ppo2.py:137][0m Total time:      20.26 min
[32m[20221213 12:36:12 @agent_ppo2.py:139][0m 1357824 total steps have happened
[32m[20221213 12:36:12 @agent_ppo2.py:115][0m #------------------------ Iteration 663 --------------------------#
[32m[20221213 12:36:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:13 @agent_ppo2.py:179][0m |           0.0943 |          34.8632 |         -45.4634 |
[32m[20221213 12:36:13 @agent_ppo2.py:179][0m |           0.0586 |          30.9943 |         -37.5467 |
[32m[20221213 12:36:13 @agent_ppo2.py:179][0m |           0.0106 |          30.4756 |         -48.8760 |
[32m[20221213 12:36:13 @agent_ppo2.py:179][0m |          -0.0136 |          30.0372 |         -53.6991 |
[32m[20221213 12:36:13 @agent_ppo2.py:179][0m |          -0.0256 |          29.8273 |         -55.0641 |
[32m[20221213 12:36:13 @agent_ppo2.py:179][0m |          -0.0345 |          29.7581 |         -58.9017 |
[32m[20221213 12:36:14 @agent_ppo2.py:179][0m |          -0.0292 |          31.2025 |         -58.4004 |
[32m[20221213 12:36:14 @agent_ppo2.py:179][0m |          -0.0367 |          29.3925 |         -56.7388 |
[32m[20221213 12:36:14 @agent_ppo2.py:179][0m |          -0.0432 |          29.1819 |         -60.1999 |
[32m[20221213 12:36:14 @agent_ppo2.py:179][0m |          -0.0455 |          29.0744 |         -61.9471 |
[32m[20221213 12:36:14 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 295.94
[32m[20221213 12:36:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.62
[32m[20221213 12:36:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 264.98
[32m[20221213 12:36:14 @agent_ppo2.py:137][0m Total time:      20.29 min
[32m[20221213 12:36:14 @agent_ppo2.py:139][0m 1359872 total steps have happened
[32m[20221213 12:36:14 @agent_ppo2.py:115][0m #------------------------ Iteration 664 --------------------------#
[32m[20221213 12:36:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |           0.0626 |          22.1047 |         -49.2095 |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |           0.0245 |          19.5375 |         -48.7406 |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |          -0.0100 |          18.7507 |         -52.8387 |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |          -0.0201 |          18.1598 |         -54.1019 |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |          -0.0273 |          17.8992 |         -56.4036 |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |          -0.0206 |          18.7472 |         -58.2417 |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |          -0.0343 |          17.1728 |         -57.0481 |
[32m[20221213 12:36:15 @agent_ppo2.py:179][0m |          -0.0259 |          19.4050 |         -59.4420 |
[32m[20221213 12:36:16 @agent_ppo2.py:179][0m |          -0.0284 |          17.0634 |         -59.6160 |
[32m[20221213 12:36:16 @agent_ppo2.py:179][0m |          -0.0311 |          16.5873 |         -58.4453 |
[32m[20221213 12:36:16 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:36:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 187.28
[32m[20221213 12:36:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.00
[32m[20221213 12:36:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.59
[32m[20221213 12:36:16 @agent_ppo2.py:137][0m Total time:      20.32 min
[32m[20221213 12:36:16 @agent_ppo2.py:139][0m 1361920 total steps have happened
[32m[20221213 12:36:16 @agent_ppo2.py:115][0m #------------------------ Iteration 665 --------------------------#
[32m[20221213 12:36:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:16 @agent_ppo2.py:179][0m |           0.0829 |          29.9267 |         -53.6503 |
[32m[20221213 12:36:16 @agent_ppo2.py:179][0m |           0.0736 |          28.5794 |         -34.5864 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |           0.0376 |          29.3164 |         -45.4372 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |           0.0181 |          30.3530 |         -49.1894 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |           0.0045 |          27.3168 |         -51.1821 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |          -0.0028 |          28.9339 |         -54.5856 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |          -0.0155 |          26.8302 |         -56.8015 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |          -0.0198 |          26.5727 |         -58.0063 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |          -0.0290 |          26.5177 |         -60.8092 |
[32m[20221213 12:36:17 @agent_ppo2.py:179][0m |          -0.0265 |          26.5490 |         -61.8750 |
[32m[20221213 12:36:17 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:36:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.85
[32m[20221213 12:36:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.46
[32m[20221213 12:36:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.08
[32m[20221213 12:36:18 @agent_ppo2.py:137][0m Total time:      20.35 min
[32m[20221213 12:36:18 @agent_ppo2.py:139][0m 1363968 total steps have happened
[32m[20221213 12:36:18 @agent_ppo2.py:115][0m #------------------------ Iteration 666 --------------------------#
[32m[20221213 12:36:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:18 @agent_ppo2.py:179][0m |           0.1165 |          32.1858 |         -49.8305 |
[32m[20221213 12:36:18 @agent_ppo2.py:179][0m |           0.1129 |          31.9606 |         -25.6632 |
[32m[20221213 12:36:18 @agent_ppo2.py:179][0m |           0.0674 |          29.8587 |         -31.9885 |
[32m[20221213 12:36:18 @agent_ppo2.py:179][0m |           0.0333 |          28.7700 |         -39.2888 |
[32m[20221213 12:36:19 @agent_ppo2.py:179][0m |           0.0108 |          28.6721 |         -44.2189 |
[32m[20221213 12:36:19 @agent_ppo2.py:179][0m |           0.0054 |          28.5005 |         -48.6506 |
[32m[20221213 12:36:19 @agent_ppo2.py:179][0m |          -0.0064 |          28.5141 |         -51.8993 |
[32m[20221213 12:36:19 @agent_ppo2.py:179][0m |          -0.0171 |          28.3606 |         -56.5422 |
[32m[20221213 12:36:19 @agent_ppo2.py:179][0m |          -0.0214 |          28.2925 |         -58.9457 |
[32m[20221213 12:36:19 @agent_ppo2.py:179][0m |          -0.0249 |          28.5830 |         -60.1445 |
[32m[20221213 12:36:19 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:36:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 314.97
[32m[20221213 12:36:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.88
[32m[20221213 12:36:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 314.03
[32m[20221213 12:36:19 @agent_ppo2.py:137][0m Total time:      20.38 min
[32m[20221213 12:36:19 @agent_ppo2.py:139][0m 1366016 total steps have happened
[32m[20221213 12:36:19 @agent_ppo2.py:115][0m #------------------------ Iteration 667 --------------------------#
[32m[20221213 12:36:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:20 @agent_ppo2.py:179][0m |           0.0733 |          33.5800 |         -54.6212 |
[32m[20221213 12:36:20 @agent_ppo2.py:179][0m |           0.0576 |          31.5387 |         -46.1792 |
[32m[20221213 12:36:20 @agent_ppo2.py:179][0m |          -0.0011 |          30.8205 |         -53.4644 |
[32m[20221213 12:36:20 @agent_ppo2.py:179][0m |          -0.0178 |          30.5408 |         -55.7364 |
[32m[20221213 12:36:20 @agent_ppo2.py:179][0m |          -0.0236 |          33.0059 |         -57.8275 |
[32m[20221213 12:36:20 @agent_ppo2.py:179][0m |          -0.0463 |          29.8606 |         -59.6347 |
[32m[20221213 12:36:20 @agent_ppo2.py:179][0m |          -0.0470 |          29.5463 |         -62.0640 |
[32m[20221213 12:36:21 @agent_ppo2.py:179][0m |          -0.0377 |          31.6524 |         -63.2090 |
[32m[20221213 12:36:21 @agent_ppo2.py:179][0m |          -0.0533 |          29.4708 |         -63.5843 |
[32m[20221213 12:36:21 @agent_ppo2.py:179][0m |          -0.0486 |          28.9930 |         -64.3029 |
[32m[20221213 12:36:21 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:36:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.61
[32m[20221213 12:36:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.80
[32m[20221213 12:36:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.61
[32m[20221213 12:36:21 @agent_ppo2.py:137][0m Total time:      20.41 min
[32m[20221213 12:36:21 @agent_ppo2.py:139][0m 1368064 total steps have happened
[32m[20221213 12:36:21 @agent_ppo2.py:115][0m #------------------------ Iteration 668 --------------------------#
[32m[20221213 12:36:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:21 @agent_ppo2.py:179][0m |           0.0860 |          28.6463 |         -58.7000 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |           0.0540 |          27.1786 |         -55.3223 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |           0.0071 |          26.6186 |         -58.5043 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |          -0.0127 |          26.2350 |         -62.1888 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |          -0.0211 |          25.9387 |         -62.2535 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |          -0.0367 |          25.7682 |         -64.5609 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |          -0.0370 |          25.5604 |         -66.1983 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |          -0.0368 |          25.5498 |         -68.0900 |
[32m[20221213 12:36:22 @agent_ppo2.py:179][0m |          -0.0363 |          25.2239 |         -66.8975 |
[32m[20221213 12:36:23 @agent_ppo2.py:179][0m |          -0.0447 |          25.0838 |         -68.7595 |
[32m[20221213 12:36:23 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:36:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 211.69
[32m[20221213 12:36:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 308.72
[32m[20221213 12:36:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 304.68
[32m[20221213 12:36:23 @agent_ppo2.py:137][0m Total time:      20.43 min
[32m[20221213 12:36:23 @agent_ppo2.py:139][0m 1370112 total steps have happened
[32m[20221213 12:36:23 @agent_ppo2.py:115][0m #------------------------ Iteration 669 --------------------------#
[32m[20221213 12:36:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:23 @agent_ppo2.py:179][0m |           0.0789 |          23.1406 |         -59.3216 |
[32m[20221213 12:36:23 @agent_ppo2.py:179][0m |           0.0496 |          21.5203 |         -51.9722 |
[32m[20221213 12:36:23 @agent_ppo2.py:179][0m |           0.0064 |          20.9403 |         -56.7784 |
[32m[20221213 12:36:24 @agent_ppo2.py:179][0m |          -0.0140 |          21.0663 |         -59.4129 |
[32m[20221213 12:36:24 @agent_ppo2.py:179][0m |          -0.0138 |          22.9194 |         -60.1244 |
[32m[20221213 12:36:24 @agent_ppo2.py:179][0m |          -0.0330 |          19.6911 |         -62.9474 |
[32m[20221213 12:36:24 @agent_ppo2.py:179][0m |          -0.0456 |          19.4533 |         -64.5677 |
[32m[20221213 12:36:24 @agent_ppo2.py:179][0m |          -0.0342 |          19.1686 |         -65.4973 |
[32m[20221213 12:36:24 @agent_ppo2.py:179][0m |          -0.0408 |          18.9499 |         -66.8186 |
[32m[20221213 12:36:24 @agent_ppo2.py:179][0m |          -0.0493 |          18.8424 |         -67.8983 |
[32m[20221213 12:36:24 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.16
[32m[20221213 12:36:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.18
[32m[20221213 12:36:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.23
[32m[20221213 12:36:24 @agent_ppo2.py:137][0m Total time:      20.46 min
[32m[20221213 12:36:24 @agent_ppo2.py:139][0m 1372160 total steps have happened
[32m[20221213 12:36:24 @agent_ppo2.py:115][0m #------------------------ Iteration 670 --------------------------#
[32m[20221213 12:36:25 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:36:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:25 @agent_ppo2.py:179][0m |           0.0797 |          26.0901 |         -57.2885 |
[32m[20221213 12:36:25 @agent_ppo2.py:179][0m |           0.0535 |          24.9332 |         -49.2258 |
[32m[20221213 12:36:25 @agent_ppo2.py:179][0m |           0.0041 |          23.9376 |         -57.8498 |
[32m[20221213 12:36:25 @agent_ppo2.py:179][0m |          -0.0195 |          23.5195 |         -61.3952 |
[32m[20221213 12:36:25 @agent_ppo2.py:179][0m |          -0.0273 |          23.2866 |         -62.7534 |
[32m[20221213 12:36:26 @agent_ppo2.py:179][0m |          -0.0322 |          23.0442 |         -62.7225 |
[32m[20221213 12:36:26 @agent_ppo2.py:179][0m |          -0.0397 |          22.8972 |         -64.3130 |
[32m[20221213 12:36:26 @agent_ppo2.py:179][0m |          -0.0456 |          22.5417 |         -64.0839 |
[32m[20221213 12:36:26 @agent_ppo2.py:179][0m |          -0.0468 |          22.4594 |         -66.6291 |
[32m[20221213 12:36:26 @agent_ppo2.py:179][0m |          -0.0385 |          22.6226 |         -66.7548 |
[32m[20221213 12:36:26 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:36:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.12
[32m[20221213 12:36:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.10
[32m[20221213 12:36:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.38
[32m[20221213 12:36:26 @agent_ppo2.py:137][0m Total time:      20.49 min
[32m[20221213 12:36:26 @agent_ppo2.py:139][0m 1374208 total steps have happened
[32m[20221213 12:36:26 @agent_ppo2.py:115][0m #------------------------ Iteration 671 --------------------------#
[32m[20221213 12:36:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |           0.0939 |          31.6796 |         -51.8340 |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |           0.0556 |          29.5636 |         -44.8299 |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |           0.0019 |          28.4759 |         -51.8202 |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |          -0.0203 |          27.9031 |         -55.3231 |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |          -0.0270 |          28.5288 |         -58.5428 |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |          -0.0405 |          27.1544 |         -59.2927 |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |          -0.0478 |          26.8508 |         -59.7637 |
[32m[20221213 12:36:27 @agent_ppo2.py:179][0m |          -0.0489 |          26.4863 |         -60.5048 |
[32m[20221213 12:36:28 @agent_ppo2.py:179][0m |          -0.0525 |          27.6611 |         -62.8712 |
[32m[20221213 12:36:28 @agent_ppo2.py:179][0m |          -0.0568 |          26.2058 |         -63.8760 |
[32m[20221213 12:36:28 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 283.39
[32m[20221213 12:36:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.94
[32m[20221213 12:36:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.28
[32m[20221213 12:36:28 @agent_ppo2.py:137][0m Total time:      20.52 min
[32m[20221213 12:36:28 @agent_ppo2.py:139][0m 1376256 total steps have happened
[32m[20221213 12:36:28 @agent_ppo2.py:115][0m #------------------------ Iteration 672 --------------------------#
[32m[20221213 12:36:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:28 @agent_ppo2.py:179][0m |           0.0609 |          26.5081 |         -54.4666 |
[32m[20221213 12:36:28 @agent_ppo2.py:179][0m |           0.0511 |          25.5385 |         -45.9626 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0017 |          25.1211 |         -50.3817 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0237 |          24.7901 |         -54.0631 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0313 |          24.6298 |         -55.3968 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0333 |          25.8977 |         -57.0981 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0403 |          24.1673 |         -57.2613 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0412 |          24.7441 |         -58.0186 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0201 |          24.4258 |         -54.7615 |
[32m[20221213 12:36:29 @agent_ppo2.py:179][0m |          -0.0348 |          23.8017 |         -55.7078 |
[32m[20221213 12:36:29 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:36:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.41
[32m[20221213 12:36:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.44
[32m[20221213 12:36:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 289.35
[32m[20221213 12:36:30 @agent_ppo2.py:137][0m Total time:      20.55 min
[32m[20221213 12:36:30 @agent_ppo2.py:139][0m 1378304 total steps have happened
[32m[20221213 12:36:30 @agent_ppo2.py:115][0m #------------------------ Iteration 673 --------------------------#
[32m[20221213 12:36:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:30 @agent_ppo2.py:179][0m |           0.0775 |          12.0637 |         -16.5584 |
[32m[20221213 12:36:30 @agent_ppo2.py:179][0m |           0.0276 |          11.1433 |         -10.0174 |
[32m[20221213 12:36:30 @agent_ppo2.py:179][0m |           0.0001 |          11.0015 |         -15.1384 |
[32m[20221213 12:36:30 @agent_ppo2.py:179][0m |           0.0136 |          11.5133 |         -12.2355 |
[32m[20221213 12:36:30 @agent_ppo2.py:179][0m |          -0.0185 |          10.8814 |         -13.5562 |
[32m[20221213 12:36:31 @agent_ppo2.py:179][0m |          -0.0189 |          11.0644 |         -16.1286 |
[32m[20221213 12:36:31 @agent_ppo2.py:179][0m |          -0.0241 |          10.6610 |         -15.6128 |
[32m[20221213 12:36:31 @agent_ppo2.py:179][0m |          -0.0385 |          10.5110 |         -16.8278 |
[32m[20221213 12:36:31 @agent_ppo2.py:179][0m |          -0.0371 |          10.7247 |         -25.3549 |
[32m[20221213 12:36:31 @agent_ppo2.py:179][0m |          -0.0419 |          10.4735 |         -20.9656 |
[32m[20221213 12:36:31 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:36:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 71.45
[32m[20221213 12:36:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 268.25
[32m[20221213 12:36:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 302.63
[32m[20221213 12:36:31 @agent_ppo2.py:137][0m Total time:      20.58 min
[32m[20221213 12:36:31 @agent_ppo2.py:139][0m 1380352 total steps have happened
[32m[20221213 12:36:31 @agent_ppo2.py:115][0m #------------------------ Iteration 674 --------------------------#
[32m[20221213 12:36:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:32 @agent_ppo2.py:179][0m |           0.0878 |          26.3451 |         -53.6534 |
[32m[20221213 12:36:32 @agent_ppo2.py:179][0m |           0.0441 |          24.2630 |         -38.7277 |
[32m[20221213 12:36:32 @agent_ppo2.py:179][0m |          -0.0036 |          25.7201 |         -41.5188 |
[32m[20221213 12:36:32 @agent_ppo2.py:179][0m |          -0.0215 |          23.4004 |         -43.3903 |
[32m[20221213 12:36:32 @agent_ppo2.py:179][0m |          -0.0318 |          23.1885 |         -45.0279 |
[32m[20221213 12:36:32 @agent_ppo2.py:179][0m |          -0.0356 |          23.0800 |         -45.6264 |
[32m[20221213 12:36:32 @agent_ppo2.py:179][0m |          -0.0440 |          22.9981 |         -46.8324 |
[32m[20221213 12:36:33 @agent_ppo2.py:179][0m |          -0.0414 |          22.8797 |         -48.3943 |
[32m[20221213 12:36:33 @agent_ppo2.py:179][0m |          -0.0298 |          24.9632 |         -47.8876 |
[32m[20221213 12:36:33 @agent_ppo2.py:179][0m |          -0.0503 |          22.8298 |         -49.9014 |
[32m[20221213 12:36:33 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:36:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.44
[32m[20221213 12:36:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 318.33
[32m[20221213 12:36:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.70
[32m[20221213 12:36:33 @agent_ppo2.py:137][0m Total time:      20.60 min
[32m[20221213 12:36:33 @agent_ppo2.py:139][0m 1382400 total steps have happened
[32m[20221213 12:36:33 @agent_ppo2.py:115][0m #------------------------ Iteration 675 --------------------------#
[32m[20221213 12:36:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:33 @agent_ppo2.py:179][0m |           0.0867 |          25.2050 |         -54.0187 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |           0.1133 |          24.8709 |         -40.2041 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |           0.0340 |          22.9513 |         -46.4513 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |          -0.0095 |          22.3145 |         -51.2858 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |          -0.0236 |          21.9695 |         -52.7515 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |          -0.0204 |          22.2814 |         -55.0573 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |          -0.0397 |          21.2936 |         -58.0265 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |          -0.0444 |          20.8984 |         -58.9140 |
[32m[20221213 12:36:34 @agent_ppo2.py:179][0m |          -0.0477 |          20.6583 |         -60.2215 |
[32m[20221213 12:36:35 @agent_ppo2.py:179][0m |          -0.0516 |          20.4750 |         -61.9307 |
[32m[20221213 12:36:35 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:36:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.60
[32m[20221213 12:36:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.93
[32m[20221213 12:36:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 291.06
[32m[20221213 12:36:35 @agent_ppo2.py:137][0m Total time:      20.63 min
[32m[20221213 12:36:35 @agent_ppo2.py:139][0m 1384448 total steps have happened
[32m[20221213 12:36:35 @agent_ppo2.py:115][0m #------------------------ Iteration 676 --------------------------#
[32m[20221213 12:36:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:35 @agent_ppo2.py:179][0m |           0.0659 |          20.2575 |         -49.3490 |
[32m[20221213 12:36:35 @agent_ppo2.py:179][0m |           0.0408 |          19.1170 |         -47.9993 |
[32m[20221213 12:36:35 @agent_ppo2.py:179][0m |           0.0510 |          18.6729 |         -35.3087 |
[32m[20221213 12:36:36 @agent_ppo2.py:179][0m |          -0.0218 |          18.5977 |         -34.7834 |
[32m[20221213 12:36:36 @agent_ppo2.py:179][0m |          -0.0349 |          18.2609 |         -37.6014 |
[32m[20221213 12:36:36 @agent_ppo2.py:179][0m |          -0.0469 |          18.1113 |         -39.6612 |
[32m[20221213 12:36:36 @agent_ppo2.py:179][0m |          -0.0553 |          17.9371 |         -41.1480 |
[32m[20221213 12:36:36 @agent_ppo2.py:179][0m |          -0.0577 |          17.8615 |         -42.4535 |
[32m[20221213 12:36:36 @agent_ppo2.py:179][0m |          -0.0613 |          17.7201 |         -44.7921 |
[32m[20221213 12:36:36 @agent_ppo2.py:179][0m |          -0.0628 |          17.6880 |         -44.7658 |
[32m[20221213 12:36:36 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:36:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 196.75
[32m[20221213 12:36:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.78
[32m[20221213 12:36:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 310.90
[32m[20221213 12:36:36 @agent_ppo2.py:137][0m Total time:      20.66 min
[32m[20221213 12:36:36 @agent_ppo2.py:139][0m 1386496 total steps have happened
[32m[20221213 12:36:36 @agent_ppo2.py:115][0m #------------------------ Iteration 677 --------------------------#
[32m[20221213 12:36:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:37 @agent_ppo2.py:179][0m |           0.0711 |          31.4680 |         -58.9054 |
[32m[20221213 12:36:37 @agent_ppo2.py:179][0m |           0.0539 |          29.6607 |         -52.6233 |
[32m[20221213 12:36:37 @agent_ppo2.py:179][0m |          -0.0067 |          28.8128 |         -56.9784 |
[32m[20221213 12:36:37 @agent_ppo2.py:179][0m |          -0.0224 |          28.2264 |         -59.2114 |
[32m[20221213 12:36:37 @agent_ppo2.py:179][0m |          -0.0299 |          27.9247 |         -60.4702 |
[32m[20221213 12:36:37 @agent_ppo2.py:179][0m |          -0.0409 |          27.6171 |         -63.3288 |
[32m[20221213 12:36:38 @agent_ppo2.py:179][0m |          -0.0421 |          27.4100 |         -63.7598 |
[32m[20221213 12:36:38 @agent_ppo2.py:179][0m |          -0.0440 |          27.2111 |         -65.1966 |
[32m[20221213 12:36:38 @agent_ppo2.py:179][0m |          -0.0572 |          26.9003 |         -66.3583 |
[32m[20221213 12:36:38 @agent_ppo2.py:179][0m |          -0.0527 |          27.3069 |         -68.5521 |
[32m[20221213 12:36:38 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.63
[32m[20221213 12:36:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 294.50
[32m[20221213 12:36:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.20
[32m[20221213 12:36:38 @agent_ppo2.py:137][0m Total time:      20.69 min
[32m[20221213 12:36:38 @agent_ppo2.py:139][0m 1388544 total steps have happened
[32m[20221213 12:36:38 @agent_ppo2.py:115][0m #------------------------ Iteration 678 --------------------------#
[32m[20221213 12:36:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |           0.0888 |          12.8762 |         -55.8728 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |           0.0420 |          12.1139 |         -14.0402 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |          -0.0061 |          11.8922 |         -14.8292 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |          -0.0188 |          11.6575 |         -17.1417 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |          -0.0299 |          11.5413 |         -17.7876 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |          -0.0350 |          11.3612 |         -19.7319 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |          -0.0419 |          11.3057 |         -19.8895 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |          -0.0408 |          11.1780 |         -20.8603 |
[32m[20221213 12:36:39 @agent_ppo2.py:179][0m |          -0.0397 |          11.1922 |         -21.7073 |
[32m[20221213 12:36:40 @agent_ppo2.py:179][0m |          -0.0425 |          11.0084 |         -21.6851 |
[32m[20221213 12:36:40 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:36:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 91.40
[32m[20221213 12:36:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 280.29
[32m[20221213 12:36:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 325.66
[32m[20221213 12:36:40 @agent_ppo2.py:137][0m Total time:      20.72 min
[32m[20221213 12:36:40 @agent_ppo2.py:139][0m 1390592 total steps have happened
[32m[20221213 12:36:40 @agent_ppo2.py:115][0m #------------------------ Iteration 679 --------------------------#
[32m[20221213 12:36:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:40 @agent_ppo2.py:179][0m |           0.1039 |          25.5195 |         -71.5831 |
[32m[20221213 12:36:40 @agent_ppo2.py:179][0m |           0.0950 |          23.4024 |         -48.2183 |
[32m[20221213 12:36:40 @agent_ppo2.py:179][0m |           0.0351 |          23.7612 |         -64.4301 |
[32m[20221213 12:36:41 @agent_ppo2.py:179][0m |           0.0025 |          22.2474 |         -72.8625 |
[32m[20221213 12:36:41 @agent_ppo2.py:179][0m |          -0.0072 |          21.8920 |         -75.6836 |
[32m[20221213 12:36:41 @agent_ppo2.py:179][0m |          -0.0204 |          21.5927 |         -80.2864 |
[32m[20221213 12:36:41 @agent_ppo2.py:179][0m |          -0.0318 |          21.3802 |         -82.2668 |
[32m[20221213 12:36:41 @agent_ppo2.py:179][0m |          -0.0185 |          24.6548 |         -83.8214 |
[32m[20221213 12:36:41 @agent_ppo2.py:179][0m |          -0.0088 |          21.0658 |         -74.8307 |
[32m[20221213 12:36:41 @agent_ppo2.py:179][0m |          -0.0229 |          20.9556 |         -81.8832 |
[32m[20221213 12:36:41 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 240.81
[32m[20221213 12:36:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 319.74
[32m[20221213 12:36:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 263.11
[32m[20221213 12:36:42 @agent_ppo2.py:137][0m Total time:      20.75 min
[32m[20221213 12:36:42 @agent_ppo2.py:139][0m 1392640 total steps have happened
[32m[20221213 12:36:42 @agent_ppo2.py:115][0m #------------------------ Iteration 680 --------------------------#
[32m[20221213 12:36:42 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:36:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:42 @agent_ppo2.py:179][0m |           0.0570 |          20.3927 |         -77.8488 |
[32m[20221213 12:36:42 @agent_ppo2.py:179][0m |           0.0234 |          19.1368 |         -75.7135 |
[32m[20221213 12:36:42 @agent_ppo2.py:179][0m |          -0.0042 |          18.4643 |         -79.3386 |
[32m[20221213 12:36:42 @agent_ppo2.py:179][0m |          -0.0210 |          17.9120 |         -82.3349 |
[32m[20221213 12:36:42 @agent_ppo2.py:179][0m |          -0.0175 |          17.4865 |         -82.1548 |
[32m[20221213 12:36:43 @agent_ppo2.py:179][0m |          -0.0211 |          17.1847 |         -84.0242 |
[32m[20221213 12:36:43 @agent_ppo2.py:179][0m |          -0.0317 |          16.9544 |         -85.6968 |
[32m[20221213 12:36:43 @agent_ppo2.py:179][0m |          -0.0350 |          16.6824 |         -87.3639 |
[32m[20221213 12:36:43 @agent_ppo2.py:179][0m |          -0.0370 |          16.4874 |         -87.8583 |
[32m[20221213 12:36:43 @agent_ppo2.py:179][0m |          -0.0427 |          16.3029 |         -90.7212 |
[32m[20221213 12:36:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:36:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.34
[32m[20221213 12:36:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.22
[32m[20221213 12:36:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.88
[32m[20221213 12:36:43 @agent_ppo2.py:137][0m Total time:      20.77 min
[32m[20221213 12:36:43 @agent_ppo2.py:139][0m 1394688 total steps have happened
[32m[20221213 12:36:43 @agent_ppo2.py:115][0m #------------------------ Iteration 681 --------------------------#
[32m[20221213 12:36:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0708 |           5.2493 |         -86.1932 |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0564 |           4.8358 |         -68.3963 |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0181 |           4.8123 |         -76.5823 |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0238 |           4.7826 |         -73.4993 |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0071 |           4.7620 |         -76.8603 |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0318 |           4.7688 |         -71.5122 |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0199 |           5.3088 |         -74.6136 |
[32m[20221213 12:36:44 @agent_ppo2.py:179][0m |           0.0102 |           4.7201 |         -75.9062 |
[32m[20221213 12:36:45 @agent_ppo2.py:179][0m |           0.0029 |           4.6987 |         -75.2129 |
[32m[20221213 12:36:45 @agent_ppo2.py:179][0m |           0.0245 |           4.7271 |         -72.9802 |
[32m[20221213 12:36:45 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:36:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 45.66
[32m[20221213 12:36:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 50.83
[32m[20221213 12:36:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 78.93
[32m[20221213 12:36:45 @agent_ppo2.py:137][0m Total time:      20.80 min
[32m[20221213 12:36:45 @agent_ppo2.py:139][0m 1396736 total steps have happened
[32m[20221213 12:36:45 @agent_ppo2.py:115][0m #------------------------ Iteration 682 --------------------------#
[32m[20221213 12:36:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:45 @agent_ppo2.py:179][0m |           0.0553 |          24.6466 |         -71.8673 |
[32m[20221213 12:36:45 @agent_ppo2.py:179][0m |           0.0251 |          22.8322 |         -69.4176 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |           0.0065 |          21.8554 |         -71.9894 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |          -0.0172 |          21.4667 |         -75.5770 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |          -0.0279 |          20.9634 |         -77.6827 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |          -0.0226 |          23.4934 |         -77.8049 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |          -0.0360 |          22.1448 |         -79.3043 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |          -0.0476 |          20.1267 |         -81.1782 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |          -0.0527 |          19.7632 |         -82.5939 |
[32m[20221213 12:36:46 @agent_ppo2.py:179][0m |          -0.0475 |          19.6625 |         -82.2141 |
[32m[20221213 12:36:46 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.65
[32m[20221213 12:36:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.48
[32m[20221213 12:36:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.18
[32m[20221213 12:36:47 @agent_ppo2.py:137][0m Total time:      20.83 min
[32m[20221213 12:36:47 @agent_ppo2.py:139][0m 1398784 total steps have happened
[32m[20221213 12:36:47 @agent_ppo2.py:115][0m #------------------------ Iteration 683 --------------------------#
[32m[20221213 12:36:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:47 @agent_ppo2.py:179][0m |           0.0729 |          30.8185 |         -69.9574 |
[32m[20221213 12:36:47 @agent_ppo2.py:179][0m |           0.1281 |          29.6628 |         -59.4714 |
[32m[20221213 12:36:47 @agent_ppo2.py:179][0m |          -0.0035 |          28.2017 |         -68.2369 |
[32m[20221213 12:36:47 @agent_ppo2.py:179][0m |          -0.0161 |          27.4953 |         -70.8946 |
[32m[20221213 12:36:47 @agent_ppo2.py:179][0m |          -0.0228 |          27.0924 |         -72.3927 |
[32m[20221213 12:36:48 @agent_ppo2.py:179][0m |          -0.0271 |          27.8199 |         -73.4150 |
[32m[20221213 12:36:48 @agent_ppo2.py:179][0m |          -0.0410 |          26.3706 |         -75.2035 |
[32m[20221213 12:36:48 @agent_ppo2.py:179][0m |          -0.0420 |          28.3669 |         -76.2124 |
[32m[20221213 12:36:48 @agent_ppo2.py:179][0m |          -0.0471 |          25.8696 |         -78.7994 |
[32m[20221213 12:36:48 @agent_ppo2.py:179][0m |          -0.0502 |          25.5379 |         -79.7275 |
[32m[20221213 12:36:48 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:36:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.18
[32m[20221213 12:36:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.92
[32m[20221213 12:36:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 50.05
[32m[20221213 12:36:48 @agent_ppo2.py:137][0m Total time:      20.86 min
[32m[20221213 12:36:48 @agent_ppo2.py:139][0m 1400832 total steps have happened
[32m[20221213 12:36:48 @agent_ppo2.py:115][0m #------------------------ Iteration 684 --------------------------#
[32m[20221213 12:36:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:49 @agent_ppo2.py:179][0m |           0.0788 |          32.1221 |         -64.6063 |
[32m[20221213 12:36:49 @agent_ppo2.py:179][0m |           0.0718 |          30.1827 |         -53.7085 |
[32m[20221213 12:36:49 @agent_ppo2.py:179][0m |           0.0207 |          29.4400 |         -59.0799 |
[32m[20221213 12:36:49 @agent_ppo2.py:179][0m |           0.0056 |          30.1177 |         -61.8586 |
[32m[20221213 12:36:49 @agent_ppo2.py:179][0m |          -0.0023 |          28.6275 |         -64.0929 |
[32m[20221213 12:36:49 @agent_ppo2.py:179][0m |          -0.0247 |          28.2609 |         -69.6985 |
[32m[20221213 12:36:49 @agent_ppo2.py:179][0m |          -0.0347 |          28.1363 |         -72.9947 |
[32m[20221213 12:36:50 @agent_ppo2.py:179][0m |          -0.0310 |          29.2437 |         -74.8798 |
[32m[20221213 12:36:50 @agent_ppo2.py:179][0m |          -0.0380 |          27.7330 |         -75.6669 |
[32m[20221213 12:36:50 @agent_ppo2.py:179][0m |          -0.0407 |          27.5052 |         -75.6958 |
[32m[20221213 12:36:50 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:36:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 306.72
[32m[20221213 12:36:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.74
[32m[20221213 12:36:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.37
[32m[20221213 12:36:50 @agent_ppo2.py:137][0m Total time:      20.89 min
[32m[20221213 12:36:50 @agent_ppo2.py:139][0m 1402880 total steps have happened
[32m[20221213 12:36:50 @agent_ppo2.py:115][0m #------------------------ Iteration 685 --------------------------#
[32m[20221213 12:36:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:50 @agent_ppo2.py:179][0m |           0.0614 |          21.3765 |         -70.2822 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |           0.0639 |          20.0430 |         -61.5420 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |           0.0260 |          19.5668 |         -60.6061 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |          -0.0174 |          19.2099 |         -73.0974 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |          -0.0242 |          18.9629 |         -74.7380 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |          -0.0283 |          18.8663 |         -75.7248 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |          -0.0312 |          18.9174 |         -78.4929 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |          -0.0349 |          18.6309 |         -78.6819 |
[32m[20221213 12:36:51 @agent_ppo2.py:179][0m |          -0.0428 |          18.4531 |         -79.7051 |
[32m[20221213 12:36:52 @agent_ppo2.py:179][0m |          -0.0401 |          18.3811 |         -80.4639 |
[32m[20221213 12:36:52 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:36:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 196.32
[32m[20221213 12:36:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 305.27
[32m[20221213 12:36:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.56
[32m[20221213 12:36:52 @agent_ppo2.py:137][0m Total time:      20.92 min
[32m[20221213 12:36:52 @agent_ppo2.py:139][0m 1404928 total steps have happened
[32m[20221213 12:36:52 @agent_ppo2.py:115][0m #------------------------ Iteration 686 --------------------------#
[32m[20221213 12:36:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:52 @agent_ppo2.py:179][0m |           0.0689 |          27.6238 |         -69.9085 |
[32m[20221213 12:36:52 @agent_ppo2.py:179][0m |           0.0373 |          25.2824 |         -65.6531 |
[32m[20221213 12:36:52 @agent_ppo2.py:179][0m |          -0.0038 |          25.1488 |         -68.6278 |
[32m[20221213 12:36:53 @agent_ppo2.py:179][0m |          -0.0222 |          24.2452 |         -71.4012 |
[32m[20221213 12:36:53 @agent_ppo2.py:179][0m |          -0.0204 |          25.9052 |         -73.1775 |
[32m[20221213 12:36:53 @agent_ppo2.py:179][0m |          -0.0433 |          23.6813 |         -73.2571 |
[32m[20221213 12:36:53 @agent_ppo2.py:179][0m |          -0.0439 |          23.9141 |         -75.4625 |
[32m[20221213 12:36:53 @agent_ppo2.py:179][0m |          -0.0496 |          23.2408 |         -76.8204 |
[32m[20221213 12:36:53 @agent_ppo2.py:179][0m |          -0.0485 |          22.9698 |         -77.7084 |
[32m[20221213 12:36:53 @agent_ppo2.py:179][0m |          -0.0517 |          23.2210 |         -77.0744 |
[32m[20221213 12:36:53 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:36:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 220.00
[32m[20221213 12:36:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.67
[32m[20221213 12:36:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.63
[32m[20221213 12:36:54 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 351.63
[32m[20221213 12:36:54 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 351.63
[32m[20221213 12:36:54 @agent_ppo2.py:137][0m Total time:      20.95 min
[32m[20221213 12:36:54 @agent_ppo2.py:139][0m 1406976 total steps have happened
[32m[20221213 12:36:54 @agent_ppo2.py:115][0m #------------------------ Iteration 687 --------------------------#
[32m[20221213 12:36:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:54 @agent_ppo2.py:179][0m |           0.0463 |          25.0841 |         -69.3883 |
[32m[20221213 12:36:54 @agent_ppo2.py:179][0m |           0.0774 |          23.8392 |         -63.4482 |
[32m[20221213 12:36:54 @agent_ppo2.py:179][0m |           0.0042 |          23.4253 |         -67.7930 |
[32m[20221213 12:36:54 @agent_ppo2.py:179][0m |          -0.0244 |          22.8846 |         -70.8665 |
[32m[20221213 12:36:54 @agent_ppo2.py:179][0m |          -0.0304 |          22.5720 |         -72.2512 |
[32m[20221213 12:36:55 @agent_ppo2.py:179][0m |          -0.0333 |          22.5837 |         -73.9196 |
[32m[20221213 12:36:55 @agent_ppo2.py:179][0m |          -0.0437 |          22.0996 |         -74.8687 |
[32m[20221213 12:36:55 @agent_ppo2.py:179][0m |          -0.0426 |          22.0319 |         -76.3725 |
[32m[20221213 12:36:55 @agent_ppo2.py:179][0m |          -0.0404 |          21.8136 |         -75.6806 |
[32m[20221213 12:36:55 @agent_ppo2.py:179][0m |          -0.0449 |          21.5999 |         -77.1760 |
[32m[20221213 12:36:55 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:36:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 181.44
[32m[20221213 12:36:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.39
[32m[20221213 12:36:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.68
[32m[20221213 12:36:55 @agent_ppo2.py:137][0m Total time:      20.98 min
[32m[20221213 12:36:55 @agent_ppo2.py:139][0m 1409024 total steps have happened
[32m[20221213 12:36:55 @agent_ppo2.py:115][0m #------------------------ Iteration 688 --------------------------#
[32m[20221213 12:36:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:56 @agent_ppo2.py:179][0m |           0.0648 |          28.3872 |         -68.0261 |
[32m[20221213 12:36:56 @agent_ppo2.py:179][0m |           0.0482 |          27.4493 |         -65.2895 |
[32m[20221213 12:36:56 @agent_ppo2.py:179][0m |           0.0049 |          26.8450 |         -66.2434 |
[32m[20221213 12:36:56 @agent_ppo2.py:179][0m |          -0.0129 |          26.6940 |         -67.7074 |
[32m[20221213 12:36:56 @agent_ppo2.py:179][0m |          -0.0295 |          26.3478 |         -69.9706 |
[32m[20221213 12:36:56 @agent_ppo2.py:179][0m |          -0.0381 |          26.4265 |         -73.0292 |
[32m[20221213 12:36:56 @agent_ppo2.py:179][0m |          -0.0306 |          26.0540 |         -71.5031 |
[32m[20221213 12:36:57 @agent_ppo2.py:179][0m |          -0.0311 |          25.8855 |         -73.4362 |
[32m[20221213 12:36:57 @agent_ppo2.py:179][0m |          -0.0413 |          25.7766 |         -73.8910 |
[32m[20221213 12:36:57 @agent_ppo2.py:179][0m |          -0.0493 |          25.6588 |         -75.8393 |
[32m[20221213 12:36:57 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:36:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 274.86
[32m[20221213 12:36:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.32
[32m[20221213 12:36:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.41
[32m[20221213 12:36:57 @agent_ppo2.py:137][0m Total time:      21.00 min
[32m[20221213 12:36:57 @agent_ppo2.py:139][0m 1411072 total steps have happened
[32m[20221213 12:36:57 @agent_ppo2.py:115][0m #------------------------ Iteration 689 --------------------------#
[32m[20221213 12:36:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:36:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:57 @agent_ppo2.py:179][0m |           0.0756 |          28.3239 |         -66.1848 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |           0.0716 |          27.0065 |         -52.7826 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |           0.0255 |          26.2520 |         -58.7078 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |          -0.0072 |          25.7571 |         -62.8971 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |          -0.0262 |          25.4722 |         -66.6739 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |          -0.0363 |          25.2719 |         -69.2748 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |          -0.0391 |          25.0797 |         -70.2285 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |          -0.0437 |          24.9148 |         -72.2667 |
[32m[20221213 12:36:58 @agent_ppo2.py:179][0m |          -0.0469 |          24.8549 |         -72.9398 |
[32m[20221213 12:36:59 @agent_ppo2.py:179][0m |          -0.0465 |          24.6548 |         -73.9404 |
[32m[20221213 12:36:59 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:36:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 261.91
[32m[20221213 12:36:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 284.39
[32m[20221213 12:36:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.33
[32m[20221213 12:36:59 @agent_ppo2.py:137][0m Total time:      21.03 min
[32m[20221213 12:36:59 @agent_ppo2.py:139][0m 1413120 total steps have happened
[32m[20221213 12:36:59 @agent_ppo2.py:115][0m #------------------------ Iteration 690 --------------------------#
[32m[20221213 12:36:59 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:36:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:36:59 @agent_ppo2.py:179][0m |           0.0672 |          21.6206 |         -65.2472 |
[32m[20221213 12:36:59 @agent_ppo2.py:179][0m |           0.0384 |          20.6417 |         -60.7352 |
[32m[20221213 12:36:59 @agent_ppo2.py:179][0m |          -0.0005 |          20.1791 |         -67.5435 |
[32m[20221213 12:37:00 @agent_ppo2.py:179][0m |          -0.0043 |          20.0483 |         -68.2063 |
[32m[20221213 12:37:00 @agent_ppo2.py:179][0m |          -0.0174 |          19.5141 |         -67.3026 |
[32m[20221213 12:37:00 @agent_ppo2.py:179][0m |          -0.0291 |          19.2362 |         -71.2819 |
[32m[20221213 12:37:00 @agent_ppo2.py:179][0m |          -0.0352 |          19.1072 |         -72.9841 |
[32m[20221213 12:37:00 @agent_ppo2.py:179][0m |          -0.0250 |          20.8380 |         -73.8946 |
[32m[20221213 12:37:00 @agent_ppo2.py:179][0m |          -0.0383 |          19.0775 |         -74.9090 |
[32m[20221213 12:37:00 @agent_ppo2.py:179][0m |          -0.0364 |          19.1677 |         -76.6880 |
[32m[20221213 12:37:00 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:37:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.35
[32m[20221213 12:37:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 311.40
[32m[20221213 12:37:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.46
[32m[20221213 12:37:00 @agent_ppo2.py:137][0m Total time:      21.06 min
[32m[20221213 12:37:00 @agent_ppo2.py:139][0m 1415168 total steps have happened
[32m[20221213 12:37:00 @agent_ppo2.py:115][0m #------------------------ Iteration 691 --------------------------#
[32m[20221213 12:37:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:01 @agent_ppo2.py:179][0m |           0.0721 |           3.5318 |         -65.8548 |
[32m[20221213 12:37:01 @agent_ppo2.py:179][0m |           0.0520 |           3.0374 |         -55.2483 |
[32m[20221213 12:37:01 @agent_ppo2.py:179][0m |           0.0276 |           2.9756 |         -58.1146 |
[32m[20221213 12:37:01 @agent_ppo2.py:179][0m |           0.0172 |           2.8885 |         -61.4205 |
[32m[20221213 12:37:01 @agent_ppo2.py:179][0m |           0.0064 |           2.8739 |         -67.0644 |
[32m[20221213 12:37:01 @agent_ppo2.py:179][0m |           0.0149 |           2.8188 |         -65.6419 |
[32m[20221213 12:37:02 @agent_ppo2.py:179][0m |           0.0016 |           2.7806 |         -67.7414 |
[32m[20221213 12:37:02 @agent_ppo2.py:179][0m |           0.0196 |           2.7754 |         -66.0292 |
[32m[20221213 12:37:02 @agent_ppo2.py:179][0m |           0.0040 |           2.7297 |         -65.1445 |
[32m[20221213 12:37:02 @agent_ppo2.py:179][0m |           0.0079 |           2.7223 |         -68.6557 |
[32m[20221213 12:37:02 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:37:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 27.09
[32m[20221213 12:37:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 46.18
[32m[20221213 12:37:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 310.02
[32m[20221213 12:37:02 @agent_ppo2.py:137][0m Total time:      21.09 min
[32m[20221213 12:37:02 @agent_ppo2.py:139][0m 1417216 total steps have happened
[32m[20221213 12:37:02 @agent_ppo2.py:115][0m #------------------------ Iteration 692 --------------------------#
[32m[20221213 12:37:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |           0.0688 |          26.6643 |         -60.2439 |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |           0.0355 |          24.7886 |         -54.9532 |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |           0.0066 |          24.3298 |         -60.3078 |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |          -0.0224 |          23.2446 |         -64.4438 |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |          -0.0325 |          22.8515 |         -66.8155 |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |          -0.0353 |          22.3861 |         -68.5990 |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |          -0.0364 |          22.2413 |         -69.9311 |
[32m[20221213 12:37:03 @agent_ppo2.py:179][0m |          -0.0428 |          21.9239 |         -70.9166 |
[32m[20221213 12:37:04 @agent_ppo2.py:179][0m |          -0.0341 |          22.0853 |         -71.4956 |
[32m[20221213 12:37:04 @agent_ppo2.py:179][0m |          -0.0470 |          21.6815 |         -73.2157 |
[32m[20221213 12:37:04 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:37:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 298.58
[32m[20221213 12:37:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.20
[32m[20221213 12:37:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 325.12
[32m[20221213 12:37:04 @agent_ppo2.py:137][0m Total time:      21.12 min
[32m[20221213 12:37:04 @agent_ppo2.py:139][0m 1419264 total steps have happened
[32m[20221213 12:37:04 @agent_ppo2.py:115][0m #------------------------ Iteration 693 --------------------------#
[32m[20221213 12:37:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:04 @agent_ppo2.py:179][0m |           0.0835 |          24.4565 |         -58.9316 |
[32m[20221213 12:37:04 @agent_ppo2.py:179][0m |           0.0583 |          19.8211 |         -53.0814 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |           0.0016 |          19.2472 |         -59.2569 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |          -0.0053 |          19.2819 |         -61.0583 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |          -0.0356 |          18.1938 |         -62.6666 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |          -0.0384 |          17.9198 |         -63.1334 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |          -0.0473 |          17.7410 |         -64.9393 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |          -0.0458 |          17.5721 |         -66.1447 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |          -0.0510 |          17.3928 |         -67.7298 |
[32m[20221213 12:37:05 @agent_ppo2.py:179][0m |          -0.0538 |          17.2540 |         -67.3244 |
[32m[20221213 12:37:05 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:37:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 221.25
[32m[20221213 12:37:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 293.86
[32m[20221213 12:37:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 51.68
[32m[20221213 12:37:06 @agent_ppo2.py:137][0m Total time:      21.15 min
[32m[20221213 12:37:06 @agent_ppo2.py:139][0m 1421312 total steps have happened
[32m[20221213 12:37:06 @agent_ppo2.py:115][0m #------------------------ Iteration 694 --------------------------#
[32m[20221213 12:37:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:06 @agent_ppo2.py:179][0m |           0.0492 |          23.8989 |         -62.2373 |
[32m[20221213 12:37:06 @agent_ppo2.py:179][0m |           0.0452 |          23.1809 |         -57.1610 |
[32m[20221213 12:37:06 @agent_ppo2.py:179][0m |           0.0116 |          22.8685 |         -61.3741 |
[32m[20221213 12:37:06 @agent_ppo2.py:179][0m |          -0.0047 |          22.6466 |         -63.6324 |
[32m[20221213 12:37:06 @agent_ppo2.py:179][0m |          -0.0186 |          22.4679 |         -66.1622 |
[32m[20221213 12:37:07 @agent_ppo2.py:179][0m |          -0.0319 |          22.3703 |         -68.9784 |
[32m[20221213 12:37:07 @agent_ppo2.py:179][0m |          -0.0302 |          22.2900 |         -69.3261 |
[32m[20221213 12:37:07 @agent_ppo2.py:179][0m |          -0.0339 |          22.2763 |         -69.7680 |
[32m[20221213 12:37:07 @agent_ppo2.py:179][0m |          -0.0356 |          22.1560 |         -71.3823 |
[32m[20221213 12:37:07 @agent_ppo2.py:179][0m |          -0.0403 |          22.0240 |         -73.2639 |
[32m[20221213 12:37:07 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:37:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.29
[32m[20221213 12:37:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 323.58
[32m[20221213 12:37:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 318.68
[32m[20221213 12:37:07 @agent_ppo2.py:137][0m Total time:      21.18 min
[32m[20221213 12:37:07 @agent_ppo2.py:139][0m 1423360 total steps have happened
[32m[20221213 12:37:07 @agent_ppo2.py:115][0m #------------------------ Iteration 695 --------------------------#
[32m[20221213 12:37:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:08 @agent_ppo2.py:179][0m |           0.0461 |          20.4738 |         -65.5079 |
[32m[20221213 12:37:08 @agent_ppo2.py:179][0m |           0.0257 |          18.5088 |         -60.2543 |
[32m[20221213 12:37:08 @agent_ppo2.py:179][0m |          -0.0072 |          17.8851 |         -64.0121 |
[32m[20221213 12:37:08 @agent_ppo2.py:179][0m |          -0.0166 |          17.5976 |         -68.1292 |
[32m[20221213 12:37:08 @agent_ppo2.py:179][0m |          -0.0202 |          17.6639 |         -69.3721 |
[32m[20221213 12:37:08 @agent_ppo2.py:179][0m |          -0.0325 |          17.1870 |         -70.2901 |
[32m[20221213 12:37:08 @agent_ppo2.py:179][0m |          -0.0372 |          17.0904 |         -71.9926 |
[32m[20221213 12:37:09 @agent_ppo2.py:179][0m |          -0.0399 |          16.9288 |         -72.5302 |
[32m[20221213 12:37:09 @agent_ppo2.py:179][0m |          -0.0404 |          16.8627 |         -74.3042 |
[32m[20221213 12:37:09 @agent_ppo2.py:179][0m |          -0.0426 |          16.8071 |         -74.4962 |
[32m[20221213 12:37:09 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:37:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.43
[32m[20221213 12:37:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.49
[32m[20221213 12:37:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.17
[32m[20221213 12:37:09 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 353.17
[32m[20221213 12:37:09 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 353.17
[32m[20221213 12:37:09 @agent_ppo2.py:137][0m Total time:      21.20 min
[32m[20221213 12:37:09 @agent_ppo2.py:139][0m 1425408 total steps have happened
[32m[20221213 12:37:09 @agent_ppo2.py:115][0m #------------------------ Iteration 696 --------------------------#
[32m[20221213 12:37:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:09 @agent_ppo2.py:179][0m |           0.0626 |          26.7548 |         -64.3391 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |           0.0524 |          23.7894 |         -54.9802 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |           0.0019 |          22.6869 |         -64.1777 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |          -0.0188 |          22.1055 |         -69.1160 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |          -0.0294 |          21.6152 |         -70.4966 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |          -0.0457 |          20.9977 |         -71.9334 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |          -0.0368 |          20.8558 |         -71.8241 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |          -0.0464 |          20.4294 |         -73.0044 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |          -0.0544 |          19.9978 |         -74.8422 |
[32m[20221213 12:37:10 @agent_ppo2.py:179][0m |          -0.0532 |          19.7623 |         -76.6797 |
[32m[20221213 12:37:10 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:37:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.18
[32m[20221213 12:37:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 323.46
[32m[20221213 12:37:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.12
[32m[20221213 12:37:11 @agent_ppo2.py:137][0m Total time:      21.23 min
[32m[20221213 12:37:11 @agent_ppo2.py:139][0m 1427456 total steps have happened
[32m[20221213 12:37:11 @agent_ppo2.py:115][0m #------------------------ Iteration 697 --------------------------#
[32m[20221213 12:37:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:11 @agent_ppo2.py:179][0m |           0.0496 |          24.5056 |         -68.5639 |
[32m[20221213 12:37:11 @agent_ppo2.py:179][0m |           0.0269 |          23.7559 |         -65.5937 |
[32m[20221213 12:37:11 @agent_ppo2.py:179][0m |           0.0058 |          22.4442 |         -65.5000 |
[32m[20221213 12:37:11 @agent_ppo2.py:179][0m |          -0.0227 |          22.0921 |         -70.7725 |
[32m[20221213 12:37:12 @agent_ppo2.py:179][0m |          -0.0335 |          21.9653 |         -71.7177 |
[32m[20221213 12:37:12 @agent_ppo2.py:179][0m |          -0.0324 |          21.7495 |         -73.6234 |
[32m[20221213 12:37:12 @agent_ppo2.py:179][0m |          -0.0385 |          21.5851 |         -75.6277 |
[32m[20221213 12:37:12 @agent_ppo2.py:179][0m |          -0.0468 |          21.4941 |         -77.3101 |
[32m[20221213 12:37:12 @agent_ppo2.py:179][0m |          -0.0383 |          21.4496 |         -76.7390 |
[32m[20221213 12:37:12 @agent_ppo2.py:179][0m |          -0.0445 |          21.2839 |         -77.5744 |
[32m[20221213 12:37:12 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:37:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.84
[32m[20221213 12:37:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 323.87
[32m[20221213 12:37:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.68
[32m[20221213 12:37:12 @agent_ppo2.py:137][0m Total time:      21.26 min
[32m[20221213 12:37:12 @agent_ppo2.py:139][0m 1429504 total steps have happened
[32m[20221213 12:37:12 @agent_ppo2.py:115][0m #------------------------ Iteration 698 --------------------------#
[32m[20221213 12:37:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:13 @agent_ppo2.py:179][0m |           0.0840 |          29.0221 |         -61.9578 |
[32m[20221213 12:37:13 @agent_ppo2.py:179][0m |           0.0865 |          26.9755 |         -42.3318 |
[32m[20221213 12:37:13 @agent_ppo2.py:179][0m |           0.0356 |          26.3661 |         -49.4558 |
[32m[20221213 12:37:13 @agent_ppo2.py:179][0m |           0.0142 |          26.9451 |         -59.4483 |
[32m[20221213 12:37:13 @agent_ppo2.py:179][0m |          -0.0117 |          25.2630 |         -62.1350 |
[32m[20221213 12:37:14 @agent_ppo2.py:179][0m |          -0.0233 |          24.8850 |         -66.0113 |
[32m[20221213 12:37:14 @agent_ppo2.py:179][0m |          -0.0353 |          24.5966 |         -68.5776 |
[32m[20221213 12:37:14 @agent_ppo2.py:179][0m |          -0.0282 |          24.2911 |         -70.3880 |
[32m[20221213 12:37:14 @agent_ppo2.py:179][0m |          -0.0371 |          24.0885 |         -71.5662 |
[32m[20221213 12:37:14 @agent_ppo2.py:179][0m |          -0.0376 |          24.1019 |         -73.9262 |
[32m[20221213 12:37:14 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:37:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 293.80
[32m[20221213 12:37:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.21
[32m[20221213 12:37:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.95
[32m[20221213 12:37:14 @agent_ppo2.py:137][0m Total time:      21.29 min
[32m[20221213 12:37:14 @agent_ppo2.py:139][0m 1431552 total steps have happened
[32m[20221213 12:37:14 @agent_ppo2.py:115][0m #------------------------ Iteration 699 --------------------------#
[32m[20221213 12:37:14 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:37:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:15 @agent_ppo2.py:179][0m |           0.0707 |           8.1317 |         -64.4209 |
[32m[20221213 12:37:15 @agent_ppo2.py:179][0m |           0.0425 |           6.2846 |         -60.9171 |
[32m[20221213 12:37:15 @agent_ppo2.py:179][0m |           0.0041 |           5.7586 |         -63.5229 |
[32m[20221213 12:37:15 @agent_ppo2.py:179][0m |          -0.0066 |           5.3132 |         -63.9733 |
[32m[20221213 12:37:15 @agent_ppo2.py:179][0m |          -0.0139 |           5.0542 |         -65.9781 |
[32m[20221213 12:37:15 @agent_ppo2.py:179][0m |          -0.0247 |           4.8822 |         -68.0943 |
[32m[20221213 12:37:15 @agent_ppo2.py:179][0m |          -0.0327 |           4.7301 |         -68.0942 |
[32m[20221213 12:37:16 @agent_ppo2.py:179][0m |          -0.0315 |           4.5924 |         -68.9394 |
[32m[20221213 12:37:16 @agent_ppo2.py:179][0m |          -0.0231 |           4.4683 |         -65.5492 |
[32m[20221213 12:37:16 @agent_ppo2.py:179][0m |          -0.0311 |           4.3791 |         -70.4638 |
[32m[20221213 12:37:16 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:37:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.95
[32m[20221213 12:37:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 271.39
[32m[20221213 12:37:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.73
[32m[20221213 12:37:16 @agent_ppo2.py:137][0m Total time:      21.32 min
[32m[20221213 12:37:16 @agent_ppo2.py:139][0m 1433600 total steps have happened
[32m[20221213 12:37:16 @agent_ppo2.py:115][0m #------------------------ Iteration 700 --------------------------#
[32m[20221213 12:37:16 @agent_ppo2.py:121][0m Sampling time: 0.25 s by 5 slaves
[32m[20221213 12:37:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |           0.0836 |          25.2150 |         -61.2912 |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |           0.0295 |          22.6202 |         -64.1178 |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |           0.0051 |          21.6973 |         -64.2786 |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |          -0.0168 |          21.2341 |         -66.3290 |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |          -0.0344 |          20.8008 |         -68.9075 |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |          -0.0284 |          23.1802 |         -69.9477 |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |          -0.0445 |          20.3462 |         -71.9855 |
[32m[20221213 12:37:17 @agent_ppo2.py:179][0m |          -0.0455 |          20.0730 |         -73.4067 |
[32m[20221213 12:37:18 @agent_ppo2.py:179][0m |          -0.0439 |          19.9308 |         -73.5157 |
[32m[20221213 12:37:18 @agent_ppo2.py:179][0m |          -0.0475 |          19.7654 |         -73.9272 |
[32m[20221213 12:37:18 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:37:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 185.20
[32m[20221213 12:37:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.98
[32m[20221213 12:37:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 284.83
[32m[20221213 12:37:18 @agent_ppo2.py:137][0m Total time:      21.35 min
[32m[20221213 12:37:18 @agent_ppo2.py:139][0m 1435648 total steps have happened
[32m[20221213 12:37:18 @agent_ppo2.py:115][0m #------------------------ Iteration 701 --------------------------#
[32m[20221213 12:37:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:18 @agent_ppo2.py:179][0m |           0.0857 |          22.6330 |         -55.9097 |
[32m[20221213 12:37:18 @agent_ppo2.py:179][0m |           0.0605 |          20.6312 |         -45.9938 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |           0.0240 |          19.9448 |         -47.2728 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |          -0.0098 |          19.6567 |         -53.2239 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |          -0.0250 |          19.1110 |         -56.9384 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |          -0.0323 |          18.8679 |         -58.7390 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |          -0.0395 |          18.6974 |         -59.6274 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |          -0.0460 |          18.4941 |         -60.9249 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |          -0.0471 |          18.3336 |         -63.2513 |
[32m[20221213 12:37:19 @agent_ppo2.py:179][0m |          -0.0543 |          18.1383 |         -65.0018 |
[32m[20221213 12:37:19 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:37:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 197.20
[32m[20221213 12:37:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 300.74
[32m[20221213 12:37:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.80
[32m[20221213 12:37:20 @agent_ppo2.py:137][0m Total time:      21.38 min
[32m[20221213 12:37:20 @agent_ppo2.py:139][0m 1437696 total steps have happened
[32m[20221213 12:37:20 @agent_ppo2.py:115][0m #------------------------ Iteration 702 --------------------------#
[32m[20221213 12:37:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:20 @agent_ppo2.py:179][0m |           0.0735 |          25.0255 |         -63.9245 |
[32m[20221213 12:37:20 @agent_ppo2.py:179][0m |           0.0402 |          21.7246 |         -56.6576 |
[32m[20221213 12:37:20 @agent_ppo2.py:179][0m |          -0.0074 |          20.5941 |         -65.4053 |
[32m[20221213 12:37:20 @agent_ppo2.py:179][0m |          -0.0259 |          19.8425 |         -67.0293 |
[32m[20221213 12:37:21 @agent_ppo2.py:179][0m |          -0.0261 |          19.4523 |         -67.3519 |
[32m[20221213 12:37:21 @agent_ppo2.py:179][0m |          -0.0340 |          18.9968 |         -70.4072 |
[32m[20221213 12:37:21 @agent_ppo2.py:179][0m |          -0.0244 |          21.6686 |         -70.4918 |
[32m[20221213 12:37:21 @agent_ppo2.py:179][0m |          -0.0373 |          19.7807 |         -72.8765 |
[32m[20221213 12:37:21 @agent_ppo2.py:179][0m |          -0.0329 |          18.1212 |         -73.0016 |
[32m[20221213 12:37:21 @agent_ppo2.py:179][0m |          -0.0428 |          18.1454 |         -73.7577 |
[32m[20221213 12:37:21 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:37:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.15
[32m[20221213 12:37:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.93
[32m[20221213 12:37:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 3.71
[32m[20221213 12:37:21 @agent_ppo2.py:137][0m Total time:      21.41 min
[32m[20221213 12:37:21 @agent_ppo2.py:139][0m 1439744 total steps have happened
[32m[20221213 12:37:21 @agent_ppo2.py:115][0m #------------------------ Iteration 703 --------------------------#
[32m[20221213 12:37:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:22 @agent_ppo2.py:179][0m |           0.0817 |          28.2654 |         -64.9397 |
[32m[20221213 12:37:22 @agent_ppo2.py:179][0m |           0.0341 |          25.5962 |         -49.8652 |
[32m[20221213 12:37:22 @agent_ppo2.py:179][0m |          -0.0007 |          24.8085 |         -53.3426 |
[32m[20221213 12:37:22 @agent_ppo2.py:179][0m |          -0.0242 |          23.9798 |         -58.6310 |
[32m[20221213 12:37:22 @agent_ppo2.py:179][0m |          -0.0392 |          23.6604 |         -60.3107 |
[32m[20221213 12:37:22 @agent_ppo2.py:179][0m |          -0.0491 |          23.1667 |         -63.3610 |
[32m[20221213 12:37:22 @agent_ppo2.py:179][0m |          -0.0577 |          22.6156 |         -65.4550 |
[32m[20221213 12:37:23 @agent_ppo2.py:179][0m |          -0.0655 |          22.5580 |         -66.8794 |
[32m[20221213 12:37:23 @agent_ppo2.py:179][0m |          -0.0584 |          22.1286 |         -67.4704 |
[32m[20221213 12:37:23 @agent_ppo2.py:179][0m |          -0.0663 |          21.9177 |         -68.9875 |
[32m[20221213 12:37:23 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:37:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 231.11
[32m[20221213 12:37:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.06
[32m[20221213 12:37:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 269.03
[32m[20221213 12:37:23 @agent_ppo2.py:137][0m Total time:      21.44 min
[32m[20221213 12:37:23 @agent_ppo2.py:139][0m 1441792 total steps have happened
[32m[20221213 12:37:23 @agent_ppo2.py:115][0m #------------------------ Iteration 704 --------------------------#
[32m[20221213 12:37:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |           0.1312 |          34.5921 |         -65.5870 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |           0.0693 |          32.2789 |         -53.0855 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |           0.0014 |          30.5324 |         -61.8025 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |          -0.0180 |          30.1446 |         -64.5628 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |          -0.0301 |          30.3777 |         -67.1465 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |          -0.0399 |          28.2792 |         -67.6263 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |          -0.0467 |          28.2264 |         -70.5854 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |          -0.0594 |          27.7452 |         -72.4234 |
[32m[20221213 12:37:24 @agent_ppo2.py:179][0m |          -0.0576 |          27.6094 |         -73.7781 |
[32m[20221213 12:37:25 @agent_ppo2.py:179][0m |          -0.0515 |          29.8786 |         -75.3825 |
[32m[20221213 12:37:25 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:37:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.78
[32m[20221213 12:37:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 279.22
[32m[20221213 12:37:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 256.17
[32m[20221213 12:37:25 @agent_ppo2.py:137][0m Total time:      21.47 min
[32m[20221213 12:37:25 @agent_ppo2.py:139][0m 1443840 total steps have happened
[32m[20221213 12:37:25 @agent_ppo2.py:115][0m #------------------------ Iteration 705 --------------------------#
[32m[20221213 12:37:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:25 @agent_ppo2.py:179][0m |           0.0753 |          24.4492 |         -66.5314 |
[32m[20221213 12:37:25 @agent_ppo2.py:179][0m |           0.1120 |          22.3488 |         -43.2448 |
[32m[20221213 12:37:25 @agent_ppo2.py:179][0m |           0.0321 |          21.6248 |         -51.6348 |
[32m[20221213 12:37:26 @agent_ppo2.py:179][0m |           0.0029 |          21.1589 |         -56.4700 |
[32m[20221213 12:37:26 @agent_ppo2.py:179][0m |          -0.0196 |          20.7764 |         -64.2277 |
[32m[20221213 12:37:26 @agent_ppo2.py:179][0m |          -0.0292 |          20.5414 |         -67.2255 |
[32m[20221213 12:37:26 @agent_ppo2.py:179][0m |          -0.0317 |          20.5486 |         -69.8752 |
[32m[20221213 12:37:26 @agent_ppo2.py:179][0m |          -0.0358 |          20.2434 |         -71.3511 |
[32m[20221213 12:37:26 @agent_ppo2.py:179][0m |          -0.0433 |          19.9404 |         -72.6575 |
[32m[20221213 12:37:26 @agent_ppo2.py:179][0m |          -0.0472 |          19.7759 |         -73.7825 |
[32m[20221213 12:37:26 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:37:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.44
[32m[20221213 12:37:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.06
[32m[20221213 12:37:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.08
[32m[20221213 12:37:26 @agent_ppo2.py:137][0m Total time:      21.50 min
[32m[20221213 12:37:26 @agent_ppo2.py:139][0m 1445888 total steps have happened
[32m[20221213 12:37:26 @agent_ppo2.py:115][0m #------------------------ Iteration 706 --------------------------#
[32m[20221213 12:37:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:27 @agent_ppo2.py:179][0m |           0.0952 |          26.1094 |         -65.9486 |
[32m[20221213 12:37:27 @agent_ppo2.py:179][0m |           0.0872 |          25.0270 |         -48.1676 |
[32m[20221213 12:37:27 @agent_ppo2.py:179][0m |           0.0236 |          23.7397 |         -60.8430 |
[32m[20221213 12:37:27 @agent_ppo2.py:179][0m |          -0.0083 |          23.3309 |         -69.1595 |
[32m[20221213 12:37:27 @agent_ppo2.py:179][0m |          -0.0271 |          23.0306 |         -72.3190 |
[32m[20221213 12:37:28 @agent_ppo2.py:179][0m |          -0.0373 |          22.8250 |         -75.3258 |
[32m[20221213 12:37:28 @agent_ppo2.py:179][0m |          -0.0438 |          22.6182 |         -77.5476 |
[32m[20221213 12:37:28 @agent_ppo2.py:179][0m |          -0.0441 |          22.4511 |         -79.1399 |
[32m[20221213 12:37:28 @agent_ppo2.py:179][0m |          -0.0340 |          25.5032 |         -78.7589 |
[32m[20221213 12:37:28 @agent_ppo2.py:179][0m |          -0.0408 |          23.5541 |         -81.0713 |
[32m[20221213 12:37:28 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:37:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.27
[32m[20221213 12:37:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.87
[32m[20221213 12:37:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.39
[32m[20221213 12:37:28 @agent_ppo2.py:137][0m Total time:      21.52 min
[32m[20221213 12:37:28 @agent_ppo2.py:139][0m 1447936 total steps have happened
[32m[20221213 12:37:28 @agent_ppo2.py:115][0m #------------------------ Iteration 707 --------------------------#
[32m[20221213 12:37:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:29 @agent_ppo2.py:179][0m |           0.0521 |          28.0844 |         -75.0922 |
[32m[20221213 12:37:29 @agent_ppo2.py:179][0m |           0.0306 |          25.6319 |         -67.8963 |
[32m[20221213 12:37:29 @agent_ppo2.py:179][0m |           0.0177 |          24.5989 |         -64.5284 |
[32m[20221213 12:37:29 @agent_ppo2.py:179][0m |          -0.0179 |          23.7950 |         -70.0772 |
[32m[20221213 12:37:29 @agent_ppo2.py:179][0m |          -0.0363 |          23.2220 |         -76.0628 |
[32m[20221213 12:37:29 @agent_ppo2.py:179][0m |          -0.0416 |          22.8172 |         -79.0242 |
[32m[20221213 12:37:29 @agent_ppo2.py:179][0m |          -0.0431 |          22.3616 |         -78.9590 |
[32m[20221213 12:37:30 @agent_ppo2.py:179][0m |          -0.0522 |          22.0296 |         -81.1062 |
[32m[20221213 12:37:30 @agent_ppo2.py:179][0m |          -0.0527 |          21.7372 |         -81.5108 |
[32m[20221213 12:37:30 @agent_ppo2.py:179][0m |          -0.0488 |          21.5027 |         -83.1805 |
[32m[20221213 12:37:30 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:37:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 272.40
[32m[20221213 12:37:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.68
[32m[20221213 12:37:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.38
[32m[20221213 12:37:30 @agent_ppo2.py:137][0m Total time:      21.55 min
[32m[20221213 12:37:30 @agent_ppo2.py:139][0m 1449984 total steps have happened
[32m[20221213 12:37:30 @agent_ppo2.py:115][0m #------------------------ Iteration 708 --------------------------#
[32m[20221213 12:37:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:30 @agent_ppo2.py:179][0m |           0.0748 |          32.3312 |         -69.1868 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |           0.0855 |          29.4042 |         -57.7760 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |           0.0173 |          27.8892 |         -65.6761 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |          -0.0132 |          27.2220 |         -70.7281 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |          -0.0310 |          26.5070 |         -73.3158 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |          -0.0323 |          26.1453 |         -73.7126 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |          -0.0383 |          27.2452 |         -76.5255 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |          -0.0327 |          25.6142 |         -73.3140 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |          -0.0388 |          26.8527 |         -75.8361 |
[32m[20221213 12:37:31 @agent_ppo2.py:179][0m |          -0.0491 |          24.9700 |         -78.4910 |
[32m[20221213 12:37:31 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:37:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 285.01
[32m[20221213 12:37:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.45
[32m[20221213 12:37:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.75
[32m[20221213 12:37:32 @agent_ppo2.py:137][0m Total time:      21.58 min
[32m[20221213 12:37:32 @agent_ppo2.py:139][0m 1452032 total steps have happened
[32m[20221213 12:37:32 @agent_ppo2.py:115][0m #------------------------ Iteration 709 --------------------------#
[32m[20221213 12:37:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:32 @agent_ppo2.py:179][0m |           0.0577 |          27.1841 |         -71.1347 |
[32m[20221213 12:37:32 @agent_ppo2.py:179][0m |           0.0516 |          25.4960 |         -61.9791 |
[32m[20221213 12:37:32 @agent_ppo2.py:179][0m |          -0.0020 |          24.7138 |         -67.2124 |
[32m[20221213 12:37:32 @agent_ppo2.py:179][0m |          -0.0248 |          24.1039 |         -72.3494 |
[32m[20221213 12:37:33 @agent_ppo2.py:179][0m |          -0.0352 |          23.6281 |         -73.4524 |
[32m[20221213 12:37:33 @agent_ppo2.py:179][0m |          -0.0429 |          23.3182 |         -73.1494 |
[32m[20221213 12:37:33 @agent_ppo2.py:179][0m |          -0.0457 |          22.8458 |         -76.5628 |
[32m[20221213 12:37:33 @agent_ppo2.py:179][0m |          -0.0444 |          24.4896 |         -77.2843 |
[32m[20221213 12:37:33 @agent_ppo2.py:179][0m |          -0.0554 |          22.2652 |         -80.2997 |
[32m[20221213 12:37:33 @agent_ppo2.py:179][0m |          -0.0549 |          21.8465 |         -81.0637 |
[32m[20221213 12:37:33 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:37:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.09
[32m[20221213 12:37:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.59
[32m[20221213 12:37:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 292.66
[32m[20221213 12:37:33 @agent_ppo2.py:137][0m Total time:      21.61 min
[32m[20221213 12:37:33 @agent_ppo2.py:139][0m 1454080 total steps have happened
[32m[20221213 12:37:33 @agent_ppo2.py:115][0m #------------------------ Iteration 710 --------------------------#
[32m[20221213 12:37:34 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:37:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:34 @agent_ppo2.py:179][0m |           0.0739 |          29.5367 |         -67.2684 |
[32m[20221213 12:37:34 @agent_ppo2.py:179][0m |           0.1046 |          27.5631 |         -48.2576 |
[32m[20221213 12:37:34 @agent_ppo2.py:179][0m |           0.0421 |          26.5246 |         -50.3018 |
[32m[20221213 12:37:34 @agent_ppo2.py:179][0m |          -0.0031 |          25.9448 |         -61.0851 |
[32m[20221213 12:37:34 @agent_ppo2.py:179][0m |          -0.0184 |          25.7424 |         -66.4976 |
[32m[20221213 12:37:34 @agent_ppo2.py:179][0m |          -0.0341 |          25.3267 |         -66.5696 |
[32m[20221213 12:37:35 @agent_ppo2.py:179][0m |          -0.0356 |          24.9808 |         -69.5356 |
[32m[20221213 12:37:35 @agent_ppo2.py:179][0m |          -0.0348 |          24.7756 |         -70.0137 |
[32m[20221213 12:37:35 @agent_ppo2.py:179][0m |          -0.0459 |          24.5449 |         -73.5809 |
[32m[20221213 12:37:35 @agent_ppo2.py:179][0m |          -0.0427 |          24.3551 |         -74.1503 |
[32m[20221213 12:37:35 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:37:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 282.10
[32m[20221213 12:37:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 306.19
[32m[20221213 12:37:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.09
[32m[20221213 12:37:35 @agent_ppo2.py:137][0m Total time:      21.64 min
[32m[20221213 12:37:35 @agent_ppo2.py:139][0m 1456128 total steps have happened
[32m[20221213 12:37:35 @agent_ppo2.py:115][0m #------------------------ Iteration 711 --------------------------#
[32m[20221213 12:37:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |           0.1138 |          28.4406 |         -59.9351 |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |           0.0886 |          27.3775 |         -52.8152 |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |           0.0278 |          25.8822 |         -55.3315 |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |          -0.0023 |          25.5745 |         -60.5127 |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |          -0.0164 |          24.7406 |         -62.4244 |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |          -0.0119 |          28.3762 |         -66.9965 |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |          -0.0381 |          24.5319 |         -68.9960 |
[32m[20221213 12:37:36 @agent_ppo2.py:179][0m |          -0.0421 |          23.8697 |         -69.7286 |
[32m[20221213 12:37:37 @agent_ppo2.py:179][0m |          -0.0508 |          23.5900 |         -70.5227 |
[32m[20221213 12:37:37 @agent_ppo2.py:179][0m |          -0.0559 |          23.5030 |         -72.9044 |
[32m[20221213 12:37:37 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:37:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.48
[32m[20221213 12:37:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 306.35
[32m[20221213 12:37:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.06
[32m[20221213 12:37:37 @agent_ppo2.py:137][0m Total time:      21.67 min
[32m[20221213 12:37:37 @agent_ppo2.py:139][0m 1458176 total steps have happened
[32m[20221213 12:37:37 @agent_ppo2.py:115][0m #------------------------ Iteration 712 --------------------------#
[32m[20221213 12:37:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:37 @agent_ppo2.py:179][0m |           0.0643 |          32.2704 |         -63.1312 |
[32m[20221213 12:37:37 @agent_ppo2.py:179][0m |           0.0585 |          30.6924 |         -52.7302 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0083 |          29.5346 |         -59.9192 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0220 |          28.5725 |         -64.6836 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0243 |          29.3001 |         -65.4630 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0353 |          27.6421 |         -65.7479 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0428 |          27.2932 |         -67.7401 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0486 |          26.9637 |         -69.1923 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0412 |          27.1130 |         -70.8496 |
[32m[20221213 12:37:38 @agent_ppo2.py:179][0m |          -0.0533 |          26.4326 |         -73.3492 |
[32m[20221213 12:37:38 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:37:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 291.88
[32m[20221213 12:37:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.12
[32m[20221213 12:37:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.70
[32m[20221213 12:37:39 @agent_ppo2.py:137][0m Total time:      21.70 min
[32m[20221213 12:37:39 @agent_ppo2.py:139][0m 1460224 total steps have happened
[32m[20221213 12:37:39 @agent_ppo2.py:115][0m #------------------------ Iteration 713 --------------------------#
[32m[20221213 12:37:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:39 @agent_ppo2.py:179][0m |           0.0688 |          15.8675 |         -70.1310 |
[32m[20221213 12:37:39 @agent_ppo2.py:179][0m |           0.0655 |          16.1962 |         -61.2406 |
[32m[20221213 12:37:39 @agent_ppo2.py:179][0m |          -0.0028 |          13.0840 |         -65.8903 |
[32m[20221213 12:37:39 @agent_ppo2.py:179][0m |          -0.0259 |          12.5917 |         -69.2668 |
[32m[20221213 12:37:39 @agent_ppo2.py:179][0m |          -0.0345 |          12.1724 |         -71.6453 |
[32m[20221213 12:37:40 @agent_ppo2.py:179][0m |          -0.0481 |          12.0961 |         -72.9791 |
[32m[20221213 12:37:40 @agent_ppo2.py:179][0m |          -0.0432 |          11.7951 |         -73.8328 |
[32m[20221213 12:37:40 @agent_ppo2.py:179][0m |          -0.0461 |          11.4077 |         -74.6520 |
[32m[20221213 12:37:40 @agent_ppo2.py:179][0m |          -0.0556 |          11.2161 |         -75.2880 |
[32m[20221213 12:37:40 @agent_ppo2.py:179][0m |          -0.0571 |          11.0658 |         -77.8133 |
[32m[20221213 12:37:40 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:37:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 132.39
[32m[20221213 12:37:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.44
[32m[20221213 12:37:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.78
[32m[20221213 12:37:40 @agent_ppo2.py:137][0m Total time:      21.73 min
[32m[20221213 12:37:40 @agent_ppo2.py:139][0m 1462272 total steps have happened
[32m[20221213 12:37:40 @agent_ppo2.py:115][0m #------------------------ Iteration 714 --------------------------#
[32m[20221213 12:37:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:41 @agent_ppo2.py:179][0m |           0.0888 |          31.7871 |         -67.4709 |
[32m[20221213 12:37:41 @agent_ppo2.py:179][0m |           0.0705 |          30.6517 |         -56.8635 |
[32m[20221213 12:37:41 @agent_ppo2.py:179][0m |           0.0189 |          28.4914 |         -63.6025 |
[32m[20221213 12:37:41 @agent_ppo2.py:179][0m |          -0.0087 |          27.6495 |         -68.2285 |
[32m[20221213 12:37:41 @agent_ppo2.py:179][0m |          -0.0128 |          27.0490 |         -69.1061 |
[32m[20221213 12:37:41 @agent_ppo2.py:179][0m |          -0.0318 |          26.3223 |         -73.2265 |
[32m[20221213 12:37:41 @agent_ppo2.py:179][0m |          -0.0430 |          25.7249 |         -74.2708 |
[32m[20221213 12:37:42 @agent_ppo2.py:179][0m |          -0.0479 |          25.3344 |         -75.9490 |
[32m[20221213 12:37:42 @agent_ppo2.py:179][0m |          -0.0498 |          24.9532 |         -76.9847 |
[32m[20221213 12:37:42 @agent_ppo2.py:179][0m |          -0.0511 |          24.6605 |         -78.4429 |
[32m[20221213 12:37:42 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:37:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.17
[32m[20221213 12:37:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.82
[32m[20221213 12:37:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.30
[32m[20221213 12:37:42 @agent_ppo2.py:137][0m Total time:      21.76 min
[32m[20221213 12:37:42 @agent_ppo2.py:139][0m 1464320 total steps have happened
[32m[20221213 12:37:42 @agent_ppo2.py:115][0m #------------------------ Iteration 715 --------------------------#
[32m[20221213 12:37:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:42 @agent_ppo2.py:179][0m |           0.0888 |          31.3025 |         -63.6137 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |           0.0240 |          28.9707 |         -59.0652 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |           0.0009 |          28.1517 |         -65.3298 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |          -0.0129 |          29.0764 |         -67.9351 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |          -0.0391 |          27.3651 |         -71.3849 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |          -0.0414 |          26.7817 |         -71.5067 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |          -0.0461 |          26.4946 |         -74.3402 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |          -0.0502 |          26.3948 |         -75.1710 |
[32m[20221213 12:37:43 @agent_ppo2.py:179][0m |          -0.0462 |          26.1620 |         -76.4998 |
[32m[20221213 12:37:44 @agent_ppo2.py:179][0m |          -0.0510 |          25.8476 |         -77.2507 |
[32m[20221213 12:37:44 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:37:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.37
[32m[20221213 12:37:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 305.28
[32m[20221213 12:37:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 273.00
[32m[20221213 12:37:44 @agent_ppo2.py:137][0m Total time:      21.78 min
[32m[20221213 12:37:44 @agent_ppo2.py:139][0m 1466368 total steps have happened
[32m[20221213 12:37:44 @agent_ppo2.py:115][0m #------------------------ Iteration 716 --------------------------#
[32m[20221213 12:37:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:44 @agent_ppo2.py:179][0m |           0.0815 |          24.3667 |         -62.4191 |
[32m[20221213 12:37:44 @agent_ppo2.py:179][0m |           0.0311 |          22.5613 |         -61.1191 |
[32m[20221213 12:37:44 @agent_ppo2.py:179][0m |           0.0163 |          21.4149 |         -58.0926 |
[32m[20221213 12:37:45 @agent_ppo2.py:179][0m |          -0.0153 |          20.9033 |         -63.7402 |
[32m[20221213 12:37:45 @agent_ppo2.py:179][0m |          -0.0327 |          20.5863 |         -65.6911 |
[32m[20221213 12:37:45 @agent_ppo2.py:179][0m |          -0.0293 |          23.2228 |         -66.8644 |
[32m[20221213 12:37:45 @agent_ppo2.py:179][0m |          -0.0364 |          20.0344 |         -67.3811 |
[32m[20221213 12:37:45 @agent_ppo2.py:179][0m |          -0.0373 |          20.6404 |         -70.4451 |
[32m[20221213 12:37:45 @agent_ppo2.py:179][0m |          -0.0488 |          19.5538 |         -71.5095 |
[32m[20221213 12:37:45 @agent_ppo2.py:179][0m |          -0.0533 |          19.4597 |         -73.4207 |
[32m[20221213 12:37:45 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:37:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 261.26
[32m[20221213 12:37:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.66
[32m[20221213 12:37:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.43
[32m[20221213 12:37:46 @agent_ppo2.py:137][0m Total time:      21.81 min
[32m[20221213 12:37:46 @agent_ppo2.py:139][0m 1468416 total steps have happened
[32m[20221213 12:37:46 @agent_ppo2.py:115][0m #------------------------ Iteration 717 --------------------------#
[32m[20221213 12:37:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:46 @agent_ppo2.py:179][0m |           0.1752 |          32.8749 |         -64.4753 |
[32m[20221213 12:37:46 @agent_ppo2.py:179][0m |           0.0386 |          30.3674 |         -54.8999 |
[32m[20221213 12:37:46 @agent_ppo2.py:179][0m |          -0.0017 |          29.3947 |         -61.0215 |
[32m[20221213 12:37:46 @agent_ppo2.py:179][0m |          -0.0248 |          29.1639 |         -64.2213 |
[32m[20221213 12:37:46 @agent_ppo2.py:179][0m |          -0.0339 |          28.5754 |         -66.2276 |
[32m[20221213 12:37:47 @agent_ppo2.py:179][0m |          -0.0453 |          28.3080 |         -68.0884 |
[32m[20221213 12:37:47 @agent_ppo2.py:179][0m |          -0.0393 |          29.5923 |         -70.0656 |
[32m[20221213 12:37:47 @agent_ppo2.py:179][0m |          -0.0451 |          27.9183 |         -71.6829 |
[32m[20221213 12:37:47 @agent_ppo2.py:179][0m |          -0.0528 |          27.5780 |         -71.8975 |
[32m[20221213 12:37:47 @agent_ppo2.py:179][0m |          -0.0560 |          27.3975 |         -73.3734 |
[32m[20221213 12:37:47 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:37:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.57
[32m[20221213 12:37:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.95
[32m[20221213 12:37:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 312.86
[32m[20221213 12:37:47 @agent_ppo2.py:137][0m Total time:      21.84 min
[32m[20221213 12:37:47 @agent_ppo2.py:139][0m 1470464 total steps have happened
[32m[20221213 12:37:47 @agent_ppo2.py:115][0m #------------------------ Iteration 718 --------------------------#
[32m[20221213 12:37:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:48 @agent_ppo2.py:179][0m |           0.0906 |          31.0541 |         -57.9853 |
[32m[20221213 12:37:48 @agent_ppo2.py:179][0m |           0.1190 |          29.8263 |         -36.0639 |
[32m[20221213 12:37:48 @agent_ppo2.py:179][0m |           0.0296 |          30.4541 |         -47.9979 |
[32m[20221213 12:37:48 @agent_ppo2.py:179][0m |           0.0095 |          28.9272 |         -49.2339 |
[32m[20221213 12:37:48 @agent_ppo2.py:179][0m |          -0.0102 |          28.5511 |         -54.1475 |
[32m[20221213 12:37:48 @agent_ppo2.py:179][0m |          -0.0214 |          28.3114 |         -57.0491 |
[32m[20221213 12:37:48 @agent_ppo2.py:179][0m |          -0.0305 |          28.1027 |         -58.8307 |
[32m[20221213 12:37:49 @agent_ppo2.py:179][0m |          -0.0340 |          27.9634 |         -61.1761 |
[32m[20221213 12:37:49 @agent_ppo2.py:179][0m |          -0.0391 |          27.7382 |         -63.8465 |
[32m[20221213 12:37:49 @agent_ppo2.py:179][0m |          -0.0428 |          27.5757 |         -65.3327 |
[32m[20221213 12:37:49 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:37:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.46
[32m[20221213 12:37:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.27
[32m[20221213 12:37:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.33
[32m[20221213 12:37:49 @agent_ppo2.py:137][0m Total time:      21.87 min
[32m[20221213 12:37:49 @agent_ppo2.py:139][0m 1472512 total steps have happened
[32m[20221213 12:37:49 @agent_ppo2.py:115][0m #------------------------ Iteration 719 --------------------------#
[32m[20221213 12:37:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:49 @agent_ppo2.py:179][0m |           0.0794 |          32.7031 |         -57.8264 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |           0.1080 |          29.3731 |         -37.2015 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |           0.0360 |          28.9356 |         -47.9128 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |           0.0057 |          28.6140 |         -55.3319 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |          -0.0120 |          28.4049 |         -59.9882 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |          -0.0239 |          28.2091 |         -62.3816 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |          -0.0302 |          28.0321 |         -64.3789 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |          -0.0298 |          28.0149 |         -65.9877 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |          -0.0344 |          28.1222 |         -66.5162 |
[32m[20221213 12:37:50 @agent_ppo2.py:179][0m |          -0.0314 |          27.7007 |         -66.9329 |
[32m[20221213 12:37:50 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:37:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.40
[32m[20221213 12:37:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.96
[32m[20221213 12:37:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.98
[32m[20221213 12:37:51 @agent_ppo2.py:137][0m Total time:      21.90 min
[32m[20221213 12:37:51 @agent_ppo2.py:139][0m 1474560 total steps have happened
[32m[20221213 12:37:51 @agent_ppo2.py:115][0m #------------------------ Iteration 720 --------------------------#
[32m[20221213 12:37:51 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:37:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:51 @agent_ppo2.py:179][0m |           0.0661 |          21.5429 |         -60.1045 |
[32m[20221213 12:37:51 @agent_ppo2.py:179][0m |           0.0436 |          18.7664 |         -52.9835 |
[32m[20221213 12:37:51 @agent_ppo2.py:179][0m |          -0.0023 |          17.7303 |         -59.8351 |
[32m[20221213 12:37:52 @agent_ppo2.py:179][0m |          -0.0169 |          17.3188 |         -59.8495 |
[32m[20221213 12:37:52 @agent_ppo2.py:179][0m |          -0.0256 |          16.8157 |         -62.4403 |
[32m[20221213 12:37:52 @agent_ppo2.py:179][0m |          -0.0374 |          16.6155 |         -64.2684 |
[32m[20221213 12:37:52 @agent_ppo2.py:179][0m |          -0.0378 |          16.3802 |         -66.6279 |
[32m[20221213 12:37:52 @agent_ppo2.py:179][0m |          -0.0404 |          16.0222 |         -68.0359 |
[32m[20221213 12:37:52 @agent_ppo2.py:179][0m |          -0.0392 |          15.8827 |         -67.7159 |
[32m[20221213 12:37:52 @agent_ppo2.py:179][0m |          -0.0444 |          15.5924 |         -68.9831 |
[32m[20221213 12:37:52 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:37:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.45
[32m[20221213 12:37:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.86
[32m[20221213 12:37:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.92
[32m[20221213 12:37:52 @agent_ppo2.py:137][0m Total time:      21.93 min
[32m[20221213 12:37:52 @agent_ppo2.py:139][0m 1476608 total steps have happened
[32m[20221213 12:37:52 @agent_ppo2.py:115][0m #------------------------ Iteration 721 --------------------------#
[32m[20221213 12:37:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:53 @agent_ppo2.py:179][0m |           0.0817 |          15.1733 |         -54.4328 |
[32m[20221213 12:37:53 @agent_ppo2.py:179][0m |           0.0332 |          13.8169 |         -53.1703 |
[32m[20221213 12:37:53 @agent_ppo2.py:179][0m |           0.0070 |          13.3434 |         -59.5724 |
[32m[20221213 12:37:53 @agent_ppo2.py:179][0m |          -0.0250 |          13.0744 |         -61.7888 |
[32m[20221213 12:37:53 @agent_ppo2.py:179][0m |          -0.0173 |          12.8067 |         -65.8175 |
[32m[20221213 12:37:53 @agent_ppo2.py:179][0m |          -0.0287 |          12.5788 |         -68.2441 |
[32m[20221213 12:37:54 @agent_ppo2.py:179][0m |          -0.0354 |          12.4416 |         -68.6435 |
[32m[20221213 12:37:54 @agent_ppo2.py:179][0m |          -0.0387 |          12.2826 |         -69.4520 |
[32m[20221213 12:37:54 @agent_ppo2.py:179][0m |          -0.0344 |          12.2062 |         -70.1593 |
[32m[20221213 12:37:54 @agent_ppo2.py:179][0m |          -0.0418 |          12.1399 |         -71.4406 |
[32m[20221213 12:37:54 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:37:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 137.33
[32m[20221213 12:37:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 311.76
[32m[20221213 12:37:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.55
[32m[20221213 12:37:54 @agent_ppo2.py:137][0m Total time:      21.96 min
[32m[20221213 12:37:54 @agent_ppo2.py:139][0m 1478656 total steps have happened
[32m[20221213 12:37:54 @agent_ppo2.py:115][0m #------------------------ Iteration 722 --------------------------#
[32m[20221213 12:37:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |           0.0676 |          27.7279 |         -72.1311 |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |           0.0475 |          25.0298 |         -64.6400 |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |           0.0248 |          25.9766 |         -67.4527 |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |          -0.0086 |          23.9811 |         -72.1062 |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |          -0.0285 |          23.6986 |         -75.7407 |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |          -0.0260 |          23.5713 |         -75.7466 |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |          -0.0371 |          23.2751 |         -78.1030 |
[32m[20221213 12:37:55 @agent_ppo2.py:179][0m |          -0.0401 |          23.0279 |         -79.1043 |
[32m[20221213 12:37:56 @agent_ppo2.py:179][0m |          -0.0382 |          22.9623 |         -80.5040 |
[32m[20221213 12:37:56 @agent_ppo2.py:179][0m |          -0.0462 |          22.7476 |         -82.4915 |
[32m[20221213 12:37:56 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:37:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 259.66
[32m[20221213 12:37:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.48
[32m[20221213 12:37:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.27
[32m[20221213 12:37:56 @agent_ppo2.py:137][0m Total time:      21.98 min
[32m[20221213 12:37:56 @agent_ppo2.py:139][0m 1480704 total steps have happened
[32m[20221213 12:37:56 @agent_ppo2.py:115][0m #------------------------ Iteration 723 --------------------------#
[32m[20221213 12:37:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:56 @agent_ppo2.py:179][0m |           0.0756 |          29.5083 |         -69.8617 |
[32m[20221213 12:37:56 @agent_ppo2.py:179][0m |           0.0608 |          27.7679 |         -60.2016 |
[32m[20221213 12:37:56 @agent_ppo2.py:179][0m |           0.0106 |          26.8858 |         -65.7742 |
[32m[20221213 12:37:57 @agent_ppo2.py:179][0m |          -0.0129 |          26.4705 |         -71.8958 |
[32m[20221213 12:37:57 @agent_ppo2.py:179][0m |          -0.0292 |          26.2174 |         -75.0882 |
[32m[20221213 12:37:57 @agent_ppo2.py:179][0m |          -0.0380 |          25.8854 |         -77.2678 |
[32m[20221213 12:37:57 @agent_ppo2.py:179][0m |          -0.0371 |          25.7305 |         -79.0925 |
[32m[20221213 12:37:57 @agent_ppo2.py:179][0m |          -0.0352 |          25.7190 |         -80.1930 |
[32m[20221213 12:37:57 @agent_ppo2.py:179][0m |          -0.0361 |          27.8335 |         -81.8957 |
[32m[20221213 12:37:57 @agent_ppo2.py:179][0m |          -0.0472 |          25.2544 |         -85.3329 |
[32m[20221213 12:37:57 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:37:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.79
[32m[20221213 12:37:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.37
[32m[20221213 12:37:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.29
[32m[20221213 12:37:58 @agent_ppo2.py:137][0m Total time:      22.01 min
[32m[20221213 12:37:58 @agent_ppo2.py:139][0m 1482752 total steps have happened
[32m[20221213 12:37:58 @agent_ppo2.py:115][0m #------------------------ Iteration 724 --------------------------#
[32m[20221213 12:37:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:37:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:37:58 @agent_ppo2.py:179][0m |           0.0846 |          32.1461 |         -63.2443 |
[32m[20221213 12:37:58 @agent_ppo2.py:179][0m |           0.0820 |          30.7980 |         -48.6606 |
[32m[20221213 12:37:58 @agent_ppo2.py:179][0m |           0.0280 |          30.1736 |         -55.4401 |
[32m[20221213 12:37:58 @agent_ppo2.py:179][0m |           0.0136 |          29.7187 |         -59.6186 |
[32m[20221213 12:37:58 @agent_ppo2.py:179][0m |          -0.0152 |          29.3216 |         -63.5129 |
[32m[20221213 12:37:59 @agent_ppo2.py:179][0m |          -0.0267 |          29.0620 |         -66.1646 |
[32m[20221213 12:37:59 @agent_ppo2.py:179][0m |          -0.0271 |          29.4326 |         -67.5573 |
[32m[20221213 12:37:59 @agent_ppo2.py:179][0m |          -0.0317 |          28.5684 |         -69.1021 |
[32m[20221213 12:37:59 @agent_ppo2.py:179][0m |          -0.0341 |          28.3467 |         -70.4836 |
[32m[20221213 12:37:59 @agent_ppo2.py:179][0m |          -0.0384 |          28.1822 |         -72.1415 |
[32m[20221213 12:37:59 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:37:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 268.95
[32m[20221213 12:37:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.49
[32m[20221213 12:37:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 293.24
[32m[20221213 12:37:59 @agent_ppo2.py:137][0m Total time:      22.04 min
[32m[20221213 12:37:59 @agent_ppo2.py:139][0m 1484800 total steps have happened
[32m[20221213 12:37:59 @agent_ppo2.py:115][0m #------------------------ Iteration 725 --------------------------#
[32m[20221213 12:38:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:00 @agent_ppo2.py:179][0m |           0.1273 |          35.4328 |         -55.9232 |
[32m[20221213 12:38:00 @agent_ppo2.py:179][0m |           0.1062 |          30.3787 |         -40.5036 |
[32m[20221213 12:38:00 @agent_ppo2.py:179][0m |           0.0329 |          29.7785 |         -55.7249 |
[32m[20221213 12:38:00 @agent_ppo2.py:179][0m |           0.0024 |          29.5700 |         -62.5826 |
[32m[20221213 12:38:00 @agent_ppo2.py:179][0m |          -0.0124 |          29.0471 |         -64.6449 |
[32m[20221213 12:38:00 @agent_ppo2.py:179][0m |          -0.0200 |          28.8006 |         -67.3150 |
[32m[20221213 12:38:00 @agent_ppo2.py:179][0m |          -0.0302 |          28.5857 |         -69.6644 |
[32m[20221213 12:38:01 @agent_ppo2.py:179][0m |          -0.0344 |          28.4097 |         -71.8452 |
[32m[20221213 12:38:01 @agent_ppo2.py:179][0m |          -0.0360 |          28.1683 |         -73.5459 |
[32m[20221213 12:38:01 @agent_ppo2.py:179][0m |          -0.0362 |          27.9955 |         -73.6820 |
[32m[20221213 12:38:01 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:38:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 293.53
[32m[20221213 12:38:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.27
[32m[20221213 12:38:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.75
[32m[20221213 12:38:01 @agent_ppo2.py:137][0m Total time:      22.07 min
[32m[20221213 12:38:01 @agent_ppo2.py:139][0m 1486848 total steps have happened
[32m[20221213 12:38:01 @agent_ppo2.py:115][0m #------------------------ Iteration 726 --------------------------#
[32m[20221213 12:38:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:01 @agent_ppo2.py:179][0m |           0.0688 |          31.8653 |         -62.1922 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |           0.0516 |          30.2352 |         -51.8441 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |           0.0246 |          29.7025 |         -55.5897 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |          -0.0075 |          29.4669 |         -58.9240 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |          -0.0218 |          29.2474 |         -64.8061 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |          -0.0342 |          29.0864 |         -66.7456 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |          -0.0356 |          28.9417 |         -68.8149 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |          -0.0339 |          28.7330 |         -68.2624 |
[32m[20221213 12:38:02 @agent_ppo2.py:179][0m |          -0.0379 |          29.0757 |         -71.9122 |
[32m[20221213 12:38:03 @agent_ppo2.py:179][0m |          -0.0402 |          28.5150 |         -70.9842 |
[32m[20221213 12:38:03 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:38:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.54
[32m[20221213 12:38:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.84
[32m[20221213 12:38:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.26
[32m[20221213 12:38:03 @agent_ppo2.py:137][0m Total time:      22.10 min
[32m[20221213 12:38:03 @agent_ppo2.py:139][0m 1488896 total steps have happened
[32m[20221213 12:38:03 @agent_ppo2.py:115][0m #------------------------ Iteration 727 --------------------------#
[32m[20221213 12:38:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:03 @agent_ppo2.py:179][0m |           0.0486 |          23.2120 |         -64.9225 |
[32m[20221213 12:38:03 @agent_ppo2.py:179][0m |           0.0263 |          21.6834 |         -62.3729 |
[32m[20221213 12:38:03 @agent_ppo2.py:179][0m |          -0.0017 |          21.1518 |         -62.9965 |
[32m[20221213 12:38:04 @agent_ppo2.py:179][0m |          -0.0136 |          20.8620 |         -65.7800 |
[32m[20221213 12:38:04 @agent_ppo2.py:179][0m |          -0.0212 |          20.6017 |         -66.2771 |
[32m[20221213 12:38:04 @agent_ppo2.py:179][0m |          -0.0267 |          20.3636 |         -66.1032 |
[32m[20221213 12:38:04 @agent_ppo2.py:179][0m |          -0.0349 |          20.0856 |         -67.5362 |
[32m[20221213 12:38:04 @agent_ppo2.py:179][0m |          -0.0382 |          19.8624 |         -69.5526 |
[32m[20221213 12:38:04 @agent_ppo2.py:179][0m |          -0.0305 |          19.6480 |         -69.7586 |
[32m[20221213 12:38:04 @agent_ppo2.py:179][0m |          -0.0381 |          19.4519 |         -71.0990 |
[32m[20221213 12:38:04 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:38:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 193.35
[32m[20221213 12:38:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.48
[32m[20221213 12:38:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.59
[32m[20221213 12:38:04 @agent_ppo2.py:137][0m Total time:      22.13 min
[32m[20221213 12:38:04 @agent_ppo2.py:139][0m 1490944 total steps have happened
[32m[20221213 12:38:04 @agent_ppo2.py:115][0m #------------------------ Iteration 728 --------------------------#
[32m[20221213 12:38:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:05 @agent_ppo2.py:179][0m |           0.0604 |          27.4121 |         -53.9964 |
[32m[20221213 12:38:05 @agent_ppo2.py:179][0m |           0.0383 |          25.1266 |         -50.9596 |
[32m[20221213 12:38:05 @agent_ppo2.py:179][0m |          -0.0016 |          23.9774 |         -54.0903 |
[32m[20221213 12:38:05 @agent_ppo2.py:179][0m |          -0.0182 |          23.3661 |         -56.8924 |
[32m[20221213 12:38:05 @agent_ppo2.py:179][0m |          -0.0244 |          22.8141 |         -58.3648 |
[32m[20221213 12:38:05 @agent_ppo2.py:179][0m |          -0.0335 |          22.2297 |         -59.4205 |
[32m[20221213 12:38:06 @agent_ppo2.py:179][0m |          -0.0335 |          22.0597 |         -61.2511 |
[32m[20221213 12:38:06 @agent_ppo2.py:179][0m |          -0.0395 |          21.5166 |         -62.7004 |
[32m[20221213 12:38:06 @agent_ppo2.py:179][0m |          -0.0427 |          21.3429 |         -63.2297 |
[32m[20221213 12:38:06 @agent_ppo2.py:179][0m |          -0.0494 |          21.0450 |         -64.9010 |
[32m[20221213 12:38:06 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:38:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.78
[32m[20221213 12:38:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 327.28
[32m[20221213 12:38:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.95
[32m[20221213 12:38:06 @agent_ppo2.py:137][0m Total time:      22.16 min
[32m[20221213 12:38:06 @agent_ppo2.py:139][0m 1492992 total steps have happened
[32m[20221213 12:38:06 @agent_ppo2.py:115][0m #------------------------ Iteration 729 --------------------------#
[32m[20221213 12:38:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |           0.0855 |          34.3518 |         -56.8709 |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |           0.0695 |          31.6191 |         -49.9549 |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |           0.0192 |          30.6945 |         -51.6153 |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |          -0.0065 |          30.0473 |         -54.6975 |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |          -0.0318 |          29.6032 |         -60.4698 |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |          -0.0368 |          29.1971 |         -62.1858 |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |          -0.0374 |          28.9057 |         -64.2112 |
[32m[20221213 12:38:07 @agent_ppo2.py:179][0m |          -0.0413 |          28.5708 |         -64.2114 |
[32m[20221213 12:38:08 @agent_ppo2.py:179][0m |          -0.0466 |          28.3894 |         -67.0454 |
[32m[20221213 12:38:08 @agent_ppo2.py:179][0m |          -0.0498 |          28.2283 |         -67.6169 |
[32m[20221213 12:38:08 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:38:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 299.01
[32m[20221213 12:38:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.63
[32m[20221213 12:38:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.72
[32m[20221213 12:38:08 @agent_ppo2.py:137][0m Total time:      22.19 min
[32m[20221213 12:38:08 @agent_ppo2.py:139][0m 1495040 total steps have happened
[32m[20221213 12:38:08 @agent_ppo2.py:115][0m #------------------------ Iteration 730 --------------------------#
[32m[20221213 12:38:08 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:38:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:08 @agent_ppo2.py:179][0m |           0.0960 |          30.6486 |         -60.4443 |
[32m[20221213 12:38:08 @agent_ppo2.py:179][0m |           0.0880 |          28.8043 |         -36.3319 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |           0.0255 |          28.1978 |         -50.0571 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |           0.0038 |          27.6772 |         -57.3423 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |          -0.0063 |          28.7946 |         -62.7295 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |          -0.0198 |          26.9647 |         -63.8215 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |          -0.0218 |          29.7188 |         -67.9680 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |          -0.0310 |          26.6844 |         -66.8406 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |          -0.0356 |          26.4490 |         -69.2472 |
[32m[20221213 12:38:09 @agent_ppo2.py:179][0m |          -0.0376 |          26.1771 |         -69.9203 |
[32m[20221213 12:38:09 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:38:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.79
[32m[20221213 12:38:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.36
[32m[20221213 12:38:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.55
[32m[20221213 12:38:10 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 353.55
[32m[20221213 12:38:10 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 353.55
[32m[20221213 12:38:10 @agent_ppo2.py:137][0m Total time:      22.21 min
[32m[20221213 12:38:10 @agent_ppo2.py:139][0m 1497088 total steps have happened
[32m[20221213 12:38:10 @agent_ppo2.py:115][0m #------------------------ Iteration 731 --------------------------#
[32m[20221213 12:38:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:10 @agent_ppo2.py:179][0m |           0.0632 |          32.7231 |         -64.7578 |
[32m[20221213 12:38:10 @agent_ppo2.py:179][0m |           0.0684 |          31.1966 |         -56.8953 |
[32m[20221213 12:38:10 @agent_ppo2.py:179][0m |           0.0216 |          30.4774 |         -59.0574 |
[32m[20221213 12:38:10 @agent_ppo2.py:179][0m |          -0.0184 |          30.0211 |         -62.4890 |
[32m[20221213 12:38:11 @agent_ppo2.py:179][0m |          -0.0323 |          30.0985 |         -65.5404 |
[32m[20221213 12:38:11 @agent_ppo2.py:179][0m |          -0.0364 |          29.6020 |         -66.5281 |
[32m[20221213 12:38:11 @agent_ppo2.py:179][0m |          -0.0434 |          29.2081 |         -68.4807 |
[32m[20221213 12:38:11 @agent_ppo2.py:179][0m |          -0.0395 |          29.5255 |         -70.3972 |
[32m[20221213 12:38:11 @agent_ppo2.py:179][0m |          -0.0468 |          28.8202 |         -70.4736 |
[32m[20221213 12:38:11 @agent_ppo2.py:179][0m |          -0.0418 |          28.7126 |         -70.9932 |
[32m[20221213 12:38:11 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:38:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 258.97
[32m[20221213 12:38:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 295.89
[32m[20221213 12:38:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.97
[32m[20221213 12:38:11 @agent_ppo2.py:137][0m Total time:      22.24 min
[32m[20221213 12:38:11 @agent_ppo2.py:139][0m 1499136 total steps have happened
[32m[20221213 12:38:11 @agent_ppo2.py:115][0m #------------------------ Iteration 732 --------------------------#
[32m[20221213 12:38:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:12 @agent_ppo2.py:179][0m |           0.0507 |          29.8937 |         -62.6040 |
[32m[20221213 12:38:12 @agent_ppo2.py:179][0m |           0.0440 |          28.9711 |         -55.9671 |
[32m[20221213 12:38:12 @agent_ppo2.py:179][0m |           0.0018 |          28.4660 |         -60.9627 |
[32m[20221213 12:38:12 @agent_ppo2.py:179][0m |           0.0022 |          28.2183 |         -65.9281 |
[32m[20221213 12:38:12 @agent_ppo2.py:179][0m |          -0.0182 |          27.8904 |         -66.8468 |
[32m[20221213 12:38:12 @agent_ppo2.py:179][0m |          -0.0308 |          27.7011 |         -69.9718 |
[32m[20221213 12:38:12 @agent_ppo2.py:179][0m |          -0.0359 |          27.6157 |         -72.4934 |
[32m[20221213 12:38:13 @agent_ppo2.py:179][0m |          -0.0398 |          27.4292 |         -73.4853 |
[32m[20221213 12:38:13 @agent_ppo2.py:179][0m |          -0.0414 |          27.2584 |         -73.9912 |
[32m[20221213 12:38:13 @agent_ppo2.py:179][0m |          -0.0451 |          27.1077 |         -75.9805 |
[32m[20221213 12:38:13 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:38:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.90
[32m[20221213 12:38:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.70
[32m[20221213 12:38:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 280.95
[32m[20221213 12:38:13 @agent_ppo2.py:137][0m Total time:      22.27 min
[32m[20221213 12:38:13 @agent_ppo2.py:139][0m 1501184 total steps have happened
[32m[20221213 12:38:13 @agent_ppo2.py:115][0m #------------------------ Iteration 733 --------------------------#
[32m[20221213 12:38:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:13 @agent_ppo2.py:179][0m |           0.0646 |          30.5863 |         -62.5143 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |           0.0671 |          29.5071 |         -53.6871 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |           0.0142 |          29.0112 |         -59.4566 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |          -0.0045 |          28.9356 |         -61.2978 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |          -0.0224 |          28.1574 |         -64.8244 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |          -0.0277 |          27.7969 |         -66.7991 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |          -0.0368 |          28.6406 |         -67.0167 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |          -0.0483 |          27.2809 |         -69.8355 |
[32m[20221213 12:38:14 @agent_ppo2.py:179][0m |          -0.0392 |          27.0607 |         -70.4868 |
[32m[20221213 12:38:15 @agent_ppo2.py:179][0m |          -0.0493 |          26.7235 |         -71.5912 |
[32m[20221213 12:38:15 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:38:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 282.85
[32m[20221213 12:38:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.99
[32m[20221213 12:38:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.75
[32m[20221213 12:38:15 @agent_ppo2.py:137][0m Total time:      22.30 min
[32m[20221213 12:38:15 @agent_ppo2.py:139][0m 1503232 total steps have happened
[32m[20221213 12:38:15 @agent_ppo2.py:115][0m #------------------------ Iteration 734 --------------------------#
[32m[20221213 12:38:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:15 @agent_ppo2.py:179][0m |           0.1037 |          29.5868 |         -61.3796 |
[32m[20221213 12:38:15 @agent_ppo2.py:179][0m |           0.0873 |          28.4876 |         -45.5039 |
[32m[20221213 12:38:15 @agent_ppo2.py:179][0m |           0.0442 |          28.0756 |         -49.4878 |
[32m[20221213 12:38:16 @agent_ppo2.py:179][0m |           0.0103 |          27.7348 |         -56.2857 |
[32m[20221213 12:38:16 @agent_ppo2.py:179][0m |           0.0084 |          29.8497 |         -61.8773 |
[32m[20221213 12:38:16 @agent_ppo2.py:179][0m |          -0.0174 |          27.4793 |         -64.9506 |
[32m[20221213 12:38:16 @agent_ppo2.py:179][0m |          -0.0243 |          27.3558 |         -67.8937 |
[32m[20221213 12:38:16 @agent_ppo2.py:179][0m |          -0.0332 |          27.0758 |         -69.5604 |
[32m[20221213 12:38:16 @agent_ppo2.py:179][0m |          -0.0339 |          26.9468 |         -72.9062 |
[32m[20221213 12:38:16 @agent_ppo2.py:179][0m |          -0.0360 |          27.0278 |         -75.0306 |
[32m[20221213 12:38:16 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.64
[32m[20221213 12:38:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.95
[32m[20221213 12:38:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.39
[32m[20221213 12:38:16 @agent_ppo2.py:137][0m Total time:      22.33 min
[32m[20221213 12:38:16 @agent_ppo2.py:139][0m 1505280 total steps have happened
[32m[20221213 12:38:16 @agent_ppo2.py:115][0m #------------------------ Iteration 735 --------------------------#
[32m[20221213 12:38:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:17 @agent_ppo2.py:179][0m |           0.0726 |          28.1327 |         -66.9562 |
[32m[20221213 12:38:17 @agent_ppo2.py:179][0m |           0.0854 |          28.2706 |         -54.3273 |
[32m[20221213 12:38:17 @agent_ppo2.py:179][0m |           0.0238 |          26.5012 |         -63.9037 |
[32m[20221213 12:38:17 @agent_ppo2.py:179][0m |          -0.0103 |          26.2261 |         -69.1908 |
[32m[20221213 12:38:17 @agent_ppo2.py:179][0m |          -0.0284 |          25.9718 |         -71.1613 |
[32m[20221213 12:38:17 @agent_ppo2.py:179][0m |          -0.0313 |          25.7220 |         -73.6033 |
[32m[20221213 12:38:18 @agent_ppo2.py:179][0m |          -0.0388 |          25.4699 |         -74.9072 |
[32m[20221213 12:38:18 @agent_ppo2.py:179][0m |          -0.0428 |          25.3448 |         -77.6939 |
[32m[20221213 12:38:18 @agent_ppo2.py:179][0m |          -0.0359 |          27.2973 |         -78.4599 |
[32m[20221213 12:38:18 @agent_ppo2.py:179][0m |          -0.0430 |          25.0184 |         -78.4490 |
[32m[20221213 12:38:18 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.79
[32m[20221213 12:38:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.67
[32m[20221213 12:38:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.87
[32m[20221213 12:38:18 @agent_ppo2.py:137][0m Total time:      22.36 min
[32m[20221213 12:38:18 @agent_ppo2.py:139][0m 1507328 total steps have happened
[32m[20221213 12:38:18 @agent_ppo2.py:115][0m #------------------------ Iteration 736 --------------------------#
[32m[20221213 12:38:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |           0.0365 |          27.4946 |         -71.8603 |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |           0.0101 |          23.7946 |         -69.7291 |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |          -0.0189 |          22.7475 |         -75.0361 |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |          -0.0251 |          21.3622 |         -75.5767 |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |          -0.0355 |          20.4376 |         -76.8801 |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |          -0.0370 |          20.0052 |         -78.8670 |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |          -0.0388 |          20.8708 |         -80.5266 |
[32m[20221213 12:38:19 @agent_ppo2.py:179][0m |          -0.0469 |          19.0814 |         -81.2961 |
[32m[20221213 12:38:20 @agent_ppo2.py:179][0m |          -0.0468 |          18.6705 |         -83.5508 |
[32m[20221213 12:38:20 @agent_ppo2.py:179][0m |          -0.0514 |          18.4149 |         -83.6903 |
[32m[20221213 12:38:20 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:38:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.05
[32m[20221213 12:38:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.34
[32m[20221213 12:38:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.37
[32m[20221213 12:38:20 @agent_ppo2.py:137][0m Total time:      22.39 min
[32m[20221213 12:38:20 @agent_ppo2.py:139][0m 1509376 total steps have happened
[32m[20221213 12:38:20 @agent_ppo2.py:115][0m #------------------------ Iteration 737 --------------------------#
[32m[20221213 12:38:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:20 @agent_ppo2.py:179][0m |           0.0590 |          28.7825 |         -77.5728 |
[32m[20221213 12:38:20 @agent_ppo2.py:179][0m |           0.0277 |          25.4707 |         -79.5594 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0099 |          24.4540 |         -80.4804 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0249 |          23.7460 |         -82.4213 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0339 |          23.1017 |         -82.9634 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0442 |          22.6909 |         -85.4110 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0499 |          22.3889 |         -86.5943 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0556 |          22.1111 |         -88.9839 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0471 |          21.8136 |         -89.2087 |
[32m[20221213 12:38:21 @agent_ppo2.py:179][0m |          -0.0539 |          21.6555 |         -90.6640 |
[32m[20221213 12:38:21 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:38:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.24
[32m[20221213 12:38:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.56
[32m[20221213 12:38:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 366.06
[32m[20221213 12:38:22 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 366.06
[32m[20221213 12:38:22 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 366.06
[32m[20221213 12:38:22 @agent_ppo2.py:137][0m Total time:      22.41 min
[32m[20221213 12:38:22 @agent_ppo2.py:139][0m 1511424 total steps have happened
[32m[20221213 12:38:22 @agent_ppo2.py:115][0m #------------------------ Iteration 738 --------------------------#
[32m[20221213 12:38:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:22 @agent_ppo2.py:179][0m |           0.0733 |          25.6252 |         -72.1100 |
[32m[20221213 12:38:22 @agent_ppo2.py:179][0m |           0.0458 |          23.6195 |         -63.0826 |
[32m[20221213 12:38:22 @agent_ppo2.py:179][0m |           0.0091 |          22.9254 |         -73.0014 |
[32m[20221213 12:38:22 @agent_ppo2.py:179][0m |          -0.0093 |          22.2371 |         -77.5258 |
[32m[20221213 12:38:23 @agent_ppo2.py:179][0m |          -0.0262 |          21.8269 |         -81.9923 |
[32m[20221213 12:38:23 @agent_ppo2.py:179][0m |          -0.0357 |          21.4733 |         -83.6315 |
[32m[20221213 12:38:23 @agent_ppo2.py:179][0m |          -0.0445 |          21.2290 |         -85.1899 |
[32m[20221213 12:38:23 @agent_ppo2.py:179][0m |          -0.0408 |          20.9854 |         -86.6427 |
[32m[20221213 12:38:23 @agent_ppo2.py:179][0m |          -0.0266 |          23.9305 |         -89.3016 |
[32m[20221213 12:38:23 @agent_ppo2.py:179][0m |          -0.0152 |          26.2553 |         -86.1298 |
[32m[20221213 12:38:23 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:38:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 256.63
[32m[20221213 12:38:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.27
[32m[20221213 12:38:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 325.31
[32m[20221213 12:38:23 @agent_ppo2.py:137][0m Total time:      22.44 min
[32m[20221213 12:38:23 @agent_ppo2.py:139][0m 1513472 total steps have happened
[32m[20221213 12:38:23 @agent_ppo2.py:115][0m #------------------------ Iteration 739 --------------------------#
[32m[20221213 12:38:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:24 @agent_ppo2.py:179][0m |           0.0640 |          11.4012 |         -73.2519 |
[32m[20221213 12:38:24 @agent_ppo2.py:179][0m |           0.0271 |          10.1096 |         -48.3295 |
[32m[20221213 12:38:24 @agent_ppo2.py:179][0m |          -0.0066 |           9.6662 |         -52.5247 |
[32m[20221213 12:38:24 @agent_ppo2.py:179][0m |          -0.0227 |           9.3689 |         -54.9554 |
[32m[20221213 12:38:24 @agent_ppo2.py:179][0m |          -0.0271 |           9.2672 |         -56.6503 |
[32m[20221213 12:38:24 @agent_ppo2.py:179][0m |          -0.0374 |           9.0220 |         -58.2729 |
[32m[20221213 12:38:24 @agent_ppo2.py:179][0m |          -0.0374 |           8.8875 |         -58.7228 |
[32m[20221213 12:38:25 @agent_ppo2.py:179][0m |          -0.0400 |           8.7478 |         -61.1740 |
[32m[20221213 12:38:25 @agent_ppo2.py:179][0m |          -0.0443 |           8.6346 |         -61.5093 |
[32m[20221213 12:38:25 @agent_ppo2.py:179][0m |          -0.0432 |           8.5690 |         -62.7433 |
[32m[20221213 12:38:25 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:38:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.30
[32m[20221213 12:38:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 298.44
[32m[20221213 12:38:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.79
[32m[20221213 12:38:25 @agent_ppo2.py:137][0m Total time:      22.47 min
[32m[20221213 12:38:25 @agent_ppo2.py:139][0m 1515520 total steps have happened
[32m[20221213 12:38:25 @agent_ppo2.py:115][0m #------------------------ Iteration 740 --------------------------#
[32m[20221213 12:38:25 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:38:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:25 @agent_ppo2.py:179][0m |           0.0819 |          31.4988 |         -84.7157 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |           0.0689 |          29.6334 |         -70.7024 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |           0.0096 |          28.9169 |         -81.3702 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |          -0.0187 |          28.1872 |         -86.8450 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |          -0.0259 |          27.9009 |         -89.2959 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |          -0.0375 |          27.5514 |         -91.6432 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |          -0.0379 |          27.1924 |         -95.7659 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |          -0.0399 |          27.0750 |         -93.3420 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |          -0.0420 |          26.7199 |         -93.9861 |
[32m[20221213 12:38:26 @agent_ppo2.py:179][0m |          -0.0390 |          26.5480 |         -98.3343 |
[32m[20221213 12:38:26 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:38:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.02
[32m[20221213 12:38:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.36
[32m[20221213 12:38:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 3.59
[32m[20221213 12:38:27 @agent_ppo2.py:137][0m Total time:      22.50 min
[32m[20221213 12:38:27 @agent_ppo2.py:139][0m 1517568 total steps have happened
[32m[20221213 12:38:27 @agent_ppo2.py:115][0m #------------------------ Iteration 741 --------------------------#
[32m[20221213 12:38:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:27 @agent_ppo2.py:179][0m |           0.0528 |          20.7402 |         -88.7323 |
[32m[20221213 12:38:27 @agent_ppo2.py:179][0m |           0.0374 |          19.5424 |         -81.6310 |
[32m[20221213 12:38:27 @agent_ppo2.py:179][0m |           0.0061 |          19.2148 |         -90.6977 |
[32m[20221213 12:38:27 @agent_ppo2.py:179][0m |           0.0021 |          18.9713 |         -85.9878 |
[32m[20221213 12:38:28 @agent_ppo2.py:179][0m |          -0.0129 |          18.6683 |         -85.8107 |
[32m[20221213 12:38:28 @agent_ppo2.py:179][0m |          -0.0117 |          18.3791 |         -82.7769 |
[32m[20221213 12:38:28 @agent_ppo2.py:179][0m |          -0.0148 |          18.2347 |         -81.2980 |
[32m[20221213 12:38:28 @agent_ppo2.py:179][0m |          -0.0262 |          18.0566 |         -80.0281 |
[32m[20221213 12:38:28 @agent_ppo2.py:179][0m |          -0.0372 |          18.1343 |         -80.2372 |
[32m[20221213 12:38:28 @agent_ppo2.py:179][0m |          -0.0466 |          17.8267 |         -82.7568 |
[32m[20221213 12:38:28 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:38:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 205.04
[32m[20221213 12:38:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.42
[32m[20221213 12:38:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.77
[32m[20221213 12:38:28 @agent_ppo2.py:137][0m Total time:      22.53 min
[32m[20221213 12:38:28 @agent_ppo2.py:139][0m 1519616 total steps have happened
[32m[20221213 12:38:28 @agent_ppo2.py:115][0m #------------------------ Iteration 742 --------------------------#
[32m[20221213 12:38:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:29 @agent_ppo2.py:179][0m |           0.2208 |           5.7348 |         -54.2386 |
[32m[20221213 12:38:29 @agent_ppo2.py:179][0m |           0.0553 |           4.9341 |         -19.3239 |
[32m[20221213 12:38:29 @agent_ppo2.py:179][0m |           0.0465 |           5.1001 |         -18.5488 |
[32m[20221213 12:38:29 @agent_ppo2.py:179][0m |           0.0311 |           4.7863 |         -18.8580 |
[32m[20221213 12:38:29 @agent_ppo2.py:179][0m |           0.0286 |           4.7610 |         -18.9541 |
[32m[20221213 12:38:29 @agent_ppo2.py:179][0m |           0.0231 |           4.7464 |         -18.6789 |
[32m[20221213 12:38:29 @agent_ppo2.py:179][0m |           0.0207 |           4.8172 |         -19.2150 |
[32m[20221213 12:38:30 @agent_ppo2.py:179][0m |           0.0145 |           4.7445 |         -19.7497 |
[32m[20221213 12:38:30 @agent_ppo2.py:179][0m |           0.0217 |           4.7314 |         -19.6039 |
[32m[20221213 12:38:30 @agent_ppo2.py:179][0m |           0.0253 |           5.0711 |         -20.3005 |
[32m[20221213 12:38:30 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:38:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 39.09
[32m[20221213 12:38:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 43.29
[32m[20221213 12:38:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.47
[32m[20221213 12:38:30 @agent_ppo2.py:137][0m Total time:      22.55 min
[32m[20221213 12:38:30 @agent_ppo2.py:139][0m 1521664 total steps have happened
[32m[20221213 12:38:30 @agent_ppo2.py:115][0m #------------------------ Iteration 743 --------------------------#
[32m[20221213 12:38:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:30 @agent_ppo2.py:179][0m |           0.0986 |          32.0320 |         -76.2470 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |           0.0784 |          31.1210 |         -56.0775 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |           0.0311 |          30.0910 |         -66.7688 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |          -0.0016 |          29.5770 |         -75.9750 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |          -0.0138 |          29.2651 |         -82.4880 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |          -0.0198 |          29.0653 |         -83.5774 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |          -0.0168 |          29.9987 |         -84.5416 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |          -0.0347 |          28.5750 |         -88.9933 |
[32m[20221213 12:38:31 @agent_ppo2.py:179][0m |          -0.0413 |          28.3491 |         -91.7447 |
[32m[20221213 12:38:32 @agent_ppo2.py:179][0m |          -0.0442 |          28.1977 |         -94.1835 |
[32m[20221213 12:38:32 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:38:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.00
[32m[20221213 12:38:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.65
[32m[20221213 12:38:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.82
[32m[20221213 12:38:32 @agent_ppo2.py:137][0m Total time:      22.58 min
[32m[20221213 12:38:32 @agent_ppo2.py:139][0m 1523712 total steps have happened
[32m[20221213 12:38:32 @agent_ppo2.py:115][0m #------------------------ Iteration 744 --------------------------#
[32m[20221213 12:38:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:32 @agent_ppo2.py:179][0m |           0.0913 |          24.1608 |         -76.0007 |
[32m[20221213 12:38:32 @agent_ppo2.py:179][0m |           0.0656 |          22.0017 |         -67.5413 |
[32m[20221213 12:38:32 @agent_ppo2.py:179][0m |           0.0119 |          21.1985 |         -79.2015 |
[32m[20221213 12:38:33 @agent_ppo2.py:179][0m |          -0.0123 |          20.5742 |         -83.6984 |
[32m[20221213 12:38:33 @agent_ppo2.py:179][0m |          -0.0271 |          20.1404 |         -86.3790 |
[32m[20221213 12:38:33 @agent_ppo2.py:179][0m |          -0.0307 |          19.9026 |         -88.5974 |
[32m[20221213 12:38:33 @agent_ppo2.py:179][0m |          -0.0339 |          19.4369 |         -89.5780 |
[32m[20221213 12:38:33 @agent_ppo2.py:179][0m |          -0.0309 |          19.1315 |         -90.4355 |
[32m[20221213 12:38:33 @agent_ppo2.py:179][0m |          -0.0424 |          18.9963 |         -89.2718 |
[32m[20221213 12:38:33 @agent_ppo2.py:179][0m |          -0.0493 |          18.9690 |         -92.5397 |
[32m[20221213 12:38:33 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:38:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 200.00
[32m[20221213 12:38:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 316.00
[32m[20221213 12:38:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 324.18
[32m[20221213 12:38:33 @agent_ppo2.py:137][0m Total time:      22.61 min
[32m[20221213 12:38:33 @agent_ppo2.py:139][0m 1525760 total steps have happened
[32m[20221213 12:38:33 @agent_ppo2.py:115][0m #------------------------ Iteration 745 --------------------------#
[32m[20221213 12:38:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:34 @agent_ppo2.py:179][0m |           0.0629 |          32.9908 |         -76.2326 |
[32m[20221213 12:38:34 @agent_ppo2.py:179][0m |           0.0447 |          31.5573 |         -65.9526 |
[32m[20221213 12:38:34 @agent_ppo2.py:179][0m |           0.0103 |          31.0013 |         -68.3913 |
[32m[20221213 12:38:34 @agent_ppo2.py:179][0m |          -0.0073 |          30.9706 |         -76.8227 |
[32m[20221213 12:38:34 @agent_ppo2.py:179][0m |          -0.0192 |          30.6544 |         -82.8065 |
[32m[20221213 12:38:34 @agent_ppo2.py:179][0m |          -0.0213 |          30.4166 |         -85.9788 |
[32m[20221213 12:38:35 @agent_ppo2.py:179][0m |          -0.0294 |          30.6124 |         -88.0596 |
[32m[20221213 12:38:35 @agent_ppo2.py:179][0m |          -0.0261 |          30.2036 |         -87.3679 |
[32m[20221213 12:38:35 @agent_ppo2.py:179][0m |          -0.0360 |          30.0016 |         -91.7524 |
[32m[20221213 12:38:35 @agent_ppo2.py:179][0m |          -0.0355 |          29.9374 |         -91.4135 |
[32m[20221213 12:38:35 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:38:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 318.04
[32m[20221213 12:38:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.72
[32m[20221213 12:38:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.61
[32m[20221213 12:38:35 @agent_ppo2.py:137][0m Total time:      22.64 min
[32m[20221213 12:38:35 @agent_ppo2.py:139][0m 1527808 total steps have happened
[32m[20221213 12:38:35 @agent_ppo2.py:115][0m #------------------------ Iteration 746 --------------------------#
[32m[20221213 12:38:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |           0.0449 |          28.7298 |         -80.1599 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |           0.0454 |          27.0488 |         -68.3301 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |           0.0087 |          26.2596 |         -73.7834 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |          -0.0122 |          25.5019 |         -81.9058 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |          -0.0209 |          25.0931 |         -86.9848 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |          -0.0210 |          24.8027 |         -87.2409 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |          -0.0287 |          24.4041 |         -91.1887 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |          -0.0266 |          24.2924 |         -92.6197 |
[32m[20221213 12:38:36 @agent_ppo2.py:179][0m |          -0.0255 |          26.3569 |         -93.9467 |
[32m[20221213 12:38:37 @agent_ppo2.py:179][0m |          -0.0393 |          23.8018 |         -97.1790 |
[32m[20221213 12:38:37 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.24
[32m[20221213 12:38:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.26
[32m[20221213 12:38:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 304.37
[32m[20221213 12:38:37 @agent_ppo2.py:137][0m Total time:      22.67 min
[32m[20221213 12:38:37 @agent_ppo2.py:139][0m 1529856 total steps have happened
[32m[20221213 12:38:37 @agent_ppo2.py:115][0m #------------------------ Iteration 747 --------------------------#
[32m[20221213 12:38:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:37 @agent_ppo2.py:179][0m |           0.0619 |          27.4534 |         -77.2466 |
[32m[20221213 12:38:37 @agent_ppo2.py:179][0m |           0.0695 |          25.1319 |         -67.3358 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |           0.0156 |          23.7320 |         -73.0072 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |          -0.0017 |          23.0165 |         -73.6736 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |          -0.0053 |          22.2641 |         -76.5319 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |          -0.0322 |          21.7729 |         -81.6787 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |          -0.0148 |          22.7401 |         -83.6039 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |          -0.0392 |          21.1002 |         -85.7358 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |          -0.0391 |          20.8445 |         -89.9755 |
[32m[20221213 12:38:38 @agent_ppo2.py:179][0m |          -0.0480 |          20.5740 |         -90.9993 |
[32m[20221213 12:38:38 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.70
[32m[20221213 12:38:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.31
[32m[20221213 12:38:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 305.77
[32m[20221213 12:38:39 @agent_ppo2.py:137][0m Total time:      22.70 min
[32m[20221213 12:38:39 @agent_ppo2.py:139][0m 1531904 total steps have happened
[32m[20221213 12:38:39 @agent_ppo2.py:115][0m #------------------------ Iteration 748 --------------------------#
[32m[20221213 12:38:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:39 @agent_ppo2.py:179][0m |           0.0843 |          34.3298 |         -79.5168 |
[32m[20221213 12:38:39 @agent_ppo2.py:179][0m |           0.0720 |          32.7400 |         -61.4666 |
[32m[20221213 12:38:39 @agent_ppo2.py:179][0m |           0.0130 |          31.8996 |         -74.1231 |
[32m[20221213 12:38:39 @agent_ppo2.py:179][0m |          -0.0062 |          31.5980 |         -81.6812 |
[32m[20221213 12:38:39 @agent_ppo2.py:179][0m |          -0.0044 |          35.4522 |         -84.5500 |
[32m[20221213 12:38:40 @agent_ppo2.py:179][0m |          -0.0185 |          30.9656 |         -80.5630 |
[32m[20221213 12:38:40 @agent_ppo2.py:179][0m |          -0.0381 |          30.7007 |         -87.5476 |
[32m[20221213 12:38:40 @agent_ppo2.py:179][0m |          -0.0464 |          30.5149 |         -90.9478 |
[32m[20221213 12:38:40 @agent_ppo2.py:179][0m |          -0.0468 |          30.3516 |         -91.9016 |
[32m[20221213 12:38:40 @agent_ppo2.py:179][0m |          -0.0514 |          30.2317 |         -93.2501 |
[32m[20221213 12:38:40 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:38:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 299.99
[32m[20221213 12:38:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.00
[32m[20221213 12:38:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 305.19
[32m[20221213 12:38:40 @agent_ppo2.py:137][0m Total time:      22.73 min
[32m[20221213 12:38:40 @agent_ppo2.py:139][0m 1533952 total steps have happened
[32m[20221213 12:38:40 @agent_ppo2.py:115][0m #------------------------ Iteration 749 --------------------------#
[32m[20221213 12:38:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:41 @agent_ppo2.py:179][0m |           0.0611 |          17.5271 |         -79.5016 |
[32m[20221213 12:38:41 @agent_ppo2.py:179][0m |           0.0558 |          16.1917 |         -71.6378 |
[32m[20221213 12:38:41 @agent_ppo2.py:179][0m |           0.0574 |          15.6349 |         -71.8091 |
[32m[20221213 12:38:41 @agent_ppo2.py:179][0m |           0.0139 |          15.4779 |         -68.8377 |
[32m[20221213 12:38:41 @agent_ppo2.py:179][0m |          -0.0152 |          15.1409 |         -70.7998 |
[32m[20221213 12:38:41 @agent_ppo2.py:179][0m |          -0.0361 |          14.9180 |         -75.1981 |
[32m[20221213 12:38:41 @agent_ppo2.py:179][0m |          -0.0423 |          14.7938 |         -77.5028 |
[32m[20221213 12:38:42 @agent_ppo2.py:179][0m |          -0.0478 |          14.7314 |         -81.5227 |
[32m[20221213 12:38:42 @agent_ppo2.py:179][0m |          -0.0438 |          15.0574 |         -81.4713 |
[32m[20221213 12:38:42 @agent_ppo2.py:179][0m |          -0.0506 |          14.5616 |         -82.7988 |
[32m[20221213 12:38:42 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:38:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.03
[32m[20221213 12:38:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 296.91
[32m[20221213 12:38:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 315.26
[32m[20221213 12:38:42 @agent_ppo2.py:137][0m Total time:      22.75 min
[32m[20221213 12:38:42 @agent_ppo2.py:139][0m 1536000 total steps have happened
[32m[20221213 12:38:42 @agent_ppo2.py:115][0m #------------------------ Iteration 750 --------------------------#
[32m[20221213 12:38:42 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:38:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:42 @agent_ppo2.py:179][0m |           0.0628 |          30.8246 |         -82.5898 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |           0.0624 |          29.0440 |         -70.1058 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |           0.0092 |          28.2420 |         -80.7269 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |          -0.0062 |          27.8881 |         -84.9614 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |          -0.0264 |          27.2035 |         -90.8549 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |          -0.0362 |          26.8657 |         -93.1186 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |          -0.0316 |          26.5439 |         -93.4814 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |          -0.0429 |          26.2329 |         -97.7400 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |          -0.0439 |          25.9982 |        -100.3019 |
[32m[20221213 12:38:43 @agent_ppo2.py:179][0m |          -0.0508 |          25.9111 |        -102.7237 |
[32m[20221213 12:38:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:38:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 287.96
[32m[20221213 12:38:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 302.29
[32m[20221213 12:38:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 292.81
[32m[20221213 12:38:44 @agent_ppo2.py:137][0m Total time:      22.78 min
[32m[20221213 12:38:44 @agent_ppo2.py:139][0m 1538048 total steps have happened
[32m[20221213 12:38:44 @agent_ppo2.py:115][0m #------------------------ Iteration 751 --------------------------#
[32m[20221213 12:38:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:44 @agent_ppo2.py:179][0m |           0.0530 |          27.9443 |         -87.1695 |
[32m[20221213 12:38:44 @agent_ppo2.py:179][0m |           0.0323 |          24.9689 |         -78.8580 |
[32m[20221213 12:38:44 @agent_ppo2.py:179][0m |           0.0084 |          24.5001 |         -84.3938 |
[32m[20221213 12:38:44 @agent_ppo2.py:179][0m |          -0.0114 |          24.2097 |         -86.2727 |
[32m[20221213 12:38:45 @agent_ppo2.py:179][0m |          -0.0310 |          24.0002 |         -91.8786 |
[32m[20221213 12:38:45 @agent_ppo2.py:179][0m |          -0.0304 |          23.8939 |         -93.0690 |
[32m[20221213 12:38:45 @agent_ppo2.py:179][0m |          -0.0413 |          23.7189 |         -94.9122 |
[32m[20221213 12:38:45 @agent_ppo2.py:179][0m |          -0.0382 |          23.6014 |         -96.4314 |
[32m[20221213 12:38:45 @agent_ppo2.py:179][0m |          -0.0418 |          23.7504 |         -98.6957 |
[32m[20221213 12:38:45 @agent_ppo2.py:179][0m |          -0.0422 |          23.4372 |         -98.4575 |
[32m[20221213 12:38:45 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:38:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 227.21
[32m[20221213 12:38:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.72
[32m[20221213 12:38:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 298.56
[32m[20221213 12:38:45 @agent_ppo2.py:137][0m Total time:      22.81 min
[32m[20221213 12:38:45 @agent_ppo2.py:139][0m 1540096 total steps have happened
[32m[20221213 12:38:45 @agent_ppo2.py:115][0m #------------------------ Iteration 752 --------------------------#
[32m[20221213 12:38:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:46 @agent_ppo2.py:179][0m |           0.0866 |          27.0086 |         -85.7044 |
[32m[20221213 12:38:46 @agent_ppo2.py:179][0m |           0.0374 |          25.9250 |         -78.8153 |
[32m[20221213 12:38:46 @agent_ppo2.py:179][0m |          -0.0050 |          25.3565 |         -86.8911 |
[32m[20221213 12:38:46 @agent_ppo2.py:179][0m |          -0.0194 |          25.1065 |         -90.9937 |
[32m[20221213 12:38:46 @agent_ppo2.py:179][0m |          -0.0162 |          24.7037 |         -89.6655 |
[32m[20221213 12:38:46 @agent_ppo2.py:179][0m |          -0.0344 |          24.6908 |         -94.8999 |
[32m[20221213 12:38:46 @agent_ppo2.py:179][0m |          -0.0285 |          24.3832 |         -94.0893 |
[32m[20221213 12:38:47 @agent_ppo2.py:179][0m |          -0.0408 |          24.2235 |         -97.8494 |
[32m[20221213 12:38:47 @agent_ppo2.py:179][0m |          -0.0465 |          24.0779 |        -100.8667 |
[32m[20221213 12:38:47 @agent_ppo2.py:179][0m |          -0.0403 |          24.0381 |        -101.0957 |
[32m[20221213 12:38:47 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 245.59
[32m[20221213 12:38:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.99
[32m[20221213 12:38:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.71
[32m[20221213 12:38:47 @agent_ppo2.py:137][0m Total time:      22.84 min
[32m[20221213 12:38:47 @agent_ppo2.py:139][0m 1542144 total steps have happened
[32m[20221213 12:38:47 @agent_ppo2.py:115][0m #------------------------ Iteration 753 --------------------------#
[32m[20221213 12:38:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:47 @agent_ppo2.py:179][0m |           0.2327 |          27.2902 |         -83.5490 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |           0.1029 |          25.5621 |         -45.9732 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |           0.0271 |          25.1099 |         -56.4730 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |          -0.0024 |          24.8208 |         -67.5829 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |          -0.0195 |          24.6399 |         -70.5770 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |          -0.0259 |          24.3340 |         -72.6558 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |          -0.0306 |          24.5742 |         -73.6294 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |          -0.0430 |          24.0394 |         -77.0136 |
[32m[20221213 12:38:48 @agent_ppo2.py:179][0m |          -0.0389 |          23.9414 |         -78.2848 |
[32m[20221213 12:38:49 @agent_ppo2.py:179][0m |          -0.0460 |          23.6882 |         -80.0757 |
[32m[20221213 12:38:49 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:38:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.39
[32m[20221213 12:38:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.36
[32m[20221213 12:38:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 308.13
[32m[20221213 12:38:49 @agent_ppo2.py:137][0m Total time:      22.87 min
[32m[20221213 12:38:49 @agent_ppo2.py:139][0m 1544192 total steps have happened
[32m[20221213 12:38:49 @agent_ppo2.py:115][0m #------------------------ Iteration 754 --------------------------#
[32m[20221213 12:38:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:49 @agent_ppo2.py:179][0m |           0.1182 |          25.5559 |         -87.3783 |
[32m[20221213 12:38:49 @agent_ppo2.py:179][0m |           0.0528 |          21.8158 |         -80.4801 |
[32m[20221213 12:38:49 @agent_ppo2.py:179][0m |           0.0092 |          20.7605 |         -88.4895 |
[32m[20221213 12:38:50 @agent_ppo2.py:179][0m |           0.0074 |          20.0765 |         -88.2913 |
[32m[20221213 12:38:50 @agent_ppo2.py:179][0m |          -0.0088 |          19.7037 |         -90.0450 |
[32m[20221213 12:38:50 @agent_ppo2.py:179][0m |          -0.0273 |          19.3031 |         -94.7133 |
[32m[20221213 12:38:50 @agent_ppo2.py:179][0m |          -0.0385 |          19.0537 |         -98.0459 |
[32m[20221213 12:38:50 @agent_ppo2.py:179][0m |          -0.0450 |          18.7377 |         -99.0538 |
[32m[20221213 12:38:50 @agent_ppo2.py:179][0m |          -0.0487 |          18.5695 |        -101.1971 |
[32m[20221213 12:38:50 @agent_ppo2.py:179][0m |          -0.0410 |          18.3893 |        -101.4942 |
[32m[20221213 12:38:50 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.66
[32m[20221213 12:38:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.11
[32m[20221213 12:38:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 257.06
[32m[20221213 12:38:50 @agent_ppo2.py:137][0m Total time:      22.90 min
[32m[20221213 12:38:50 @agent_ppo2.py:139][0m 1546240 total steps have happened
[32m[20221213 12:38:50 @agent_ppo2.py:115][0m #------------------------ Iteration 755 --------------------------#
[32m[20221213 12:38:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:51 @agent_ppo2.py:179][0m |           0.0951 |          28.7734 |         -79.3246 |
[32m[20221213 12:38:51 @agent_ppo2.py:179][0m |           0.0608 |          27.2526 |         -66.9878 |
[32m[20221213 12:38:51 @agent_ppo2.py:179][0m |           0.0044 |          26.5976 |         -85.3876 |
[32m[20221213 12:38:51 @agent_ppo2.py:179][0m |          -0.0153 |          26.2938 |         -90.9180 |
[32m[20221213 12:38:51 @agent_ppo2.py:179][0m |          -0.0230 |          25.8858 |         -92.7862 |
[32m[20221213 12:38:52 @agent_ppo2.py:179][0m |          -0.0266 |          25.7431 |         -95.2334 |
[32m[20221213 12:38:52 @agent_ppo2.py:179][0m |          -0.0276 |          25.5168 |         -93.8082 |
[32m[20221213 12:38:52 @agent_ppo2.py:179][0m |          -0.0366 |          25.3523 |         -95.5148 |
[32m[20221213 12:38:52 @agent_ppo2.py:179][0m |          -0.0397 |          25.2605 |         -97.9317 |
[32m[20221213 12:38:52 @agent_ppo2.py:179][0m |          -0.0423 |          25.0898 |         -99.9090 |
[32m[20221213 12:38:52 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 307.74
[32m[20221213 12:38:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.12
[32m[20221213 12:38:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.44
[32m[20221213 12:38:52 @agent_ppo2.py:137][0m Total time:      22.92 min
[32m[20221213 12:38:52 @agent_ppo2.py:139][0m 1548288 total steps have happened
[32m[20221213 12:38:52 @agent_ppo2.py:115][0m #------------------------ Iteration 756 --------------------------#
[32m[20221213 12:38:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |           0.0734 |          32.8346 |         -82.6863 |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |           0.0393 |          31.2946 |         -81.2824 |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |           0.0130 |          30.4219 |         -84.7605 |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |          -0.0170 |          29.8292 |         -88.2477 |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |          -0.0323 |          29.5375 |         -92.3201 |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |          -0.0317 |          29.1477 |         -93.6802 |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |          -0.0390 |          28.8174 |         -94.7354 |
[32m[20221213 12:38:53 @agent_ppo2.py:179][0m |          -0.0500 |          28.6286 |         -97.2755 |
[32m[20221213 12:38:54 @agent_ppo2.py:179][0m |          -0.0492 |          28.3497 |         -97.8270 |
[32m[20221213 12:38:54 @agent_ppo2.py:179][0m |          -0.0472 |          30.2168 |         -99.3350 |
[32m[20221213 12:38:54 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 274.16
[32m[20221213 12:38:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.89
[32m[20221213 12:38:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.44
[32m[20221213 12:38:54 @agent_ppo2.py:137][0m Total time:      22.95 min
[32m[20221213 12:38:54 @agent_ppo2.py:139][0m 1550336 total steps have happened
[32m[20221213 12:38:54 @agent_ppo2.py:115][0m #------------------------ Iteration 757 --------------------------#
[32m[20221213 12:38:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:54 @agent_ppo2.py:179][0m |           0.0783 |          26.5387 |         -87.4455 |
[32m[20221213 12:38:54 @agent_ppo2.py:179][0m |           0.0402 |          25.3681 |         -79.4450 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |           0.0083 |          24.7347 |         -88.2827 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |          -0.0155 |          24.2678 |         -95.9438 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |          -0.0285 |          23.9400 |         -99.9078 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |          -0.0292 |          23.6098 |         -99.7862 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |          -0.0366 |          23.2875 |        -102.5176 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |          -0.0423 |          22.9950 |        -103.2051 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |          -0.0474 |          22.8851 |        -104.7927 |
[32m[20221213 12:38:55 @agent_ppo2.py:179][0m |          -0.0425 |          22.5924 |        -106.6769 |
[32m[20221213 12:38:55 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:38:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.69
[32m[20221213 12:38:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.27
[32m[20221213 12:38:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.64
[32m[20221213 12:38:56 @agent_ppo2.py:137][0m Total time:      22.98 min
[32m[20221213 12:38:56 @agent_ppo2.py:139][0m 1552384 total steps have happened
[32m[20221213 12:38:56 @agent_ppo2.py:115][0m #------------------------ Iteration 758 --------------------------#
[32m[20221213 12:38:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:56 @agent_ppo2.py:179][0m |           0.0847 |          11.8565 |         -90.4147 |
[32m[20221213 12:38:56 @agent_ppo2.py:179][0m |           0.0509 |          10.1899 |         -68.5916 |
[32m[20221213 12:38:56 @agent_ppo2.py:179][0m |           0.0264 |           9.7453 |         -71.0925 |
[32m[20221213 12:38:56 @agent_ppo2.py:179][0m |          -0.0067 |           9.4835 |         -76.0274 |
[32m[20221213 12:38:56 @agent_ppo2.py:179][0m |          -0.0206 |           9.2417 |         -78.8122 |
[32m[20221213 12:38:57 @agent_ppo2.py:179][0m |          -0.0261 |           9.1411 |         -80.9144 |
[32m[20221213 12:38:57 @agent_ppo2.py:179][0m |          -0.0321 |           8.9436 |         -79.6921 |
[32m[20221213 12:38:57 @agent_ppo2.py:179][0m |          -0.0443 |           8.8181 |         -86.2312 |
[32m[20221213 12:38:57 @agent_ppo2.py:179][0m |          -0.0438 |           8.7204 |         -86.8293 |
[32m[20221213 12:38:57 @agent_ppo2.py:179][0m |          -0.0503 |           8.6551 |         -89.8455 |
[32m[20221213 12:38:57 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:38:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.11
[32m[20221213 12:38:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 300.87
[32m[20221213 12:38:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 52.79
[32m[20221213 12:38:57 @agent_ppo2.py:137][0m Total time:      23.01 min
[32m[20221213 12:38:57 @agent_ppo2.py:139][0m 1554432 total steps have happened
[32m[20221213 12:38:57 @agent_ppo2.py:115][0m #------------------------ Iteration 759 --------------------------#
[32m[20221213 12:38:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:38:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |           0.0728 |           2.6448 |         -94.9067 |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |          -0.0003 |           1.7783 |         -96.5269 |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |          -0.0103 |           1.6419 |         -97.2628 |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |          -0.0165 |           1.5589 |         -98.9772 |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |          -0.0081 |           1.5112 |         -96.9973 |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |          -0.0146 |           1.5023 |        -101.5539 |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |          -0.0176 |           1.4911 |        -102.5461 |
[32m[20221213 12:38:58 @agent_ppo2.py:179][0m |          -0.0178 |           1.4088 |        -101.2893 |
[32m[20221213 12:38:59 @agent_ppo2.py:179][0m |          -0.0160 |           1.3848 |        -101.4592 |
[32m[20221213 12:38:59 @agent_ppo2.py:179][0m |          -0.0214 |           1.3668 |        -104.0345 |
[32m[20221213 12:38:59 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:38:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.10
[32m[20221213 12:38:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 24.27
[32m[20221213 12:38:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.35
[32m[20221213 12:38:59 @agent_ppo2.py:137][0m Total time:      23.04 min
[32m[20221213 12:38:59 @agent_ppo2.py:139][0m 1556480 total steps have happened
[32m[20221213 12:38:59 @agent_ppo2.py:115][0m #------------------------ Iteration 760 --------------------------#
[32m[20221213 12:38:59 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:38:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:38:59 @agent_ppo2.py:179][0m |           0.1218 |          27.8581 |         -98.7479 |
[32m[20221213 12:38:59 @agent_ppo2.py:179][0m |           0.0793 |          25.2556 |         -77.1306 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |           0.0147 |          24.3457 |         -87.6667 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |          -0.0055 |          27.1238 |         -95.4319 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |          -0.0334 |          23.4861 |        -102.8673 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |          -0.0335 |          22.9825 |        -105.9683 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |          -0.0519 |          22.5978 |        -109.3522 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |          -0.0526 |          22.3655 |        -112.2480 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |          -0.0463 |          22.1194 |        -112.6123 |
[32m[20221213 12:39:00 @agent_ppo2.py:179][0m |          -0.0539 |          21.9273 |        -115.8136 |
[32m[20221213 12:39:00 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:39:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.67
[32m[20221213 12:39:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.93
[32m[20221213 12:39:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.95
[32m[20221213 12:39:01 @agent_ppo2.py:137][0m Total time:      23.07 min
[32m[20221213 12:39:01 @agent_ppo2.py:139][0m 1558528 total steps have happened
[32m[20221213 12:39:01 @agent_ppo2.py:115][0m #------------------------ Iteration 761 --------------------------#
[32m[20221213 12:39:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:01 @agent_ppo2.py:179][0m |           0.0847 |          28.4914 |        -102.8864 |
[32m[20221213 12:39:01 @agent_ppo2.py:179][0m |           0.0822 |          26.9641 |         -88.2072 |
[32m[20221213 12:39:01 @agent_ppo2.py:179][0m |           0.0338 |          24.5256 |         -94.1505 |
[32m[20221213 12:39:01 @agent_ppo2.py:179][0m |          -0.0104 |          23.4425 |        -109.3682 |
[32m[20221213 12:39:02 @agent_ppo2.py:179][0m |          -0.0242 |          22.9793 |        -115.7991 |
[32m[20221213 12:39:02 @agent_ppo2.py:179][0m |          -0.0258 |          22.7022 |        -118.8230 |
[32m[20221213 12:39:02 @agent_ppo2.py:179][0m |          -0.0379 |          22.4810 |        -120.6899 |
[32m[20221213 12:39:02 @agent_ppo2.py:179][0m |          -0.0385 |          22.4054 |        -122.5801 |
[32m[20221213 12:39:02 @agent_ppo2.py:179][0m |          -0.0456 |          22.6810 |        -125.4212 |
[32m[20221213 12:39:02 @agent_ppo2.py:179][0m |          -0.0474 |          21.9765 |        -125.6488 |
[32m[20221213 12:39:02 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:39:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.95
[32m[20221213 12:39:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.21
[32m[20221213 12:39:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 248.80
[32m[20221213 12:39:02 @agent_ppo2.py:137][0m Total time:      23.09 min
[32m[20221213 12:39:02 @agent_ppo2.py:139][0m 1560576 total steps have happened
[32m[20221213 12:39:02 @agent_ppo2.py:115][0m #------------------------ Iteration 762 --------------------------#
[32m[20221213 12:39:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:03 @agent_ppo2.py:179][0m |           0.1726 |          27.7072 |         -89.2023 |
[32m[20221213 12:39:03 @agent_ppo2.py:179][0m |           0.1137 |          25.9323 |         -51.6519 |
[32m[20221213 12:39:03 @agent_ppo2.py:179][0m |           0.0489 |          25.1797 |         -73.6111 |
[32m[20221213 12:39:03 @agent_ppo2.py:179][0m |           0.0315 |          28.3421 |         -88.5222 |
[32m[20221213 12:39:03 @agent_ppo2.py:179][0m |          -0.0012 |          24.5894 |         -98.4677 |
[32m[20221213 12:39:03 @agent_ppo2.py:179][0m |          -0.0161 |          24.1949 |        -105.1045 |
[32m[20221213 12:39:03 @agent_ppo2.py:179][0m |          -0.0249 |          24.0523 |        -110.2908 |
[32m[20221213 12:39:04 @agent_ppo2.py:179][0m |          -0.0320 |          23.8415 |        -114.5920 |
[32m[20221213 12:39:04 @agent_ppo2.py:179][0m |          -0.0345 |          23.6167 |        -117.9176 |
[32m[20221213 12:39:04 @agent_ppo2.py:179][0m |          -0.0356 |          23.4407 |        -118.9424 |
[32m[20221213 12:39:04 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:39:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.40
[32m[20221213 12:39:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.83
[32m[20221213 12:39:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.31
[32m[20221213 12:39:04 @agent_ppo2.py:137][0m Total time:      23.12 min
[32m[20221213 12:39:04 @agent_ppo2.py:139][0m 1562624 total steps have happened
[32m[20221213 12:39:04 @agent_ppo2.py:115][0m #------------------------ Iteration 763 --------------------------#
[32m[20221213 12:39:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |           0.0752 |          34.9181 |        -108.0279 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |           0.0683 |          30.7163 |         -82.9463 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |           0.0190 |          30.3815 |         -89.9678 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |          -0.0175 |          29.5077 |        -100.2649 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |          -0.0252 |          28.9988 |        -104.4666 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |          -0.0338 |          28.7945 |        -107.5786 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |          -0.0294 |          31.0195 |        -111.6365 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |          -0.0397 |          28.3229 |        -112.7308 |
[32m[20221213 12:39:05 @agent_ppo2.py:179][0m |          -0.0442 |          27.9644 |        -116.4591 |
[32m[20221213 12:39:06 @agent_ppo2.py:179][0m |          -0.0456 |          28.9930 |        -120.0761 |
[32m[20221213 12:39:06 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:39:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.60
[32m[20221213 12:39:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.37
[32m[20221213 12:39:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 209.15
[32m[20221213 12:39:06 @agent_ppo2.py:137][0m Total time:      23.15 min
[32m[20221213 12:39:06 @agent_ppo2.py:139][0m 1564672 total steps have happened
[32m[20221213 12:39:06 @agent_ppo2.py:115][0m #------------------------ Iteration 764 --------------------------#
[32m[20221213 12:39:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:06 @agent_ppo2.py:179][0m |           0.0913 |          31.8766 |        -106.1873 |
[32m[20221213 12:39:06 @agent_ppo2.py:179][0m |           0.0560 |          30.9013 |         -93.1680 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |           0.0280 |          33.4268 |        -108.5872 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |          -0.0010 |          29.9663 |        -119.7950 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |          -0.0201 |          29.5925 |        -125.2779 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |          -0.0211 |          29.3392 |        -126.3735 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |          -0.0289 |          29.1206 |        -130.1802 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |          -0.0264 |          28.8408 |        -128.6968 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |          -0.0381 |          28.7439 |        -133.2029 |
[32m[20221213 12:39:07 @agent_ppo2.py:179][0m |          -0.0420 |          28.5038 |        -135.4832 |
[32m[20221213 12:39:07 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:39:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.55
[32m[20221213 12:39:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.92
[32m[20221213 12:39:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.73
[32m[20221213 12:39:08 @agent_ppo2.py:137][0m Total time:      23.18 min
[32m[20221213 12:39:08 @agent_ppo2.py:139][0m 1566720 total steps have happened
[32m[20221213 12:39:08 @agent_ppo2.py:115][0m #------------------------ Iteration 765 --------------------------#
[32m[20221213 12:39:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:08 @agent_ppo2.py:179][0m |           0.0843 |          25.3179 |        -102.5921 |
[32m[20221213 12:39:08 @agent_ppo2.py:179][0m |           0.0577 |          22.3885 |         -82.7040 |
[32m[20221213 12:39:08 @agent_ppo2.py:179][0m |           0.0239 |          21.3901 |         -89.8154 |
[32m[20221213 12:39:08 @agent_ppo2.py:179][0m |          -0.0065 |          20.8211 |         -93.7307 |
[32m[20221213 12:39:09 @agent_ppo2.py:179][0m |          -0.0319 |          20.3738 |        -101.5392 |
[32m[20221213 12:39:09 @agent_ppo2.py:179][0m |          -0.0342 |          20.0249 |        -104.8528 |
[32m[20221213 12:39:09 @agent_ppo2.py:179][0m |          -0.0366 |          19.9588 |        -109.4553 |
[32m[20221213 12:39:09 @agent_ppo2.py:179][0m |          -0.0501 |          19.5816 |        -111.6819 |
[32m[20221213 12:39:09 @agent_ppo2.py:179][0m |          -0.0476 |          20.1354 |        -112.5117 |
[32m[20221213 12:39:09 @agent_ppo2.py:179][0m |          -0.0544 |          18.9393 |        -115.2891 |
[32m[20221213 12:39:09 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:39:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 225.36
[32m[20221213 12:39:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.47
[32m[20221213 12:39:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.29
[32m[20221213 12:39:09 @agent_ppo2.py:137][0m Total time:      23.21 min
[32m[20221213 12:39:09 @agent_ppo2.py:139][0m 1568768 total steps have happened
[32m[20221213 12:39:09 @agent_ppo2.py:115][0m #------------------------ Iteration 766 --------------------------#
[32m[20221213 12:39:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:10 @agent_ppo2.py:179][0m |           0.0905 |          33.8680 |        -134.6965 |
[32m[20221213 12:39:10 @agent_ppo2.py:179][0m |           0.1110 |          32.2134 |        -109.7527 |
[32m[20221213 12:39:10 @agent_ppo2.py:179][0m |           0.0237 |          31.2081 |        -129.1908 |
[32m[20221213 12:39:10 @agent_ppo2.py:179][0m |          -0.0050 |          30.8066 |        -139.0652 |
[32m[20221213 12:39:10 @agent_ppo2.py:179][0m |          -0.0242 |          30.3289 |        -143.3018 |
[32m[20221213 12:39:10 @agent_ppo2.py:179][0m |          -0.0321 |          30.0815 |        -149.1376 |
[32m[20221213 12:39:11 @agent_ppo2.py:179][0m |          -0.0329 |          29.8932 |        -152.1485 |
[32m[20221213 12:39:11 @agent_ppo2.py:179][0m |          -0.0395 |          29.6808 |        -152.2438 |
[32m[20221213 12:39:11 @agent_ppo2.py:179][0m |          -0.0447 |          29.3478 |        -158.5079 |
[32m[20221213 12:39:11 @agent_ppo2.py:179][0m |          -0.0511 |          29.2738 |        -162.6839 |
[32m[20221213 12:39:11 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:39:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 287.63
[32m[20221213 12:39:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.23
[32m[20221213 12:39:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.93
[32m[20221213 12:39:11 @agent_ppo2.py:137][0m Total time:      23.24 min
[32m[20221213 12:39:11 @agent_ppo2.py:139][0m 1570816 total steps have happened
[32m[20221213 12:39:11 @agent_ppo2.py:115][0m #------------------------ Iteration 767 --------------------------#
[32m[20221213 12:39:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0863 |           3.1394 |        -111.6614 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0139 |           2.6839 |         -96.1605 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0041 |           2.5878 |         -97.9956 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0137 |           2.6310 |         -97.6927 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0027 |           2.5538 |         -98.0929 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |          -0.0005 |           2.4708 |         -99.0639 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0019 |           2.4196 |         -99.4819 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0004 |           2.4094 |        -102.4048 |
[32m[20221213 12:39:12 @agent_ppo2.py:179][0m |           0.0111 |           2.6495 |        -101.2963 |
[32m[20221213 12:39:13 @agent_ppo2.py:179][0m |          -0.0029 |           2.3718 |        -102.9106 |
[32m[20221213 12:39:13 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:39:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 29.76
[32m[20221213 12:39:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 45.20
[32m[20221213 12:39:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.35
[32m[20221213 12:39:13 @agent_ppo2.py:137][0m Total time:      23.27 min
[32m[20221213 12:39:13 @agent_ppo2.py:139][0m 1572864 total steps have happened
[32m[20221213 12:39:13 @agent_ppo2.py:115][0m #------------------------ Iteration 768 --------------------------#
[32m[20221213 12:39:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:13 @agent_ppo2.py:179][0m |           0.1145 |          28.6450 |        -136.4969 |
[32m[20221213 12:39:13 @agent_ppo2.py:179][0m |           0.0696 |          25.8362 |        -105.4959 |
[32m[20221213 12:39:13 @agent_ppo2.py:179][0m |           0.0145 |          24.5325 |        -133.6595 |
[32m[20221213 12:39:14 @agent_ppo2.py:179][0m |          -0.0063 |          23.9556 |        -145.6787 |
[32m[20221213 12:39:14 @agent_ppo2.py:179][0m |          -0.0117 |          24.1133 |        -149.6584 |
[32m[20221213 12:39:14 @agent_ppo2.py:179][0m |          -0.0234 |          23.4188 |        -158.0615 |
[32m[20221213 12:39:14 @agent_ppo2.py:179][0m |          -0.0227 |          23.0620 |        -160.5272 |
[32m[20221213 12:39:14 @agent_ppo2.py:179][0m |          -0.0319 |          22.4678 |        -162.2672 |
[32m[20221213 12:39:14 @agent_ppo2.py:179][0m |          -0.0364 |          22.1848 |        -167.6449 |
[32m[20221213 12:39:14 @agent_ppo2.py:179][0m |          -0.0410 |          21.9903 |        -168.1752 |
[32m[20221213 12:39:14 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:39:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.98
[32m[20221213 12:39:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.70
[32m[20221213 12:39:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.46
[32m[20221213 12:39:14 @agent_ppo2.py:137][0m Total time:      23.30 min
[32m[20221213 12:39:14 @agent_ppo2.py:139][0m 1574912 total steps have happened
[32m[20221213 12:39:14 @agent_ppo2.py:115][0m #------------------------ Iteration 769 --------------------------#
[32m[20221213 12:39:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:15 @agent_ppo2.py:179][0m |           0.0761 |          28.6634 |        -142.2838 |
[32m[20221213 12:39:15 @agent_ppo2.py:179][0m |           0.0788 |          26.4920 |        -130.8293 |
[32m[20221213 12:39:15 @agent_ppo2.py:179][0m |           0.0134 |          26.8682 |        -135.6636 |
[32m[20221213 12:39:15 @agent_ppo2.py:179][0m |          -0.0030 |          25.7836 |        -137.4218 |
[32m[20221213 12:39:15 @agent_ppo2.py:179][0m |          -0.0275 |          24.6965 |        -144.4604 |
[32m[20221213 12:39:16 @agent_ppo2.py:179][0m |          -0.0366 |          24.2737 |        -145.8254 |
[32m[20221213 12:39:16 @agent_ppo2.py:179][0m |          -0.0445 |          23.9756 |        -153.6338 |
[32m[20221213 12:39:16 @agent_ppo2.py:179][0m |          -0.0458 |          23.7337 |        -153.1871 |
[32m[20221213 12:39:16 @agent_ppo2.py:179][0m |          -0.0463 |          23.3254 |        -155.1435 |
[32m[20221213 12:39:16 @agent_ppo2.py:179][0m |          -0.0369 |          26.6933 |        -157.8893 |
[32m[20221213 12:39:16 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:39:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 210.05
[32m[20221213 12:39:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 311.13
[32m[20221213 12:39:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 321.17
[32m[20221213 12:39:16 @agent_ppo2.py:137][0m Total time:      23.33 min
[32m[20221213 12:39:16 @agent_ppo2.py:139][0m 1576960 total steps have happened
[32m[20221213 12:39:16 @agent_ppo2.py:115][0m #------------------------ Iteration 770 --------------------------#
[32m[20221213 12:39:17 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:39:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:17 @agent_ppo2.py:179][0m |           0.0822 |          31.0973 |        -137.1852 |
[32m[20221213 12:39:17 @agent_ppo2.py:179][0m |           0.0766 |          28.4085 |        -111.4578 |
[32m[20221213 12:39:17 @agent_ppo2.py:179][0m |           0.0153 |          27.3661 |        -128.4427 |
[32m[20221213 12:39:17 @agent_ppo2.py:179][0m |          -0.0185 |          27.0443 |        -137.9422 |
[32m[20221213 12:39:17 @agent_ppo2.py:179][0m |          -0.0177 |          26.4832 |        -141.0741 |
[32m[20221213 12:39:17 @agent_ppo2.py:179][0m |          -0.0326 |          25.9863 |        -145.6222 |
[32m[20221213 12:39:18 @agent_ppo2.py:179][0m |          -0.0338 |          26.2740 |        -147.3786 |
[32m[20221213 12:39:18 @agent_ppo2.py:179][0m |          -0.0400 |          25.4719 |        -144.6630 |
[32m[20221213 12:39:18 @agent_ppo2.py:179][0m |          -0.0404 |          25.2208 |        -147.2702 |
[32m[20221213 12:39:18 @agent_ppo2.py:179][0m |          -0.0411 |          24.9113 |        -147.6456 |
[32m[20221213 12:39:18 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:39:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 262.15
[32m[20221213 12:39:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.40
[32m[20221213 12:39:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 318.33
[32m[20221213 12:39:18 @agent_ppo2.py:137][0m Total time:      23.36 min
[32m[20221213 12:39:18 @agent_ppo2.py:139][0m 1579008 total steps have happened
[32m[20221213 12:39:18 @agent_ppo2.py:115][0m #------------------------ Iteration 771 --------------------------#
[32m[20221213 12:39:18 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:39:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |           0.0863 |          22.6797 |        -129.4642 |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |           0.0651 |          20.4675 |        -117.1115 |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |           0.0120 |          19.5995 |        -119.6352 |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |          -0.0087 |          19.2793 |        -129.8172 |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |          -0.0275 |          18.8914 |        -134.2382 |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |          -0.0370 |          18.3455 |        -138.4450 |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |          -0.0448 |          17.9201 |        -140.8810 |
[32m[20221213 12:39:19 @agent_ppo2.py:179][0m |          -0.0564 |          17.7722 |        -145.5351 |
[32m[20221213 12:39:20 @agent_ppo2.py:179][0m |          -0.0468 |          17.4624 |        -145.0295 |
[32m[20221213 12:39:20 @agent_ppo2.py:179][0m |          -0.0498 |          17.3207 |        -148.7819 |
[32m[20221213 12:39:20 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:39:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 200.22
[32m[20221213 12:39:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 310.90
[32m[20221213 12:39:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.49
[32m[20221213 12:39:20 @agent_ppo2.py:137][0m Total time:      23.39 min
[32m[20221213 12:39:20 @agent_ppo2.py:139][0m 1581056 total steps have happened
[32m[20221213 12:39:20 @agent_ppo2.py:115][0m #------------------------ Iteration 772 --------------------------#
[32m[20221213 12:39:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:20 @agent_ppo2.py:179][0m |           0.0931 |           6.7267 |        -147.5577 |
[32m[20221213 12:39:20 @agent_ppo2.py:179][0m |           0.0222 |           5.3604 |        -141.4535 |
[32m[20221213 12:39:21 @agent_ppo2.py:179][0m |          -0.0120 |           4.8691 |        -142.5985 |
[32m[20221213 12:39:21 @agent_ppo2.py:179][0m |          -0.0256 |           4.5774 |        -146.6883 |
[32m[20221213 12:39:21 @agent_ppo2.py:179][0m |          -0.0360 |           4.3058 |        -147.9788 |
[32m[20221213 12:39:21 @agent_ppo2.py:179][0m |          -0.0404 |           4.2057 |        -149.8638 |
[32m[20221213 12:39:21 @agent_ppo2.py:179][0m |          -0.0367 |           4.0157 |        -152.1019 |
[32m[20221213 12:39:21 @agent_ppo2.py:179][0m |          -0.0496 |           3.9428 |        -153.2239 |
[32m[20221213 12:39:21 @agent_ppo2.py:179][0m |          -0.0573 |           3.8455 |        -156.6457 |
[32m[20221213 12:39:22 @agent_ppo2.py:179][0m |          -0.0489 |           3.7314 |        -160.0324 |
[32m[20221213 12:39:22 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:39:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 58.42
[32m[20221213 12:39:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 185.30
[32m[20221213 12:39:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.88
[32m[20221213 12:39:22 @agent_ppo2.py:137][0m Total time:      23.42 min
[32m[20221213 12:39:22 @agent_ppo2.py:139][0m 1583104 total steps have happened
[32m[20221213 12:39:22 @agent_ppo2.py:115][0m #------------------------ Iteration 773 --------------------------#
[32m[20221213 12:39:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:22 @agent_ppo2.py:179][0m |           0.0767 |          31.7866 |        -134.7637 |
[32m[20221213 12:39:22 @agent_ppo2.py:179][0m |           0.0348 |          29.9483 |        -127.8738 |
[32m[20221213 12:39:22 @agent_ppo2.py:179][0m |          -0.0081 |          29.3850 |        -143.9530 |
[32m[20221213 12:39:23 @agent_ppo2.py:179][0m |          -0.0222 |          28.5474 |        -147.4966 |
[32m[20221213 12:39:23 @agent_ppo2.py:179][0m |          -0.0169 |          30.9572 |        -148.4962 |
[32m[20221213 12:39:23 @agent_ppo2.py:179][0m |          -0.0257 |          29.4822 |        -153.3453 |
[32m[20221213 12:39:23 @agent_ppo2.py:179][0m |          -0.0384 |          27.4157 |        -157.4341 |
[32m[20221213 12:39:23 @agent_ppo2.py:179][0m |          -0.0470 |          27.2648 |        -158.7624 |
[32m[20221213 12:39:23 @agent_ppo2.py:179][0m |          -0.0402 |          27.0080 |        -159.5721 |
[32m[20221213 12:39:23 @agent_ppo2.py:179][0m |          -0.0301 |          30.6508 |        -162.0965 |
[32m[20221213 12:39:23 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:39:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.11
[32m[20221213 12:39:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.76
[32m[20221213 12:39:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.10
[32m[20221213 12:39:24 @agent_ppo2.py:137][0m Total time:      23.45 min
[32m[20221213 12:39:24 @agent_ppo2.py:139][0m 1585152 total steps have happened
[32m[20221213 12:39:24 @agent_ppo2.py:115][0m #------------------------ Iteration 774 --------------------------#
[32m[20221213 12:39:24 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:39:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:24 @agent_ppo2.py:179][0m |           0.0946 |          28.3837 |        -138.9191 |
[32m[20221213 12:39:24 @agent_ppo2.py:179][0m |           0.0562 |          26.5949 |        -118.1337 |
[32m[20221213 12:39:24 @agent_ppo2.py:179][0m |           0.0169 |          25.5331 |        -128.3655 |
[32m[20221213 12:39:24 @agent_ppo2.py:179][0m |          -0.0107 |          24.9359 |        -141.2077 |
[32m[20221213 12:39:25 @agent_ppo2.py:179][0m |          -0.0164 |          25.6542 |        -146.9775 |
[32m[20221213 12:39:25 @agent_ppo2.py:179][0m |          -0.0358 |          23.8208 |        -153.5283 |
[32m[20221213 12:39:25 @agent_ppo2.py:179][0m |          -0.0313 |          24.3730 |        -155.6774 |
[32m[20221213 12:39:25 @agent_ppo2.py:179][0m |          -0.0409 |          23.0373 |        -158.8945 |
[32m[20221213 12:39:25 @agent_ppo2.py:179][0m |          -0.0445 |          22.7821 |        -161.7325 |
[32m[20221213 12:39:25 @agent_ppo2.py:179][0m |          -0.0477 |          22.4253 |        -163.3395 |
[32m[20221213 12:39:25 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:39:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.01
[32m[20221213 12:39:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.62
[32m[20221213 12:39:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.60
[32m[20221213 12:39:25 @agent_ppo2.py:137][0m Total time:      23.48 min
[32m[20221213 12:39:25 @agent_ppo2.py:139][0m 1587200 total steps have happened
[32m[20221213 12:39:25 @agent_ppo2.py:115][0m #------------------------ Iteration 775 --------------------------#
[32m[20221213 12:39:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:26 @agent_ppo2.py:179][0m |           0.1271 |          34.5079 |        -131.5120 |
[32m[20221213 12:39:26 @agent_ppo2.py:179][0m |           0.1362 |          33.3790 |         -78.6120 |
[32m[20221213 12:39:26 @agent_ppo2.py:179][0m |           0.0484 |          32.6573 |         -99.5452 |
[32m[20221213 12:39:26 @agent_ppo2.py:179][0m |           0.0163 |          30.8609 |        -108.1246 |
[32m[20221213 12:39:26 @agent_ppo2.py:179][0m |          -0.0026 |          30.9385 |        -115.4664 |
[32m[20221213 12:39:26 @agent_ppo2.py:179][0m |          -0.0210 |          28.6996 |        -123.4694 |
[32m[20221213 12:39:27 @agent_ppo2.py:179][0m |          -0.0258 |          28.3587 |        -124.5516 |
[32m[20221213 12:39:27 @agent_ppo2.py:179][0m |          -0.0376 |          28.1864 |        -131.1742 |
[32m[20221213 12:39:27 @agent_ppo2.py:179][0m |          -0.0422 |          27.8340 |        -134.7489 |
[32m[20221213 12:39:27 @agent_ppo2.py:179][0m |          -0.0468 |          27.7795 |        -138.1855 |
[32m[20221213 12:39:27 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:39:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 271.59
[32m[20221213 12:39:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.25
[32m[20221213 12:39:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.57
[32m[20221213 12:39:27 @agent_ppo2.py:137][0m Total time:      23.51 min
[32m[20221213 12:39:27 @agent_ppo2.py:139][0m 1589248 total steps have happened
[32m[20221213 12:39:27 @agent_ppo2.py:115][0m #------------------------ Iteration 776 --------------------------#
[32m[20221213 12:39:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:28 @agent_ppo2.py:179][0m |           0.1065 |           8.9827 |        -122.8865 |
[32m[20221213 12:39:28 @agent_ppo2.py:179][0m |           0.1235 |           7.5458 |        -113.7200 |
[32m[20221213 12:39:28 @agent_ppo2.py:179][0m |           0.0113 |           7.1153 |        -137.2315 |
[32m[20221213 12:39:28 @agent_ppo2.py:179][0m |          -0.0023 |           6.9021 |        -138.1832 |
[32m[20221213 12:39:28 @agent_ppo2.py:179][0m |          -0.0126 |           6.6481 |        -143.7309 |
[32m[20221213 12:39:28 @agent_ppo2.py:179][0m |          -0.0008 |           9.1970 |        -147.0952 |
[32m[20221213 12:39:28 @agent_ppo2.py:179][0m |          -0.0200 |           6.4748 |        -150.3109 |
[32m[20221213 12:39:29 @agent_ppo2.py:179][0m |          -0.0280 |           6.2335 |        -150.3660 |
[32m[20221213 12:39:29 @agent_ppo2.py:179][0m |          -0.0335 |           6.1668 |        -153.1707 |
[32m[20221213 12:39:29 @agent_ppo2.py:179][0m |          -0.0360 |           6.1047 |        -155.7113 |
[32m[20221213 12:39:29 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:39:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 70.89
[32m[20221213 12:39:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 285.07
[32m[20221213 12:39:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 92.64
[32m[20221213 12:39:29 @agent_ppo2.py:137][0m Total time:      23.54 min
[32m[20221213 12:39:29 @agent_ppo2.py:139][0m 1591296 total steps have happened
[32m[20221213 12:39:29 @agent_ppo2.py:115][0m #------------------------ Iteration 777 --------------------------#
[32m[20221213 12:39:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:29 @agent_ppo2.py:179][0m |           0.0582 |           9.2704 |        -133.7106 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |           0.0729 |           8.1470 |        -108.2022 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |           0.0242 |           7.5042 |        -110.2959 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |           0.0097 |           7.0723 |        -118.5481 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |          -0.0056 |           6.6919 |        -119.9511 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |          -0.0178 |           6.4282 |        -125.8013 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |           0.0033 |           7.5564 |        -119.4945 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |          -0.0263 |           5.9312 |        -124.4313 |
[32m[20221213 12:39:30 @agent_ppo2.py:179][0m |          -0.0331 |           5.6462 |        -128.6233 |
[32m[20221213 12:39:31 @agent_ppo2.py:179][0m |          -0.0316 |           5.6485 |        -132.2876 |
[32m[20221213 12:39:31 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:39:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 80.12
[32m[20221213 12:39:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 250.32
[32m[20221213 12:39:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.74
[32m[20221213 12:39:31 @agent_ppo2.py:137][0m Total time:      23.57 min
[32m[20221213 12:39:31 @agent_ppo2.py:139][0m 1593344 total steps have happened
[32m[20221213 12:39:31 @agent_ppo2.py:115][0m #------------------------ Iteration 778 --------------------------#
[32m[20221213 12:39:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:31 @agent_ppo2.py:179][0m |           0.0895 |          29.2038 |        -127.4184 |
[32m[20221213 12:39:31 @agent_ppo2.py:179][0m |           0.0322 |          25.7089 |        -123.4534 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0123 |          24.5928 |        -128.1660 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0296 |          23.7041 |        -135.6148 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0475 |          23.8900 |        -137.8959 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0565 |          22.5562 |        -140.3702 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0596 |          22.1761 |        -142.9300 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0628 |          21.9035 |        -145.8720 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0745 |          22.0661 |        -147.1069 |
[32m[20221213 12:39:32 @agent_ppo2.py:179][0m |          -0.0692 |          21.3630 |        -150.4921 |
[32m[20221213 12:39:32 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:39:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 223.55
[32m[20221213 12:39:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.34
[32m[20221213 12:39:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.48
[32m[20221213 12:39:33 @agent_ppo2.py:137][0m Total time:      23.60 min
[32m[20221213 12:39:33 @agent_ppo2.py:139][0m 1595392 total steps have happened
[32m[20221213 12:39:33 @agent_ppo2.py:115][0m #------------------------ Iteration 779 --------------------------#
[32m[20221213 12:39:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:33 @agent_ppo2.py:179][0m |           0.0802 |          29.4513 |        -140.3902 |
[32m[20221213 12:39:33 @agent_ppo2.py:179][0m |           0.0710 |          27.4229 |        -120.3060 |
[32m[20221213 12:39:33 @agent_ppo2.py:179][0m |           0.0250 |          26.5198 |        -134.5667 |
[32m[20221213 12:39:33 @agent_ppo2.py:179][0m |           0.0051 |          30.1718 |        -141.0581 |
[32m[20221213 12:39:34 @agent_ppo2.py:179][0m |          -0.0258 |          25.7445 |        -151.9244 |
[32m[20221213 12:39:34 @agent_ppo2.py:179][0m |          -0.0314 |          25.3763 |        -156.7363 |
[32m[20221213 12:39:34 @agent_ppo2.py:179][0m |          -0.0363 |          24.9790 |        -156.4875 |
[32m[20221213 12:39:34 @agent_ppo2.py:179][0m |          -0.0419 |          24.6390 |        -159.9179 |
[32m[20221213 12:39:34 @agent_ppo2.py:179][0m |          -0.0423 |          24.4294 |        -161.5356 |
[32m[20221213 12:39:34 @agent_ppo2.py:179][0m |          -0.0454 |          24.2633 |        -164.0872 |
[32m[20221213 12:39:34 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:39:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.56
[32m[20221213 12:39:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 327.57
[32m[20221213 12:39:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 268.35
[32m[20221213 12:39:34 @agent_ppo2.py:137][0m Total time:      23.63 min
[32m[20221213 12:39:34 @agent_ppo2.py:139][0m 1597440 total steps have happened
[32m[20221213 12:39:34 @agent_ppo2.py:115][0m #------------------------ Iteration 780 --------------------------#
[32m[20221213 12:39:35 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:39:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:35 @agent_ppo2.py:179][0m |           0.0531 |           1.6715 |        -150.1015 |
[32m[20221213 12:39:35 @agent_ppo2.py:179][0m |           0.0069 |           1.3376 |        -147.7068 |
[32m[20221213 12:39:35 @agent_ppo2.py:179][0m |          -0.0068 |           1.2246 |        -148.0206 |
[32m[20221213 12:39:35 @agent_ppo2.py:179][0m |          -0.0145 |           1.1674 |        -152.3284 |
[32m[20221213 12:39:35 @agent_ppo2.py:179][0m |          -0.0211 |           1.1202 |        -153.8011 |
[32m[20221213 12:39:35 @agent_ppo2.py:179][0m |          -0.0247 |           1.0897 |        -155.8008 |
[32m[20221213 12:39:36 @agent_ppo2.py:179][0m |          -0.0289 |           1.0720 |        -157.8333 |
[32m[20221213 12:39:36 @agent_ppo2.py:179][0m |          -0.0201 |           1.0562 |        -157.1784 |
[32m[20221213 12:39:36 @agent_ppo2.py:179][0m |          -0.0265 |           1.0324 |        -158.5414 |
[32m[20221213 12:39:36 @agent_ppo2.py:179][0m |          -0.0299 |           1.0218 |        -160.6964 |
[32m[20221213 12:39:36 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:39:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.06
[32m[20221213 12:39:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 32.96
[32m[20221213 12:39:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 252.60
[32m[20221213 12:39:36 @agent_ppo2.py:137][0m Total time:      23.66 min
[32m[20221213 12:39:36 @agent_ppo2.py:139][0m 1599488 total steps have happened
[32m[20221213 12:39:36 @agent_ppo2.py:115][0m #------------------------ Iteration 781 --------------------------#
[32m[20221213 12:39:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |           0.0129 |           1.3654 |        -166.4174 |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |          -0.0169 |           0.8212 |        -168.2483 |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |          -0.0181 |           0.7868 |        -166.7749 |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |          -0.0226 |           0.7732 |        -166.6212 |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |          -0.0247 |           0.7628 |        -168.3321 |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |          -0.0237 |           0.7591 |        -169.5330 |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |          -0.0257 |           0.7541 |        -171.2277 |
[32m[20221213 12:39:37 @agent_ppo2.py:179][0m |          -0.0265 |           0.7512 |        -173.6976 |
[32m[20221213 12:39:38 @agent_ppo2.py:179][0m |          -0.0219 |           0.7573 |        -172.4995 |
[32m[20221213 12:39:38 @agent_ppo2.py:179][0m |          -0.0197 |           0.7531 |        -170.0472 |
[32m[20221213 12:39:38 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:39:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 17.51
[32m[20221213 12:39:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 17.75
[32m[20221213 12:39:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 283.69
[32m[20221213 12:39:38 @agent_ppo2.py:137][0m Total time:      23.69 min
[32m[20221213 12:39:38 @agent_ppo2.py:139][0m 1601536 total steps have happened
[32m[20221213 12:39:38 @agent_ppo2.py:115][0m #------------------------ Iteration 782 --------------------------#
[32m[20221213 12:39:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:38 @agent_ppo2.py:179][0m |           0.1323 |          24.3811 |        -127.5979 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |           0.1446 |          21.5318 |         -76.7765 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |           0.0612 |          20.4476 |        -105.6685 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |           0.0283 |          19.8195 |        -120.0716 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |           0.0092 |          19.3657 |        -134.1115 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |          -0.0045 |          19.0300 |        -140.1188 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |          -0.0236 |          18.6966 |        -148.7655 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |          -0.0337 |          18.4517 |        -156.4013 |
[32m[20221213 12:39:39 @agent_ppo2.py:179][0m |          -0.0408 |          18.2676 |        -158.8449 |
[32m[20221213 12:39:40 @agent_ppo2.py:179][0m |          -0.0494 |          18.0459 |        -166.9258 |
[32m[20221213 12:39:40 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:39:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 180.17
[32m[20221213 12:39:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.89
[32m[20221213 12:39:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.70
[32m[20221213 12:39:40 @agent_ppo2.py:137][0m Total time:      23.72 min
[32m[20221213 12:39:40 @agent_ppo2.py:139][0m 1603584 total steps have happened
[32m[20221213 12:39:40 @agent_ppo2.py:115][0m #------------------------ Iteration 783 --------------------------#
[32m[20221213 12:39:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:40 @agent_ppo2.py:179][0m |           0.1293 |          18.7117 |        -152.1444 |
[32m[20221213 12:39:40 @agent_ppo2.py:179][0m |           0.0768 |          16.8315 |        -128.3979 |
[32m[20221213 12:39:40 @agent_ppo2.py:179][0m |           0.0368 |          17.2272 |        -137.3832 |
[32m[20221213 12:39:41 @agent_ppo2.py:179][0m |          -0.0059 |          15.2804 |        -144.4909 |
[32m[20221213 12:39:41 @agent_ppo2.py:179][0m |          -0.0223 |          14.7196 |        -144.1657 |
[32m[20221213 12:39:41 @agent_ppo2.py:179][0m |          -0.0377 |          14.4613 |        -149.0215 |
[32m[20221213 12:39:41 @agent_ppo2.py:179][0m |          -0.0460 |          14.1101 |        -153.5556 |
[32m[20221213 12:39:41 @agent_ppo2.py:179][0m |          -0.0483 |          13.8398 |        -156.7121 |
[32m[20221213 12:39:41 @agent_ppo2.py:179][0m |          -0.0504 |          13.5248 |        -159.1370 |
[32m[20221213 12:39:41 @agent_ppo2.py:179][0m |          -0.0464 |          13.3643 |        -161.4551 |
[32m[20221213 12:39:41 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:39:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 220.78
[32m[20221213 12:39:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 307.40
[32m[20221213 12:39:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 271.96
[32m[20221213 12:39:42 @agent_ppo2.py:137][0m Total time:      23.75 min
[32m[20221213 12:39:42 @agent_ppo2.py:139][0m 1605632 total steps have happened
[32m[20221213 12:39:42 @agent_ppo2.py:115][0m #------------------------ Iteration 784 --------------------------#
[32m[20221213 12:39:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:42 @agent_ppo2.py:179][0m |           0.0762 |          25.5252 |        -146.2870 |
[32m[20221213 12:39:42 @agent_ppo2.py:179][0m |           0.0553 |          25.9884 |        -141.2098 |
[32m[20221213 12:39:42 @agent_ppo2.py:179][0m |           0.0256 |          23.0193 |        -138.1189 |
[32m[20221213 12:39:42 @agent_ppo2.py:179][0m |          -0.0206 |          21.6352 |        -150.7750 |
[32m[20221213 12:39:43 @agent_ppo2.py:179][0m |          -0.0357 |          21.1471 |        -154.1102 |
[32m[20221213 12:39:43 @agent_ppo2.py:179][0m |          -0.0368 |          20.8840 |        -154.8833 |
[32m[20221213 12:39:43 @agent_ppo2.py:179][0m |          -0.0515 |          20.4769 |        -159.6062 |
[32m[20221213 12:39:43 @agent_ppo2.py:179][0m |          -0.0516 |          20.4821 |        -162.1453 |
[32m[20221213 12:39:43 @agent_ppo2.py:179][0m |          -0.0539 |          20.3055 |        -165.1653 |
[32m[20221213 12:39:43 @agent_ppo2.py:179][0m |          -0.0576 |          19.8029 |        -163.7555 |
[32m[20221213 12:39:43 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:39:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 181.42
[32m[20221213 12:39:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 284.68
[32m[20221213 12:39:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.38
[32m[20221213 12:39:43 @agent_ppo2.py:137][0m Total time:      23.78 min
[32m[20221213 12:39:43 @agent_ppo2.py:139][0m 1607680 total steps have happened
[32m[20221213 12:39:43 @agent_ppo2.py:115][0m #------------------------ Iteration 785 --------------------------#
[32m[20221213 12:39:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:44 @agent_ppo2.py:179][0m |           0.0662 |          28.2030 |        -147.3730 |
[32m[20221213 12:39:44 @agent_ppo2.py:179][0m |           0.0336 |          26.6275 |        -148.4734 |
[32m[20221213 12:39:44 @agent_ppo2.py:179][0m |          -0.0017 |          25.7973 |        -153.8973 |
[32m[20221213 12:39:44 @agent_ppo2.py:179][0m |          -0.0102 |          25.2287 |        -153.9762 |
[32m[20221213 12:39:44 @agent_ppo2.py:179][0m |          -0.0285 |          25.5189 |        -161.4509 |
[32m[20221213 12:39:45 @agent_ppo2.py:179][0m |          -0.0413 |          24.2708 |        -167.0617 |
[32m[20221213 12:39:45 @agent_ppo2.py:179][0m |          -0.0442 |          23.9650 |        -167.1073 |
[32m[20221213 12:39:45 @agent_ppo2.py:179][0m |          -0.0477 |          23.7222 |        -169.2767 |
[32m[20221213 12:39:45 @agent_ppo2.py:179][0m |          -0.0530 |          24.3167 |        -171.5746 |
[32m[20221213 12:39:45 @agent_ppo2.py:179][0m |          -0.0491 |          23.3587 |        -169.3031 |
[32m[20221213 12:39:45 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:39:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 268.59
[32m[20221213 12:39:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.45
[32m[20221213 12:39:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.25
[32m[20221213 12:39:45 @agent_ppo2.py:137][0m Total time:      23.81 min
[32m[20221213 12:39:45 @agent_ppo2.py:139][0m 1609728 total steps have happened
[32m[20221213 12:39:45 @agent_ppo2.py:115][0m #------------------------ Iteration 786 --------------------------#
[32m[20221213 12:39:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:46 @agent_ppo2.py:179][0m |           0.0765 |          22.9647 |        -151.1726 |
[32m[20221213 12:39:46 @agent_ppo2.py:179][0m |           0.0314 |          20.2371 |        -141.2950 |
[32m[20221213 12:39:46 @agent_ppo2.py:179][0m |           0.0007 |          20.1933 |        -140.8171 |
[32m[20221213 12:39:46 @agent_ppo2.py:179][0m |          -0.0216 |          18.7062 |        -148.3269 |
[32m[20221213 12:39:46 @agent_ppo2.py:179][0m |          -0.0389 |          18.2641 |        -152.6834 |
[32m[20221213 12:39:46 @agent_ppo2.py:179][0m |          -0.0374 |          18.3782 |        -156.5059 |
[32m[20221213 12:39:47 @agent_ppo2.py:179][0m |          -0.0507 |          17.6010 |        -158.2791 |
[32m[20221213 12:39:47 @agent_ppo2.py:179][0m |          -0.0415 |          17.4154 |        -157.1469 |
[32m[20221213 12:39:47 @agent_ppo2.py:179][0m |          -0.0528 |          17.5229 |        -162.8959 |
[32m[20221213 12:39:47 @agent_ppo2.py:179][0m |          -0.0628 |          16.8645 |        -163.9185 |
[32m[20221213 12:39:47 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:39:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 228.36
[32m[20221213 12:39:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.91
[32m[20221213 12:39:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.30
[32m[20221213 12:39:47 @agent_ppo2.py:137][0m Total time:      23.84 min
[32m[20221213 12:39:47 @agent_ppo2.py:139][0m 1611776 total steps have happened
[32m[20221213 12:39:47 @agent_ppo2.py:115][0m #------------------------ Iteration 787 --------------------------#
[32m[20221213 12:39:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |           0.0853 |          25.4548 |        -143.1134 |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |           0.0492 |          23.4003 |        -130.6411 |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |          -0.0013 |          21.9889 |        -144.8357 |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |          -0.0188 |          21.3968 |        -146.8288 |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |          -0.0422 |          20.7542 |        -152.0014 |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |          -0.0511 |          20.3782 |        -156.3284 |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |          -0.0533 |          20.0670 |        -155.1107 |
[32m[20221213 12:39:48 @agent_ppo2.py:179][0m |          -0.0573 |          19.8423 |        -156.7130 |
[32m[20221213 12:39:49 @agent_ppo2.py:179][0m |          -0.0613 |          19.6299 |        -159.5480 |
[32m[20221213 12:39:49 @agent_ppo2.py:179][0m |          -0.0574 |          19.3564 |        -161.9545 |
[32m[20221213 12:39:49 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:39:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.59
[32m[20221213 12:39:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 288.82
[32m[20221213 12:39:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 284.85
[32m[20221213 12:39:49 @agent_ppo2.py:137][0m Total time:      23.87 min
[32m[20221213 12:39:49 @agent_ppo2.py:139][0m 1613824 total steps have happened
[32m[20221213 12:39:49 @agent_ppo2.py:115][0m #------------------------ Iteration 788 --------------------------#
[32m[20221213 12:39:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:49 @agent_ppo2.py:179][0m |           0.0897 |          22.9954 |        -140.5005 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |           0.0578 |          20.8024 |        -122.8832 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |          -0.0012 |          19.7509 |        -139.5582 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |          -0.0194 |          19.1473 |        -143.2833 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |          -0.0269 |          18.6693 |        -148.3522 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |          -0.0424 |          18.1154 |        -153.7525 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |          -0.0477 |          17.7619 |        -158.0788 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |          -0.0469 |          17.8233 |        -159.6220 |
[32m[20221213 12:39:50 @agent_ppo2.py:179][0m |          -0.0579 |          17.2096 |        -163.6845 |
[32m[20221213 12:39:51 @agent_ppo2.py:179][0m |          -0.0563 |          16.9690 |        -164.5137 |
[32m[20221213 12:39:51 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:39:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 222.99
[32m[20221213 12:39:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 308.59
[32m[20221213 12:39:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.12
[32m[20221213 12:39:51 @agent_ppo2.py:137][0m Total time:      23.90 min
[32m[20221213 12:39:51 @agent_ppo2.py:139][0m 1615872 total steps have happened
[32m[20221213 12:39:51 @agent_ppo2.py:115][0m #------------------------ Iteration 789 --------------------------#
[32m[20221213 12:39:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:51 @agent_ppo2.py:179][0m |           0.1355 |          27.2329 |        -149.8936 |
[32m[20221213 12:39:51 @agent_ppo2.py:179][0m |           0.0648 |          25.4020 |        -124.7623 |
[32m[20221213 12:39:51 @agent_ppo2.py:179][0m |           0.0011 |          24.4877 |        -143.5482 |
[32m[20221213 12:39:52 @agent_ppo2.py:179][0m |          -0.0175 |          23.6976 |        -150.9057 |
[32m[20221213 12:39:52 @agent_ppo2.py:179][0m |          -0.0270 |          23.1957 |        -157.8371 |
[32m[20221213 12:39:52 @agent_ppo2.py:179][0m |          -0.0400 |          22.7907 |        -164.8579 |
[32m[20221213 12:39:52 @agent_ppo2.py:179][0m |          -0.0461 |          22.3038 |        -169.7025 |
[32m[20221213 12:39:52 @agent_ppo2.py:179][0m |          -0.0485 |          21.9151 |        -171.2836 |
[32m[20221213 12:39:52 @agent_ppo2.py:179][0m |          -0.0490 |          21.6815 |        -173.2708 |
[32m[20221213 12:39:52 @agent_ppo2.py:179][0m |          -0.0559 |          21.3264 |        -180.5494 |
[32m[20221213 12:39:52 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:39:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 212.57
[32m[20221213 12:39:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 298.90
[32m[20221213 12:39:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.09
[32m[20221213 12:39:53 @agent_ppo2.py:137][0m Total time:      23.93 min
[32m[20221213 12:39:53 @agent_ppo2.py:139][0m 1617920 total steps have happened
[32m[20221213 12:39:53 @agent_ppo2.py:115][0m #------------------------ Iteration 790 --------------------------#
[32m[20221213 12:39:53 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:39:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:53 @agent_ppo2.py:179][0m |           0.1049 |          24.0078 |        -150.0176 |
[32m[20221213 12:39:53 @agent_ppo2.py:179][0m |           0.0545 |          22.1248 |        -144.0798 |
[32m[20221213 12:39:53 @agent_ppo2.py:179][0m |          -0.0003 |          21.2982 |        -153.7196 |
[32m[20221213 12:39:53 @agent_ppo2.py:179][0m |          -0.0180 |          20.9051 |        -162.6348 |
[32m[20221213 12:39:54 @agent_ppo2.py:179][0m |          -0.0330 |          20.4186 |        -164.8038 |
[32m[20221213 12:39:54 @agent_ppo2.py:179][0m |          -0.0342 |          20.5580 |        -168.1251 |
[32m[20221213 12:39:54 @agent_ppo2.py:179][0m |          -0.0435 |          19.8273 |        -170.3497 |
[32m[20221213 12:39:54 @agent_ppo2.py:179][0m |          -0.0511 |          19.6475 |        -173.1943 |
[32m[20221213 12:39:54 @agent_ppo2.py:179][0m |          -0.0557 |          19.3478 |        -177.5483 |
[32m[20221213 12:39:54 @agent_ppo2.py:179][0m |          -0.0535 |          19.1707 |        -178.3631 |
[32m[20221213 12:39:54 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:39:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.19
[32m[20221213 12:39:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.65
[32m[20221213 12:39:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.24
[32m[20221213 12:39:54 @agent_ppo2.py:137][0m Total time:      23.96 min
[32m[20221213 12:39:54 @agent_ppo2.py:139][0m 1619968 total steps have happened
[32m[20221213 12:39:54 @agent_ppo2.py:115][0m #------------------------ Iteration 791 --------------------------#
[32m[20221213 12:39:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:55 @agent_ppo2.py:179][0m |           0.1168 |          27.2867 |        -161.7349 |
[32m[20221213 12:39:55 @agent_ppo2.py:179][0m |           0.1054 |          25.6363 |        -118.5158 |
[32m[20221213 12:39:55 @agent_ppo2.py:179][0m |           0.0362 |          24.6518 |        -131.8745 |
[32m[20221213 12:39:55 @agent_ppo2.py:179][0m |          -0.0052 |          24.0704 |        -153.2290 |
[32m[20221213 12:39:55 @agent_ppo2.py:179][0m |          -0.0241 |          23.5538 |        -166.1084 |
[32m[20221213 12:39:56 @agent_ppo2.py:179][0m |          -0.0336 |          25.1937 |        -174.5738 |
[32m[20221213 12:39:56 @agent_ppo2.py:179][0m |          -0.0385 |          24.1905 |        -179.5417 |
[32m[20221213 12:39:56 @agent_ppo2.py:179][0m |          -0.0498 |          22.9329 |        -180.2816 |
[32m[20221213 12:39:56 @agent_ppo2.py:179][0m |          -0.0548 |          22.4579 |        -185.9384 |
[32m[20221213 12:39:56 @agent_ppo2.py:179][0m |          -0.0606 |          22.1057 |        -188.6306 |
[32m[20221213 12:39:56 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:39:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 242.52
[32m[20221213 12:39:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 287.78
[32m[20221213 12:39:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 230.76
[32m[20221213 12:39:56 @agent_ppo2.py:137][0m Total time:      23.99 min
[32m[20221213 12:39:56 @agent_ppo2.py:139][0m 1622016 total steps have happened
[32m[20221213 12:39:56 @agent_ppo2.py:115][0m #------------------------ Iteration 792 --------------------------#
[32m[20221213 12:39:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:39:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:57 @agent_ppo2.py:179][0m |           0.0568 |          22.9530 |        -165.1100 |
[32m[20221213 12:39:57 @agent_ppo2.py:179][0m |           0.0248 |          21.6198 |        -153.5959 |
[32m[20221213 12:39:57 @agent_ppo2.py:179][0m |           0.0224 |          23.1401 |        -158.8111 |
[32m[20221213 12:39:57 @agent_ppo2.py:179][0m |          -0.0080 |          20.2578 |        -159.4446 |
[32m[20221213 12:39:57 @agent_ppo2.py:179][0m |          -0.0391 |          19.7706 |        -166.5489 |
[32m[20221213 12:39:57 @agent_ppo2.py:179][0m |          -0.0410 |          20.0449 |        -168.4516 |
[32m[20221213 12:39:57 @agent_ppo2.py:179][0m |          -0.0500 |          18.8737 |        -173.0521 |
[32m[20221213 12:39:58 @agent_ppo2.py:179][0m |          -0.0598 |          18.6279 |        -175.4035 |
[32m[20221213 12:39:58 @agent_ppo2.py:179][0m |          -0.0519 |          18.5020 |        -180.0159 |
[32m[20221213 12:39:58 @agent_ppo2.py:179][0m |          -0.0648 |          18.0619 |        -182.4634 |
[32m[20221213 12:39:58 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:39:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.31
[32m[20221213 12:39:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 291.84
[32m[20221213 12:39:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 67.32
[32m[20221213 12:39:58 @agent_ppo2.py:137][0m Total time:      24.02 min
[32m[20221213 12:39:58 @agent_ppo2.py:139][0m 1624064 total steps have happened
[32m[20221213 12:39:58 @agent_ppo2.py:115][0m #------------------------ Iteration 793 --------------------------#
[32m[20221213 12:39:58 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:39:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:39:58 @agent_ppo2.py:179][0m |           0.0748 |          25.4110 |        -168.9735 |
[32m[20221213 12:39:59 @agent_ppo2.py:179][0m |           0.0306 |          23.0805 |        -156.5832 |
[32m[20221213 12:39:59 @agent_ppo2.py:179][0m |          -0.0119 |          22.0754 |        -170.9585 |
[32m[20221213 12:39:59 @agent_ppo2.py:179][0m |          -0.0272 |          21.6605 |        -170.6887 |
[32m[20221213 12:39:59 @agent_ppo2.py:179][0m |          -0.0419 |          21.0799 |        -176.8469 |
[32m[20221213 12:39:59 @agent_ppo2.py:179][0m |          -0.0436 |          20.7101 |        -180.8195 |
[32m[20221213 12:39:59 @agent_ppo2.py:179][0m |          -0.0450 |          20.3912 |        -183.3009 |
[32m[20221213 12:39:59 @agent_ppo2.py:179][0m |          -0.0504 |          20.2998 |        -187.1569 |
[32m[20221213 12:40:00 @agent_ppo2.py:179][0m |          -0.0548 |          19.9643 |        -190.4388 |
[32m[20221213 12:40:00 @agent_ppo2.py:179][0m |          -0.0611 |          19.7424 |        -191.9699 |
[32m[20221213 12:40:00 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:40:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.22
[32m[20221213 12:40:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.97
[32m[20221213 12:40:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.44
[32m[20221213 12:40:00 @agent_ppo2.py:137][0m Total time:      24.05 min
[32m[20221213 12:40:00 @agent_ppo2.py:139][0m 1626112 total steps have happened
[32m[20221213 12:40:00 @agent_ppo2.py:115][0m #------------------------ Iteration 794 --------------------------#
[32m[20221213 12:40:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:00 @agent_ppo2.py:179][0m |           0.0431 |           0.9746 |        -171.5170 |
[32m[20221213 12:40:00 @agent_ppo2.py:179][0m |           0.0077 |           0.8168 |        -142.9807 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0060 |           0.7652 |        -181.2809 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0104 |           0.7427 |        -197.6122 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0137 |           0.7320 |        -191.2273 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0126 |           0.7263 |        -196.7355 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0168 |           0.7219 |        -196.0171 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0198 |           0.7187 |        -197.1124 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0185 |           0.7164 |        -197.1983 |
[32m[20221213 12:40:01 @agent_ppo2.py:179][0m |          -0.0227 |           0.7133 |        -197.4206 |
[32m[20221213 12:40:01 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:40:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 18.66
[32m[20221213 12:40:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 19.41
[32m[20221213 12:40:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 58.29
[32m[20221213 12:40:02 @agent_ppo2.py:137][0m Total time:      24.08 min
[32m[20221213 12:40:02 @agent_ppo2.py:139][0m 1628160 total steps have happened
[32m[20221213 12:40:02 @agent_ppo2.py:115][0m #------------------------ Iteration 795 --------------------------#
[32m[20221213 12:40:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:02 @agent_ppo2.py:179][0m |           0.0418 |           7.7871 |        -185.0158 |
[32m[20221213 12:40:02 @agent_ppo2.py:179][0m |           0.0211 |           6.8760 |        -179.9202 |
[32m[20221213 12:40:02 @agent_ppo2.py:179][0m |           0.0052 |           6.5959 |        -180.9762 |
[32m[20221213 12:40:03 @agent_ppo2.py:179][0m |          -0.0092 |           6.3188 |        -180.9537 |
[32m[20221213 12:40:03 @agent_ppo2.py:179][0m |          -0.0206 |           6.2023 |        -186.9249 |
[32m[20221213 12:40:03 @agent_ppo2.py:179][0m |          -0.0270 |           6.0659 |        -188.5376 |
[32m[20221213 12:40:03 @agent_ppo2.py:179][0m |          -0.0243 |           5.9295 |        -191.8836 |
[32m[20221213 12:40:03 @agent_ppo2.py:179][0m |          -0.0283 |           5.8389 |        -193.9697 |
[32m[20221213 12:40:03 @agent_ppo2.py:179][0m |          -0.0328 |           5.7541 |        -194.6172 |
[32m[20221213 12:40:03 @agent_ppo2.py:179][0m |          -0.0314 |           5.7196 |        -197.1423 |
[32m[20221213 12:40:03 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:40:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 64.22
[32m[20221213 12:40:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 175.25
[32m[20221213 12:40:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 270.73
[32m[20221213 12:40:03 @agent_ppo2.py:137][0m Total time:      24.11 min
[32m[20221213 12:40:03 @agent_ppo2.py:139][0m 1630208 total steps have happened
[32m[20221213 12:40:03 @agent_ppo2.py:115][0m #------------------------ Iteration 796 --------------------------#
[32m[20221213 12:40:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:04 @agent_ppo2.py:179][0m |           0.0873 |          20.4152 |        -171.8386 |
[32m[20221213 12:40:04 @agent_ppo2.py:179][0m |           0.0924 |          18.2967 |        -125.2864 |
[32m[20221213 12:40:04 @agent_ppo2.py:179][0m |           0.0346 |          17.2485 |        -134.1295 |
[32m[20221213 12:40:04 @agent_ppo2.py:179][0m |          -0.0005 |          16.6139 |        -156.3017 |
[32m[20221213 12:40:04 @agent_ppo2.py:179][0m |          -0.0164 |          16.2192 |        -161.7067 |
[32m[20221213 12:40:05 @agent_ppo2.py:179][0m |          -0.0328 |          15.7502 |        -172.5902 |
[32m[20221213 12:40:05 @agent_ppo2.py:179][0m |          -0.0442 |          15.4699 |        -179.5750 |
[32m[20221213 12:40:05 @agent_ppo2.py:179][0m |          -0.0459 |          15.2152 |        -180.3137 |
[32m[20221213 12:40:05 @agent_ppo2.py:179][0m |          -0.0484 |          15.0616 |        -185.1378 |
[32m[20221213 12:40:05 @agent_ppo2.py:179][0m |          -0.0497 |          14.8061 |        -188.7732 |
[32m[20221213 12:40:05 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:40:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 219.06
[32m[20221213 12:40:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 302.56
[32m[20221213 12:40:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 193.37
[32m[20221213 12:40:05 @agent_ppo2.py:137][0m Total time:      24.14 min
[32m[20221213 12:40:05 @agent_ppo2.py:139][0m 1632256 total steps have happened
[32m[20221213 12:40:05 @agent_ppo2.py:115][0m #------------------------ Iteration 797 --------------------------#
[32m[20221213 12:40:06 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:40:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:06 @agent_ppo2.py:179][0m |           0.0778 |          26.6838 |        -158.8499 |
[32m[20221213 12:40:06 @agent_ppo2.py:179][0m |           0.0552 |          25.0060 |        -144.6589 |
[32m[20221213 12:40:06 @agent_ppo2.py:179][0m |           0.0103 |          24.9819 |        -161.5877 |
[32m[20221213 12:40:06 @agent_ppo2.py:179][0m |          -0.0143 |          23.7116 |        -169.3733 |
[32m[20221213 12:40:06 @agent_ppo2.py:179][0m |          -0.0243 |          23.8949 |        -174.8240 |
[32m[20221213 12:40:06 @agent_ppo2.py:179][0m |          -0.0352 |          23.1370 |        -180.7316 |
[32m[20221213 12:40:07 @agent_ppo2.py:179][0m |          -0.0361 |          23.6000 |        -183.3466 |
[32m[20221213 12:40:07 @agent_ppo2.py:179][0m |          -0.0480 |          22.4922 |        -188.1029 |
[32m[20221213 12:40:07 @agent_ppo2.py:179][0m |          -0.0426 |          23.0138 |        -191.3829 |
[32m[20221213 12:40:07 @agent_ppo2.py:179][0m |          -0.0518 |          22.0779 |        -192.3609 |
[32m[20221213 12:40:07 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:40:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 255.82
[32m[20221213 12:40:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 298.93
[32m[20221213 12:40:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 20.70
[32m[20221213 12:40:07 @agent_ppo2.py:137][0m Total time:      24.17 min
[32m[20221213 12:40:07 @agent_ppo2.py:139][0m 1634304 total steps have happened
[32m[20221213 12:40:07 @agent_ppo2.py:115][0m #------------------------ Iteration 798 --------------------------#
[32m[20221213 12:40:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |           0.0637 |           9.2489 |        -185.5589 |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |           0.0175 |           8.0798 |        -178.2819 |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |          -0.0033 |           7.5945 |        -180.9704 |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |          -0.0126 |           7.2677 |        -187.6060 |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |          -0.0365 |           7.1702 |        -194.2326 |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |          -0.0286 |           6.7620 |        -196.8402 |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |          -0.0348 |           6.5884 |        -201.0881 |
[32m[20221213 12:40:08 @agent_ppo2.py:179][0m |          -0.0398 |           6.4317 |        -202.2740 |
[32m[20221213 12:40:09 @agent_ppo2.py:179][0m |          -0.0351 |           6.3001 |        -204.5327 |
[32m[20221213 12:40:09 @agent_ppo2.py:179][0m |          -0.0415 |           6.1741 |        -206.3947 |
[32m[20221213 12:40:09 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:40:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.26
[32m[20221213 12:40:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.41
[32m[20221213 12:40:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 309.63
[32m[20221213 12:40:09 @agent_ppo2.py:137][0m Total time:      24.20 min
[32m[20221213 12:40:09 @agent_ppo2.py:139][0m 1636352 total steps have happened
[32m[20221213 12:40:09 @agent_ppo2.py:115][0m #------------------------ Iteration 799 --------------------------#
[32m[20221213 12:40:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:09 @agent_ppo2.py:179][0m |           0.1048 |          23.3139 |        -156.6344 |
[32m[20221213 12:40:09 @agent_ppo2.py:179][0m |           0.0964 |          21.2557 |        -124.0267 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |           0.0142 |          19.3866 |        -158.8206 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |          -0.0101 |          19.4977 |        -173.8388 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |          -0.0226 |          18.1967 |        -176.8423 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |          -0.0348 |          17.6852 |        -185.3301 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |          -0.0326 |          18.7703 |        -186.3851 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |          -0.0469 |          17.0448 |        -192.5282 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |          -0.0445 |          16.9311 |        -195.5132 |
[32m[20221213 12:40:10 @agent_ppo2.py:179][0m |          -0.0545 |          16.6586 |        -200.2860 |
[32m[20221213 12:40:10 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:40:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.54
[32m[20221213 12:40:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.60
[32m[20221213 12:40:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.87
[32m[20221213 12:40:11 @agent_ppo2.py:137][0m Total time:      24.23 min
[32m[20221213 12:40:11 @agent_ppo2.py:139][0m 1638400 total steps have happened
[32m[20221213 12:40:11 @agent_ppo2.py:115][0m #------------------------ Iteration 800 --------------------------#
[32m[20221213 12:40:11 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:40:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:11 @agent_ppo2.py:179][0m |           0.0961 |          25.4862 |        -171.4338 |
[32m[20221213 12:40:11 @agent_ppo2.py:179][0m |           0.0794 |          23.3604 |        -133.2019 |
[32m[20221213 12:40:11 @agent_ppo2.py:179][0m |           0.0395 |          22.4997 |        -164.0748 |
[32m[20221213 12:40:12 @agent_ppo2.py:179][0m |          -0.0028 |          21.7679 |        -176.2007 |
[32m[20221213 12:40:12 @agent_ppo2.py:179][0m |          -0.0150 |          21.3262 |        -183.9576 |
[32m[20221213 12:40:12 @agent_ppo2.py:179][0m |          -0.0276 |          20.9431 |        -188.7344 |
[32m[20221213 12:40:12 @agent_ppo2.py:179][0m |          -0.0384 |          20.5288 |        -191.6140 |
[32m[20221213 12:40:12 @agent_ppo2.py:179][0m |          -0.0434 |          20.2439 |        -195.3789 |
[32m[20221213 12:40:12 @agent_ppo2.py:179][0m |          -0.0486 |          19.9521 |        -201.2959 |
[32m[20221213 12:40:12 @agent_ppo2.py:179][0m |          -0.0461 |          19.7483 |        -203.2603 |
[32m[20221213 12:40:12 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:40:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.96
[32m[20221213 12:40:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.16
[32m[20221213 12:40:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.04
[32m[20221213 12:40:13 @agent_ppo2.py:137][0m Total time:      24.26 min
[32m[20221213 12:40:13 @agent_ppo2.py:139][0m 1640448 total steps have happened
[32m[20221213 12:40:13 @agent_ppo2.py:115][0m #------------------------ Iteration 801 --------------------------#
[32m[20221213 12:40:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:13 @agent_ppo2.py:179][0m |           0.0763 |          31.3532 |        -185.9718 |
[32m[20221213 12:40:13 @agent_ppo2.py:179][0m |           0.0469 |          27.5256 |        -168.1202 |
[32m[20221213 12:40:13 @agent_ppo2.py:179][0m |           0.0002 |          25.8788 |        -182.8537 |
[32m[20221213 12:40:13 @agent_ppo2.py:179][0m |          -0.0379 |          24.8155 |        -194.7150 |
[32m[20221213 12:40:13 @agent_ppo2.py:179][0m |          -0.0349 |          24.5912 |        -195.8553 |
[32m[20221213 12:40:14 @agent_ppo2.py:179][0m |          -0.0473 |          23.5742 |        -197.8666 |
[32m[20221213 12:40:14 @agent_ppo2.py:179][0m |          -0.0576 |          22.8254 |        -204.7437 |
[32m[20221213 12:40:14 @agent_ppo2.py:179][0m |          -0.0609 |          22.3131 |        -207.5316 |
[32m[20221213 12:40:14 @agent_ppo2.py:179][0m |          -0.0618 |          21.9542 |        -212.5138 |
[32m[20221213 12:40:14 @agent_ppo2.py:179][0m |          -0.0629 |          21.7723 |        -218.2634 |
[32m[20221213 12:40:14 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:40:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.44
[32m[20221213 12:40:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 280.68
[32m[20221213 12:40:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.82
[32m[20221213 12:40:14 @agent_ppo2.py:137][0m Total time:      24.29 min
[32m[20221213 12:40:14 @agent_ppo2.py:139][0m 1642496 total steps have happened
[32m[20221213 12:40:14 @agent_ppo2.py:115][0m #------------------------ Iteration 802 --------------------------#
[32m[20221213 12:40:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:15 @agent_ppo2.py:179][0m |           0.0911 |          27.8028 |        -169.5686 |
[32m[20221213 12:40:15 @agent_ppo2.py:179][0m |           0.0656 |          25.6513 |        -155.1387 |
[32m[20221213 12:40:15 @agent_ppo2.py:179][0m |           0.0196 |          24.6916 |        -163.1743 |
[32m[20221213 12:40:15 @agent_ppo2.py:179][0m |          -0.0173 |          24.1393 |        -178.4563 |
[32m[20221213 12:40:15 @agent_ppo2.py:179][0m |          -0.0311 |          23.7582 |        -183.3680 |
[32m[20221213 12:40:15 @agent_ppo2.py:179][0m |          -0.0350 |          23.3929 |        -188.8576 |
[32m[20221213 12:40:16 @agent_ppo2.py:179][0m |          -0.0503 |          23.0827 |        -196.1248 |
[32m[20221213 12:40:16 @agent_ppo2.py:179][0m |          -0.0445 |          22.8969 |        -200.8753 |
[32m[20221213 12:40:16 @agent_ppo2.py:179][0m |          -0.0488 |          22.6940 |        -198.3977 |
[32m[20221213 12:40:16 @agent_ppo2.py:179][0m |          -0.0565 |          22.5012 |        -202.1594 |
[32m[20221213 12:40:16 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:40:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 270.08
[32m[20221213 12:40:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.26
[32m[20221213 12:40:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 122.73
[32m[20221213 12:40:16 @agent_ppo2.py:137][0m Total time:      24.32 min
[32m[20221213 12:40:16 @agent_ppo2.py:139][0m 1644544 total steps have happened
[32m[20221213 12:40:16 @agent_ppo2.py:115][0m #------------------------ Iteration 803 --------------------------#
[32m[20221213 12:40:16 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:40:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |           0.0684 |          21.9477 |        -160.2541 |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |           0.0169 |          18.8471 |        -141.7310 |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |          -0.0136 |          18.3350 |        -148.4955 |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |          -0.0228 |          17.9911 |        -155.5169 |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |          -0.0420 |          17.6687 |        -165.4448 |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |          -0.0564 |          17.5018 |        -171.7438 |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |          -0.0604 |          17.2782 |        -174.3113 |
[32m[20221213 12:40:17 @agent_ppo2.py:179][0m |          -0.0639 |          17.0606 |        -178.8265 |
[32m[20221213 12:40:18 @agent_ppo2.py:179][0m |          -0.0618 |          16.9037 |        -182.0262 |
[32m[20221213 12:40:18 @agent_ppo2.py:179][0m |          -0.0489 |          16.7989 |        -158.1089 |
[32m[20221213 12:40:18 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:40:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 197.11
[32m[20221213 12:40:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.95
[32m[20221213 12:40:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.22
[32m[20221213 12:40:18 @agent_ppo2.py:137][0m Total time:      24.35 min
[32m[20221213 12:40:18 @agent_ppo2.py:139][0m 1646592 total steps have happened
[32m[20221213 12:40:18 @agent_ppo2.py:115][0m #------------------------ Iteration 804 --------------------------#
[32m[20221213 12:40:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:18 @agent_ppo2.py:179][0m |           0.0620 |          21.9494 |        -163.5351 |
[32m[20221213 12:40:19 @agent_ppo2.py:179][0m |           0.0541 |          19.0387 |        -144.5038 |
[32m[20221213 12:40:19 @agent_ppo2.py:179][0m |           0.0077 |          18.2454 |        -150.7271 |
[32m[20221213 12:40:19 @agent_ppo2.py:179][0m |          -0.0070 |          17.6855 |        -159.6351 |
[32m[20221213 12:40:19 @agent_ppo2.py:179][0m |          -0.0379 |          17.2460 |        -169.5141 |
[32m[20221213 12:40:19 @agent_ppo2.py:179][0m |          -0.0447 |          17.7099 |        -176.0059 |
[32m[20221213 12:40:19 @agent_ppo2.py:179][0m |          -0.0628 |          16.9189 |        -180.3771 |
[32m[20221213 12:40:19 @agent_ppo2.py:179][0m |          -0.0670 |          16.4903 |        -184.0688 |
[32m[20221213 12:40:20 @agent_ppo2.py:179][0m |          -0.0720 |          16.2163 |        -190.7655 |
[32m[20221213 12:40:20 @agent_ppo2.py:179][0m |          -0.0646 |          16.0295 |        -191.2659 |
[32m[20221213 12:40:20 @agent_ppo2.py:124][0m Policy update time: 1.45 s
[32m[20221213 12:40:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 223.32
[32m[20221213 12:40:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 293.27
[32m[20221213 12:40:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.30
[32m[20221213 12:40:20 @agent_ppo2.py:137][0m Total time:      24.39 min
[32m[20221213 12:40:20 @agent_ppo2.py:139][0m 1648640 total steps have happened
[32m[20221213 12:40:20 @agent_ppo2.py:115][0m #------------------------ Iteration 805 --------------------------#
[32m[20221213 12:40:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:20 @agent_ppo2.py:179][0m |           0.0816 |          16.6115 |        -209.6345 |
[32m[20221213 12:40:20 @agent_ppo2.py:179][0m |           0.0680 |          14.5926 |        -193.3698 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |           0.0392 |          13.7772 |        -199.8832 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |           0.0164 |          16.7270 |        -217.7557 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |          -0.0213 |          13.0422 |        -229.7978 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |          -0.0271 |          12.6979 |        -237.6260 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |          -0.0257 |          14.3032 |        -241.8206 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |          -0.0382 |          12.3839 |        -245.9101 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |          -0.0337 |          12.1145 |        -245.9574 |
[32m[20221213 12:40:21 @agent_ppo2.py:179][0m |          -0.0428 |          12.0198 |        -253.5201 |
[32m[20221213 12:40:21 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:40:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 131.14
[32m[20221213 12:40:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.88
[32m[20221213 12:40:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 207.80
[32m[20221213 12:40:22 @agent_ppo2.py:137][0m Total time:      24.41 min
[32m[20221213 12:40:22 @agent_ppo2.py:139][0m 1650688 total steps have happened
[32m[20221213 12:40:22 @agent_ppo2.py:115][0m #------------------------ Iteration 806 --------------------------#
[32m[20221213 12:40:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:22 @agent_ppo2.py:179][0m |           0.1690 |           1.0285 |         -77.4077 |
[32m[20221213 12:40:22 @agent_ppo2.py:179][0m |           0.0759 |           0.9608 |          -4.6480 |
[32m[20221213 12:40:22 @agent_ppo2.py:179][0m |           0.0676 |           0.9416 |          -5.2413 |
[32m[20221213 12:40:22 @agent_ppo2.py:179][0m |           0.0635 |           0.9337 |          -5.9056 |
[32m[20221213 12:40:23 @agent_ppo2.py:179][0m |           0.0596 |           0.9279 |          -7.3712 |
[32m[20221213 12:40:23 @agent_ppo2.py:179][0m |           0.0532 |           0.9182 |          -7.9529 |
[32m[20221213 12:40:23 @agent_ppo2.py:179][0m |           0.0557 |           0.9125 |          -7.0401 |
[32m[20221213 12:40:23 @agent_ppo2.py:179][0m |           0.0544 |           0.9100 |          -7.7350 |
[32m[20221213 12:40:23 @agent_ppo2.py:179][0m |           0.0526 |           0.9047 |          -8.5376 |
[32m[20221213 12:40:23 @agent_ppo2.py:179][0m |           0.0506 |           0.9040 |          -9.2596 |
[32m[20221213 12:40:23 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:40:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 18.72
[32m[20221213 12:40:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 19.12
[32m[20221213 12:40:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.35
[32m[20221213 12:40:23 @agent_ppo2.py:137][0m Total time:      24.44 min
[32m[20221213 12:40:23 @agent_ppo2.py:139][0m 1652736 total steps have happened
[32m[20221213 12:40:23 @agent_ppo2.py:115][0m #------------------------ Iteration 807 --------------------------#
[32m[20221213 12:40:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:24 @agent_ppo2.py:179][0m |           0.1049 |          21.0961 |        -184.3231 |
[32m[20221213 12:40:24 @agent_ppo2.py:179][0m |           0.1134 |          19.4901 |        -117.3688 |
[32m[20221213 12:40:24 @agent_ppo2.py:179][0m |           0.0440 |          18.7555 |        -154.8380 |
[32m[20221213 12:40:24 @agent_ppo2.py:179][0m |           0.0160 |          18.2561 |        -178.1849 |
[32m[20221213 12:40:24 @agent_ppo2.py:179][0m |          -0.0085 |          17.8983 |        -185.1628 |
[32m[20221213 12:40:24 @agent_ppo2.py:179][0m |          -0.0227 |          17.6421 |        -194.7890 |
[32m[20221213 12:40:25 @agent_ppo2.py:179][0m |          -0.0222 |          17.4483 |        -193.9227 |
[32m[20221213 12:40:25 @agent_ppo2.py:179][0m |          -0.0330 |          17.2954 |        -200.6402 |
[32m[20221213 12:40:25 @agent_ppo2.py:179][0m |          -0.0363 |          17.1591 |        -202.7433 |
[32m[20221213 12:40:25 @agent_ppo2.py:179][0m |          -0.0422 |          16.9546 |        -210.1246 |
[32m[20221213 12:40:25 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:40:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 192.57
[32m[20221213 12:40:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.39
[32m[20221213 12:40:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.91
[32m[20221213 12:40:25 @agent_ppo2.py:137][0m Total time:      24.47 min
[32m[20221213 12:40:25 @agent_ppo2.py:139][0m 1654784 total steps have happened
[32m[20221213 12:40:25 @agent_ppo2.py:115][0m #------------------------ Iteration 808 --------------------------#
[32m[20221213 12:40:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |           0.0920 |          25.3563 |        -184.9267 |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |           0.1515 |          24.0813 |         -97.8212 |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |           0.1086 |          23.1209 |         -90.1881 |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |           0.0713 |          22.5657 |        -114.2368 |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |           0.0364 |          22.2707 |        -130.3065 |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |           0.0213 |          21.9906 |        -151.2339 |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |           0.0034 |          21.7297 |        -162.6926 |
[32m[20221213 12:40:26 @agent_ppo2.py:179][0m |          -0.0041 |          21.7014 |        -179.5553 |
[32m[20221213 12:40:27 @agent_ppo2.py:179][0m |          -0.0196 |          21.3707 |        -185.3427 |
[32m[20221213 12:40:27 @agent_ppo2.py:179][0m |          -0.0196 |          21.4764 |        -196.1508 |
[32m[20221213 12:40:27 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:40:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 253.30
[32m[20221213 12:40:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.98
[32m[20221213 12:40:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.39
[32m[20221213 12:40:27 @agent_ppo2.py:137][0m Total time:      24.50 min
[32m[20221213 12:40:27 @agent_ppo2.py:139][0m 1656832 total steps have happened
[32m[20221213 12:40:27 @agent_ppo2.py:115][0m #------------------------ Iteration 809 --------------------------#
[32m[20221213 12:40:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:27 @agent_ppo2.py:179][0m |           0.0542 |          14.7036 |        -186.0059 |
[32m[20221213 12:40:27 @agent_ppo2.py:179][0m |           0.0167 |          12.6497 |        -158.2375 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |           0.0188 |          11.8883 |        -136.9297 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |          -0.0076 |          11.4663 |        -119.8432 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |          -0.0230 |          11.0631 |        -128.3057 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |          -0.0294 |          10.5727 |        -129.3593 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |          -0.0314 |          10.3100 |        -133.5583 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |          -0.0392 |          10.0901 |        -135.1810 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |          -0.0430 |           9.7825 |        -136.8362 |
[32m[20221213 12:40:28 @agent_ppo2.py:179][0m |          -0.0520 |           9.4647 |        -138.8092 |
[32m[20221213 12:40:28 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:40:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 135.47
[32m[20221213 12:40:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.45
[32m[20221213 12:40:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 49.00
[32m[20221213 12:40:29 @agent_ppo2.py:137][0m Total time:      24.53 min
[32m[20221213 12:40:29 @agent_ppo2.py:139][0m 1658880 total steps have happened
[32m[20221213 12:40:29 @agent_ppo2.py:115][0m #------------------------ Iteration 810 --------------------------#
[32m[20221213 12:40:29 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:40:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:29 @agent_ppo2.py:179][0m |           0.0754 |          25.4006 |        -180.9555 |
[32m[20221213 12:40:29 @agent_ppo2.py:179][0m |           0.0715 |          23.4552 |        -143.4852 |
[32m[20221213 12:40:29 @agent_ppo2.py:179][0m |           0.0168 |          22.6205 |        -161.1362 |
[32m[20221213 12:40:30 @agent_ppo2.py:179][0m |          -0.0066 |          23.1986 |        -179.6070 |
[32m[20221213 12:40:30 @agent_ppo2.py:179][0m |          -0.0254 |          21.8020 |        -185.5750 |
[32m[20221213 12:40:30 @agent_ppo2.py:179][0m |          -0.0343 |          21.3083 |        -188.1494 |
[32m[20221213 12:40:30 @agent_ppo2.py:179][0m |          -0.0426 |          20.9042 |        -194.9073 |
[32m[20221213 12:40:30 @agent_ppo2.py:179][0m |          -0.0442 |          20.6089 |        -199.1304 |
[32m[20221213 12:40:30 @agent_ppo2.py:179][0m |          -0.0447 |          20.3872 |        -201.5509 |
[32m[20221213 12:40:30 @agent_ppo2.py:179][0m |          -0.0496 |          20.2252 |        -204.7062 |
[32m[20221213 12:40:30 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:40:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 247.64
[32m[20221213 12:40:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.55
[32m[20221213 12:40:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.77
[32m[20221213 12:40:30 @agent_ppo2.py:137][0m Total time:      24.56 min
[32m[20221213 12:40:30 @agent_ppo2.py:139][0m 1660928 total steps have happened
[32m[20221213 12:40:30 @agent_ppo2.py:115][0m #------------------------ Iteration 811 --------------------------#
[32m[20221213 12:40:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:31 @agent_ppo2.py:179][0m |           0.0830 |          31.0887 |        -180.5519 |
[32m[20221213 12:40:31 @agent_ppo2.py:179][0m |           0.0789 |          29.4623 |        -141.6921 |
[32m[20221213 12:40:31 @agent_ppo2.py:179][0m |           0.0226 |          28.8562 |        -173.0607 |
[32m[20221213 12:40:31 @agent_ppo2.py:179][0m |          -0.0009 |          30.9743 |        -193.1916 |
[32m[20221213 12:40:31 @agent_ppo2.py:179][0m |          -0.0157 |          28.0675 |        -190.2602 |
[32m[20221213 12:40:32 @agent_ppo2.py:179][0m |          -0.0331 |          27.6509 |        -198.4015 |
[32m[20221213 12:40:32 @agent_ppo2.py:179][0m |          -0.0288 |          30.7866 |        -204.5919 |
[32m[20221213 12:40:32 @agent_ppo2.py:179][0m |          -0.0409 |          27.3032 |        -209.5395 |
[32m[20221213 12:40:32 @agent_ppo2.py:179][0m |          -0.0474 |          26.9129 |        -211.6923 |
[32m[20221213 12:40:32 @agent_ppo2.py:179][0m |          -0.0538 |          26.6702 |        -217.6365 |
[32m[20221213 12:40:32 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:40:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 271.97
[32m[20221213 12:40:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.43
[32m[20221213 12:40:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.79
[32m[20221213 12:40:32 @agent_ppo2.py:137][0m Total time:      24.59 min
[32m[20221213 12:40:32 @agent_ppo2.py:139][0m 1662976 total steps have happened
[32m[20221213 12:40:32 @agent_ppo2.py:115][0m #------------------------ Iteration 812 --------------------------#
[32m[20221213 12:40:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:33 @agent_ppo2.py:179][0m |           0.1449 |          28.1942 |        -171.4617 |
[32m[20221213 12:40:33 @agent_ppo2.py:179][0m |           0.0652 |          26.1953 |        -139.0430 |
[32m[20221213 12:40:33 @agent_ppo2.py:179][0m |           0.0124 |          25.2706 |        -170.8365 |
[32m[20221213 12:40:33 @agent_ppo2.py:179][0m |          -0.0137 |          24.5710 |        -183.3624 |
[32m[20221213 12:40:33 @agent_ppo2.py:179][0m |          -0.0247 |          24.2528 |        -190.8380 |
[32m[20221213 12:40:33 @agent_ppo2.py:179][0m |          -0.0336 |          23.9127 |        -192.1683 |
[32m[20221213 12:40:33 @agent_ppo2.py:179][0m |          -0.0349 |          24.4009 |        -200.0008 |
[32m[20221213 12:40:34 @agent_ppo2.py:179][0m |          -0.0332 |          23.8826 |        -203.0425 |
[32m[20221213 12:40:34 @agent_ppo2.py:179][0m |          -0.0357 |          23.4249 |        -203.1829 |
[32m[20221213 12:40:34 @agent_ppo2.py:179][0m |          -0.0444 |          22.8578 |        -206.3022 |
[32m[20221213 12:40:34 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:40:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 284.88
[32m[20221213 12:40:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 327.66
[32m[20221213 12:40:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.10
[32m[20221213 12:40:34 @agent_ppo2.py:137][0m Total time:      24.62 min
[32m[20221213 12:40:34 @agent_ppo2.py:139][0m 1665024 total steps have happened
[32m[20221213 12:40:34 @agent_ppo2.py:115][0m #------------------------ Iteration 813 --------------------------#
[32m[20221213 12:40:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |           0.1163 |          28.1693 |        -170.9578 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |           0.0870 |          26.7056 |        -150.5181 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |           0.0486 |          27.0939 |        -143.6374 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |           0.0223 |          26.2496 |        -153.1747 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |          -0.0119 |          25.1453 |        -170.9125 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |          -0.0234 |          24.9580 |        -179.0274 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |          -0.0276 |          24.7492 |        -184.7490 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |          -0.0307 |          24.5986 |        -187.0300 |
[32m[20221213 12:40:35 @agent_ppo2.py:179][0m |          -0.0364 |          24.3694 |        -190.5158 |
[32m[20221213 12:40:36 @agent_ppo2.py:179][0m |          -0.0383 |          24.1543 |        -194.4020 |
[32m[20221213 12:40:36 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:40:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.37
[32m[20221213 12:40:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.47
[32m[20221213 12:40:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.24
[32m[20221213 12:40:36 @agent_ppo2.py:137][0m Total time:      24.65 min
[32m[20221213 12:40:36 @agent_ppo2.py:139][0m 1667072 total steps have happened
[32m[20221213 12:40:36 @agent_ppo2.py:115][0m #------------------------ Iteration 814 --------------------------#
[32m[20221213 12:40:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:36 @agent_ppo2.py:179][0m |           0.0709 |          26.0630 |        -172.8661 |
[32m[20221213 12:40:36 @agent_ppo2.py:179][0m |           0.0419 |          24.3004 |        -150.6468 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |           0.0235 |          23.6343 |        -158.7073 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |          -0.0083 |          22.7123 |        -164.7949 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |          -0.0226 |          22.1671 |        -172.9399 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |          -0.0418 |          21.7597 |        -179.2099 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |          -0.0448 |          21.4289 |        -181.4496 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |          -0.0490 |          21.0617 |        -185.9471 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |          -0.0494 |          20.8065 |        -188.3080 |
[32m[20221213 12:40:37 @agent_ppo2.py:179][0m |          -0.0529 |          20.5611 |        -189.4954 |
[32m[20221213 12:40:37 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:40:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.62
[32m[20221213 12:40:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.71
[32m[20221213 12:40:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.39
[32m[20221213 12:40:38 @agent_ppo2.py:137][0m Total time:      24.68 min
[32m[20221213 12:40:38 @agent_ppo2.py:139][0m 1669120 total steps have happened
[32m[20221213 12:40:38 @agent_ppo2.py:115][0m #------------------------ Iteration 815 --------------------------#
[32m[20221213 12:40:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:38 @agent_ppo2.py:179][0m |           0.0787 |          25.5386 |        -157.3223 |
[32m[20221213 12:40:38 @agent_ppo2.py:179][0m |           0.0490 |          23.0938 |        -150.3742 |
[32m[20221213 12:40:38 @agent_ppo2.py:179][0m |           0.0092 |          21.8057 |        -146.2735 |
[32m[20221213 12:40:39 @agent_ppo2.py:179][0m |          -0.0141 |          20.7597 |        -160.9038 |
[32m[20221213 12:40:39 @agent_ppo2.py:179][0m |          -0.0274 |          20.3137 |        -170.8276 |
[32m[20221213 12:40:39 @agent_ppo2.py:179][0m |          -0.0323 |          20.9678 |        -175.5279 |
[32m[20221213 12:40:39 @agent_ppo2.py:179][0m |          -0.0410 |          19.1227 |        -178.2524 |
[32m[20221213 12:40:39 @agent_ppo2.py:179][0m |          -0.0495 |          18.6501 |        -182.4600 |
[32m[20221213 12:40:39 @agent_ppo2.py:179][0m |          -0.0517 |          18.1934 |        -185.4727 |
[32m[20221213 12:40:39 @agent_ppo2.py:179][0m |          -0.0528 |          17.8704 |        -188.0626 |
[32m[20221213 12:40:39 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:40:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 256.73
[32m[20221213 12:40:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.64
[32m[20221213 12:40:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.40
[32m[20221213 12:40:39 @agent_ppo2.py:137][0m Total time:      24.71 min
[32m[20221213 12:40:39 @agent_ppo2.py:139][0m 1671168 total steps have happened
[32m[20221213 12:40:39 @agent_ppo2.py:115][0m #------------------------ Iteration 816 --------------------------#
[32m[20221213 12:40:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:40 @agent_ppo2.py:179][0m |           0.0796 |          30.3861 |        -173.7166 |
[32m[20221213 12:40:40 @agent_ppo2.py:179][0m |           0.0928 |          26.9274 |        -143.4514 |
[32m[20221213 12:40:40 @agent_ppo2.py:179][0m |           0.0028 |          25.8104 |        -166.5809 |
[32m[20221213 12:40:40 @agent_ppo2.py:179][0m |          -0.0216 |          25.0672 |        -177.1023 |
[32m[20221213 12:40:40 @agent_ppo2.py:179][0m |          -0.0354 |          24.5709 |        -183.7428 |
[32m[20221213 12:40:41 @agent_ppo2.py:179][0m |          -0.0403 |          24.1162 |        -187.0937 |
[32m[20221213 12:40:41 @agent_ppo2.py:179][0m |          -0.0493 |          23.7505 |        -187.9404 |
[32m[20221213 12:40:41 @agent_ppo2.py:179][0m |          -0.0522 |          23.5939 |        -194.0303 |
[32m[20221213 12:40:41 @agent_ppo2.py:179][0m |          -0.0468 |          23.2047 |        -194.7177 |
[32m[20221213 12:40:41 @agent_ppo2.py:179][0m |          -0.0492 |          22.7050 |        -195.3205 |
[32m[20221213 12:40:41 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:40:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.10
[32m[20221213 12:40:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.23
[32m[20221213 12:40:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.37
[32m[20221213 12:40:41 @agent_ppo2.py:137][0m Total time:      24.74 min
[32m[20221213 12:40:41 @agent_ppo2.py:139][0m 1673216 total steps have happened
[32m[20221213 12:40:41 @agent_ppo2.py:115][0m #------------------------ Iteration 817 --------------------------#
[32m[20221213 12:40:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:42 @agent_ppo2.py:179][0m |           0.0854 |          37.8552 |        -174.8294 |
[32m[20221213 12:40:42 @agent_ppo2.py:179][0m |           0.0699 |          37.1633 |        -146.1334 |
[32m[20221213 12:40:42 @agent_ppo2.py:179][0m |           0.0199 |          33.7844 |        -152.4171 |
[32m[20221213 12:40:42 @agent_ppo2.py:179][0m |          -0.0060 |          33.0049 |        -168.3435 |
[32m[20221213 12:40:42 @agent_ppo2.py:179][0m |          -0.0292 |          32.3712 |        -175.6645 |
[32m[20221213 12:40:42 @agent_ppo2.py:179][0m |          -0.0386 |          31.8064 |        -179.9590 |
[32m[20221213 12:40:42 @agent_ppo2.py:179][0m |          -0.0418 |          31.9150 |        -183.8646 |
[32m[20221213 12:40:43 @agent_ppo2.py:179][0m |          -0.0441 |          31.6732 |        -187.6959 |
[32m[20221213 12:40:43 @agent_ppo2.py:179][0m |          -0.0526 |          30.7029 |        -190.3838 |
[32m[20221213 12:40:43 @agent_ppo2.py:179][0m |          -0.0536 |          30.4606 |        -193.0462 |
[32m[20221213 12:40:43 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:40:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 287.40
[32m[20221213 12:40:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.65
[32m[20221213 12:40:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 248.33
[32m[20221213 12:40:43 @agent_ppo2.py:137][0m Total time:      24.77 min
[32m[20221213 12:40:43 @agent_ppo2.py:139][0m 1675264 total steps have happened
[32m[20221213 12:40:43 @agent_ppo2.py:115][0m #------------------------ Iteration 818 --------------------------#
[32m[20221213 12:40:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |           0.0743 |          32.9099 |        -152.0464 |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |           0.0455 |          30.7901 |        -135.9942 |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |           0.0051 |          29.8451 |        -153.8067 |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |          -0.0100 |          29.1808 |        -156.7535 |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |          -0.0326 |          28.5749 |        -162.5761 |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |          -0.0387 |          28.1860 |        -164.9612 |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |          -0.0468 |          27.7789 |        -171.2904 |
[32m[20221213 12:40:44 @agent_ppo2.py:179][0m |          -0.0436 |          27.8543 |        -174.8149 |
[32m[20221213 12:40:45 @agent_ppo2.py:179][0m |          -0.0417 |          29.1635 |        -177.9540 |
[32m[20221213 12:40:45 @agent_ppo2.py:179][0m |          -0.0595 |          26.9954 |        -182.4630 |
[32m[20221213 12:40:45 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:40:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.38
[32m[20221213 12:40:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.73
[32m[20221213 12:40:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 286.85
[32m[20221213 12:40:45 @agent_ppo2.py:137][0m Total time:      24.80 min
[32m[20221213 12:40:45 @agent_ppo2.py:139][0m 1677312 total steps have happened
[32m[20221213 12:40:45 @agent_ppo2.py:115][0m #------------------------ Iteration 819 --------------------------#
[32m[20221213 12:40:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:45 @agent_ppo2.py:179][0m |           0.0809 |          12.1607 |        -158.1604 |
[32m[20221213 12:40:45 @agent_ppo2.py:179][0m |           0.1305 |           9.9163 |        -116.5478 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0052 |           8.8154 |        -130.2777 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0322 |           8.1629 |        -139.9432 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0393 |           7.7079 |        -144.1990 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0410 |           7.4146 |        -146.4153 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0450 |           7.2361 |        -151.0419 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0494 |           6.8529 |        -153.0431 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0575 |           6.6219 |        -156.5701 |
[32m[20221213 12:40:46 @agent_ppo2.py:179][0m |          -0.0581 |           6.4897 |        -158.4891 |
[32m[20221213 12:40:46 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:40:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 119.02
[32m[20221213 12:40:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 285.65
[32m[20221213 12:40:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.73
[32m[20221213 12:40:47 @agent_ppo2.py:137][0m Total time:      24.83 min
[32m[20221213 12:40:47 @agent_ppo2.py:139][0m 1679360 total steps have happened
[32m[20221213 12:40:47 @agent_ppo2.py:115][0m #------------------------ Iteration 820 --------------------------#
[32m[20221213 12:40:47 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:40:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:47 @agent_ppo2.py:179][0m |           0.1330 |          34.2562 |        -146.3039 |
[32m[20221213 12:40:47 @agent_ppo2.py:179][0m |           0.1252 |          31.9193 |         -88.1017 |
[32m[20221213 12:40:47 @agent_ppo2.py:179][0m |           0.0537 |          29.8260 |        -123.3337 |
[32m[20221213 12:40:47 @agent_ppo2.py:179][0m |           0.0166 |          29.2424 |        -144.9450 |
[32m[20221213 12:40:48 @agent_ppo2.py:179][0m |           0.0062 |          28.4526 |        -148.3387 |
[32m[20221213 12:40:48 @agent_ppo2.py:179][0m |          -0.0217 |          28.0883 |        -162.2797 |
[32m[20221213 12:40:48 @agent_ppo2.py:179][0m |          -0.0314 |          27.7491 |        -169.4253 |
[32m[20221213 12:40:48 @agent_ppo2.py:179][0m |          -0.0428 |          27.4180 |        -175.4164 |
[32m[20221213 12:40:48 @agent_ppo2.py:179][0m |          -0.0414 |          27.2140 |        -180.5323 |
[32m[20221213 12:40:48 @agent_ppo2.py:179][0m |          -0.0397 |          28.6300 |        -180.3548 |
[32m[20221213 12:40:48 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:40:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 306.88
[32m[20221213 12:40:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.26
[32m[20221213 12:40:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.08
[32m[20221213 12:40:48 @agent_ppo2.py:137][0m Total time:      24.86 min
[32m[20221213 12:40:48 @agent_ppo2.py:139][0m 1681408 total steps have happened
[32m[20221213 12:40:48 @agent_ppo2.py:115][0m #------------------------ Iteration 821 --------------------------#
[32m[20221213 12:40:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:49 @agent_ppo2.py:179][0m |           0.0849 |          29.3867 |        -170.0784 |
[32m[20221213 12:40:49 @agent_ppo2.py:179][0m |           0.0930 |          27.3838 |        -121.3381 |
[32m[20221213 12:40:49 @agent_ppo2.py:179][0m |           0.0307 |          26.4716 |        -130.7763 |
[32m[20221213 12:40:49 @agent_ppo2.py:179][0m |          -0.0034 |          26.0676 |        -141.3668 |
[32m[20221213 12:40:49 @agent_ppo2.py:179][0m |          -0.0259 |          25.3530 |        -147.9179 |
[32m[20221213 12:40:50 @agent_ppo2.py:179][0m |          -0.0350 |          24.9205 |        -155.7494 |
[32m[20221213 12:40:50 @agent_ppo2.py:179][0m |          -0.0436 |          24.5336 |        -157.2014 |
[32m[20221213 12:40:50 @agent_ppo2.py:179][0m |          -0.0499 |          24.2733 |        -159.5666 |
[32m[20221213 12:40:50 @agent_ppo2.py:179][0m |          -0.0480 |          23.9190 |        -159.9480 |
[32m[20221213 12:40:50 @agent_ppo2.py:179][0m |          -0.0478 |          23.8827 |        -165.1515 |
[32m[20221213 12:40:50 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:40:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 197.64
[32m[20221213 12:40:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.89
[32m[20221213 12:40:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.54
[32m[20221213 12:40:50 @agent_ppo2.py:137][0m Total time:      24.89 min
[32m[20221213 12:40:50 @agent_ppo2.py:139][0m 1683456 total steps have happened
[32m[20221213 12:40:50 @agent_ppo2.py:115][0m #------------------------ Iteration 822 --------------------------#
[32m[20221213 12:40:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:51 @agent_ppo2.py:179][0m |           0.0868 |          24.0118 |        -165.8244 |
[32m[20221213 12:40:51 @agent_ppo2.py:179][0m |           0.0772 |          21.2985 |        -151.6512 |
[32m[20221213 12:40:51 @agent_ppo2.py:179][0m |           0.0041 |          20.5155 |        -161.8183 |
[32m[20221213 12:40:51 @agent_ppo2.py:179][0m |           0.0001 |          23.9543 |        -170.1350 |
[32m[20221213 12:40:51 @agent_ppo2.py:179][0m |          -0.0209 |          19.9474 |        -174.0740 |
[32m[20221213 12:40:51 @agent_ppo2.py:179][0m |          -0.0337 |          19.3512 |        -179.9449 |
[32m[20221213 12:40:52 @agent_ppo2.py:179][0m |          -0.0285 |          23.0723 |        -181.1965 |
[32m[20221213 12:40:52 @agent_ppo2.py:179][0m |          -0.0389 |          19.0566 |        -184.8406 |
[32m[20221213 12:40:52 @agent_ppo2.py:179][0m |          -0.0471 |          18.9953 |        -189.7891 |
[32m[20221213 12:40:52 @agent_ppo2.py:179][0m |          -0.0509 |          18.5331 |        -193.6228 |
[32m[20221213 12:40:52 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:40:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.17
[32m[20221213 12:40:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.16
[32m[20221213 12:40:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 219.51
[32m[20221213 12:40:52 @agent_ppo2.py:137][0m Total time:      24.92 min
[32m[20221213 12:40:52 @agent_ppo2.py:139][0m 1685504 total steps have happened
[32m[20221213 12:40:52 @agent_ppo2.py:115][0m #------------------------ Iteration 823 --------------------------#
[32m[20221213 12:40:52 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:40:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |           0.0704 |          29.1829 |        -177.8683 |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |           0.0730 |          27.2434 |        -154.7586 |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |           0.0277 |          29.8150 |        -166.6477 |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |           0.0012 |          25.9422 |        -173.1704 |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |          -0.0232 |          25.5791 |        -181.0590 |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |          -0.0227 |          28.6276 |        -185.0133 |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |          -0.0398 |          25.0196 |        -191.0504 |
[32m[20221213 12:40:53 @agent_ppo2.py:179][0m |          -0.0518 |          24.7061 |        -194.6844 |
[32m[20221213 12:40:54 @agent_ppo2.py:179][0m |          -0.0504 |          24.4491 |        -196.2239 |
[32m[20221213 12:40:54 @agent_ppo2.py:179][0m |          -0.0561 |          24.1857 |        -199.8128 |
[32m[20221213 12:40:54 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:40:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 214.17
[32m[20221213 12:40:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 283.44
[32m[20221213 12:40:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 342.46
[32m[20221213 12:40:54 @agent_ppo2.py:137][0m Total time:      24.95 min
[32m[20221213 12:40:54 @agent_ppo2.py:139][0m 1687552 total steps have happened
[32m[20221213 12:40:54 @agent_ppo2.py:115][0m #------------------------ Iteration 824 --------------------------#
[32m[20221213 12:40:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:54 @agent_ppo2.py:179][0m |           0.0665 |          32.6558 |        -195.5124 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |           0.0232 |          28.9126 |        -179.4536 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |          -0.0090 |          27.9480 |        -190.6464 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |          -0.0280 |          27.2760 |        -195.4624 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |          -0.0420 |          26.6998 |        -199.7242 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |          -0.0367 |          30.2035 |        -202.2072 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |          -0.0377 |          26.1702 |        -200.2259 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |          -0.0454 |          25.7839 |        -200.5943 |
[32m[20221213 12:40:55 @agent_ppo2.py:179][0m |          -0.0558 |          25.4383 |        -208.4181 |
[32m[20221213 12:40:56 @agent_ppo2.py:179][0m |          -0.0558 |          25.2970 |        -211.7837 |
[32m[20221213 12:40:56 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:40:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 248.76
[32m[20221213 12:40:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.38
[32m[20221213 12:40:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.99
[32m[20221213 12:40:56 @agent_ppo2.py:137][0m Total time:      24.98 min
[32m[20221213 12:40:56 @agent_ppo2.py:139][0m 1689600 total steps have happened
[32m[20221213 12:40:56 @agent_ppo2.py:115][0m #------------------------ Iteration 825 --------------------------#
[32m[20221213 12:40:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:56 @agent_ppo2.py:179][0m |           0.0639 |          30.4956 |        -177.8405 |
[32m[20221213 12:40:56 @agent_ppo2.py:179][0m |           0.0251 |          28.0181 |        -181.4893 |
[32m[20221213 12:40:56 @agent_ppo2.py:179][0m |          -0.0034 |          27.0508 |        -193.8831 |
[32m[20221213 12:40:57 @agent_ppo2.py:179][0m |          -0.0159 |          26.1744 |        -193.3631 |
[32m[20221213 12:40:57 @agent_ppo2.py:179][0m |          -0.0331 |          25.7026 |        -209.3849 |
[32m[20221213 12:40:57 @agent_ppo2.py:179][0m |          -0.0326 |          25.4097 |        -209.3677 |
[32m[20221213 12:40:57 @agent_ppo2.py:179][0m |          -0.0398 |          24.9406 |        -214.4430 |
[32m[20221213 12:40:57 @agent_ppo2.py:179][0m |          -0.0367 |          24.7225 |        -214.3071 |
[32m[20221213 12:40:57 @agent_ppo2.py:179][0m |          -0.0492 |          24.6030 |        -217.4817 |
[32m[20221213 12:40:57 @agent_ppo2.py:179][0m |          -0.0476 |          24.3557 |        -220.7567 |
[32m[20221213 12:40:57 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:40:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.13
[32m[20221213 12:40:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.24
[32m[20221213 12:40:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.01
[32m[20221213 12:40:58 @agent_ppo2.py:137][0m Total time:      25.01 min
[32m[20221213 12:40:58 @agent_ppo2.py:139][0m 1691648 total steps have happened
[32m[20221213 12:40:58 @agent_ppo2.py:115][0m #------------------------ Iteration 826 --------------------------#
[32m[20221213 12:40:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:40:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:40:58 @agent_ppo2.py:179][0m |           0.0684 |          25.1732 |        -196.9991 |
[32m[20221213 12:40:58 @agent_ppo2.py:179][0m |           0.0438 |          23.0494 |        -183.8527 |
[32m[20221213 12:40:58 @agent_ppo2.py:179][0m |           0.0082 |          22.0767 |        -186.6910 |
[32m[20221213 12:40:58 @agent_ppo2.py:179][0m |          -0.0223 |          21.4092 |        -196.9344 |
[32m[20221213 12:40:59 @agent_ppo2.py:179][0m |          -0.0404 |          20.8029 |        -206.2460 |
[32m[20221213 12:40:59 @agent_ppo2.py:179][0m |          -0.0431 |          20.3685 |        -210.0102 |
[32m[20221213 12:40:59 @agent_ppo2.py:179][0m |          -0.0430 |          20.4619 |        -212.7104 |
[32m[20221213 12:40:59 @agent_ppo2.py:179][0m |          -0.0532 |          19.8525 |        -214.6345 |
[32m[20221213 12:40:59 @agent_ppo2.py:179][0m |          -0.0538 |          19.5794 |        -215.6632 |
[32m[20221213 12:40:59 @agent_ppo2.py:179][0m |          -0.0455 |          21.1289 |        -219.7880 |
[32m[20221213 12:40:59 @agent_ppo2.py:124][0m Policy update time: 1.46 s
[32m[20221213 12:40:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.05
[32m[20221213 12:40:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.23
[32m[20221213 12:40:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 171.07
[32m[20221213 12:40:59 @agent_ppo2.py:137][0m Total time:      25.05 min
[32m[20221213 12:40:59 @agent_ppo2.py:139][0m 1693696 total steps have happened
[32m[20221213 12:40:59 @agent_ppo2.py:115][0m #------------------------ Iteration 827 --------------------------#
[32m[20221213 12:41:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:00 @agent_ppo2.py:179][0m |           0.0984 |          31.4504 |        -187.2219 |
[32m[20221213 12:41:00 @agent_ppo2.py:179][0m |           0.0877 |          29.2284 |        -142.2160 |
[32m[20221213 12:41:00 @agent_ppo2.py:179][0m |           0.0627 |          28.4781 |        -150.6445 |
[32m[20221213 12:41:00 @agent_ppo2.py:179][0m |           0.0074 |          28.3412 |        -173.8571 |
[32m[20221213 12:41:00 @agent_ppo2.py:179][0m |          -0.0094 |          27.0702 |        -180.9587 |
[32m[20221213 12:41:01 @agent_ppo2.py:179][0m |          -0.0242 |          26.5643 |        -191.3316 |
[32m[20221213 12:41:01 @agent_ppo2.py:179][0m |          -0.0239 |          26.9082 |        -198.2916 |
[32m[20221213 12:41:01 @agent_ppo2.py:179][0m |          -0.0283 |          26.0754 |        -200.4315 |
[32m[20221213 12:41:01 @agent_ppo2.py:179][0m |          -0.0313 |          25.6748 |        -202.0202 |
[32m[20221213 12:41:01 @agent_ppo2.py:179][0m |          -0.0412 |          25.4005 |        -211.5358 |
[32m[20221213 12:41:01 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:41:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.24
[32m[20221213 12:41:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 319.27
[32m[20221213 12:41:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.22
[32m[20221213 12:41:01 @agent_ppo2.py:137][0m Total time:      25.08 min
[32m[20221213 12:41:01 @agent_ppo2.py:139][0m 1695744 total steps have happened
[32m[20221213 12:41:01 @agent_ppo2.py:115][0m #------------------------ Iteration 828 --------------------------#
[32m[20221213 12:41:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:02 @agent_ppo2.py:179][0m |           0.0864 |          38.0787 |        -181.4287 |
[32m[20221213 12:41:02 @agent_ppo2.py:179][0m |           0.0635 |          34.8510 |        -152.1583 |
[32m[20221213 12:41:02 @agent_ppo2.py:179][0m |           0.0236 |          33.5169 |        -172.8516 |
[32m[20221213 12:41:02 @agent_ppo2.py:179][0m |          -0.0136 |          33.2621 |        -188.1231 |
[32m[20221213 12:41:02 @agent_ppo2.py:179][0m |          -0.0230 |          32.2221 |        -192.9945 |
[32m[20221213 12:41:02 @agent_ppo2.py:179][0m |          -0.0308 |          31.5054 |        -191.6968 |
[32m[20221213 12:41:02 @agent_ppo2.py:179][0m |          -0.0400 |          31.3202 |        -203.7094 |
[32m[20221213 12:41:03 @agent_ppo2.py:179][0m |          -0.0485 |          31.2156 |        -207.4606 |
[32m[20221213 12:41:03 @agent_ppo2.py:179][0m |          -0.0509 |          30.6666 |        -214.3506 |
[32m[20221213 12:41:03 @agent_ppo2.py:179][0m |          -0.0505 |          30.8105 |        -212.5896 |
[32m[20221213 12:41:03 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:41:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.94
[32m[20221213 12:41:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.23
[32m[20221213 12:41:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.19
[32m[20221213 12:41:03 @agent_ppo2.py:137][0m Total time:      25.11 min
[32m[20221213 12:41:03 @agent_ppo2.py:139][0m 1697792 total steps have happened
[32m[20221213 12:41:03 @agent_ppo2.py:115][0m #------------------------ Iteration 829 --------------------------#
[32m[20221213 12:41:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:03 @agent_ppo2.py:179][0m |           0.1054 |          14.7336 |         -93.7166 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |           0.0281 |          10.2358 |         -35.5984 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |           0.0059 |           9.6039 |         -41.2619 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |          -0.0107 |           9.2784 |         -48.5847 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |          -0.0068 |          10.2189 |         -48.1132 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |          -0.0066 |           8.9266 |         -45.0972 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |          -0.0137 |           8.7336 |         -52.2408 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |          -0.0185 |           8.6200 |         -52.9637 |
[32m[20221213 12:41:04 @agent_ppo2.py:179][0m |          -0.0249 |           8.4706 |         -56.9081 |
[32m[20221213 12:41:05 @agent_ppo2.py:179][0m |          -0.0261 |           8.5321 |         -58.3118 |
[32m[20221213 12:41:05 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:41:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 75.17
[32m[20221213 12:41:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 296.08
[32m[20221213 12:41:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.62
[32m[20221213 12:41:05 @agent_ppo2.py:137][0m Total time:      25.13 min
[32m[20221213 12:41:05 @agent_ppo2.py:139][0m 1699840 total steps have happened
[32m[20221213 12:41:05 @agent_ppo2.py:115][0m #------------------------ Iteration 830 --------------------------#
[32m[20221213 12:41:05 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:41:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:05 @agent_ppo2.py:179][0m |           0.0864 |          10.8027 |        -163.6755 |
[32m[20221213 12:41:05 @agent_ppo2.py:179][0m |           0.1104 |           9.4306 |         -35.5650 |
[32m[20221213 12:41:05 @agent_ppo2.py:179][0m |           0.0667 |           9.0148 |         -42.6752 |
[32m[20221213 12:41:06 @agent_ppo2.py:179][0m |           0.0407 |           8.5793 |         -51.0654 |
[32m[20221213 12:41:06 @agent_ppo2.py:179][0m |           0.0272 |           8.2897 |         -55.9358 |
[32m[20221213 12:41:06 @agent_ppo2.py:179][0m |           0.0146 |           8.1003 |         -59.3433 |
[32m[20221213 12:41:06 @agent_ppo2.py:179][0m |           0.0076 |           7.9550 |         -64.5684 |
[32m[20221213 12:41:06 @agent_ppo2.py:179][0m |           0.0039 |           7.8221 |         -68.2503 |
[32m[20221213 12:41:06 @agent_ppo2.py:179][0m |          -0.0008 |           7.7259 |         -71.2665 |
[32m[20221213 12:41:06 @agent_ppo2.py:179][0m |          -0.0087 |           7.6149 |         -71.8639 |
[32m[20221213 12:41:06 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:41:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.81
[32m[20221213 12:41:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.17
[32m[20221213 12:41:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 192.04
[32m[20221213 12:41:06 @agent_ppo2.py:137][0m Total time:      25.16 min
[32m[20221213 12:41:06 @agent_ppo2.py:139][0m 1701888 total steps have happened
[32m[20221213 12:41:06 @agent_ppo2.py:115][0m #------------------------ Iteration 831 --------------------------#
[32m[20221213 12:41:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:07 @agent_ppo2.py:179][0m |           0.1280 |          23.9134 |        -177.5515 |
[32m[20221213 12:41:07 @agent_ppo2.py:179][0m |           0.1761 |          19.6704 |         -89.0181 |
[32m[20221213 12:41:07 @agent_ppo2.py:179][0m |           0.0764 |          18.8690 |         -63.5787 |
[32m[20221213 12:41:07 @agent_ppo2.py:179][0m |           0.0429 |          18.1847 |         -86.9847 |
[32m[20221213 12:41:07 @agent_ppo2.py:179][0m |           0.0119 |          17.8683 |         -98.7525 |
[32m[20221213 12:41:07 @agent_ppo2.py:179][0m |          -0.0107 |          17.4114 |        -117.3885 |
[32m[20221213 12:41:08 @agent_ppo2.py:179][0m |          -0.0214 |          17.1108 |        -123.9385 |
[32m[20221213 12:41:08 @agent_ppo2.py:179][0m |          -0.0352 |          16.8820 |        -133.2189 |
[32m[20221213 12:41:08 @agent_ppo2.py:179][0m |          -0.0304 |          16.6866 |        -134.5191 |
[32m[20221213 12:41:08 @agent_ppo2.py:179][0m |          -0.0260 |          16.4966 |        -134.9906 |
[32m[20221213 12:41:08 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 193.40
[32m[20221213 12:41:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 316.89
[32m[20221213 12:41:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 280.88
[32m[20221213 12:41:08 @agent_ppo2.py:137][0m Total time:      25.19 min
[32m[20221213 12:41:08 @agent_ppo2.py:139][0m 1703936 total steps have happened
[32m[20221213 12:41:08 @agent_ppo2.py:115][0m #------------------------ Iteration 832 --------------------------#
[32m[20221213 12:41:08 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:41:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.1337 |           1.9440 |        -200.5962 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0788 |           1.6777 |        -173.0373 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0404 |           1.6862 |        -210.7514 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0383 |           1.6452 |        -209.0620 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0297 |           1.6404 |        -210.3787 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0140 |           1.6428 |        -210.9471 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0083 |           1.6330 |        -223.4526 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0235 |           1.6325 |        -210.8358 |
[32m[20221213 12:41:09 @agent_ppo2.py:179][0m |           0.0065 |           1.6345 |        -218.6325 |
[32m[20221213 12:41:10 @agent_ppo2.py:179][0m |           0.0087 |           1.6260 |        -217.4492 |
[32m[20221213 12:41:10 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:41:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 18.20
[32m[20221213 12:41:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 18.23
[32m[20221213 12:41:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 274.85
[32m[20221213 12:41:10 @agent_ppo2.py:137][0m Total time:      25.22 min
[32m[20221213 12:41:10 @agent_ppo2.py:139][0m 1705984 total steps have happened
[32m[20221213 12:41:10 @agent_ppo2.py:115][0m #------------------------ Iteration 833 --------------------------#
[32m[20221213 12:41:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:10 @agent_ppo2.py:179][0m |           0.1346 |           4.3441 |        -121.2808 |
[32m[20221213 12:41:10 @agent_ppo2.py:179][0m |           0.0961 |           3.3418 |         -94.9271 |
[32m[20221213 12:41:10 @agent_ppo2.py:179][0m |           0.0433 |           3.1915 |         -96.6935 |
[32m[20221213 12:41:11 @agent_ppo2.py:179][0m |           0.0351 |           3.1504 |         -98.3546 |
[32m[20221213 12:41:11 @agent_ppo2.py:179][0m |           0.0325 |           3.0876 |        -100.1036 |
[32m[20221213 12:41:11 @agent_ppo2.py:179][0m |           0.0247 |           3.0438 |         -99.0271 |
[32m[20221213 12:41:11 @agent_ppo2.py:179][0m |           0.0277 |           3.0524 |        -100.7235 |
[32m[20221213 12:41:11 @agent_ppo2.py:179][0m |           0.0189 |           3.0086 |        -100.3442 |
[32m[20221213 12:41:11 @agent_ppo2.py:179][0m |           0.0305 |           2.9812 |        -100.9710 |
[32m[20221213 12:41:11 @agent_ppo2.py:179][0m |           0.0217 |           3.0011 |        -102.0028 |
[32m[20221213 12:41:11 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:41:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 33.22
[32m[20221213 12:41:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 43.74
[32m[20221213 12:41:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.62
[32m[20221213 12:41:11 @agent_ppo2.py:137][0m Total time:      25.24 min
[32m[20221213 12:41:11 @agent_ppo2.py:139][0m 1708032 total steps have happened
[32m[20221213 12:41:11 @agent_ppo2.py:115][0m #------------------------ Iteration 834 --------------------------#
[32m[20221213 12:41:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:12 @agent_ppo2.py:179][0m |           0.0758 |          27.7531 |        -193.4305 |
[32m[20221213 12:41:12 @agent_ppo2.py:179][0m |           0.0531 |          25.5164 |        -167.2534 |
[32m[20221213 12:41:12 @agent_ppo2.py:179][0m |           0.0017 |          24.4381 |        -204.1800 |
[32m[20221213 12:41:12 @agent_ppo2.py:179][0m |          -0.0166 |          23.8414 |        -213.2426 |
[32m[20221213 12:41:12 @agent_ppo2.py:179][0m |          -0.0351 |          23.4308 |        -223.3606 |
[32m[20221213 12:41:12 @agent_ppo2.py:179][0m |          -0.0341 |          23.2707 |        -226.8160 |
[32m[20221213 12:41:13 @agent_ppo2.py:179][0m |          -0.0374 |          23.4802 |        -231.2558 |
[32m[20221213 12:41:13 @agent_ppo2.py:179][0m |          -0.0443 |          22.7621 |        -235.6935 |
[32m[20221213 12:41:13 @agent_ppo2.py:179][0m |          -0.0368 |          24.3280 |        -237.8062 |
[32m[20221213 12:41:13 @agent_ppo2.py:179][0m |          -0.0456 |          22.5808 |        -241.5881 |
[32m[20221213 12:41:13 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:41:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 284.63
[32m[20221213 12:41:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.09
[32m[20221213 12:41:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.83
[32m[20221213 12:41:13 @agent_ppo2.py:137][0m Total time:      25.27 min
[32m[20221213 12:41:13 @agent_ppo2.py:139][0m 1710080 total steps have happened
[32m[20221213 12:41:13 @agent_ppo2.py:115][0m #------------------------ Iteration 835 --------------------------#
[32m[20221213 12:41:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |           0.0971 |          30.0645 |        -168.3720 |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |           0.0859 |          28.1300 |        -123.9846 |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |           0.0355 |          27.6112 |        -159.3252 |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |           0.0133 |          27.2842 |        -177.3117 |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |          -0.0015 |          27.1982 |        -191.3511 |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |          -0.0075 |          26.9465 |        -199.1985 |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |          -0.0091 |          26.8970 |        -197.2175 |
[32m[20221213 12:41:14 @agent_ppo2.py:179][0m |          -0.0290 |          26.6660 |        -213.6978 |
[32m[20221213 12:41:15 @agent_ppo2.py:179][0m |          -0.0105 |          28.9571 |        -216.6141 |
[32m[20221213 12:41:15 @agent_ppo2.py:179][0m |          -0.0288 |          26.5477 |        -222.6588 |
[32m[20221213 12:41:15 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:41:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 295.21
[32m[20221213 12:41:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 311.58
[32m[20221213 12:41:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 296.73
[32m[20221213 12:41:15 @agent_ppo2.py:137][0m Total time:      25.30 min
[32m[20221213 12:41:15 @agent_ppo2.py:139][0m 1712128 total steps have happened
[32m[20221213 12:41:15 @agent_ppo2.py:115][0m #------------------------ Iteration 836 --------------------------#
[32m[20221213 12:41:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:15 @agent_ppo2.py:179][0m |           0.0768 |          25.8199 |        -193.5511 |
[32m[20221213 12:41:15 @agent_ppo2.py:179][0m |           0.0480 |          22.8782 |        -153.2672 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |           0.0079 |          21.7863 |        -172.9042 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |          -0.0141 |          21.0444 |        -178.2053 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |          -0.0272 |          20.6062 |        -184.8008 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |          -0.0335 |          20.2341 |        -193.9686 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |          -0.0368 |          19.8917 |        -195.5135 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |          -0.0431 |          19.6042 |        -202.1420 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |          -0.0443 |          19.3858 |        -205.4970 |
[32m[20221213 12:41:16 @agent_ppo2.py:179][0m |          -0.0391 |          19.4171 |        -205.5194 |
[32m[20221213 12:41:16 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 292.14
[32m[20221213 12:41:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.75
[32m[20221213 12:41:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 288.06
[32m[20221213 12:41:17 @agent_ppo2.py:137][0m Total time:      25.33 min
[32m[20221213 12:41:17 @agent_ppo2.py:139][0m 1714176 total steps have happened
[32m[20221213 12:41:17 @agent_ppo2.py:115][0m #------------------------ Iteration 837 --------------------------#
[32m[20221213 12:41:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:17 @agent_ppo2.py:179][0m |           0.1060 |          30.8588 |        -136.1542 |
[32m[20221213 12:41:17 @agent_ppo2.py:179][0m |           0.1007 |          28.9486 |         -84.5212 |
[32m[20221213 12:41:17 @agent_ppo2.py:179][0m |           0.0650 |          28.3636 |        -117.7381 |
[32m[20221213 12:41:17 @agent_ppo2.py:179][0m |           0.0346 |          27.7385 |        -137.7595 |
[32m[20221213 12:41:17 @agent_ppo2.py:179][0m |           0.0158 |          30.2224 |        -167.3315 |
[32m[20221213 12:41:18 @agent_ppo2.py:179][0m |          -0.0108 |          27.1895 |        -179.8678 |
[32m[20221213 12:41:18 @agent_ppo2.py:179][0m |          -0.0230 |          26.7782 |        -189.3199 |
[32m[20221213 12:41:18 @agent_ppo2.py:179][0m |          -0.0285 |          26.6590 |        -195.7360 |
[32m[20221213 12:41:18 @agent_ppo2.py:179][0m |          -0.0313 |          26.3611 |        -196.8416 |
[32m[20221213 12:41:18 @agent_ppo2.py:179][0m |          -0.0272 |          28.1656 |        -202.5861 |
[32m[20221213 12:41:18 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.81
[32m[20221213 12:41:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 323.74
[32m[20221213 12:41:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 299.70
[32m[20221213 12:41:18 @agent_ppo2.py:137][0m Total time:      25.36 min
[32m[20221213 12:41:18 @agent_ppo2.py:139][0m 1716224 total steps have happened
[32m[20221213 12:41:18 @agent_ppo2.py:115][0m #------------------------ Iteration 838 --------------------------#
[32m[20221213 12:41:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |           0.0971 |          28.8661 |        -169.7810 |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |           0.0573 |          26.8356 |        -135.2378 |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |           0.0199 |          26.0171 |        -155.5285 |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |           0.0113 |          25.6882 |        -162.7967 |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |          -0.0172 |          24.9772 |        -176.7741 |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |          -0.0246 |          24.6070 |        -183.0739 |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |          -0.0224 |          25.7437 |        -186.6619 |
[32m[20221213 12:41:19 @agent_ppo2.py:179][0m |          -0.0288 |          24.0959 |        -188.5728 |
[32m[20221213 12:41:20 @agent_ppo2.py:179][0m |          -0.0390 |          23.7180 |        -191.4945 |
[32m[20221213 12:41:20 @agent_ppo2.py:179][0m |          -0.0393 |          23.6393 |        -197.0060 |
[32m[20221213 12:41:20 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.94
[32m[20221213 12:41:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.03
[32m[20221213 12:41:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.47
[32m[20221213 12:41:20 @agent_ppo2.py:137][0m Total time:      25.39 min
[32m[20221213 12:41:20 @agent_ppo2.py:139][0m 1718272 total steps have happened
[32m[20221213 12:41:20 @agent_ppo2.py:115][0m #------------------------ Iteration 839 --------------------------#
[32m[20221213 12:41:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:20 @agent_ppo2.py:179][0m |           0.1006 |          33.1961 |        -169.8908 |
[32m[20221213 12:41:20 @agent_ppo2.py:179][0m |           0.0565 |          33.1489 |        -143.2333 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |           0.0307 |          30.5800 |        -151.3563 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |           0.0013 |          29.7883 |        -156.0622 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |          -0.0166 |          29.3660 |        -162.6147 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |          -0.0275 |          29.1082 |        -169.8453 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |          -0.0201 |          30.3595 |        -179.1152 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |          -0.0279 |          28.5362 |        -177.2134 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |          -0.0340 |          28.0978 |        -183.2958 |
[32m[20221213 12:41:21 @agent_ppo2.py:179][0m |          -0.0442 |          27.9809 |        -189.3253 |
[32m[20221213 12:41:21 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:41:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 292.33
[32m[20221213 12:41:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.86
[32m[20221213 12:41:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 20.05
[32m[20221213 12:41:22 @agent_ppo2.py:137][0m Total time:      25.42 min
[32m[20221213 12:41:22 @agent_ppo2.py:139][0m 1720320 total steps have happened
[32m[20221213 12:41:22 @agent_ppo2.py:115][0m #------------------------ Iteration 840 --------------------------#
[32m[20221213 12:41:22 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:41:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:22 @agent_ppo2.py:179][0m |           0.1041 |          14.2310 |        -177.5714 |
[32m[20221213 12:41:22 @agent_ppo2.py:179][0m |           0.0667 |          11.7583 |        -159.8467 |
[32m[20221213 12:41:22 @agent_ppo2.py:179][0m |           0.0085 |          10.7903 |        -170.5063 |
[32m[20221213 12:41:22 @agent_ppo2.py:179][0m |          -0.0153 |          10.1445 |        -179.7221 |
[32m[20221213 12:41:23 @agent_ppo2.py:179][0m |          -0.0265 |           9.6486 |        -183.1777 |
[32m[20221213 12:41:23 @agent_ppo2.py:179][0m |          -0.0341 |           9.3410 |        -183.2018 |
[32m[20221213 12:41:23 @agent_ppo2.py:179][0m |          -0.0475 |           8.9248 |        -186.9414 |
[32m[20221213 12:41:23 @agent_ppo2.py:179][0m |          -0.0463 |           8.5872 |        -190.9203 |
[32m[20221213 12:41:23 @agent_ppo2.py:179][0m |          -0.0456 |           8.2244 |        -185.3232 |
[32m[20221213 12:41:23 @agent_ppo2.py:179][0m |          -0.0519 |           8.6978 |        -196.8948 |
[32m[20221213 12:41:23 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 122.72
[32m[20221213 12:41:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.38
[32m[20221213 12:41:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.14
[32m[20221213 12:41:23 @agent_ppo2.py:137][0m Total time:      25.44 min
[32m[20221213 12:41:23 @agent_ppo2.py:139][0m 1722368 total steps have happened
[32m[20221213 12:41:23 @agent_ppo2.py:115][0m #------------------------ Iteration 841 --------------------------#
[32m[20221213 12:41:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:24 @agent_ppo2.py:179][0m |           0.1083 |          28.6311 |        -147.5412 |
[32m[20221213 12:41:24 @agent_ppo2.py:179][0m |           0.0926 |          26.2835 |        -114.3845 |
[32m[20221213 12:41:24 @agent_ppo2.py:179][0m |           0.0240 |          25.2345 |        -145.5791 |
[32m[20221213 12:41:24 @agent_ppo2.py:179][0m |           0.0002 |          24.4383 |        -151.7364 |
[32m[20221213 12:41:24 @agent_ppo2.py:179][0m |          -0.0104 |          23.9620 |        -157.7933 |
[32m[20221213 12:41:24 @agent_ppo2.py:179][0m |          -0.0264 |          23.5258 |        -165.6126 |
[32m[20221213 12:41:24 @agent_ppo2.py:179][0m |          -0.0339 |          23.1873 |        -168.6438 |
[32m[20221213 12:41:25 @agent_ppo2.py:179][0m |          -0.0375 |          23.0478 |        -173.2520 |
[32m[20221213 12:41:25 @agent_ppo2.py:179][0m |          -0.0429 |          22.7381 |        -176.5763 |
[32m[20221213 12:41:25 @agent_ppo2.py:179][0m |          -0.0447 |          22.5245 |        -178.8917 |
[32m[20221213 12:41:25 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:41:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.04
[32m[20221213 12:41:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.46
[32m[20221213 12:41:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.31
[32m[20221213 12:41:25 @agent_ppo2.py:137][0m Total time:      25.47 min
[32m[20221213 12:41:25 @agent_ppo2.py:139][0m 1724416 total steps have happened
[32m[20221213 12:41:25 @agent_ppo2.py:115][0m #------------------------ Iteration 842 --------------------------#
[32m[20221213 12:41:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:25 @agent_ppo2.py:179][0m |           0.0492 |          21.9824 |        -169.8363 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |           0.0308 |          20.3872 |        -156.3722 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |          -0.0008 |          19.7970 |        -166.7177 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |          -0.0171 |          19.4845 |        -174.0777 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |          -0.0286 |          19.2455 |        -177.2352 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |          -0.0337 |          19.0181 |        -184.1648 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |          -0.0369 |          18.7851 |        -186.6463 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |          -0.0395 |          18.6337 |        -193.1571 |
[32m[20221213 12:41:26 @agent_ppo2.py:179][0m |          -0.0377 |          18.4536 |        -194.1072 |
[32m[20221213 12:41:27 @agent_ppo2.py:179][0m |          -0.0351 |          18.3639 |        -190.3042 |
[32m[20221213 12:41:27 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:41:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 202.16
[32m[20221213 12:41:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.16
[32m[20221213 12:41:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 19.98
[32m[20221213 12:41:27 @agent_ppo2.py:137][0m Total time:      25.50 min
[32m[20221213 12:41:27 @agent_ppo2.py:139][0m 1726464 total steps have happened
[32m[20221213 12:41:27 @agent_ppo2.py:115][0m #------------------------ Iteration 843 --------------------------#
[32m[20221213 12:41:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:27 @agent_ppo2.py:179][0m |           0.0864 |          31.0184 |        -158.6610 |
[32m[20221213 12:41:27 @agent_ppo2.py:179][0m |           0.1374 |          29.8813 |        -120.3296 |
[32m[20221213 12:41:27 @agent_ppo2.py:179][0m |           0.0316 |          29.4427 |        -149.2998 |
[32m[20221213 12:41:28 @agent_ppo2.py:179][0m |           0.0022 |          29.1686 |        -169.1600 |
[32m[20221213 12:41:28 @agent_ppo2.py:179][0m |          -0.0113 |          29.0388 |        -172.8464 |
[32m[20221213 12:41:28 @agent_ppo2.py:179][0m |          -0.0191 |          28.8559 |        -185.0370 |
[32m[20221213 12:41:28 @agent_ppo2.py:179][0m |          -0.0209 |          28.7273 |        -179.0521 |
[32m[20221213 12:41:28 @agent_ppo2.py:179][0m |          -0.0304 |          28.5605 |        -190.5915 |
[32m[20221213 12:41:28 @agent_ppo2.py:179][0m |          -0.0374 |          28.3894 |        -194.5175 |
[32m[20221213 12:41:28 @agent_ppo2.py:179][0m |          -0.0373 |          28.2305 |        -199.4642 |
[32m[20221213 12:41:28 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 304.32
[32m[20221213 12:41:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.50
[32m[20221213 12:41:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.25
[32m[20221213 12:41:28 @agent_ppo2.py:137][0m Total time:      25.53 min
[32m[20221213 12:41:28 @agent_ppo2.py:139][0m 1728512 total steps have happened
[32m[20221213 12:41:28 @agent_ppo2.py:115][0m #------------------------ Iteration 844 --------------------------#
[32m[20221213 12:41:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:29 @agent_ppo2.py:179][0m |           0.0739 |          34.5182 |        -174.0298 |
[32m[20221213 12:41:29 @agent_ppo2.py:179][0m |           0.0405 |          31.9360 |        -163.5497 |
[32m[20221213 12:41:29 @agent_ppo2.py:179][0m |          -0.0048 |          30.3847 |        -170.7687 |
[32m[20221213 12:41:29 @agent_ppo2.py:179][0m |          -0.0262 |          29.6843 |        -176.3990 |
[32m[20221213 12:41:29 @agent_ppo2.py:179][0m |          -0.0369 |          29.4641 |        -183.5449 |
[32m[20221213 12:41:29 @agent_ppo2.py:179][0m |          -0.0422 |          28.9996 |        -186.1894 |
[32m[20221213 12:41:30 @agent_ppo2.py:179][0m |          -0.0470 |          28.6660 |        -189.4178 |
[32m[20221213 12:41:30 @agent_ppo2.py:179][0m |          -0.0498 |          28.3971 |        -189.8906 |
[32m[20221213 12:41:30 @agent_ppo2.py:179][0m |          -0.0537 |          28.2584 |        -194.3717 |
[32m[20221213 12:41:30 @agent_ppo2.py:179][0m |          -0.0520 |          28.2286 |        -196.5131 |
[32m[20221213 12:41:30 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:41:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 272.21
[32m[20221213 12:41:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 318.33
[32m[20221213 12:41:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 156.75
[32m[20221213 12:41:30 @agent_ppo2.py:137][0m Total time:      25.56 min
[32m[20221213 12:41:30 @agent_ppo2.py:139][0m 1730560 total steps have happened
[32m[20221213 12:41:30 @agent_ppo2.py:115][0m #------------------------ Iteration 845 --------------------------#
[32m[20221213 12:41:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |           0.1319 |          35.8812 |        -165.0977 |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |           0.0899 |          31.0138 |        -128.8317 |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |           0.0202 |          30.2131 |        -155.7400 |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |          -0.0103 |          29.9711 |        -168.5169 |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |          -0.0206 |          29.5436 |        -178.2444 |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |          -0.0297 |          29.3902 |        -183.6131 |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |          -0.0370 |          29.1813 |        -186.6470 |
[32m[20221213 12:41:31 @agent_ppo2.py:179][0m |          -0.0391 |          29.1032 |        -192.2063 |
[32m[20221213 12:41:32 @agent_ppo2.py:179][0m |          -0.0435 |          28.9129 |        -196.1070 |
[32m[20221213 12:41:32 @agent_ppo2.py:179][0m |          -0.0427 |          28.6650 |        -196.3335 |
[32m[20221213 12:41:32 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.62
[32m[20221213 12:41:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.25
[32m[20221213 12:41:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 297.12
[32m[20221213 12:41:32 @agent_ppo2.py:137][0m Total time:      25.58 min
[32m[20221213 12:41:32 @agent_ppo2.py:139][0m 1732608 total steps have happened
[32m[20221213 12:41:32 @agent_ppo2.py:115][0m #------------------------ Iteration 846 --------------------------#
[32m[20221213 12:41:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:32 @agent_ppo2.py:179][0m |           0.0926 |          33.9069 |        -162.8236 |
[32m[20221213 12:41:32 @agent_ppo2.py:179][0m |           0.0691 |          32.8397 |        -122.2068 |
[32m[20221213 12:41:32 @agent_ppo2.py:179][0m |           0.0492 |          32.9169 |        -150.3241 |
[32m[20221213 12:41:33 @agent_ppo2.py:179][0m |           0.0172 |          32.1506 |        -146.4749 |
[32m[20221213 12:41:33 @agent_ppo2.py:179][0m |          -0.0062 |          31.5042 |        -159.6665 |
[32m[20221213 12:41:33 @agent_ppo2.py:179][0m |          -0.0179 |          31.1978 |        -168.2123 |
[32m[20221213 12:41:33 @agent_ppo2.py:179][0m |          -0.0238 |          31.0616 |        -175.0283 |
[32m[20221213 12:41:33 @agent_ppo2.py:179][0m |          -0.0223 |          30.7160 |        -173.3848 |
[32m[20221213 12:41:33 @agent_ppo2.py:179][0m |          -0.0337 |          30.6081 |        -180.6472 |
[32m[20221213 12:41:33 @agent_ppo2.py:179][0m |          -0.0388 |          30.8117 |        -186.1663 |
[32m[20221213 12:41:33 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:41:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.18
[32m[20221213 12:41:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.45
[32m[20221213 12:41:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.62
[32m[20221213 12:41:34 @agent_ppo2.py:137][0m Total time:      25.61 min
[32m[20221213 12:41:34 @agent_ppo2.py:139][0m 1734656 total steps have happened
[32m[20221213 12:41:34 @agent_ppo2.py:115][0m #------------------------ Iteration 847 --------------------------#
[32m[20221213 12:41:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:34 @agent_ppo2.py:179][0m |           0.0946 |          34.1500 |        -149.1012 |
[32m[20221213 12:41:34 @agent_ppo2.py:179][0m |           0.0962 |          31.8877 |        -108.3930 |
[32m[20221213 12:41:34 @agent_ppo2.py:179][0m |           0.0332 |          31.2195 |        -125.8962 |
[32m[20221213 12:41:34 @agent_ppo2.py:179][0m |           0.0070 |          31.6400 |        -144.5010 |
[32m[20221213 12:41:34 @agent_ppo2.py:179][0m |          -0.0118 |          30.2900 |        -154.9396 |
[32m[20221213 12:41:35 @agent_ppo2.py:179][0m |          -0.0244 |          29.9788 |        -158.8660 |
[32m[20221213 12:41:35 @agent_ppo2.py:179][0m |          -0.0228 |          31.2903 |        -167.6793 |
[32m[20221213 12:41:35 @agent_ppo2.py:179][0m |          -0.0358 |          29.6427 |        -172.7228 |
[32m[20221213 12:41:35 @agent_ppo2.py:179][0m |          -0.0433 |          29.3847 |        -175.8539 |
[32m[20221213 12:41:35 @agent_ppo2.py:179][0m |          -0.0425 |          29.1084 |        -178.3584 |
[32m[20221213 12:41:35 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:41:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.27
[32m[20221213 12:41:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 319.48
[32m[20221213 12:41:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 286.40
[32m[20221213 12:41:35 @agent_ppo2.py:137][0m Total time:      25.64 min
[32m[20221213 12:41:35 @agent_ppo2.py:139][0m 1736704 total steps have happened
[32m[20221213 12:41:35 @agent_ppo2.py:115][0m #------------------------ Iteration 848 --------------------------#
[32m[20221213 12:41:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |           0.0804 |          17.5367 |        -150.0365 |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |           0.0783 |          16.4791 |        -124.3082 |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |           0.0243 |          15.3639 |        -145.9324 |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |          -0.0066 |          14.7643 |        -168.3364 |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |          -0.0205 |          14.3535 |        -174.3297 |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |          -0.0273 |          14.0072 |        -177.5350 |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |          -0.0336 |          13.7352 |        -183.0104 |
[32m[20221213 12:41:36 @agent_ppo2.py:179][0m |          -0.0340 |          13.4476 |        -183.4552 |
[32m[20221213 12:41:37 @agent_ppo2.py:179][0m |          -0.0443 |          13.3289 |        -189.6589 |
[32m[20221213 12:41:37 @agent_ppo2.py:179][0m |          -0.0412 |          13.0531 |        -192.9595 |
[32m[20221213 12:41:37 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:41:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.49
[32m[20221213 12:41:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.71
[32m[20221213 12:41:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 271.68
[32m[20221213 12:41:37 @agent_ppo2.py:137][0m Total time:      25.67 min
[32m[20221213 12:41:37 @agent_ppo2.py:139][0m 1738752 total steps have happened
[32m[20221213 12:41:37 @agent_ppo2.py:115][0m #------------------------ Iteration 849 --------------------------#
[32m[20221213 12:41:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:37 @agent_ppo2.py:179][0m |           0.0714 |          34.9889 |        -163.2278 |
[32m[20221213 12:41:37 @agent_ppo2.py:179][0m |           0.0515 |          32.8569 |        -154.3773 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |           0.0168 |          31.9394 |        -159.7658 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |          -0.0093 |          31.3850 |        -173.4366 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |          -0.0323 |          30.8727 |        -182.1870 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |          -0.0262 |          30.5892 |        -183.9358 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |          -0.0338 |          30.3889 |        -187.7411 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |          -0.0341 |          31.2832 |        -191.3364 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |          -0.0300 |          30.0038 |        -193.1433 |
[32m[20221213 12:41:38 @agent_ppo2.py:179][0m |          -0.0301 |          29.8139 |        -191.3622 |
[32m[20221213 12:41:38 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:41:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.06
[32m[20221213 12:41:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.34
[32m[20221213 12:41:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.88
[32m[20221213 12:41:39 @agent_ppo2.py:137][0m Total time:      25.70 min
[32m[20221213 12:41:39 @agent_ppo2.py:139][0m 1740800 total steps have happened
[32m[20221213 12:41:39 @agent_ppo2.py:115][0m #------------------------ Iteration 850 --------------------------#
[32m[20221213 12:41:39 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:41:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:39 @agent_ppo2.py:179][0m |           0.0594 |          33.2800 |        -188.2287 |
[32m[20221213 12:41:39 @agent_ppo2.py:179][0m |           0.0665 |          31.9558 |        -146.1321 |
[32m[20221213 12:41:39 @agent_ppo2.py:179][0m |           0.0476 |          31.4072 |        -148.0204 |
[32m[20221213 12:41:39 @agent_ppo2.py:179][0m |           0.0092 |          30.9820 |        -170.5585 |
[32m[20221213 12:41:40 @agent_ppo2.py:179][0m |          -0.0027 |          30.5607 |        -170.8714 |
[32m[20221213 12:41:40 @agent_ppo2.py:179][0m |          -0.0157 |          30.3271 |        -179.9701 |
[32m[20221213 12:41:40 @agent_ppo2.py:179][0m |          -0.0229 |          30.1054 |        -184.7536 |
[32m[20221213 12:41:40 @agent_ppo2.py:179][0m |          -0.0265 |          29.9285 |        -186.7526 |
[32m[20221213 12:41:40 @agent_ppo2.py:179][0m |          -0.0316 |          29.7762 |        -191.7772 |
[32m[20221213 12:41:40 @agent_ppo2.py:179][0m |          -0.0335 |          29.7405 |        -191.8019 |
[32m[20221213 12:41:40 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:41:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.81
[32m[20221213 12:41:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.11
[32m[20221213 12:41:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 294.04
[32m[20221213 12:41:40 @agent_ppo2.py:137][0m Total time:      25.73 min
[32m[20221213 12:41:40 @agent_ppo2.py:139][0m 1742848 total steps have happened
[32m[20221213 12:41:40 @agent_ppo2.py:115][0m #------------------------ Iteration 851 --------------------------#
[32m[20221213 12:41:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:41 @agent_ppo2.py:179][0m |           0.0577 |          30.1455 |        -172.0739 |
[32m[20221213 12:41:41 @agent_ppo2.py:179][0m |           0.0536 |          28.2531 |        -155.1125 |
[32m[20221213 12:41:41 @agent_ppo2.py:179][0m |          -0.0009 |          27.3969 |        -167.9570 |
[32m[20221213 12:41:41 @agent_ppo2.py:179][0m |          -0.0165 |          26.7622 |        -178.1682 |
[32m[20221213 12:41:41 @agent_ppo2.py:179][0m |          -0.0120 |          26.4344 |        -176.0248 |
[32m[20221213 12:41:41 @agent_ppo2.py:179][0m |          -0.0247 |          27.1954 |        -181.3935 |
[32m[20221213 12:41:41 @agent_ppo2.py:179][0m |          -0.0366 |          25.8206 |        -187.7974 |
[32m[20221213 12:41:42 @agent_ppo2.py:179][0m |          -0.0401 |          25.5608 |        -189.1975 |
[32m[20221213 12:41:42 @agent_ppo2.py:179][0m |          -0.0309 |          29.1160 |        -191.7333 |
[32m[20221213 12:41:42 @agent_ppo2.py:179][0m |          -0.0364 |          26.2417 |        -195.6567 |
[32m[20221213 12:41:42 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.45
[32m[20221213 12:41:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.82
[32m[20221213 12:41:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.39
[32m[20221213 12:41:42 @agent_ppo2.py:137][0m Total time:      25.75 min
[32m[20221213 12:41:42 @agent_ppo2.py:139][0m 1744896 total steps have happened
[32m[20221213 12:41:42 @agent_ppo2.py:115][0m #------------------------ Iteration 852 --------------------------#
[32m[20221213 12:41:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:42 @agent_ppo2.py:179][0m |           0.0557 |          18.0100 |        -175.4815 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |           0.0403 |          16.6294 |        -170.2127 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |           0.0161 |          15.8104 |        -174.8486 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |          -0.0090 |          15.1984 |        -179.9072 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |          -0.0223 |          15.0360 |        -186.2241 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |          -0.0236 |          14.6037 |        -192.0256 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |          -0.0360 |          14.4062 |        -191.6165 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |          -0.0316 |          14.2121 |        -193.9567 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |          -0.0402 |          13.9949 |        -198.2535 |
[32m[20221213 12:41:43 @agent_ppo2.py:179][0m |          -0.0365 |          13.8574 |        -200.7487 |
[32m[20221213 12:41:43 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:41:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.03
[32m[20221213 12:41:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.50
[32m[20221213 12:41:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 70.73
[32m[20221213 12:41:44 @agent_ppo2.py:137][0m Total time:      25.78 min
[32m[20221213 12:41:44 @agent_ppo2.py:139][0m 1746944 total steps have happened
[32m[20221213 12:41:44 @agent_ppo2.py:115][0m #------------------------ Iteration 853 --------------------------#
[32m[20221213 12:41:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:44 @agent_ppo2.py:179][0m |           0.1033 |          33.4097 |        -142.5727 |
[32m[20221213 12:41:44 @agent_ppo2.py:179][0m |           0.0730 |          31.2106 |        -114.9272 |
[32m[20221213 12:41:44 @agent_ppo2.py:179][0m |           0.0206 |          30.6860 |        -140.5522 |
[32m[20221213 12:41:44 @agent_ppo2.py:179][0m |           0.0009 |          30.1268 |        -154.8844 |
[32m[20221213 12:41:45 @agent_ppo2.py:179][0m |          -0.0137 |          30.0811 |        -166.9823 |
[32m[20221213 12:41:45 @agent_ppo2.py:179][0m |          -0.0103 |          29.7553 |        -161.1212 |
[32m[20221213 12:41:45 @agent_ppo2.py:179][0m |          -0.0150 |          29.4254 |        -160.6256 |
[32m[20221213 12:41:45 @agent_ppo2.py:179][0m |          -0.0281 |          29.4928 |        -171.0333 |
[32m[20221213 12:41:45 @agent_ppo2.py:179][0m |          -0.0395 |          29.1157 |        -179.4458 |
[32m[20221213 12:41:45 @agent_ppo2.py:179][0m |          -0.0406 |          29.0093 |        -181.9380 |
[32m[20221213 12:41:45 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:41:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.86
[32m[20221213 12:41:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.62
[32m[20221213 12:41:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.55
[32m[20221213 12:41:45 @agent_ppo2.py:137][0m Total time:      25.81 min
[32m[20221213 12:41:45 @agent_ppo2.py:139][0m 1748992 total steps have happened
[32m[20221213 12:41:45 @agent_ppo2.py:115][0m #------------------------ Iteration 854 --------------------------#
[32m[20221213 12:41:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:46 @agent_ppo2.py:179][0m |           0.0600 |          10.8061 |        -150.4863 |
[32m[20221213 12:41:46 @agent_ppo2.py:179][0m |           0.0124 |           9.5499 |        -133.1356 |
[32m[20221213 12:41:46 @agent_ppo2.py:179][0m |           0.0148 |          12.9346 |        -137.9039 |
[32m[20221213 12:41:46 @agent_ppo2.py:179][0m |          -0.0139 |           8.8051 |        -142.5483 |
[32m[20221213 12:41:46 @agent_ppo2.py:179][0m |          -0.0215 |           8.3400 |        -146.9523 |
[32m[20221213 12:41:46 @agent_ppo2.py:179][0m |          -0.0260 |           8.1458 |        -152.0677 |
[32m[20221213 12:41:47 @agent_ppo2.py:179][0m |          -0.0316 |           8.0024 |        -154.3748 |
[32m[20221213 12:41:47 @agent_ppo2.py:179][0m |          -0.0221 |           7.8509 |        -149.6421 |
[32m[20221213 12:41:47 @agent_ppo2.py:179][0m |          -0.0380 |           7.7324 |        -160.8201 |
[32m[20221213 12:41:47 @agent_ppo2.py:179][0m |          -0.0393 |           7.6806 |        -165.2878 |
[32m[20221213 12:41:47 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:41:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.76
[32m[20221213 12:41:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.85
[32m[20221213 12:41:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.76
[32m[20221213 12:41:47 @agent_ppo2.py:137][0m Total time:      25.84 min
[32m[20221213 12:41:47 @agent_ppo2.py:139][0m 1751040 total steps have happened
[32m[20221213 12:41:47 @agent_ppo2.py:115][0m #------------------------ Iteration 855 --------------------------#
[32m[20221213 12:41:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:47 @agent_ppo2.py:179][0m |           0.1198 |          33.5558 |        -150.9985 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |           0.0641 |          31.3858 |        -133.2082 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |           0.0253 |          30.7630 |        -147.9145 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |          -0.0112 |          30.2927 |        -157.0810 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |          -0.0171 |          30.1044 |        -162.5410 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |          -0.0302 |          29.7357 |        -169.3366 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |          -0.0284 |          29.5301 |        -170.9741 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |          -0.0371 |          29.4743 |        -170.7285 |
[32m[20221213 12:41:48 @agent_ppo2.py:179][0m |          -0.0360 |          29.1729 |        -177.9620 |
[32m[20221213 12:41:49 @agent_ppo2.py:179][0m |          -0.0392 |          28.9458 |        -178.0850 |
[32m[20221213 12:41:49 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.21
[32m[20221213 12:41:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.38
[32m[20221213 12:41:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.58
[32m[20221213 12:41:49 @agent_ppo2.py:137][0m Total time:      25.87 min
[32m[20221213 12:41:49 @agent_ppo2.py:139][0m 1753088 total steps have happened
[32m[20221213 12:41:49 @agent_ppo2.py:115][0m #------------------------ Iteration 856 --------------------------#
[32m[20221213 12:41:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:49 @agent_ppo2.py:179][0m |           0.1017 |          32.8907 |        -159.4862 |
[32m[20221213 12:41:49 @agent_ppo2.py:179][0m |           0.0634 |          30.9214 |        -140.9448 |
[32m[20221213 12:41:49 @agent_ppo2.py:179][0m |           0.0450 |          30.3055 |        -143.3903 |
[32m[20221213 12:41:50 @agent_ppo2.py:179][0m |           0.0031 |          29.8257 |        -159.9553 |
[32m[20221213 12:41:50 @agent_ppo2.py:179][0m |          -0.0201 |          29.2029 |        -172.3717 |
[32m[20221213 12:41:50 @agent_ppo2.py:179][0m |          -0.0284 |          28.8203 |        -174.1422 |
[32m[20221213 12:41:50 @agent_ppo2.py:179][0m |          -0.0382 |          28.4094 |        -180.9128 |
[32m[20221213 12:41:50 @agent_ppo2.py:179][0m |          -0.0418 |          28.2109 |        -182.2075 |
[32m[20221213 12:41:50 @agent_ppo2.py:179][0m |          -0.0442 |          27.9347 |        -186.7220 |
[32m[20221213 12:41:50 @agent_ppo2.py:179][0m |          -0.0449 |          27.7973 |        -185.7711 |
[32m[20221213 12:41:50 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 292.63
[32m[20221213 12:41:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.10
[32m[20221213 12:41:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.04
[32m[20221213 12:41:50 @agent_ppo2.py:137][0m Total time:      25.90 min
[32m[20221213 12:41:50 @agent_ppo2.py:139][0m 1755136 total steps have happened
[32m[20221213 12:41:50 @agent_ppo2.py:115][0m #------------------------ Iteration 857 --------------------------#
[32m[20221213 12:41:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:51 @agent_ppo2.py:179][0m |           0.1067 |          29.6816 |        -148.8398 |
[32m[20221213 12:41:51 @agent_ppo2.py:179][0m |           0.1110 |          30.8974 |        -112.2055 |
[32m[20221213 12:41:51 @agent_ppo2.py:179][0m |           0.0382 |          26.8788 |        -137.7244 |
[32m[20221213 12:41:51 @agent_ppo2.py:179][0m |          -0.0067 |          26.1043 |        -156.7831 |
[32m[20221213 12:41:51 @agent_ppo2.py:179][0m |          -0.0224 |          25.7229 |        -165.1654 |
[32m[20221213 12:41:52 @agent_ppo2.py:179][0m |          -0.0176 |          27.5182 |        -167.5659 |
[32m[20221213 12:41:52 @agent_ppo2.py:179][0m |          -0.0323 |          25.2460 |        -174.1229 |
[32m[20221213 12:41:52 @agent_ppo2.py:179][0m |          -0.0424 |          25.0567 |        -179.5842 |
[32m[20221213 12:41:52 @agent_ppo2.py:179][0m |          -0.0442 |          24.8459 |        -183.9788 |
[32m[20221213 12:41:52 @agent_ppo2.py:179][0m |          -0.0460 |          24.7147 |        -186.2688 |
[32m[20221213 12:41:52 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:41:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.91
[32m[20221213 12:41:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.44
[32m[20221213 12:41:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.18
[32m[20221213 12:41:52 @agent_ppo2.py:137][0m Total time:      25.92 min
[32m[20221213 12:41:52 @agent_ppo2.py:139][0m 1757184 total steps have happened
[32m[20221213 12:41:52 @agent_ppo2.py:115][0m #------------------------ Iteration 858 --------------------------#
[32m[20221213 12:41:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |           0.0837 |          22.4542 |        -156.4301 |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |           0.0555 |          20.4684 |        -139.8916 |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |           0.0051 |          19.6369 |        -153.4289 |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |          -0.0198 |          18.9984 |        -160.1658 |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |          -0.0337 |          18.6388 |        -161.4397 |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |          -0.0348 |          18.2517 |        -164.6228 |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |          -0.0345 |          18.0541 |        -165.8655 |
[32m[20221213 12:41:53 @agent_ppo2.py:179][0m |          -0.0382 |          17.7223 |        -170.1043 |
[32m[20221213 12:41:54 @agent_ppo2.py:179][0m |          -0.0357 |          17.5014 |        -170.0258 |
[32m[20221213 12:41:54 @agent_ppo2.py:179][0m |          -0.0441 |          17.3258 |        -173.7514 |
[32m[20221213 12:41:54 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:41:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 207.67
[32m[20221213 12:41:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.62
[32m[20221213 12:41:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.58
[32m[20221213 12:41:54 @agent_ppo2.py:137][0m Total time:      25.95 min
[32m[20221213 12:41:54 @agent_ppo2.py:139][0m 1759232 total steps have happened
[32m[20221213 12:41:54 @agent_ppo2.py:115][0m #------------------------ Iteration 859 --------------------------#
[32m[20221213 12:41:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:54 @agent_ppo2.py:179][0m |           0.0656 |          10.8839 |         -72.9106 |
[32m[20221213 12:41:54 @agent_ppo2.py:179][0m |           0.0512 |           9.9496 |         -42.8681 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |           0.0234 |           9.5363 |         -39.5652 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |          -0.0012 |           9.2524 |         -53.6700 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |          -0.0105 |           9.0613 |         -51.4888 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |          -0.0120 |           8.8912 |         -51.9023 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |          -0.0180 |           8.8769 |         -54.9503 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |          -0.0281 |           8.6873 |         -51.7592 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |          -0.0261 |           8.6207 |         -52.3378 |
[32m[20221213 12:41:55 @agent_ppo2.py:179][0m |          -0.0300 |           8.5319 |         -52.6729 |
[32m[20221213 12:41:55 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:41:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 99.39
[32m[20221213 12:41:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.58
[32m[20221213 12:41:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 342.82
[32m[20221213 12:41:56 @agent_ppo2.py:137][0m Total time:      25.98 min
[32m[20221213 12:41:56 @agent_ppo2.py:139][0m 1761280 total steps have happened
[32m[20221213 12:41:56 @agent_ppo2.py:115][0m #------------------------ Iteration 860 --------------------------#
[32m[20221213 12:41:56 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:41:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:56 @agent_ppo2.py:179][0m |           0.0799 |          33.0767 |        -142.4804 |
[32m[20221213 12:41:56 @agent_ppo2.py:179][0m |           0.0874 |          30.8881 |        -110.6575 |
[32m[20221213 12:41:56 @agent_ppo2.py:179][0m |           0.0200 |          30.1615 |        -126.3318 |
[32m[20221213 12:41:56 @agent_ppo2.py:179][0m |          -0.0090 |          29.5461 |        -148.7580 |
[32m[20221213 12:41:57 @agent_ppo2.py:179][0m |          -0.0247 |          29.1300 |        -155.2897 |
[32m[20221213 12:41:57 @agent_ppo2.py:179][0m |          -0.0276 |          28.9124 |        -159.8783 |
[32m[20221213 12:41:57 @agent_ppo2.py:179][0m |          -0.0385 |          28.5737 |        -164.0358 |
[32m[20221213 12:41:57 @agent_ppo2.py:179][0m |          -0.0385 |          28.3374 |        -168.4070 |
[32m[20221213 12:41:57 @agent_ppo2.py:179][0m |          -0.0413 |          28.0837 |        -169.8416 |
[32m[20221213 12:41:57 @agent_ppo2.py:179][0m |          -0.0426 |          27.8200 |        -170.2627 |
[32m[20221213 12:41:57 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:41:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 318.61
[32m[20221213 12:41:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.60
[32m[20221213 12:41:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.48
[32m[20221213 12:41:57 @agent_ppo2.py:137][0m Total time:      26.01 min
[32m[20221213 12:41:57 @agent_ppo2.py:139][0m 1763328 total steps have happened
[32m[20221213 12:41:57 @agent_ppo2.py:115][0m #------------------------ Iteration 861 --------------------------#
[32m[20221213 12:41:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:58 @agent_ppo2.py:179][0m |           0.0871 |           3.8631 |         -99.0383 |
[32m[20221213 12:41:58 @agent_ppo2.py:179][0m |           0.0401 |           3.4252 |         -79.6065 |
[32m[20221213 12:41:58 @agent_ppo2.py:179][0m |           0.0083 |           3.3568 |         -92.6494 |
[32m[20221213 12:41:58 @agent_ppo2.py:179][0m |           0.0004 |           3.3474 |         -95.7886 |
[32m[20221213 12:41:58 @agent_ppo2.py:179][0m |           0.0016 |           3.3359 |         -96.2498 |
[32m[20221213 12:41:58 @agent_ppo2.py:179][0m |           0.0031 |           3.4032 |         -96.2123 |
[32m[20221213 12:41:58 @agent_ppo2.py:179][0m |           0.0136 |           3.2984 |         -90.8790 |
[32m[20221213 12:41:59 @agent_ppo2.py:179][0m |          -0.0057 |           3.2772 |         -98.9303 |
[32m[20221213 12:41:59 @agent_ppo2.py:179][0m |           0.0048 |           3.3010 |         -97.9403 |
[32m[20221213 12:41:59 @agent_ppo2.py:179][0m |           0.0253 |           3.3697 |         -91.4876 |
[32m[20221213 12:41:59 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:41:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 21.54
[32m[20221213 12:41:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.09
[32m[20221213 12:41:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.09
[32m[20221213 12:41:59 @agent_ppo2.py:137][0m Total time:      26.04 min
[32m[20221213 12:41:59 @agent_ppo2.py:139][0m 1765376 total steps have happened
[32m[20221213 12:41:59 @agent_ppo2.py:115][0m #------------------------ Iteration 862 --------------------------#
[32m[20221213 12:41:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:41:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:41:59 @agent_ppo2.py:179][0m |           0.1614 |          31.5506 |        -139.5597 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |           0.0956 |          30.1453 |         -92.3788 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |           0.0246 |          26.6784 |        -114.0738 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |          -0.0053 |          25.9067 |        -126.9888 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |          -0.0231 |          25.4909 |        -133.1531 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |          -0.0336 |          25.1087 |        -139.4697 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |          -0.0405 |          24.9620 |        -143.9754 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |          -0.0441 |          24.7779 |        -145.6106 |
[32m[20221213 12:42:00 @agent_ppo2.py:179][0m |          -0.0416 |          24.4124 |        -141.9231 |
[32m[20221213 12:42:01 @agent_ppo2.py:179][0m |          -0.0506 |          24.1894 |        -147.5056 |
[32m[20221213 12:42:01 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:42:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.01
[32m[20221213 12:42:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.69
[32m[20221213 12:42:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.42
[32m[20221213 12:42:01 @agent_ppo2.py:137][0m Total time:      26.07 min
[32m[20221213 12:42:01 @agent_ppo2.py:139][0m 1767424 total steps have happened
[32m[20221213 12:42:01 @agent_ppo2.py:115][0m #------------------------ Iteration 863 --------------------------#
[32m[20221213 12:42:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:01 @agent_ppo2.py:179][0m |           0.1328 |          17.3412 |         -96.7783 |
[32m[20221213 12:42:01 @agent_ppo2.py:179][0m |           0.0521 |          15.7260 |         -64.0798 |
[32m[20221213 12:42:01 @agent_ppo2.py:179][0m |          -0.0004 |          16.7750 |         -70.8739 |
[32m[20221213 12:42:01 @agent_ppo2.py:179][0m |          -0.0279 |          14.8212 |         -75.1121 |
[32m[20221213 12:42:02 @agent_ppo2.py:179][0m |          -0.0401 |          14.5454 |         -81.2145 |
[32m[20221213 12:42:02 @agent_ppo2.py:179][0m |          -0.0455 |          14.2009 |         -81.5543 |
[32m[20221213 12:42:02 @agent_ppo2.py:179][0m |          -0.0546 |          14.1196 |         -86.3430 |
[32m[20221213 12:42:02 @agent_ppo2.py:179][0m |          -0.0573 |          13.8863 |         -89.8270 |
[32m[20221213 12:42:02 @agent_ppo2.py:179][0m |          -0.0564 |          13.7035 |         -91.2734 |
[32m[20221213 12:42:02 @agent_ppo2.py:179][0m |          -0.0634 |          13.5133 |         -94.0635 |
[32m[20221213 12:42:02 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:42:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.19
[32m[20221213 12:42:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.36
[32m[20221213 12:42:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.28
[32m[20221213 12:42:02 @agent_ppo2.py:137][0m Total time:      26.09 min
[32m[20221213 12:42:02 @agent_ppo2.py:139][0m 1769472 total steps have happened
[32m[20221213 12:42:02 @agent_ppo2.py:115][0m #------------------------ Iteration 864 --------------------------#
[32m[20221213 12:42:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:03 @agent_ppo2.py:179][0m |           0.0638 |          31.9566 |        -186.3971 |
[32m[20221213 12:42:03 @agent_ppo2.py:179][0m |           0.0372 |          30.1641 |        -167.6415 |
[32m[20221213 12:42:03 @agent_ppo2.py:179][0m |           0.0253 |          29.3342 |        -180.6010 |
[32m[20221213 12:42:03 @agent_ppo2.py:179][0m |          -0.0011 |          28.8624 |        -190.5118 |
[32m[20221213 12:42:03 @agent_ppo2.py:179][0m |          -0.0105 |          28.5819 |        -195.0855 |
[32m[20221213 12:42:03 @agent_ppo2.py:179][0m |          -0.0238 |          28.4433 |        -202.6079 |
[32m[20221213 12:42:04 @agent_ppo2.py:179][0m |          -0.0210 |          27.9730 |        -203.9685 |
[32m[20221213 12:42:04 @agent_ppo2.py:179][0m |          -0.0336 |          27.6525 |        -210.4430 |
[32m[20221213 12:42:04 @agent_ppo2.py:179][0m |          -0.0365 |          27.5852 |        -213.0279 |
[32m[20221213 12:42:04 @agent_ppo2.py:179][0m |          -0.0435 |          27.2603 |        -218.7862 |
[32m[20221213 12:42:04 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:42:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.58
[32m[20221213 12:42:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.14
[32m[20221213 12:42:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 262.01
[32m[20221213 12:42:04 @agent_ppo2.py:137][0m Total time:      26.12 min
[32m[20221213 12:42:04 @agent_ppo2.py:139][0m 1771520 total steps have happened
[32m[20221213 12:42:04 @agent_ppo2.py:115][0m #------------------------ Iteration 865 --------------------------#
[32m[20221213 12:42:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |           0.0821 |          24.5759 |        -175.2481 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |           0.0474 |          22.3488 |        -162.0792 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |           0.0143 |          22.4212 |        -176.2200 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |          -0.0032 |          20.8041 |        -182.9124 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |          -0.0285 |          20.2452 |        -193.0517 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |          -0.0346 |          19.9506 |        -194.8373 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |          -0.0385 |          19.6139 |        -200.6312 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |          -0.0372 |          19.5476 |        -203.7178 |
[32m[20221213 12:42:05 @agent_ppo2.py:179][0m |          -0.0457 |          19.2491 |        -210.1799 |
[32m[20221213 12:42:06 @agent_ppo2.py:179][0m |          -0.0549 |          18.9990 |        -210.7881 |
[32m[20221213 12:42:06 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:42:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.22
[32m[20221213 12:42:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.21
[32m[20221213 12:42:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.28
[32m[20221213 12:42:06 @agent_ppo2.py:137][0m Total time:      26.15 min
[32m[20221213 12:42:06 @agent_ppo2.py:139][0m 1773568 total steps have happened
[32m[20221213 12:42:06 @agent_ppo2.py:115][0m #------------------------ Iteration 866 --------------------------#
[32m[20221213 12:42:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:06 @agent_ppo2.py:179][0m |           0.2012 |          25.9761 |        -159.2607 |
[32m[20221213 12:42:06 @agent_ppo2.py:179][0m |           0.0466 |          23.0878 |        -126.4871 |
[32m[20221213 12:42:06 @agent_ppo2.py:179][0m |          -0.0107 |          22.1150 |        -152.7494 |
[32m[20221213 12:42:07 @agent_ppo2.py:179][0m |          -0.0230 |          21.5597 |        -159.4479 |
[32m[20221213 12:42:07 @agent_ppo2.py:179][0m |          -0.0283 |          21.1570 |        -162.2935 |
[32m[20221213 12:42:07 @agent_ppo2.py:179][0m |          -0.0415 |          20.8108 |        -170.6158 |
[32m[20221213 12:42:07 @agent_ppo2.py:179][0m |          -0.0497 |          20.4690 |        -176.5466 |
[32m[20221213 12:42:07 @agent_ppo2.py:179][0m |          -0.0546 |          20.3065 |        -179.1612 |
[32m[20221213 12:42:07 @agent_ppo2.py:179][0m |          -0.0508 |          19.9684 |        -180.1782 |
[32m[20221213 12:42:07 @agent_ppo2.py:179][0m |          -0.0569 |          20.0845 |        -183.4779 |
[32m[20221213 12:42:07 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:42:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 255.52
[32m[20221213 12:42:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.31
[32m[20221213 12:42:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.98
[32m[20221213 12:42:07 @agent_ppo2.py:137][0m Total time:      26.18 min
[32m[20221213 12:42:07 @agent_ppo2.py:139][0m 1775616 total steps have happened
[32m[20221213 12:42:07 @agent_ppo2.py:115][0m #------------------------ Iteration 867 --------------------------#
[32m[20221213 12:42:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:08 @agent_ppo2.py:179][0m |           0.0976 |          34.8634 |        -182.8673 |
[32m[20221213 12:42:08 @agent_ppo2.py:179][0m |           0.0451 |          32.1573 |        -168.1905 |
[32m[20221213 12:42:08 @agent_ppo2.py:179][0m |           0.0118 |          31.2090 |        -182.3113 |
[32m[20221213 12:42:08 @agent_ppo2.py:179][0m |          -0.0089 |          30.5581 |        -193.3871 |
[32m[20221213 12:42:08 @agent_ppo2.py:179][0m |          -0.0190 |          30.1018 |        -198.8810 |
[32m[20221213 12:42:09 @agent_ppo2.py:179][0m |          -0.0255 |          29.6045 |        -205.1826 |
[32m[20221213 12:42:09 @agent_ppo2.py:179][0m |          -0.0371 |          29.2344 |        -210.4377 |
[32m[20221213 12:42:09 @agent_ppo2.py:179][0m |          -0.0381 |          28.9481 |        -217.2336 |
[32m[20221213 12:42:09 @agent_ppo2.py:179][0m |          -0.0463 |          29.6831 |        -223.3891 |
[32m[20221213 12:42:09 @agent_ppo2.py:179][0m |          -0.0509 |          28.4069 |        -226.0702 |
[32m[20221213 12:42:09 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:42:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 292.92
[32m[20221213 12:42:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.71
[32m[20221213 12:42:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 11.01
[32m[20221213 12:42:09 @agent_ppo2.py:137][0m Total time:      26.21 min
[32m[20221213 12:42:09 @agent_ppo2.py:139][0m 1777664 total steps have happened
[32m[20221213 12:42:09 @agent_ppo2.py:115][0m #------------------------ Iteration 868 --------------------------#
[32m[20221213 12:42:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |           0.0842 |          32.7031 |        -212.7692 |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |           0.0709 |          30.8853 |        -158.9961 |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |           0.0274 |          30.1726 |        -189.0520 |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |           0.0084 |          32.1180 |        -205.4478 |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |          -0.0095 |          29.6799 |        -216.1880 |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |          -0.0183 |          29.3385 |        -220.6725 |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |          -0.0151 |          31.0991 |        -225.4837 |
[32m[20221213 12:42:10 @agent_ppo2.py:179][0m |          -0.0216 |          29.1669 |        -228.8331 |
[32m[20221213 12:42:11 @agent_ppo2.py:179][0m |          -0.0294 |          29.0909 |        -234.8214 |
[32m[20221213 12:42:11 @agent_ppo2.py:179][0m |          -0.0260 |          28.7819 |        -238.5549 |
[32m[20221213 12:42:11 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:42:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.06
[32m[20221213 12:42:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.47
[32m[20221213 12:42:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.86
[32m[20221213 12:42:11 @agent_ppo2.py:137][0m Total time:      26.24 min
[32m[20221213 12:42:11 @agent_ppo2.py:139][0m 1779712 total steps have happened
[32m[20221213 12:42:11 @agent_ppo2.py:115][0m #------------------------ Iteration 869 --------------------------#
[32m[20221213 12:42:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:11 @agent_ppo2.py:179][0m |           0.0730 |          28.9986 |        -219.2067 |
[32m[20221213 12:42:11 @agent_ppo2.py:179][0m |           0.0535 |          26.6572 |        -204.3286 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |           0.0038 |          25.2623 |        -222.5635 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |          -0.0181 |          24.3362 |        -228.5325 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |          -0.0264 |          23.6907 |        -235.9960 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |          -0.0397 |          23.2258 |        -242.2271 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |          -0.0205 |          23.8804 |        -243.4909 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |          -0.0456 |          22.4196 |        -250.1737 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |          -0.0469 |          22.0893 |        -252.7897 |
[32m[20221213 12:42:12 @agent_ppo2.py:179][0m |          -0.0562 |          21.9099 |        -258.1381 |
[32m[20221213 12:42:12 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:42:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.10
[32m[20221213 12:42:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.98
[32m[20221213 12:42:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.32
[32m[20221213 12:42:13 @agent_ppo2.py:137][0m Total time:      26.26 min
[32m[20221213 12:42:13 @agent_ppo2.py:139][0m 1781760 total steps have happened
[32m[20221213 12:42:13 @agent_ppo2.py:115][0m #------------------------ Iteration 870 --------------------------#
[32m[20221213 12:42:13 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:42:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:13 @agent_ppo2.py:179][0m |           0.1039 |          17.3778 |        -151.4806 |
[32m[20221213 12:42:13 @agent_ppo2.py:179][0m |           0.1013 |          16.0292 |         -91.0260 |
[32m[20221213 12:42:13 @agent_ppo2.py:179][0m |           0.0150 |          15.6198 |        -122.6523 |
[32m[20221213 12:42:13 @agent_ppo2.py:179][0m |          -0.0160 |          15.4052 |        -139.8120 |
[32m[20221213 12:42:14 @agent_ppo2.py:179][0m |          -0.0239 |          15.4641 |        -152.0597 |
[32m[20221213 12:42:14 @agent_ppo2.py:179][0m |          -0.0367 |          15.1571 |        -159.2281 |
[32m[20221213 12:42:14 @agent_ppo2.py:179][0m |          -0.0453 |          14.9381 |        -163.8422 |
[32m[20221213 12:42:14 @agent_ppo2.py:179][0m |          -0.0523 |          14.9052 |        -170.9301 |
[32m[20221213 12:42:14 @agent_ppo2.py:179][0m |          -0.0502 |          14.8117 |        -172.3596 |
[32m[20221213 12:42:14 @agent_ppo2.py:179][0m |          -0.0444 |          14.7453 |        -171.3645 |
[32m[20221213 12:42:14 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:42:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 155.13
[32m[20221213 12:42:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.05
[32m[20221213 12:42:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 299.09
[32m[20221213 12:42:14 @agent_ppo2.py:137][0m Total time:      26.29 min
[32m[20221213 12:42:14 @agent_ppo2.py:139][0m 1783808 total steps have happened
[32m[20221213 12:42:14 @agent_ppo2.py:115][0m #------------------------ Iteration 871 --------------------------#
[32m[20221213 12:42:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:15 @agent_ppo2.py:179][0m |           0.0829 |          29.4882 |        -203.0748 |
[32m[20221213 12:42:15 @agent_ppo2.py:179][0m |           0.0361 |          26.6251 |        -174.4892 |
[32m[20221213 12:42:15 @agent_ppo2.py:179][0m |          -0.0038 |          25.6005 |        -195.2810 |
[32m[20221213 12:42:15 @agent_ppo2.py:179][0m |          -0.0277 |          24.9014 |        -205.8447 |
[32m[20221213 12:42:15 @agent_ppo2.py:179][0m |          -0.0371 |          24.3863 |        -214.6489 |
[32m[20221213 12:42:15 @agent_ppo2.py:179][0m |          -0.0497 |          23.9054 |        -222.4508 |
[32m[20221213 12:42:15 @agent_ppo2.py:179][0m |          -0.0441 |          23.6424 |        -219.4152 |
[32m[20221213 12:42:16 @agent_ppo2.py:179][0m |          -0.0529 |          23.2964 |        -230.3931 |
[32m[20221213 12:42:16 @agent_ppo2.py:179][0m |          -0.0578 |          22.9852 |        -236.1157 |
[32m[20221213 12:42:16 @agent_ppo2.py:179][0m |          -0.0538 |          23.4011 |        -240.2714 |
[32m[20221213 12:42:16 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:42:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 241.69
[32m[20221213 12:42:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.09
[32m[20221213 12:42:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 376.00
[32m[20221213 12:42:16 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 376.00
[32m[20221213 12:42:16 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 376.00
[32m[20221213 12:42:16 @agent_ppo2.py:137][0m Total time:      26.32 min
[32m[20221213 12:42:16 @agent_ppo2.py:139][0m 1785856 total steps have happened
[32m[20221213 12:42:16 @agent_ppo2.py:115][0m #------------------------ Iteration 872 --------------------------#
[32m[20221213 12:42:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:16 @agent_ppo2.py:179][0m |           0.0895 |          34.0828 |        -210.2774 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |           0.0546 |          30.3770 |        -200.2018 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |           0.0322 |          29.3725 |        -220.5536 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |           0.0062 |          29.1565 |        -239.3138 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |          -0.0132 |          28.3634 |        -252.3084 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |          -0.0287 |          28.0466 |        -266.7657 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |          -0.0306 |          27.7428 |        -272.7447 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |          -0.0361 |          27.5949 |        -274.8206 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |          -0.0337 |          27.3976 |        -281.1626 |
[32m[20221213 12:42:17 @agent_ppo2.py:179][0m |          -0.0231 |          27.8705 |        -280.7578 |
[32m[20221213 12:42:17 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:42:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.15
[32m[20221213 12:42:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.83
[32m[20221213 12:42:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.61
[32m[20221213 12:42:18 @agent_ppo2.py:137][0m Total time:      26.35 min
[32m[20221213 12:42:18 @agent_ppo2.py:139][0m 1787904 total steps have happened
[32m[20221213 12:42:18 @agent_ppo2.py:115][0m #------------------------ Iteration 873 --------------------------#
[32m[20221213 12:42:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:18 @agent_ppo2.py:179][0m |           0.1042 |          34.1148 |        -218.2225 |
[32m[20221213 12:42:18 @agent_ppo2.py:179][0m |           0.1242 |          33.7850 |        -158.9404 |
[32m[20221213 12:42:18 @agent_ppo2.py:179][0m |           0.0817 |          32.4838 |        -165.1082 |
[32m[20221213 12:42:18 @agent_ppo2.py:179][0m |           0.0311 |          31.9944 |        -185.9049 |
[32m[20221213 12:42:19 @agent_ppo2.py:179][0m |           0.0075 |          31.6968 |        -219.7998 |
[32m[20221213 12:42:19 @agent_ppo2.py:179][0m |          -0.0005 |          31.5097 |        -234.6683 |
[32m[20221213 12:42:19 @agent_ppo2.py:179][0m |          -0.0070 |          31.3318 |        -240.3831 |
[32m[20221213 12:42:19 @agent_ppo2.py:179][0m |          -0.0168 |          31.1922 |        -250.5099 |
[32m[20221213 12:42:19 @agent_ppo2.py:179][0m |          -0.0267 |          31.0520 |        -259.9737 |
[32m[20221213 12:42:19 @agent_ppo2.py:179][0m |          -0.0302 |          31.0143 |        -263.6708 |
[32m[20221213 12:42:19 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:42:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.16
[32m[20221213 12:42:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.67
[32m[20221213 12:42:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.31
[32m[20221213 12:42:19 @agent_ppo2.py:137][0m Total time:      26.38 min
[32m[20221213 12:42:19 @agent_ppo2.py:139][0m 1789952 total steps have happened
[32m[20221213 12:42:19 @agent_ppo2.py:115][0m #------------------------ Iteration 874 --------------------------#
[32m[20221213 12:42:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:20 @agent_ppo2.py:179][0m |           0.1531 |          33.6323 |        -196.4004 |
[32m[20221213 12:42:20 @agent_ppo2.py:179][0m |           0.0907 |          32.4065 |        -143.9704 |
[32m[20221213 12:42:20 @agent_ppo2.py:179][0m |           0.0517 |          31.8230 |        -168.9630 |
[32m[20221213 12:42:20 @agent_ppo2.py:179][0m |           0.0183 |          31.4713 |        -189.8319 |
[32m[20221213 12:42:20 @agent_ppo2.py:179][0m |           0.0013 |          31.2588 |        -203.9569 |
[32m[20221213 12:42:20 @agent_ppo2.py:179][0m |          -0.0009 |          31.8814 |        -222.0995 |
[32m[20221213 12:42:21 @agent_ppo2.py:179][0m |          -0.0112 |          31.9098 |        -227.3346 |
[32m[20221213 12:42:21 @agent_ppo2.py:179][0m |          -0.0236 |          30.7957 |        -238.6074 |
[32m[20221213 12:42:21 @agent_ppo2.py:179][0m |          -0.0258 |          30.6453 |        -243.4548 |
[32m[20221213 12:42:21 @agent_ppo2.py:179][0m |          -0.0256 |          30.5185 |        -246.9858 |
[32m[20221213 12:42:21 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:42:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.82
[32m[20221213 12:42:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.18
[32m[20221213 12:42:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.56
[32m[20221213 12:42:21 @agent_ppo2.py:137][0m Total time:      26.41 min
[32m[20221213 12:42:21 @agent_ppo2.py:139][0m 1792000 total steps have happened
[32m[20221213 12:42:21 @agent_ppo2.py:115][0m #------------------------ Iteration 875 --------------------------#
[32m[20221213 12:42:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |           0.0655 |          32.1297 |        -214.1234 |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |           0.0958 |          31.7994 |        -149.6866 |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |           0.0478 |          31.1190 |        -183.7914 |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |           0.0089 |          30.8684 |        -201.7859 |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |           0.0019 |          33.4327 |        -219.8527 |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |          -0.0192 |          30.6243 |        -227.3844 |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |          -0.0237 |          30.4454 |        -226.8498 |
[32m[20221213 12:42:22 @agent_ppo2.py:179][0m |          -0.0296 |          30.3648 |        -231.5280 |
[32m[20221213 12:42:23 @agent_ppo2.py:179][0m |          -0.0328 |          30.2315 |        -239.3012 |
[32m[20221213 12:42:23 @agent_ppo2.py:179][0m |          -0.0318 |          30.1618 |        -241.6774 |
[32m[20221213 12:42:23 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:42:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.92
[32m[20221213 12:42:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.24
[32m[20221213 12:42:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 193.18
[32m[20221213 12:42:23 @agent_ppo2.py:137][0m Total time:      26.44 min
[32m[20221213 12:42:23 @agent_ppo2.py:139][0m 1794048 total steps have happened
[32m[20221213 12:42:23 @agent_ppo2.py:115][0m #------------------------ Iteration 876 --------------------------#
[32m[20221213 12:42:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:23 @agent_ppo2.py:179][0m |           0.0599 |          27.1291 |        -208.1613 |
[32m[20221213 12:42:23 @agent_ppo2.py:179][0m |           0.0595 |          25.9516 |        -184.4538 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |           0.0162 |          28.2337 |        -204.5611 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |          -0.0039 |          25.0698 |        -208.0109 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |          -0.0209 |          24.7677 |        -220.2270 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |          -0.0211 |          24.5683 |        -224.8677 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |          -0.0311 |          24.3737 |        -232.5681 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |          -0.0280 |          25.8239 |        -235.8537 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |          -0.0439 |          24.0843 |        -241.5396 |
[32m[20221213 12:42:24 @agent_ppo2.py:179][0m |          -0.0439 |          23.9465 |        -247.4281 |
[32m[20221213 12:42:24 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:42:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 262.49
[32m[20221213 12:42:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.36
[32m[20221213 12:42:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.43
[32m[20221213 12:42:25 @agent_ppo2.py:137][0m Total time:      26.46 min
[32m[20221213 12:42:25 @agent_ppo2.py:139][0m 1796096 total steps have happened
[32m[20221213 12:42:25 @agent_ppo2.py:115][0m #------------------------ Iteration 877 --------------------------#
[32m[20221213 12:42:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:25 @agent_ppo2.py:179][0m |           0.0894 |          28.4817 |        -214.7832 |
[32m[20221213 12:42:25 @agent_ppo2.py:179][0m |           0.0567 |          26.8250 |        -190.5243 |
[32m[20221213 12:42:25 @agent_ppo2.py:179][0m |           0.0226 |          26.0257 |        -212.7748 |
[32m[20221213 12:42:25 @agent_ppo2.py:179][0m |           0.0075 |          25.3948 |        -220.8069 |
[32m[20221213 12:42:25 @agent_ppo2.py:179][0m |          -0.0172 |          24.9932 |        -229.9684 |
[32m[20221213 12:42:26 @agent_ppo2.py:179][0m |          -0.0252 |          24.7309 |        -237.4719 |
[32m[20221213 12:42:26 @agent_ppo2.py:179][0m |          -0.0346 |          24.4102 |        -245.4705 |
[32m[20221213 12:42:26 @agent_ppo2.py:179][0m |          -0.0365 |          24.4916 |        -252.4476 |
[32m[20221213 12:42:26 @agent_ppo2.py:179][0m |          -0.0351 |          24.0082 |        -251.6819 |
[32m[20221213 12:42:26 @agent_ppo2.py:179][0m |          -0.0301 |          24.0455 |        -252.2020 |
[32m[20221213 12:42:26 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:42:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.37
[32m[20221213 12:42:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.26
[32m[20221213 12:42:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.12
[32m[20221213 12:42:26 @agent_ppo2.py:137][0m Total time:      26.49 min
[32m[20221213 12:42:26 @agent_ppo2.py:139][0m 1798144 total steps have happened
[32m[20221213 12:42:26 @agent_ppo2.py:115][0m #------------------------ Iteration 878 --------------------------#
[32m[20221213 12:42:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |           0.1146 |          26.8873 |        -215.4778 |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |           0.0591 |          24.3881 |        -191.9101 |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |           0.0044 |          23.1889 |        -225.1185 |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |          -0.0147 |          22.6892 |        -238.8213 |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |          -0.0116 |          23.7729 |        -241.4293 |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |          -0.0266 |          20.8547 |        -247.8950 |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |          -0.0081 |          20.3232 |        -240.9508 |
[32m[20221213 12:42:27 @agent_ppo2.py:179][0m |          -0.0323 |          19.9506 |        -254.9064 |
[32m[20221213 12:42:28 @agent_ppo2.py:179][0m |          -0.0361 |          19.5841 |        -260.9871 |
[32m[20221213 12:42:28 @agent_ppo2.py:179][0m |          -0.0430 |          19.3145 |        -264.8794 |
[32m[20221213 12:42:28 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:42:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 273.16
[32m[20221213 12:42:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.60
[32m[20221213 12:42:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.36
[32m[20221213 12:42:28 @agent_ppo2.py:137][0m Total time:      26.52 min
[32m[20221213 12:42:28 @agent_ppo2.py:139][0m 1800192 total steps have happened
[32m[20221213 12:42:28 @agent_ppo2.py:115][0m #------------------------ Iteration 879 --------------------------#
[32m[20221213 12:42:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:28 @agent_ppo2.py:179][0m |           0.1316 |          36.7249 |        -195.1643 |
[32m[20221213 12:42:28 @agent_ppo2.py:179][0m |           0.0735 |          34.5343 |        -162.4287 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |           0.0328 |          33.7584 |        -184.1539 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |           0.0094 |          34.5070 |        -205.3778 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |           0.0043 |          32.9068 |        -200.8255 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |          -0.0066 |          34.8984 |        -219.3827 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |          -0.0192 |          32.5095 |        -216.4831 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |          -0.0252 |          33.4854 |        -229.1395 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |          -0.0373 |          32.0503 |        -239.1935 |
[32m[20221213 12:42:29 @agent_ppo2.py:179][0m |          -0.0432 |          31.7706 |        -243.0921 |
[32m[20221213 12:42:29 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:42:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.07
[32m[20221213 12:42:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.91
[32m[20221213 12:42:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.14
[32m[20221213 12:42:30 @agent_ppo2.py:137][0m Total time:      26.55 min
[32m[20221213 12:42:30 @agent_ppo2.py:139][0m 1802240 total steps have happened
[32m[20221213 12:42:30 @agent_ppo2.py:115][0m #------------------------ Iteration 880 --------------------------#
[32m[20221213 12:42:30 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:42:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:30 @agent_ppo2.py:179][0m |           0.1074 |          23.2307 |        -211.9200 |
[32m[20221213 12:42:30 @agent_ppo2.py:179][0m |           0.0702 |          21.0719 |        -176.3966 |
[32m[20221213 12:42:30 @agent_ppo2.py:179][0m |           0.0150 |          19.8164 |        -198.8025 |
[32m[20221213 12:42:30 @agent_ppo2.py:179][0m |          -0.0176 |          19.1115 |        -210.5020 |
[32m[20221213 12:42:31 @agent_ppo2.py:179][0m |          -0.0355 |          18.4410 |        -218.3956 |
[32m[20221213 12:42:31 @agent_ppo2.py:179][0m |          -0.0330 |          18.3360 |        -224.7306 |
[32m[20221213 12:42:31 @agent_ppo2.py:179][0m |          -0.0417 |          18.3642 |        -228.3458 |
[32m[20221213 12:42:31 @agent_ppo2.py:179][0m |          -0.0582 |          17.2340 |        -234.4981 |
[32m[20221213 12:42:31 @agent_ppo2.py:179][0m |          -0.0601 |          16.9310 |        -238.5418 |
[32m[20221213 12:42:31 @agent_ppo2.py:179][0m |          -0.0555 |          16.6516 |        -242.0856 |
[32m[20221213 12:42:31 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:42:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 233.14
[32m[20221213 12:42:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.36
[32m[20221213 12:42:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.58
[32m[20221213 12:42:31 @agent_ppo2.py:137][0m Total time:      26.58 min
[32m[20221213 12:42:31 @agent_ppo2.py:139][0m 1804288 total steps have happened
[32m[20221213 12:42:31 @agent_ppo2.py:115][0m #------------------------ Iteration 881 --------------------------#
[32m[20221213 12:42:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:32 @agent_ppo2.py:179][0m |           0.1152 |           3.1904 |        -180.4591 |
[32m[20221213 12:42:32 @agent_ppo2.py:179][0m |           0.0432 |           2.5402 |        -184.9112 |
[32m[20221213 12:42:32 @agent_ppo2.py:179][0m |           0.0256 |           2.4317 |        -189.2240 |
[32m[20221213 12:42:32 @agent_ppo2.py:179][0m |           0.0222 |           2.3992 |        -185.9618 |
[32m[20221213 12:42:32 @agent_ppo2.py:179][0m |           0.0234 |           2.3751 |        -188.2898 |
[32m[20221213 12:42:32 @agent_ppo2.py:179][0m |           0.0180 |           2.3681 |        -189.3999 |
[32m[20221213 12:42:33 @agent_ppo2.py:179][0m |           0.0138 |           2.3541 |        -188.3093 |
[32m[20221213 12:42:33 @agent_ppo2.py:179][0m |           0.0184 |           2.3453 |        -191.2182 |
[32m[20221213 12:42:33 @agent_ppo2.py:179][0m |           0.0182 |           2.4467 |        -191.6027 |
[32m[20221213 12:42:33 @agent_ppo2.py:179][0m |           0.0141 |           2.3481 |        -195.7827 |
[32m[20221213 12:42:33 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:42:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 23.43
[32m[20221213 12:42:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.52
[32m[20221213 12:42:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.68
[32m[20221213 12:42:33 @agent_ppo2.py:137][0m Total time:      26.61 min
[32m[20221213 12:42:33 @agent_ppo2.py:139][0m 1806336 total steps have happened
[32m[20221213 12:42:33 @agent_ppo2.py:115][0m #------------------------ Iteration 882 --------------------------#
[32m[20221213 12:42:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:33 @agent_ppo2.py:179][0m |           0.1000 |          30.9809 |        -225.6809 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |           0.0367 |          28.0063 |        -206.0385 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |          -0.0070 |          26.8485 |        -234.8476 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |          -0.0211 |          26.8244 |        -244.0441 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |          -0.0312 |          25.9671 |        -245.1532 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |          -0.0382 |          25.4891 |        -253.3083 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |          -0.0441 |          25.2413 |        -262.7723 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |          -0.0390 |          27.4662 |        -265.2196 |
[32m[20221213 12:42:34 @agent_ppo2.py:179][0m |          -0.0457 |          24.6848 |        -271.2356 |
[32m[20221213 12:42:35 @agent_ppo2.py:179][0m |          -0.0502 |          24.4450 |        -270.6815 |
[32m[20221213 12:42:35 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:42:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.41
[32m[20221213 12:42:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.68
[32m[20221213 12:42:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.44
[32m[20221213 12:42:35 @agent_ppo2.py:137][0m Total time:      26.63 min
[32m[20221213 12:42:35 @agent_ppo2.py:139][0m 1808384 total steps have happened
[32m[20221213 12:42:35 @agent_ppo2.py:115][0m #------------------------ Iteration 883 --------------------------#
[32m[20221213 12:42:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:35 @agent_ppo2.py:179][0m |           0.0888 |          24.2650 |        -200.6214 |
[32m[20221213 12:42:35 @agent_ppo2.py:179][0m |           0.1058 |          24.0745 |        -186.2033 |
[32m[20221213 12:42:35 @agent_ppo2.py:179][0m |           0.0226 |          20.6220 |        -196.4115 |
[32m[20221213 12:42:36 @agent_ppo2.py:179][0m |          -0.0155 |          19.2931 |        -210.8177 |
[32m[20221213 12:42:36 @agent_ppo2.py:179][0m |          -0.0263 |          18.7010 |        -215.0501 |
[32m[20221213 12:42:36 @agent_ppo2.py:179][0m |          -0.0350 |          18.0641 |        -220.5468 |
[32m[20221213 12:42:36 @agent_ppo2.py:179][0m |          -0.0338 |          17.9708 |        -222.7692 |
[32m[20221213 12:42:36 @agent_ppo2.py:179][0m |          -0.0296 |          17.1774 |        -220.7136 |
[32m[20221213 12:42:36 @agent_ppo2.py:179][0m |          -0.0440 |          16.7692 |        -227.9162 |
[32m[20221213 12:42:36 @agent_ppo2.py:179][0m |          -0.0491 |          16.4098 |        -234.8031 |
[32m[20221213 12:42:36 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:42:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.82
[32m[20221213 12:42:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.26
[32m[20221213 12:42:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 284.26
[32m[20221213 12:42:37 @agent_ppo2.py:137][0m Total time:      26.66 min
[32m[20221213 12:42:37 @agent_ppo2.py:139][0m 1810432 total steps have happened
[32m[20221213 12:42:37 @agent_ppo2.py:115][0m #------------------------ Iteration 884 --------------------------#
[32m[20221213 12:42:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:37 @agent_ppo2.py:179][0m |           0.0621 |          11.2121 |        -238.9527 |
[32m[20221213 12:42:37 @agent_ppo2.py:179][0m |           0.0462 |           9.5762 |        -183.7384 |
[32m[20221213 12:42:37 @agent_ppo2.py:179][0m |           0.0217 |           9.2714 |        -168.0401 |
[32m[20221213 12:42:37 @agent_ppo2.py:179][0m |          -0.0027 |           8.7910 |        -177.1712 |
[32m[20221213 12:42:37 @agent_ppo2.py:179][0m |          -0.0169 |           8.5529 |        -193.6041 |
[32m[20221213 12:42:38 @agent_ppo2.py:179][0m |          -0.0246 |           8.4577 |        -195.8337 |
[32m[20221213 12:42:38 @agent_ppo2.py:179][0m |          -0.0404 |           8.2973 |        -202.3211 |
[32m[20221213 12:42:38 @agent_ppo2.py:179][0m |          -0.0272 |           8.1785 |        -204.1018 |
[32m[20221213 12:42:38 @agent_ppo2.py:179][0m |          -0.0328 |           8.0800 |        -207.9255 |
[32m[20221213 12:42:38 @agent_ppo2.py:179][0m |          -0.0357 |           8.0700 |        -210.1542 |
[32m[20221213 12:42:38 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:42:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 82.94
[32m[20221213 12:42:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.09
[32m[20221213 12:42:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.59
[32m[20221213 12:42:38 @agent_ppo2.py:137][0m Total time:      26.69 min
[32m[20221213 12:42:38 @agent_ppo2.py:139][0m 1812480 total steps have happened
[32m[20221213 12:42:38 @agent_ppo2.py:115][0m #------------------------ Iteration 885 --------------------------#
[32m[20221213 12:42:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |           0.0859 |          35.8316 |        -208.1373 |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |           0.0569 |          32.4053 |        -178.2493 |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |           0.0200 |          31.1701 |        -198.9290 |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |          -0.0091 |          30.3527 |        -208.9247 |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |          -0.0245 |          29.9056 |        -219.0521 |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |          -0.0337 |          29.4153 |        -227.2930 |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |          -0.0417 |          29.1609 |        -231.3694 |
[32m[20221213 12:42:39 @agent_ppo2.py:179][0m |          -0.0425 |          28.8392 |        -231.3509 |
[32m[20221213 12:42:40 @agent_ppo2.py:179][0m |          -0.0487 |          28.5555 |        -233.9547 |
[32m[20221213 12:42:40 @agent_ppo2.py:179][0m |          -0.0513 |          28.4488 |        -240.3828 |
[32m[20221213 12:42:40 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:42:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.56
[32m[20221213 12:42:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.36
[32m[20221213 12:42:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.61
[32m[20221213 12:42:40 @agent_ppo2.py:137][0m Total time:      26.72 min
[32m[20221213 12:42:40 @agent_ppo2.py:139][0m 1814528 total steps have happened
[32m[20221213 12:42:40 @agent_ppo2.py:115][0m #------------------------ Iteration 886 --------------------------#
[32m[20221213 12:42:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:40 @agent_ppo2.py:179][0m |           0.0876 |          21.9866 |        -216.1227 |
[32m[20221213 12:42:40 @agent_ppo2.py:179][0m |           0.0684 |          19.3417 |        -170.6039 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |           0.0260 |          18.2561 |        -197.2334 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |          -0.0001 |          18.0307 |        -214.0417 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |          -0.0180 |          17.5163 |        -220.9994 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |          -0.0371 |          16.8245 |        -230.0986 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |          -0.0447 |          16.4923 |        -236.7941 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |          -0.0489 |          16.3755 |        -238.6859 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |          -0.0471 |          16.0165 |        -246.0310 |
[32m[20221213 12:42:41 @agent_ppo2.py:179][0m |          -0.0563 |          15.8616 |        -250.8995 |
[32m[20221213 12:42:41 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:42:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 210.72
[32m[20221213 12:42:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.32
[32m[20221213 12:42:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.06
[32m[20221213 12:42:42 @agent_ppo2.py:137][0m Total time:      26.75 min
[32m[20221213 12:42:42 @agent_ppo2.py:139][0m 1816576 total steps have happened
[32m[20221213 12:42:42 @agent_ppo2.py:115][0m #------------------------ Iteration 887 --------------------------#
[32m[20221213 12:42:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:42 @agent_ppo2.py:179][0m |           0.0891 |          31.1577 |        -210.0328 |
[32m[20221213 12:42:42 @agent_ppo2.py:179][0m |           0.0785 |          29.4073 |        -163.5246 |
[32m[20221213 12:42:42 @agent_ppo2.py:179][0m |           0.0465 |          28.6633 |        -172.7580 |
[32m[20221213 12:42:42 @agent_ppo2.py:179][0m |           0.0072 |          28.1549 |        -199.0412 |
[32m[20221213 12:42:43 @agent_ppo2.py:179][0m |          -0.0045 |          27.8216 |        -211.5726 |
[32m[20221213 12:42:43 @agent_ppo2.py:179][0m |          -0.0206 |          27.5896 |        -220.2374 |
[32m[20221213 12:42:43 @agent_ppo2.py:179][0m |          -0.0211 |          27.1470 |        -229.2922 |
[32m[20221213 12:42:43 @agent_ppo2.py:179][0m |          -0.0330 |          26.9916 |        -237.7415 |
[32m[20221213 12:42:43 @agent_ppo2.py:179][0m |          -0.0417 |          26.7479 |        -242.7171 |
[32m[20221213 12:42:43 @agent_ppo2.py:179][0m |          -0.0360 |          26.5118 |        -241.4259 |
[32m[20221213 12:42:43 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:42:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.27
[32m[20221213 12:42:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.17
[32m[20221213 12:42:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 372.81
[32m[20221213 12:42:43 @agent_ppo2.py:137][0m Total time:      26.78 min
[32m[20221213 12:42:43 @agent_ppo2.py:139][0m 1818624 total steps have happened
[32m[20221213 12:42:43 @agent_ppo2.py:115][0m #------------------------ Iteration 888 --------------------------#
[32m[20221213 12:42:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:44 @agent_ppo2.py:179][0m |           0.1469 |          33.5130 |        -199.3507 |
[32m[20221213 12:42:44 @agent_ppo2.py:179][0m |           0.0717 |          27.9302 |        -167.1102 |
[32m[20221213 12:42:44 @agent_ppo2.py:179][0m |           0.0020 |          26.4442 |        -190.2538 |
[32m[20221213 12:42:44 @agent_ppo2.py:179][0m |          -0.0077 |          25.4192 |        -199.8904 |
[32m[20221213 12:42:44 @agent_ppo2.py:179][0m |          -0.0255 |          24.5821 |        -216.2507 |
[32m[20221213 12:42:44 @agent_ppo2.py:179][0m |          -0.0418 |          23.9830 |        -222.9279 |
[32m[20221213 12:42:45 @agent_ppo2.py:179][0m |          -0.0467 |          23.4224 |        -226.5648 |
[32m[20221213 12:42:45 @agent_ppo2.py:179][0m |          -0.0532 |          22.9264 |        -231.0999 |
[32m[20221213 12:42:45 @agent_ppo2.py:179][0m |          -0.0569 |          22.5691 |        -236.0937 |
[32m[20221213 12:42:45 @agent_ppo2.py:179][0m |          -0.0596 |          22.2407 |        -240.4682 |
[32m[20221213 12:42:45 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:42:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 256.70
[32m[20221213 12:42:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.03
[32m[20221213 12:42:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.53
[32m[20221213 12:42:45 @agent_ppo2.py:137][0m Total time:      26.81 min
[32m[20221213 12:42:45 @agent_ppo2.py:139][0m 1820672 total steps have happened
[32m[20221213 12:42:45 @agent_ppo2.py:115][0m #------------------------ Iteration 889 --------------------------#
[32m[20221213 12:42:45 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:42:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |           0.0809 |          32.4718 |        -190.4574 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |           0.0959 |          27.5150 |        -122.3913 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |           0.0216 |          26.2587 |        -132.3958 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |          -0.0034 |          25.0549 |        -147.2486 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |          -0.0290 |          24.2926 |        -159.5530 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |          -0.0438 |          23.7061 |        -169.2455 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |          -0.0457 |          23.2036 |        -173.8091 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |          -0.0520 |          23.1950 |        -177.4750 |
[32m[20221213 12:42:46 @agent_ppo2.py:179][0m |          -0.0502 |          22.5281 |        -179.9765 |
[32m[20221213 12:42:47 @agent_ppo2.py:179][0m |          -0.0517 |          22.9766 |        -182.3035 |
[32m[20221213 12:42:47 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:42:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 221.15
[32m[20221213 12:42:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.38
[32m[20221213 12:42:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.16
[32m[20221213 12:42:47 @agent_ppo2.py:137][0m Total time:      26.83 min
[32m[20221213 12:42:47 @agent_ppo2.py:139][0m 1822720 total steps have happened
[32m[20221213 12:42:47 @agent_ppo2.py:115][0m #------------------------ Iteration 890 --------------------------#
[32m[20221213 12:42:47 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:42:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:47 @agent_ppo2.py:179][0m |           0.1028 |          19.6452 |        -185.7508 |
[32m[20221213 12:42:47 @agent_ppo2.py:179][0m |           0.0751 |          17.5226 |        -150.8123 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |           0.0101 |          16.2664 |        -172.4830 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |          -0.0142 |          15.7400 |        -180.9709 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |          -0.0313 |          15.8008 |        -192.6832 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |          -0.0386 |          15.2750 |        -197.0188 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |          -0.0527 |          14.6246 |        -201.7352 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |          -0.0502 |          14.4516 |        -206.3536 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |          -0.0524 |          14.1372 |        -205.5644 |
[32m[20221213 12:42:48 @agent_ppo2.py:179][0m |          -0.0598 |          13.9476 |        -212.7577 |
[32m[20221213 12:42:48 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:42:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 227.08
[32m[20221213 12:42:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.52
[32m[20221213 12:42:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 371.19
[32m[20221213 12:42:49 @agent_ppo2.py:137][0m Total time:      26.86 min
[32m[20221213 12:42:49 @agent_ppo2.py:139][0m 1824768 total steps have happened
[32m[20221213 12:42:49 @agent_ppo2.py:115][0m #------------------------ Iteration 891 --------------------------#
[32m[20221213 12:42:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:49 @agent_ppo2.py:179][0m |           0.0771 |          36.6037 |        -212.7380 |
[32m[20221213 12:42:49 @agent_ppo2.py:179][0m |           0.0325 |          33.9489 |        -204.0148 |
[32m[20221213 12:42:49 @agent_ppo2.py:179][0m |          -0.0089 |          32.7630 |        -215.3069 |
[32m[20221213 12:42:49 @agent_ppo2.py:179][0m |          -0.0296 |          31.9728 |        -221.7144 |
[32m[20221213 12:42:49 @agent_ppo2.py:179][0m |          -0.0380 |          31.5669 |        -228.9836 |
[32m[20221213 12:42:50 @agent_ppo2.py:179][0m |          -0.0443 |          31.3968 |        -233.6376 |
[32m[20221213 12:42:50 @agent_ppo2.py:179][0m |          -0.0373 |          32.1940 |        -232.2741 |
[32m[20221213 12:42:50 @agent_ppo2.py:179][0m |          -0.0354 |          30.6292 |        -230.8175 |
[32m[20221213 12:42:50 @agent_ppo2.py:179][0m |          -0.0540 |          30.2229 |        -235.5449 |
[32m[20221213 12:42:50 @agent_ppo2.py:179][0m |          -0.0618 |          29.9472 |        -240.4033 |
[32m[20221213 12:42:50 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:42:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 292.13
[32m[20221213 12:42:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.88
[32m[20221213 12:42:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 384.62
[32m[20221213 12:42:50 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 384.62
[32m[20221213 12:42:50 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 384.62
[32m[20221213 12:42:50 @agent_ppo2.py:137][0m Total time:      26.89 min
[32m[20221213 12:42:50 @agent_ppo2.py:139][0m 1826816 total steps have happened
[32m[20221213 12:42:50 @agent_ppo2.py:115][0m #------------------------ Iteration 892 --------------------------#
[32m[20221213 12:42:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:51 @agent_ppo2.py:179][0m |           0.0481 |          18.6652 |        -177.3346 |
[32m[20221213 12:42:51 @agent_ppo2.py:179][0m |           0.0088 |          16.8422 |        -133.3749 |
[32m[20221213 12:42:51 @agent_ppo2.py:179][0m |          -0.0022 |          15.9681 |        -137.0656 |
[32m[20221213 12:42:51 @agent_ppo2.py:179][0m |          -0.0233 |          15.3913 |        -139.7460 |
[32m[20221213 12:42:51 @agent_ppo2.py:179][0m |          -0.0393 |          14.9504 |        -155.0785 |
[32m[20221213 12:42:51 @agent_ppo2.py:179][0m |          -0.0476 |          14.6324 |        -158.6423 |
[32m[20221213 12:42:51 @agent_ppo2.py:179][0m |          -0.0484 |          14.3919 |        -161.1196 |
[32m[20221213 12:42:52 @agent_ppo2.py:179][0m |          -0.0531 |          14.1652 |        -167.1761 |
[32m[20221213 12:42:52 @agent_ppo2.py:179][0m |          -0.0575 |          13.9720 |        -168.4088 |
[32m[20221213 12:42:52 @agent_ppo2.py:179][0m |          -0.0523 |          13.8240 |        -170.0832 |
[32m[20221213 12:42:52 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:42:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.59
[32m[20221213 12:42:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.72
[32m[20221213 12:42:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 278.18
[32m[20221213 12:42:52 @agent_ppo2.py:137][0m Total time:      26.92 min
[32m[20221213 12:42:52 @agent_ppo2.py:139][0m 1828864 total steps have happened
[32m[20221213 12:42:52 @agent_ppo2.py:115][0m #------------------------ Iteration 893 --------------------------#
[32m[20221213 12:42:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:52 @agent_ppo2.py:179][0m |           0.0761 |          11.7096 |        -221.0570 |
[32m[20221213 12:42:52 @agent_ppo2.py:179][0m |           0.0277 |           9.2333 |        -215.0413 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0035 |           8.2621 |        -217.6472 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0315 |           7.7473 |        -231.8680 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0434 |           7.3457 |        -232.8976 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0491 |           7.0150 |        -234.8710 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0554 |           6.7322 |        -242.9817 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0547 |           6.5159 |        -239.1911 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0613 |           6.2622 |        -248.7294 |
[32m[20221213 12:42:53 @agent_ppo2.py:179][0m |          -0.0627 |           6.0156 |        -248.9416 |
[32m[20221213 12:42:53 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:42:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 126.96
[32m[20221213 12:42:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 283.57
[32m[20221213 12:42:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 301.61
[32m[20221213 12:42:54 @agent_ppo2.py:137][0m Total time:      26.95 min
[32m[20221213 12:42:54 @agent_ppo2.py:139][0m 1830912 total steps have happened
[32m[20221213 12:42:54 @agent_ppo2.py:115][0m #------------------------ Iteration 894 --------------------------#
[32m[20221213 12:42:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:54 @agent_ppo2.py:179][0m |           0.0981 |          19.8069 |        -100.8337 |
[32m[20221213 12:42:54 @agent_ppo2.py:179][0m |           0.0115 |          17.0988 |         -81.0578 |
[32m[20221213 12:42:54 @agent_ppo2.py:179][0m |          -0.0202 |          15.9178 |        -100.1676 |
[32m[20221213 12:42:54 @agent_ppo2.py:179][0m |          -0.0385 |          15.2506 |        -104.5318 |
[32m[20221213 12:42:54 @agent_ppo2.py:179][0m |          -0.0424 |          14.7203 |        -107.5974 |
[32m[20221213 12:42:55 @agent_ppo2.py:179][0m |          -0.0439 |          14.2680 |        -107.2100 |
[32m[20221213 12:42:55 @agent_ppo2.py:179][0m |          -0.0578 |          14.1589 |        -116.4243 |
[32m[20221213 12:42:55 @agent_ppo2.py:179][0m |          -0.0543 |          13.9168 |        -118.4037 |
[32m[20221213 12:42:55 @agent_ppo2.py:179][0m |          -0.0709 |          13.6621 |        -123.8222 |
[32m[20221213 12:42:55 @agent_ppo2.py:179][0m |          -0.0682 |          13.2744 |        -122.3745 |
[32m[20221213 12:42:55 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:42:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.98
[32m[20221213 12:42:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.78
[32m[20221213 12:42:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.37
[32m[20221213 12:42:55 @agent_ppo2.py:137][0m Total time:      26.98 min
[32m[20221213 12:42:55 @agent_ppo2.py:139][0m 1832960 total steps have happened
[32m[20221213 12:42:55 @agent_ppo2.py:115][0m #------------------------ Iteration 895 --------------------------#
[32m[20221213 12:42:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:56 @agent_ppo2.py:179][0m |           0.1002 |          33.5666 |        -199.2317 |
[32m[20221213 12:42:56 @agent_ppo2.py:179][0m |           0.0753 |          30.9766 |        -156.3855 |
[32m[20221213 12:42:56 @agent_ppo2.py:179][0m |           0.0303 |          29.9865 |        -195.5198 |
[32m[20221213 12:42:56 @agent_ppo2.py:179][0m |           0.0009 |          29.1993 |        -214.5300 |
[32m[20221213 12:42:56 @agent_ppo2.py:179][0m |          -0.0126 |          28.8038 |        -227.9319 |
[32m[20221213 12:42:56 @agent_ppo2.py:179][0m |          -0.0261 |          28.4099 |        -240.9395 |
[32m[20221213 12:42:56 @agent_ppo2.py:179][0m |          -0.0356 |          28.0709 |        -247.0602 |
[32m[20221213 12:42:57 @agent_ppo2.py:179][0m |          -0.0433 |          27.9294 |        -257.7311 |
[32m[20221213 12:42:57 @agent_ppo2.py:179][0m |          -0.0279 |          30.8851 |        -263.5489 |
[32m[20221213 12:42:57 @agent_ppo2.py:179][0m |          -0.0410 |          27.4085 |        -263.5260 |
[32m[20221213 12:42:57 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:42:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.61
[32m[20221213 12:42:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.70
[32m[20221213 12:42:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.04
[32m[20221213 12:42:57 @agent_ppo2.py:137][0m Total time:      27.00 min
[32m[20221213 12:42:57 @agent_ppo2.py:139][0m 1835008 total steps have happened
[32m[20221213 12:42:57 @agent_ppo2.py:115][0m #------------------------ Iteration 896 --------------------------#
[32m[20221213 12:42:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:57 @agent_ppo2.py:179][0m |           0.1333 |          23.7240 |        -208.1130 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |           0.1118 |          25.5779 |        -112.2454 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |           0.0471 |          21.7116 |        -143.3395 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |           0.0085 |          21.3186 |        -174.2602 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |          -0.0108 |          21.4066 |        -187.4889 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |          -0.0264 |          20.6860 |        -192.6715 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |          -0.0342 |          20.3075 |        -200.1095 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |          -0.0403 |          20.1806 |        -204.9531 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |          -0.0436 |          19.8622 |        -212.4310 |
[32m[20221213 12:42:58 @agent_ppo2.py:179][0m |          -0.0509 |          19.8364 |        -216.0454 |
[32m[20221213 12:42:58 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:42:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 220.48
[32m[20221213 12:42:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.78
[32m[20221213 12:42:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.09
[32m[20221213 12:42:59 @agent_ppo2.py:137][0m Total time:      27.03 min
[32m[20221213 12:42:59 @agent_ppo2.py:139][0m 1837056 total steps have happened
[32m[20221213 12:42:59 @agent_ppo2.py:115][0m #------------------------ Iteration 897 --------------------------#
[32m[20221213 12:42:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:42:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:42:59 @agent_ppo2.py:179][0m |           0.0735 |          10.8031 |        -207.3176 |
[32m[20221213 12:42:59 @agent_ppo2.py:179][0m |           0.0297 |           9.9765 |        -179.1431 |
[32m[20221213 12:42:59 @agent_ppo2.py:179][0m |          -0.0006 |           9.6624 |        -190.1650 |
[32m[20221213 12:42:59 @agent_ppo2.py:179][0m |          -0.0064 |          10.0125 |        -198.3891 |
[32m[20221213 12:43:00 @agent_ppo2.py:179][0m |          -0.0218 |           9.4037 |        -199.4279 |
[32m[20221213 12:43:00 @agent_ppo2.py:179][0m |          -0.0182 |           9.3292 |        -194.4804 |
[32m[20221213 12:43:00 @agent_ppo2.py:179][0m |          -0.0325 |           9.1292 |        -204.4864 |
[32m[20221213 12:43:00 @agent_ppo2.py:179][0m |          -0.0396 |           9.0975 |        -207.2205 |
[32m[20221213 12:43:00 @agent_ppo2.py:179][0m |          -0.0374 |           9.0075 |        -212.6660 |
[32m[20221213 12:43:00 @agent_ppo2.py:179][0m |          -0.0385 |           8.9330 |        -210.3879 |
[32m[20221213 12:43:00 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:43:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 86.61
[32m[20221213 12:43:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.06
[32m[20221213 12:43:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.18
[32m[20221213 12:43:00 @agent_ppo2.py:137][0m Total time:      27.06 min
[32m[20221213 12:43:00 @agent_ppo2.py:139][0m 1839104 total steps have happened
[32m[20221213 12:43:00 @agent_ppo2.py:115][0m #------------------------ Iteration 898 --------------------------#
[32m[20221213 12:43:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:01 @agent_ppo2.py:179][0m |           0.1033 |          30.3247 |        -190.7312 |
[32m[20221213 12:43:01 @agent_ppo2.py:179][0m |           0.0855 |          28.0355 |        -119.9640 |
[32m[20221213 12:43:01 @agent_ppo2.py:179][0m |           0.0435 |          26.9517 |        -151.1850 |
[32m[20221213 12:43:01 @agent_ppo2.py:179][0m |           0.0009 |          26.2339 |        -182.6726 |
[32m[20221213 12:43:01 @agent_ppo2.py:179][0m |          -0.0146 |          25.8223 |        -198.8154 |
[32m[20221213 12:43:01 @agent_ppo2.py:179][0m |          -0.0187 |          25.5794 |        -205.2286 |
[32m[20221213 12:43:02 @agent_ppo2.py:179][0m |          -0.0278 |          25.8273 |        -214.5696 |
[32m[20221213 12:43:02 @agent_ppo2.py:179][0m |          -0.0363 |          25.1401 |        -223.1437 |
[32m[20221213 12:43:02 @agent_ppo2.py:179][0m |          -0.0429 |          25.1094 |        -228.5428 |
[32m[20221213 12:43:02 @agent_ppo2.py:179][0m |          -0.0489 |          24.7715 |        -234.1714 |
[32m[20221213 12:43:02 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:43:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.65
[32m[20221213 12:43:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 367.51
[32m[20221213 12:43:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 368.47
[32m[20221213 12:43:02 @agent_ppo2.py:137][0m Total time:      27.09 min
[32m[20221213 12:43:02 @agent_ppo2.py:139][0m 1841152 total steps have happened
[32m[20221213 12:43:02 @agent_ppo2.py:115][0m #------------------------ Iteration 899 --------------------------#
[32m[20221213 12:43:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:03 @agent_ppo2.py:179][0m |           0.1344 |          32.8886 |        -200.6907 |
[32m[20221213 12:43:03 @agent_ppo2.py:179][0m |           0.1140 |          28.9553 |        -128.8308 |
[32m[20221213 12:43:03 @agent_ppo2.py:179][0m |           0.0396 |          27.3445 |        -148.6225 |
[32m[20221213 12:43:03 @agent_ppo2.py:179][0m |           0.0108 |          27.3640 |        -180.2187 |
[32m[20221213 12:43:03 @agent_ppo2.py:179][0m |          -0.0173 |          26.0440 |        -191.7279 |
[32m[20221213 12:43:03 @agent_ppo2.py:179][0m |          -0.0259 |          25.5989 |        -199.2969 |
[32m[20221213 12:43:03 @agent_ppo2.py:179][0m |          -0.0357 |          25.2481 |        -209.0004 |
[32m[20221213 12:43:04 @agent_ppo2.py:179][0m |          -0.0455 |          24.9233 |        -214.8564 |
[32m[20221213 12:43:04 @agent_ppo2.py:179][0m |          -0.0409 |          24.7250 |        -213.4592 |
[32m[20221213 12:43:04 @agent_ppo2.py:179][0m |          -0.0373 |          24.6363 |        -218.1808 |
[32m[20221213 12:43:04 @agent_ppo2.py:124][0m Policy update time: 1.43 s
[32m[20221213 12:43:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 277.26
[32m[20221213 12:43:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.28
[32m[20221213 12:43:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 24.93
[32m[20221213 12:43:04 @agent_ppo2.py:137][0m Total time:      27.12 min
[32m[20221213 12:43:04 @agent_ppo2.py:139][0m 1843200 total steps have happened
[32m[20221213 12:43:04 @agent_ppo2.py:115][0m #------------------------ Iteration 900 --------------------------#
[32m[20221213 12:43:04 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:43:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |           0.0613 |          32.1500 |        -221.5201 |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |           0.0738 |          30.0428 |        -187.4161 |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |           0.0868 |          28.7815 |        -150.7017 |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |           0.0290 |          28.1289 |        -194.5840 |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |           0.0003 |          27.7382 |        -217.7464 |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |          -0.0098 |          27.1407 |        -227.7909 |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |          -0.0230 |          26.5712 |        -235.1396 |
[32m[20221213 12:43:05 @agent_ppo2.py:179][0m |          -0.0272 |          26.2722 |        -239.1290 |
[32m[20221213 12:43:06 @agent_ppo2.py:179][0m |          -0.0277 |          26.0574 |        -237.7504 |
[32m[20221213 12:43:06 @agent_ppo2.py:179][0m |          -0.0340 |          25.6219 |        -244.5853 |
[32m[20221213 12:43:06 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:43:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.12
[32m[20221213 12:43:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 380.55
[32m[20221213 12:43:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.29
[32m[20221213 12:43:06 @agent_ppo2.py:137][0m Total time:      27.15 min
[32m[20221213 12:43:06 @agent_ppo2.py:139][0m 1845248 total steps have happened
[32m[20221213 12:43:06 @agent_ppo2.py:115][0m #------------------------ Iteration 901 --------------------------#
[32m[20221213 12:43:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:06 @agent_ppo2.py:179][0m |           0.0701 |          22.3524 |        -196.8419 |
[32m[20221213 12:43:06 @agent_ppo2.py:179][0m |           0.0293 |          18.7843 |        -161.8322 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0110 |          17.5113 |        -174.7375 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0342 |          16.8136 |        -187.3908 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0459 |          16.1373 |        -191.7016 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0510 |          15.6867 |        -198.3476 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0551 |          15.1808 |        -199.6671 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0568 |          14.9096 |        -202.3932 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0626 |          14.5735 |        -208.3088 |
[32m[20221213 12:43:07 @agent_ppo2.py:179][0m |          -0.0610 |          14.4128 |        -210.3753 |
[32m[20221213 12:43:07 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:43:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.99
[32m[20221213 12:43:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 309.50
[32m[20221213 12:43:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.04
[32m[20221213 12:43:08 @agent_ppo2.py:137][0m Total time:      27.18 min
[32m[20221213 12:43:08 @agent_ppo2.py:139][0m 1847296 total steps have happened
[32m[20221213 12:43:08 @agent_ppo2.py:115][0m #------------------------ Iteration 902 --------------------------#
[32m[20221213 12:43:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:08 @agent_ppo2.py:179][0m |           0.0608 |          33.8350 |        -222.5355 |
[32m[20221213 12:43:08 @agent_ppo2.py:179][0m |           0.0535 |          30.1061 |        -192.7414 |
[32m[20221213 12:43:08 @agent_ppo2.py:179][0m |           0.0021 |          29.1078 |        -220.7441 |
[32m[20221213 12:43:08 @agent_ppo2.py:179][0m |          -0.0147 |          28.1308 |        -226.8935 |
[32m[20221213 12:43:09 @agent_ppo2.py:179][0m |          -0.0267 |          27.5523 |        -228.2389 |
[32m[20221213 12:43:09 @agent_ppo2.py:179][0m |          -0.0451 |          27.1133 |        -236.7056 |
[32m[20221213 12:43:09 @agent_ppo2.py:179][0m |          -0.0400 |          27.2217 |        -239.3238 |
[32m[20221213 12:43:09 @agent_ppo2.py:179][0m |          -0.0459 |          26.5417 |        -244.0818 |
[32m[20221213 12:43:09 @agent_ppo2.py:179][0m |          -0.0492 |          26.0976 |        -248.5488 |
[32m[20221213 12:43:09 @agent_ppo2.py:179][0m |          -0.0457 |          25.8086 |        -243.7396 |
[32m[20221213 12:43:09 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:43:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.43
[32m[20221213 12:43:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.87
[32m[20221213 12:43:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.09
[32m[20221213 12:43:09 @agent_ppo2.py:137][0m Total time:      27.21 min
[32m[20221213 12:43:09 @agent_ppo2.py:139][0m 1849344 total steps have happened
[32m[20221213 12:43:09 @agent_ppo2.py:115][0m #------------------------ Iteration 903 --------------------------#
[32m[20221213 12:43:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:10 @agent_ppo2.py:179][0m |           0.1533 |           1.4540 |        -109.1092 |
[32m[20221213 12:43:10 @agent_ppo2.py:179][0m |           0.1850 |           1.3396 |         -69.0727 |
[32m[20221213 12:43:10 @agent_ppo2.py:179][0m |           0.0382 |           1.2986 |        -222.4968 |
[32m[20221213 12:43:10 @agent_ppo2.py:179][0m |           0.0274 |           1.3448 |        -247.5190 |
[32m[20221213 12:43:10 @agent_ppo2.py:179][0m |           0.0527 |           1.2847 |        -217.5610 |
[32m[20221213 12:43:10 @agent_ppo2.py:179][0m |           0.0114 |           1.2907 |        -241.1880 |
[32m[20221213 12:43:10 @agent_ppo2.py:179][0m |          -0.0068 |           1.2607 |        -258.5532 |
[32m[20221213 12:43:11 @agent_ppo2.py:179][0m |          -0.0104 |           1.2536 |        -259.3510 |
[32m[20221213 12:43:11 @agent_ppo2.py:179][0m |           0.0037 |           1.2536 |        -252.2003 |
[32m[20221213 12:43:11 @agent_ppo2.py:179][0m |          -0.0047 |           1.2483 |        -257.4596 |
[32m[20221213 12:43:11 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:43:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 22.11
[32m[20221213 12:43:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.26
[32m[20221213 12:43:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.57
[32m[20221213 12:43:11 @agent_ppo2.py:137][0m Total time:      27.24 min
[32m[20221213 12:43:11 @agent_ppo2.py:139][0m 1851392 total steps have happened
[32m[20221213 12:43:11 @agent_ppo2.py:115][0m #------------------------ Iteration 904 --------------------------#
[32m[20221213 12:43:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:11 @agent_ppo2.py:179][0m |           0.0687 |          33.7580 |        -233.9423 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |           0.0221 |          29.9092 |        -212.4407 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |          -0.0107 |          28.6270 |        -232.2531 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |          -0.0225 |          28.0235 |        -234.9996 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |          -0.0326 |          28.0906 |        -237.7463 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |          -0.0317 |          27.6991 |        -243.5675 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |          -0.0486 |          26.4612 |        -249.2731 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |          -0.0516 |          25.9672 |        -256.2180 |
[32m[20221213 12:43:12 @agent_ppo2.py:179][0m |          -0.0569 |          25.5826 |        -256.0658 |
[32m[20221213 12:43:13 @agent_ppo2.py:179][0m |          -0.0484 |          26.6833 |        -257.5503 |
[32m[20221213 12:43:13 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:43:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.97
[32m[20221213 12:43:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.24
[32m[20221213 12:43:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.54
[32m[20221213 12:43:13 @agent_ppo2.py:137][0m Total time:      27.27 min
[32m[20221213 12:43:13 @agent_ppo2.py:139][0m 1853440 total steps have happened
[32m[20221213 12:43:13 @agent_ppo2.py:115][0m #------------------------ Iteration 905 --------------------------#
[32m[20221213 12:43:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:13 @agent_ppo2.py:179][0m |           0.1309 |          32.6076 |        -217.9295 |
[32m[20221213 12:43:13 @agent_ppo2.py:179][0m |           0.0744 |          30.1076 |        -170.0062 |
[32m[20221213 12:43:13 @agent_ppo2.py:179][0m |           0.0262 |          28.7925 |        -200.6374 |
[32m[20221213 12:43:14 @agent_ppo2.py:179][0m |          -0.0044 |          27.6977 |        -221.1951 |
[32m[20221213 12:43:14 @agent_ppo2.py:179][0m |          -0.0208 |          27.0635 |        -231.0910 |
[32m[20221213 12:43:14 @agent_ppo2.py:179][0m |          -0.0316 |          26.5329 |        -243.5180 |
[32m[20221213 12:43:14 @agent_ppo2.py:179][0m |          -0.0351 |          26.1224 |        -251.5831 |
[32m[20221213 12:43:14 @agent_ppo2.py:179][0m |          -0.0427 |          25.7916 |        -255.6370 |
[32m[20221213 12:43:14 @agent_ppo2.py:179][0m |          -0.0418 |          26.0523 |        -259.9498 |
[32m[20221213 12:43:14 @agent_ppo2.py:179][0m |          -0.0429 |          25.3439 |        -260.9167 |
[32m[20221213 12:43:14 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:43:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 294.75
[32m[20221213 12:43:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.91
[32m[20221213 12:43:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.28
[32m[20221213 12:43:14 @agent_ppo2.py:137][0m Total time:      27.30 min
[32m[20221213 12:43:14 @agent_ppo2.py:139][0m 1855488 total steps have happened
[32m[20221213 12:43:14 @agent_ppo2.py:115][0m #------------------------ Iteration 906 --------------------------#
[32m[20221213 12:43:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:15 @agent_ppo2.py:179][0m |           0.0650 |          24.3884 |        -233.7255 |
[32m[20221213 12:43:15 @agent_ppo2.py:179][0m |           0.0645 |          21.8500 |        -203.2896 |
[32m[20221213 12:43:15 @agent_ppo2.py:179][0m |          -0.0014 |          20.8551 |        -221.8127 |
[32m[20221213 12:43:15 @agent_ppo2.py:179][0m |          -0.0185 |          20.0864 |        -235.0713 |
[32m[20221213 12:43:15 @agent_ppo2.py:179][0m |          -0.0179 |          21.1363 |        -240.2707 |
[32m[20221213 12:43:15 @agent_ppo2.py:179][0m |          -0.0381 |          19.2760 |        -250.7931 |
[32m[20221213 12:43:16 @agent_ppo2.py:179][0m |          -0.0399 |          18.7911 |        -250.6207 |
[32m[20221213 12:43:16 @agent_ppo2.py:179][0m |          -0.0433 |          18.4602 |        -257.7378 |
[32m[20221213 12:43:16 @agent_ppo2.py:179][0m |          -0.0470 |          18.2620 |        -260.9056 |
[32m[20221213 12:43:16 @agent_ppo2.py:179][0m |          -0.0545 |          18.0071 |        -268.0318 |
[32m[20221213 12:43:16 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:43:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 198.42
[32m[20221213 12:43:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.33
[32m[20221213 12:43:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.24
[32m[20221213 12:43:16 @agent_ppo2.py:137][0m Total time:      27.32 min
[32m[20221213 12:43:16 @agent_ppo2.py:139][0m 1857536 total steps have happened
[32m[20221213 12:43:16 @agent_ppo2.py:115][0m #------------------------ Iteration 907 --------------------------#
[32m[20221213 12:43:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |           0.0927 |          24.4730 |        -245.0695 |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |           0.0416 |          20.5970 |        -224.1458 |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |          -0.0039 |          19.1253 |        -221.5071 |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |          -0.0191 |          18.4122 |        -235.0690 |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |          -0.0327 |          17.8947 |        -244.7110 |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |          -0.0495 |          17.3606 |        -251.4316 |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |          -0.0500 |          17.0007 |        -258.2061 |
[32m[20221213 12:43:17 @agent_ppo2.py:179][0m |          -0.0603 |          16.7546 |        -263.8171 |
[32m[20221213 12:43:18 @agent_ppo2.py:179][0m |          -0.0664 |          16.5449 |        -266.0705 |
[32m[20221213 12:43:18 @agent_ppo2.py:179][0m |          -0.0669 |          16.2508 |        -271.2770 |
[32m[20221213 12:43:18 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:43:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 195.41
[32m[20221213 12:43:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.22
[32m[20221213 12:43:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 314.29
[32m[20221213 12:43:18 @agent_ppo2.py:137][0m Total time:      27.35 min
[32m[20221213 12:43:18 @agent_ppo2.py:139][0m 1859584 total steps have happened
[32m[20221213 12:43:18 @agent_ppo2.py:115][0m #------------------------ Iteration 908 --------------------------#
[32m[20221213 12:43:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:18 @agent_ppo2.py:179][0m |           0.0809 |          17.9053 |        -249.0239 |
[32m[20221213 12:43:18 @agent_ppo2.py:179][0m |           0.0361 |          16.3001 |        -188.6660 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |           0.0027 |          15.9890 |        -202.4352 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |          -0.0031 |          15.0020 |        -194.2091 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |          -0.0187 |          14.6142 |        -207.0997 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |          -0.0363 |          14.2538 |        -218.5147 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |          -0.0378 |          14.4636 |        -222.7839 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |          -0.0465 |          13.9213 |        -226.0993 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |          -0.0486 |          13.8816 |        -228.3767 |
[32m[20221213 12:43:19 @agent_ppo2.py:179][0m |          -0.0406 |          13.5579 |        -225.7722 |
[32m[20221213 12:43:19 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 154.50
[32m[20221213 12:43:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.48
[32m[20221213 12:43:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.96
[32m[20221213 12:43:20 @agent_ppo2.py:137][0m Total time:      27.38 min
[32m[20221213 12:43:20 @agent_ppo2.py:139][0m 1861632 total steps have happened
[32m[20221213 12:43:20 @agent_ppo2.py:115][0m #------------------------ Iteration 909 --------------------------#
[32m[20221213 12:43:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:20 @agent_ppo2.py:179][0m |           0.0967 |          33.8896 |        -242.7704 |
[32m[20221213 12:43:20 @agent_ppo2.py:179][0m |           0.0755 |          30.7976 |        -203.2305 |
[32m[20221213 12:43:20 @agent_ppo2.py:179][0m |           0.0218 |          29.5676 |        -241.6534 |
[32m[20221213 12:43:20 @agent_ppo2.py:179][0m |          -0.0033 |          29.1493 |        -250.7236 |
[32m[20221213 12:43:21 @agent_ppo2.py:179][0m |          -0.0188 |          28.7378 |        -267.9319 |
[32m[20221213 12:43:21 @agent_ppo2.py:179][0m |          -0.0244 |          28.0968 |        -274.4770 |
[32m[20221213 12:43:21 @agent_ppo2.py:179][0m |          -0.0329 |          28.1992 |        -283.1234 |
[32m[20221213 12:43:21 @agent_ppo2.py:179][0m |          -0.0407 |          27.6281 |        -287.1518 |
[32m[20221213 12:43:21 @agent_ppo2.py:179][0m |          -0.0343 |          28.8491 |        -290.8219 |
[32m[20221213 12:43:21 @agent_ppo2.py:179][0m |          -0.0348 |          27.1138 |        -290.7568 |
[32m[20221213 12:43:21 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.89
[32m[20221213 12:43:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.90
[32m[20221213 12:43:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.34
[32m[20221213 12:43:21 @agent_ppo2.py:137][0m Total time:      27.41 min
[32m[20221213 12:43:21 @agent_ppo2.py:139][0m 1863680 total steps have happened
[32m[20221213 12:43:21 @agent_ppo2.py:115][0m #------------------------ Iteration 910 --------------------------#
[32m[20221213 12:43:22 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:43:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:22 @agent_ppo2.py:179][0m |           0.0654 |          22.6059 |        -259.9783 |
[32m[20221213 12:43:22 @agent_ppo2.py:179][0m |           0.0251 |          19.3070 |        -255.3668 |
[32m[20221213 12:43:22 @agent_ppo2.py:179][0m |           0.0027 |          17.9483 |        -260.4817 |
[32m[20221213 12:43:22 @agent_ppo2.py:179][0m |          -0.0155 |          17.0947 |        -272.5991 |
[32m[20221213 12:43:22 @agent_ppo2.py:179][0m |          -0.0203 |          16.5454 |        -283.7733 |
[32m[20221213 12:43:22 @agent_ppo2.py:179][0m |          -0.0337 |          15.8809 |        -293.5294 |
[32m[20221213 12:43:23 @agent_ppo2.py:179][0m |          -0.0398 |          15.3739 |        -293.7833 |
[32m[20221213 12:43:23 @agent_ppo2.py:179][0m |          -0.0472 |          15.0618 |        -298.7479 |
[32m[20221213 12:43:23 @agent_ppo2.py:179][0m |          -0.0374 |          15.4194 |        -296.5155 |
[32m[20221213 12:43:23 @agent_ppo2.py:179][0m |          -0.0428 |          14.3310 |        -295.4379 |
[32m[20221213 12:43:23 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:43:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 167.78
[32m[20221213 12:43:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.15
[32m[20221213 12:43:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.43
[32m[20221213 12:43:23 @agent_ppo2.py:137][0m Total time:      27.44 min
[32m[20221213 12:43:23 @agent_ppo2.py:139][0m 1865728 total steps have happened
[32m[20221213 12:43:23 @agent_ppo2.py:115][0m #------------------------ Iteration 911 --------------------------#
[32m[20221213 12:43:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |           0.1561 |          35.6970 |        -207.2325 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |           0.0623 |          31.9305 |        -199.9223 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |           0.0337 |          31.5733 |        -213.7359 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |           0.0009 |          29.7886 |        -231.1902 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |          -0.0150 |          29.3744 |        -244.5648 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |          -0.0247 |          29.2708 |        -257.8064 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |          -0.0367 |          28.9111 |        -264.3352 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |          -0.0355 |          28.5667 |        -271.0391 |
[32m[20221213 12:43:24 @agent_ppo2.py:179][0m |          -0.0441 |          28.4613 |        -276.8301 |
[32m[20221213 12:43:25 @agent_ppo2.py:179][0m |          -0.0449 |          28.2572 |        -279.8540 |
[32m[20221213 12:43:25 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.90
[32m[20221213 12:43:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.42
[32m[20221213 12:43:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.76
[32m[20221213 12:43:25 @agent_ppo2.py:137][0m Total time:      27.47 min
[32m[20221213 12:43:25 @agent_ppo2.py:139][0m 1867776 total steps have happened
[32m[20221213 12:43:25 @agent_ppo2.py:115][0m #------------------------ Iteration 912 --------------------------#
[32m[20221213 12:43:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:25 @agent_ppo2.py:179][0m |           0.0729 |          10.3829 |        -257.0287 |
[32m[20221213 12:43:25 @agent_ppo2.py:179][0m |           0.0364 |           7.2286 |        -215.5979 |
[32m[20221213 12:43:25 @agent_ppo2.py:179][0m |           0.0098 |           6.3149 |        -228.4427 |
[32m[20221213 12:43:26 @agent_ppo2.py:179][0m |          -0.0043 |           5.6856 |        -229.1176 |
[32m[20221213 12:43:26 @agent_ppo2.py:179][0m |          -0.0071 |           5.2333 |        -214.2464 |
[32m[20221213 12:43:26 @agent_ppo2.py:179][0m |          -0.0212 |           4.9455 |        -226.1023 |
[32m[20221213 12:43:26 @agent_ppo2.py:179][0m |          -0.0017 |           4.6940 |        -188.7247 |
[32m[20221213 12:43:26 @agent_ppo2.py:179][0m |          -0.0253 |           4.4959 |        -186.7937 |
[32m[20221213 12:43:26 @agent_ppo2.py:179][0m |          -0.0312 |           4.3508 |        -204.9241 |
[32m[20221213 12:43:26 @agent_ppo2.py:179][0m |          -0.0327 |           4.2188 |        -223.5831 |
[32m[20221213 12:43:26 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:43:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 53.37
[32m[20221213 12:43:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 140.56
[32m[20221213 12:43:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 312.14
[32m[20221213 12:43:26 @agent_ppo2.py:137][0m Total time:      27.50 min
[32m[20221213 12:43:26 @agent_ppo2.py:139][0m 1869824 total steps have happened
[32m[20221213 12:43:26 @agent_ppo2.py:115][0m #------------------------ Iteration 913 --------------------------#
[32m[20221213 12:43:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:27 @agent_ppo2.py:179][0m |           0.0631 |          20.5598 |        -277.9569 |
[32m[20221213 12:43:27 @agent_ppo2.py:179][0m |           0.0342 |          17.6050 |        -249.1933 |
[32m[20221213 12:43:27 @agent_ppo2.py:179][0m |          -0.0012 |          16.0827 |        -239.1583 |
[32m[20221213 12:43:27 @agent_ppo2.py:179][0m |          -0.0242 |          15.1595 |        -268.7530 |
[32m[20221213 12:43:27 @agent_ppo2.py:179][0m |          -0.0398 |          14.3076 |        -281.6939 |
[32m[20221213 12:43:28 @agent_ppo2.py:179][0m |          -0.0461 |          13.8054 |        -291.7519 |
[32m[20221213 12:43:28 @agent_ppo2.py:179][0m |          -0.0508 |          13.3012 |        -295.2412 |
[32m[20221213 12:43:28 @agent_ppo2.py:179][0m |          -0.0518 |          12.9720 |        -295.7866 |
[32m[20221213 12:43:28 @agent_ppo2.py:179][0m |          -0.0529 |          12.4945 |        -302.7019 |
[32m[20221213 12:43:28 @agent_ppo2.py:179][0m |          -0.0586 |          12.2688 |        -304.5953 |
[32m[20221213 12:43:28 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:43:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.21
[32m[20221213 12:43:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.36
[32m[20221213 12:43:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 108.94
[32m[20221213 12:43:28 @agent_ppo2.py:137][0m Total time:      27.53 min
[32m[20221213 12:43:28 @agent_ppo2.py:139][0m 1871872 total steps have happened
[32m[20221213 12:43:28 @agent_ppo2.py:115][0m #------------------------ Iteration 914 --------------------------#
[32m[20221213 12:43:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:29 @agent_ppo2.py:179][0m |           0.0983 |          25.4944 |        -244.1000 |
[32m[20221213 12:43:29 @agent_ppo2.py:179][0m |           0.0861 |          22.1534 |        -227.3847 |
[32m[20221213 12:43:29 @agent_ppo2.py:179][0m |           0.0163 |          20.8219 |        -237.5847 |
[32m[20221213 12:43:29 @agent_ppo2.py:179][0m |          -0.0169 |          19.9909 |        -254.8744 |
[32m[20221213 12:43:29 @agent_ppo2.py:179][0m |          -0.0275 |          19.3330 |        -262.4757 |
[32m[20221213 12:43:29 @agent_ppo2.py:179][0m |          -0.0437 |          18.9717 |        -271.2111 |
[32m[20221213 12:43:29 @agent_ppo2.py:179][0m |          -0.0540 |          18.7472 |        -277.7356 |
[32m[20221213 12:43:30 @agent_ppo2.py:179][0m |          -0.0476 |          22.2776 |        -279.9748 |
[32m[20221213 12:43:30 @agent_ppo2.py:179][0m |          -0.0491 |          18.1312 |        -278.1679 |
[32m[20221213 12:43:30 @agent_ppo2.py:179][0m |          -0.0589 |          18.4987 |        -289.3072 |
[32m[20221213 12:43:30 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 215.39
[32m[20221213 12:43:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.09
[32m[20221213 12:43:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.24
[32m[20221213 12:43:30 @agent_ppo2.py:137][0m Total time:      27.55 min
[32m[20221213 12:43:30 @agent_ppo2.py:139][0m 1873920 total steps have happened
[32m[20221213 12:43:30 @agent_ppo2.py:115][0m #------------------------ Iteration 915 --------------------------#
[32m[20221213 12:43:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:30 @agent_ppo2.py:179][0m |           0.0744 |          28.6750 |        -268.5304 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |           0.0620 |          21.3822 |        -223.0509 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |           0.0070 |          19.7920 |        -250.0599 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |          -0.0198 |          18.7940 |        -269.1356 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |          -0.0367 |          18.2445 |        -283.0633 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |          -0.0378 |          17.4303 |        -288.5555 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |          -0.0440 |          16.9963 |        -289.9029 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |          -0.0500 |          16.4599 |        -291.9985 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |          -0.0549 |          16.1479 |        -296.9192 |
[32m[20221213 12:43:31 @agent_ppo2.py:179][0m |          -0.0602 |          15.7460 |        -301.5377 |
[32m[20221213 12:43:31 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:43:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 201.93
[32m[20221213 12:43:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.18
[32m[20221213 12:43:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.79
[32m[20221213 12:43:32 @agent_ppo2.py:137][0m Total time:      27.58 min
[32m[20221213 12:43:32 @agent_ppo2.py:139][0m 1875968 total steps have happened
[32m[20221213 12:43:32 @agent_ppo2.py:115][0m #------------------------ Iteration 916 --------------------------#
[32m[20221213 12:43:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:32 @agent_ppo2.py:179][0m |           0.0766 |          34.4782 |        -256.8479 |
[32m[20221213 12:43:32 @agent_ppo2.py:179][0m |           0.0379 |          31.9448 |        -235.7454 |
[32m[20221213 12:43:32 @agent_ppo2.py:179][0m |          -0.0035 |          30.6101 |        -245.1623 |
[32m[20221213 12:43:32 @agent_ppo2.py:179][0m |          -0.0253 |          29.8995 |        -259.5421 |
[32m[20221213 12:43:33 @agent_ppo2.py:179][0m |          -0.0371 |          29.2037 |        -267.3552 |
[32m[20221213 12:43:33 @agent_ppo2.py:179][0m |          -0.0335 |          28.8974 |        -271.4656 |
[32m[20221213 12:43:33 @agent_ppo2.py:179][0m |          -0.0437 |          28.7624 |        -274.3536 |
[32m[20221213 12:43:33 @agent_ppo2.py:179][0m |          -0.0490 |          28.0152 |        -272.1770 |
[32m[20221213 12:43:33 @agent_ppo2.py:179][0m |          -0.0512 |          27.6311 |        -279.8577 |
[32m[20221213 12:43:33 @agent_ppo2.py:179][0m |          -0.0586 |          27.4244 |        -288.4233 |
[32m[20221213 12:43:33 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.79
[32m[20221213 12:43:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.92
[32m[20221213 12:43:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.37
[32m[20221213 12:43:33 @agent_ppo2.py:137][0m Total time:      27.61 min
[32m[20221213 12:43:33 @agent_ppo2.py:139][0m 1878016 total steps have happened
[32m[20221213 12:43:33 @agent_ppo2.py:115][0m #------------------------ Iteration 917 --------------------------#
[32m[20221213 12:43:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:34 @agent_ppo2.py:179][0m |           0.0765 |          35.6913 |        -249.8923 |
[32m[20221213 12:43:34 @agent_ppo2.py:179][0m |           0.0439 |          32.4157 |        -230.8113 |
[32m[20221213 12:43:34 @agent_ppo2.py:179][0m |          -0.0032 |          30.9261 |        -256.6124 |
[32m[20221213 12:43:34 @agent_ppo2.py:179][0m |          -0.0144 |          30.9684 |        -270.7658 |
[32m[20221213 12:43:34 @agent_ppo2.py:179][0m |          -0.0357 |          29.3370 |        -281.3162 |
[32m[20221213 12:43:34 @agent_ppo2.py:179][0m |          -0.0359 |          28.7288 |        -283.9409 |
[32m[20221213 12:43:34 @agent_ppo2.py:179][0m |          -0.0340 |          28.2130 |        -284.8711 |
[32m[20221213 12:43:35 @agent_ppo2.py:179][0m |          -0.0373 |          28.7907 |        -294.3449 |
[32m[20221213 12:43:35 @agent_ppo2.py:179][0m |          -0.0487 |          27.3144 |        -298.0830 |
[32m[20221213 12:43:35 @agent_ppo2.py:179][0m |          -0.0420 |          28.3795 |        -302.3970 |
[32m[20221213 12:43:35 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:43:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.44
[32m[20221213 12:43:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.13
[32m[20221213 12:43:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 369.41
[32m[20221213 12:43:35 @agent_ppo2.py:137][0m Total time:      27.64 min
[32m[20221213 12:43:35 @agent_ppo2.py:139][0m 1880064 total steps have happened
[32m[20221213 12:43:35 @agent_ppo2.py:115][0m #------------------------ Iteration 918 --------------------------#
[32m[20221213 12:43:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:35 @agent_ppo2.py:179][0m |           0.0707 |          23.6425 |        -243.7562 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |           0.0738 |          20.6357 |        -211.7932 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |           0.0279 |          19.3848 |        -194.0805 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |          -0.0088 |          18.5137 |        -199.6529 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |          -0.0313 |          17.9848 |        -209.9516 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |          -0.0407 |          17.5636 |        -214.6108 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |          -0.0469 |          17.1763 |        -222.7613 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |          -0.0469 |          16.9180 |        -224.9924 |
[32m[20221213 12:43:36 @agent_ppo2.py:179][0m |          -0.0491 |          16.6048 |        -229.6780 |
[32m[20221213 12:43:37 @agent_ppo2.py:179][0m |          -0.0588 |          16.5005 |        -236.2333 |
[32m[20221213 12:43:37 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:43:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.43
[32m[20221213 12:43:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 327.72
[32m[20221213 12:43:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.17
[32m[20221213 12:43:37 @agent_ppo2.py:137][0m Total time:      27.67 min
[32m[20221213 12:43:37 @agent_ppo2.py:139][0m 1882112 total steps have happened
[32m[20221213 12:43:37 @agent_ppo2.py:115][0m #------------------------ Iteration 919 --------------------------#
[32m[20221213 12:43:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:37 @agent_ppo2.py:179][0m |           0.0585 |          11.2724 |        -278.3283 |
[32m[20221213 12:43:37 @agent_ppo2.py:179][0m |           0.0573 |          10.1380 |        -244.3631 |
[32m[20221213 12:43:37 @agent_ppo2.py:179][0m |           0.0154 |           9.5460 |        -254.5450 |
[32m[20221213 12:43:38 @agent_ppo2.py:179][0m |          -0.0047 |           9.1348 |        -269.9540 |
[32m[20221213 12:43:38 @agent_ppo2.py:179][0m |          -0.0139 |           8.9682 |        -274.5977 |
[32m[20221213 12:43:38 @agent_ppo2.py:179][0m |          -0.0257 |           8.7474 |        -281.2190 |
[32m[20221213 12:43:38 @agent_ppo2.py:179][0m |          -0.0200 |           8.5470 |        -279.9920 |
[32m[20221213 12:43:38 @agent_ppo2.py:179][0m |          -0.0182 |           9.0292 |        -277.7759 |
[32m[20221213 12:43:38 @agent_ppo2.py:179][0m |          -0.0343 |           8.3245 |        -289.8970 |
[32m[20221213 12:43:38 @agent_ppo2.py:179][0m |          -0.0286 |           8.1858 |        -288.6425 |
[32m[20221213 12:43:38 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:43:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.12
[32m[20221213 12:43:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.38
[32m[20221213 12:43:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.00
[32m[20221213 12:43:38 @agent_ppo2.py:137][0m Total time:      27.70 min
[32m[20221213 12:43:38 @agent_ppo2.py:139][0m 1884160 total steps have happened
[32m[20221213 12:43:38 @agent_ppo2.py:115][0m #------------------------ Iteration 920 --------------------------#
[32m[20221213 12:43:39 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:43:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:39 @agent_ppo2.py:179][0m |           0.0930 |          31.8050 |        -212.8075 |
[32m[20221213 12:43:39 @agent_ppo2.py:179][0m |           0.0543 |          28.4621 |        -194.2409 |
[32m[20221213 12:43:39 @agent_ppo2.py:179][0m |           0.0142 |          27.3252 |        -219.2737 |
[32m[20221213 12:43:39 @agent_ppo2.py:179][0m |          -0.0160 |          26.7264 |        -239.7553 |
[32m[20221213 12:43:39 @agent_ppo2.py:179][0m |          -0.0144 |          28.6419 |        -252.9713 |
[32m[20221213 12:43:40 @agent_ppo2.py:179][0m |          -0.0266 |          26.2779 |        -252.8444 |
[32m[20221213 12:43:40 @agent_ppo2.py:179][0m |          -0.0334 |          25.5787 |        -257.9096 |
[32m[20221213 12:43:40 @agent_ppo2.py:179][0m |          -0.0384 |          25.3847 |        -259.7564 |
[32m[20221213 12:43:40 @agent_ppo2.py:179][0m |          -0.0321 |          26.2748 |        -268.0160 |
[32m[20221213 12:43:40 @agent_ppo2.py:179][0m |          -0.0383 |          25.3633 |        -269.7074 |
[32m[20221213 12:43:40 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:43:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.42
[32m[20221213 12:43:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.16
[32m[20221213 12:43:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.12
[32m[20221213 12:43:40 @agent_ppo2.py:137][0m Total time:      27.72 min
[32m[20221213 12:43:40 @agent_ppo2.py:139][0m 1886208 total steps have happened
[32m[20221213 12:43:40 @agent_ppo2.py:115][0m #------------------------ Iteration 921 --------------------------#
[32m[20221213 12:43:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |           0.1070 |          41.3421 |        -237.8360 |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |           0.0726 |          34.0850 |        -186.4762 |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |           0.0369 |          34.0105 |        -177.4391 |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |          -0.0062 |          31.2404 |        -208.8679 |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |          -0.0198 |          30.6984 |        -223.2684 |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |          -0.0326 |          30.0950 |        -232.6036 |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |          -0.0404 |          29.5394 |        -239.6449 |
[32m[20221213 12:43:41 @agent_ppo2.py:179][0m |          -0.0427 |          29.3179 |        -246.8106 |
[32m[20221213 12:43:42 @agent_ppo2.py:179][0m |          -0.0492 |          28.7809 |        -250.2944 |
[32m[20221213 12:43:42 @agent_ppo2.py:179][0m |          -0.0529 |          28.6467 |        -253.8203 |
[32m[20221213 12:43:42 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.63
[32m[20221213 12:43:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 367.11
[32m[20221213 12:43:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 305.34
[32m[20221213 12:43:42 @agent_ppo2.py:137][0m Total time:      27.75 min
[32m[20221213 12:43:42 @agent_ppo2.py:139][0m 1888256 total steps have happened
[32m[20221213 12:43:42 @agent_ppo2.py:115][0m #------------------------ Iteration 922 --------------------------#
[32m[20221213 12:43:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:42 @agent_ppo2.py:179][0m |           0.1042 |          26.8870 |        -213.0872 |
[32m[20221213 12:43:42 @agent_ppo2.py:179][0m |           0.0888 |          24.2627 |        -163.4983 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |           0.0134 |          23.2652 |        -210.8879 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |          -0.0076 |          22.5398 |        -227.3420 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |          -0.0227 |          21.9177 |        -232.1605 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |          -0.0323 |          21.4925 |        -238.6675 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |          -0.0331 |          21.2341 |        -243.8392 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |          -0.0390 |          20.9258 |        -242.7206 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |          -0.0444 |          20.5644 |        -247.7980 |
[32m[20221213 12:43:43 @agent_ppo2.py:179][0m |          -0.0503 |          20.3457 |        -255.1495 |
[32m[20221213 12:43:43 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:43:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 212.83
[32m[20221213 12:43:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 318.79
[32m[20221213 12:43:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.26
[32m[20221213 12:43:44 @agent_ppo2.py:137][0m Total time:      27.78 min
[32m[20221213 12:43:44 @agent_ppo2.py:139][0m 1890304 total steps have happened
[32m[20221213 12:43:44 @agent_ppo2.py:115][0m #------------------------ Iteration 923 --------------------------#
[32m[20221213 12:43:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:44 @agent_ppo2.py:179][0m |           0.0900 |          30.5205 |        -216.4874 |
[32m[20221213 12:43:44 @agent_ppo2.py:179][0m |           0.0532 |          23.7592 |        -198.0276 |
[32m[20221213 12:43:44 @agent_ppo2.py:179][0m |           0.0011 |          22.2887 |        -223.1375 |
[32m[20221213 12:43:44 @agent_ppo2.py:179][0m |          -0.0190 |          21.4867 |        -231.4422 |
[32m[20221213 12:43:44 @agent_ppo2.py:179][0m |          -0.0257 |          20.6743 |        -236.0958 |
[32m[20221213 12:43:45 @agent_ppo2.py:179][0m |          -0.0414 |          20.1076 |        -243.2045 |
[32m[20221213 12:43:45 @agent_ppo2.py:179][0m |          -0.0507 |          19.7385 |        -252.4643 |
[32m[20221213 12:43:45 @agent_ppo2.py:179][0m |          -0.0496 |          19.4184 |        -255.6615 |
[32m[20221213 12:43:45 @agent_ppo2.py:179][0m |          -0.0478 |          19.0691 |        -251.0363 |
[32m[20221213 12:43:45 @agent_ppo2.py:179][0m |          -0.0595 |          18.8826 |        -260.4038 |
[32m[20221213 12:43:45 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:43:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 229.43
[32m[20221213 12:43:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.95
[32m[20221213 12:43:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.42
[32m[20221213 12:43:45 @agent_ppo2.py:137][0m Total time:      27.81 min
[32m[20221213 12:43:45 @agent_ppo2.py:139][0m 1892352 total steps have happened
[32m[20221213 12:43:45 @agent_ppo2.py:115][0m #------------------------ Iteration 924 --------------------------#
[32m[20221213 12:43:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:46 @agent_ppo2.py:179][0m |           0.0533 |           9.9641 |        -240.2735 |
[32m[20221213 12:43:46 @agent_ppo2.py:179][0m |           0.0372 |           8.8958 |        -222.8431 |
[32m[20221213 12:43:46 @agent_ppo2.py:179][0m |           0.0111 |           8.5718 |        -230.4122 |
[32m[20221213 12:43:46 @agent_ppo2.py:179][0m |          -0.0008 |           8.1870 |        -226.6970 |
[32m[20221213 12:43:46 @agent_ppo2.py:179][0m |          -0.0143 |           7.9285 |        -233.4926 |
[32m[20221213 12:43:46 @agent_ppo2.py:179][0m |          -0.0284 |           7.7468 |        -240.6831 |
[32m[20221213 12:43:46 @agent_ppo2.py:179][0m |          -0.0332 |           7.5805 |        -243.8402 |
[32m[20221213 12:43:47 @agent_ppo2.py:179][0m |          -0.0215 |           7.4139 |        -246.1480 |
[32m[20221213 12:43:47 @agent_ppo2.py:179][0m |          -0.0346 |           7.2498 |        -247.3639 |
[32m[20221213 12:43:47 @agent_ppo2.py:179][0m |          -0.0333 |           7.1344 |        -251.6736 |
[32m[20221213 12:43:47 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:43:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.15
[32m[20221213 12:43:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.51
[32m[20221213 12:43:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.97
[32m[20221213 12:43:47 @agent_ppo2.py:137][0m Total time:      27.84 min
[32m[20221213 12:43:47 @agent_ppo2.py:139][0m 1894400 total steps have happened
[32m[20221213 12:43:47 @agent_ppo2.py:115][0m #------------------------ Iteration 925 --------------------------#
[32m[20221213 12:43:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:47 @agent_ppo2.py:179][0m |           0.1230 |          27.1921 |        -174.9793 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |           0.0395 |          24.2342 |        -140.4875 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |           0.0042 |          22.8478 |        -159.6647 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |          -0.0129 |          22.1172 |        -174.5186 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |          -0.0138 |          21.6544 |        -176.0521 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |          -0.0232 |          21.1333 |        -181.7768 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |          -0.0376 |          20.8440 |        -192.8586 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |          -0.0393 |          20.9545 |        -199.5729 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |          -0.0496 |          20.3864 |        -204.5729 |
[32m[20221213 12:43:48 @agent_ppo2.py:179][0m |          -0.0518 |          20.2712 |        -207.0280 |
[32m[20221213 12:43:48 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:43:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 274.62
[32m[20221213 12:43:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 367.67
[32m[20221213 12:43:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 317.35
[32m[20221213 12:43:49 @agent_ppo2.py:137][0m Total time:      27.87 min
[32m[20221213 12:43:49 @agent_ppo2.py:139][0m 1896448 total steps have happened
[32m[20221213 12:43:49 @agent_ppo2.py:115][0m #------------------------ Iteration 926 --------------------------#
[32m[20221213 12:43:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:49 @agent_ppo2.py:179][0m |           0.0622 |          10.8149 |        -236.2248 |
[32m[20221213 12:43:49 @agent_ppo2.py:179][0m |           0.0264 |           9.7228 |        -228.1408 |
[32m[20221213 12:43:49 @agent_ppo2.py:179][0m |           0.0021 |           9.2361 |        -229.7060 |
[32m[20221213 12:43:49 @agent_ppo2.py:179][0m |           0.0016 |           8.9715 |        -233.3711 |
[32m[20221213 12:43:50 @agent_ppo2.py:179][0m |          -0.0133 |           8.7450 |        -240.8840 |
[32m[20221213 12:43:50 @agent_ppo2.py:179][0m |          -0.0186 |           8.5816 |        -246.2160 |
[32m[20221213 12:43:50 @agent_ppo2.py:179][0m |          -0.0225 |           8.4328 |        -242.5409 |
[32m[20221213 12:43:50 @agent_ppo2.py:179][0m |          -0.0285 |           8.2633 |        -249.3880 |
[32m[20221213 12:43:50 @agent_ppo2.py:179][0m |          -0.0365 |           8.1479 |        -254.6289 |
[32m[20221213 12:43:50 @agent_ppo2.py:179][0m |          -0.0333 |           8.1015 |        -255.3722 |
[32m[20221213 12:43:50 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:43:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 92.79
[32m[20221213 12:43:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.69
[32m[20221213 12:43:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.15
[32m[20221213 12:43:50 @agent_ppo2.py:137][0m Total time:      27.89 min
[32m[20221213 12:43:50 @agent_ppo2.py:139][0m 1898496 total steps have happened
[32m[20221213 12:43:50 @agent_ppo2.py:115][0m #------------------------ Iteration 927 --------------------------#
[32m[20221213 12:43:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:51 @agent_ppo2.py:179][0m |           0.0487 |           1.2407 |        -235.2382 |
[32m[20221213 12:43:51 @agent_ppo2.py:179][0m |           0.0123 |           1.1311 |        -238.2685 |
[32m[20221213 12:43:51 @agent_ppo2.py:179][0m |           0.0040 |           1.1173 |        -247.6402 |
[32m[20221213 12:43:51 @agent_ppo2.py:179][0m |           0.0008 |           1.1053 |        -249.1867 |
[32m[20221213 12:43:51 @agent_ppo2.py:179][0m |           0.0087 |           1.1005 |        -249.7880 |
[32m[20221213 12:43:51 @agent_ppo2.py:179][0m |           0.0195 |           1.0976 |        -253.0028 |
[32m[20221213 12:43:51 @agent_ppo2.py:179][0m |           0.0566 |           1.0909 |        -201.7895 |
[32m[20221213 12:43:52 @agent_ppo2.py:179][0m |           0.0081 |           1.0941 |        -259.6521 |
[32m[20221213 12:43:52 @agent_ppo2.py:179][0m |          -0.0104 |           1.0864 |        -261.7825 |
[32m[20221213 12:43:52 @agent_ppo2.py:179][0m |           0.0049 |           1.1000 |        -247.6984 |
[32m[20221213 12:43:52 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:43:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 21.97
[32m[20221213 12:43:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 22.06
[32m[20221213 12:43:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 286.42
[32m[20221213 12:43:52 @agent_ppo2.py:137][0m Total time:      27.92 min
[32m[20221213 12:43:52 @agent_ppo2.py:139][0m 1900544 total steps have happened
[32m[20221213 12:43:52 @agent_ppo2.py:115][0m #------------------------ Iteration 928 --------------------------#
[32m[20221213 12:43:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:52 @agent_ppo2.py:179][0m |           0.0433 |          10.9819 |        -251.7415 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |           0.0374 |           7.9076 |        -243.7118 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0116 |           6.7590 |        -257.3495 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0250 |           6.1684 |        -257.4967 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0216 |           5.6016 |        -260.9538 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0280 |           5.2425 |        -264.4799 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0353 |           4.9286 |        -267.2406 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0368 |           4.6923 |        -270.6963 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0346 |           4.5028 |        -272.2232 |
[32m[20221213 12:43:53 @agent_ppo2.py:179][0m |          -0.0359 |           4.3392 |        -258.8093 |
[32m[20221213 12:43:53 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:43:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.63
[32m[20221213 12:43:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.56
[32m[20221213 12:43:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 46.95
[32m[20221213 12:43:54 @agent_ppo2.py:137][0m Total time:      27.95 min
[32m[20221213 12:43:54 @agent_ppo2.py:139][0m 1902592 total steps have happened
[32m[20221213 12:43:54 @agent_ppo2.py:115][0m #------------------------ Iteration 929 --------------------------#
[32m[20221213 12:43:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:54 @agent_ppo2.py:179][0m |           0.0817 |          20.4243 |        -238.0101 |
[32m[20221213 12:43:54 @agent_ppo2.py:179][0m |           0.0417 |          16.9236 |        -224.5586 |
[32m[20221213 12:43:54 @agent_ppo2.py:179][0m |           0.0003 |          16.0197 |        -235.2351 |
[32m[20221213 12:43:54 @agent_ppo2.py:179][0m |          -0.0249 |          15.4389 |        -252.1021 |
[32m[20221213 12:43:55 @agent_ppo2.py:179][0m |          -0.0261 |          15.0914 |        -257.0784 |
[32m[20221213 12:43:55 @agent_ppo2.py:179][0m |          -0.0378 |          14.8209 |        -265.7196 |
[32m[20221213 12:43:55 @agent_ppo2.py:179][0m |          -0.0361 |          14.6223 |        -263.8631 |
[32m[20221213 12:43:55 @agent_ppo2.py:179][0m |          -0.0386 |          14.4671 |        -264.0484 |
[32m[20221213 12:43:55 @agent_ppo2.py:179][0m |          -0.0437 |          14.2166 |        -269.0977 |
[32m[20221213 12:43:55 @agent_ppo2.py:179][0m |          -0.0408 |          14.1166 |        -270.3174 |
[32m[20221213 12:43:55 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.43
[32m[20221213 12:43:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.60
[32m[20221213 12:43:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 325.33
[32m[20221213 12:43:55 @agent_ppo2.py:137][0m Total time:      27.98 min
[32m[20221213 12:43:55 @agent_ppo2.py:139][0m 1904640 total steps have happened
[32m[20221213 12:43:55 @agent_ppo2.py:115][0m #------------------------ Iteration 930 --------------------------#
[32m[20221213 12:43:56 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:43:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:56 @agent_ppo2.py:179][0m |           0.0749 |          17.4764 |        -222.0824 |
[32m[20221213 12:43:56 @agent_ppo2.py:179][0m |           0.0885 |          14.3652 |        -193.3900 |
[32m[20221213 12:43:56 @agent_ppo2.py:179][0m |           0.0235 |          13.1763 |        -214.6647 |
[32m[20221213 12:43:56 @agent_ppo2.py:179][0m |          -0.0117 |          12.1158 |        -231.3941 |
[32m[20221213 12:43:56 @agent_ppo2.py:179][0m |          -0.0261 |          11.3326 |        -243.8205 |
[32m[20221213 12:43:56 @agent_ppo2.py:179][0m |          -0.0381 |          10.8575 |        -249.5078 |
[32m[20221213 12:43:57 @agent_ppo2.py:179][0m |          -0.0410 |          10.3567 |        -248.1379 |
[32m[20221213 12:43:57 @agent_ppo2.py:179][0m |          -0.0410 |           9.8887 |        -255.0202 |
[32m[20221213 12:43:57 @agent_ppo2.py:179][0m |          -0.0433 |           9.5185 |        -256.2196 |
[32m[20221213 12:43:57 @agent_ppo2.py:179][0m |          -0.0451 |           9.2016 |        -259.2571 |
[32m[20221213 12:43:57 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:43:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.99
[32m[20221213 12:43:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.26
[32m[20221213 12:43:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 299.47
[32m[20221213 12:43:57 @agent_ppo2.py:137][0m Total time:      28.01 min
[32m[20221213 12:43:57 @agent_ppo2.py:139][0m 1906688 total steps have happened
[32m[20221213 12:43:57 @agent_ppo2.py:115][0m #------------------------ Iteration 931 --------------------------#
[32m[20221213 12:43:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |           0.0880 |          10.1816 |        -199.8740 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |           0.0685 |           7.6366 |         -98.5550 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |           0.0264 |           6.7699 |         -89.1047 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |           0.0080 |           5.9848 |         -93.0807 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |          -0.0038 |           5.4935 |         -94.7722 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |          -0.0145 |           5.1869 |        -100.1682 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |          -0.0202 |           4.8662 |        -103.9661 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |          -0.0271 |           4.6229 |        -105.1014 |
[32m[20221213 12:43:58 @agent_ppo2.py:179][0m |          -0.0286 |           4.4709 |        -106.7190 |
[32m[20221213 12:43:59 @agent_ppo2.py:179][0m |          -0.0308 |           4.2748 |        -108.6850 |
[32m[20221213 12:43:59 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:43:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 96.21
[32m[20221213 12:43:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.12
[32m[20221213 12:43:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.49
[32m[20221213 12:43:59 @agent_ppo2.py:137][0m Total time:      28.03 min
[32m[20221213 12:43:59 @agent_ppo2.py:139][0m 1908736 total steps have happened
[32m[20221213 12:43:59 @agent_ppo2.py:115][0m #------------------------ Iteration 932 --------------------------#
[32m[20221213 12:43:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:43:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:43:59 @agent_ppo2.py:179][0m |           0.1197 |          37.7521 |        -201.2546 |
[32m[20221213 12:43:59 @agent_ppo2.py:179][0m |           0.1113 |          34.1218 |        -146.2729 |
[32m[20221213 12:43:59 @agent_ppo2.py:179][0m |           0.0733 |          32.3052 |        -141.0809 |
[32m[20221213 12:44:00 @agent_ppo2.py:179][0m |           0.0300 |          31.0224 |        -181.9829 |
[32m[20221213 12:44:00 @agent_ppo2.py:179][0m |           0.0076 |          30.4697 |        -205.4443 |
[32m[20221213 12:44:00 @agent_ppo2.py:179][0m |          -0.0100 |          29.6955 |        -216.6850 |
[32m[20221213 12:44:00 @agent_ppo2.py:179][0m |          -0.0194 |          29.3424 |        -222.5700 |
[32m[20221213 12:44:00 @agent_ppo2.py:179][0m |          -0.0219 |          29.3340 |        -235.1702 |
[32m[20221213 12:44:00 @agent_ppo2.py:179][0m |          -0.0292 |          31.4971 |        -240.3712 |
[32m[20221213 12:44:00 @agent_ppo2.py:179][0m |          -0.0210 |          31.6400 |        -245.5785 |
[32m[20221213 12:44:00 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:44:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.14
[32m[20221213 12:44:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 368.07
[32m[20221213 12:44:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.51
[32m[20221213 12:44:01 @agent_ppo2.py:137][0m Total time:      28.06 min
[32m[20221213 12:44:01 @agent_ppo2.py:139][0m 1910784 total steps have happened
[32m[20221213 12:44:01 @agent_ppo2.py:115][0m #------------------------ Iteration 933 --------------------------#
[32m[20221213 12:44:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:01 @agent_ppo2.py:179][0m |           0.0873 |          33.2323 |        -201.7080 |
[32m[20221213 12:44:01 @agent_ppo2.py:179][0m |           0.0748 |          29.4906 |        -168.7151 |
[32m[20221213 12:44:01 @agent_ppo2.py:179][0m |           0.0417 |          32.3215 |        -194.0673 |
[32m[20221213 12:44:01 @agent_ppo2.py:179][0m |          -0.0020 |          26.7852 |        -198.7418 |
[32m[20221213 12:44:01 @agent_ppo2.py:179][0m |          -0.0225 |          26.3751 |        -210.0145 |
[32m[20221213 12:44:02 @agent_ppo2.py:179][0m |          -0.0346 |          25.2593 |        -216.8352 |
[32m[20221213 12:44:02 @agent_ppo2.py:179][0m |          -0.0414 |          24.7851 |        -222.1413 |
[32m[20221213 12:44:02 @agent_ppo2.py:179][0m |          -0.0546 |          24.1642 |        -226.9337 |
[32m[20221213 12:44:02 @agent_ppo2.py:179][0m |          -0.0564 |          23.8240 |        -231.2168 |
[32m[20221213 12:44:02 @agent_ppo2.py:179][0m |          -0.0575 |          23.5483 |        -232.8168 |
[32m[20221213 12:44:02 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.66
[32m[20221213 12:44:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 318.71
[32m[20221213 12:44:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.81
[32m[20221213 12:44:02 @agent_ppo2.py:137][0m Total time:      28.09 min
[32m[20221213 12:44:02 @agent_ppo2.py:139][0m 1912832 total steps have happened
[32m[20221213 12:44:02 @agent_ppo2.py:115][0m #------------------------ Iteration 934 --------------------------#
[32m[20221213 12:44:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:03 @agent_ppo2.py:179][0m |           0.0610 |          23.9027 |        -220.1907 |
[32m[20221213 12:44:03 @agent_ppo2.py:179][0m |           0.0397 |          20.7395 |        -205.5626 |
[32m[20221213 12:44:03 @agent_ppo2.py:179][0m |           0.0017 |          19.2945 |        -218.9534 |
[32m[20221213 12:44:03 @agent_ppo2.py:179][0m |          -0.0212 |          18.5044 |        -228.2056 |
[32m[20221213 12:44:03 @agent_ppo2.py:179][0m |          -0.0357 |          17.9274 |        -233.9136 |
[32m[20221213 12:44:03 @agent_ppo2.py:179][0m |          -0.0306 |          17.2455 |        -229.0821 |
[32m[20221213 12:44:03 @agent_ppo2.py:179][0m |          -0.0497 |          16.8735 |        -240.7799 |
[32m[20221213 12:44:04 @agent_ppo2.py:179][0m |          -0.0488 |          16.5978 |        -240.6837 |
[32m[20221213 12:44:04 @agent_ppo2.py:179][0m |          -0.0453 |          16.1738 |        -238.6169 |
[32m[20221213 12:44:04 @agent_ppo2.py:179][0m |          -0.0493 |          15.9039 |        -241.7201 |
[32m[20221213 12:44:04 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 184.65
[32m[20221213 12:44:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 320.02
[32m[20221213 12:44:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.76
[32m[20221213 12:44:04 @agent_ppo2.py:137][0m Total time:      28.12 min
[32m[20221213 12:44:04 @agent_ppo2.py:139][0m 1914880 total steps have happened
[32m[20221213 12:44:04 @agent_ppo2.py:115][0m #------------------------ Iteration 935 --------------------------#
[32m[20221213 12:44:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:04 @agent_ppo2.py:179][0m |           0.1100 |          33.4538 |        -204.5274 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |           0.0980 |          29.6326 |        -162.1833 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |           0.0445 |          27.8595 |        -177.7505 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |           0.0223 |          28.4758 |        -192.8229 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |          -0.0226 |          26.2960 |        -201.4109 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |          -0.0283 |          25.8067 |        -210.8885 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |          -0.0388 |          25.3662 |        -216.7503 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |          -0.0464 |          25.3103 |        -223.1609 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |          -0.0428 |          24.8896 |        -221.1586 |
[32m[20221213 12:44:05 @agent_ppo2.py:179][0m |          -0.0383 |          27.1120 |        -225.2915 |
[32m[20221213 12:44:05 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 258.89
[32m[20221213 12:44:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.75
[32m[20221213 12:44:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.73
[32m[20221213 12:44:06 @agent_ppo2.py:137][0m Total time:      28.15 min
[32m[20221213 12:44:06 @agent_ppo2.py:139][0m 1916928 total steps have happened
[32m[20221213 12:44:06 @agent_ppo2.py:115][0m #------------------------ Iteration 936 --------------------------#
[32m[20221213 12:44:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:06 @agent_ppo2.py:179][0m |           0.0839 |          36.6694 |        -204.2257 |
[32m[20221213 12:44:06 @agent_ppo2.py:179][0m |           0.0799 |          34.0936 |        -179.3527 |
[32m[20221213 12:44:06 @agent_ppo2.py:179][0m |           0.0173 |          33.0026 |        -190.6872 |
[32m[20221213 12:44:06 @agent_ppo2.py:179][0m |          -0.0076 |          36.0615 |        -199.3504 |
[32m[20221213 12:44:07 @agent_ppo2.py:179][0m |          -0.0314 |          32.2729 |        -202.8909 |
[32m[20221213 12:44:07 @agent_ppo2.py:179][0m |          -0.0394 |          31.6458 |        -206.1761 |
[32m[20221213 12:44:07 @agent_ppo2.py:179][0m |          -0.0490 |          31.3838 |        -215.0240 |
[32m[20221213 12:44:07 @agent_ppo2.py:179][0m |          -0.0512 |          31.0385 |        -215.4557 |
[32m[20221213 12:44:07 @agent_ppo2.py:179][0m |          -0.0554 |          30.8439 |        -217.4896 |
[32m[20221213 12:44:07 @agent_ppo2.py:179][0m |          -0.0551 |          30.5975 |        -222.0199 |
[32m[20221213 12:44:07 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.10
[32m[20221213 12:44:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.12
[32m[20221213 12:44:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.81
[32m[20221213 12:44:07 @agent_ppo2.py:137][0m Total time:      28.18 min
[32m[20221213 12:44:07 @agent_ppo2.py:139][0m 1918976 total steps have happened
[32m[20221213 12:44:07 @agent_ppo2.py:115][0m #------------------------ Iteration 937 --------------------------#
[32m[20221213 12:44:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:08 @agent_ppo2.py:179][0m |           0.0506 |          28.7556 |        -194.1569 |
[32m[20221213 12:44:08 @agent_ppo2.py:179][0m |           0.0213 |          26.3950 |        -184.6649 |
[32m[20221213 12:44:08 @agent_ppo2.py:179][0m |          -0.0011 |          25.4941 |        -191.2995 |
[32m[20221213 12:44:08 @agent_ppo2.py:179][0m |          -0.0319 |          24.7814 |        -199.7038 |
[32m[20221213 12:44:08 @agent_ppo2.py:179][0m |          -0.0292 |          25.9385 |        -204.2814 |
[32m[20221213 12:44:08 @agent_ppo2.py:179][0m |          -0.0366 |          24.0716 |        -206.1835 |
[32m[20221213 12:44:09 @agent_ppo2.py:179][0m |          -0.0428 |          23.7374 |        -212.4527 |
[32m[20221213 12:44:09 @agent_ppo2.py:179][0m |          -0.0479 |          23.3334 |        -212.1967 |
[32m[20221213 12:44:09 @agent_ppo2.py:179][0m |          -0.0506 |          23.1746 |        -216.8465 |
[32m[20221213 12:44:09 @agent_ppo2.py:179][0m |          -0.0550 |          23.0095 |        -219.7489 |
[32m[20221213 12:44:09 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.91
[32m[20221213 12:44:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.46
[32m[20221213 12:44:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.28
[32m[20221213 12:44:09 @agent_ppo2.py:137][0m Total time:      28.21 min
[32m[20221213 12:44:09 @agent_ppo2.py:139][0m 1921024 total steps have happened
[32m[20221213 12:44:09 @agent_ppo2.py:115][0m #------------------------ Iteration 938 --------------------------#
[32m[20221213 12:44:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |           0.1211 |          32.7388 |        -168.8812 |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |           0.0984 |          30.9323 |        -120.1194 |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |           0.0155 |          30.0594 |        -162.8655 |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |          -0.0026 |          29.7026 |        -183.2238 |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |          -0.0169 |          29.0818 |        -187.8859 |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |          -0.0337 |          28.6765 |        -199.5605 |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |          -0.0387 |          28.3938 |        -203.6676 |
[32m[20221213 12:44:10 @agent_ppo2.py:179][0m |          -0.0446 |          28.1265 |        -210.7135 |
[32m[20221213 12:44:11 @agent_ppo2.py:179][0m |          -0.0464 |          27.8506 |        -212.4130 |
[32m[20221213 12:44:11 @agent_ppo2.py:179][0m |          -0.0380 |          30.7934 |        -217.2835 |
[32m[20221213 12:44:11 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.25
[32m[20221213 12:44:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.21
[32m[20221213 12:44:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.92
[32m[20221213 12:44:11 @agent_ppo2.py:137][0m Total time:      28.24 min
[32m[20221213 12:44:11 @agent_ppo2.py:139][0m 1923072 total steps have happened
[32m[20221213 12:44:11 @agent_ppo2.py:115][0m #------------------------ Iteration 939 --------------------------#
[32m[20221213 12:44:11 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:44:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:11 @agent_ppo2.py:179][0m |           0.1180 |          11.0256 |         -51.3441 |
[32m[20221213 12:44:11 @agent_ppo2.py:179][0m |           0.0789 |          10.0496 |         -24.2283 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |           0.0448 |           9.9401 |         -32.2788 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |           0.0301 |          10.7159 |         -39.7118 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |          -0.0012 |           9.4672 |         -45.6906 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |          -0.0133 |           9.2796 |         -47.6978 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |          -0.0201 |           9.1621 |         -50.5237 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |          -0.0305 |           9.1280 |         -54.1215 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |          -0.0244 |           8.9897 |         -56.0459 |
[32m[20221213 12:44:12 @agent_ppo2.py:179][0m |          -0.0284 |           8.9417 |         -57.3849 |
[32m[20221213 12:44:12 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:44:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 102.73
[32m[20221213 12:44:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.16
[32m[20221213 12:44:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.52
[32m[20221213 12:44:13 @agent_ppo2.py:137][0m Total time:      28.26 min
[32m[20221213 12:44:13 @agent_ppo2.py:139][0m 1925120 total steps have happened
[32m[20221213 12:44:13 @agent_ppo2.py:115][0m #------------------------ Iteration 940 --------------------------#
[32m[20221213 12:44:13 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:44:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:13 @agent_ppo2.py:179][0m |           0.0150 |           1.9370 |        -216.0331 |
[32m[20221213 12:44:13 @agent_ppo2.py:179][0m |          -0.0140 |           1.4838 |        -219.5537 |
[32m[20221213 12:44:13 @agent_ppo2.py:179][0m |           0.0146 |           1.3721 |        -188.7718 |
[32m[20221213 12:44:13 @agent_ppo2.py:179][0m |          -0.0206 |           1.3350 |        -218.4259 |
[32m[20221213 12:44:13 @agent_ppo2.py:179][0m |          -0.0126 |           1.3237 |        -222.4777 |
[32m[20221213 12:44:14 @agent_ppo2.py:179][0m |          -0.0204 |           1.2912 |        -221.9112 |
[32m[20221213 12:44:14 @agent_ppo2.py:179][0m |          -0.0230 |           1.2694 |        -224.1118 |
[32m[20221213 12:44:14 @agent_ppo2.py:179][0m |          -0.0200 |           1.2568 |        -223.9549 |
[32m[20221213 12:44:14 @agent_ppo2.py:179][0m |          -0.0243 |           1.2452 |        -226.3982 |
[32m[20221213 12:44:14 @agent_ppo2.py:179][0m |          -0.0169 |           1.2424 |        -222.2318 |
[32m[20221213 12:44:14 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:44:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 18.08
[32m[20221213 12:44:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 18.90
[32m[20221213 12:44:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.39
[32m[20221213 12:44:14 @agent_ppo2.py:137][0m Total time:      28.29 min
[32m[20221213 12:44:14 @agent_ppo2.py:139][0m 1927168 total steps have happened
[32m[20221213 12:44:14 @agent_ppo2.py:115][0m #------------------------ Iteration 941 --------------------------#
[32m[20221213 12:44:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |           0.1419 |          31.9782 |        -178.4791 |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |           0.1197 |          28.4592 |         -92.2542 |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |           0.0726 |          31.7810 |        -119.7838 |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |           0.0282 |          26.8581 |        -135.3771 |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |          -0.0004 |          26.0087 |        -151.3318 |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |          -0.0158 |          25.5785 |        -165.9002 |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |          -0.0286 |          25.3165 |        -172.0721 |
[32m[20221213 12:44:15 @agent_ppo2.py:179][0m |          -0.0347 |          24.9305 |        -178.6332 |
[32m[20221213 12:44:16 @agent_ppo2.py:179][0m |          -0.0462 |          24.7220 |        -187.7159 |
[32m[20221213 12:44:16 @agent_ppo2.py:179][0m |          -0.0478 |          24.4256 |        -188.0482 |
[32m[20221213 12:44:16 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:44:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.89
[32m[20221213 12:44:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 302.73
[32m[20221213 12:44:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 369.89
[32m[20221213 12:44:16 @agent_ppo2.py:137][0m Total time:      28.32 min
[32m[20221213 12:44:16 @agent_ppo2.py:139][0m 1929216 total steps have happened
[32m[20221213 12:44:16 @agent_ppo2.py:115][0m #------------------------ Iteration 942 --------------------------#
[32m[20221213 12:44:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:16 @agent_ppo2.py:179][0m |           0.0987 |          22.4666 |        -222.1952 |
[32m[20221213 12:44:16 @agent_ppo2.py:179][0m |           0.0597 |          20.1508 |        -176.5447 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |           0.0122 |          19.2593 |        -187.3043 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |          -0.0031 |          18.9591 |        -200.8059 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |          -0.0272 |          18.3550 |        -212.9211 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |          -0.0386 |          18.0020 |        -225.6355 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |          -0.0417 |          17.6693 |        -229.2377 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |          -0.0500 |          17.4098 |        -234.4251 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |          -0.0536 |          17.2257 |        -242.7528 |
[32m[20221213 12:44:17 @agent_ppo2.py:179][0m |          -0.0564 |          17.0934 |        -245.0687 |
[32m[20221213 12:44:17 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:44:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 151.82
[32m[20221213 12:44:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 243.64
[32m[20221213 12:44:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 321.79
[32m[20221213 12:44:18 @agent_ppo2.py:137][0m Total time:      28.35 min
[32m[20221213 12:44:18 @agent_ppo2.py:139][0m 1931264 total steps have happened
[32m[20221213 12:44:18 @agent_ppo2.py:115][0m #------------------------ Iteration 943 --------------------------#
[32m[20221213 12:44:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:18 @agent_ppo2.py:179][0m |           0.0255 |          15.8269 |        -230.4522 |
[32m[20221213 12:44:18 @agent_ppo2.py:179][0m |           0.0287 |          14.2823 |        -211.7226 |
[32m[20221213 12:44:18 @agent_ppo2.py:179][0m |          -0.0149 |          13.6669 |        -232.5761 |
[32m[20221213 12:44:18 @agent_ppo2.py:179][0m |          -0.0151 |          13.1626 |        -231.5293 |
[32m[20221213 12:44:19 @agent_ppo2.py:179][0m |          -0.0353 |          13.1496 |        -237.4963 |
[32m[20221213 12:44:19 @agent_ppo2.py:179][0m |          -0.0267 |          12.6246 |        -239.3879 |
[32m[20221213 12:44:19 @agent_ppo2.py:179][0m |          -0.0371 |          12.4838 |        -245.6584 |
[32m[20221213 12:44:19 @agent_ppo2.py:179][0m |          -0.0404 |          12.3076 |        -249.5829 |
[32m[20221213 12:44:19 @agent_ppo2.py:179][0m |          -0.0298 |          14.1992 |        -250.3206 |
[32m[20221213 12:44:19 @agent_ppo2.py:179][0m |          -0.0404 |          12.1070 |        -253.8771 |
[32m[20221213 12:44:19 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:44:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.38
[32m[20221213 12:44:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.03
[32m[20221213 12:44:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 289.55
[32m[20221213 12:44:19 @agent_ppo2.py:137][0m Total time:      28.38 min
[32m[20221213 12:44:19 @agent_ppo2.py:139][0m 1933312 total steps have happened
[32m[20221213 12:44:19 @agent_ppo2.py:115][0m #------------------------ Iteration 944 --------------------------#
[32m[20221213 12:44:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:20 @agent_ppo2.py:179][0m |           0.1082 |          32.5908 |        -179.7482 |
[32m[20221213 12:44:20 @agent_ppo2.py:179][0m |           0.0674 |          30.6580 |        -175.3798 |
[32m[20221213 12:44:20 @agent_ppo2.py:179][0m |           0.0338 |          30.4046 |        -191.4037 |
[32m[20221213 12:44:20 @agent_ppo2.py:179][0m |          -0.0017 |          29.5548 |        -211.9595 |
[32m[20221213 12:44:20 @agent_ppo2.py:179][0m |          -0.0160 |          29.1005 |        -218.3872 |
[32m[20221213 12:44:20 @agent_ppo2.py:179][0m |          -0.0218 |          28.7808 |        -226.5262 |
[32m[20221213 12:44:20 @agent_ppo2.py:179][0m |          -0.0288 |          28.5948 |        -225.2646 |
[32m[20221213 12:44:21 @agent_ppo2.py:179][0m |          -0.0417 |          28.3172 |        -234.9531 |
[32m[20221213 12:44:21 @agent_ppo2.py:179][0m |          -0.0198 |          28.0490 |        -225.9633 |
[32m[20221213 12:44:21 @agent_ppo2.py:179][0m |          -0.0215 |          28.4539 |        -227.3503 |
[32m[20221213 12:44:21 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.00
[32m[20221213 12:44:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 323.42
[32m[20221213 12:44:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.08
[32m[20221213 12:44:21 @agent_ppo2.py:137][0m Total time:      28.41 min
[32m[20221213 12:44:21 @agent_ppo2.py:139][0m 1935360 total steps have happened
[32m[20221213 12:44:21 @agent_ppo2.py:115][0m #------------------------ Iteration 945 --------------------------#
[32m[20221213 12:44:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:21 @agent_ppo2.py:179][0m |           0.1204 |          26.7373 |        -186.1062 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |           0.0637 |          24.8312 |        -164.5301 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |           0.0150 |          24.0345 |        -183.8017 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |           0.0054 |          23.3482 |        -196.8887 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |          -0.0195 |          23.1230 |        -202.9133 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |          -0.0284 |          22.7006 |        -212.5792 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |          -0.0347 |          22.4954 |        -219.1418 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |          -0.0286 |          22.2135 |        -214.8621 |
[32m[20221213 12:44:22 @agent_ppo2.py:179][0m |          -0.0380 |          22.0555 |        -222.6082 |
[32m[20221213 12:44:23 @agent_ppo2.py:179][0m |          -0.0414 |          21.7404 |        -224.5283 |
[32m[20221213 12:44:23 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:44:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.69
[32m[20221213 12:44:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 349.47
[32m[20221213 12:44:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.88
[32m[20221213 12:44:23 @agent_ppo2.py:137][0m Total time:      28.43 min
[32m[20221213 12:44:23 @agent_ppo2.py:139][0m 1937408 total steps have happened
[32m[20221213 12:44:23 @agent_ppo2.py:115][0m #------------------------ Iteration 946 --------------------------#
[32m[20221213 12:44:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:23 @agent_ppo2.py:179][0m |           0.1084 |          31.4537 |        -169.3719 |
[32m[20221213 12:44:23 @agent_ppo2.py:179][0m |           0.0914 |          31.7388 |        -131.4221 |
[32m[20221213 12:44:23 @agent_ppo2.py:179][0m |           0.0171 |          28.6990 |        -171.3725 |
[32m[20221213 12:44:24 @agent_ppo2.py:179][0m |          -0.0050 |          28.1023 |        -184.2512 |
[32m[20221213 12:44:24 @agent_ppo2.py:179][0m |          -0.0184 |          27.6362 |        -193.0298 |
[32m[20221213 12:44:24 @agent_ppo2.py:179][0m |          -0.0232 |          27.2368 |        -199.3157 |
[32m[20221213 12:44:24 @agent_ppo2.py:179][0m |          -0.0302 |          26.8934 |        -206.2488 |
[32m[20221213 12:44:24 @agent_ppo2.py:179][0m |          -0.0311 |          26.5627 |        -212.7652 |
[32m[20221213 12:44:24 @agent_ppo2.py:179][0m |          -0.0357 |          26.2859 |        -219.4269 |
[32m[20221213 12:44:24 @agent_ppo2.py:179][0m |          -0.0376 |          26.1294 |        -215.6464 |
[32m[20221213 12:44:24 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:44:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.32
[32m[20221213 12:44:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.41
[32m[20221213 12:44:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.99
[32m[20221213 12:44:25 @agent_ppo2.py:137][0m Total time:      28.46 min
[32m[20221213 12:44:25 @agent_ppo2.py:139][0m 1939456 total steps have happened
[32m[20221213 12:44:25 @agent_ppo2.py:115][0m #------------------------ Iteration 947 --------------------------#
[32m[20221213 12:44:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:25 @agent_ppo2.py:179][0m |           0.0906 |          32.4429 |        -178.0374 |
[32m[20221213 12:44:25 @agent_ppo2.py:179][0m |           0.0385 |          29.4560 |        -162.5633 |
[32m[20221213 12:44:25 @agent_ppo2.py:179][0m |          -0.0030 |          28.5515 |        -181.6656 |
[32m[20221213 12:44:25 @agent_ppo2.py:179][0m |          -0.0225 |          27.9476 |        -186.8115 |
[32m[20221213 12:44:25 @agent_ppo2.py:179][0m |          -0.0299 |          27.4823 |        -193.4586 |
[32m[20221213 12:44:26 @agent_ppo2.py:179][0m |          -0.0310 |          26.9851 |        -191.2803 |
[32m[20221213 12:44:26 @agent_ppo2.py:179][0m |          -0.0278 |          28.3743 |        -192.3530 |
[32m[20221213 12:44:26 @agent_ppo2.py:179][0m |          -0.0393 |          26.3494 |        -198.4297 |
[32m[20221213 12:44:26 @agent_ppo2.py:179][0m |          -0.0441 |          25.9380 |        -201.1393 |
[32m[20221213 12:44:26 @agent_ppo2.py:179][0m |          -0.0394 |          25.6616 |        -200.8623 |
[32m[20221213 12:44:26 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 292.77
[32m[20221213 12:44:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.31
[32m[20221213 12:44:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.64
[32m[20221213 12:44:26 @agent_ppo2.py:137][0m Total time:      28.49 min
[32m[20221213 12:44:26 @agent_ppo2.py:139][0m 1941504 total steps have happened
[32m[20221213 12:44:26 @agent_ppo2.py:115][0m #------------------------ Iteration 948 --------------------------#
[32m[20221213 12:44:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |           0.0488 |           2.7814 |        -193.4394 |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |           0.0076 |           2.2434 |        -192.6087 |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |          -0.0085 |           2.1916 |        -198.2550 |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |          -0.0083 |           2.2155 |        -200.2237 |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |          -0.0006 |           2.1662 |        -189.4639 |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |           0.0022 |           2.1546 |        -195.7083 |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |          -0.0126 |           2.1486 |        -196.3506 |
[32m[20221213 12:44:27 @agent_ppo2.py:179][0m |          -0.0102 |           2.1454 |        -196.9219 |
[32m[20221213 12:44:28 @agent_ppo2.py:179][0m |          -0.0122 |           2.1241 |        -197.4460 |
[32m[20221213 12:44:28 @agent_ppo2.py:179][0m |          -0.0101 |           2.1407 |        -199.8765 |
[32m[20221213 12:44:28 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:44:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 19.45
[32m[20221213 12:44:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 20.48
[32m[20221213 12:44:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.92
[32m[20221213 12:44:28 @agent_ppo2.py:137][0m Total time:      28.52 min
[32m[20221213 12:44:28 @agent_ppo2.py:139][0m 1943552 total steps have happened
[32m[20221213 12:44:28 @agent_ppo2.py:115][0m #------------------------ Iteration 949 --------------------------#
[32m[20221213 12:44:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:28 @agent_ppo2.py:179][0m |           0.0768 |          30.1983 |        -170.3467 |
[32m[20221213 12:44:28 @agent_ppo2.py:179][0m |           0.0458 |          28.5335 |        -155.4508 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |           0.0050 |          27.9204 |        -170.9228 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |          -0.0145 |          27.6392 |        -187.3609 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |          -0.0236 |          27.2403 |        -193.2021 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |          -0.0321 |          27.0272 |        -199.2045 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |          -0.0377 |          27.0405 |        -201.2013 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |          -0.0305 |          30.0096 |        -207.8040 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |          -0.0418 |          26.7814 |        -207.5554 |
[32m[20221213 12:44:29 @agent_ppo2.py:179][0m |          -0.0332 |          27.9637 |        -209.6172 |
[32m[20221213 12:44:29 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.41
[32m[20221213 12:44:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.60
[32m[20221213 12:44:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.63
[32m[20221213 12:44:30 @agent_ppo2.py:137][0m Total time:      28.55 min
[32m[20221213 12:44:30 @agent_ppo2.py:139][0m 1945600 total steps have happened
[32m[20221213 12:44:30 @agent_ppo2.py:115][0m #------------------------ Iteration 950 --------------------------#
[32m[20221213 12:44:30 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:44:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:30 @agent_ppo2.py:179][0m |           0.0982 |          24.8938 |        -166.9763 |
[32m[20221213 12:44:30 @agent_ppo2.py:179][0m |           0.0556 |          23.5845 |        -156.5663 |
[32m[20221213 12:44:30 @agent_ppo2.py:179][0m |           0.0178 |          23.6644 |        -171.9900 |
[32m[20221213 12:44:30 @agent_ppo2.py:179][0m |          -0.0086 |          22.9033 |        -178.3709 |
[32m[20221213 12:44:31 @agent_ppo2.py:179][0m |          -0.0260 |          22.5706 |        -187.9159 |
[32m[20221213 12:44:31 @agent_ppo2.py:179][0m |          -0.0233 |          22.7470 |        -187.0684 |
[32m[20221213 12:44:31 @agent_ppo2.py:179][0m |          -0.0368 |          22.1002 |        -194.9700 |
[32m[20221213 12:44:31 @agent_ppo2.py:179][0m |          -0.0352 |          21.9207 |        -197.4551 |
[32m[20221213 12:44:31 @agent_ppo2.py:179][0m |          -0.0474 |          21.7182 |        -200.7998 |
[32m[20221213 12:44:31 @agent_ppo2.py:179][0m |          -0.0449 |          21.6309 |        -199.6732 |
[32m[20221213 12:44:31 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 214.67
[32m[20221213 12:44:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 294.96
[32m[20221213 12:44:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.59
[32m[20221213 12:44:31 @agent_ppo2.py:137][0m Total time:      28.58 min
[32m[20221213 12:44:31 @agent_ppo2.py:139][0m 1947648 total steps have happened
[32m[20221213 12:44:31 @agent_ppo2.py:115][0m #------------------------ Iteration 951 --------------------------#
[32m[20221213 12:44:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:32 @agent_ppo2.py:179][0m |           0.1272 |          30.4600 |        -168.9499 |
[32m[20221213 12:44:32 @agent_ppo2.py:179][0m |           0.0730 |          26.0547 |        -133.8321 |
[32m[20221213 12:44:32 @agent_ppo2.py:179][0m |           0.0147 |          25.1845 |        -150.0395 |
[32m[20221213 12:44:32 @agent_ppo2.py:179][0m |           0.0002 |          24.7243 |        -160.0263 |
[32m[20221213 12:44:32 @agent_ppo2.py:179][0m |          -0.0193 |          24.3998 |        -166.5174 |
[32m[20221213 12:44:32 @agent_ppo2.py:179][0m |          -0.0298 |          24.3496 |        -172.5347 |
[32m[20221213 12:44:32 @agent_ppo2.py:179][0m |          -0.0423 |          23.8545 |        -180.6441 |
[32m[20221213 12:44:33 @agent_ppo2.py:179][0m |          -0.0497 |          23.5275 |        -186.1095 |
[32m[20221213 12:44:33 @agent_ppo2.py:179][0m |          -0.0507 |          23.5372 |        -189.1691 |
[32m[20221213 12:44:33 @agent_ppo2.py:179][0m |          -0.0556 |          23.1878 |        -192.3992 |
[32m[20221213 12:44:33 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:44:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 270.59
[32m[20221213 12:44:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.48
[32m[20221213 12:44:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.74
[32m[20221213 12:44:33 @agent_ppo2.py:137][0m Total time:      28.61 min
[32m[20221213 12:44:33 @agent_ppo2.py:139][0m 1949696 total steps have happened
[32m[20221213 12:44:33 @agent_ppo2.py:115][0m #------------------------ Iteration 952 --------------------------#
[32m[20221213 12:44:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:33 @agent_ppo2.py:179][0m |           0.0730 |          31.3122 |        -167.0622 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |           0.0960 |          29.3950 |        -129.2383 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |           0.0331 |          28.6231 |        -149.2004 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |          -0.0022 |          27.9875 |        -170.6383 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |          -0.0233 |          27.6436 |        -181.9805 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |          -0.0243 |          27.1188 |        -182.9502 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |          -0.0335 |          26.7367 |        -187.5593 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |          -0.0358 |          26.5559 |        -186.7797 |
[32m[20221213 12:44:34 @agent_ppo2.py:179][0m |          -0.0316 |          27.3953 |        -191.4620 |
[32m[20221213 12:44:35 @agent_ppo2.py:179][0m |          -0.0437 |          26.1536 |        -195.4173 |
[32m[20221213 12:44:35 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:44:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.82
[32m[20221213 12:44:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.24
[32m[20221213 12:44:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 312.03
[32m[20221213 12:44:35 @agent_ppo2.py:137][0m Total time:      28.63 min
[32m[20221213 12:44:35 @agent_ppo2.py:139][0m 1951744 total steps have happened
[32m[20221213 12:44:35 @agent_ppo2.py:115][0m #------------------------ Iteration 953 --------------------------#
[32m[20221213 12:44:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:35 @agent_ppo2.py:179][0m |           0.0629 |          32.2446 |        -191.5749 |
[32m[20221213 12:44:35 @agent_ppo2.py:179][0m |           0.0706 |          30.8346 |        -168.1180 |
[32m[20221213 12:44:35 @agent_ppo2.py:179][0m |           0.0045 |          31.4750 |        -179.5934 |
[32m[20221213 12:44:36 @agent_ppo2.py:179][0m |          -0.0153 |          30.4994 |        -192.4587 |
[32m[20221213 12:44:36 @agent_ppo2.py:179][0m |          -0.0262 |          29.4388 |        -192.0052 |
[32m[20221213 12:44:36 @agent_ppo2.py:179][0m |          -0.0264 |          29.4761 |        -192.0978 |
[32m[20221213 12:44:36 @agent_ppo2.py:179][0m |          -0.0335 |          28.9586 |        -195.0241 |
[32m[20221213 12:44:36 @agent_ppo2.py:179][0m |          -0.0396 |          28.7039 |        -196.8285 |
[32m[20221213 12:44:36 @agent_ppo2.py:179][0m |          -0.0449 |          28.5428 |        -202.7658 |
[32m[20221213 12:44:36 @agent_ppo2.py:179][0m |          -0.0435 |          28.3748 |        -204.4212 |
[32m[20221213 12:44:36 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:44:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.09
[32m[20221213 12:44:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.68
[32m[20221213 12:44:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 280.64
[32m[20221213 12:44:36 @agent_ppo2.py:137][0m Total time:      28.66 min
[32m[20221213 12:44:36 @agent_ppo2.py:139][0m 1953792 total steps have happened
[32m[20221213 12:44:36 @agent_ppo2.py:115][0m #------------------------ Iteration 954 --------------------------#
[32m[20221213 12:44:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:37 @agent_ppo2.py:179][0m |           0.0390 |          15.9942 |        -160.1784 |
[32m[20221213 12:44:37 @agent_ppo2.py:179][0m |           0.0464 |          15.0406 |        -138.9420 |
[32m[20221213 12:44:37 @agent_ppo2.py:179][0m |           0.0132 |          14.5120 |        -142.9595 |
[32m[20221213 12:44:37 @agent_ppo2.py:179][0m |          -0.0060 |          14.2188 |        -154.7368 |
[32m[20221213 12:44:37 @agent_ppo2.py:179][0m |          -0.0244 |          14.0159 |        -159.8388 |
[32m[20221213 12:44:38 @agent_ppo2.py:179][0m |          -0.0360 |          13.6185 |        -169.7911 |
[32m[20221213 12:44:38 @agent_ppo2.py:179][0m |          -0.0297 |          15.3460 |        -172.1040 |
[32m[20221213 12:44:38 @agent_ppo2.py:179][0m |          -0.0349 |          13.2163 |        -172.0151 |
[32m[20221213 12:44:38 @agent_ppo2.py:179][0m |          -0.0421 |          13.0710 |        -175.9034 |
[32m[20221213 12:44:38 @agent_ppo2.py:179][0m |          -0.0494 |          12.9574 |        -178.9559 |
[32m[20221213 12:44:38 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:44:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.27
[32m[20221213 12:44:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.32
[32m[20221213 12:44:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 368.40
[32m[20221213 12:44:38 @agent_ppo2.py:137][0m Total time:      28.69 min
[32m[20221213 12:44:38 @agent_ppo2.py:139][0m 1955840 total steps have happened
[32m[20221213 12:44:38 @agent_ppo2.py:115][0m #------------------------ Iteration 955 --------------------------#
[32m[20221213 12:44:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |           0.0937 |          28.8018 |        -169.7229 |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |           0.1019 |          27.4240 |        -102.1675 |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |           0.0301 |          26.4561 |        -125.9133 |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |          -0.0012 |          25.8919 |        -143.1398 |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |          -0.0110 |          25.4422 |        -146.9162 |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |          -0.0245 |          25.1268 |        -154.0950 |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |          -0.0330 |          24.9194 |        -159.3108 |
[32m[20221213 12:44:39 @agent_ppo2.py:179][0m |          -0.0429 |          24.7600 |        -168.3529 |
[32m[20221213 12:44:40 @agent_ppo2.py:179][0m |          -0.0399 |          24.4735 |        -166.7657 |
[32m[20221213 12:44:40 @agent_ppo2.py:179][0m |          -0.0389 |          24.3125 |        -169.4142 |
[32m[20221213 12:44:40 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:44:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.77
[32m[20221213 12:44:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.56
[32m[20221213 12:44:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.68
[32m[20221213 12:44:40 @agent_ppo2.py:137][0m Total time:      28.72 min
[32m[20221213 12:44:40 @agent_ppo2.py:139][0m 1957888 total steps have happened
[32m[20221213 12:44:40 @agent_ppo2.py:115][0m #------------------------ Iteration 956 --------------------------#
[32m[20221213 12:44:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:40 @agent_ppo2.py:179][0m |           0.0828 |          32.7431 |        -172.9459 |
[32m[20221213 12:44:40 @agent_ppo2.py:179][0m |           0.0827 |          30.9835 |        -137.8059 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |           0.0189 |          29.9882 |        -174.8550 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |          -0.0083 |          29.2463 |        -194.6327 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |          -0.0135 |          28.7656 |        -198.9350 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |          -0.0255 |          28.3516 |        -206.0793 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |          -0.0278 |          28.1490 |        -206.6805 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |          -0.0313 |          27.8361 |        -210.6294 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |          -0.0346 |          27.8321 |        -217.0024 |
[32m[20221213 12:44:41 @agent_ppo2.py:179][0m |          -0.0307 |          27.4572 |        -216.9000 |
[32m[20221213 12:44:41 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.76
[32m[20221213 12:44:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.43
[32m[20221213 12:44:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.81
[32m[20221213 12:44:42 @agent_ppo2.py:137][0m Total time:      28.75 min
[32m[20221213 12:44:42 @agent_ppo2.py:139][0m 1959936 total steps have happened
[32m[20221213 12:44:42 @agent_ppo2.py:115][0m #------------------------ Iteration 957 --------------------------#
[32m[20221213 12:44:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:42 @agent_ppo2.py:179][0m |           0.0485 |          16.9142 |        -160.2977 |
[32m[20221213 12:44:42 @agent_ppo2.py:179][0m |           0.0136 |          16.0090 |        -117.0347 |
[32m[20221213 12:44:42 @agent_ppo2.py:179][0m |          -0.0133 |          15.9370 |        -122.0596 |
[32m[20221213 12:44:42 @agent_ppo2.py:179][0m |          -0.0337 |          15.3889 |        -129.6595 |
[32m[20221213 12:44:43 @agent_ppo2.py:179][0m |          -0.0262 |          15.2055 |        -115.2177 |
[32m[20221213 12:44:43 @agent_ppo2.py:179][0m |          -0.0434 |          14.8698 |        -135.9570 |
[32m[20221213 12:44:43 @agent_ppo2.py:179][0m |          -0.0480 |          14.7200 |        -141.0472 |
[32m[20221213 12:44:43 @agent_ppo2.py:179][0m |          -0.0554 |          14.5889 |        -143.2769 |
[32m[20221213 12:44:43 @agent_ppo2.py:179][0m |          -0.0579 |          14.4460 |        -147.5124 |
[32m[20221213 12:44:43 @agent_ppo2.py:179][0m |          -0.0557 |          14.2247 |        -146.5710 |
[32m[20221213 12:44:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:44:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 153.77
[32m[20221213 12:44:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.97
[32m[20221213 12:44:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.80
[32m[20221213 12:44:43 @agent_ppo2.py:137][0m Total time:      28.78 min
[32m[20221213 12:44:43 @agent_ppo2.py:139][0m 1961984 total steps have happened
[32m[20221213 12:44:43 @agent_ppo2.py:115][0m #------------------------ Iteration 958 --------------------------#
[32m[20221213 12:44:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:44 @agent_ppo2.py:179][0m |           0.1368 |          37.9011 |        -172.9407 |
[32m[20221213 12:44:44 @agent_ppo2.py:179][0m |           0.0980 |          32.1760 |        -124.1991 |
[32m[20221213 12:44:44 @agent_ppo2.py:179][0m |           0.0334 |          31.2029 |        -155.7287 |
[32m[20221213 12:44:44 @agent_ppo2.py:179][0m |           0.0085 |          30.9581 |        -173.3243 |
[32m[20221213 12:44:44 @agent_ppo2.py:179][0m |          -0.0103 |          30.4137 |        -177.6538 |
[32m[20221213 12:44:44 @agent_ppo2.py:179][0m |          -0.0179 |          32.8467 |        -192.7158 |
[32m[20221213 12:44:44 @agent_ppo2.py:179][0m |          -0.0271 |          29.8555 |        -196.1612 |
[32m[20221213 12:44:45 @agent_ppo2.py:179][0m |          -0.0400 |          29.4798 |        -203.8969 |
[32m[20221213 12:44:45 @agent_ppo2.py:179][0m |          -0.0427 |          29.2126 |        -207.9089 |
[32m[20221213 12:44:45 @agent_ppo2.py:179][0m |          -0.0464 |          29.0088 |        -210.7812 |
[32m[20221213 12:44:45 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.52
[32m[20221213 12:44:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.11
[32m[20221213 12:44:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.21
[32m[20221213 12:44:45 @agent_ppo2.py:137][0m Total time:      28.81 min
[32m[20221213 12:44:45 @agent_ppo2.py:139][0m 1964032 total steps have happened
[32m[20221213 12:44:45 @agent_ppo2.py:115][0m #------------------------ Iteration 959 --------------------------#
[32m[20221213 12:44:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:45 @agent_ppo2.py:179][0m |           0.0594 |          34.3841 |        -195.4564 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |           0.0341 |          35.8893 |        -179.4284 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |          -0.0071 |          32.1399 |        -194.1016 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |          -0.0248 |          31.4392 |        -199.8465 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |          -0.0326 |          30.9031 |        -204.7876 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |          -0.0283 |          31.7262 |        -207.1637 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |          -0.0439 |          30.0964 |        -212.0404 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |          -0.0467 |          29.7370 |        -218.1023 |
[32m[20221213 12:44:46 @agent_ppo2.py:179][0m |          -0.0511 |          29.4657 |        -219.0872 |
[32m[20221213 12:44:47 @agent_ppo2.py:179][0m |          -0.0498 |          29.2804 |        -218.4953 |
[32m[20221213 12:44:47 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 300.27
[32m[20221213 12:44:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.17
[32m[20221213 12:44:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.11
[32m[20221213 12:44:47 @agent_ppo2.py:137][0m Total time:      28.83 min
[32m[20221213 12:44:47 @agent_ppo2.py:139][0m 1966080 total steps have happened
[32m[20221213 12:44:47 @agent_ppo2.py:115][0m #------------------------ Iteration 960 --------------------------#
[32m[20221213 12:44:47 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:44:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:47 @agent_ppo2.py:179][0m |           0.0724 |          30.6652 |        -185.4246 |
[32m[20221213 12:44:47 @agent_ppo2.py:179][0m |           0.0400 |          28.4064 |        -177.9515 |
[32m[20221213 12:44:47 @agent_ppo2.py:179][0m |           0.0065 |          27.3946 |        -182.6184 |
[32m[20221213 12:44:48 @agent_ppo2.py:179][0m |          -0.0149 |          26.8735 |        -193.0709 |
[32m[20221213 12:44:48 @agent_ppo2.py:179][0m |          -0.0277 |          26.3773 |        -197.0543 |
[32m[20221213 12:44:48 @agent_ppo2.py:179][0m |          -0.0401 |          26.0876 |        -204.8812 |
[32m[20221213 12:44:48 @agent_ppo2.py:179][0m |          -0.0461 |          25.8533 |        -212.8360 |
[32m[20221213 12:44:48 @agent_ppo2.py:179][0m |          -0.0451 |          25.7200 |        -210.1559 |
[32m[20221213 12:44:48 @agent_ppo2.py:179][0m |          -0.0499 |          25.4835 |        -215.3263 |
[32m[20221213 12:44:48 @agent_ppo2.py:179][0m |          -0.0531 |          25.3621 |        -222.8231 |
[32m[20221213 12:44:48 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 238.38
[32m[20221213 12:44:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.09
[32m[20221213 12:44:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.47
[32m[20221213 12:44:49 @agent_ppo2.py:137][0m Total time:      28.86 min
[32m[20221213 12:44:49 @agent_ppo2.py:139][0m 1968128 total steps have happened
[32m[20221213 12:44:49 @agent_ppo2.py:115][0m #------------------------ Iteration 961 --------------------------#
[32m[20221213 12:44:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:49 @agent_ppo2.py:179][0m |           0.0718 |          15.7597 |        -189.9564 |
[32m[20221213 12:44:49 @agent_ppo2.py:179][0m |           0.0409 |          13.4197 |        -187.9820 |
[32m[20221213 12:44:49 @agent_ppo2.py:179][0m |           0.0003 |          12.2728 |        -201.5035 |
[32m[20221213 12:44:49 @agent_ppo2.py:179][0m |          -0.0130 |          11.6829 |        -208.3838 |
[32m[20221213 12:44:49 @agent_ppo2.py:179][0m |          -0.0280 |          11.1052 |        -215.2432 |
[32m[20221213 12:44:50 @agent_ppo2.py:179][0m |          -0.0334 |          10.8432 |        -218.6505 |
[32m[20221213 12:44:50 @agent_ppo2.py:179][0m |          -0.0412 |          10.4563 |        -223.4347 |
[32m[20221213 12:44:50 @agent_ppo2.py:179][0m |          -0.0455 |          10.2391 |        -230.2229 |
[32m[20221213 12:44:50 @agent_ppo2.py:179][0m |          -0.0472 |           9.8984 |        -228.7054 |
[32m[20221213 12:44:50 @agent_ppo2.py:179][0m |          -0.0527 |           9.6830 |        -239.3497 |
[32m[20221213 12:44:50 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:44:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.54
[32m[20221213 12:44:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.38
[32m[20221213 12:44:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 266.01
[32m[20221213 12:44:50 @agent_ppo2.py:137][0m Total time:      28.89 min
[32m[20221213 12:44:50 @agent_ppo2.py:139][0m 1970176 total steps have happened
[32m[20221213 12:44:50 @agent_ppo2.py:115][0m #------------------------ Iteration 962 --------------------------#
[32m[20221213 12:44:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |           0.0538 |          11.5683 |        -184.2486 |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |           0.0181 |          10.1244 |        -178.9347 |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |          -0.0103 |           9.8041 |        -180.6489 |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |          -0.0082 |           9.2988 |        -183.6397 |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |          -0.0257 |           9.0651 |        -191.4243 |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |          -0.0291 |           8.8869 |        -196.9231 |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |          -0.0282 |           8.8370 |        -193.4004 |
[32m[20221213 12:44:51 @agent_ppo2.py:179][0m |          -0.0340 |           8.7934 |        -194.2291 |
[32m[20221213 12:44:52 @agent_ppo2.py:179][0m |          -0.0454 |           8.6632 |        -196.5387 |
[32m[20221213 12:44:52 @agent_ppo2.py:179][0m |          -0.0471 |           8.6269 |        -204.0660 |
[32m[20221213 12:44:52 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:44:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 89.58
[32m[20221213 12:44:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 319.03
[32m[20221213 12:44:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.95
[32m[20221213 12:44:52 @agent_ppo2.py:137][0m Total time:      28.92 min
[32m[20221213 12:44:52 @agent_ppo2.py:139][0m 1972224 total steps have happened
[32m[20221213 12:44:52 @agent_ppo2.py:115][0m #------------------------ Iteration 963 --------------------------#
[32m[20221213 12:44:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:52 @agent_ppo2.py:179][0m |           0.1905 |          33.5728 |        -160.7550 |
[32m[20221213 12:44:52 @agent_ppo2.py:179][0m |           0.0999 |          30.7566 |        -124.8854 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |           0.0607 |          29.5287 |        -138.3909 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |           0.0236 |          29.0007 |        -166.3332 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |          -0.0020 |          28.4214 |        -184.8566 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |          -0.0206 |          28.1023 |        -198.1683 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |          -0.0275 |          27.8779 |        -209.4415 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |          -0.0268 |          27.4517 |        -213.4379 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |          -0.0336 |          27.0885 |        -219.2519 |
[32m[20221213 12:44:53 @agent_ppo2.py:179][0m |          -0.0407 |          26.7466 |        -221.0426 |
[32m[20221213 12:44:53 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.04
[32m[20221213 12:44:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.60
[32m[20221213 12:44:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 315.21
[32m[20221213 12:44:54 @agent_ppo2.py:137][0m Total time:      28.95 min
[32m[20221213 12:44:54 @agent_ppo2.py:139][0m 1974272 total steps have happened
[32m[20221213 12:44:54 @agent_ppo2.py:115][0m #------------------------ Iteration 964 --------------------------#
[32m[20221213 12:44:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:54 @agent_ppo2.py:179][0m |           0.3793 |          31.1348 |        -183.3434 |
[32m[20221213 12:44:54 @agent_ppo2.py:179][0m |           0.0656 |          28.7692 |        -143.8655 |
[32m[20221213 12:44:54 @agent_ppo2.py:179][0m |           0.0422 |          32.7899 |        -157.0935 |
[32m[20221213 12:44:54 @agent_ppo2.py:179][0m |           0.0091 |          27.4863 |        -147.2921 |
[32m[20221213 12:44:55 @agent_ppo2.py:179][0m |          -0.0135 |          29.9823 |        -177.0375 |
[32m[20221213 12:44:55 @agent_ppo2.py:179][0m |          -0.0354 |          27.2753 |        -184.5193 |
[32m[20221213 12:44:55 @agent_ppo2.py:179][0m |          -0.0392 |          26.5658 |        -191.2316 |
[32m[20221213 12:44:55 @agent_ppo2.py:179][0m |          -0.0388 |          26.3289 |        -194.7970 |
[32m[20221213 12:44:55 @agent_ppo2.py:179][0m |          -0.0492 |          26.1773 |        -198.1002 |
[32m[20221213 12:44:55 @agent_ppo2.py:179][0m |          -0.0453 |          29.0187 |        -199.2472 |
[32m[20221213 12:44:55 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:44:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 261.34
[32m[20221213 12:44:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.14
[32m[20221213 12:44:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.46
[32m[20221213 12:44:55 @agent_ppo2.py:137][0m Total time:      28.98 min
[32m[20221213 12:44:55 @agent_ppo2.py:139][0m 1976320 total steps have happened
[32m[20221213 12:44:55 @agent_ppo2.py:115][0m #------------------------ Iteration 965 --------------------------#
[32m[20221213 12:44:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:56 @agent_ppo2.py:179][0m |           0.0864 |          35.0639 |        -191.4399 |
[32m[20221213 12:44:56 @agent_ppo2.py:179][0m |           0.0965 |          38.7498 |        -173.5839 |
[32m[20221213 12:44:56 @agent_ppo2.py:179][0m |           0.0354 |          33.4635 |        -167.3087 |
[32m[20221213 12:44:56 @agent_ppo2.py:179][0m |           0.0006 |          33.0004 |        -189.1594 |
[32m[20221213 12:44:56 @agent_ppo2.py:179][0m |          -0.0210 |          32.4053 |        -203.8394 |
[32m[20221213 12:44:56 @agent_ppo2.py:179][0m |          -0.0285 |          32.1606 |        -214.5168 |
[32m[20221213 12:44:56 @agent_ppo2.py:179][0m |          -0.0314 |          31.9343 |        -215.3354 |
[32m[20221213 12:44:57 @agent_ppo2.py:179][0m |          -0.0395 |          31.6076 |        -218.7227 |
[32m[20221213 12:44:57 @agent_ppo2.py:179][0m |          -0.0435 |          31.5131 |        -227.2274 |
[32m[20221213 12:44:57 @agent_ppo2.py:179][0m |          -0.0421 |          31.2953 |        -227.4381 |
[32m[20221213 12:44:57 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.17
[32m[20221213 12:44:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.32
[32m[20221213 12:44:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.13
[32m[20221213 12:44:57 @agent_ppo2.py:137][0m Total time:      29.01 min
[32m[20221213 12:44:57 @agent_ppo2.py:139][0m 1978368 total steps have happened
[32m[20221213 12:44:57 @agent_ppo2.py:115][0m #------------------------ Iteration 966 --------------------------#
[32m[20221213 12:44:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:57 @agent_ppo2.py:179][0m |           0.1331 |          28.8054 |        -173.5047 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |           0.0487 |          25.4398 |        -132.8893 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |           0.0115 |          24.2286 |        -155.7511 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |          -0.0151 |          23.6433 |        -163.9113 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |          -0.0352 |          23.1236 |        -173.3684 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |          -0.0441 |          22.7890 |        -176.9683 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |          -0.0409 |          23.6577 |        -181.2489 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |          -0.0606 |          22.2286 |        -186.4134 |
[32m[20221213 12:44:58 @agent_ppo2.py:179][0m |          -0.0576 |          21.9310 |        -186.6857 |
[32m[20221213 12:44:59 @agent_ppo2.py:179][0m |          -0.0530 |          21.7073 |        -190.1124 |
[32m[20221213 12:44:59 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:44:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.17
[32m[20221213 12:44:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.02
[32m[20221213 12:44:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.26
[32m[20221213 12:44:59 @agent_ppo2.py:137][0m Total time:      29.03 min
[32m[20221213 12:44:59 @agent_ppo2.py:139][0m 1980416 total steps have happened
[32m[20221213 12:44:59 @agent_ppo2.py:115][0m #------------------------ Iteration 967 --------------------------#
[32m[20221213 12:44:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:44:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:44:59 @agent_ppo2.py:179][0m |           0.0676 |          34.5489 |        -203.7664 |
[32m[20221213 12:44:59 @agent_ppo2.py:179][0m |           0.0528 |          33.1136 |        -176.8667 |
[32m[20221213 12:44:59 @agent_ppo2.py:179][0m |           0.0140 |          32.4367 |        -187.0009 |
[32m[20221213 12:45:00 @agent_ppo2.py:179][0m |          -0.0080 |          32.1235 |        -206.1545 |
[32m[20221213 12:45:00 @agent_ppo2.py:179][0m |          -0.0232 |          32.0703 |        -217.9851 |
[32m[20221213 12:45:00 @agent_ppo2.py:179][0m |          -0.0246 |          33.6443 |        -224.9597 |
[32m[20221213 12:45:00 @agent_ppo2.py:179][0m |          -0.0386 |          31.4350 |        -233.2605 |
[32m[20221213 12:45:00 @agent_ppo2.py:179][0m |          -0.0418 |          31.3070 |        -236.2452 |
[32m[20221213 12:45:00 @agent_ppo2.py:179][0m |          -0.0417 |          31.0703 |        -233.4834 |
[32m[20221213 12:45:00 @agent_ppo2.py:179][0m |          -0.0449 |          30.9893 |        -238.2010 |
[32m[20221213 12:45:00 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:45:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.02
[32m[20221213 12:45:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.97
[32m[20221213 12:45:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.01
[32m[20221213 12:45:01 @agent_ppo2.py:137][0m Total time:      29.06 min
[32m[20221213 12:45:01 @agent_ppo2.py:139][0m 1982464 total steps have happened
[32m[20221213 12:45:01 @agent_ppo2.py:115][0m #------------------------ Iteration 968 --------------------------#
[32m[20221213 12:45:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:01 @agent_ppo2.py:179][0m |           0.0783 |          17.8416 |        -169.5022 |
[32m[20221213 12:45:01 @agent_ppo2.py:179][0m |           0.0201 |          16.5262 |        -146.4811 |
[32m[20221213 12:45:01 @agent_ppo2.py:179][0m |          -0.0107 |          16.2017 |        -161.7430 |
[32m[20221213 12:45:01 @agent_ppo2.py:179][0m |          -0.0245 |          15.6581 |        -172.4823 |
[32m[20221213 12:45:01 @agent_ppo2.py:179][0m |          -0.0370 |          15.3148 |        -183.6251 |
[32m[20221213 12:45:02 @agent_ppo2.py:179][0m |          -0.0439 |          15.0680 |        -182.2661 |
[32m[20221213 12:45:02 @agent_ppo2.py:179][0m |          -0.0488 |          14.8732 |        -185.4546 |
[32m[20221213 12:45:02 @agent_ppo2.py:179][0m |          -0.0424 |          14.6380 |        -185.2683 |
[32m[20221213 12:45:02 @agent_ppo2.py:179][0m |          -0.0496 |          14.4343 |        -190.9887 |
[32m[20221213 12:45:02 @agent_ppo2.py:179][0m |          -0.0499 |          14.3171 |        -186.9686 |
[32m[20221213 12:45:02 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:45:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 136.04
[32m[20221213 12:45:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 300.19
[32m[20221213 12:45:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.61
[32m[20221213 12:45:02 @agent_ppo2.py:137][0m Total time:      29.09 min
[32m[20221213 12:45:02 @agent_ppo2.py:139][0m 1984512 total steps have happened
[32m[20221213 12:45:02 @agent_ppo2.py:115][0m #------------------------ Iteration 969 --------------------------#
[32m[20221213 12:45:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:03 @agent_ppo2.py:179][0m |           0.0696 |          35.0471 |        -212.9517 |
[32m[20221213 12:45:03 @agent_ppo2.py:179][0m |           0.0358 |          32.0373 |        -208.0435 |
[32m[20221213 12:45:03 @agent_ppo2.py:179][0m |           0.0054 |          30.8715 |        -209.1455 |
[32m[20221213 12:45:03 @agent_ppo2.py:179][0m |          -0.0088 |          30.2537 |        -221.7281 |
[32m[20221213 12:45:03 @agent_ppo2.py:179][0m |          -0.0190 |          29.7932 |        -231.3661 |
[32m[20221213 12:45:03 @agent_ppo2.py:179][0m |          -0.0290 |          29.5586 |        -235.6582 |
[32m[20221213 12:45:03 @agent_ppo2.py:179][0m |          -0.0268 |          29.3550 |        -234.9361 |
[32m[20221213 12:45:04 @agent_ppo2.py:179][0m |          -0.0300 |          28.9073 |        -242.7851 |
[32m[20221213 12:45:04 @agent_ppo2.py:179][0m |          -0.0391 |          28.7562 |        -249.2629 |
[32m[20221213 12:45:04 @agent_ppo2.py:179][0m |          -0.0402 |          28.4532 |        -254.2704 |
[32m[20221213 12:45:04 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:45:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.99
[32m[20221213 12:45:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.58
[32m[20221213 12:45:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 304.92
[32m[20221213 12:45:04 @agent_ppo2.py:137][0m Total time:      29.12 min
[32m[20221213 12:45:04 @agent_ppo2.py:139][0m 1986560 total steps have happened
[32m[20221213 12:45:04 @agent_ppo2.py:115][0m #------------------------ Iteration 970 --------------------------#
[32m[20221213 12:45:04 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:45:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:04 @agent_ppo2.py:179][0m |           0.0694 |          32.3963 |        -200.4071 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |           0.0418 |          29.5838 |        -176.2892 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |           0.0260 |          30.1986 |        -193.4347 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |          -0.0029 |          27.7134 |        -196.1148 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |          -0.0246 |          27.1279 |        -208.8740 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |          -0.0306 |          26.5719 |        -218.1304 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |          -0.0357 |          26.4087 |        -223.6042 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |          -0.0400 |          26.0387 |        -225.0474 |
[32m[20221213 12:45:05 @agent_ppo2.py:179][0m |          -0.0440 |          25.7142 |        -232.0321 |
[32m[20221213 12:45:06 @agent_ppo2.py:179][0m |          -0.0502 |          25.4711 |        -236.1194 |
[32m[20221213 12:45:06 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:45:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 289.05
[32m[20221213 12:45:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 316.38
[32m[20221213 12:45:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 44.43
[32m[20221213 12:45:06 @agent_ppo2.py:137][0m Total time:      29.15 min
[32m[20221213 12:45:06 @agent_ppo2.py:139][0m 1988608 total steps have happened
[32m[20221213 12:45:06 @agent_ppo2.py:115][0m #------------------------ Iteration 971 --------------------------#
[32m[20221213 12:45:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:06 @agent_ppo2.py:179][0m |           0.1615 |          30.1370 |        -182.4532 |
[32m[20221213 12:45:06 @agent_ppo2.py:179][0m |           0.0911 |          27.2650 |        -127.7163 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |           0.0370 |          26.4834 |        -170.3106 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |           0.0163 |          26.0185 |        -191.1171 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |           0.0082 |          25.6631 |        -197.6762 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |          -0.0034 |          25.3323 |        -200.7119 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |          -0.0160 |          25.1423 |        -214.1434 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |          -0.0306 |          24.8343 |        -217.1519 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |          -0.0233 |          24.7015 |        -220.6055 |
[32m[20221213 12:45:07 @agent_ppo2.py:179][0m |          -0.0123 |          24.5298 |        -219.5730 |
[32m[20221213 12:45:07 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:45:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.11
[32m[20221213 12:45:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.95
[32m[20221213 12:45:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 281.22
[32m[20221213 12:45:08 @agent_ppo2.py:137][0m Total time:      29.18 min
[32m[20221213 12:45:08 @agent_ppo2.py:139][0m 1990656 total steps have happened
[32m[20221213 12:45:08 @agent_ppo2.py:115][0m #------------------------ Iteration 972 --------------------------#
[32m[20221213 12:45:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:08 @agent_ppo2.py:179][0m |           0.0590 |          34.3064 |        -179.9259 |
[32m[20221213 12:45:08 @agent_ppo2.py:179][0m |           0.0708 |          31.9700 |        -134.1008 |
[32m[20221213 12:45:08 @agent_ppo2.py:179][0m |           0.0433 |          31.1348 |        -145.6126 |
[32m[20221213 12:45:08 @agent_ppo2.py:179][0m |           0.0010 |          30.7881 |        -173.8717 |
[32m[20221213 12:45:09 @agent_ppo2.py:179][0m |          -0.0177 |          30.2811 |        -189.4735 |
[32m[20221213 12:45:09 @agent_ppo2.py:179][0m |          -0.0201 |          29.9692 |        -192.7266 |
[32m[20221213 12:45:09 @agent_ppo2.py:179][0m |          -0.0276 |          29.6849 |        -199.7880 |
[32m[20221213 12:45:09 @agent_ppo2.py:179][0m |          -0.0330 |          29.3618 |        -203.1221 |
[32m[20221213 12:45:09 @agent_ppo2.py:179][0m |          -0.0303 |          29.2457 |        -202.8639 |
[32m[20221213 12:45:09 @agent_ppo2.py:179][0m |          -0.0344 |          29.0404 |        -204.6602 |
[32m[20221213 12:45:09 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:45:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 350.24
[32m[20221213 12:45:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 372.25
[32m[20221213 12:45:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.48
[32m[20221213 12:45:09 @agent_ppo2.py:137][0m Total time:      29.21 min
[32m[20221213 12:45:09 @agent_ppo2.py:139][0m 1992704 total steps have happened
[32m[20221213 12:45:09 @agent_ppo2.py:115][0m #------------------------ Iteration 973 --------------------------#
[32m[20221213 12:45:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:10 @agent_ppo2.py:179][0m |           0.0979 |          35.4818 |        -141.0769 |
[32m[20221213 12:45:10 @agent_ppo2.py:179][0m |           0.1014 |          33.9774 |         -69.8004 |
[32m[20221213 12:45:10 @agent_ppo2.py:179][0m |           0.0595 |          33.2788 |         -99.5686 |
[32m[20221213 12:45:10 @agent_ppo2.py:179][0m |           0.0280 |          33.2088 |        -147.7994 |
[32m[20221213 12:45:10 @agent_ppo2.py:179][0m |           0.0123 |          33.2398 |        -168.7955 |
[32m[20221213 12:45:10 @agent_ppo2.py:179][0m |           0.0053 |          32.7077 |        -175.2328 |
[32m[20221213 12:45:11 @agent_ppo2.py:179][0m |          -0.0115 |          32.5548 |        -190.2287 |
[32m[20221213 12:45:11 @agent_ppo2.py:179][0m |          -0.0144 |          34.8709 |        -198.4645 |
[32m[20221213 12:45:11 @agent_ppo2.py:179][0m |          -0.0212 |          35.8531 |        -203.9876 |
[32m[20221213 12:45:11 @agent_ppo2.py:179][0m |          -0.0343 |          32.0341 |        -211.9082 |
[32m[20221213 12:45:11 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:45:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 350.77
[32m[20221213 12:45:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.36
[32m[20221213 12:45:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.86
[32m[20221213 12:45:11 @agent_ppo2.py:137][0m Total time:      29.24 min
[32m[20221213 12:45:11 @agent_ppo2.py:139][0m 1994752 total steps have happened
[32m[20221213 12:45:11 @agent_ppo2.py:115][0m #------------------------ Iteration 974 --------------------------#
[32m[20221213 12:45:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |           0.1162 |          34.6075 |        -147.5354 |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |           0.1237 |          32.3774 |         -73.5392 |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |           0.0627 |          31.5044 |        -106.9428 |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |           0.0397 |          30.8411 |        -135.2241 |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |           0.0186 |          30.4610 |        -141.4737 |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |          -0.0018 |          30.1100 |        -161.1642 |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |          -0.0081 |          30.2394 |        -169.8720 |
[32m[20221213 12:45:12 @agent_ppo2.py:179][0m |          -0.0174 |          29.7011 |        -178.6857 |
[32m[20221213 12:45:13 @agent_ppo2.py:179][0m |          -0.0236 |          29.1629 |        -182.7436 |
[32m[20221213 12:45:13 @agent_ppo2.py:179][0m |          -0.0241 |          29.0333 |        -184.9594 |
[32m[20221213 12:45:13 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:45:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.76
[32m[20221213 12:45:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.88
[32m[20221213 12:45:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.71
[32m[20221213 12:45:13 @agent_ppo2.py:137][0m Total time:      29.27 min
[32m[20221213 12:45:13 @agent_ppo2.py:139][0m 1996800 total steps have happened
[32m[20221213 12:45:13 @agent_ppo2.py:115][0m #------------------------ Iteration 975 --------------------------#
[32m[20221213 12:45:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:13 @agent_ppo2.py:179][0m |           0.0728 |          36.2728 |        -148.4681 |
[32m[20221213 12:45:13 @agent_ppo2.py:179][0m |           0.0532 |          32.9675 |        -122.0415 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |           0.0105 |          32.4188 |        -136.9988 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |          -0.0127 |          32.0618 |        -148.6796 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |          -0.0204 |          31.7693 |        -155.0101 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |          -0.0293 |          31.5158 |        -156.3781 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |          -0.0374 |          31.3107 |        -160.8450 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |          -0.0421 |          31.1645 |        -164.1839 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |          -0.0423 |          30.9168 |        -164.4888 |
[32m[20221213 12:45:14 @agent_ppo2.py:179][0m |          -0.0476 |          30.8417 |        -167.4193 |
[32m[20221213 12:45:14 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:45:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.86
[32m[20221213 12:45:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.66
[32m[20221213 12:45:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.96
[32m[20221213 12:45:15 @agent_ppo2.py:137][0m Total time:      29.30 min
[32m[20221213 12:45:15 @agent_ppo2.py:139][0m 1998848 total steps have happened
[32m[20221213 12:45:15 @agent_ppo2.py:115][0m #------------------------ Iteration 976 --------------------------#
[32m[20221213 12:45:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:15 @agent_ppo2.py:179][0m |           0.0731 |          22.7257 |        -146.5234 |
[32m[20221213 12:45:15 @agent_ppo2.py:179][0m |           0.0515 |          21.2273 |         -99.5772 |
[32m[20221213 12:45:15 @agent_ppo2.py:179][0m |           0.0141 |          20.5895 |        -101.9596 |
[32m[20221213 12:45:15 @agent_ppo2.py:179][0m |          -0.0211 |          20.1080 |        -114.2447 |
[32m[20221213 12:45:15 @agent_ppo2.py:179][0m |          -0.0336 |          19.7053 |        -122.1711 |
[32m[20221213 12:45:16 @agent_ppo2.py:179][0m |          -0.0438 |          19.5107 |        -129.6087 |
[32m[20221213 12:45:16 @agent_ppo2.py:179][0m |          -0.0479 |          19.2373 |        -135.5332 |
[32m[20221213 12:45:16 @agent_ppo2.py:179][0m |          -0.0475 |          19.0786 |        -138.5287 |
[32m[20221213 12:45:16 @agent_ppo2.py:179][0m |          -0.0522 |          18.9014 |        -144.5299 |
[32m[20221213 12:45:16 @agent_ppo2.py:179][0m |          -0.0394 |          20.8048 |        -147.2153 |
[32m[20221213 12:45:16 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:45:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 187.33
[32m[20221213 12:45:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.74
[32m[20221213 12:45:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.97
[32m[20221213 12:45:16 @agent_ppo2.py:137][0m Total time:      29.33 min
[32m[20221213 12:45:16 @agent_ppo2.py:139][0m 2000896 total steps have happened
[32m[20221213 12:45:16 @agent_ppo2.py:115][0m #------------------------ Iteration 977 --------------------------#
[32m[20221213 12:45:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:17 @agent_ppo2.py:179][0m |           0.1016 |           6.3049 |        -108.1413 |
[32m[20221213 12:45:17 @agent_ppo2.py:179][0m |           0.1082 |           5.7259 |         -60.6486 |
[32m[20221213 12:45:17 @agent_ppo2.py:179][0m |           0.0954 |           5.6477 |         -36.5113 |
[32m[20221213 12:45:17 @agent_ppo2.py:179][0m |           0.0863 |           5.5956 |         -39.0301 |
[32m[20221213 12:45:17 @agent_ppo2.py:179][0m |           0.0746 |           5.5687 |         -41.1779 |
[32m[20221213 12:45:17 @agent_ppo2.py:179][0m |           0.0711 |           5.5611 |         -48.0090 |
[32m[20221213 12:45:17 @agent_ppo2.py:179][0m |           0.0589 |           5.5717 |         -56.1296 |
[32m[20221213 12:45:18 @agent_ppo2.py:179][0m |           0.0736 |           5.4756 |         -60.0426 |
[32m[20221213 12:45:18 @agent_ppo2.py:179][0m |           0.1138 |           5.5160 |         -42.9313 |
[32m[20221213 12:45:18 @agent_ppo2.py:179][0m |           0.0720 |           5.5659 |         -56.9477 |
[32m[20221213 12:45:18 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:45:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 38.73
[32m[20221213 12:45:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 44.22
[32m[20221213 12:45:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.61
[32m[20221213 12:45:18 @agent_ppo2.py:137][0m Total time:      29.35 min
[32m[20221213 12:45:18 @agent_ppo2.py:139][0m 2002944 total steps have happened
[32m[20221213 12:45:18 @agent_ppo2.py:115][0m #------------------------ Iteration 978 --------------------------#
[32m[20221213 12:45:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:18 @agent_ppo2.py:179][0m |           0.0809 |          33.0404 |        -143.8859 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |           0.0653 |          31.1396 |        -115.2995 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |           0.0213 |          30.1480 |        -137.6263 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |           0.0100 |          30.5421 |        -141.4218 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |          -0.0039 |          29.4998 |        -143.8721 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |          -0.0164 |          30.1044 |        -152.2669 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |          -0.0286 |          28.6428 |        -157.9835 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |          -0.0331 |          28.3854 |        -160.0256 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |          -0.0368 |          28.2101 |        -162.2266 |
[32m[20221213 12:45:19 @agent_ppo2.py:179][0m |          -0.0386 |          28.1055 |        -163.6858 |
[32m[20221213 12:45:19 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:45:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.60
[32m[20221213 12:45:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.54
[32m[20221213 12:45:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.29
[32m[20221213 12:45:20 @agent_ppo2.py:137][0m Total time:      29.38 min
[32m[20221213 12:45:20 @agent_ppo2.py:139][0m 2004992 total steps have happened
[32m[20221213 12:45:20 @agent_ppo2.py:115][0m #------------------------ Iteration 979 --------------------------#
[32m[20221213 12:45:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:20 @agent_ppo2.py:179][0m |           0.0832 |          16.4469 |        -136.6589 |
[32m[20221213 12:45:20 @agent_ppo2.py:179][0m |           0.0605 |          15.2317 |        -109.6827 |
[32m[20221213 12:45:20 @agent_ppo2.py:179][0m |           0.0127 |          14.7519 |        -127.5290 |
[32m[20221213 12:45:20 @agent_ppo2.py:179][0m |           0.0007 |          14.4741 |        -124.4325 |
[32m[20221213 12:45:21 @agent_ppo2.py:179][0m |          -0.0206 |          14.2924 |        -136.4770 |
[32m[20221213 12:45:21 @agent_ppo2.py:179][0m |          -0.0339 |          14.1353 |        -138.9762 |
[32m[20221213 12:45:21 @agent_ppo2.py:179][0m |          -0.0392 |          14.0384 |        -142.7629 |
[32m[20221213 12:45:21 @agent_ppo2.py:179][0m |          -0.0403 |          14.1440 |        -142.4565 |
[32m[20221213 12:45:21 @agent_ppo2.py:179][0m |          -0.0497 |          13.8102 |        -146.0439 |
[32m[20221213 12:45:21 @agent_ppo2.py:179][0m |          -0.0495 |          13.6963 |        -147.5812 |
[32m[20221213 12:45:21 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:45:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.72
[32m[20221213 12:45:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.86
[32m[20221213 12:45:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 18.15
[32m[20221213 12:45:21 @agent_ppo2.py:137][0m Total time:      29.41 min
[32m[20221213 12:45:21 @agent_ppo2.py:139][0m 2007040 total steps have happened
[32m[20221213 12:45:21 @agent_ppo2.py:115][0m #------------------------ Iteration 980 --------------------------#
[32m[20221213 12:45:22 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:45:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:22 @agent_ppo2.py:179][0m |           0.0854 |          25.0020 |        -143.7844 |
[32m[20221213 12:45:22 @agent_ppo2.py:179][0m |           0.0625 |          23.2078 |         -94.1552 |
[32m[20221213 12:45:22 @agent_ppo2.py:179][0m |           0.0152 |          22.4945 |        -103.7874 |
[32m[20221213 12:45:22 @agent_ppo2.py:179][0m |          -0.0121 |          21.9892 |        -116.5087 |
[32m[20221213 12:45:22 @agent_ppo2.py:179][0m |          -0.0268 |          21.6025 |        -121.7054 |
[32m[20221213 12:45:22 @agent_ppo2.py:179][0m |          -0.0357 |          21.3667 |        -126.8191 |
[32m[20221213 12:45:23 @agent_ppo2.py:179][0m |          -0.0410 |          21.0517 |        -130.0130 |
[32m[20221213 12:45:23 @agent_ppo2.py:179][0m |          -0.0443 |          20.8731 |        -133.3988 |
[32m[20221213 12:45:23 @agent_ppo2.py:179][0m |          -0.0398 |          20.7060 |        -134.1679 |
[32m[20221213 12:45:23 @agent_ppo2.py:179][0m |          -0.0402 |          20.5023 |        -135.4457 |
[32m[20221213 12:45:23 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:45:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 269.62
[32m[20221213 12:45:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.93
[32m[20221213 12:45:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 365.07
[32m[20221213 12:45:23 @agent_ppo2.py:137][0m Total time:      29.44 min
[32m[20221213 12:45:23 @agent_ppo2.py:139][0m 2009088 total steps have happened
[32m[20221213 12:45:23 @agent_ppo2.py:115][0m #------------------------ Iteration 981 --------------------------#
[32m[20221213 12:45:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |           0.0832 |          34.2070 |        -155.7317 |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |           0.0605 |          31.9778 |        -139.4180 |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |          -0.0029 |          31.2258 |        -148.7922 |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |          -0.0152 |          30.7633 |        -156.1798 |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |          -0.0244 |          30.3555 |        -156.7402 |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |          -0.0352 |          30.1040 |        -162.3334 |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |          -0.0416 |          29.7707 |        -165.9124 |
[32m[20221213 12:45:24 @agent_ppo2.py:179][0m |          -0.0420 |          29.5666 |        -169.9798 |
[32m[20221213 12:45:25 @agent_ppo2.py:179][0m |          -0.0471 |          29.3323 |        -168.6158 |
[32m[20221213 12:45:25 @agent_ppo2.py:179][0m |          -0.0443 |          29.5225 |        -173.2756 |
[32m[20221213 12:45:25 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:45:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.10
[32m[20221213 12:45:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.57
[32m[20221213 12:45:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 34.94
[32m[20221213 12:45:25 @agent_ppo2.py:137][0m Total time:      29.47 min
[32m[20221213 12:45:25 @agent_ppo2.py:139][0m 2011136 total steps have happened
[32m[20221213 12:45:25 @agent_ppo2.py:115][0m #------------------------ Iteration 982 --------------------------#
[32m[20221213 12:45:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:25 @agent_ppo2.py:179][0m |           0.0607 |          19.2045 |        -158.5552 |
[32m[20221213 12:45:25 @agent_ppo2.py:179][0m |           0.0441 |          17.5725 |        -155.3104 |
[32m[20221213 12:45:25 @agent_ppo2.py:179][0m |           0.0175 |          15.9081 |        -158.5209 |
[32m[20221213 12:45:26 @agent_ppo2.py:179][0m |          -0.0113 |          15.3839 |        -165.8017 |
[32m[20221213 12:45:26 @agent_ppo2.py:179][0m |          -0.0281 |          15.0495 |        -169.5265 |
[32m[20221213 12:45:26 @agent_ppo2.py:179][0m |          -0.0288 |          14.7675 |        -173.1869 |
[32m[20221213 12:45:26 @agent_ppo2.py:179][0m |          -0.0213 |          14.8811 |        -171.4244 |
[32m[20221213 12:45:26 @agent_ppo2.py:179][0m |          -0.0360 |          14.4518 |        -176.3374 |
[32m[20221213 12:45:26 @agent_ppo2.py:179][0m |          -0.0414 |          14.3105 |        -179.4303 |
[32m[20221213 12:45:26 @agent_ppo2.py:179][0m |          -0.0312 |          14.1699 |        -178.7764 |
[32m[20221213 12:45:26 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:45:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 178.23
[32m[20221213 12:45:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.11
[32m[20221213 12:45:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 17.67
[32m[20221213 12:45:27 @agent_ppo2.py:137][0m Total time:      29.50 min
[32m[20221213 12:45:27 @agent_ppo2.py:139][0m 2013184 total steps have happened
[32m[20221213 12:45:27 @agent_ppo2.py:115][0m #------------------------ Iteration 983 --------------------------#
[32m[20221213 12:45:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:27 @agent_ppo2.py:179][0m |           0.0835 |          19.6464 |        -134.1757 |
[32m[20221213 12:45:27 @agent_ppo2.py:179][0m |           0.0373 |          17.6749 |        -103.2951 |
[32m[20221213 12:45:27 @agent_ppo2.py:179][0m |          -0.0069 |          16.9904 |        -118.7528 |
[32m[20221213 12:45:27 @agent_ppo2.py:179][0m |          -0.0273 |          16.5814 |        -127.8584 |
[32m[20221213 12:45:27 @agent_ppo2.py:179][0m |          -0.0383 |          16.6445 |        -133.9291 |
[32m[20221213 12:45:28 @agent_ppo2.py:179][0m |          -0.0447 |          16.2634 |        -135.0193 |
[32m[20221213 12:45:28 @agent_ppo2.py:179][0m |          -0.0453 |          16.1007 |        -139.5483 |
[32m[20221213 12:45:28 @agent_ppo2.py:179][0m |          -0.0542 |          15.9897 |        -140.7105 |
[32m[20221213 12:45:28 @agent_ppo2.py:179][0m |          -0.0587 |          15.8829 |        -145.7777 |
[32m[20221213 12:45:28 @agent_ppo2.py:179][0m |          -0.0577 |          15.8678 |        -146.6990 |
[32m[20221213 12:45:28 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:45:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 160.47
[32m[20221213 12:45:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 372.57
[32m[20221213 12:45:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 377.84
[32m[20221213 12:45:28 @agent_ppo2.py:137][0m Total time:      29.53 min
[32m[20221213 12:45:28 @agent_ppo2.py:139][0m 2015232 total steps have happened
[32m[20221213 12:45:28 @agent_ppo2.py:115][0m #------------------------ Iteration 984 --------------------------#
[32m[20221213 12:45:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:29 @agent_ppo2.py:179][0m |           0.0589 |          24.1332 |        -146.3352 |
[32m[20221213 12:45:29 @agent_ppo2.py:179][0m |           0.0443 |          22.0944 |        -123.4629 |
[32m[20221213 12:45:29 @agent_ppo2.py:179][0m |          -0.0054 |          21.6385 |        -141.2751 |
[32m[20221213 12:45:29 @agent_ppo2.py:179][0m |          -0.0196 |          21.4384 |        -147.9213 |
[32m[20221213 12:45:29 @agent_ppo2.py:179][0m |          -0.0329 |          21.2680 |        -150.5462 |
[32m[20221213 12:45:29 @agent_ppo2.py:179][0m |          -0.0308 |          21.1303 |        -151.4956 |
[32m[20221213 12:45:29 @agent_ppo2.py:179][0m |          -0.0389 |          21.0596 |        -156.6499 |
[32m[20221213 12:45:30 @agent_ppo2.py:179][0m |          -0.0408 |          21.0877 |        -162.6760 |
[32m[20221213 12:45:30 @agent_ppo2.py:179][0m |          -0.0389 |          20.8631 |        -161.2352 |
[32m[20221213 12:45:30 @agent_ppo2.py:179][0m |          -0.0475 |          20.7480 |        -165.1347 |
[32m[20221213 12:45:30 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:45:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 213.79
[32m[20221213 12:45:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.29
[32m[20221213 12:45:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.28
[32m[20221213 12:45:30 @agent_ppo2.py:137][0m Total time:      29.55 min
[32m[20221213 12:45:30 @agent_ppo2.py:139][0m 2017280 total steps have happened
[32m[20221213 12:45:30 @agent_ppo2.py:115][0m #------------------------ Iteration 985 --------------------------#
[32m[20221213 12:45:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:30 @agent_ppo2.py:179][0m |           0.0861 |          35.7496 |        -190.9839 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |           0.1043 |          33.7944 |        -132.5121 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |           0.0342 |          32.9342 |        -165.3302 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |           0.0055 |          32.3864 |        -177.1001 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |          -0.0218 |          31.7814 |        -191.0794 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |          -0.0287 |          31.4681 |        -198.1036 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |          -0.0367 |          31.3739 |        -206.1257 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |          -0.0453 |          30.9709 |        -211.0107 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |          -0.0508 |          30.7850 |        -215.5397 |
[32m[20221213 12:45:31 @agent_ppo2.py:179][0m |          -0.0390 |          31.7734 |        -216.6275 |
[32m[20221213 12:45:31 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:45:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 282.43
[32m[20221213 12:45:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.07
[32m[20221213 12:45:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.40
[32m[20221213 12:45:32 @agent_ppo2.py:137][0m Total time:      29.58 min
[32m[20221213 12:45:32 @agent_ppo2.py:139][0m 2019328 total steps have happened
[32m[20221213 12:45:32 @agent_ppo2.py:115][0m #------------------------ Iteration 986 --------------------------#
[32m[20221213 12:45:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:32 @agent_ppo2.py:179][0m |           0.0824 |          39.6454 |        -200.7461 |
[32m[20221213 12:45:32 @agent_ppo2.py:179][0m |           0.0654 |          32.6116 |        -162.3999 |
[32m[20221213 12:45:32 @agent_ppo2.py:179][0m |           0.0170 |          31.8016 |        -184.5769 |
[32m[20221213 12:45:32 @agent_ppo2.py:179][0m |          -0.0109 |          31.3463 |        -205.3074 |
[32m[20221213 12:45:33 @agent_ppo2.py:179][0m |          -0.0180 |          33.1218 |        -217.9850 |
[32m[20221213 12:45:33 @agent_ppo2.py:179][0m |          -0.0343 |          30.7296 |        -220.1791 |
[32m[20221213 12:45:33 @agent_ppo2.py:179][0m |          -0.0367 |          30.4775 |        -225.3873 |
[32m[20221213 12:45:33 @agent_ppo2.py:179][0m |          -0.0414 |          30.3607 |        -229.2847 |
[32m[20221213 12:45:33 @agent_ppo2.py:179][0m |          -0.0421 |          30.1060 |        -227.6093 |
[32m[20221213 12:45:33 @agent_ppo2.py:179][0m |          -0.0462 |          29.9812 |        -226.6792 |
[32m[20221213 12:45:33 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:45:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 298.66
[32m[20221213 12:45:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.66
[32m[20221213 12:45:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.57
[32m[20221213 12:45:33 @agent_ppo2.py:137][0m Total time:      29.61 min
[32m[20221213 12:45:33 @agent_ppo2.py:139][0m 2021376 total steps have happened
[32m[20221213 12:45:33 @agent_ppo2.py:115][0m #------------------------ Iteration 987 --------------------------#
[32m[20221213 12:45:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:34 @agent_ppo2.py:179][0m |           0.0716 |          33.7839 |        -180.6292 |
[32m[20221213 12:45:34 @agent_ppo2.py:179][0m |           0.0406 |          31.9363 |        -169.9568 |
[32m[20221213 12:45:34 @agent_ppo2.py:179][0m |           0.0099 |          31.1270 |        -178.5014 |
[32m[20221213 12:45:34 @agent_ppo2.py:179][0m |          -0.0040 |          31.3736 |        -188.1189 |
[32m[20221213 12:45:34 @agent_ppo2.py:179][0m |          -0.0203 |          30.5206 |        -194.7763 |
[32m[20221213 12:45:34 @agent_ppo2.py:179][0m |          -0.0227 |          30.4939 |        -198.4213 |
[32m[20221213 12:45:34 @agent_ppo2.py:179][0m |          -0.0268 |          29.8751 |        -204.3218 |
[32m[20221213 12:45:35 @agent_ppo2.py:179][0m |          -0.0314 |          29.5337 |        -198.9510 |
[32m[20221213 12:45:35 @agent_ppo2.py:179][0m |          -0.0425 |          29.3340 |        -209.1856 |
[32m[20221213 12:45:35 @agent_ppo2.py:179][0m |          -0.0372 |          29.1132 |        -211.9319 |
[32m[20221213 12:45:35 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:45:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.29
[32m[20221213 12:45:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.44
[32m[20221213 12:45:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 302.86
[32m[20221213 12:45:35 @agent_ppo2.py:137][0m Total time:      29.64 min
[32m[20221213 12:45:35 @agent_ppo2.py:139][0m 2023424 total steps have happened
[32m[20221213 12:45:35 @agent_ppo2.py:115][0m #------------------------ Iteration 988 --------------------------#
[32m[20221213 12:45:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:35 @agent_ppo2.py:179][0m |           0.0883 |          26.2221 |        -190.1457 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |           0.0664 |          23.2540 |        -168.3225 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |           0.0190 |          22.4449 |        -185.5746 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |          -0.0141 |          20.6045 |        -199.0815 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |          -0.0206 |          19.8969 |        -201.6303 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |          -0.0318 |          19.5122 |        -205.2986 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |          -0.0340 |          19.1570 |        -208.8732 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |          -0.0405 |          18.8657 |        -211.4510 |
[32m[20221213 12:45:36 @agent_ppo2.py:179][0m |          -0.0419 |          18.6034 |        -217.0220 |
[32m[20221213 12:45:37 @agent_ppo2.py:179][0m |          -0.0521 |          18.3783 |        -221.0013 |
[32m[20221213 12:45:37 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:45:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 230.91
[32m[20221213 12:45:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.31
[32m[20221213 12:45:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.76
[32m[20221213 12:45:37 @agent_ppo2.py:137][0m Total time:      29.67 min
[32m[20221213 12:45:37 @agent_ppo2.py:139][0m 2025472 total steps have happened
[32m[20221213 12:45:37 @agent_ppo2.py:115][0m #------------------------ Iteration 989 --------------------------#
[32m[20221213 12:45:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:37 @agent_ppo2.py:179][0m |           0.0993 |          33.3559 |        -193.0205 |
[32m[20221213 12:45:37 @agent_ppo2.py:179][0m |           0.0570 |          30.6293 |        -155.9145 |
[32m[20221213 12:45:37 @agent_ppo2.py:179][0m |           0.0103 |          29.5261 |        -176.4862 |
[32m[20221213 12:45:38 @agent_ppo2.py:179][0m |          -0.0147 |          28.8338 |        -192.7575 |
[32m[20221213 12:45:38 @agent_ppo2.py:179][0m |          -0.0226 |          28.5014 |        -201.6132 |
[32m[20221213 12:45:38 @agent_ppo2.py:179][0m |          -0.0294 |          28.7500 |        -208.3683 |
[32m[20221213 12:45:38 @agent_ppo2.py:179][0m |          -0.0346 |          27.6504 |        -210.1999 |
[32m[20221213 12:45:38 @agent_ppo2.py:179][0m |          -0.0392 |          27.2443 |        -213.2339 |
[32m[20221213 12:45:38 @agent_ppo2.py:179][0m |          -0.0449 |          27.1093 |        -218.8051 |
[32m[20221213 12:45:38 @agent_ppo2.py:179][0m |          -0.0485 |          26.8042 |        -223.6110 |
[32m[20221213 12:45:38 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:45:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 294.91
[32m[20221213 12:45:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.48
[32m[20221213 12:45:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.33
[32m[20221213 12:45:38 @agent_ppo2.py:137][0m Total time:      29.70 min
[32m[20221213 12:45:38 @agent_ppo2.py:139][0m 2027520 total steps have happened
[32m[20221213 12:45:38 @agent_ppo2.py:115][0m #------------------------ Iteration 990 --------------------------#
[32m[20221213 12:45:39 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:45:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:39 @agent_ppo2.py:179][0m |           0.0793 |           5.0382 |        -185.2445 |
[32m[20221213 12:45:39 @agent_ppo2.py:179][0m |           0.0639 |           4.1508 |        -155.7534 |
[32m[20221213 12:45:39 @agent_ppo2.py:179][0m |           0.0358 |           3.9875 |        -159.8478 |
[32m[20221213 12:45:39 @agent_ppo2.py:179][0m |           0.0312 |           3.8782 |        -155.8614 |
[32m[20221213 12:45:39 @agent_ppo2.py:179][0m |           0.0430 |           3.8117 |        -151.3637 |
[32m[20221213 12:45:39 @agent_ppo2.py:179][0m |           0.0258 |           3.8045 |        -164.1278 |
[32m[20221213 12:45:40 @agent_ppo2.py:179][0m |           0.0346 |           4.0079 |        -161.5891 |
[32m[20221213 12:45:40 @agent_ppo2.py:179][0m |           0.0452 |           3.7238 |        -159.9861 |
[32m[20221213 12:45:40 @agent_ppo2.py:179][0m |           0.0315 |           3.6926 |        -164.0992 |
[32m[20221213 12:45:40 @agent_ppo2.py:179][0m |           0.0207 |           3.6727 |        -163.5933 |
[32m[20221213 12:45:40 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:45:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.37
[32m[20221213 12:45:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 37.94
[32m[20221213 12:45:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.55
[32m[20221213 12:45:40 @agent_ppo2.py:137][0m Total time:      29.72 min
[32m[20221213 12:45:40 @agent_ppo2.py:139][0m 2029568 total steps have happened
[32m[20221213 12:45:40 @agent_ppo2.py:115][0m #------------------------ Iteration 991 --------------------------#
[32m[20221213 12:45:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |           0.1131 |          39.3590 |        -161.5391 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |           0.0548 |          33.8317 |        -139.1536 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |           0.0289 |          37.3240 |        -153.2156 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |          -0.0059 |          32.9186 |        -169.6385 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |          -0.0224 |          32.5193 |        -181.9645 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |          -0.0300 |          32.2998 |        -190.8209 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |          -0.0318 |          32.0744 |        -190.9692 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |          -0.0451 |          31.8584 |        -197.1421 |
[32m[20221213 12:45:41 @agent_ppo2.py:179][0m |          -0.0365 |          32.4574 |        -198.8530 |
[32m[20221213 12:45:42 @agent_ppo2.py:179][0m |          -0.0384 |          31.6140 |        -202.0659 |
[32m[20221213 12:45:42 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:45:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.09
[32m[20221213 12:45:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.09
[32m[20221213 12:45:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.87
[32m[20221213 12:45:42 @agent_ppo2.py:137][0m Total time:      29.75 min
[32m[20221213 12:45:42 @agent_ppo2.py:139][0m 2031616 total steps have happened
[32m[20221213 12:45:42 @agent_ppo2.py:115][0m #------------------------ Iteration 992 --------------------------#
[32m[20221213 12:45:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:42 @agent_ppo2.py:179][0m |           0.0805 |          35.4849 |        -170.0624 |
[32m[20221213 12:45:42 @agent_ppo2.py:179][0m |           0.0707 |          36.9036 |        -137.1339 |
[32m[20221213 12:45:42 @agent_ppo2.py:179][0m |           0.0161 |          33.1084 |        -148.4294 |
[32m[20221213 12:45:43 @agent_ppo2.py:179][0m |          -0.0070 |          32.5659 |        -165.4521 |
[32m[20221213 12:45:43 @agent_ppo2.py:179][0m |          -0.0148 |          32.3525 |        -174.3274 |
[32m[20221213 12:45:43 @agent_ppo2.py:179][0m |          -0.0302 |          32.2300 |        -182.6314 |
[32m[20221213 12:45:43 @agent_ppo2.py:179][0m |          -0.0315 |          32.3386 |        -187.1565 |
[32m[20221213 12:45:43 @agent_ppo2.py:179][0m |          -0.0354 |          32.8981 |        -188.8927 |
[32m[20221213 12:45:43 @agent_ppo2.py:179][0m |          -0.0316 |          33.6374 |        -192.9325 |
[32m[20221213 12:45:43 @agent_ppo2.py:179][0m |          -0.0399 |          31.8831 |        -194.8435 |
[32m[20221213 12:45:43 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:45:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.01
[32m[20221213 12:45:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.35
[32m[20221213 12:45:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 372.99
[32m[20221213 12:45:44 @agent_ppo2.py:137][0m Total time:      29.78 min
[32m[20221213 12:45:44 @agent_ppo2.py:139][0m 2033664 total steps have happened
[32m[20221213 12:45:44 @agent_ppo2.py:115][0m #------------------------ Iteration 993 --------------------------#
[32m[20221213 12:45:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:44 @agent_ppo2.py:179][0m |           0.0589 |          28.7069 |        -199.6086 |
[32m[20221213 12:45:44 @agent_ppo2.py:179][0m |           0.0358 |          27.4695 |        -180.6422 |
[32m[20221213 12:45:44 @agent_ppo2.py:179][0m |           0.0058 |          26.1831 |        -185.2152 |
[32m[20221213 12:45:44 @agent_ppo2.py:179][0m |          -0.0028 |          29.1067 |        -198.3742 |
[32m[20221213 12:45:44 @agent_ppo2.py:179][0m |          -0.0176 |          24.8290 |        -197.6273 |
[32m[20221213 12:45:45 @agent_ppo2.py:179][0m |          -0.0301 |          24.3440 |        -205.1954 |
[32m[20221213 12:45:45 @agent_ppo2.py:179][0m |          -0.0364 |          24.0567 |        -212.2316 |
[32m[20221213 12:45:45 @agent_ppo2.py:179][0m |          -0.0415 |          23.8091 |        -217.3083 |
[32m[20221213 12:45:45 @agent_ppo2.py:179][0m |          -0.0323 |          23.5455 |        -214.6233 |
[32m[20221213 12:45:45 @agent_ppo2.py:179][0m |          -0.0411 |          23.2557 |        -224.4755 |
[32m[20221213 12:45:45 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:45:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 261.70
[32m[20221213 12:45:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.77
[32m[20221213 12:45:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 282.90
[32m[20221213 12:45:45 @agent_ppo2.py:137][0m Total time:      29.81 min
[32m[20221213 12:45:45 @agent_ppo2.py:139][0m 2035712 total steps have happened
[32m[20221213 12:45:45 @agent_ppo2.py:115][0m #------------------------ Iteration 994 --------------------------#
[32m[20221213 12:45:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:46 @agent_ppo2.py:179][0m |           0.0953 |          30.6517 |        -185.0396 |
[32m[20221213 12:45:46 @agent_ppo2.py:179][0m |           0.0577 |          28.6067 |        -173.9086 |
[32m[20221213 12:45:46 @agent_ppo2.py:179][0m |           0.0105 |          27.1205 |        -191.3216 |
[32m[20221213 12:45:46 @agent_ppo2.py:179][0m |          -0.0104 |          29.3818 |        -200.0413 |
[32m[20221213 12:45:46 @agent_ppo2.py:179][0m |          -0.0357 |          26.0438 |        -207.4466 |
[32m[20221213 12:45:46 @agent_ppo2.py:179][0m |          -0.0278 |          26.0483 |        -207.8514 |
[32m[20221213 12:45:46 @agent_ppo2.py:179][0m |          -0.0384 |          25.1727 |        -213.2241 |
[32m[20221213 12:45:47 @agent_ppo2.py:179][0m |          -0.0423 |          24.9274 |        -215.0369 |
[32m[20221213 12:45:47 @agent_ppo2.py:179][0m |          -0.0462 |          24.6013 |        -216.0250 |
[32m[20221213 12:45:47 @agent_ppo2.py:179][0m |          -0.0515 |          24.4225 |        -223.6153 |
[32m[20221213 12:45:47 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:45:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 246.95
[32m[20221213 12:45:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.04
[32m[20221213 12:45:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.03
[32m[20221213 12:45:47 @agent_ppo2.py:137][0m Total time:      29.84 min
[32m[20221213 12:45:47 @agent_ppo2.py:139][0m 2037760 total steps have happened
[32m[20221213 12:45:47 @agent_ppo2.py:115][0m #------------------------ Iteration 995 --------------------------#
[32m[20221213 12:45:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:47 @agent_ppo2.py:179][0m |           0.0637 |          16.6452 |        -180.9323 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |           0.0444 |          15.2286 |        -172.6072 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |           0.0151 |          14.4838 |        -167.8755 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |          -0.0124 |          14.0730 |        -189.8693 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |          -0.0218 |          13.9177 |        -190.6073 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |          -0.0215 |          13.7330 |        -194.5476 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |          -0.0270 |          13.6313 |        -198.0188 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |          -0.0299 |          13.4942 |        -199.3049 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |          -0.0315 |          13.3808 |        -203.7708 |
[32m[20221213 12:45:48 @agent_ppo2.py:179][0m |          -0.0389 |          13.2869 |        -208.5091 |
[32m[20221213 12:45:48 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:45:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 140.02
[32m[20221213 12:45:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.55
[32m[20221213 12:45:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.84
[32m[20221213 12:45:49 @agent_ppo2.py:137][0m Total time:      29.87 min
[32m[20221213 12:45:49 @agent_ppo2.py:139][0m 2039808 total steps have happened
[32m[20221213 12:45:49 @agent_ppo2.py:115][0m #------------------------ Iteration 996 --------------------------#
[32m[20221213 12:45:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:49 @agent_ppo2.py:179][0m |           0.1002 |          29.6897 |        -185.1146 |
[32m[20221213 12:45:49 @agent_ppo2.py:179][0m |           0.0596 |          27.6071 |        -148.5739 |
[32m[20221213 12:45:49 @agent_ppo2.py:179][0m |           0.0068 |          27.0424 |        -173.0125 |
[32m[20221213 12:45:49 @agent_ppo2.py:179][0m |           0.0021 |          29.2027 |        -183.7532 |
[32m[20221213 12:45:50 @agent_ppo2.py:179][0m |          -0.0199 |          26.5366 |        -187.3467 |
[32m[20221213 12:45:50 @agent_ppo2.py:179][0m |          -0.0233 |          26.0950 |        -189.2211 |
[32m[20221213 12:45:50 @agent_ppo2.py:179][0m |          -0.0273 |          26.0480 |        -195.3701 |
[32m[20221213 12:45:50 @agent_ppo2.py:179][0m |          -0.0320 |          25.9871 |        -196.9482 |
[32m[20221213 12:45:50 @agent_ppo2.py:179][0m |          -0.0400 |          25.7516 |        -203.0355 |
[32m[20221213 12:45:50 @agent_ppo2.py:179][0m |          -0.0431 |          25.6022 |        -203.9219 |
[32m[20221213 12:45:50 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:45:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 270.77
[32m[20221213 12:45:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.35
[32m[20221213 12:45:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 300.20
[32m[20221213 12:45:50 @agent_ppo2.py:137][0m Total time:      29.89 min
[32m[20221213 12:45:50 @agent_ppo2.py:139][0m 2041856 total steps have happened
[32m[20221213 12:45:50 @agent_ppo2.py:115][0m #------------------------ Iteration 997 --------------------------#
[32m[20221213 12:45:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:51 @agent_ppo2.py:179][0m |           0.0981 |          34.8726 |        -156.5273 |
[32m[20221213 12:45:51 @agent_ppo2.py:179][0m |           0.1048 |          32.5254 |         -79.6326 |
[32m[20221213 12:45:51 @agent_ppo2.py:179][0m |           0.0719 |          31.7123 |         -84.6943 |
[32m[20221213 12:45:51 @agent_ppo2.py:179][0m |           0.0353 |          31.3577 |        -118.7386 |
[32m[20221213 12:45:51 @agent_ppo2.py:179][0m |           0.0056 |          31.0531 |        -138.3368 |
[32m[20221213 12:45:52 @agent_ppo2.py:179][0m |          -0.0056 |          30.8240 |        -150.2292 |
[32m[20221213 12:45:52 @agent_ppo2.py:179][0m |          -0.0214 |          30.6706 |        -161.1337 |
[32m[20221213 12:45:52 @agent_ppo2.py:179][0m |          -0.0293 |          30.3193 |        -168.3172 |
[32m[20221213 12:45:52 @agent_ppo2.py:179][0m |          -0.0272 |          30.2349 |        -170.4227 |
[32m[20221213 12:45:52 @agent_ppo2.py:179][0m |          -0.0329 |          30.0213 |        -177.1234 |
[32m[20221213 12:45:52 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:45:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.43
[32m[20221213 12:45:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.75
[32m[20221213 12:45:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 26.94
[32m[20221213 12:45:52 @agent_ppo2.py:137][0m Total time:      29.93 min
[32m[20221213 12:45:52 @agent_ppo2.py:139][0m 2043904 total steps have happened
[32m[20221213 12:45:52 @agent_ppo2.py:115][0m #------------------------ Iteration 998 --------------------------#
[32m[20221213 12:45:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:53 @agent_ppo2.py:179][0m |           0.1081 |          33.5859 |        -149.5506 |
[32m[20221213 12:45:53 @agent_ppo2.py:179][0m |           0.0586 |          32.2343 |        -120.6566 |
[32m[20221213 12:45:53 @agent_ppo2.py:179][0m |           0.0247 |          31.4432 |        -144.2617 |
[32m[20221213 12:45:53 @agent_ppo2.py:179][0m |          -0.0049 |          31.0605 |        -156.3006 |
[32m[20221213 12:45:53 @agent_ppo2.py:179][0m |          -0.0157 |          31.1545 |        -164.1384 |
[32m[20221213 12:45:53 @agent_ppo2.py:179][0m |          -0.0315 |          30.4096 |        -172.4878 |
[32m[20221213 12:45:53 @agent_ppo2.py:179][0m |          -0.0301 |          30.2635 |        -176.5776 |
[32m[20221213 12:45:54 @agent_ppo2.py:179][0m |          -0.0260 |          30.0554 |        -175.6848 |
[32m[20221213 12:45:54 @agent_ppo2.py:179][0m |          -0.0349 |          30.1278 |        -180.5832 |
[32m[20221213 12:45:54 @agent_ppo2.py:179][0m |          -0.0277 |          29.6812 |        -178.6644 |
[32m[20221213 12:45:54 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:45:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.20
[32m[20221213 12:45:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 372.81
[32m[20221213 12:45:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 374.61
[32m[20221213 12:45:54 @agent_ppo2.py:137][0m Total time:      29.95 min
[32m[20221213 12:45:54 @agent_ppo2.py:139][0m 2045952 total steps have happened
[32m[20221213 12:45:54 @agent_ppo2.py:115][0m #------------------------ Iteration 999 --------------------------#
[32m[20221213 12:45:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:54 @agent_ppo2.py:179][0m |           0.0997 |          34.1700 |        -142.1142 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |           0.0663 |          32.5958 |        -121.3536 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |           0.0225 |          31.9330 |        -137.6213 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |           0.0130 |          31.4948 |        -137.3275 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |          -0.0124 |          31.2318 |        -152.0007 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |          -0.0248 |          30.9651 |        -160.8014 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |          -0.0278 |          30.9245 |        -166.9682 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |          -0.0197 |          30.5505 |        -163.3474 |
[32m[20221213 12:45:55 @agent_ppo2.py:179][0m |          -0.0367 |          30.4512 |        -172.5611 |
[32m[20221213 12:45:56 @agent_ppo2.py:179][0m |          -0.0377 |          30.3088 |        -173.6136 |
[32m[20221213 12:45:56 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:45:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.42
[32m[20221213 12:45:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.07
[32m[20221213 12:45:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.18
[32m[20221213 12:45:56 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 384.62
[32m[20221213 12:45:56 @agent_ppo2.py:137][0m Total time:      29.98 min
[32m[20221213 12:45:56 @agent_ppo2.py:139][0m 2048000 total steps have happened
[32m[20221213 12:45:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221213 12:45:56 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:45:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:56 @agent_ppo2.py:179][0m |           0.1056 |          32.7013 |        -145.5449 |
[32m[20221213 12:45:56 @agent_ppo2.py:179][0m |           0.1031 |          32.9331 |         -81.1501 |
[32m[20221213 12:45:56 @agent_ppo2.py:179][0m |           0.0398 |          31.3034 |        -111.5350 |
[32m[20221213 12:45:57 @agent_ppo2.py:179][0m |           0.0246 |          31.0115 |        -128.0138 |
[32m[20221213 12:45:57 @agent_ppo2.py:179][0m |          -0.0016 |          30.7291 |        -149.0728 |
[32m[20221213 12:45:57 @agent_ppo2.py:179][0m |          -0.0074 |          30.6102 |        -156.0364 |
[32m[20221213 12:45:57 @agent_ppo2.py:179][0m |          -0.0158 |          30.3059 |        -161.8292 |
[32m[20221213 12:45:57 @agent_ppo2.py:179][0m |          -0.0225 |          30.1802 |        -167.6934 |
[32m[20221213 12:45:57 @agent_ppo2.py:179][0m |          -0.0215 |          29.9380 |        -168.8945 |
[32m[20221213 12:45:57 @agent_ppo2.py:179][0m |          -0.0261 |          29.7703 |        -174.6355 |
[32m[20221213 12:45:57 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:45:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 356.51
[32m[20221213 12:45:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 372.16
[32m[20221213 12:45:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 21.21
[32m[20221213 12:45:58 @agent_ppo2.py:137][0m Total time:      30.01 min
[32m[20221213 12:45:58 @agent_ppo2.py:139][0m 2050048 total steps have happened
[32m[20221213 12:45:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221213 12:45:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:45:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:45:58 @agent_ppo2.py:179][0m |           0.0552 |          28.0284 |        -156.8828 |
[32m[20221213 12:45:58 @agent_ppo2.py:179][0m |           0.0765 |          26.6122 |        -142.1057 |
[32m[20221213 12:45:58 @agent_ppo2.py:179][0m |           0.0148 |          26.2485 |        -153.4523 |
[32m[20221213 12:45:58 @agent_ppo2.py:179][0m |          -0.0117 |          26.0447 |        -162.1948 |
[32m[20221213 12:45:58 @agent_ppo2.py:179][0m |          -0.0130 |          25.6697 |        -158.8384 |
[32m[20221213 12:45:59 @agent_ppo2.py:179][0m |          -0.0221 |          25.4298 |        -164.1379 |
[32m[20221213 12:45:59 @agent_ppo2.py:179][0m |          -0.0189 |          26.6704 |        -167.3624 |
[32m[20221213 12:45:59 @agent_ppo2.py:179][0m |          -0.0280 |          25.1019 |        -167.0869 |
[32m[20221213 12:45:59 @agent_ppo2.py:179][0m |          -0.0300 |          24.9287 |        -171.5877 |
[32m[20221213 12:45:59 @agent_ppo2.py:179][0m |          -0.0368 |          24.8043 |        -174.0475 |
[32m[20221213 12:45:59 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:45:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.69
[32m[20221213 12:45:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.41
[32m[20221213 12:45:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.85
[32m[20221213 12:45:59 @agent_ppo2.py:137][0m Total time:      30.04 min
[32m[20221213 12:45:59 @agent_ppo2.py:139][0m 2052096 total steps have happened
[32m[20221213 12:45:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221213 12:45:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:00 @agent_ppo2.py:179][0m |           0.0771 |          20.7072 |        -150.5024 |
[32m[20221213 12:46:00 @agent_ppo2.py:179][0m |           0.0398 |          17.7659 |        -149.9270 |
[32m[20221213 12:46:00 @agent_ppo2.py:179][0m |           0.0008 |          16.4956 |        -164.1414 |
[32m[20221213 12:46:00 @agent_ppo2.py:179][0m |          -0.0182 |          15.7259 |        -168.4940 |
[32m[20221213 12:46:00 @agent_ppo2.py:179][0m |          -0.0242 |          15.4799 |        -172.1789 |
[32m[20221213 12:46:00 @agent_ppo2.py:179][0m |          -0.0290 |          14.8268 |        -175.3004 |
[32m[20221213 12:46:00 @agent_ppo2.py:179][0m |          -0.0316 |          14.4188 |        -176.8628 |
[32m[20221213 12:46:01 @agent_ppo2.py:179][0m |          -0.0378 |          14.1053 |        -177.8240 |
[32m[20221213 12:46:01 @agent_ppo2.py:179][0m |          -0.0376 |          13.7049 |        -179.9936 |
[32m[20221213 12:46:01 @agent_ppo2.py:179][0m |          -0.0380 |          13.3761 |        -182.4493 |
[32m[20221213 12:46:01 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:46:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 205.27
[32m[20221213 12:46:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.99
[32m[20221213 12:46:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.97
[32m[20221213 12:46:01 @agent_ppo2.py:137][0m Total time:      30.07 min
[32m[20221213 12:46:01 @agent_ppo2.py:139][0m 2054144 total steps have happened
[32m[20221213 12:46:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221213 12:46:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:01 @agent_ppo2.py:179][0m |           0.1839 |          36.2151 |        -111.1695 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |           0.0861 |          34.4935 |         -78.3990 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |           0.0359 |          33.9390 |        -101.2763 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |           0.0127 |          33.3854 |        -114.8706 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |          -0.0018 |          33.1489 |        -131.4673 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |          -0.0139 |          32.6865 |        -136.6878 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |          -0.0247 |          32.4789 |        -142.4846 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |          -0.0263 |          32.3066 |        -146.9959 |
[32m[20221213 12:46:02 @agent_ppo2.py:179][0m |          -0.0321 |          32.0551 |        -150.4322 |
[32m[20221213 12:46:03 @agent_ppo2.py:179][0m |          -0.0387 |          31.8120 |        -153.2410 |
[32m[20221213 12:46:03 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:46:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.16
[32m[20221213 12:46:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.88
[32m[20221213 12:46:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.06
[32m[20221213 12:46:03 @agent_ppo2.py:137][0m Total time:      30.10 min
[32m[20221213 12:46:03 @agent_ppo2.py:139][0m 2056192 total steps have happened
[32m[20221213 12:46:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221213 12:46:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:03 @agent_ppo2.py:179][0m |           0.0732 |          37.5074 |        -121.7611 |
[32m[20221213 12:46:03 @agent_ppo2.py:179][0m |           0.0431 |          35.4857 |        -109.7021 |
[32m[20221213 12:46:03 @agent_ppo2.py:179][0m |           0.0098 |          34.8395 |        -123.3645 |
[32m[20221213 12:46:04 @agent_ppo2.py:179][0m |          -0.0089 |          34.4375 |        -135.6412 |
[32m[20221213 12:46:04 @agent_ppo2.py:179][0m |          -0.0196 |          34.1968 |        -141.2087 |
[32m[20221213 12:46:04 @agent_ppo2.py:179][0m |          -0.0269 |          34.4893 |        -147.3402 |
[32m[20221213 12:46:04 @agent_ppo2.py:179][0m |          -0.0363 |          33.8514 |        -152.7731 |
[32m[20221213 12:46:04 @agent_ppo2.py:179][0m |          -0.0375 |          33.7332 |        -156.1712 |
[32m[20221213 12:46:04 @agent_ppo2.py:179][0m |          -0.0435 |          33.6378 |        -158.4829 |
[32m[20221213 12:46:04 @agent_ppo2.py:179][0m |          -0.0403 |          34.4630 |        -158.2528 |
[32m[20221213 12:46:04 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.42
[32m[20221213 12:46:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.04
[32m[20221213 12:46:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.35
[32m[20221213 12:46:05 @agent_ppo2.py:137][0m Total time:      30.13 min
[32m[20221213 12:46:05 @agent_ppo2.py:139][0m 2058240 total steps have happened
[32m[20221213 12:46:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221213 12:46:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:05 @agent_ppo2.py:179][0m |           0.0814 |          35.9705 |        -125.2045 |
[32m[20221213 12:46:05 @agent_ppo2.py:179][0m |           0.0891 |          34.4368 |         -88.1211 |
[32m[20221213 12:46:05 @agent_ppo2.py:179][0m |           0.0284 |          33.8526 |        -114.6912 |
[32m[20221213 12:46:05 @agent_ppo2.py:179][0m |          -0.0043 |          33.3206 |        -130.9397 |
[32m[20221213 12:46:05 @agent_ppo2.py:179][0m |          -0.0147 |          32.9996 |        -134.8313 |
[32m[20221213 12:46:06 @agent_ppo2.py:179][0m |          -0.0198 |          32.8370 |        -139.2325 |
[32m[20221213 12:46:06 @agent_ppo2.py:179][0m |          -0.0231 |          32.3671 |        -140.1201 |
[32m[20221213 12:46:06 @agent_ppo2.py:179][0m |          -0.0273 |          32.2004 |        -143.2173 |
[32m[20221213 12:46:06 @agent_ppo2.py:179][0m |          -0.0159 |          32.0754 |        -136.6818 |
[32m[20221213 12:46:06 @agent_ppo2.py:179][0m |          -0.0318 |          31.7769 |        -144.3893 |
[32m[20221213 12:46:06 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:46:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.05
[32m[20221213 12:46:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.68
[32m[20221213 12:46:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.21
[32m[20221213 12:46:06 @agent_ppo2.py:137][0m Total time:      30.16 min
[32m[20221213 12:46:06 @agent_ppo2.py:139][0m 2060288 total steps have happened
[32m[20221213 12:46:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221213 12:46:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |           0.0699 |          36.7376 |        -127.5984 |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |           0.0354 |          34.1823 |        -111.2404 |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |           0.0084 |          33.6145 |        -121.5289 |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |          -0.0135 |          33.3649 |        -129.9562 |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |          -0.0198 |          33.0552 |        -138.1484 |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |          -0.0248 |          32.9156 |        -136.3403 |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |          -0.0273 |          32.7225 |        -139.1228 |
[32m[20221213 12:46:07 @agent_ppo2.py:179][0m |          -0.0370 |          32.5806 |        -141.0345 |
[32m[20221213 12:46:08 @agent_ppo2.py:179][0m |          -0.0382 |          32.5145 |        -144.9676 |
[32m[20221213 12:46:08 @agent_ppo2.py:179][0m |          -0.0415 |          32.4750 |        -149.5296 |
[32m[20221213 12:46:08 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.12
[32m[20221213 12:46:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 323.11
[32m[20221213 12:46:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.08
[32m[20221213 12:46:08 @agent_ppo2.py:137][0m Total time:      30.19 min
[32m[20221213 12:46:08 @agent_ppo2.py:139][0m 2062336 total steps have happened
[32m[20221213 12:46:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221213 12:46:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:08 @agent_ppo2.py:179][0m |           0.0700 |          34.2215 |        -124.8335 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |           0.0395 |          33.2201 |        -117.8220 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |           0.0048 |          31.2137 |        -125.2647 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |          -0.0179 |          30.6447 |        -134.7490 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |          -0.0283 |          30.2896 |        -141.2589 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |          -0.0380 |          29.5309 |        -144.5230 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |          -0.0337 |          29.8242 |        -144.8980 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |          -0.0379 |          30.0797 |        -149.6944 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |          -0.0439 |          28.3402 |        -150.2467 |
[32m[20221213 12:46:09 @agent_ppo2.py:179][0m |          -0.0445 |          28.0096 |        -153.0541 |
[32m[20221213 12:46:09 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.28
[32m[20221213 12:46:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.79
[32m[20221213 12:46:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 389.60
[32m[20221213 12:46:10 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 389.60
[32m[20221213 12:46:10 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 389.60
[32m[20221213 12:46:10 @agent_ppo2.py:137][0m Total time:      30.22 min
[32m[20221213 12:46:10 @agent_ppo2.py:139][0m 2064384 total steps have happened
[32m[20221213 12:46:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221213 12:46:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:10 @agent_ppo2.py:179][0m |           0.0977 |          31.1912 |        -118.9071 |
[32m[20221213 12:46:10 @agent_ppo2.py:179][0m |           0.0615 |          29.2772 |         -83.5822 |
[32m[20221213 12:46:10 @agent_ppo2.py:179][0m |           0.0074 |          29.1456 |        -103.5844 |
[32m[20221213 12:46:10 @agent_ppo2.py:179][0m |          -0.0243 |          27.9085 |        -109.1022 |
[32m[20221213 12:46:11 @agent_ppo2.py:179][0m |          -0.0378 |          27.5872 |        -112.6762 |
[32m[20221213 12:46:11 @agent_ppo2.py:179][0m |          -0.0382 |          27.3174 |        -114.9847 |
[32m[20221213 12:46:11 @agent_ppo2.py:179][0m |          -0.0423 |          26.9657 |        -116.2285 |
[32m[20221213 12:46:11 @agent_ppo2.py:179][0m |          -0.0430 |          26.9543 |        -118.0240 |
[32m[20221213 12:46:11 @agent_ppo2.py:179][0m |          -0.0393 |          26.6318 |        -115.1143 |
[32m[20221213 12:46:11 @agent_ppo2.py:179][0m |          -0.0405 |          27.0942 |        -117.8293 |
[32m[20221213 12:46:11 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.61
[32m[20221213 12:46:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.98
[32m[20221213 12:46:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.79
[32m[20221213 12:46:11 @agent_ppo2.py:137][0m Total time:      30.24 min
[32m[20221213 12:46:11 @agent_ppo2.py:139][0m 2066432 total steps have happened
[32m[20221213 12:46:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221213 12:46:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:12 @agent_ppo2.py:179][0m |           0.1018 |          29.3069 |        -127.9271 |
[32m[20221213 12:46:12 @agent_ppo2.py:179][0m |           0.1008 |          26.7243 |         -78.8076 |
[32m[20221213 12:46:12 @agent_ppo2.py:179][0m |           0.0379 |          26.1395 |         -83.2298 |
[32m[20221213 12:46:12 @agent_ppo2.py:179][0m |           0.0000 |          25.6749 |         -99.8959 |
[32m[20221213 12:46:12 @agent_ppo2.py:179][0m |          -0.0061 |          25.3952 |        -104.7276 |
[32m[20221213 12:46:12 @agent_ppo2.py:179][0m |          -0.0194 |          25.1571 |        -111.4371 |
[32m[20221213 12:46:13 @agent_ppo2.py:179][0m |          -0.0270 |          25.0423 |        -114.0374 |
[32m[20221213 12:46:13 @agent_ppo2.py:179][0m |          -0.0355 |          24.8062 |        -120.5352 |
[32m[20221213 12:46:13 @agent_ppo2.py:179][0m |          -0.0349 |          24.6781 |        -122.2759 |
[32m[20221213 12:46:13 @agent_ppo2.py:179][0m |          -0.0431 |          24.6093 |        -127.0502 |
[32m[20221213 12:46:13 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.44
[32m[20221213 12:46:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.54
[32m[20221213 12:46:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 310.17
[32m[20221213 12:46:13 @agent_ppo2.py:137][0m Total time:      30.27 min
[32m[20221213 12:46:13 @agent_ppo2.py:139][0m 2068480 total steps have happened
[32m[20221213 12:46:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221213 12:46:13 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:46:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |           0.0983 |          35.2223 |        -145.3335 |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |           0.0720 |          33.0839 |        -108.8214 |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |           0.0178 |          32.2202 |        -120.0532 |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |          -0.0107 |          31.4836 |        -134.3914 |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |          -0.0190 |          30.9969 |        -140.5978 |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |          -0.0266 |          30.6116 |        -147.7003 |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |          -0.0231 |          31.3051 |        -150.9579 |
[32m[20221213 12:46:14 @agent_ppo2.py:179][0m |          -0.0280 |          29.8161 |        -150.8643 |
[32m[20221213 12:46:15 @agent_ppo2.py:179][0m |          -0.0371 |          29.5814 |        -154.1942 |
[32m[20221213 12:46:15 @agent_ppo2.py:179][0m |          -0.0409 |          29.3671 |        -159.2791 |
[32m[20221213 12:46:15 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:46:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.01
[32m[20221213 12:46:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.46
[32m[20221213 12:46:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.66
[32m[20221213 12:46:15 @agent_ppo2.py:137][0m Total time:      30.30 min
[32m[20221213 12:46:15 @agent_ppo2.py:139][0m 2070528 total steps have happened
[32m[20221213 12:46:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221213 12:46:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:15 @agent_ppo2.py:179][0m |           0.0739 |          31.0886 |        -139.0477 |
[32m[20221213 12:46:15 @agent_ppo2.py:179][0m |           0.0542 |          28.0085 |        -122.3385 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0027 |          26.5280 |        -141.2555 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0238 |          25.7707 |        -148.0279 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0310 |          24.8468 |        -151.4533 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0329 |          24.3581 |        -157.2572 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0395 |          23.9225 |        -159.2285 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0442 |          23.5377 |        -160.8184 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0417 |          23.5106 |        -165.5855 |
[32m[20221213 12:46:16 @agent_ppo2.py:179][0m |          -0.0457 |          22.9940 |        -167.6354 |
[32m[20221213 12:46:16 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:46:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 272.51
[32m[20221213 12:46:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.28
[32m[20221213 12:46:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.66
[32m[20221213 12:46:17 @agent_ppo2.py:137][0m Total time:      30.33 min
[32m[20221213 12:46:17 @agent_ppo2.py:139][0m 2072576 total steps have happened
[32m[20221213 12:46:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221213 12:46:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:17 @agent_ppo2.py:179][0m |           0.1415 |          38.1688 |        -127.4998 |
[32m[20221213 12:46:17 @agent_ppo2.py:179][0m |           0.0996 |          36.4266 |         -92.1335 |
[32m[20221213 12:46:17 @agent_ppo2.py:179][0m |           0.0374 |          35.7642 |        -113.9063 |
[32m[20221213 12:46:17 @agent_ppo2.py:179][0m |           0.0123 |          35.3408 |        -128.1802 |
[32m[20221213 12:46:17 @agent_ppo2.py:179][0m |           0.0015 |          38.8748 |        -140.3436 |
[32m[20221213 12:46:18 @agent_ppo2.py:179][0m |          -0.0174 |          34.0850 |        -144.3528 |
[32m[20221213 12:46:18 @agent_ppo2.py:179][0m |          -0.0253 |          33.5682 |        -150.4072 |
[32m[20221213 12:46:18 @agent_ppo2.py:179][0m |          -0.0239 |          33.1510 |        -150.5802 |
[32m[20221213 12:46:18 @agent_ppo2.py:179][0m |          -0.0348 |          32.8119 |        -156.6630 |
[32m[20221213 12:46:18 @agent_ppo2.py:179][0m |          -0.0398 |          32.6839 |        -159.0325 |
[32m[20221213 12:46:18 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.91
[32m[20221213 12:46:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.27
[32m[20221213 12:46:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 375.34
[32m[20221213 12:46:18 @agent_ppo2.py:137][0m Total time:      30.36 min
[32m[20221213 12:46:18 @agent_ppo2.py:139][0m 2074624 total steps have happened
[32m[20221213 12:46:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221213 12:46:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:19 @agent_ppo2.py:179][0m |           0.0915 |          39.5825 |        -130.5883 |
[32m[20221213 12:46:19 @agent_ppo2.py:179][0m |           0.0368 |          37.0834 |        -123.8590 |
[32m[20221213 12:46:19 @agent_ppo2.py:179][0m |           0.0019 |          36.1768 |        -134.3660 |
[32m[20221213 12:46:19 @agent_ppo2.py:179][0m |          -0.0109 |          37.0201 |        -136.1476 |
[32m[20221213 12:46:19 @agent_ppo2.py:179][0m |          -0.0125 |          35.4583 |        -133.5144 |
[32m[20221213 12:46:19 @agent_ppo2.py:179][0m |          -0.0278 |          35.2035 |        -144.8616 |
[32m[20221213 12:46:19 @agent_ppo2.py:179][0m |          -0.0306 |          35.5703 |        -145.1319 |
[32m[20221213 12:46:20 @agent_ppo2.py:179][0m |          -0.0329 |          34.9482 |        -145.0580 |
[32m[20221213 12:46:20 @agent_ppo2.py:179][0m |          -0.0386 |          34.4383 |        -148.6867 |
[32m[20221213 12:46:20 @agent_ppo2.py:179][0m |          -0.0428 |          34.3200 |        -150.8370 |
[32m[20221213 12:46:20 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 318.62
[32m[20221213 12:46:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.88
[32m[20221213 12:46:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 386.76
[32m[20221213 12:46:20 @agent_ppo2.py:137][0m Total time:      30.39 min
[32m[20221213 12:46:20 @agent_ppo2.py:139][0m 2076672 total steps have happened
[32m[20221213 12:46:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221213 12:46:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:20 @agent_ppo2.py:179][0m |           0.0936 |          36.3189 |        -101.2700 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |           0.0889 |          35.4910 |         -69.7793 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |           0.0474 |          35.0567 |         -88.5266 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |           0.0153 |          34.6550 |        -111.0206 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |          -0.0039 |          34.6135 |        -119.3774 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |          -0.0179 |          34.3070 |        -126.4362 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |          -0.0202 |          34.5525 |        -132.1706 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |          -0.0313 |          33.8199 |        -135.2470 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |          -0.0279 |          33.7005 |        -136.6390 |
[32m[20221213 12:46:21 @agent_ppo2.py:179][0m |          -0.0340 |          33.6756 |        -141.3160 |
[32m[20221213 12:46:21 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:46:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.11
[32m[20221213 12:46:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.93
[32m[20221213 12:46:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.94
[32m[20221213 12:46:22 @agent_ppo2.py:137][0m Total time:      30.42 min
[32m[20221213 12:46:22 @agent_ppo2.py:139][0m 2078720 total steps have happened
[32m[20221213 12:46:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221213 12:46:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:22 @agent_ppo2.py:179][0m |           0.0619 |          31.4350 |        -129.5282 |
[32m[20221213 12:46:22 @agent_ppo2.py:179][0m |           0.0588 |          29.8786 |         -92.6416 |
[32m[20221213 12:46:22 @agent_ppo2.py:179][0m |           0.0101 |          29.3131 |         -95.3314 |
[32m[20221213 12:46:23 @agent_ppo2.py:179][0m |          -0.0073 |          29.2467 |         -99.3740 |
[32m[20221213 12:46:23 @agent_ppo2.py:179][0m |          -0.0266 |          28.4336 |        -104.6799 |
[32m[20221213 12:46:23 @agent_ppo2.py:179][0m |          -0.0291 |          28.1436 |        -108.3497 |
[32m[20221213 12:46:23 @agent_ppo2.py:179][0m |          -0.0327 |          27.8044 |        -111.3696 |
[32m[20221213 12:46:23 @agent_ppo2.py:179][0m |          -0.0376 |          27.5476 |        -113.3341 |
[32m[20221213 12:46:23 @agent_ppo2.py:179][0m |          -0.0473 |          27.3965 |        -115.4871 |
[32m[20221213 12:46:23 @agent_ppo2.py:179][0m |          -0.0421 |          27.1960 |        -115.8128 |
[32m[20221213 12:46:23 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 274.51
[32m[20221213 12:46:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.77
[32m[20221213 12:46:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.56
[32m[20221213 12:46:23 @agent_ppo2.py:137][0m Total time:      30.45 min
[32m[20221213 12:46:23 @agent_ppo2.py:139][0m 2080768 total steps have happened
[32m[20221213 12:46:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221213 12:46:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:24 @agent_ppo2.py:179][0m |           0.0882 |          34.9878 |        -111.3689 |
[32m[20221213 12:46:24 @agent_ppo2.py:179][0m |           0.0703 |          33.7817 |         -95.3084 |
[32m[20221213 12:46:24 @agent_ppo2.py:179][0m |           0.0222 |          33.0945 |        -111.8903 |
[32m[20221213 12:46:24 @agent_ppo2.py:179][0m |          -0.0084 |          32.5696 |        -125.1221 |
[32m[20221213 12:46:24 @agent_ppo2.py:179][0m |          -0.0090 |          32.1767 |        -125.9239 |
[32m[20221213 12:46:24 @agent_ppo2.py:179][0m |          -0.0140 |          31.8116 |        -126.9385 |
[32m[20221213 12:46:25 @agent_ppo2.py:179][0m |          -0.0151 |          34.8260 |        -134.4863 |
[32m[20221213 12:46:25 @agent_ppo2.py:179][0m |          -0.0143 |          31.5807 |        -130.0190 |
[32m[20221213 12:46:25 @agent_ppo2.py:179][0m |          -0.0206 |          31.0617 |        -132.6486 |
[32m[20221213 12:46:25 @agent_ppo2.py:179][0m |          -0.0344 |          30.8535 |        -142.3331 |
[32m[20221213 12:46:25 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:46:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 359.07
[32m[20221213 12:46:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.34
[32m[20221213 12:46:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.10
[32m[20221213 12:46:25 @agent_ppo2.py:137][0m Total time:      30.47 min
[32m[20221213 12:46:25 @agent_ppo2.py:139][0m 2082816 total steps have happened
[32m[20221213 12:46:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221213 12:46:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |           0.0761 |          36.6131 |        -114.9995 |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |           0.0865 |          35.1336 |         -80.5626 |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |           0.0458 |          34.5399 |        -104.0509 |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |           0.0068 |          34.1504 |        -115.9437 |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |          -0.0090 |          33.8894 |        -124.5412 |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |          -0.0175 |          33.5946 |        -124.0230 |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |          -0.0280 |          33.4751 |        -129.5440 |
[32m[20221213 12:46:26 @agent_ppo2.py:179][0m |          -0.0275 |          33.1929 |        -132.5782 |
[32m[20221213 12:46:27 @agent_ppo2.py:179][0m |          -0.0332 |          33.1073 |        -134.4574 |
[32m[20221213 12:46:27 @agent_ppo2.py:179][0m |          -0.0288 |          33.4780 |        -137.1129 |
[32m[20221213 12:46:27 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.06
[32m[20221213 12:46:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.58
[32m[20221213 12:46:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.53
[32m[20221213 12:46:27 @agent_ppo2.py:137][0m Total time:      30.50 min
[32m[20221213 12:46:27 @agent_ppo2.py:139][0m 2084864 total steps have happened
[32m[20221213 12:46:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221213 12:46:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:27 @agent_ppo2.py:179][0m |           0.0859 |          37.3734 |         -99.6005 |
[32m[20221213 12:46:27 @agent_ppo2.py:179][0m |           0.0460 |          35.9146 |         -90.1838 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |           0.0084 |          35.2571 |        -100.7376 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |          -0.0088 |          34.9631 |        -108.0970 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |          -0.0178 |          34.8226 |        -115.2080 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |          -0.0257 |          34.4434 |        -117.1878 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |          -0.0316 |          34.2353 |        -119.0431 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |          -0.0389 |          34.1102 |        -123.2226 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |          -0.0372 |          33.7602 |        -121.8837 |
[32m[20221213 12:46:28 @agent_ppo2.py:179][0m |          -0.0222 |          33.6707 |        -115.2794 |
[32m[20221213 12:46:28 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.43
[32m[20221213 12:46:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.13
[32m[20221213 12:46:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.17
[32m[20221213 12:46:29 @agent_ppo2.py:137][0m Total time:      30.53 min
[32m[20221213 12:46:29 @agent_ppo2.py:139][0m 2086912 total steps have happened
[32m[20221213 12:46:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221213 12:46:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:29 @agent_ppo2.py:179][0m |           0.0719 |          33.8348 |        -105.7277 |
[32m[20221213 12:46:29 @agent_ppo2.py:179][0m |           0.0363 |          31.7016 |         -82.0491 |
[32m[20221213 12:46:29 @agent_ppo2.py:179][0m |           0.0154 |          32.5430 |        -100.7288 |
[32m[20221213 12:46:29 @agent_ppo2.py:179][0m |          -0.0124 |          30.1722 |        -107.1195 |
[32m[20221213 12:46:30 @agent_ppo2.py:179][0m |          -0.0231 |          29.7488 |        -110.8538 |
[32m[20221213 12:46:30 @agent_ppo2.py:179][0m |          -0.0303 |          29.3798 |        -114.8597 |
[32m[20221213 12:46:30 @agent_ppo2.py:179][0m |          -0.0350 |          29.0598 |        -118.0352 |
[32m[20221213 12:46:30 @agent_ppo2.py:179][0m |          -0.0278 |          29.0286 |        -116.5222 |
[32m[20221213 12:46:30 @agent_ppo2.py:179][0m |          -0.0320 |          28.7502 |        -118.9329 |
[32m[20221213 12:46:30 @agent_ppo2.py:179][0m |          -0.0357 |          28.7756 |        -122.2126 |
[32m[20221213 12:46:30 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:46:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 360.16
[32m[20221213 12:46:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 373.83
[32m[20221213 12:46:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.18
[32m[20221213 12:46:30 @agent_ppo2.py:137][0m Total time:      30.56 min
[32m[20221213 12:46:30 @agent_ppo2.py:139][0m 2088960 total steps have happened
[32m[20221213 12:46:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221213 12:46:31 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:46:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:31 @agent_ppo2.py:179][0m |           0.1216 |          42.7192 |         -96.3907 |
[32m[20221213 12:46:31 @agent_ppo2.py:179][0m |           0.0972 |          41.2056 |         -60.6644 |
[32m[20221213 12:46:31 @agent_ppo2.py:179][0m |           0.0328 |          36.8498 |         -96.9861 |
[32m[20221213 12:46:31 @agent_ppo2.py:179][0m |           0.0059 |          35.4832 |        -101.9183 |
[32m[20221213 12:46:31 @agent_ppo2.py:179][0m |          -0.0136 |          35.2217 |        -112.4069 |
[32m[20221213 12:46:31 @agent_ppo2.py:179][0m |          -0.0213 |          35.0034 |        -115.8220 |
[32m[20221213 12:46:31 @agent_ppo2.py:179][0m |          -0.0314 |          34.8527 |        -119.2260 |
[32m[20221213 12:46:32 @agent_ppo2.py:179][0m |          -0.0349 |          34.7229 |        -123.4499 |
[32m[20221213 12:46:32 @agent_ppo2.py:179][0m |          -0.0343 |          34.5961 |        -124.6936 |
[32m[20221213 12:46:32 @agent_ppo2.py:179][0m |          -0.0378 |          34.4858 |        -128.9442 |
[32m[20221213 12:46:32 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:46:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 348.12
[32m[20221213 12:46:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.34
[32m[20221213 12:46:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.22
[32m[20221213 12:46:32 @agent_ppo2.py:137][0m Total time:      30.59 min
[32m[20221213 12:46:32 @agent_ppo2.py:139][0m 2091008 total steps have happened
[32m[20221213 12:46:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221213 12:46:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |           0.1205 |          40.8636 |        -100.9541 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |           0.1157 |          35.5918 |         -61.5252 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |           0.0532 |          35.0716 |         -72.7324 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |           0.0127 |          34.4929 |         -94.5217 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |          -0.0057 |          34.1494 |        -102.0563 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |          -0.0170 |          33.8573 |        -104.1431 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |          -0.0280 |          33.5418 |        -107.1374 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |          -0.0314 |          33.2998 |        -110.3068 |
[32m[20221213 12:46:33 @agent_ppo2.py:179][0m |          -0.0317 |          33.0609 |        -110.5526 |
[32m[20221213 12:46:34 @agent_ppo2.py:179][0m |          -0.0367 |          32.8314 |        -113.9139 |
[32m[20221213 12:46:34 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.92
[32m[20221213 12:46:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.93
[32m[20221213 12:46:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 324.47
[32m[20221213 12:46:34 @agent_ppo2.py:137][0m Total time:      30.62 min
[32m[20221213 12:46:34 @agent_ppo2.py:139][0m 2093056 total steps have happened
[32m[20221213 12:46:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221213 12:46:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:34 @agent_ppo2.py:179][0m |           0.0730 |          37.3501 |         -95.8167 |
[32m[20221213 12:46:34 @agent_ppo2.py:179][0m |           0.0514 |          36.3401 |         -87.5132 |
[32m[20221213 12:46:34 @agent_ppo2.py:179][0m |           0.0108 |          35.6313 |         -95.8044 |
[32m[20221213 12:46:35 @agent_ppo2.py:179][0m |          -0.0034 |          37.6959 |        -104.2901 |
[32m[20221213 12:46:35 @agent_ppo2.py:179][0m |          -0.0187 |          34.9834 |        -107.5308 |
[32m[20221213 12:46:35 @agent_ppo2.py:179][0m |          -0.0133 |          34.6627 |        -107.0417 |
[32m[20221213 12:46:35 @agent_ppo2.py:179][0m |          -0.0291 |          34.5388 |        -112.3237 |
[32m[20221213 12:46:35 @agent_ppo2.py:179][0m |          -0.0363 |          34.3521 |        -115.3620 |
[32m[20221213 12:46:35 @agent_ppo2.py:179][0m |          -0.0406 |          34.1364 |        -117.6819 |
[32m[20221213 12:46:35 @agent_ppo2.py:179][0m |          -0.0373 |          34.0498 |        -120.4239 |
[32m[20221213 12:46:35 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:46:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.36
[32m[20221213 12:46:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.11
[32m[20221213 12:46:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.37
[32m[20221213 12:46:35 @agent_ppo2.py:137][0m Total time:      30.65 min
[32m[20221213 12:46:35 @agent_ppo2.py:139][0m 2095104 total steps have happened
[32m[20221213 12:46:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221213 12:46:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:36 @agent_ppo2.py:179][0m |           0.0785 |          39.7270 |        -103.2814 |
[32m[20221213 12:46:36 @agent_ppo2.py:179][0m |           0.0569 |          33.5619 |         -88.1493 |
[32m[20221213 12:46:36 @agent_ppo2.py:179][0m |           0.0125 |          32.6057 |        -101.9308 |
[32m[20221213 12:46:36 @agent_ppo2.py:179][0m |          -0.0031 |          31.7315 |        -105.8379 |
[32m[20221213 12:46:36 @agent_ppo2.py:179][0m |          -0.0233 |          31.2382 |        -111.2519 |
[32m[20221213 12:46:37 @agent_ppo2.py:179][0m |          -0.0342 |          30.7931 |        -114.2238 |
[32m[20221213 12:46:37 @agent_ppo2.py:179][0m |          -0.0375 |          30.4884 |        -116.8361 |
[32m[20221213 12:46:37 @agent_ppo2.py:179][0m |          -0.0366 |          30.5259 |        -120.7755 |
[32m[20221213 12:46:37 @agent_ppo2.py:179][0m |          -0.0417 |          30.3508 |        -122.6134 |
[32m[20221213 12:46:37 @agent_ppo2.py:179][0m |          -0.0484 |          29.5188 |        -122.7994 |
[32m[20221213 12:46:37 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:46:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.73
[32m[20221213 12:46:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 319.91
[32m[20221213 12:46:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 375.83
[32m[20221213 12:46:37 @agent_ppo2.py:137][0m Total time:      30.68 min
[32m[20221213 12:46:37 @agent_ppo2.py:139][0m 2097152 total steps have happened
[32m[20221213 12:46:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221213 12:46:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |           0.0837 |          38.0738 |         -94.9579 |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |           0.0564 |          36.0958 |         -84.7399 |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |           0.0281 |          36.7957 |         -90.6679 |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |           0.0081 |          35.2294 |         -95.5646 |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |          -0.0139 |          35.0755 |        -102.1713 |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |          -0.0225 |          34.7476 |        -107.9730 |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |          -0.0214 |          34.7645 |        -106.8982 |
[32m[20221213 12:46:38 @agent_ppo2.py:179][0m |          -0.0242 |          34.5238 |        -110.3661 |
[32m[20221213 12:46:39 @agent_ppo2.py:179][0m |          -0.0286 |          34.3513 |        -108.6143 |
[32m[20221213 12:46:39 @agent_ppo2.py:179][0m |          -0.0333 |          35.8302 |        -111.7702 |
[32m[20221213 12:46:39 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:46:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 318.97
[32m[20221213 12:46:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.55
[32m[20221213 12:46:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 383.65
[32m[20221213 12:46:39 @agent_ppo2.py:137][0m Total time:      30.70 min
[32m[20221213 12:46:39 @agent_ppo2.py:139][0m 2099200 total steps have happened
[32m[20221213 12:46:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221213 12:46:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:39 @agent_ppo2.py:179][0m |           0.0619 |          36.0532 |         -98.5468 |
[32m[20221213 12:46:39 @agent_ppo2.py:179][0m |           0.0373 |          35.2156 |         -87.4616 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |           0.0281 |          34.9052 |         -94.2133 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |           0.0311 |          34.9971 |         -82.3436 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |          -0.0121 |          34.4192 |         -99.5481 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |          -0.0233 |          34.3506 |        -104.5187 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |          -0.0243 |          34.3536 |        -105.5213 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |          -0.0306 |          34.0857 |        -105.4610 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |          -0.0324 |          34.7965 |        -109.1760 |
[32m[20221213 12:46:40 @agent_ppo2.py:179][0m |          -0.0369 |          34.6234 |        -112.3736 |
[32m[20221213 12:46:40 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:46:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.09
[32m[20221213 12:46:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.82
[32m[20221213 12:46:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 371.20
[32m[20221213 12:46:41 @agent_ppo2.py:137][0m Total time:      30.73 min
[32m[20221213 12:46:41 @agent_ppo2.py:139][0m 2101248 total steps have happened
[32m[20221213 12:46:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221213 12:46:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:41 @agent_ppo2.py:179][0m |           0.0712 |          36.0014 |         -89.1271 |
[32m[20221213 12:46:41 @agent_ppo2.py:179][0m |           0.0814 |          37.2932 |         -68.6607 |
[32m[20221213 12:46:41 @agent_ppo2.py:179][0m |           0.0369 |          35.0708 |         -81.2881 |
[32m[20221213 12:46:41 @agent_ppo2.py:179][0m |           0.0146 |          33.9077 |         -84.1783 |
[32m[20221213 12:46:42 @agent_ppo2.py:179][0m |          -0.0110 |          33.6755 |         -97.0868 |
[32m[20221213 12:46:42 @agent_ppo2.py:179][0m |          -0.0206 |          33.4228 |        -100.9877 |
[32m[20221213 12:46:42 @agent_ppo2.py:179][0m |          -0.0225 |          33.1922 |        -102.3522 |
[32m[20221213 12:46:42 @agent_ppo2.py:179][0m |          -0.0245 |          33.7622 |        -104.4216 |
[32m[20221213 12:46:42 @agent_ppo2.py:179][0m |          -0.0346 |          32.8330 |        -107.5777 |
[32m[20221213 12:46:42 @agent_ppo2.py:179][0m |          -0.0345 |          32.7029 |        -107.5571 |
[32m[20221213 12:46:42 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:46:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 314.05
[32m[20221213 12:46:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.71
[32m[20221213 12:46:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.53
[32m[20221213 12:46:42 @agent_ppo2.py:137][0m Total time:      30.76 min
[32m[20221213 12:46:42 @agent_ppo2.py:139][0m 2103296 total steps have happened
[32m[20221213 12:46:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221213 12:46:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:43 @agent_ppo2.py:179][0m |           0.0730 |          35.3681 |         -81.4495 |
[32m[20221213 12:46:43 @agent_ppo2.py:179][0m |           0.0789 |          34.4684 |         -56.4778 |
[32m[20221213 12:46:43 @agent_ppo2.py:179][0m |           0.0383 |          34.0320 |         -66.1681 |
[32m[20221213 12:46:43 @agent_ppo2.py:179][0m |           0.0112 |          33.7891 |         -79.8076 |
[32m[20221213 12:46:43 @agent_ppo2.py:179][0m |          -0.0104 |          33.4986 |         -86.2758 |
[32m[20221213 12:46:43 @agent_ppo2.py:179][0m |          -0.0111 |          33.3680 |         -86.8898 |
[32m[20221213 12:46:44 @agent_ppo2.py:179][0m |          -0.0223 |          33.1674 |         -92.6618 |
[32m[20221213 12:46:44 @agent_ppo2.py:179][0m |          -0.0303 |          33.0200 |         -95.5695 |
[32m[20221213 12:46:44 @agent_ppo2.py:179][0m |          -0.0313 |          32.8932 |         -97.0664 |
[32m[20221213 12:46:44 @agent_ppo2.py:179][0m |          -0.0357 |          32.8689 |         -99.7030 |
[32m[20221213 12:46:44 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:46:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 335.59
[32m[20221213 12:46:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.70
[32m[20221213 12:46:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.33
[32m[20221213 12:46:44 @agent_ppo2.py:137][0m Total time:      30.79 min
[32m[20221213 12:46:44 @agent_ppo2.py:139][0m 2105344 total steps have happened
[32m[20221213 12:46:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221213 12:46:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |           0.0813 |          36.1426 |         -80.4466 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |           0.0494 |          35.0195 |         -77.1154 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |           0.0011 |          34.5342 |         -87.8541 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |          -0.0135 |          34.1530 |         -90.3750 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |          -0.0266 |          33.9104 |         -93.8467 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |          -0.0371 |          33.6922 |         -95.3864 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |          -0.0365 |          33.6852 |         -95.9279 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |          -0.0450 |          33.2731 |         -98.1151 |
[32m[20221213 12:46:45 @agent_ppo2.py:179][0m |          -0.0416 |          33.1158 |         -99.1193 |
[32m[20221213 12:46:46 @agent_ppo2.py:179][0m |          -0.0428 |          33.0242 |        -100.7688 |
[32m[20221213 12:46:46 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:46:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 282.33
[32m[20221213 12:46:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.63
[32m[20221213 12:46:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.41
[32m[20221213 12:46:46 @agent_ppo2.py:137][0m Total time:      30.82 min
[32m[20221213 12:46:46 @agent_ppo2.py:139][0m 2107392 total steps have happened
[32m[20221213 12:46:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221213 12:46:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:46 @agent_ppo2.py:179][0m |           0.0589 |          30.2532 |         -82.3509 |
[32m[20221213 12:46:46 @agent_ppo2.py:179][0m |           0.0490 |          29.4843 |         -63.5947 |
[32m[20221213 12:46:46 @agent_ppo2.py:179][0m |           0.0054 |          29.2134 |         -74.6226 |
[32m[20221213 12:46:47 @agent_ppo2.py:179][0m |           0.0025 |          28.8986 |         -73.6822 |
[32m[20221213 12:46:47 @agent_ppo2.py:179][0m |          -0.0217 |          28.7073 |         -82.1897 |
[32m[20221213 12:46:47 @agent_ppo2.py:179][0m |          -0.0287 |          28.7070 |         -84.1358 |
[32m[20221213 12:46:47 @agent_ppo2.py:179][0m |          -0.0299 |          28.5492 |         -87.8000 |
[32m[20221213 12:46:47 @agent_ppo2.py:179][0m |          -0.0358 |          28.5586 |         -90.3979 |
[32m[20221213 12:46:47 @agent_ppo2.py:179][0m |          -0.0330 |          28.5546 |         -88.9777 |
[32m[20221213 12:46:47 @agent_ppo2.py:179][0m |          -0.0361 |          28.4109 |         -90.7383 |
[32m[20221213 12:46:47 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:46:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.15
[32m[20221213 12:46:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 369.88
[32m[20221213 12:46:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.85
[32m[20221213 12:46:48 @agent_ppo2.py:137][0m Total time:      30.85 min
[32m[20221213 12:46:48 @agent_ppo2.py:139][0m 2109440 total steps have happened
[32m[20221213 12:46:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221213 12:46:48 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:46:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:48 @agent_ppo2.py:179][0m |           0.0599 |          35.0971 |         -82.7267 |
[32m[20221213 12:46:48 @agent_ppo2.py:179][0m |           0.0619 |          33.9956 |         -69.8635 |
[32m[20221213 12:46:48 @agent_ppo2.py:179][0m |           0.0617 |          33.6083 |         -56.8505 |
[32m[20221213 12:46:48 @agent_ppo2.py:179][0m |           0.0285 |          33.3244 |         -73.4884 |
[32m[20221213 12:46:48 @agent_ppo2.py:179][0m |           0.0099 |          33.1401 |         -76.8971 |
[32m[20221213 12:46:49 @agent_ppo2.py:179][0m |          -0.0071 |          33.0548 |         -83.1806 |
[32m[20221213 12:46:49 @agent_ppo2.py:179][0m |          -0.0184 |          32.7485 |         -87.4415 |
[32m[20221213 12:46:49 @agent_ppo2.py:179][0m |          -0.0208 |          32.6569 |         -90.0724 |
[32m[20221213 12:46:49 @agent_ppo2.py:179][0m |          -0.0217 |          33.8094 |         -91.1402 |
[32m[20221213 12:46:49 @agent_ppo2.py:179][0m |          -0.0223 |          32.6365 |         -92.7967 |
[32m[20221213 12:46:49 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:46:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.27
[32m[20221213 12:46:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.89
[32m[20221213 12:46:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.87
[32m[20221213 12:46:49 @agent_ppo2.py:137][0m Total time:      30.87 min
[32m[20221213 12:46:49 @agent_ppo2.py:139][0m 2111488 total steps have happened
[32m[20221213 12:46:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221213 12:46:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |           0.0617 |          35.1620 |         -76.3578 |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |           0.0666 |          33.7298 |         -61.5488 |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |           0.0261 |          32.9980 |         -66.2650 |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |           0.0121 |          32.6020 |         -70.3087 |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |          -0.0120 |          32.4045 |         -78.3415 |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |          -0.0243 |          32.1144 |         -82.7725 |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |          -0.0198 |          32.2245 |         -84.4497 |
[32m[20221213 12:46:50 @agent_ppo2.py:179][0m |          -0.0177 |          31.7189 |         -83.3265 |
[32m[20221213 12:46:51 @agent_ppo2.py:179][0m |          -0.0278 |          31.4848 |         -83.6066 |
[32m[20221213 12:46:51 @agent_ppo2.py:179][0m |          -0.0246 |          31.6609 |         -86.6991 |
[32m[20221213 12:46:51 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:46:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 351.55
[32m[20221213 12:46:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 367.47
[32m[20221213 12:46:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.23
[32m[20221213 12:46:51 @agent_ppo2.py:137][0m Total time:      30.90 min
[32m[20221213 12:46:51 @agent_ppo2.py:139][0m 2113536 total steps have happened
[32m[20221213 12:46:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221213 12:46:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:51 @agent_ppo2.py:179][0m |           0.0894 |          39.5582 |         -70.1928 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |           0.1011 |          35.2499 |         -49.8070 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |           0.0383 |          34.3629 |         -55.9453 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |          -0.0027 |          34.0912 |         -62.9690 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |          -0.0216 |          33.5477 |         -66.5991 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |          -0.0340 |          33.2423 |         -70.9055 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |          -0.0379 |          33.7638 |         -72.4929 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |          -0.0388 |          32.8870 |         -74.8146 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |          -0.0538 |          32.6648 |         -76.7815 |
[32m[20221213 12:46:52 @agent_ppo2.py:179][0m |          -0.0493 |          32.5183 |         -78.1953 |
[32m[20221213 12:46:52 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:46:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.50
[32m[20221213 12:46:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.89
[32m[20221213 12:46:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.54
[32m[20221213 12:46:53 @agent_ppo2.py:137][0m Total time:      30.93 min
[32m[20221213 12:46:53 @agent_ppo2.py:139][0m 2115584 total steps have happened
[32m[20221213 12:46:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221213 12:46:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:53 @agent_ppo2.py:179][0m |           0.0786 |          35.0749 |         -69.5419 |
[32m[20221213 12:46:53 @agent_ppo2.py:179][0m |           0.0858 |          33.2595 |         -53.7571 |
[32m[20221213 12:46:53 @agent_ppo2.py:179][0m |           0.0230 |          31.9666 |         -59.6462 |
[32m[20221213 12:46:53 @agent_ppo2.py:179][0m |          -0.0026 |          31.3924 |         -67.7823 |
[32m[20221213 12:46:54 @agent_ppo2.py:179][0m |          -0.0151 |          30.8916 |         -69.6884 |
[32m[20221213 12:46:54 @agent_ppo2.py:179][0m |          -0.0211 |          30.6796 |         -72.0733 |
[32m[20221213 12:46:54 @agent_ppo2.py:179][0m |          -0.0323 |          30.4616 |         -75.4155 |
[32m[20221213 12:46:54 @agent_ppo2.py:179][0m |          -0.0303 |          30.2766 |         -76.9943 |
[32m[20221213 12:46:54 @agent_ppo2.py:179][0m |          -0.0292 |          29.7803 |         -77.0318 |
[32m[20221213 12:46:54 @agent_ppo2.py:179][0m |          -0.0373 |          29.7149 |         -80.4623 |
[32m[20221213 12:46:54 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:46:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 299.39
[32m[20221213 12:46:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.27
[32m[20221213 12:46:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 365.85
[32m[20221213 12:46:54 @agent_ppo2.py:137][0m Total time:      30.96 min
[32m[20221213 12:46:54 @agent_ppo2.py:139][0m 2117632 total steps have happened
[32m[20221213 12:46:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221213 12:46:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:55 @agent_ppo2.py:179][0m |           0.0710 |          35.1916 |         -61.0178 |
[32m[20221213 12:46:55 @agent_ppo2.py:179][0m |           0.0947 |          34.3079 |         -37.6269 |
[32m[20221213 12:46:55 @agent_ppo2.py:179][0m |           0.0450 |          33.7700 |         -50.6371 |
[32m[20221213 12:46:55 @agent_ppo2.py:179][0m |           0.0110 |          33.4405 |         -59.3955 |
[32m[20221213 12:46:55 @agent_ppo2.py:179][0m |          -0.0083 |          33.2027 |         -64.9931 |
[32m[20221213 12:46:55 @agent_ppo2.py:179][0m |          -0.0187 |          33.0702 |         -68.7199 |
[32m[20221213 12:46:56 @agent_ppo2.py:179][0m |          -0.0266 |          32.9830 |         -72.2894 |
[32m[20221213 12:46:56 @agent_ppo2.py:179][0m |          -0.0250 |          32.6515 |         -73.1457 |
[32m[20221213 12:46:56 @agent_ppo2.py:179][0m |          -0.0288 |          32.6752 |         -73.8929 |
[32m[20221213 12:46:56 @agent_ppo2.py:179][0m |          -0.0276 |          32.6252 |         -74.0690 |
[32m[20221213 12:46:56 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:46:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 354.25
[32m[20221213 12:46:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.64
[32m[20221213 12:46:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.75
[32m[20221213 12:46:56 @agent_ppo2.py:137][0m Total time:      30.99 min
[32m[20221213 12:46:56 @agent_ppo2.py:139][0m 2119680 total steps have happened
[32m[20221213 12:46:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221213 12:46:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |           0.1096 |          33.4200 |         -62.2430 |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |           0.1017 |          30.4124 |         -44.2837 |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |           0.0587 |          28.9853 |         -46.3690 |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |           0.0155 |          28.1823 |         -57.8969 |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |           0.0115 |          27.5208 |         -61.2627 |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |          -0.0072 |          27.0827 |         -66.7138 |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |          -0.0122 |          26.6989 |         -66.4273 |
[32m[20221213 12:46:57 @agent_ppo2.py:179][0m |          -0.0205 |          26.5095 |         -69.2882 |
[32m[20221213 12:46:58 @agent_ppo2.py:179][0m |          -0.0230 |          26.4614 |         -70.0540 |
[32m[20221213 12:46:58 @agent_ppo2.py:179][0m |          -0.0295 |          26.0069 |         -72.0450 |
[32m[20221213 12:46:58 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:46:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.27
[32m[20221213 12:46:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.50
[32m[20221213 12:46:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.16
[32m[20221213 12:46:58 @agent_ppo2.py:137][0m Total time:      31.02 min
[32m[20221213 12:46:58 @agent_ppo2.py:139][0m 2121728 total steps have happened
[32m[20221213 12:46:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221213 12:46:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:46:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:46:58 @agent_ppo2.py:179][0m |           0.0899 |          35.0577 |         -56.7887 |
[32m[20221213 12:46:58 @agent_ppo2.py:179][0m |           0.0439 |          33.8494 |         -41.9283 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |           0.0073 |          33.4609 |         -49.9821 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |          -0.0100 |          33.1316 |         -53.1289 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |          -0.0189 |          32.9793 |         -56.5593 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |          -0.0275 |          32.8287 |         -57.9040 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |          -0.0357 |          32.6732 |         -60.0144 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |          -0.0315 |          32.5168 |         -61.6239 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |          -0.0337 |          32.3956 |         -60.4813 |
[32m[20221213 12:46:59 @agent_ppo2.py:179][0m |          -0.0378 |          32.2729 |         -62.4270 |
[32m[20221213 12:46:59 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:47:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.34
[32m[20221213 12:47:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.55
[32m[20221213 12:47:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 370.13
[32m[20221213 12:47:00 @agent_ppo2.py:137][0m Total time:      31.05 min
[32m[20221213 12:47:00 @agent_ppo2.py:139][0m 2123776 total steps have happened
[32m[20221213 12:47:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221213 12:47:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:00 @agent_ppo2.py:179][0m |           0.0823 |          34.3116 |         -56.6856 |
[32m[20221213 12:47:00 @agent_ppo2.py:179][0m |           0.0783 |          33.4319 |         -40.9080 |
[32m[20221213 12:47:00 @agent_ppo2.py:179][0m |           0.0140 |          32.9272 |         -55.6772 |
[32m[20221213 12:47:00 @agent_ppo2.py:179][0m |           0.0002 |          34.2563 |         -57.6167 |
[32m[20221213 12:47:01 @agent_ppo2.py:179][0m |          -0.0171 |          32.4035 |         -60.4302 |
[32m[20221213 12:47:01 @agent_ppo2.py:179][0m |          -0.0217 |          31.9832 |         -58.8592 |
[32m[20221213 12:47:01 @agent_ppo2.py:179][0m |          -0.0327 |          31.7422 |         -60.9731 |
[32m[20221213 12:47:01 @agent_ppo2.py:179][0m |          -0.0348 |          31.5697 |         -63.3479 |
[32m[20221213 12:47:01 @agent_ppo2.py:179][0m |          -0.0408 |          31.3553 |         -63.9616 |
[32m[20221213 12:47:01 @agent_ppo2.py:179][0m |          -0.0412 |          31.2931 |         -66.5844 |
[32m[20221213 12:47:01 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:47:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 298.62
[32m[20221213 12:47:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.81
[32m[20221213 12:47:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.26
[32m[20221213 12:47:01 @agent_ppo2.py:137][0m Total time:      31.08 min
[32m[20221213 12:47:01 @agent_ppo2.py:139][0m 2125824 total steps have happened
[32m[20221213 12:47:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1038 --------------------------#
[32m[20221213 12:47:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:02 @agent_ppo2.py:179][0m |           0.0821 |          34.8209 |         -48.9011 |
[32m[20221213 12:47:02 @agent_ppo2.py:179][0m |           0.0938 |          34.0304 |         -36.6844 |
[32m[20221213 12:47:02 @agent_ppo2.py:179][0m |           0.0313 |          32.8787 |         -39.1183 |
[32m[20221213 12:47:02 @agent_ppo2.py:179][0m |           0.0102 |          32.4979 |         -44.2069 |
[32m[20221213 12:47:02 @agent_ppo2.py:179][0m |          -0.0023 |          32.1394 |         -47.9493 |
[32m[20221213 12:47:02 @agent_ppo2.py:179][0m |          -0.0140 |          31.9664 |         -51.5599 |
[32m[20221213 12:47:02 @agent_ppo2.py:179][0m |          -0.0121 |          34.5722 |         -53.0805 |
[32m[20221213 12:47:03 @agent_ppo2.py:179][0m |          -0.0196 |          33.3752 |         -56.5079 |
[32m[20221213 12:47:03 @agent_ppo2.py:179][0m |          -0.0311 |          31.5842 |         -57.3870 |
[32m[20221213 12:47:03 @agent_ppo2.py:179][0m |          -0.0342 |          31.2447 |         -59.2549 |
[32m[20221213 12:47:03 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:47:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.26
[32m[20221213 12:47:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.81
[32m[20221213 12:47:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.00
[32m[20221213 12:47:03 @agent_ppo2.py:137][0m Total time:      31.11 min
[32m[20221213 12:47:03 @agent_ppo2.py:139][0m 2127872 total steps have happened
[32m[20221213 12:47:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1039 --------------------------#
[32m[20221213 12:47:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:03 @agent_ppo2.py:179][0m |           0.0755 |          38.0818 |         -54.3297 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |           0.0775 |          32.5964 |         -38.4616 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |           0.0467 |          31.7949 |         -41.1245 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |           0.0144 |          31.7592 |         -47.4732 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |          -0.0113 |          31.1576 |         -54.0675 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |          -0.0199 |          31.0703 |         -57.0849 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |          -0.0270 |          30.8367 |         -59.1130 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |          -0.0307 |          30.7512 |         -61.4906 |
[32m[20221213 12:47:04 @agent_ppo2.py:179][0m |          -0.0352 |          30.5345 |         -62.3173 |
[32m[20221213 12:47:05 @agent_ppo2.py:179][0m |          -0.0370 |          30.3699 |         -63.4680 |
[32m[20221213 12:47:05 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.48
[32m[20221213 12:47:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.36
[32m[20221213 12:47:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.07
[32m[20221213 12:47:05 @agent_ppo2.py:137][0m Total time:      31.13 min
[32m[20221213 12:47:05 @agent_ppo2.py:139][0m 2129920 total steps have happened
[32m[20221213 12:47:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1040 --------------------------#
[32m[20221213 12:47:05 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:47:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:05 @agent_ppo2.py:179][0m |           0.1103 |          32.6939 |         -42.5203 |
[32m[20221213 12:47:05 @agent_ppo2.py:179][0m |           0.0993 |          30.2901 |         -32.3056 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |           0.0316 |          29.4381 |         -43.4884 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |           0.0116 |          28.7605 |         -49.3828 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |          -0.0062 |          28.7775 |         -54.1230 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |          -0.0028 |          32.4137 |         -54.4220 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |          -0.0281 |          27.7080 |         -58.5234 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |          -0.0326 |          27.4739 |         -58.5679 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |          -0.0412 |          27.3533 |         -60.5542 |
[32m[20221213 12:47:06 @agent_ppo2.py:179][0m |          -0.0405 |          27.0996 |         -62.7110 |
[32m[20221213 12:47:06 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:47:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.90
[32m[20221213 12:47:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.73
[32m[20221213 12:47:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 367.37
[32m[20221213 12:47:07 @agent_ppo2.py:137][0m Total time:      31.16 min
[32m[20221213 12:47:07 @agent_ppo2.py:139][0m 2131968 total steps have happened
[32m[20221213 12:47:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1041 --------------------------#
[32m[20221213 12:47:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:07 @agent_ppo2.py:179][0m |           0.0694 |          34.9712 |         -53.8817 |
[32m[20221213 12:47:07 @agent_ppo2.py:179][0m |           0.0824 |          33.2393 |         -39.2109 |
[32m[20221213 12:47:07 @agent_ppo2.py:179][0m |           0.0312 |          32.6948 |         -41.7719 |
[32m[20221213 12:47:07 @agent_ppo2.py:179][0m |          -0.0016 |          32.4363 |         -51.9412 |
[32m[20221213 12:47:08 @agent_ppo2.py:179][0m |          -0.0131 |          32.4850 |         -55.9812 |
[32m[20221213 12:47:08 @agent_ppo2.py:179][0m |          -0.0194 |          32.0985 |         -57.4867 |
[32m[20221213 12:47:08 @agent_ppo2.py:179][0m |          -0.0214 |          32.0132 |         -58.9933 |
[32m[20221213 12:47:08 @agent_ppo2.py:179][0m |          -0.0324 |          31.7942 |         -60.7852 |
[32m[20221213 12:47:08 @agent_ppo2.py:179][0m |          -0.0368 |          31.7852 |         -61.8893 |
[32m[20221213 12:47:08 @agent_ppo2.py:179][0m |          -0.0214 |          35.1676 |         -62.5913 |
[32m[20221213 12:47:08 @agent_ppo2.py:124][0m Policy update time: 1.42 s
[32m[20221213 12:47:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.19
[32m[20221213 12:47:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.43
[32m[20221213 12:47:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.53
[32m[20221213 12:47:08 @agent_ppo2.py:137][0m Total time:      31.20 min
[32m[20221213 12:47:08 @agent_ppo2.py:139][0m 2134016 total steps have happened
[32m[20221213 12:47:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1042 --------------------------#
[32m[20221213 12:47:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:09 @agent_ppo2.py:179][0m |           0.0874 |          32.0552 |         -44.9995 |
[32m[20221213 12:47:09 @agent_ppo2.py:179][0m |           0.0473 |          30.4331 |         -35.9471 |
[32m[20221213 12:47:09 @agent_ppo2.py:179][0m |          -0.0059 |          29.9227 |         -41.3216 |
[32m[20221213 12:47:09 @agent_ppo2.py:179][0m |          -0.0162 |          29.5796 |         -43.4349 |
[32m[20221213 12:47:09 @agent_ppo2.py:179][0m |          -0.0345 |          29.3187 |         -45.7160 |
[32m[20221213 12:47:09 @agent_ppo2.py:179][0m |          -0.0376 |          29.1341 |         -46.9627 |
[32m[20221213 12:47:10 @agent_ppo2.py:179][0m |          -0.0417 |          28.8630 |         -49.0314 |
[32m[20221213 12:47:10 @agent_ppo2.py:179][0m |          -0.0474 |          28.6673 |         -49.9789 |
[32m[20221213 12:47:10 @agent_ppo2.py:179][0m |          -0.0486 |          28.9048 |         -50.9196 |
[32m[20221213 12:47:10 @agent_ppo2.py:179][0m |          -0.0565 |          28.5471 |         -52.4900 |
[32m[20221213 12:47:10 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:47:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.66
[32m[20221213 12:47:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 305.61
[32m[20221213 12:47:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.40
[32m[20221213 12:47:10 @agent_ppo2.py:137][0m Total time:      31.22 min
[32m[20221213 12:47:10 @agent_ppo2.py:139][0m 2136064 total steps have happened
[32m[20221213 12:47:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1043 --------------------------#
[32m[20221213 12:47:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:11 @agent_ppo2.py:179][0m |           0.0746 |          34.6073 |         -53.4856 |
[32m[20221213 12:47:11 @agent_ppo2.py:179][0m |           0.1141 |          33.8290 |         -30.9345 |
[32m[20221213 12:47:11 @agent_ppo2.py:179][0m |           0.0431 |          33.5025 |         -37.4420 |
[32m[20221213 12:47:11 @agent_ppo2.py:179][0m |           0.0114 |          34.1703 |         -47.0723 |
[32m[20221213 12:47:11 @agent_ppo2.py:179][0m |          -0.0021 |          33.1382 |         -49.4660 |
[32m[20221213 12:47:11 @agent_ppo2.py:179][0m |          -0.0166 |          32.9150 |         -52.8400 |
[32m[20221213 12:47:11 @agent_ppo2.py:179][0m |          -0.0241 |          32.7753 |         -53.6764 |
[32m[20221213 12:47:12 @agent_ppo2.py:179][0m |          -0.0244 |          32.6827 |         -54.2785 |
[32m[20221213 12:47:12 @agent_ppo2.py:179][0m |          -0.0264 |          32.5916 |         -54.6124 |
[32m[20221213 12:47:12 @agent_ppo2.py:179][0m |          -0.0311 |          32.5616 |         -56.0507 |
[32m[20221213 12:47:12 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:47:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.48
[32m[20221213 12:47:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.21
[32m[20221213 12:47:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.43
[32m[20221213 12:47:12 @agent_ppo2.py:137][0m Total time:      31.26 min
[32m[20221213 12:47:12 @agent_ppo2.py:139][0m 2138112 total steps have happened
[32m[20221213 12:47:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1044 --------------------------#
[32m[20221213 12:47:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:12 @agent_ppo2.py:179][0m |           0.1350 |          34.4778 |         -49.3825 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |           0.0519 |          35.8324 |         -42.0030 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |           0.0111 |          33.0466 |         -46.6829 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |          -0.0077 |          32.6949 |         -52.4849 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |          -0.0232 |          32.4335 |         -57.2999 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |          -0.0259 |          32.3026 |         -59.0873 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |          -0.0315 |          32.2181 |         -59.6931 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |          -0.0306 |          32.8915 |         -62.0306 |
[32m[20221213 12:47:13 @agent_ppo2.py:179][0m |          -0.0186 |          31.9345 |         -58.5658 |
[32m[20221213 12:47:14 @agent_ppo2.py:179][0m |          -0.0289 |          31.7524 |         -60.3810 |
[32m[20221213 12:47:14 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:47:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.87
[32m[20221213 12:47:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.34
[32m[20221213 12:47:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.92
[32m[20221213 12:47:14 @agent_ppo2.py:137][0m Total time:      31.28 min
[32m[20221213 12:47:14 @agent_ppo2.py:139][0m 2140160 total steps have happened
[32m[20221213 12:47:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1045 --------------------------#
[32m[20221213 12:47:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:14 @agent_ppo2.py:179][0m |           0.0754 |          39.4563 |         -49.8551 |
[32m[20221213 12:47:14 @agent_ppo2.py:179][0m |           0.0423 |          34.1764 |         -47.3937 |
[32m[20221213 12:47:14 @agent_ppo2.py:179][0m |           0.0165 |          33.5498 |         -50.0528 |
[32m[20221213 12:47:15 @agent_ppo2.py:179][0m |          -0.0056 |          33.3130 |         -51.9903 |
[32m[20221213 12:47:15 @agent_ppo2.py:179][0m |          -0.0175 |          32.8204 |         -53.8947 |
[32m[20221213 12:47:15 @agent_ppo2.py:179][0m |          -0.0284 |          32.7178 |         -55.3667 |
[32m[20221213 12:47:15 @agent_ppo2.py:179][0m |          -0.0309 |          32.5512 |         -57.5983 |
[32m[20221213 12:47:15 @agent_ppo2.py:179][0m |          -0.0318 |          32.3953 |         -58.2571 |
[32m[20221213 12:47:15 @agent_ppo2.py:179][0m |          -0.0363 |          32.2134 |         -58.1629 |
[32m[20221213 12:47:15 @agent_ppo2.py:179][0m |          -0.0257 |          32.6166 |         -58.5293 |
[32m[20221213 12:47:15 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:47:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 324.10
[32m[20221213 12:47:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.81
[32m[20221213 12:47:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.34
[32m[20221213 12:47:15 @agent_ppo2.py:137][0m Total time:      31.31 min
[32m[20221213 12:47:15 @agent_ppo2.py:139][0m 2142208 total steps have happened
[32m[20221213 12:47:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1046 --------------------------#
[32m[20221213 12:47:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:16 @agent_ppo2.py:179][0m |           0.0630 |          35.4680 |         -48.7238 |
[32m[20221213 12:47:16 @agent_ppo2.py:179][0m |           0.0661 |          34.3357 |         -36.0352 |
[32m[20221213 12:47:16 @agent_ppo2.py:179][0m |           0.0293 |          33.8319 |         -40.9854 |
[32m[20221213 12:47:16 @agent_ppo2.py:179][0m |           0.0022 |          33.3915 |         -46.8125 |
[32m[20221213 12:47:16 @agent_ppo2.py:179][0m |          -0.0184 |          33.2281 |         -49.5733 |
[32m[20221213 12:47:16 @agent_ppo2.py:179][0m |          -0.0283 |          32.7843 |         -51.0429 |
[32m[20221213 12:47:17 @agent_ppo2.py:179][0m |          -0.0337 |          32.6272 |         -53.0125 |
[32m[20221213 12:47:17 @agent_ppo2.py:179][0m |          -0.0208 |          37.1419 |         -53.2696 |
[32m[20221213 12:47:17 @agent_ppo2.py:179][0m |          -0.0134 |          32.8298 |         -49.2891 |
[32m[20221213 12:47:17 @agent_ppo2.py:179][0m |          -0.0335 |          32.3000 |         -52.9659 |
[32m[20221213 12:47:17 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:47:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.83
[32m[20221213 12:47:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.36
[32m[20221213 12:47:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 374.55
[32m[20221213 12:47:17 @agent_ppo2.py:137][0m Total time:      31.34 min
[32m[20221213 12:47:17 @agent_ppo2.py:139][0m 2144256 total steps have happened
[32m[20221213 12:47:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1047 --------------------------#
[32m[20221213 12:47:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |           0.0640 |          34.2804 |         -37.3923 |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |           0.0376 |          33.1084 |         -31.8585 |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |           0.0144 |          32.4865 |         -36.5789 |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |          -0.0032 |          32.0519 |         -39.6150 |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |          -0.0096 |          31.7699 |         -39.5395 |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |          -0.0239 |          31.4523 |         -43.8264 |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |          -0.0275 |          31.2214 |         -45.3865 |
[32m[20221213 12:47:18 @agent_ppo2.py:179][0m |          -0.0296 |          31.0747 |         -46.5684 |
[32m[20221213 12:47:19 @agent_ppo2.py:179][0m |          -0.0264 |          30.9160 |         -46.6155 |
[32m[20221213 12:47:19 @agent_ppo2.py:179][0m |          -0.0272 |          30.6590 |         -45.6189 |
[32m[20221213 12:47:19 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:47:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.09
[32m[20221213 12:47:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.69
[32m[20221213 12:47:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.68
[32m[20221213 12:47:19 @agent_ppo2.py:137][0m Total time:      31.37 min
[32m[20221213 12:47:19 @agent_ppo2.py:139][0m 2146304 total steps have happened
[32m[20221213 12:47:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1048 --------------------------#
[32m[20221213 12:47:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:19 @agent_ppo2.py:179][0m |           0.0834 |          34.8603 |         -42.6828 |
[32m[20221213 12:47:19 @agent_ppo2.py:179][0m |           0.0452 |          33.4181 |         -35.1751 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |           0.0118 |          32.8002 |         -39.3837 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |          -0.0072 |          33.1588 |         -39.9246 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |          -0.0203 |          32.3257 |         -41.4732 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |          -0.0342 |          32.1051 |         -44.4122 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |          -0.0418 |          31.8215 |         -45.9457 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |          -0.0422 |          31.8596 |         -46.5885 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |          -0.0423 |          31.8078 |         -47.7976 |
[32m[20221213 12:47:20 @agent_ppo2.py:179][0m |          -0.0462 |          31.5316 |         -48.4534 |
[32m[20221213 12:47:20 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:47:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.24
[32m[20221213 12:47:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 370.57
[32m[20221213 12:47:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.59
[32m[20221213 12:47:21 @agent_ppo2.py:137][0m Total time:      31.40 min
[32m[20221213 12:47:21 @agent_ppo2.py:139][0m 2148352 total steps have happened
[32m[20221213 12:47:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1049 --------------------------#
[32m[20221213 12:47:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:21 @agent_ppo2.py:179][0m |           0.1144 |          35.0104 |         -32.9251 |
[32m[20221213 12:47:21 @agent_ppo2.py:179][0m |           0.0606 |          34.0403 |         -29.9555 |
[32m[20221213 12:47:21 @agent_ppo2.py:179][0m |           0.0229 |          34.3366 |         -34.4784 |
[32m[20221213 12:47:21 @agent_ppo2.py:179][0m |          -0.0089 |          33.3138 |         -38.3975 |
[32m[20221213 12:47:21 @agent_ppo2.py:179][0m |          -0.0202 |          32.9986 |         -40.0551 |
[32m[20221213 12:47:22 @agent_ppo2.py:179][0m |          -0.0267 |          32.9959 |         -41.3422 |
[32m[20221213 12:47:22 @agent_ppo2.py:179][0m |          -0.0183 |          35.5029 |         -41.6976 |
[32m[20221213 12:47:22 @agent_ppo2.py:179][0m |          -0.0280 |          32.8041 |         -41.9567 |
[32m[20221213 12:47:22 @agent_ppo2.py:179][0m |          -0.0224 |          36.4299 |         -42.0213 |
[32m[20221213 12:47:22 @agent_ppo2.py:179][0m |          -0.0277 |          34.4910 |         -43.5367 |
[32m[20221213 12:47:22 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:47:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.40
[32m[20221213 12:47:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.55
[32m[20221213 12:47:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.18
[32m[20221213 12:47:22 @agent_ppo2.py:137][0m Total time:      31.43 min
[32m[20221213 12:47:22 @agent_ppo2.py:139][0m 2150400 total steps have happened
[32m[20221213 12:47:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1050 --------------------------#
[32m[20221213 12:47:22 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:47:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:23 @agent_ppo2.py:179][0m |           0.0752 |          34.2182 |         -31.9087 |
[32m[20221213 12:47:23 @agent_ppo2.py:179][0m |           0.0300 |          33.2870 |         -28.6382 |
[32m[20221213 12:47:23 @agent_ppo2.py:179][0m |           0.0189 |          32.8869 |         -30.9834 |
[32m[20221213 12:47:23 @agent_ppo2.py:179][0m |           0.0031 |          33.0930 |         -34.9697 |
[32m[20221213 12:47:23 @agent_ppo2.py:179][0m |          -0.0129 |          33.3213 |         -36.0604 |
[32m[20221213 12:47:23 @agent_ppo2.py:179][0m |          -0.0235 |          32.2978 |         -37.9743 |
[32m[20221213 12:47:23 @agent_ppo2.py:179][0m |          -0.0291 |          32.1078 |         -39.3749 |
[32m[20221213 12:47:24 @agent_ppo2.py:179][0m |          -0.0232 |          34.1688 |         -39.5797 |
[32m[20221213 12:47:24 @agent_ppo2.py:179][0m |          -0.0371 |          31.9581 |         -40.7701 |
[32m[20221213 12:47:24 @agent_ppo2.py:179][0m |          -0.0381 |          31.7742 |         -41.3354 |
[32m[20221213 12:47:24 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:47:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 349.41
[32m[20221213 12:47:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.38
[32m[20221213 12:47:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 299.69
[32m[20221213 12:47:24 @agent_ppo2.py:137][0m Total time:      31.45 min
[32m[20221213 12:47:24 @agent_ppo2.py:139][0m 2152448 total steps have happened
[32m[20221213 12:47:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1051 --------------------------#
[32m[20221213 12:47:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:24 @agent_ppo2.py:179][0m |           0.0910 |          34.2837 |         -27.1770 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |           0.1000 |          33.3511 |         -19.3641 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |           0.0238 |          32.4824 |         -27.1684 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |           0.0111 |          33.8625 |         -30.6460 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |          -0.0031 |          32.4664 |         -30.7481 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |          -0.0168 |          31.3154 |         -32.3710 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |          -0.0205 |          30.5919 |         -33.9410 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |          -0.0269 |          30.2310 |         -34.4641 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |          -0.0330 |          29.8362 |         -34.7815 |
[32m[20221213 12:47:25 @agent_ppo2.py:179][0m |          -0.0303 |          31.1378 |         -37.0442 |
[32m[20221213 12:47:25 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.38
[32m[20221213 12:47:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.74
[32m[20221213 12:47:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.55
[32m[20221213 12:47:26 @agent_ppo2.py:137][0m Total time:      31.48 min
[32m[20221213 12:47:26 @agent_ppo2.py:139][0m 2154496 total steps have happened
[32m[20221213 12:47:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1052 --------------------------#
[32m[20221213 12:47:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:26 @agent_ppo2.py:179][0m |           0.0568 |          35.2333 |         -32.9218 |
[32m[20221213 12:47:26 @agent_ppo2.py:179][0m |           0.0570 |          37.8256 |         -26.0125 |
[32m[20221213 12:47:26 @agent_ppo2.py:179][0m |          -0.0047 |          34.0868 |         -30.7060 |
[32m[20221213 12:47:26 @agent_ppo2.py:179][0m |          -0.0179 |          34.0225 |         -32.3121 |
[32m[20221213 12:47:27 @agent_ppo2.py:179][0m |          -0.0243 |          33.7041 |         -33.5791 |
[32m[20221213 12:47:27 @agent_ppo2.py:179][0m |          -0.0267 |          33.5109 |         -34.5642 |
[32m[20221213 12:47:27 @agent_ppo2.py:179][0m |          -0.0364 |          33.5034 |         -35.2597 |
[32m[20221213 12:47:27 @agent_ppo2.py:179][0m |          -0.0338 |          33.7075 |         -35.3616 |
[32m[20221213 12:47:27 @agent_ppo2.py:179][0m |          -0.0274 |          34.7393 |         -35.8777 |
[32m[20221213 12:47:27 @agent_ppo2.py:179][0m |          -0.0385 |          33.1368 |         -36.4772 |
[32m[20221213 12:47:27 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:47:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.45
[32m[20221213 12:47:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.49
[32m[20221213 12:47:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.35
[32m[20221213 12:47:27 @agent_ppo2.py:137][0m Total time:      31.51 min
[32m[20221213 12:47:27 @agent_ppo2.py:139][0m 2156544 total steps have happened
[32m[20221213 12:47:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1053 --------------------------#
[32m[20221213 12:47:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:28 @agent_ppo2.py:179][0m |           0.0691 |          34.7779 |         -27.7566 |
[32m[20221213 12:47:28 @agent_ppo2.py:179][0m |           0.1648 |          34.1801 |         -13.7536 |
[32m[20221213 12:47:28 @agent_ppo2.py:179][0m |           0.0553 |          37.1738 |         -17.5373 |
[32m[20221213 12:47:28 @agent_ppo2.py:179][0m |           0.0198 |          33.1080 |         -22.6574 |
[32m[20221213 12:47:28 @agent_ppo2.py:179][0m |          -0.0011 |          32.8150 |         -26.4947 |
[32m[20221213 12:47:28 @agent_ppo2.py:179][0m |          -0.0089 |          32.9124 |         -29.0708 |
[32m[20221213 12:47:29 @agent_ppo2.py:179][0m |          -0.0129 |          32.4191 |         -29.7196 |
[32m[20221213 12:47:29 @agent_ppo2.py:179][0m |          -0.0253 |          32.3550 |         -31.1285 |
[32m[20221213 12:47:29 @agent_ppo2.py:179][0m |          -0.0246 |          32.1949 |         -32.2408 |
[32m[20221213 12:47:29 @agent_ppo2.py:179][0m |          -0.0315 |          32.1749 |         -33.6403 |
[32m[20221213 12:47:29 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 345.38
[32m[20221213 12:47:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.61
[32m[20221213 12:47:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.66
[32m[20221213 12:47:29 @agent_ppo2.py:137][0m Total time:      31.54 min
[32m[20221213 12:47:29 @agent_ppo2.py:139][0m 2158592 total steps have happened
[32m[20221213 12:47:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1054 --------------------------#
[32m[20221213 12:47:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |           0.0456 |          34.6686 |         -22.2644 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |           0.0386 |          33.9942 |         -20.8130 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |           0.0303 |          33.7095 |         -20.7599 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |           0.0130 |          33.6408 |         -21.2696 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |          -0.0104 |          33.4192 |         -24.8879 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |          -0.0100 |          34.9971 |         -26.2232 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |          -0.0186 |          33.2661 |         -26.8834 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |          -0.0250 |          33.0750 |         -28.4785 |
[32m[20221213 12:47:30 @agent_ppo2.py:179][0m |          -0.0271 |          32.9811 |         -30.1321 |
[32m[20221213 12:47:31 @agent_ppo2.py:179][0m |          -0.0243 |          32.9460 |         -31.2334 |
[32m[20221213 12:47:31 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:47:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 352.67
[32m[20221213 12:47:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.90
[32m[20221213 12:47:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 289.10
[32m[20221213 12:47:31 @agent_ppo2.py:137][0m Total time:      31.57 min
[32m[20221213 12:47:31 @agent_ppo2.py:139][0m 2160640 total steps have happened
[32m[20221213 12:47:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1055 --------------------------#
[32m[20221213 12:47:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:31 @agent_ppo2.py:179][0m |           0.0633 |          35.1022 |         -21.3943 |
[32m[20221213 12:47:31 @agent_ppo2.py:179][0m |           0.0400 |          36.6013 |         -19.3475 |
[32m[20221213 12:47:31 @agent_ppo2.py:179][0m |           0.0089 |          33.5543 |         -22.8638 |
[32m[20221213 12:47:32 @agent_ppo2.py:179][0m |          -0.0138 |          33.3375 |         -24.4891 |
[32m[20221213 12:47:32 @agent_ppo2.py:179][0m |          -0.0198 |          33.0436 |         -24.2890 |
[32m[20221213 12:47:32 @agent_ppo2.py:179][0m |          -0.0233 |          32.8300 |         -25.8782 |
[32m[20221213 12:47:32 @agent_ppo2.py:179][0m |          -0.0266 |          32.6590 |         -26.1112 |
[32m[20221213 12:47:32 @agent_ppo2.py:179][0m |          -0.0299 |          32.6634 |         -26.7479 |
[32m[20221213 12:47:32 @agent_ppo2.py:179][0m |          -0.0227 |          36.3256 |         -27.3007 |
[32m[20221213 12:47:32 @agent_ppo2.py:179][0m |          -0.0256 |          32.4043 |         -27.1940 |
[32m[20221213 12:47:32 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:47:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.45
[32m[20221213 12:47:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.14
[32m[20221213 12:47:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.22
[32m[20221213 12:47:33 @agent_ppo2.py:137][0m Total time:      31.60 min
[32m[20221213 12:47:33 @agent_ppo2.py:139][0m 2162688 total steps have happened
[32m[20221213 12:47:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1056 --------------------------#
[32m[20221213 12:47:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:33 @agent_ppo2.py:179][0m |           0.0865 |          37.4902 |         -20.2646 |
[32m[20221213 12:47:33 @agent_ppo2.py:179][0m |           0.0773 |          34.2601 |         -13.8419 |
[32m[20221213 12:47:33 @agent_ppo2.py:179][0m |           0.0326 |          33.5642 |         -17.6782 |
[32m[20221213 12:47:33 @agent_ppo2.py:179][0m |           0.0171 |          33.3155 |         -19.1509 |
[32m[20221213 12:47:33 @agent_ppo2.py:179][0m |          -0.0067 |          33.0406 |         -22.4291 |
[32m[20221213 12:47:34 @agent_ppo2.py:179][0m |          -0.0171 |          33.5080 |         -23.7424 |
[32m[20221213 12:47:34 @agent_ppo2.py:179][0m |          -0.0089 |          35.2950 |         -24.6494 |
[32m[20221213 12:47:34 @agent_ppo2.py:179][0m |          -0.0274 |          32.5821 |         -25.5774 |
[32m[20221213 12:47:34 @agent_ppo2.py:179][0m |          -0.0276 |          32.3511 |         -25.8014 |
[32m[20221213 12:47:34 @agent_ppo2.py:179][0m |          -0.0344 |          32.2617 |         -26.6200 |
[32m[20221213 12:47:34 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 302.93
[32m[20221213 12:47:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.04
[32m[20221213 12:47:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.53
[32m[20221213 12:47:34 @agent_ppo2.py:137][0m Total time:      31.63 min
[32m[20221213 12:47:34 @agent_ppo2.py:139][0m 2164736 total steps have happened
[32m[20221213 12:47:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1057 --------------------------#
[32m[20221213 12:47:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:35 @agent_ppo2.py:179][0m |           0.0613 |          34.2652 |         -18.6311 |
[32m[20221213 12:47:35 @agent_ppo2.py:179][0m |           0.0545 |          33.9641 |         -13.4823 |
[32m[20221213 12:47:35 @agent_ppo2.py:179][0m |           0.0291 |          33.6390 |         -15.3892 |
[32m[20221213 12:47:35 @agent_ppo2.py:179][0m |          -0.0014 |          33.3355 |         -17.2683 |
[32m[20221213 12:47:35 @agent_ppo2.py:179][0m |          -0.0061 |          33.1456 |         -18.2813 |
[32m[20221213 12:47:35 @agent_ppo2.py:179][0m |          -0.0138 |          33.0787 |         -19.0435 |
[32m[20221213 12:47:35 @agent_ppo2.py:179][0m |          -0.0214 |          33.0876 |         -19.9985 |
[32m[20221213 12:47:36 @agent_ppo2.py:179][0m |          -0.0204 |          33.0403 |         -19.4256 |
[32m[20221213 12:47:36 @agent_ppo2.py:179][0m |          -0.0255 |          32.9604 |         -20.3526 |
[32m[20221213 12:47:36 @agent_ppo2.py:179][0m |          -0.0256 |          32.8559 |         -21.5129 |
[32m[20221213 12:47:36 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:47:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.19
[32m[20221213 12:47:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.80
[32m[20221213 12:47:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.11
[32m[20221213 12:47:36 @agent_ppo2.py:137][0m Total time:      31.65 min
[32m[20221213 12:47:36 @agent_ppo2.py:139][0m 2166784 total steps have happened
[32m[20221213 12:47:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1058 --------------------------#
[32m[20221213 12:47:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:36 @agent_ppo2.py:179][0m |           0.0886 |          32.7961 |         -15.6349 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |           0.0412 |          29.8978 |         -11.5456 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0019 |          29.0092 |         -14.8524 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0227 |          28.3011 |         -17.1467 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0283 |          27.6580 |         -18.3855 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0332 |          27.0888 |         -18.5296 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0346 |          26.8721 |         -19.7446 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0382 |          26.4696 |         -20.0779 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0387 |          26.0950 |         -20.1050 |
[32m[20221213 12:47:37 @agent_ppo2.py:179][0m |          -0.0263 |          26.5715 |         -20.8887 |
[32m[20221213 12:47:37 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:47:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.55
[32m[20221213 12:47:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.89
[32m[20221213 12:47:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 271.36
[32m[20221213 12:47:38 @agent_ppo2.py:137][0m Total time:      31.68 min
[32m[20221213 12:47:38 @agent_ppo2.py:139][0m 2168832 total steps have happened
[32m[20221213 12:47:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1059 --------------------------#
[32m[20221213 12:47:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:38 @agent_ppo2.py:179][0m |           0.1264 |          37.7101 |         -13.9912 |
[32m[20221213 12:47:38 @agent_ppo2.py:179][0m |           0.0631 |          35.9707 |         -11.3645 |
[32m[20221213 12:47:38 @agent_ppo2.py:179][0m |           0.0152 |          36.1772 |         -13.8168 |
[32m[20221213 12:47:38 @agent_ppo2.py:179][0m |          -0.0137 |          34.9409 |         -16.0724 |
[32m[20221213 12:47:39 @agent_ppo2.py:179][0m |          -0.0257 |          34.5466 |         -16.8796 |
[32m[20221213 12:47:39 @agent_ppo2.py:179][0m |          -0.0328 |          34.3424 |         -18.1227 |
[32m[20221213 12:47:39 @agent_ppo2.py:179][0m |          -0.0299 |          35.9065 |         -18.6651 |
[32m[20221213 12:47:39 @agent_ppo2.py:179][0m |          -0.0320 |          33.8953 |         -18.6589 |
[32m[20221213 12:47:39 @agent_ppo2.py:179][0m |          -0.0413 |          33.7313 |         -20.3024 |
[32m[20221213 12:47:39 @agent_ppo2.py:179][0m |          -0.0430 |          33.6392 |         -20.2878 |
[32m[20221213 12:47:39 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 298.80
[32m[20221213 12:47:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.13
[32m[20221213 12:47:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.94
[32m[20221213 12:47:39 @agent_ppo2.py:137][0m Total time:      31.71 min
[32m[20221213 12:47:39 @agent_ppo2.py:139][0m 2170880 total steps have happened
[32m[20221213 12:47:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1060 --------------------------#
[32m[20221213 12:47:40 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:47:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:40 @agent_ppo2.py:179][0m |           0.0588 |          32.6291 |         -17.4381 |
[32m[20221213 12:47:40 @agent_ppo2.py:179][0m |           0.0280 |          31.4515 |         -16.4364 |
[32m[20221213 12:47:40 @agent_ppo2.py:179][0m |           0.0078 |          30.8229 |         -15.6926 |
[32m[20221213 12:47:40 @agent_ppo2.py:179][0m |          -0.0196 |          30.5116 |         -17.5322 |
[32m[20221213 12:47:40 @agent_ppo2.py:179][0m |          -0.0300 |          30.0539 |         -17.6667 |
[32m[20221213 12:47:40 @agent_ppo2.py:179][0m |          -0.0192 |          34.7479 |         -18.5646 |
[32m[20221213 12:47:41 @agent_ppo2.py:179][0m |          -0.0405 |          29.5535 |         -19.0406 |
[32m[20221213 12:47:41 @agent_ppo2.py:179][0m |          -0.0415 |          29.3530 |         -19.7308 |
[32m[20221213 12:47:41 @agent_ppo2.py:179][0m |          -0.0403 |          29.1002 |         -19.9415 |
[32m[20221213 12:47:41 @agent_ppo2.py:179][0m |          -0.0455 |          28.9136 |         -20.4377 |
[32m[20221213 12:47:41 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:47:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.21
[32m[20221213 12:47:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.62
[32m[20221213 12:47:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 314.14
[32m[20221213 12:47:41 @agent_ppo2.py:137][0m Total time:      31.74 min
[32m[20221213 12:47:41 @agent_ppo2.py:139][0m 2172928 total steps have happened
[32m[20221213 12:47:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1061 --------------------------#
[32m[20221213 12:47:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |           0.0554 |          35.2823 |         -13.9438 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |           0.0682 |          34.3798 |          -8.8737 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |           0.0252 |          34.0039 |         -10.0744 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |          -0.0041 |          33.6807 |         -13.6357 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |          -0.0147 |          33.4704 |         -15.2096 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |          -0.0222 |          33.2633 |         -15.6851 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |          -0.0306 |          33.0446 |         -17.1012 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |          -0.0283 |          33.5245 |         -17.9890 |
[32m[20221213 12:47:42 @agent_ppo2.py:179][0m |          -0.0333 |          32.7799 |         -18.6952 |
[32m[20221213 12:47:43 @agent_ppo2.py:179][0m |          -0.0348 |          32.7934 |         -19.9093 |
[32m[20221213 12:47:43 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.01
[32m[20221213 12:47:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.91
[32m[20221213 12:47:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.07
[32m[20221213 12:47:43 @agent_ppo2.py:137][0m Total time:      31.77 min
[32m[20221213 12:47:43 @agent_ppo2.py:139][0m 2174976 total steps have happened
[32m[20221213 12:47:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1062 --------------------------#
[32m[20221213 12:47:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:43 @agent_ppo2.py:179][0m |           0.0481 |          34.3907 |         -16.7324 |
[32m[20221213 12:47:43 @agent_ppo2.py:179][0m |           0.0590 |          33.7201 |         -10.0258 |
[32m[20221213 12:47:43 @agent_ppo2.py:179][0m |           0.0167 |          33.3901 |         -13.7681 |
[32m[20221213 12:47:44 @agent_ppo2.py:179][0m |           0.0133 |          37.0984 |         -15.5514 |
[32m[20221213 12:47:44 @agent_ppo2.py:179][0m |          -0.0143 |          33.1288 |         -15.7067 |
[32m[20221213 12:47:44 @agent_ppo2.py:179][0m |          -0.0158 |          35.4014 |         -16.9296 |
[32m[20221213 12:47:44 @agent_ppo2.py:179][0m |          -0.0273 |          32.8385 |         -18.1028 |
[32m[20221213 12:47:44 @agent_ppo2.py:179][0m |          -0.0212 |          32.7456 |         -18.5657 |
[32m[20221213 12:47:44 @agent_ppo2.py:179][0m |          -0.0263 |          32.7305 |         -18.5688 |
[32m[20221213 12:47:44 @agent_ppo2.py:179][0m |          -0.0338 |          32.6152 |         -19.9340 |
[32m[20221213 12:47:44 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:47:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 353.82
[32m[20221213 12:47:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.40
[32m[20221213 12:47:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.95
[32m[20221213 12:47:44 @agent_ppo2.py:137][0m Total time:      31.80 min
[32m[20221213 12:47:44 @agent_ppo2.py:139][0m 2177024 total steps have happened
[32m[20221213 12:47:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1063 --------------------------#
[32m[20221213 12:47:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:45 @agent_ppo2.py:179][0m |           0.0666 |          33.1602 |         -13.4110 |
[32m[20221213 12:47:45 @agent_ppo2.py:179][0m |           0.0770 |          30.4608 |          -9.0846 |
[32m[20221213 12:47:45 @agent_ppo2.py:179][0m |           0.0348 |          29.2415 |          -8.6222 |
[32m[20221213 12:47:45 @agent_ppo2.py:179][0m |           0.0034 |          29.0749 |         -11.6223 |
[32m[20221213 12:47:45 @agent_ppo2.py:179][0m |          -0.0164 |          28.0100 |         -13.2373 |
[32m[20221213 12:47:46 @agent_ppo2.py:179][0m |          -0.0270 |          27.7434 |         -14.4185 |
[32m[20221213 12:47:46 @agent_ppo2.py:179][0m |          -0.0280 |          27.6757 |         -14.2270 |
[32m[20221213 12:47:46 @agent_ppo2.py:179][0m |          -0.0377 |          27.3552 |         -15.6695 |
[32m[20221213 12:47:46 @agent_ppo2.py:179][0m |          -0.0368 |          27.2036 |         -16.0526 |
[32m[20221213 12:47:46 @agent_ppo2.py:179][0m |          -0.0367 |          26.9633 |         -15.8368 |
[32m[20221213 12:47:46 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:47:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.35
[32m[20221213 12:47:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.88
[32m[20221213 12:47:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.68
[32m[20221213 12:47:46 @agent_ppo2.py:137][0m Total time:      31.82 min
[32m[20221213 12:47:46 @agent_ppo2.py:139][0m 2179072 total steps have happened
[32m[20221213 12:47:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1064 --------------------------#
[32m[20221213 12:47:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |           0.1588 |          32.0091 |         -10.1743 |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |           0.0555 |          30.5029 |          -6.1267 |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |          -0.0030 |          30.0641 |          -9.2204 |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |          -0.0232 |          29.6003 |         -11.0254 |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |          -0.0385 |          29.3788 |         -11.9174 |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |          -0.0416 |          29.1801 |         -12.6350 |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |          -0.0480 |          28.8055 |         -13.4177 |
[32m[20221213 12:47:47 @agent_ppo2.py:179][0m |          -0.0456 |          28.7930 |         -13.5945 |
[32m[20221213 12:47:48 @agent_ppo2.py:179][0m |          -0.0534 |          28.5398 |         -13.9865 |
[32m[20221213 12:47:48 @agent_ppo2.py:179][0m |          -0.0575 |          28.3047 |         -14.9285 |
[32m[20221213 12:47:48 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.68
[32m[20221213 12:47:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.25
[32m[20221213 12:47:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 375.71
[32m[20221213 12:47:48 @agent_ppo2.py:137][0m Total time:      31.85 min
[32m[20221213 12:47:48 @agent_ppo2.py:139][0m 2181120 total steps have happened
[32m[20221213 12:47:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1065 --------------------------#
[32m[20221213 12:47:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:48 @agent_ppo2.py:179][0m |           0.0426 |          35.8302 |         -14.7582 |
[32m[20221213 12:47:48 @agent_ppo2.py:179][0m |           0.0118 |          34.0312 |         -12.5438 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0123 |          33.1390 |         -14.4914 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0222 |          32.6989 |         -14.6622 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0349 |          32.2464 |         -15.9112 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0416 |          32.2815 |         -16.3712 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0408 |          31.7811 |         -16.6433 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0441 |          31.6546 |         -17.5361 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0451 |          31.3321 |         -18.1553 |
[32m[20221213 12:47:49 @agent_ppo2.py:179][0m |          -0.0480 |          31.1958 |         -18.3781 |
[32m[20221213 12:47:49 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:47:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.86
[32m[20221213 12:47:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.72
[32m[20221213 12:47:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 315.57
[32m[20221213 12:47:50 @agent_ppo2.py:137][0m Total time:      31.88 min
[32m[20221213 12:47:50 @agent_ppo2.py:139][0m 2183168 total steps have happened
[32m[20221213 12:47:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1066 --------------------------#
[32m[20221213 12:47:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:50 @agent_ppo2.py:179][0m |           0.0827 |          34.7020 |         -14.0868 |
[32m[20221213 12:47:50 @agent_ppo2.py:179][0m |           0.0618 |          32.9703 |          -9.2896 |
[32m[20221213 12:47:50 @agent_ppo2.py:179][0m |           0.0322 |          32.9516 |         -11.5239 |
[32m[20221213 12:47:50 @agent_ppo2.py:179][0m |           0.0196 |          32.4176 |         -12.5705 |
[32m[20221213 12:47:51 @agent_ppo2.py:179][0m |          -0.0115 |          31.8474 |         -14.4780 |
[32m[20221213 12:47:51 @agent_ppo2.py:179][0m |          -0.0249 |          31.6167 |         -16.0422 |
[32m[20221213 12:47:51 @agent_ppo2.py:179][0m |          -0.0328 |          31.3598 |         -16.9187 |
[32m[20221213 12:47:51 @agent_ppo2.py:179][0m |          -0.0382 |          31.2431 |         -17.1744 |
[32m[20221213 12:47:51 @agent_ppo2.py:179][0m |          -0.0321 |          31.8296 |         -17.5242 |
[32m[20221213 12:47:51 @agent_ppo2.py:179][0m |          -0.0357 |          30.8987 |         -17.6095 |
[32m[20221213 12:47:51 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:47:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.31
[32m[20221213 12:47:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.42
[32m[20221213 12:47:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.08
[32m[20221213 12:47:51 @agent_ppo2.py:137][0m Total time:      31.91 min
[32m[20221213 12:47:51 @agent_ppo2.py:139][0m 2185216 total steps have happened
[32m[20221213 12:47:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1067 --------------------------#
[32m[20221213 12:47:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:52 @agent_ppo2.py:179][0m |           0.0831 |          35.0718 |         -10.7982 |
[32m[20221213 12:47:52 @agent_ppo2.py:179][0m |           0.0509 |          33.8169 |          -7.9899 |
[32m[20221213 12:47:52 @agent_ppo2.py:179][0m |           0.0226 |          33.2405 |          -8.9580 |
[32m[20221213 12:47:52 @agent_ppo2.py:179][0m |          -0.0103 |          32.9146 |         -10.9049 |
[32m[20221213 12:47:52 @agent_ppo2.py:179][0m |          -0.0201 |          32.5959 |         -11.6253 |
[32m[20221213 12:47:52 @agent_ppo2.py:179][0m |          -0.0237 |          32.4283 |         -11.9375 |
[32m[20221213 12:47:53 @agent_ppo2.py:179][0m |          -0.0317 |          32.1838 |         -12.8504 |
[32m[20221213 12:47:53 @agent_ppo2.py:179][0m |          -0.0339 |          32.4675 |         -13.4956 |
[32m[20221213 12:47:53 @agent_ppo2.py:179][0m |          -0.0385 |          32.0450 |         -14.0685 |
[32m[20221213 12:47:53 @agent_ppo2.py:179][0m |          -0.0374 |          31.8074 |         -15.1183 |
[32m[20221213 12:47:53 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.24
[32m[20221213 12:47:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.01
[32m[20221213 12:47:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 370.28
[32m[20221213 12:47:53 @agent_ppo2.py:137][0m Total time:      31.94 min
[32m[20221213 12:47:53 @agent_ppo2.py:139][0m 2187264 total steps have happened
[32m[20221213 12:47:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1068 --------------------------#
[32m[20221213 12:47:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:53 @agent_ppo2.py:179][0m |           0.0570 |          36.1786 |         -12.1189 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |           0.0239 |          34.2535 |         -11.4212 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |          -0.0150 |          33.3745 |         -13.3988 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |          -0.0194 |          32.8854 |         -13.5511 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |          -0.0209 |          33.5275 |         -14.0869 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |          -0.0341 |          32.4025 |         -15.4365 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |          -0.0359 |          31.6910 |         -15.5513 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |          -0.0334 |          36.2679 |         -16.5984 |
[32m[20221213 12:47:54 @agent_ppo2.py:179][0m |          -0.0465 |          31.6165 |         -17.2529 |
[32m[20221213 12:47:55 @agent_ppo2.py:179][0m |          -0.0365 |          31.6959 |         -17.3542 |
[32m[20221213 12:47:55 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 302.84
[32m[20221213 12:47:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.41
[32m[20221213 12:47:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 372.26
[32m[20221213 12:47:55 @agent_ppo2.py:137][0m Total time:      31.97 min
[32m[20221213 12:47:55 @agent_ppo2.py:139][0m 2189312 total steps have happened
[32m[20221213 12:47:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1069 --------------------------#
[32m[20221213 12:47:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:55 @agent_ppo2.py:179][0m |           0.0998 |          35.9501 |          -7.3046 |
[32m[20221213 12:47:55 @agent_ppo2.py:179][0m |           0.1378 |          38.6850 |          -2.8802 |
[32m[20221213 12:47:55 @agent_ppo2.py:179][0m |           0.0533 |          34.2735 |          -2.9661 |
[32m[20221213 12:47:56 @agent_ppo2.py:179][0m |           0.0314 |          33.5824 |          -5.8122 |
[32m[20221213 12:47:56 @agent_ppo2.py:179][0m |           0.0232 |          35.2203 |          -6.6719 |
[32m[20221213 12:47:56 @agent_ppo2.py:179][0m |          -0.0032 |          33.1381 |          -8.8656 |
[32m[20221213 12:47:56 @agent_ppo2.py:179][0m |          -0.0091 |          33.0816 |          -9.9925 |
[32m[20221213 12:47:56 @agent_ppo2.py:179][0m |          -0.0118 |          32.8808 |         -10.8568 |
[32m[20221213 12:47:56 @agent_ppo2.py:179][0m |          -0.0208 |          32.6793 |         -12.0639 |
[32m[20221213 12:47:56 @agent_ppo2.py:179][0m |          -0.0206 |          32.5945 |         -11.8408 |
[32m[20221213 12:47:56 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:47:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 360.64
[32m[20221213 12:47:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 373.77
[32m[20221213 12:47:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.34
[32m[20221213 12:47:56 @agent_ppo2.py:137][0m Total time:      32.00 min
[32m[20221213 12:47:56 @agent_ppo2.py:139][0m 2191360 total steps have happened
[32m[20221213 12:47:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1070 --------------------------#
[32m[20221213 12:47:57 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:47:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:57 @agent_ppo2.py:179][0m |           0.0560 |          35.3980 |          -9.6239 |
[32m[20221213 12:47:57 @agent_ppo2.py:179][0m |           0.0539 |          38.4027 |          -7.3158 |
[32m[20221213 12:47:57 @agent_ppo2.py:179][0m |          -0.0000 |          33.8499 |          -9.4009 |
[32m[20221213 12:47:57 @agent_ppo2.py:179][0m |          -0.0194 |          33.3182 |         -10.3344 |
[32m[20221213 12:47:57 @agent_ppo2.py:179][0m |          -0.0284 |          33.1035 |         -11.0784 |
[32m[20221213 12:47:58 @agent_ppo2.py:179][0m |          -0.0242 |          32.7964 |         -11.3176 |
[32m[20221213 12:47:58 @agent_ppo2.py:179][0m |          -0.0331 |          32.5075 |         -11.9347 |
[32m[20221213 12:47:58 @agent_ppo2.py:179][0m |          -0.0362 |          32.2533 |         -12.4060 |
[32m[20221213 12:47:58 @agent_ppo2.py:179][0m |          -0.0337 |          32.9379 |         -13.0159 |
[32m[20221213 12:47:58 @agent_ppo2.py:179][0m |          -0.0397 |          32.1323 |         -13.8053 |
[32m[20221213 12:47:58 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:47:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.34
[32m[20221213 12:47:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.26
[32m[20221213 12:47:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.32
[32m[20221213 12:47:58 @agent_ppo2.py:137][0m Total time:      32.02 min
[32m[20221213 12:47:58 @agent_ppo2.py:139][0m 2193408 total steps have happened
[32m[20221213 12:47:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1071 --------------------------#
[32m[20221213 12:47:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:47:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |           0.1267 |          36.7900 |         -10.0308 |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |           0.1037 |          34.8719 |          -2.6645 |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |           0.0510 |          37.2287 |          -5.1492 |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |           0.0263 |          37.0491 |          -6.7522 |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |           0.0032 |          33.9641 |          -8.2811 |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |          -0.0058 |          33.7213 |          -8.4189 |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |          -0.0193 |          33.4984 |          -9.3229 |
[32m[20221213 12:47:59 @agent_ppo2.py:179][0m |          -0.0242 |          33.4292 |         -10.6993 |
[32m[20221213 12:48:00 @agent_ppo2.py:179][0m |          -0.0192 |          34.1118 |         -10.6585 |
[32m[20221213 12:48:00 @agent_ppo2.py:179][0m |          -0.0279 |          33.3052 |         -11.5943 |
[32m[20221213 12:48:00 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.10
[32m[20221213 12:48:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.83
[32m[20221213 12:48:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.53
[32m[20221213 12:48:00 @agent_ppo2.py:137][0m Total time:      32.05 min
[32m[20221213 12:48:00 @agent_ppo2.py:139][0m 2195456 total steps have happened
[32m[20221213 12:48:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1072 --------------------------#
[32m[20221213 12:48:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:00 @agent_ppo2.py:179][0m |           0.0662 |          24.7246 |          -8.9502 |
[32m[20221213 12:48:00 @agent_ppo2.py:179][0m |           0.0271 |          23.0378 |          -4.1471 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0278 |          22.7229 |          -6.1001 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0423 |          22.6339 |          -6.7764 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0543 |          22.4717 |          -7.5863 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0585 |          22.3755 |          -8.1368 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0626 |          22.2518 |          -9.1662 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0600 |          22.1809 |          -9.5749 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0624 |          22.1004 |         -10.2145 |
[32m[20221213 12:48:01 @agent_ppo2.py:179][0m |          -0.0649 |          22.0640 |         -10.0586 |
[32m[20221213 12:48:01 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:48:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.77
[32m[20221213 12:48:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.75
[32m[20221213 12:48:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.55
[32m[20221213 12:48:02 @agent_ppo2.py:137][0m Total time:      32.08 min
[32m[20221213 12:48:02 @agent_ppo2.py:139][0m 2197504 total steps have happened
[32m[20221213 12:48:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1073 --------------------------#
[32m[20221213 12:48:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:02 @agent_ppo2.py:179][0m |           0.0652 |          31.0547 |         -15.3503 |
[32m[20221213 12:48:02 @agent_ppo2.py:179][0m |           0.0692 |          30.1293 |          -8.2987 |
[32m[20221213 12:48:02 @agent_ppo2.py:179][0m |           0.0062 |          29.7864 |         -12.7196 |
[32m[20221213 12:48:02 @agent_ppo2.py:179][0m |          -0.0199 |          29.5631 |         -14.7804 |
[32m[20221213 12:48:03 @agent_ppo2.py:179][0m |          -0.0264 |          29.4863 |         -16.2948 |
[32m[20221213 12:48:03 @agent_ppo2.py:179][0m |          -0.0377 |          29.2159 |         -17.7999 |
[32m[20221213 12:48:03 @agent_ppo2.py:179][0m |          -0.0385 |          29.1751 |         -18.7450 |
[32m[20221213 12:48:03 @agent_ppo2.py:179][0m |          -0.0444 |          29.0218 |         -19.3466 |
[32m[20221213 12:48:03 @agent_ppo2.py:179][0m |          -0.0450 |          28.9205 |         -18.9900 |
[32m[20221213 12:48:03 @agent_ppo2.py:179][0m |          -0.0476 |          28.8875 |         -20.3878 |
[32m[20221213 12:48:03 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 278.17
[32m[20221213 12:48:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.61
[32m[20221213 12:48:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.14
[32m[20221213 12:48:03 @agent_ppo2.py:137][0m Total time:      32.11 min
[32m[20221213 12:48:03 @agent_ppo2.py:139][0m 2199552 total steps have happened
[32m[20221213 12:48:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1074 --------------------------#
[32m[20221213 12:48:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:04 @agent_ppo2.py:179][0m |           0.0717 |          35.0509 |         -17.3907 |
[32m[20221213 12:48:04 @agent_ppo2.py:179][0m |           0.0684 |          32.8608 |         -11.3893 |
[32m[20221213 12:48:04 @agent_ppo2.py:179][0m |           0.0140 |          32.2334 |         -14.2442 |
[32m[20221213 12:48:04 @agent_ppo2.py:179][0m |          -0.0039 |          31.8733 |         -16.0653 |
[32m[20221213 12:48:04 @agent_ppo2.py:179][0m |          -0.0201 |          31.4690 |         -18.0546 |
[32m[20221213 12:48:04 @agent_ppo2.py:179][0m |          -0.0264 |          31.2860 |         -18.1588 |
[32m[20221213 12:48:04 @agent_ppo2.py:179][0m |          -0.0182 |          31.1326 |         -18.7480 |
[32m[20221213 12:48:05 @agent_ppo2.py:179][0m |          -0.0317 |          30.7754 |         -19.8384 |
[32m[20221213 12:48:05 @agent_ppo2.py:179][0m |          -0.0363 |          30.6767 |         -20.2655 |
[32m[20221213 12:48:05 @agent_ppo2.py:179][0m |          -0.0379 |          30.4921 |         -20.6574 |
[32m[20221213 12:48:05 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:48:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.41
[32m[20221213 12:48:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.00
[32m[20221213 12:48:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 384.47
[32m[20221213 12:48:05 @agent_ppo2.py:137][0m Total time:      32.14 min
[32m[20221213 12:48:05 @agent_ppo2.py:139][0m 2201600 total steps have happened
[32m[20221213 12:48:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1075 --------------------------#
[32m[20221213 12:48:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:05 @agent_ppo2.py:179][0m |           0.0533 |          35.0633 |         -17.3774 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |           0.0316 |          33.8835 |         -15.9578 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |           0.0164 |          37.4986 |         -17.2914 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |          -0.0146 |          32.9889 |         -18.9532 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |          -0.0299 |          32.6616 |         -19.5162 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |          -0.0361 |          32.3632 |         -20.0322 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |          -0.0298 |          32.1950 |         -20.3495 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |          -0.0294 |          34.7682 |         -20.7370 |
[32m[20221213 12:48:06 @agent_ppo2.py:179][0m |          -0.0399 |          32.0474 |         -21.4424 |
[32m[20221213 12:48:07 @agent_ppo2.py:179][0m |          -0.0409 |          31.8134 |         -22.2054 |
[32m[20221213 12:48:07 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:48:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.01
[32m[20221213 12:48:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.85
[32m[20221213 12:48:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.84
[32m[20221213 12:48:07 @agent_ppo2.py:137][0m Total time:      32.17 min
[32m[20221213 12:48:07 @agent_ppo2.py:139][0m 2203648 total steps have happened
[32m[20221213 12:48:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1076 --------------------------#
[32m[20221213 12:48:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:07 @agent_ppo2.py:179][0m |           0.0820 |          35.4951 |         -14.5414 |
[32m[20221213 12:48:07 @agent_ppo2.py:179][0m |           0.0598 |          35.1584 |          -9.3707 |
[32m[20221213 12:48:07 @agent_ppo2.py:179][0m |           0.0333 |          33.8289 |         -10.2212 |
[32m[20221213 12:48:08 @agent_ppo2.py:179][0m |           0.0093 |          33.3685 |         -14.5761 |
[32m[20221213 12:48:08 @agent_ppo2.py:179][0m |          -0.0002 |          33.9682 |         -16.5657 |
[32m[20221213 12:48:08 @agent_ppo2.py:179][0m |          -0.0123 |          33.0627 |         -16.9687 |
[32m[20221213 12:48:08 @agent_ppo2.py:179][0m |          -0.0182 |          33.0250 |         -17.8007 |
[32m[20221213 12:48:08 @agent_ppo2.py:179][0m |          -0.0270 |          32.8430 |         -18.6361 |
[32m[20221213 12:48:08 @agent_ppo2.py:179][0m |          -0.0285 |          32.7103 |         -19.3563 |
[32m[20221213 12:48:08 @agent_ppo2.py:179][0m |          -0.0258 |          32.6232 |         -18.8978 |
[32m[20221213 12:48:08 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:48:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.02
[32m[20221213 12:48:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.27
[32m[20221213 12:48:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.32
[32m[20221213 12:48:09 @agent_ppo2.py:137][0m Total time:      32.20 min
[32m[20221213 12:48:09 @agent_ppo2.py:139][0m 2205696 total steps have happened
[32m[20221213 12:48:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1077 --------------------------#
[32m[20221213 12:48:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:09 @agent_ppo2.py:179][0m |           0.0585 |          35.0091 |         -14.0591 |
[32m[20221213 12:48:09 @agent_ppo2.py:179][0m |           0.0377 |          34.0996 |         -12.0346 |
[32m[20221213 12:48:09 @agent_ppo2.py:179][0m |           0.0280 |          33.5089 |         -13.7546 |
[32m[20221213 12:48:09 @agent_ppo2.py:179][0m |          -0.0039 |          33.2086 |         -16.1665 |
[32m[20221213 12:48:09 @agent_ppo2.py:179][0m |          -0.0093 |          33.0296 |         -16.7256 |
[32m[20221213 12:48:10 @agent_ppo2.py:179][0m |          -0.0182 |          32.7492 |         -16.4343 |
[32m[20221213 12:48:10 @agent_ppo2.py:179][0m |          -0.0281 |          32.5278 |         -17.7978 |
[32m[20221213 12:48:10 @agent_ppo2.py:179][0m |          -0.0234 |          33.4397 |         -18.5628 |
[32m[20221213 12:48:10 @agent_ppo2.py:179][0m |          -0.0325 |          32.1490 |         -18.6071 |
[32m[20221213 12:48:10 @agent_ppo2.py:179][0m |          -0.0355 |          32.0145 |         -19.7067 |
[32m[20221213 12:48:10 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:48:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.94
[32m[20221213 12:48:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.22
[32m[20221213 12:48:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.25
[32m[20221213 12:48:10 @agent_ppo2.py:137][0m Total time:      32.22 min
[32m[20221213 12:48:10 @agent_ppo2.py:139][0m 2207744 total steps have happened
[32m[20221213 12:48:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1078 --------------------------#
[32m[20221213 12:48:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |           0.0824 |          32.3638 |         -13.0605 |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |           0.0245 |          30.9283 |         -10.4757 |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |          -0.0179 |          30.3546 |         -12.9526 |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |          -0.0311 |          29.8958 |         -13.7894 |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |          -0.0378 |          29.6889 |         -13.9277 |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |          -0.0396 |          29.4537 |         -15.2776 |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |          -0.0428 |          29.2826 |         -15.6626 |
[32m[20221213 12:48:11 @agent_ppo2.py:179][0m |          -0.0475 |          29.2121 |         -16.1229 |
[32m[20221213 12:48:12 @agent_ppo2.py:179][0m |          -0.0541 |          28.9742 |         -16.4541 |
[32m[20221213 12:48:12 @agent_ppo2.py:179][0m |          -0.0522 |          28.8957 |         -17.3088 |
[32m[20221213 12:48:12 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.76
[32m[20221213 12:48:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.57
[32m[20221213 12:48:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.04
[32m[20221213 12:48:12 @agent_ppo2.py:137][0m Total time:      32.25 min
[32m[20221213 12:48:12 @agent_ppo2.py:139][0m 2209792 total steps have happened
[32m[20221213 12:48:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1079 --------------------------#
[32m[20221213 12:48:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:12 @agent_ppo2.py:179][0m |           0.0813 |          34.3789 |         -12.7635 |
[32m[20221213 12:48:12 @agent_ppo2.py:179][0m |           0.0373 |          33.2160 |         -11.2794 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |           0.0090 |          32.8439 |         -15.5955 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |           0.0014 |          36.0065 |         -18.1172 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |          -0.0043 |          34.1244 |         -18.7457 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |          -0.0027 |          32.0188 |         -16.8931 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |          -0.0228 |          31.8648 |         -20.1094 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |          -0.0244 |          31.6604 |         -20.9638 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |          -0.0299 |          31.4664 |         -21.7976 |
[32m[20221213 12:48:13 @agent_ppo2.py:179][0m |          -0.0187 |          31.5078 |         -22.1778 |
[32m[20221213 12:48:13 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:48:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 358.29
[32m[20221213 12:48:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.29
[32m[20221213 12:48:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 305.69
[32m[20221213 12:48:14 @agent_ppo2.py:137][0m Total time:      32.28 min
[32m[20221213 12:48:14 @agent_ppo2.py:139][0m 2211840 total steps have happened
[32m[20221213 12:48:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1080 --------------------------#
[32m[20221213 12:48:14 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:48:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:14 @agent_ppo2.py:179][0m |           0.0977 |          35.2448 |         -14.7182 |
[32m[20221213 12:48:14 @agent_ppo2.py:179][0m |           0.0697 |          34.0967 |          -9.8328 |
[32m[20221213 12:48:14 @agent_ppo2.py:179][0m |           0.0201 |          33.4290 |         -13.1603 |
[32m[20221213 12:48:14 @agent_ppo2.py:179][0m |          -0.0058 |          32.8263 |         -17.0170 |
[32m[20221213 12:48:15 @agent_ppo2.py:179][0m |          -0.0097 |          33.1703 |         -17.5223 |
[32m[20221213 12:48:15 @agent_ppo2.py:179][0m |          -0.0245 |          32.2696 |         -19.1117 |
[32m[20221213 12:48:15 @agent_ppo2.py:179][0m |          -0.0262 |          32.0619 |         -20.2025 |
[32m[20221213 12:48:15 @agent_ppo2.py:179][0m |          -0.0303 |          31.9536 |         -20.5972 |
[32m[20221213 12:48:15 @agent_ppo2.py:179][0m |          -0.0367 |          31.8087 |         -21.9002 |
[32m[20221213 12:48:15 @agent_ppo2.py:179][0m |          -0.0299 |          34.4353 |         -22.4218 |
[32m[20221213 12:48:15 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 341.31
[32m[20221213 12:48:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.99
[32m[20221213 12:48:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.25
[32m[20221213 12:48:15 @agent_ppo2.py:137][0m Total time:      32.31 min
[32m[20221213 12:48:15 @agent_ppo2.py:139][0m 2213888 total steps have happened
[32m[20221213 12:48:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1081 --------------------------#
[32m[20221213 12:48:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:16 @agent_ppo2.py:179][0m |           0.1719 |          12.3714 |          -2.5157 |
[32m[20221213 12:48:16 @agent_ppo2.py:179][0m |           0.1559 |          11.4026 |           0.1761 |
[32m[20221213 12:48:16 @agent_ppo2.py:179][0m |           0.1510 |          11.4230 |           0.2036 |
[32m[20221213 12:48:16 @agent_ppo2.py:179][0m |           0.1469 |          11.3924 |          -0.0180 |
[32m[20221213 12:48:16 @agent_ppo2.py:179][0m |           0.1445 |          11.3840 |          -0.4414 |
[32m[20221213 12:48:16 @agent_ppo2.py:179][0m |           0.1399 |          11.3737 |          -0.4150 |
[32m[20221213 12:48:16 @agent_ppo2.py:179][0m |           0.1369 |          11.4012 |          -0.8093 |
[32m[20221213 12:48:17 @agent_ppo2.py:179][0m |           0.1367 |          11.3786 |          -0.8840 |
[32m[20221213 12:48:17 @agent_ppo2.py:179][0m |           0.1360 |          11.3902 |          -0.8161 |
[32m[20221213 12:48:17 @agent_ppo2.py:179][0m |           0.1385 |          11.3526 |          -0.8988 |
[32m[20221213 12:48:17 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:48:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 42.71
[32m[20221213 12:48:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 42.95
[32m[20221213 12:48:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.27
[32m[20221213 12:48:17 @agent_ppo2.py:137][0m Total time:      32.34 min
[32m[20221213 12:48:17 @agent_ppo2.py:139][0m 2215936 total steps have happened
[32m[20221213 12:48:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1082 --------------------------#
[32m[20221213 12:48:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:17 @agent_ppo2.py:179][0m |           0.0897 |          30.5210 |         -14.7904 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |           0.0864 |          28.4056 |          -7.7974 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |           0.0417 |          27.7427 |          -7.8760 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |           0.0148 |          27.3417 |          -9.7454 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |           0.0049 |          27.8614 |         -11.0140 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |          -0.0136 |          26.7796 |         -11.8937 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |          -0.0223 |          26.6083 |         -13.1000 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |          -0.0239 |          26.7331 |         -14.2191 |
[32m[20221213 12:48:18 @agent_ppo2.py:179][0m |          -0.0283 |          26.3912 |         -14.5487 |
[32m[20221213 12:48:19 @agent_ppo2.py:179][0m |          -0.0261 |          26.2780 |         -15.3116 |
[32m[20221213 12:48:19 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:48:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.82
[32m[20221213 12:48:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.36
[32m[20221213 12:48:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.78
[32m[20221213 12:48:19 @agent_ppo2.py:137][0m Total time:      32.37 min
[32m[20221213 12:48:19 @agent_ppo2.py:139][0m 2217984 total steps have happened
[32m[20221213 12:48:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1083 --------------------------#
[32m[20221213 12:48:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:19 @agent_ppo2.py:179][0m |           0.0855 |          35.5786 |         -13.3404 |
[32m[20221213 12:48:19 @agent_ppo2.py:179][0m |           0.0580 |          31.4419 |         -10.0877 |
[32m[20221213 12:48:19 @agent_ppo2.py:179][0m |           0.0036 |          30.8285 |         -13.3007 |
[32m[20221213 12:48:20 @agent_ppo2.py:179][0m |          -0.0174 |          30.3612 |         -15.1120 |
[32m[20221213 12:48:20 @agent_ppo2.py:179][0m |          -0.0147 |          30.1803 |         -15.4288 |
[32m[20221213 12:48:20 @agent_ppo2.py:179][0m |          -0.0270 |          29.7037 |         -16.6588 |
[32m[20221213 12:48:20 @agent_ppo2.py:179][0m |          -0.0370 |          29.7223 |         -17.6845 |
[32m[20221213 12:48:20 @agent_ppo2.py:179][0m |          -0.0292 |          29.2379 |         -17.3874 |
[32m[20221213 12:48:20 @agent_ppo2.py:179][0m |          -0.0343 |          28.9990 |         -18.4436 |
[32m[20221213 12:48:20 @agent_ppo2.py:179][0m |          -0.0349 |          28.7909 |         -17.9782 |
[32m[20221213 12:48:20 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:48:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.47
[32m[20221213 12:48:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.37
[32m[20221213 12:48:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.05
[32m[20221213 12:48:20 @agent_ppo2.py:137][0m Total time:      32.40 min
[32m[20221213 12:48:20 @agent_ppo2.py:139][0m 2220032 total steps have happened
[32m[20221213 12:48:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1084 --------------------------#
[32m[20221213 12:48:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:21 @agent_ppo2.py:179][0m |           0.0743 |          34.6771 |         -12.0194 |
[32m[20221213 12:48:21 @agent_ppo2.py:179][0m |           0.2523 |          33.6077 |          -5.2766 |
[32m[20221213 12:48:21 @agent_ppo2.py:179][0m |           0.0451 |          33.6683 |          -6.5434 |
[32m[20221213 12:48:21 @agent_ppo2.py:179][0m |           0.0191 |          34.8273 |          -9.2826 |
[32m[20221213 12:48:21 @agent_ppo2.py:179][0m |           0.0024 |          33.3379 |         -12.5803 |
[32m[20221213 12:48:22 @agent_ppo2.py:179][0m |          -0.0048 |          32.3077 |         -13.5436 |
[32m[20221213 12:48:22 @agent_ppo2.py:179][0m |          -0.0162 |          32.2955 |         -15.7255 |
[32m[20221213 12:48:22 @agent_ppo2.py:179][0m |          -0.0208 |          32.1153 |         -15.9884 |
[32m[20221213 12:48:22 @agent_ppo2.py:179][0m |          -0.0285 |          31.9126 |         -17.4232 |
[32m[20221213 12:48:22 @agent_ppo2.py:179][0m |          -0.0284 |          31.9007 |         -17.9356 |
[32m[20221213 12:48:22 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:48:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.19
[32m[20221213 12:48:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.11
[32m[20221213 12:48:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.16
[32m[20221213 12:48:22 @agent_ppo2.py:137][0m Total time:      32.42 min
[32m[20221213 12:48:22 @agent_ppo2.py:139][0m 2222080 total steps have happened
[32m[20221213 12:48:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1085 --------------------------#
[32m[20221213 12:48:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |           0.0864 |          33.9200 |         -11.3985 |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |           0.0424 |          32.9467 |          -7.4778 |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |           0.0072 |          32.5202 |          -9.8759 |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |          -0.0086 |          32.1447 |         -10.5340 |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |          -0.0228 |          31.9577 |         -11.5498 |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |          -0.0285 |          31.9965 |         -12.0413 |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |          -0.0326 |          31.8204 |         -13.0227 |
[32m[20221213 12:48:23 @agent_ppo2.py:179][0m |          -0.0291 |          31.5137 |         -12.6682 |
[32m[20221213 12:48:24 @agent_ppo2.py:179][0m |          -0.0367 |          31.3865 |         -13.4039 |
[32m[20221213 12:48:24 @agent_ppo2.py:179][0m |          -0.0388 |          31.2893 |         -14.2886 |
[32m[20221213 12:48:24 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 314.73
[32m[20221213 12:48:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.92
[32m[20221213 12:48:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.73
[32m[20221213 12:48:24 @agent_ppo2.py:137][0m Total time:      32.45 min
[32m[20221213 12:48:24 @agent_ppo2.py:139][0m 2224128 total steps have happened
[32m[20221213 12:48:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1086 --------------------------#
[32m[20221213 12:48:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:24 @agent_ppo2.py:179][0m |           0.0670 |          35.5977 |         -10.2770 |
[32m[20221213 12:48:24 @agent_ppo2.py:179][0m |           0.0464 |          34.4554 |          -8.7706 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |           0.0051 |          33.9905 |         -11.5187 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |          -0.0047 |          34.2404 |         -12.9324 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |          -0.0239 |          33.2946 |         -12.7681 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |          -0.0232 |          33.1497 |         -13.3529 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |          -0.0261 |          33.0074 |         -13.0413 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |          -0.0265 |          33.1815 |         -13.9726 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |          -0.0259 |          32.8040 |         -13.6720 |
[32m[20221213 12:48:25 @agent_ppo2.py:179][0m |          -0.0351 |          32.5882 |         -14.5440 |
[32m[20221213 12:48:25 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:48:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 327.75
[32m[20221213 12:48:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.07
[32m[20221213 12:48:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 365.32
[32m[20221213 12:48:26 @agent_ppo2.py:137][0m Total time:      32.48 min
[32m[20221213 12:48:26 @agent_ppo2.py:139][0m 2226176 total steps have happened
[32m[20221213 12:48:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1087 --------------------------#
[32m[20221213 12:48:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:26 @agent_ppo2.py:179][0m |           0.0486 |          34.1244 |         -10.4587 |
[32m[20221213 12:48:26 @agent_ppo2.py:179][0m |           0.0214 |          33.1854 |          -8.6291 |
[32m[20221213 12:48:26 @agent_ppo2.py:179][0m |           0.0024 |          32.4738 |          -8.8095 |
[32m[20221213 12:48:26 @agent_ppo2.py:179][0m |          -0.0154 |          32.0280 |         -10.9313 |
[32m[20221213 12:48:27 @agent_ppo2.py:179][0m |          -0.0275 |          31.7861 |         -11.8247 |
[32m[20221213 12:48:27 @agent_ppo2.py:179][0m |          -0.0255 |          32.2170 |         -11.7806 |
[32m[20221213 12:48:27 @agent_ppo2.py:179][0m |          -0.0356 |          31.3642 |         -12.3997 |
[32m[20221213 12:48:27 @agent_ppo2.py:179][0m |          -0.0363 |          31.0612 |         -13.0419 |
[32m[20221213 12:48:27 @agent_ppo2.py:179][0m |          -0.0323 |          31.1168 |         -13.4345 |
[32m[20221213 12:48:27 @agent_ppo2.py:179][0m |          -0.0399 |          30.8033 |         -13.9142 |
[32m[20221213 12:48:27 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:48:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.77
[32m[20221213 12:48:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.40
[32m[20221213 12:48:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.90
[32m[20221213 12:48:27 @agent_ppo2.py:137][0m Total time:      32.51 min
[32m[20221213 12:48:27 @agent_ppo2.py:139][0m 2228224 total steps have happened
[32m[20221213 12:48:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1088 --------------------------#
[32m[20221213 12:48:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:28 @agent_ppo2.py:179][0m |           0.0791 |          34.6462 |         -11.8137 |
[32m[20221213 12:48:28 @agent_ppo2.py:179][0m |           0.0297 |          33.5477 |          -8.9502 |
[32m[20221213 12:48:28 @agent_ppo2.py:179][0m |          -0.0019 |          33.1108 |         -11.4811 |
[32m[20221213 12:48:28 @agent_ppo2.py:179][0m |          -0.0083 |          32.8091 |         -11.4033 |
[32m[20221213 12:48:28 @agent_ppo2.py:179][0m |          -0.0243 |          32.7208 |         -12.8108 |
[32m[20221213 12:48:28 @agent_ppo2.py:179][0m |          -0.0259 |          32.5472 |         -12.8712 |
[32m[20221213 12:48:28 @agent_ppo2.py:179][0m |          -0.0271 |          32.4033 |         -12.7391 |
[32m[20221213 12:48:29 @agent_ppo2.py:179][0m |          -0.0349 |          32.3307 |         -13.7003 |
[32m[20221213 12:48:29 @agent_ppo2.py:179][0m |          -0.0356 |          32.3418 |         -13.5583 |
[32m[20221213 12:48:29 @agent_ppo2.py:179][0m |          -0.0305 |          32.1618 |         -13.5740 |
[32m[20221213 12:48:29 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:48:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.76
[32m[20221213 12:48:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.53
[32m[20221213 12:48:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.05
[32m[20221213 12:48:29 @agent_ppo2.py:137][0m Total time:      32.54 min
[32m[20221213 12:48:29 @agent_ppo2.py:139][0m 2230272 total steps have happened
[32m[20221213 12:48:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1089 --------------------------#
[32m[20221213 12:48:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:29 @agent_ppo2.py:179][0m |           0.0381 |          30.2271 |          -7.3403 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |           0.0338 |          29.4026 |          -5.2916 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |          -0.0093 |          29.0260 |          -5.9461 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |          -0.0224 |          29.6770 |          -7.1509 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |          -0.0325 |          28.9036 |          -8.7997 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |          -0.0417 |          28.5638 |          -9.1699 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |          -0.0422 |          28.6135 |          -9.5609 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |          -0.0478 |          28.3169 |         -10.3440 |
[32m[20221213 12:48:30 @agent_ppo2.py:179][0m |          -0.0486 |          28.1919 |         -11.0426 |
[32m[20221213 12:48:31 @agent_ppo2.py:179][0m |          -0.0500 |          28.1602 |         -11.2098 |
[32m[20221213 12:48:31 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.18
[32m[20221213 12:48:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.74
[32m[20221213 12:48:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.55
[32m[20221213 12:48:31 @agent_ppo2.py:137][0m Total time:      32.57 min
[32m[20221213 12:48:31 @agent_ppo2.py:139][0m 2232320 total steps have happened
[32m[20221213 12:48:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1090 --------------------------#
[32m[20221213 12:48:31 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:48:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:31 @agent_ppo2.py:179][0m |           0.0608 |          30.2200 |         -12.0197 |
[32m[20221213 12:48:31 @agent_ppo2.py:179][0m |           0.0458 |          29.2299 |          -6.4846 |
[32m[20221213 12:48:31 @agent_ppo2.py:179][0m |          -0.0125 |          28.8521 |          -9.6617 |
[32m[20221213 12:48:32 @agent_ppo2.py:179][0m |          -0.0191 |          28.5882 |         -10.1459 |
[32m[20221213 12:48:32 @agent_ppo2.py:179][0m |          -0.0284 |          28.5450 |         -11.2156 |
[32m[20221213 12:48:32 @agent_ppo2.py:179][0m |          -0.0386 |          28.3694 |         -11.7360 |
[32m[20221213 12:48:32 @agent_ppo2.py:179][0m |          -0.0373 |          28.2496 |         -11.9605 |
[32m[20221213 12:48:32 @agent_ppo2.py:179][0m |          -0.0412 |          28.2078 |         -12.3252 |
[32m[20221213 12:48:32 @agent_ppo2.py:179][0m |          -0.0457 |          28.0542 |         -12.9241 |
[32m[20221213 12:48:32 @agent_ppo2.py:179][0m |          -0.0440 |          28.0237 |         -13.3525 |
[32m[20221213 12:48:32 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:48:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 271.71
[32m[20221213 12:48:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.76
[32m[20221213 12:48:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 43.96
[32m[20221213 12:48:32 @agent_ppo2.py:137][0m Total time:      32.60 min
[32m[20221213 12:48:32 @agent_ppo2.py:139][0m 2234368 total steps have happened
[32m[20221213 12:48:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1091 --------------------------#
[32m[20221213 12:48:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:33 @agent_ppo2.py:179][0m |           0.0851 |          34.9015 |         -10.1192 |
[32m[20221213 12:48:33 @agent_ppo2.py:179][0m |           0.0841 |          35.0224 |          -4.0546 |
[32m[20221213 12:48:33 @agent_ppo2.py:179][0m |           0.0375 |          33.9207 |          -5.9300 |
[32m[20221213 12:48:33 @agent_ppo2.py:179][0m |           0.0107 |          33.5000 |          -8.9622 |
[32m[20221213 12:48:33 @agent_ppo2.py:179][0m |          -0.0021 |          33.4020 |          -9.9464 |
[32m[20221213 12:48:34 @agent_ppo2.py:179][0m |          -0.0158 |          33.1868 |         -11.2409 |
[32m[20221213 12:48:34 @agent_ppo2.py:179][0m |          -0.0188 |          33.7187 |         -12.5837 |
[32m[20221213 12:48:34 @agent_ppo2.py:179][0m |          -0.0285 |          33.1046 |         -12.9380 |
[32m[20221213 12:48:34 @agent_ppo2.py:179][0m |          -0.0263 |          32.9527 |         -12.9183 |
[32m[20221213 12:48:34 @agent_ppo2.py:179][0m |          -0.0283 |          32.8068 |         -13.4373 |
[32m[20221213 12:48:34 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:48:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.76
[32m[20221213 12:48:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.53
[32m[20221213 12:48:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.30
[32m[20221213 12:48:34 @agent_ppo2.py:137][0m Total time:      32.62 min
[32m[20221213 12:48:34 @agent_ppo2.py:139][0m 2236416 total steps have happened
[32m[20221213 12:48:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1092 --------------------------#
[32m[20221213 12:48:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |           0.0792 |          31.7418 |          -9.7798 |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |           0.0433 |          30.2350 |          -6.4796 |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |           0.0003 |          30.9272 |          -7.8243 |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |          -0.0105 |          29.6290 |          -7.4621 |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |          -0.0347 |          29.4098 |          -9.1839 |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |          -0.0407 |          29.2807 |          -9.9524 |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |          -0.0430 |          29.5115 |         -10.2072 |
[32m[20221213 12:48:35 @agent_ppo2.py:179][0m |          -0.0499 |          29.0390 |         -10.9439 |
[32m[20221213 12:48:36 @agent_ppo2.py:179][0m |          -0.0451 |          28.8646 |         -11.2307 |
[32m[20221213 12:48:36 @agent_ppo2.py:179][0m |          -0.0517 |          28.7879 |         -12.0743 |
[32m[20221213 12:48:36 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:48:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 278.16
[32m[20221213 12:48:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.45
[32m[20221213 12:48:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.75
[32m[20221213 12:48:36 @agent_ppo2.py:137][0m Total time:      32.65 min
[32m[20221213 12:48:36 @agent_ppo2.py:139][0m 2238464 total steps have happened
[32m[20221213 12:48:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1093 --------------------------#
[32m[20221213 12:48:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:36 @agent_ppo2.py:179][0m |           0.0722 |          37.1526 |          -8.8960 |
[32m[20221213 12:48:36 @agent_ppo2.py:179][0m |           0.0352 |          34.6542 |          -7.3590 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |           0.0021 |          34.0167 |          -8.8931 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |          -0.0160 |          33.5582 |          -9.9090 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |          -0.0269 |          33.2395 |         -10.4065 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |          -0.0295 |          32.9593 |         -10.8118 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |          -0.0296 |          32.7013 |         -11.7620 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |          -0.0306 |          32.6227 |         -11.7536 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |          -0.0320 |          32.2487 |         -11.8766 |
[32m[20221213 12:48:37 @agent_ppo2.py:179][0m |          -0.0408 |          32.1441 |         -12.2166 |
[32m[20221213 12:48:37 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:48:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.13
[32m[20221213 12:48:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.52
[32m[20221213 12:48:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 370.42
[32m[20221213 12:48:38 @agent_ppo2.py:137][0m Total time:      32.68 min
[32m[20221213 12:48:38 @agent_ppo2.py:139][0m 2240512 total steps have happened
[32m[20221213 12:48:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1094 --------------------------#
[32m[20221213 12:48:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:38 @agent_ppo2.py:179][0m |           0.0824 |          35.9326 |          -9.1380 |
[32m[20221213 12:48:38 @agent_ppo2.py:179][0m |           0.0936 |          35.6494 |          -3.8869 |
[32m[20221213 12:48:38 @agent_ppo2.py:179][0m |           0.0210 |          34.8601 |          -6.9100 |
[32m[20221213 12:48:38 @agent_ppo2.py:179][0m |          -0.0076 |          34.5197 |          -9.0539 |
[32m[20221213 12:48:39 @agent_ppo2.py:179][0m |          -0.0131 |          37.2049 |          -9.7432 |
[32m[20221213 12:48:39 @agent_ppo2.py:179][0m |          -0.0226 |          34.3010 |         -10.2610 |
[32m[20221213 12:48:39 @agent_ppo2.py:179][0m |          -0.0273 |          35.1693 |         -10.8214 |
[32m[20221213 12:48:39 @agent_ppo2.py:179][0m |          -0.0246 |          35.4513 |         -11.1155 |
[32m[20221213 12:48:39 @agent_ppo2.py:179][0m |          -0.0328 |          33.9608 |         -11.7914 |
[32m[20221213 12:48:39 @agent_ppo2.py:179][0m |          -0.0334 |          34.0148 |         -12.2586 |
[32m[20221213 12:48:39 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.04
[32m[20221213 12:48:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.56
[32m[20221213 12:48:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.82
[32m[20221213 12:48:39 @agent_ppo2.py:137][0m Total time:      32.71 min
[32m[20221213 12:48:39 @agent_ppo2.py:139][0m 2242560 total steps have happened
[32m[20221213 12:48:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1095 --------------------------#
[32m[20221213 12:48:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:40 @agent_ppo2.py:179][0m |           0.0645 |          35.4410 |          -7.8231 |
[32m[20221213 12:48:40 @agent_ppo2.py:179][0m |           0.0779 |          34.3579 |          -4.6405 |
[32m[20221213 12:48:40 @agent_ppo2.py:179][0m |           0.0354 |          33.5675 |          -5.7945 |
[32m[20221213 12:48:40 @agent_ppo2.py:179][0m |           0.0069 |          33.1337 |          -6.0444 |
[32m[20221213 12:48:40 @agent_ppo2.py:179][0m |          -0.0122 |          32.7519 |          -8.1692 |
[32m[20221213 12:48:40 @agent_ppo2.py:179][0m |          -0.0207 |          32.5384 |          -9.1355 |
[32m[20221213 12:48:41 @agent_ppo2.py:179][0m |          -0.0283 |          32.2727 |          -9.6189 |
[32m[20221213 12:48:41 @agent_ppo2.py:179][0m |          -0.0318 |          32.1511 |         -10.2016 |
[32m[20221213 12:48:41 @agent_ppo2.py:179][0m |          -0.0351 |          32.0656 |         -10.9910 |
[32m[20221213 12:48:41 @agent_ppo2.py:179][0m |          -0.0349 |          31.9104 |         -11.2113 |
[32m[20221213 12:48:41 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:48:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 352.39
[32m[20221213 12:48:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.78
[32m[20221213 12:48:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.13
[32m[20221213 12:48:41 @agent_ppo2.py:137][0m Total time:      32.74 min
[32m[20221213 12:48:41 @agent_ppo2.py:139][0m 2244608 total steps have happened
[32m[20221213 12:48:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1096 --------------------------#
[32m[20221213 12:48:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |           0.0908 |          35.0598 |          -6.1205 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |           0.0464 |          33.7721 |          -3.3590 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |           0.0170 |          36.3534 |          -5.7930 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |          -0.0040 |          33.4074 |          -6.1397 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |          -0.0152 |          32.4920 |          -7.1401 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |          -0.0178 |          32.3693 |          -7.7344 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |          -0.0229 |          31.9166 |          -8.0800 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |          -0.0327 |          31.6106 |          -8.3277 |
[32m[20221213 12:48:42 @agent_ppo2.py:179][0m |          -0.0361 |          31.4809 |          -8.7599 |
[32m[20221213 12:48:43 @agent_ppo2.py:179][0m |          -0.0371 |          31.4145 |          -9.2149 |
[32m[20221213 12:48:43 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:48:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 304.27
[32m[20221213 12:48:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.24
[32m[20221213 12:48:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.71
[32m[20221213 12:48:43 @agent_ppo2.py:137][0m Total time:      32.77 min
[32m[20221213 12:48:43 @agent_ppo2.py:139][0m 2246656 total steps have happened
[32m[20221213 12:48:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1097 --------------------------#
[32m[20221213 12:48:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:43 @agent_ppo2.py:179][0m |           0.0537 |          36.1013 |          -7.0224 |
[32m[20221213 12:48:43 @agent_ppo2.py:179][0m |           0.0386 |          35.1509 |          -4.0790 |
[32m[20221213 12:48:43 @agent_ppo2.py:179][0m |           0.0046 |          34.7065 |          -6.5243 |
[32m[20221213 12:48:44 @agent_ppo2.py:179][0m |          -0.0122 |          34.5876 |          -7.1899 |
[32m[20221213 12:48:44 @agent_ppo2.py:179][0m |          -0.0214 |          34.3224 |          -7.8896 |
[32m[20221213 12:48:44 @agent_ppo2.py:179][0m |          -0.0284 |          34.1633 |          -7.9357 |
[32m[20221213 12:48:44 @agent_ppo2.py:179][0m |          -0.0348 |          34.0028 |          -8.8505 |
[32m[20221213 12:48:44 @agent_ppo2.py:179][0m |          -0.0382 |          34.1466 |          -9.3563 |
[32m[20221213 12:48:44 @agent_ppo2.py:179][0m |          -0.0350 |          33.8496 |          -9.4807 |
[32m[20221213 12:48:44 @agent_ppo2.py:179][0m |          -0.0392 |          33.7354 |         -10.0263 |
[32m[20221213 12:48:44 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:48:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 349.21
[32m[20221213 12:48:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.67
[32m[20221213 12:48:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.89
[32m[20221213 12:48:44 @agent_ppo2.py:137][0m Total time:      32.80 min
[32m[20221213 12:48:44 @agent_ppo2.py:139][0m 2248704 total steps have happened
[32m[20221213 12:48:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1098 --------------------------#
[32m[20221213 12:48:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:45 @agent_ppo2.py:179][0m |           0.0763 |          34.2827 |          -7.3340 |
[32m[20221213 12:48:45 @agent_ppo2.py:179][0m |           0.0543 |          33.1466 |          -2.2342 |
[32m[20221213 12:48:45 @agent_ppo2.py:179][0m |           0.0070 |          32.5252 |          -5.6567 |
[32m[20221213 12:48:45 @agent_ppo2.py:179][0m |          -0.0047 |          32.0608 |          -6.8558 |
[32m[20221213 12:48:45 @agent_ppo2.py:179][0m |          -0.0159 |          31.6877 |          -7.4367 |
[32m[20221213 12:48:46 @agent_ppo2.py:179][0m |          -0.0214 |          31.2630 |          -7.9529 |
[32m[20221213 12:48:46 @agent_ppo2.py:179][0m |          -0.0307 |          30.9421 |          -8.4228 |
[32m[20221213 12:48:46 @agent_ppo2.py:179][0m |          -0.0305 |          30.9254 |          -9.0819 |
[32m[20221213 12:48:46 @agent_ppo2.py:179][0m |          -0.0254 |          30.5307 |          -9.0463 |
[32m[20221213 12:48:46 @agent_ppo2.py:179][0m |          -0.0308 |          30.9915 |          -9.5337 |
[32m[20221213 12:48:46 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:48:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 353.88
[32m[20221213 12:48:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 377.39
[32m[20221213 12:48:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.93
[32m[20221213 12:48:46 @agent_ppo2.py:137][0m Total time:      32.82 min
[32m[20221213 12:48:46 @agent_ppo2.py:139][0m 2250752 total steps have happened
[32m[20221213 12:48:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1099 --------------------------#
[32m[20221213 12:48:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |           0.0815 |          36.5337 |          -7.0793 |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |           0.0591 |          34.9644 |          -4.7420 |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |           0.0265 |          34.3132 |          -5.7756 |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |          -0.0046 |          33.8457 |          -7.0428 |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |          -0.0192 |          33.5813 |          -7.6752 |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |          -0.0280 |          33.2720 |          -7.6246 |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |          -0.0297 |          33.0447 |          -8.2309 |
[32m[20221213 12:48:47 @agent_ppo2.py:179][0m |          -0.0337 |          33.0220 |          -8.3962 |
[32m[20221213 12:48:48 @agent_ppo2.py:179][0m |          -0.0398 |          32.8364 |          -8.8619 |
[32m[20221213 12:48:48 @agent_ppo2.py:179][0m |          -0.0309 |          32.6593 |          -8.2702 |
[32m[20221213 12:48:48 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.20
[32m[20221213 12:48:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.77
[32m[20221213 12:48:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.76
[32m[20221213 12:48:48 @agent_ppo2.py:137][0m Total time:      32.85 min
[32m[20221213 12:48:48 @agent_ppo2.py:139][0m 2252800 total steps have happened
[32m[20221213 12:48:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1100 --------------------------#
[32m[20221213 12:48:48 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:48:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:48 @agent_ppo2.py:179][0m |           0.0913 |          36.2706 |          -5.2882 |
[32m[20221213 12:48:48 @agent_ppo2.py:179][0m |           0.0538 |          32.1985 |          -1.6149 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |           0.0043 |          31.3035 |          -3.4826 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |          -0.0142 |          30.7642 |          -4.4987 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |          -0.0193 |          30.3251 |          -5.4302 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |          -0.0321 |          29.9450 |          -6.3908 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |          -0.0328 |          29.7203 |          -6.8798 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |          -0.0336 |          29.3741 |          -7.1225 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |          -0.0342 |          29.4565 |          -7.9072 |
[32m[20221213 12:48:49 @agent_ppo2.py:179][0m |          -0.0416 |          28.9997 |          -8.4254 |
[32m[20221213 12:48:49 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:48:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.39
[32m[20221213 12:48:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.61
[32m[20221213 12:48:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.00
[32m[20221213 12:48:50 @agent_ppo2.py:137][0m Total time:      32.88 min
[32m[20221213 12:48:50 @agent_ppo2.py:139][0m 2254848 total steps have happened
[32m[20221213 12:48:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1101 --------------------------#
[32m[20221213 12:48:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:50 @agent_ppo2.py:179][0m |           0.0362 |          35.5789 |          -4.3640 |
[32m[20221213 12:48:50 @agent_ppo2.py:179][0m |           0.0177 |          35.2586 |          -4.3635 |
[32m[20221213 12:48:50 @agent_ppo2.py:179][0m |          -0.0059 |          34.2449 |          -5.1839 |
[32m[20221213 12:48:50 @agent_ppo2.py:179][0m |          -0.0248 |          33.2254 |          -5.5857 |
[32m[20221213 12:48:51 @agent_ppo2.py:179][0m |          -0.0218 |          32.8988 |          -5.7807 |
[32m[20221213 12:48:51 @agent_ppo2.py:179][0m |          -0.0301 |          32.5863 |          -6.2324 |
[32m[20221213 12:48:51 @agent_ppo2.py:179][0m |          -0.0336 |          32.4009 |          -6.8914 |
[32m[20221213 12:48:51 @agent_ppo2.py:179][0m |          -0.0338 |          32.4675 |          -7.4118 |
[32m[20221213 12:48:51 @agent_ppo2.py:179][0m |          -0.0379 |          32.0587 |          -7.6465 |
[32m[20221213 12:48:51 @agent_ppo2.py:179][0m |          -0.0407 |          31.7384 |          -8.1592 |
[32m[20221213 12:48:51 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.99
[32m[20221213 12:48:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.20
[32m[20221213 12:48:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.65
[32m[20221213 12:48:51 @agent_ppo2.py:137][0m Total time:      32.91 min
[32m[20221213 12:48:51 @agent_ppo2.py:139][0m 2256896 total steps have happened
[32m[20221213 12:48:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1102 --------------------------#
[32m[20221213 12:48:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:52 @agent_ppo2.py:179][0m |           0.0565 |          36.3559 |          -6.5459 |
[32m[20221213 12:48:52 @agent_ppo2.py:179][0m |           0.0233 |          34.4237 |          -3.7203 |
[32m[20221213 12:48:52 @agent_ppo2.py:179][0m |          -0.0019 |          34.1104 |          -5.2592 |
[32m[20221213 12:48:52 @agent_ppo2.py:179][0m |          -0.0096 |          33.7780 |          -6.1910 |
[32m[20221213 12:48:52 @agent_ppo2.py:179][0m |          -0.0231 |          34.0336 |          -6.7346 |
[32m[20221213 12:48:52 @agent_ppo2.py:179][0m |          -0.0300 |          33.5057 |          -7.3625 |
[32m[20221213 12:48:52 @agent_ppo2.py:179][0m |          -0.0241 |          36.4296 |          -7.6920 |
[32m[20221213 12:48:53 @agent_ppo2.py:179][0m |          -0.0257 |          35.5132 |          -8.1661 |
[32m[20221213 12:48:53 @agent_ppo2.py:179][0m |          -0.0409 |          33.2711 |          -8.7756 |
[32m[20221213 12:48:53 @agent_ppo2.py:179][0m |          -0.0387 |          33.2065 |          -9.0473 |
[32m[20221213 12:48:53 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:48:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.15
[32m[20221213 12:48:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 349.24
[32m[20221213 12:48:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.24
[32m[20221213 12:48:53 @agent_ppo2.py:137][0m Total time:      32.94 min
[32m[20221213 12:48:53 @agent_ppo2.py:139][0m 2258944 total steps have happened
[32m[20221213 12:48:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1103 --------------------------#
[32m[20221213 12:48:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:53 @agent_ppo2.py:179][0m |           0.0984 |          34.7599 |          -4.9145 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |           0.0340 |          33.6224 |          -5.1071 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |           0.0271 |          33.0017 |          -5.8579 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |          -0.0030 |          32.7755 |          -7.1548 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |          -0.0210 |          32.4310 |          -8.4991 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |          -0.0299 |          32.1815 |          -9.0286 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |          -0.0218 |          35.7983 |          -9.4980 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |          -0.0217 |          31.9243 |          -9.1151 |
[32m[20221213 12:48:54 @agent_ppo2.py:179][0m |          -0.0367 |          31.7557 |          -9.4701 |
[32m[20221213 12:48:55 @agent_ppo2.py:179][0m |          -0.0383 |          31.5228 |          -9.9743 |
[32m[20221213 12:48:55 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:48:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 337.21
[32m[20221213 12:48:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.96
[32m[20221213 12:48:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.75
[32m[20221213 12:48:55 @agent_ppo2.py:137][0m Total time:      32.97 min
[32m[20221213 12:48:55 @agent_ppo2.py:139][0m 2260992 total steps have happened
[32m[20221213 12:48:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1104 --------------------------#
[32m[20221213 12:48:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:55 @agent_ppo2.py:179][0m |           0.0170 |          21.5318 |          -2.3967 |
[32m[20221213 12:48:55 @agent_ppo2.py:179][0m |          -0.0281 |          20.0795 |          -3.2406 |
[32m[20221213 12:48:55 @agent_ppo2.py:179][0m |          -0.0389 |          19.6948 |          -3.7837 |
[32m[20221213 12:48:56 @agent_ppo2.py:179][0m |          -0.0363 |          19.5415 |          -4.3812 |
[32m[20221213 12:48:56 @agent_ppo2.py:179][0m |          -0.0433 |          19.1460 |          -4.9800 |
[32m[20221213 12:48:56 @agent_ppo2.py:179][0m |          -0.0523 |          18.9792 |          -5.2646 |
[32m[20221213 12:48:56 @agent_ppo2.py:179][0m |          -0.0599 |          18.8189 |          -5.8518 |
[32m[20221213 12:48:56 @agent_ppo2.py:179][0m |          -0.0565 |          18.7408 |          -6.1412 |
[32m[20221213 12:48:56 @agent_ppo2.py:179][0m |          -0.0547 |          18.4652 |          -6.4299 |
[32m[20221213 12:48:56 @agent_ppo2.py:179][0m |          -0.0569 |          18.4274 |          -6.5223 |
[32m[20221213 12:48:56 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:48:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 98.09
[32m[20221213 12:48:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.68
[32m[20221213 12:48:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.98
[32m[20221213 12:48:56 @agent_ppo2.py:137][0m Total time:      32.99 min
[32m[20221213 12:48:56 @agent_ppo2.py:139][0m 2263040 total steps have happened
[32m[20221213 12:48:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1105 --------------------------#
[32m[20221213 12:48:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:57 @agent_ppo2.py:179][0m |           0.2813 |          19.0783 |          -4.9682 |
[32m[20221213 12:48:57 @agent_ppo2.py:179][0m |          -0.0090 |          17.4555 |          -3.2546 |
[32m[20221213 12:48:57 @agent_ppo2.py:179][0m |          -0.0190 |          18.0870 |          -4.4317 |
[32m[20221213 12:48:57 @agent_ppo2.py:179][0m |          -0.0357 |          16.8689 |          -5.3853 |
[32m[20221213 12:48:57 @agent_ppo2.py:179][0m |          -0.0366 |          18.2505 |          -5.6665 |
[32m[20221213 12:48:57 @agent_ppo2.py:179][0m |          -0.0434 |          16.6380 |          -6.1506 |
[32m[20221213 12:48:58 @agent_ppo2.py:179][0m |          -0.0463 |          16.4873 |          -6.6674 |
[32m[20221213 12:48:58 @agent_ppo2.py:179][0m |          -0.0524 |          16.4370 |          -6.9592 |
[32m[20221213 12:48:58 @agent_ppo2.py:179][0m |          -0.0543 |          16.4271 |          -7.5301 |
[32m[20221213 12:48:58 @agent_ppo2.py:179][0m |          -0.0378 |          19.5646 |          -8.1569 |
[32m[20221213 12:48:58 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:48:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.89
[32m[20221213 12:48:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.16
[32m[20221213 12:48:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.91
[32m[20221213 12:48:58 @agent_ppo2.py:137][0m Total time:      33.02 min
[32m[20221213 12:48:58 @agent_ppo2.py:139][0m 2265088 total steps have happened
[32m[20221213 12:48:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1106 --------------------------#
[32m[20221213 12:48:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:48:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |           0.0867 |          33.7666 |         -18.0881 |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |           0.0503 |          31.6576 |         -13.3968 |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |           0.0305 |          35.0982 |         -15.4121 |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |          -0.0091 |          31.0421 |         -17.2079 |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |          -0.0044 |          30.3611 |         -16.6814 |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |          -0.0250 |          30.1345 |         -18.2791 |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |          -0.0297 |          29.8804 |         -19.8212 |
[32m[20221213 12:48:59 @agent_ppo2.py:179][0m |          -0.0233 |          32.3576 |         -20.5583 |
[32m[20221213 12:49:00 @agent_ppo2.py:179][0m |          -0.0114 |          33.6322 |         -19.0062 |
[32m[20221213 12:49:00 @agent_ppo2.py:179][0m |          -0.0377 |          29.4218 |         -21.6686 |
[32m[20221213 12:49:00 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:49:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.85
[32m[20221213 12:49:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.34
[32m[20221213 12:49:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 38.28
[32m[20221213 12:49:00 @agent_ppo2.py:137][0m Total time:      33.05 min
[32m[20221213 12:49:00 @agent_ppo2.py:139][0m 2267136 total steps have happened
[32m[20221213 12:49:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1107 --------------------------#
[32m[20221213 12:49:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:00 @agent_ppo2.py:179][0m |           0.0588 |          21.3829 |          -8.1371 |
[32m[20221213 12:49:00 @agent_ppo2.py:179][0m |          -0.0053 |          19.3707 |          -6.9364 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0241 |          18.3519 |          -8.2419 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0330 |          17.8399 |          -9.0068 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0440 |          17.5943 |         -10.3092 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0499 |          17.2527 |         -11.1045 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0501 |          16.9414 |         -11.6782 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0506 |          16.6864 |         -12.4998 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0595 |          16.5723 |         -12.5084 |
[32m[20221213 12:49:01 @agent_ppo2.py:179][0m |          -0.0602 |          16.3042 |         -12.7481 |
[32m[20221213 12:49:01 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:49:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 150.30
[32m[20221213 12:49:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 325.36
[32m[20221213 12:49:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.93
[32m[20221213 12:49:02 @agent_ppo2.py:137][0m Total time:      33.08 min
[32m[20221213 12:49:02 @agent_ppo2.py:139][0m 2269184 total steps have happened
[32m[20221213 12:49:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1108 --------------------------#
[32m[20221213 12:49:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:02 @agent_ppo2.py:179][0m |           0.1000 |          36.3228 |         -20.1676 |
[32m[20221213 12:49:02 @agent_ppo2.py:179][0m |           0.0947 |          34.9723 |          -9.4826 |
[32m[20221213 12:49:02 @agent_ppo2.py:179][0m |           0.0496 |          35.9470 |         -14.7433 |
[32m[20221213 12:49:02 @agent_ppo2.py:179][0m |           0.0080 |          34.7049 |         -19.4774 |
[32m[20221213 12:49:02 @agent_ppo2.py:179][0m |          -0.0125 |          34.8476 |         -20.8731 |
[32m[20221213 12:49:03 @agent_ppo2.py:179][0m |          -0.0174 |          35.6268 |         -22.7828 |
[32m[20221213 12:49:03 @agent_ppo2.py:179][0m |          -0.0292 |          33.1457 |         -23.6131 |
[32m[20221213 12:49:03 @agent_ppo2.py:179][0m |          -0.0315 |          32.8794 |         -24.2041 |
[32m[20221213 12:49:03 @agent_ppo2.py:179][0m |          -0.0371 |          32.6588 |         -25.1768 |
[32m[20221213 12:49:03 @agent_ppo2.py:179][0m |          -0.0395 |          32.4458 |         -26.3923 |
[32m[20221213 12:49:03 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:49:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.75
[32m[20221213 12:49:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.01
[32m[20221213 12:49:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.35
[32m[20221213 12:49:03 @agent_ppo2.py:137][0m Total time:      33.11 min
[32m[20221213 12:49:03 @agent_ppo2.py:139][0m 2271232 total steps have happened
[32m[20221213 12:49:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1109 --------------------------#
[32m[20221213 12:49:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:04 @agent_ppo2.py:179][0m |           0.0515 |          36.6192 |         -22.1229 |
[32m[20221213 12:49:04 @agent_ppo2.py:179][0m |           0.0341 |          35.3046 |         -20.1566 |
[32m[20221213 12:49:04 @agent_ppo2.py:179][0m |           0.0115 |          35.6596 |         -20.4572 |
[32m[20221213 12:49:04 @agent_ppo2.py:179][0m |          -0.0035 |          34.8528 |         -21.2921 |
[32m[20221213 12:49:04 @agent_ppo2.py:179][0m |          -0.0193 |          34.2256 |         -23.1851 |
[32m[20221213 12:49:04 @agent_ppo2.py:179][0m |          -0.0223 |          33.9926 |         -23.8063 |
[32m[20221213 12:49:04 @agent_ppo2.py:179][0m |          -0.0276 |          33.8578 |         -24.9149 |
[32m[20221213 12:49:05 @agent_ppo2.py:179][0m |          -0.0360 |          33.6916 |         -25.4152 |
[32m[20221213 12:49:05 @agent_ppo2.py:179][0m |          -0.0257 |          36.5115 |         -25.3571 |
[32m[20221213 12:49:05 @agent_ppo2.py:179][0m |          -0.0347 |          33.8166 |         -25.8620 |
[32m[20221213 12:49:05 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:49:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 355.97
[32m[20221213 12:49:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 370.62
[32m[20221213 12:49:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 39.11
[32m[20221213 12:49:05 @agent_ppo2.py:137][0m Total time:      33.14 min
[32m[20221213 12:49:05 @agent_ppo2.py:139][0m 2273280 total steps have happened
[32m[20221213 12:49:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1110 --------------------------#
[32m[20221213 12:49:05 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:49:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:05 @agent_ppo2.py:179][0m |           0.0776 |          34.9960 |         -16.2104 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |           0.1102 |          35.5125 |         -13.4565 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |           0.0109 |          33.4375 |         -17.5149 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |          -0.0087 |          32.9558 |         -19.6159 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |          -0.0110 |          32.6073 |         -19.7142 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |          -0.0234 |          32.2912 |         -21.3948 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |          -0.0309 |          32.0115 |         -22.6799 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |          -0.0319 |          31.8597 |         -23.1024 |
[32m[20221213 12:49:06 @agent_ppo2.py:179][0m |          -0.0345 |          31.6864 |         -23.9430 |
[32m[20221213 12:49:07 @agent_ppo2.py:179][0m |          -0.0327 |          31.5827 |         -24.2888 |
[32m[20221213 12:49:07 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:49:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.18
[32m[20221213 12:49:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.52
[32m[20221213 12:49:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.00
[32m[20221213 12:49:07 @agent_ppo2.py:137][0m Total time:      33.17 min
[32m[20221213 12:49:07 @agent_ppo2.py:139][0m 2275328 total steps have happened
[32m[20221213 12:49:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1111 --------------------------#
[32m[20221213 12:49:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:07 @agent_ppo2.py:179][0m |           0.0832 |          40.0720 |         -18.6129 |
[32m[20221213 12:49:07 @agent_ppo2.py:179][0m |           0.0251 |          35.0446 |         -17.7698 |
[32m[20221213 12:49:07 @agent_ppo2.py:179][0m |           0.0011 |          34.2242 |         -21.0242 |
[32m[20221213 12:49:08 @agent_ppo2.py:179][0m |          -0.0042 |          34.4718 |         -22.9995 |
[32m[20221213 12:49:08 @agent_ppo2.py:179][0m |          -0.0151 |          33.3470 |         -22.5354 |
[32m[20221213 12:49:08 @agent_ppo2.py:179][0m |          -0.0193 |          33.7557 |         -25.2021 |
[32m[20221213 12:49:08 @agent_ppo2.py:179][0m |          -0.0316 |          32.9865 |         -25.8265 |
[32m[20221213 12:49:08 @agent_ppo2.py:179][0m |          -0.0366 |          32.7910 |         -26.4925 |
[32m[20221213 12:49:08 @agent_ppo2.py:179][0m |          -0.0322 |          32.7569 |         -27.0172 |
[32m[20221213 12:49:08 @agent_ppo2.py:179][0m |          -0.0378 |          32.4539 |         -27.8220 |
[32m[20221213 12:49:08 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:49:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.96
[32m[20221213 12:49:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.87
[32m[20221213 12:49:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.86
[32m[20221213 12:49:09 @agent_ppo2.py:137][0m Total time:      33.20 min
[32m[20221213 12:49:09 @agent_ppo2.py:139][0m 2277376 total steps have happened
[32m[20221213 12:49:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1112 --------------------------#
[32m[20221213 12:49:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:09 @agent_ppo2.py:179][0m |           0.1107 |          38.6566 |         -24.1000 |
[32m[20221213 12:49:09 @agent_ppo2.py:179][0m |           0.0695 |          34.1833 |         -13.7786 |
[32m[20221213 12:49:09 @agent_ppo2.py:179][0m |           0.0178 |          33.5806 |         -16.2326 |
[32m[20221213 12:49:09 @agent_ppo2.py:179][0m |           0.0043 |          33.9093 |         -18.8650 |
[32m[20221213 12:49:10 @agent_ppo2.py:179][0m |          -0.0133 |          32.9452 |         -21.0874 |
[32m[20221213 12:49:10 @agent_ppo2.py:179][0m |          -0.0205 |          32.6697 |         -21.1994 |
[32m[20221213 12:49:10 @agent_ppo2.py:179][0m |          -0.0312 |          32.3296 |         -23.2597 |
[32m[20221213 12:49:10 @agent_ppo2.py:179][0m |          -0.0318 |          32.3487 |         -24.8116 |
[32m[20221213 12:49:10 @agent_ppo2.py:179][0m |          -0.0318 |          31.9586 |         -24.8908 |
[32m[20221213 12:49:10 @agent_ppo2.py:179][0m |          -0.0374 |          31.7917 |         -25.8467 |
[32m[20221213 12:49:10 @agent_ppo2.py:124][0m Policy update time: 1.44 s
[32m[20221213 12:49:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.34
[32m[20221213 12:49:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.92
[32m[20221213 12:49:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.53
[32m[20221213 12:49:10 @agent_ppo2.py:137][0m Total time:      33.23 min
[32m[20221213 12:49:10 @agent_ppo2.py:139][0m 2279424 total steps have happened
[32m[20221213 12:49:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1113 --------------------------#
[32m[20221213 12:49:11 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:49:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:11 @agent_ppo2.py:179][0m |           0.0715 |          35.7918 |         -17.6140 |
[32m[20221213 12:49:11 @agent_ppo2.py:179][0m |           0.0941 |          34.5385 |          -8.5519 |
[32m[20221213 12:49:11 @agent_ppo2.py:179][0m |           0.0361 |          34.0590 |         -13.4214 |
[32m[20221213 12:49:11 @agent_ppo2.py:179][0m |           0.0203 |          33.5200 |         -14.4183 |
[32m[20221213 12:49:11 @agent_ppo2.py:179][0m |          -0.0022 |          33.2711 |         -16.7200 |
[32m[20221213 12:49:11 @agent_ppo2.py:179][0m |          -0.0100 |          33.3064 |         -18.8706 |
[32m[20221213 12:49:12 @agent_ppo2.py:179][0m |          -0.0212 |          33.0445 |         -19.9433 |
[32m[20221213 12:49:12 @agent_ppo2.py:179][0m |          -0.0302 |          32.6245 |         -20.4997 |
[32m[20221213 12:49:12 @agent_ppo2.py:179][0m |          -0.0220 |          32.6002 |         -19.8786 |
[32m[20221213 12:49:12 @agent_ppo2.py:179][0m |          -0.0280 |          32.5492 |         -20.6304 |
[32m[20221213 12:49:12 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:49:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.52
[32m[20221213 12:49:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 375.13
[32m[20221213 12:49:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.07
[32m[20221213 12:49:12 @agent_ppo2.py:137][0m Total time:      33.26 min
[32m[20221213 12:49:12 @agent_ppo2.py:139][0m 2281472 total steps have happened
[32m[20221213 12:49:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1114 --------------------------#
[32m[20221213 12:49:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:13 @agent_ppo2.py:179][0m |           0.0750 |          38.7819 |         -16.6946 |
[32m[20221213 12:49:13 @agent_ppo2.py:179][0m |           0.0763 |          37.1310 |          -8.3574 |
[32m[20221213 12:49:13 @agent_ppo2.py:179][0m |           0.0310 |          34.3319 |         -11.7105 |
[32m[20221213 12:49:13 @agent_ppo2.py:179][0m |           0.0014 |          34.2355 |         -14.7470 |
[32m[20221213 12:49:13 @agent_ppo2.py:179][0m |          -0.0090 |          33.9623 |         -15.2179 |
[32m[20221213 12:49:13 @agent_ppo2.py:179][0m |          -0.0199 |          33.8130 |         -17.0777 |
[32m[20221213 12:49:13 @agent_ppo2.py:179][0m |          -0.0213 |          34.2205 |         -18.1188 |
[32m[20221213 12:49:14 @agent_ppo2.py:179][0m |          -0.0141 |          37.9516 |         -17.4317 |
[32m[20221213 12:49:14 @agent_ppo2.py:179][0m |          -0.0316 |          33.6537 |         -18.4915 |
[32m[20221213 12:49:14 @agent_ppo2.py:179][0m |          -0.0382 |          33.4269 |         -19.5535 |
[32m[20221213 12:49:14 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:49:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.58
[32m[20221213 12:49:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.49
[32m[20221213 12:49:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.16
[32m[20221213 12:49:14 @agent_ppo2.py:137][0m Total time:      33.29 min
[32m[20221213 12:49:14 @agent_ppo2.py:139][0m 2283520 total steps have happened
[32m[20221213 12:49:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1115 --------------------------#
[32m[20221213 12:49:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:14 @agent_ppo2.py:179][0m |           0.0257 |          17.5810 |          -5.1146 |
[32m[20221213 12:49:14 @agent_ppo2.py:179][0m |          -0.0110 |          16.8781 |          -3.5818 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0302 |          16.7457 |          -4.1261 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0428 |          16.6512 |          -4.7852 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0469 |          16.5726 |          -4.9558 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0473 |          16.5450 |          -5.2820 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0564 |          16.4930 |          -5.7343 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0502 |          16.5026 |          -5.9144 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0519 |          16.4240 |          -6.1919 |
[32m[20221213 12:49:15 @agent_ppo2.py:179][0m |          -0.0575 |          16.3855 |          -6.6888 |
[32m[20221213 12:49:15 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:49:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 101.20
[32m[20221213 12:49:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.15
[32m[20221213 12:49:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 387.86
[32m[20221213 12:49:16 @agent_ppo2.py:137][0m Total time:      33.31 min
[32m[20221213 12:49:16 @agent_ppo2.py:139][0m 2285568 total steps have happened
[32m[20221213 12:49:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1116 --------------------------#
[32m[20221213 12:49:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:16 @agent_ppo2.py:179][0m |           0.0844 |          34.7428 |         -17.7136 |
[32m[20221213 12:49:16 @agent_ppo2.py:179][0m |           0.0449 |          33.1914 |         -10.9940 |
[32m[20221213 12:49:16 @agent_ppo2.py:179][0m |           0.0166 |          32.3704 |         -13.1078 |
[32m[20221213 12:49:16 @agent_ppo2.py:179][0m |          -0.0065 |          31.8640 |         -17.2227 |
[32m[20221213 12:49:17 @agent_ppo2.py:179][0m |          -0.0072 |          33.9667 |         -17.6252 |
[32m[20221213 12:49:17 @agent_ppo2.py:179][0m |          -0.0236 |          31.2754 |         -17.8565 |
[32m[20221213 12:49:17 @agent_ppo2.py:179][0m |          -0.0209 |          33.3508 |         -18.9426 |
[32m[20221213 12:49:17 @agent_ppo2.py:179][0m |          -0.0340 |          30.8415 |         -19.5290 |
[32m[20221213 12:49:17 @agent_ppo2.py:179][0m |          -0.0272 |          32.3253 |         -19.9810 |
[32m[20221213 12:49:17 @agent_ppo2.py:179][0m |          -0.0298 |          32.6018 |         -21.0093 |
[32m[20221213 12:49:17 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 341.32
[32m[20221213 12:49:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.74
[32m[20221213 12:49:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.09
[32m[20221213 12:49:17 @agent_ppo2.py:137][0m Total time:      33.34 min
[32m[20221213 12:49:17 @agent_ppo2.py:139][0m 2287616 total steps have happened
[32m[20221213 12:49:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1117 --------------------------#
[32m[20221213 12:49:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:18 @agent_ppo2.py:179][0m |           0.0503 |          26.8836 |         -10.4633 |
[32m[20221213 12:49:18 @agent_ppo2.py:179][0m |           0.0148 |          25.8431 |          -9.6388 |
[32m[20221213 12:49:18 @agent_ppo2.py:179][0m |          -0.0292 |          25.3137 |         -12.6579 |
[32m[20221213 12:49:18 @agent_ppo2.py:179][0m |          -0.0432 |          25.0777 |         -14.0363 |
[32m[20221213 12:49:18 @agent_ppo2.py:179][0m |          -0.0522 |          24.9431 |         -14.5873 |
[32m[20221213 12:49:18 @agent_ppo2.py:179][0m |          -0.0514 |          24.8290 |         -15.1576 |
[32m[20221213 12:49:18 @agent_ppo2.py:179][0m |          -0.0532 |          25.5172 |         -16.2356 |
[32m[20221213 12:49:19 @agent_ppo2.py:179][0m |          -0.0541 |          24.8156 |         -15.9176 |
[32m[20221213 12:49:19 @agent_ppo2.py:179][0m |          -0.0669 |          24.5817 |         -17.6674 |
[32m[20221213 12:49:19 @agent_ppo2.py:179][0m |          -0.0632 |          24.4249 |         -17.7105 |
[32m[20221213 12:49:19 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:49:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 224.60
[32m[20221213 12:49:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.67
[32m[20221213 12:49:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.36
[32m[20221213 12:49:19 @agent_ppo2.py:137][0m Total time:      33.37 min
[32m[20221213 12:49:19 @agent_ppo2.py:139][0m 2289664 total steps have happened
[32m[20221213 12:49:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1118 --------------------------#
[32m[20221213 12:49:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:19 @agent_ppo2.py:179][0m |           0.0778 |          35.2520 |         -20.4368 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |           0.0569 |          34.1165 |         -15.6271 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |           0.0196 |          33.5334 |         -18.4214 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |          -0.0017 |          33.2108 |         -21.1345 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |          -0.0184 |          33.0547 |         -22.4212 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |          -0.0184 |          33.2040 |         -22.5892 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |          -0.0298 |          32.9540 |         -23.5699 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |          -0.0310 |          32.6279 |         -24.0421 |
[32m[20221213 12:49:20 @agent_ppo2.py:179][0m |          -0.0291 |          32.9248 |         -24.1895 |
[32m[20221213 12:49:21 @agent_ppo2.py:179][0m |          -0.0280 |          33.5489 |         -25.1698 |
[32m[20221213 12:49:21 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:49:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.01
[32m[20221213 12:49:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.67
[32m[20221213 12:49:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 372.99
[32m[20221213 12:49:21 @agent_ppo2.py:137][0m Total time:      33.40 min
[32m[20221213 12:49:21 @agent_ppo2.py:139][0m 2291712 total steps have happened
[32m[20221213 12:49:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1119 --------------------------#
[32m[20221213 12:49:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:21 @agent_ppo2.py:179][0m |           0.0487 |          34.3311 |         -23.3743 |
[32m[20221213 12:49:21 @agent_ppo2.py:179][0m |           0.0237 |          32.4442 |         -20.8489 |
[32m[20221213 12:49:21 @agent_ppo2.py:179][0m |           0.0029 |          31.4076 |         -21.9667 |
[32m[20221213 12:49:22 @agent_ppo2.py:179][0m |          -0.0166 |          30.7875 |         -23.7853 |
[32m[20221213 12:49:22 @agent_ppo2.py:179][0m |          -0.0268 |          30.6675 |         -24.9456 |
[32m[20221213 12:49:22 @agent_ppo2.py:179][0m |          -0.0334 |          29.9826 |         -25.8830 |
[32m[20221213 12:49:22 @agent_ppo2.py:179][0m |          -0.0400 |          29.6437 |         -27.4586 |
[32m[20221213 12:49:22 @agent_ppo2.py:179][0m |          -0.0405 |          29.3534 |         -28.0615 |
[32m[20221213 12:49:22 @agent_ppo2.py:179][0m |          -0.0418 |          29.0429 |         -28.6746 |
[32m[20221213 12:49:22 @agent_ppo2.py:179][0m |          -0.0441 |          28.8609 |         -29.3366 |
[32m[20221213 12:49:22 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 318.69
[32m[20221213 12:49:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.83
[32m[20221213 12:49:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 317.36
[32m[20221213 12:49:22 @agent_ppo2.py:137][0m Total time:      33.43 min
[32m[20221213 12:49:22 @agent_ppo2.py:139][0m 2293760 total steps have happened
[32m[20221213 12:49:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1120 --------------------------#
[32m[20221213 12:49:23 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:49:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:23 @agent_ppo2.py:179][0m |           0.0539 |          35.4680 |         -21.1651 |
[32m[20221213 12:49:23 @agent_ppo2.py:179][0m |           0.0795 |          37.3453 |         -15.9768 |
[32m[20221213 12:49:23 @agent_ppo2.py:179][0m |           0.0227 |          34.1076 |         -16.1806 |
[32m[20221213 12:49:23 @agent_ppo2.py:179][0m |          -0.0053 |          33.9559 |         -20.4719 |
[32m[20221213 12:49:23 @agent_ppo2.py:179][0m |          -0.0178 |          33.7733 |         -21.8734 |
[32m[20221213 12:49:24 @agent_ppo2.py:179][0m |          -0.0302 |          33.7061 |         -22.4849 |
[32m[20221213 12:49:24 @agent_ppo2.py:179][0m |          -0.0293 |          33.6360 |         -23.2764 |
[32m[20221213 12:49:24 @agent_ppo2.py:179][0m |          -0.0349 |          33.5255 |         -23.9926 |
[32m[20221213 12:49:24 @agent_ppo2.py:179][0m |          -0.0340 |          33.4511 |         -24.3758 |
[32m[20221213 12:49:24 @agent_ppo2.py:179][0m |          -0.0346 |          33.3956 |         -24.3374 |
[32m[20221213 12:49:24 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 339.67
[32m[20221213 12:49:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.09
[32m[20221213 12:49:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.93
[32m[20221213 12:49:24 @agent_ppo2.py:137][0m Total time:      33.46 min
[32m[20221213 12:49:24 @agent_ppo2.py:139][0m 2295808 total steps have happened
[32m[20221213 12:49:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1121 --------------------------#
[32m[20221213 12:49:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |           0.0335 |          21.6458 |         -10.9893 |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |          -0.0111 |          20.5090 |          -9.6541 |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |          -0.0060 |          20.2055 |         -10.7692 |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |          -0.0297 |          20.0321 |         -11.5823 |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |          -0.0442 |          19.8512 |         -12.5110 |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |          -0.0554 |          19.7723 |         -13.5422 |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |          -0.0618 |          19.6512 |         -14.7608 |
[32m[20221213 12:49:25 @agent_ppo2.py:179][0m |          -0.0579 |          19.5183 |         -14.8386 |
[32m[20221213 12:49:26 @agent_ppo2.py:179][0m |          -0.0691 |          19.3870 |         -15.3095 |
[32m[20221213 12:49:26 @agent_ppo2.py:179][0m |          -0.0668 |          19.3268 |         -16.3432 |
[32m[20221213 12:49:26 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 143.51
[32m[20221213 12:49:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.92
[32m[20221213 12:49:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.87
[32m[20221213 12:49:26 @agent_ppo2.py:137][0m Total time:      33.49 min
[32m[20221213 12:49:26 @agent_ppo2.py:139][0m 2297856 total steps have happened
[32m[20221213 12:49:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1122 --------------------------#
[32m[20221213 12:49:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:26 @agent_ppo2.py:179][0m |           0.0816 |          36.7311 |         -27.2435 |
[32m[20221213 12:49:26 @agent_ppo2.py:179][0m |           0.1013 |          34.8817 |         -17.3640 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |           0.0449 |          34.0985 |         -18.8658 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |           0.0195 |          33.7686 |         -21.4010 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |          -0.0000 |          33.4816 |         -24.9313 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |          -0.0038 |          35.8600 |         -27.7779 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |          -0.0221 |          33.1724 |         -29.0269 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |          -0.0273 |          32.9593 |         -30.1668 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |          -0.0265 |          32.9077 |         -30.6852 |
[32m[20221213 12:49:27 @agent_ppo2.py:179][0m |          -0.0266 |          32.7741 |         -31.5429 |
[32m[20221213 12:49:27 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.57
[32m[20221213 12:49:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.42
[32m[20221213 12:49:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.85
[32m[20221213 12:49:28 @agent_ppo2.py:137][0m Total time:      33.51 min
[32m[20221213 12:49:28 @agent_ppo2.py:139][0m 2299904 total steps have happened
[32m[20221213 12:49:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1123 --------------------------#
[32m[20221213 12:49:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:28 @agent_ppo2.py:179][0m |           0.0461 |          35.3244 |         -26.8351 |
[32m[20221213 12:49:28 @agent_ppo2.py:179][0m |           0.0568 |          37.4203 |         -21.8473 |
[32m[20221213 12:49:28 @agent_ppo2.py:179][0m |           0.0098 |          34.0507 |         -22.7166 |
[32m[20221213 12:49:28 @agent_ppo2.py:179][0m |          -0.0037 |          33.7347 |         -23.2818 |
[32m[20221213 12:49:29 @agent_ppo2.py:179][0m |          -0.0112 |          34.5282 |         -26.1518 |
[32m[20221213 12:49:29 @agent_ppo2.py:179][0m |          -0.0174 |          33.3614 |         -26.4521 |
[32m[20221213 12:49:29 @agent_ppo2.py:179][0m |          -0.0186 |          33.5937 |         -27.9461 |
[32m[20221213 12:49:29 @agent_ppo2.py:179][0m |          -0.0255 |          34.1799 |         -29.4236 |
[32m[20221213 12:49:29 @agent_ppo2.py:179][0m |          -0.0363 |          33.0236 |         -30.1360 |
[32m[20221213 12:49:29 @agent_ppo2.py:179][0m |          -0.0352 |          32.9230 |         -29.8174 |
[32m[20221213 12:49:29 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:49:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.23
[32m[20221213 12:49:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.56
[32m[20221213 12:49:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 379.45
[32m[20221213 12:49:29 @agent_ppo2.py:137][0m Total time:      33.54 min
[32m[20221213 12:49:29 @agent_ppo2.py:139][0m 2301952 total steps have happened
[32m[20221213 12:49:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1124 --------------------------#
[32m[20221213 12:49:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:30 @agent_ppo2.py:179][0m |           0.0594 |          32.5751 |         -21.7396 |
[32m[20221213 12:49:30 @agent_ppo2.py:179][0m |           0.0268 |          29.8094 |         -17.3976 |
[32m[20221213 12:49:30 @agent_ppo2.py:179][0m |           0.0228 |          28.6502 |         -21.1666 |
[32m[20221213 12:49:30 @agent_ppo2.py:179][0m |           0.0097 |          29.2487 |         -22.4684 |
[32m[20221213 12:49:30 @agent_ppo2.py:179][0m |          -0.0146 |          27.5556 |         -24.4845 |
[32m[20221213 12:49:30 @agent_ppo2.py:179][0m |          -0.0179 |          27.2910 |         -25.3455 |
[32m[20221213 12:49:31 @agent_ppo2.py:179][0m |          -0.0244 |          27.0900 |         -26.2662 |
[32m[20221213 12:49:31 @agent_ppo2.py:179][0m |          -0.0298 |          26.9366 |         -27.5027 |
[32m[20221213 12:49:31 @agent_ppo2.py:179][0m |          -0.0351 |          26.7765 |         -28.3957 |
[32m[20221213 12:49:31 @agent_ppo2.py:179][0m |          -0.0243 |          30.8758 |         -28.5245 |
[32m[20221213 12:49:31 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:49:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 361.98
[32m[20221213 12:49:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 373.72
[32m[20221213 12:49:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.42
[32m[20221213 12:49:31 @agent_ppo2.py:137][0m Total time:      33.57 min
[32m[20221213 12:49:31 @agent_ppo2.py:139][0m 2304000 total steps have happened
[32m[20221213 12:49:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1125 --------------------------#
[32m[20221213 12:49:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |           0.0672 |          36.2791 |         -22.3885 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |           0.0545 |          34.7170 |         -15.1860 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |           0.0117 |          34.1740 |         -20.4060 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |          -0.0106 |          34.0204 |         -24.5289 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |          -0.0222 |          33.6676 |         -26.7027 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |          -0.0163 |          34.9922 |         -26.5884 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |          -0.0295 |          33.3989 |         -25.2470 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |          -0.0353 |          33.5420 |         -27.1759 |
[32m[20221213 12:49:32 @agent_ppo2.py:179][0m |          -0.0229 |          33.1567 |         -25.7140 |
[32m[20221213 12:49:33 @agent_ppo2.py:179][0m |          -0.0310 |          33.0357 |         -26.6517 |
[32m[20221213 12:49:33 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:49:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.09
[32m[20221213 12:49:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.93
[32m[20221213 12:49:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.10
[32m[20221213 12:49:33 @agent_ppo2.py:137][0m Total time:      33.60 min
[32m[20221213 12:49:33 @agent_ppo2.py:139][0m 2306048 total steps have happened
[32m[20221213 12:49:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1126 --------------------------#
[32m[20221213 12:49:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:33 @agent_ppo2.py:179][0m |           0.0467 |          31.0563 |         -26.6971 |
[32m[20221213 12:49:33 @agent_ppo2.py:179][0m |           0.0323 |          29.3582 |         -21.3154 |
[32m[20221213 12:49:33 @agent_ppo2.py:179][0m |          -0.0080 |          28.8987 |         -19.1278 |
[32m[20221213 12:49:34 @agent_ppo2.py:179][0m |          -0.0303 |          28.5235 |         -22.2158 |
[32m[20221213 12:49:34 @agent_ppo2.py:179][0m |          -0.0371 |          28.3997 |         -23.1693 |
[32m[20221213 12:49:34 @agent_ppo2.py:179][0m |          -0.0465 |          28.0969 |         -24.1062 |
[32m[20221213 12:49:34 @agent_ppo2.py:179][0m |          -0.0419 |          28.0936 |         -24.8352 |
[32m[20221213 12:49:34 @agent_ppo2.py:179][0m |          -0.0255 |          28.1398 |         -23.7055 |
[32m[20221213 12:49:34 @agent_ppo2.py:179][0m |          -0.0302 |          27.6908 |         -23.0139 |
[32m[20221213 12:49:34 @agent_ppo2.py:179][0m |          -0.0433 |          27.7360 |         -25.6459 |
[32m[20221213 12:49:34 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 287.57
[32m[20221213 12:49:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 371.44
[32m[20221213 12:49:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 378.92
[32m[20221213 12:49:35 @agent_ppo2.py:137][0m Total time:      33.63 min
[32m[20221213 12:49:35 @agent_ppo2.py:139][0m 2308096 total steps have happened
[32m[20221213 12:49:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1127 --------------------------#
[32m[20221213 12:49:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:35 @agent_ppo2.py:179][0m |           0.0716 |          37.3765 |         -21.5977 |
[32m[20221213 12:49:35 @agent_ppo2.py:179][0m |           0.0415 |          35.2642 |         -18.2940 |
[32m[20221213 12:49:35 @agent_ppo2.py:179][0m |           0.0086 |          35.1022 |         -21.8990 |
[32m[20221213 12:49:35 @agent_ppo2.py:179][0m |           0.0010 |          34.4798 |         -21.8820 |
[32m[20221213 12:49:35 @agent_ppo2.py:179][0m |          -0.0184 |          34.0821 |         -23.5256 |
[32m[20221213 12:49:36 @agent_ppo2.py:179][0m |          -0.0292 |          33.8953 |         -25.5526 |
[32m[20221213 12:49:36 @agent_ppo2.py:179][0m |          -0.0354 |          33.6920 |         -27.5074 |
[32m[20221213 12:49:36 @agent_ppo2.py:179][0m |          -0.0380 |          33.6842 |         -28.2946 |
[32m[20221213 12:49:36 @agent_ppo2.py:179][0m |          -0.0419 |          33.5086 |         -28.2683 |
[32m[20221213 12:49:36 @agent_ppo2.py:179][0m |          -0.0398 |          33.4151 |         -28.6208 |
[32m[20221213 12:49:36 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.25
[32m[20221213 12:49:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.14
[32m[20221213 12:49:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 376.62
[32m[20221213 12:49:36 @agent_ppo2.py:137][0m Total time:      33.66 min
[32m[20221213 12:49:36 @agent_ppo2.py:139][0m 2310144 total steps have happened
[32m[20221213 12:49:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1128 --------------------------#
[32m[20221213 12:49:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |           0.0844 |          27.2489 |         -15.3936 |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |           0.0535 |          25.9877 |         -10.2931 |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |           0.0008 |          25.4964 |         -14.0940 |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |          -0.0200 |          24.9587 |         -16.0833 |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |          -0.0342 |          24.6108 |         -17.1986 |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |          -0.0428 |          24.3683 |         -17.9914 |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |          -0.0488 |          24.1461 |         -18.7131 |
[32m[20221213 12:49:37 @agent_ppo2.py:179][0m |          -0.0522 |          23.9940 |         -19.1972 |
[32m[20221213 12:49:38 @agent_ppo2.py:179][0m |          -0.0584 |          23.7348 |         -20.6424 |
[32m[20221213 12:49:38 @agent_ppo2.py:179][0m |          -0.0567 |          24.4535 |         -20.6937 |
[32m[20221213 12:49:38 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 199.44
[32m[20221213 12:49:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.19
[32m[20221213 12:49:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.68
[32m[20221213 12:49:38 @agent_ppo2.py:137][0m Total time:      33.69 min
[32m[20221213 12:49:38 @agent_ppo2.py:139][0m 2312192 total steps have happened
[32m[20221213 12:49:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1129 --------------------------#
[32m[20221213 12:49:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:38 @agent_ppo2.py:179][0m |           0.0650 |          36.2082 |         -27.3867 |
[32m[20221213 12:49:38 @agent_ppo2.py:179][0m |           0.0642 |          35.3425 |         -17.5756 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |           0.0407 |          34.3993 |         -20.8565 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |           0.0183 |          34.1934 |         -24.0058 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |          -0.0067 |          33.9058 |         -28.3070 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |          -0.0165 |          34.2993 |         -30.3604 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |          -0.0206 |          34.4742 |         -31.3724 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |          -0.0181 |          33.4685 |         -30.9476 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |          -0.0219 |          33.9952 |         -31.8785 |
[32m[20221213 12:49:39 @agent_ppo2.py:179][0m |          -0.0342 |          33.3191 |         -33.7202 |
[32m[20221213 12:49:39 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:49:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.98
[32m[20221213 12:49:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.17
[32m[20221213 12:49:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.64
[32m[20221213 12:49:40 @agent_ppo2.py:137][0m Total time:      33.71 min
[32m[20221213 12:49:40 @agent_ppo2.py:139][0m 2314240 total steps have happened
[32m[20221213 12:49:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1130 --------------------------#
[32m[20221213 12:49:40 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:49:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:40 @agent_ppo2.py:179][0m |           0.1218 |          35.9537 |         -25.9077 |
[32m[20221213 12:49:40 @agent_ppo2.py:179][0m |           0.0600 |          34.7943 |         -18.8271 |
[32m[20221213 12:49:40 @agent_ppo2.py:179][0m |           0.0181 |          34.0753 |         -25.4404 |
[32m[20221213 12:49:40 @agent_ppo2.py:179][0m |          -0.0108 |          33.8667 |         -27.3636 |
[32m[20221213 12:49:41 @agent_ppo2.py:179][0m |          -0.0230 |          33.4096 |         -28.5963 |
[32m[20221213 12:49:41 @agent_ppo2.py:179][0m |          -0.0309 |          33.1208 |         -30.9207 |
[32m[20221213 12:49:41 @agent_ppo2.py:179][0m |          -0.0322 |          33.2095 |         -31.8407 |
[32m[20221213 12:49:41 @agent_ppo2.py:179][0m |          -0.0146 |          33.8016 |         -30.7985 |
[32m[20221213 12:49:41 @agent_ppo2.py:179][0m |          -0.0247 |          32.7723 |         -31.9197 |
[32m[20221213 12:49:41 @agent_ppo2.py:179][0m |          -0.0331 |          32.6823 |         -34.4948 |
[32m[20221213 12:49:41 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:49:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.57
[32m[20221213 12:49:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.70
[32m[20221213 12:49:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.36
[32m[20221213 12:49:41 @agent_ppo2.py:137][0m Total time:      33.74 min
[32m[20221213 12:49:41 @agent_ppo2.py:139][0m 2316288 total steps have happened
[32m[20221213 12:49:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1131 --------------------------#
[32m[20221213 12:49:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:42 @agent_ppo2.py:179][0m |           0.0793 |          40.1706 |         -27.5408 |
[32m[20221213 12:49:42 @agent_ppo2.py:179][0m |           0.0426 |          35.5736 |         -19.4715 |
[32m[20221213 12:49:42 @agent_ppo2.py:179][0m |           0.0181 |          35.2062 |         -25.0152 |
[32m[20221213 12:49:42 @agent_ppo2.py:179][0m |          -0.0016 |          34.9253 |         -27.4076 |
[32m[20221213 12:49:42 @agent_ppo2.py:179][0m |          -0.0220 |          34.8423 |         -30.8694 |
[32m[20221213 12:49:42 @agent_ppo2.py:179][0m |          -0.0288 |          34.5548 |         -32.3875 |
[32m[20221213 12:49:43 @agent_ppo2.py:179][0m |          -0.0296 |          34.4138 |         -32.3648 |
[32m[20221213 12:49:43 @agent_ppo2.py:179][0m |          -0.0245 |          36.9058 |         -33.9629 |
[32m[20221213 12:49:43 @agent_ppo2.py:179][0m |          -0.0324 |          34.8601 |         -34.3486 |
[32m[20221213 12:49:43 @agent_ppo2.py:179][0m |          -0.0344 |          34.0542 |         -34.1298 |
[32m[20221213 12:49:43 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:49:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.37
[32m[20221213 12:49:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 349.29
[32m[20221213 12:49:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.87
[32m[20221213 12:49:43 @agent_ppo2.py:137][0m Total time:      33.77 min
[32m[20221213 12:49:43 @agent_ppo2.py:139][0m 2318336 total steps have happened
[32m[20221213 12:49:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1132 --------------------------#
[32m[20221213 12:49:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:43 @agent_ppo2.py:179][0m |           0.0467 |          16.0752 |          -7.5503 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0017 |          13.9496 |          -6.5930 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0014 |          13.4481 |          -7.8012 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0230 |          13.0956 |          -9.1165 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0333 |          12.8925 |          -9.8902 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0288 |          13.4219 |         -10.6829 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0453 |          12.5878 |         -11.7212 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0432 |          12.4486 |         -12.4431 |
[32m[20221213 12:49:44 @agent_ppo2.py:179][0m |          -0.0464 |          12.3576 |         -12.9957 |
[32m[20221213 12:49:45 @agent_ppo2.py:179][0m |          -0.0517 |          12.2686 |         -13.8269 |
[32m[20221213 12:49:45 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:49:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 90.24
[32m[20221213 12:49:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.01
[32m[20221213 12:49:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.51
[32m[20221213 12:49:45 @agent_ppo2.py:137][0m Total time:      33.80 min
[32m[20221213 12:49:45 @agent_ppo2.py:139][0m 2320384 total steps have happened
[32m[20221213 12:49:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1133 --------------------------#
[32m[20221213 12:49:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:45 @agent_ppo2.py:179][0m |           0.1071 |          32.4307 |         -30.1722 |
[32m[20221213 12:49:45 @agent_ppo2.py:179][0m |           0.1214 |          30.6086 |         -13.1352 |
[32m[20221213 12:49:45 @agent_ppo2.py:179][0m |           0.0215 |          30.1188 |         -23.4805 |
[32m[20221213 12:49:46 @agent_ppo2.py:179][0m |          -0.0041 |          29.8157 |         -27.7862 |
[32m[20221213 12:49:46 @agent_ppo2.py:179][0m |          -0.0164 |          29.4599 |         -29.7307 |
[32m[20221213 12:49:46 @agent_ppo2.py:179][0m |          -0.0346 |          29.3512 |         -32.4690 |
[32m[20221213 12:49:46 @agent_ppo2.py:179][0m |          -0.0337 |          29.3929 |         -33.3049 |
[32m[20221213 12:49:46 @agent_ppo2.py:179][0m |          -0.0347 |          29.0776 |         -33.0392 |
[32m[20221213 12:49:46 @agent_ppo2.py:179][0m |          -0.0308 |          28.9507 |         -33.3977 |
[32m[20221213 12:49:46 @agent_ppo2.py:179][0m |          -0.0193 |          34.0105 |         -33.9581 |
[32m[20221213 12:49:46 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:49:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 285.82
[32m[20221213 12:49:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 368.08
[32m[20221213 12:49:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 381.70
[32m[20221213 12:49:46 @agent_ppo2.py:137][0m Total time:      33.83 min
[32m[20221213 12:49:46 @agent_ppo2.py:139][0m 2322432 total steps have happened
[32m[20221213 12:49:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1134 --------------------------#
[32m[20221213 12:49:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:47 @agent_ppo2.py:179][0m |           0.0941 |          28.0742 |         -31.0535 |
[32m[20221213 12:49:47 @agent_ppo2.py:179][0m |           0.0470 |          24.4635 |         -18.0096 |
[32m[20221213 12:49:47 @agent_ppo2.py:179][0m |           0.0033 |          23.8692 |         -21.1558 |
[32m[20221213 12:49:47 @agent_ppo2.py:179][0m |          -0.0174 |          23.5710 |         -24.1072 |
[32m[20221213 12:49:47 @agent_ppo2.py:179][0m |          -0.0296 |          23.2232 |         -25.1630 |
[32m[20221213 12:49:47 @agent_ppo2.py:179][0m |          -0.0394 |          22.9795 |         -26.7569 |
[32m[20221213 12:49:48 @agent_ppo2.py:179][0m |          -0.0418 |          22.9376 |         -28.4388 |
[32m[20221213 12:49:48 @agent_ppo2.py:179][0m |          -0.0488 |          22.6310 |         -28.9062 |
[32m[20221213 12:49:48 @agent_ppo2.py:179][0m |          -0.0428 |          23.1262 |         -29.9750 |
[32m[20221213 12:49:48 @agent_ppo2.py:179][0m |          -0.0475 |          22.5555 |         -30.0074 |
[32m[20221213 12:49:48 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:49:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 211.99
[32m[20221213 12:49:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.93
[32m[20221213 12:49:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.15
[32m[20221213 12:49:48 @agent_ppo2.py:137][0m Total time:      33.86 min
[32m[20221213 12:49:48 @agent_ppo2.py:139][0m 2324480 total steps have happened
[32m[20221213 12:49:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1135 --------------------------#
[32m[20221213 12:49:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |           0.0787 |          35.9225 |         -34.9577 |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |           0.0923 |          35.3750 |         -17.6732 |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |           0.0361 |          34.8952 |         -25.2741 |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |           0.0156 |          34.5413 |         -30.0974 |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |          -0.0011 |          34.7579 |         -35.6503 |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |          -0.0129 |          34.2480 |         -38.6059 |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |          -0.0202 |          34.1199 |         -39.2449 |
[32m[20221213 12:49:49 @agent_ppo2.py:179][0m |          -0.0256 |          33.9893 |         -40.7266 |
[32m[20221213 12:49:50 @agent_ppo2.py:179][0m |          -0.0228 |          33.8860 |         -40.5527 |
[32m[20221213 12:49:50 @agent_ppo2.py:179][0m |          -0.0175 |          38.8507 |         -41.3595 |
[32m[20221213 12:49:50 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:49:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 351.92
[32m[20221213 12:49:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.85
[32m[20221213 12:49:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.60
[32m[20221213 12:49:50 @agent_ppo2.py:137][0m Total time:      33.89 min
[32m[20221213 12:49:50 @agent_ppo2.py:139][0m 2326528 total steps have happened
[32m[20221213 12:49:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1136 --------------------------#
[32m[20221213 12:49:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:50 @agent_ppo2.py:179][0m |           0.1014 |          27.7758 |         -27.2170 |
[32m[20221213 12:49:50 @agent_ppo2.py:179][0m |           0.0837 |          25.1262 |         -15.7635 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |           0.0434 |          24.5442 |         -18.3691 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |           0.0027 |          24.1047 |         -22.7736 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |          -0.0074 |          25.2094 |         -24.4610 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |          -0.0248 |          23.6248 |         -26.1637 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |          -0.0290 |          23.5350 |         -27.9782 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |          -0.0412 |          23.3390 |         -29.5663 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |          -0.0458 |          23.1857 |         -30.1532 |
[32m[20221213 12:49:51 @agent_ppo2.py:179][0m |          -0.0413 |          22.9381 |         -29.7103 |
[32m[20221213 12:49:51 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:49:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 176.06
[32m[20221213 12:49:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.00
[32m[20221213 12:49:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 383.67
[32m[20221213 12:49:52 @agent_ppo2.py:137][0m Total time:      33.91 min
[32m[20221213 12:49:52 @agent_ppo2.py:139][0m 2328576 total steps have happened
[32m[20221213 12:49:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1137 --------------------------#
[32m[20221213 12:49:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:52 @agent_ppo2.py:179][0m |           0.0648 |          30.9173 |         -33.8563 |
[32m[20221213 12:49:52 @agent_ppo2.py:179][0m |           0.0351 |          29.6838 |         -29.8243 |
[32m[20221213 12:49:52 @agent_ppo2.py:179][0m |           0.0062 |          29.0874 |         -32.6695 |
[32m[20221213 12:49:52 @agent_ppo2.py:179][0m |          -0.0109 |          28.6658 |         -36.0197 |
[32m[20221213 12:49:53 @agent_ppo2.py:179][0m |          -0.0236 |          28.4620 |         -36.6015 |
[32m[20221213 12:49:53 @agent_ppo2.py:179][0m |          -0.0200 |          28.1623 |         -37.8483 |
[32m[20221213 12:49:53 @agent_ppo2.py:179][0m |          -0.0283 |          28.0219 |         -39.1021 |
[32m[20221213 12:49:53 @agent_ppo2.py:179][0m |          -0.0227 |          31.0362 |         -39.0163 |
[32m[20221213 12:49:53 @agent_ppo2.py:179][0m |          -0.0302 |          27.7151 |         -40.1654 |
[32m[20221213 12:49:53 @agent_ppo2.py:179][0m |          -0.0270 |          27.6794 |         -40.4079 |
[32m[20221213 12:49:53 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:49:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.88
[32m[20221213 12:49:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.78
[32m[20221213 12:49:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.29
[32m[20221213 12:49:53 @agent_ppo2.py:137][0m Total time:      33.94 min
[32m[20221213 12:49:53 @agent_ppo2.py:139][0m 2330624 total steps have happened
[32m[20221213 12:49:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1138 --------------------------#
[32m[20221213 12:49:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:54 @agent_ppo2.py:179][0m |           0.0527 |          34.4683 |         -34.4790 |
[32m[20221213 12:49:54 @agent_ppo2.py:179][0m |           0.0616 |          33.4352 |         -22.7196 |
[32m[20221213 12:49:54 @agent_ppo2.py:179][0m |           0.0271 |          35.0071 |         -31.2677 |
[32m[20221213 12:49:54 @agent_ppo2.py:179][0m |          -0.0014 |          33.3901 |         -33.7965 |
[32m[20221213 12:49:54 @agent_ppo2.py:179][0m |          -0.0214 |          32.4609 |         -35.0099 |
[32m[20221213 12:49:54 @agent_ppo2.py:179][0m |          -0.0224 |          33.3515 |         -36.2553 |
[32m[20221213 12:49:54 @agent_ppo2.py:179][0m |          -0.0224 |          32.4357 |         -37.1772 |
[32m[20221213 12:49:55 @agent_ppo2.py:179][0m |          -0.0187 |          32.7125 |         -37.6208 |
[32m[20221213 12:49:55 @agent_ppo2.py:179][0m |          -0.0253 |          34.1068 |         -38.2147 |
[32m[20221213 12:49:55 @agent_ppo2.py:179][0m |          -0.0350 |          31.6885 |         -39.6271 |
[32m[20221213 12:49:55 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:49:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 348.45
[32m[20221213 12:49:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.61
[32m[20221213 12:49:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.80
[32m[20221213 12:49:55 @agent_ppo2.py:137][0m Total time:      33.97 min
[32m[20221213 12:49:55 @agent_ppo2.py:139][0m 2332672 total steps have happened
[32m[20221213 12:49:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1139 --------------------------#
[32m[20221213 12:49:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:55 @agent_ppo2.py:179][0m |           0.0714 |          34.8585 |         -34.2832 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |           0.0558 |          33.8764 |         -25.1474 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |           0.0284 |          33.5288 |         -28.7407 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |           0.0040 |          33.1571 |         -30.8624 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |          -0.0127 |          33.0320 |         -35.0600 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |          -0.0122 |          34.8835 |         -36.1801 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |          -0.0219 |          32.7948 |         -37.3730 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |          -0.0296 |          32.5419 |         -38.1833 |
[32m[20221213 12:49:56 @agent_ppo2.py:179][0m |          -0.0237 |          32.9941 |         -39.5644 |
[32m[20221213 12:49:57 @agent_ppo2.py:179][0m |          -0.0220 |          34.1891 |         -40.8068 |
[32m[20221213 12:49:57 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:49:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.07
[32m[20221213 12:49:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.12
[32m[20221213 12:49:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.37
[32m[20221213 12:49:57 @agent_ppo2.py:137][0m Total time:      34.00 min
[32m[20221213 12:49:57 @agent_ppo2.py:139][0m 2334720 total steps have happened
[32m[20221213 12:49:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1140 --------------------------#
[32m[20221213 12:49:57 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:49:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:57 @agent_ppo2.py:179][0m |           0.0761 |          34.7525 |         -29.0411 |
[32m[20221213 12:49:57 @agent_ppo2.py:179][0m |           0.0443 |          34.0647 |         -24.7405 |
[32m[20221213 12:49:57 @agent_ppo2.py:179][0m |           0.0175 |          33.5708 |         -26.5522 |
[32m[20221213 12:49:58 @agent_ppo2.py:179][0m |          -0.0080 |          33.3981 |         -30.4550 |
[32m[20221213 12:49:58 @agent_ppo2.py:179][0m |          -0.0201 |          33.1677 |         -33.1423 |
[32m[20221213 12:49:58 @agent_ppo2.py:179][0m |          -0.0278 |          32.9539 |         -34.2953 |
[32m[20221213 12:49:58 @agent_ppo2.py:179][0m |          -0.0314 |          32.8071 |         -34.8201 |
[32m[20221213 12:49:58 @agent_ppo2.py:179][0m |          -0.0321 |          32.6494 |         -36.5129 |
[32m[20221213 12:49:58 @agent_ppo2.py:179][0m |          -0.0361 |          32.4857 |         -37.1192 |
[32m[20221213 12:49:58 @agent_ppo2.py:179][0m |          -0.0420 |          32.4566 |         -39.0027 |
[32m[20221213 12:49:58 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:49:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.56
[32m[20221213 12:49:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.47
[32m[20221213 12:49:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.47
[32m[20221213 12:49:59 @agent_ppo2.py:137][0m Total time:      34.03 min
[32m[20221213 12:49:59 @agent_ppo2.py:139][0m 2336768 total steps have happened
[32m[20221213 12:49:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1141 --------------------------#
[32m[20221213 12:49:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:49:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:49:59 @agent_ppo2.py:179][0m |           0.0574 |          36.1633 |         -35.8170 |
[32m[20221213 12:49:59 @agent_ppo2.py:179][0m |           0.0436 |          32.7393 |         -28.7350 |
[32m[20221213 12:49:59 @agent_ppo2.py:179][0m |           0.0013 |          31.8176 |         -34.8287 |
[32m[20221213 12:49:59 @agent_ppo2.py:179][0m |          -0.0090 |          32.4273 |         -35.7460 |
[32m[20221213 12:49:59 @agent_ppo2.py:179][0m |          -0.0232 |          30.8203 |         -36.5489 |
[32m[20221213 12:50:00 @agent_ppo2.py:179][0m |          -0.0349 |          30.4739 |         -38.9136 |
[32m[20221213 12:50:00 @agent_ppo2.py:179][0m |          -0.0358 |          30.1117 |         -39.4149 |
[32m[20221213 12:50:00 @agent_ppo2.py:179][0m |          -0.0347 |          30.1348 |         -40.0902 |
[32m[20221213 12:50:00 @agent_ppo2.py:179][0m |          -0.0365 |          29.5809 |         -40.5304 |
[32m[20221213 12:50:00 @agent_ppo2.py:179][0m |          -0.0454 |          29.3910 |         -41.1156 |
[32m[20221213 12:50:00 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:50:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.57
[32m[20221213 12:50:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 312.78
[32m[20221213 12:50:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 387.35
[32m[20221213 12:50:00 @agent_ppo2.py:137][0m Total time:      34.06 min
[32m[20221213 12:50:00 @agent_ppo2.py:139][0m 2338816 total steps have happened
[32m[20221213 12:50:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1142 --------------------------#
[32m[20221213 12:50:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:01 @agent_ppo2.py:179][0m |           0.0625 |          34.7604 |         -37.0704 |
[32m[20221213 12:50:01 @agent_ppo2.py:179][0m |           0.0477 |          33.8904 |         -25.4620 |
[32m[20221213 12:50:01 @agent_ppo2.py:179][0m |           0.0039 |          33.1144 |         -30.2931 |
[32m[20221213 12:50:01 @agent_ppo2.py:179][0m |          -0.0094 |          32.8323 |         -33.6556 |
[32m[20221213 12:50:01 @agent_ppo2.py:179][0m |          -0.0263 |          32.5962 |         -36.2023 |
[32m[20221213 12:50:01 @agent_ppo2.py:179][0m |          -0.0288 |          32.4450 |         -36.5883 |
[32m[20221213 12:50:01 @agent_ppo2.py:179][0m |          -0.0341 |          32.3779 |         -38.2348 |
[32m[20221213 12:50:02 @agent_ppo2.py:179][0m |          -0.0386 |          32.1287 |         -39.3662 |
[32m[20221213 12:50:02 @agent_ppo2.py:179][0m |          -0.0356 |          32.0896 |         -39.3362 |
[32m[20221213 12:50:02 @agent_ppo2.py:179][0m |          -0.0394 |          32.0321 |         -39.7292 |
[32m[20221213 12:50:02 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.18
[32m[20221213 12:50:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.88
[32m[20221213 12:50:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 299.55
[32m[20221213 12:50:02 @agent_ppo2.py:137][0m Total time:      34.09 min
[32m[20221213 12:50:02 @agent_ppo2.py:139][0m 2340864 total steps have happened
[32m[20221213 12:50:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1143 --------------------------#
[32m[20221213 12:50:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:02 @agent_ppo2.py:179][0m |           0.0259 |          34.6567 |         -31.6143 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |           0.0272 |          33.8363 |         -28.0645 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |           0.0102 |          33.5179 |         -29.6411 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |          -0.0044 |          32.9514 |         -31.1002 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |          -0.0157 |          33.0397 |         -31.6860 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |          -0.0296 |          32.5366 |         -34.5097 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |          -0.0254 |          33.4704 |         -35.2510 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |          -0.0349 |          32.3153 |         -36.4823 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |          -0.0203 |          32.2110 |         -33.3423 |
[32m[20221213 12:50:03 @agent_ppo2.py:179][0m |          -0.0303 |          31.9127 |         -36.5389 |
[32m[20221213 12:50:03 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 347.16
[32m[20221213 12:50:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.03
[32m[20221213 12:50:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.48
[32m[20221213 12:50:04 @agent_ppo2.py:137][0m Total time:      34.12 min
[32m[20221213 12:50:04 @agent_ppo2.py:139][0m 2342912 total steps have happened
[32m[20221213 12:50:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1144 --------------------------#
[32m[20221213 12:50:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:04 @agent_ppo2.py:179][0m |           0.0902 |          35.8573 |         -30.9059 |
[32m[20221213 12:50:04 @agent_ppo2.py:179][0m |           0.0549 |          33.7936 |         -24.2530 |
[32m[20221213 12:50:04 @agent_ppo2.py:179][0m |           0.0126 |          32.9538 |         -28.3128 |
[32m[20221213 12:50:04 @agent_ppo2.py:179][0m |          -0.0102 |          32.3281 |         -31.5112 |
[32m[20221213 12:50:05 @agent_ppo2.py:179][0m |          -0.0207 |          31.9504 |         -32.4946 |
[32m[20221213 12:50:05 @agent_ppo2.py:179][0m |          -0.0339 |          31.8160 |         -34.1685 |
[32m[20221213 12:50:05 @agent_ppo2.py:179][0m |          -0.0323 |          31.4640 |         -33.5933 |
[32m[20221213 12:50:05 @agent_ppo2.py:179][0m |          -0.0382 |          31.2291 |         -35.3009 |
[32m[20221213 12:50:05 @agent_ppo2.py:179][0m |          -0.0412 |          31.0820 |         -35.1358 |
[32m[20221213 12:50:05 @agent_ppo2.py:179][0m |          -0.0431 |          31.0139 |         -36.8513 |
[32m[20221213 12:50:05 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:50:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 293.49
[32m[20221213 12:50:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.91
[32m[20221213 12:50:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.66
[32m[20221213 12:50:05 @agent_ppo2.py:137][0m Total time:      34.14 min
[32m[20221213 12:50:05 @agent_ppo2.py:139][0m 2344960 total steps have happened
[32m[20221213 12:50:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1145 --------------------------#
[32m[20221213 12:50:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:06 @agent_ppo2.py:179][0m |           0.0576 |          31.3503 |         -32.3631 |
[32m[20221213 12:50:06 @agent_ppo2.py:179][0m |           0.0430 |          28.6184 |         -25.4552 |
[32m[20221213 12:50:06 @agent_ppo2.py:179][0m |           0.0027 |          27.9651 |         -28.3118 |
[32m[20221213 12:50:06 @agent_ppo2.py:179][0m |          -0.0190 |          27.5238 |         -31.5175 |
[32m[20221213 12:50:06 @agent_ppo2.py:179][0m |          -0.0286 |          27.1101 |         -33.5177 |
[32m[20221213 12:50:06 @agent_ppo2.py:179][0m |          -0.0303 |          26.8289 |         -34.5629 |
[32m[20221213 12:50:07 @agent_ppo2.py:179][0m |          -0.0341 |          26.6689 |         -34.9678 |
[32m[20221213 12:50:07 @agent_ppo2.py:179][0m |          -0.0277 |          29.2271 |         -36.8823 |
[32m[20221213 12:50:07 @agent_ppo2.py:179][0m |          -0.0293 |          26.6387 |         -33.6812 |
[32m[20221213 12:50:07 @agent_ppo2.py:179][0m |          -0.0388 |          26.2491 |         -36.3945 |
[32m[20221213 12:50:07 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:50:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 341.19
[32m[20221213 12:50:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.08
[32m[20221213 12:50:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 325.21
[32m[20221213 12:50:07 @agent_ppo2.py:137][0m Total time:      34.17 min
[32m[20221213 12:50:07 @agent_ppo2.py:139][0m 2347008 total steps have happened
[32m[20221213 12:50:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1146 --------------------------#
[32m[20221213 12:50:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |           0.0972 |          37.5830 |         -25.5305 |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |           0.0490 |          35.1102 |         -21.2405 |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |           0.0152 |          34.3628 |         -25.0243 |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |           0.0016 |          34.4541 |         -27.6758 |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |          -0.0178 |          33.5092 |         -29.4868 |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |          -0.0270 |          33.1685 |         -30.4708 |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |          -0.0332 |          32.9282 |         -31.6819 |
[32m[20221213 12:50:08 @agent_ppo2.py:179][0m |          -0.0376 |          32.6598 |         -33.3085 |
[32m[20221213 12:50:09 @agent_ppo2.py:179][0m |          -0.0384 |          32.5505 |         -33.8891 |
[32m[20221213 12:50:09 @agent_ppo2.py:179][0m |          -0.0395 |          32.3607 |         -33.9196 |
[32m[20221213 12:50:09 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:50:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 351.76
[32m[20221213 12:50:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 375.61
[32m[20221213 12:50:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 302.77
[32m[20221213 12:50:09 @agent_ppo2.py:137][0m Total time:      34.20 min
[32m[20221213 12:50:09 @agent_ppo2.py:139][0m 2349056 total steps have happened
[32m[20221213 12:50:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1147 --------------------------#
[32m[20221213 12:50:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:09 @agent_ppo2.py:179][0m |           0.0879 |          31.3511 |         -24.8457 |
[32m[20221213 12:50:09 @agent_ppo2.py:179][0m |           0.0560 |          29.8425 |         -16.6487 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |           0.0177 |          29.2737 |         -18.5066 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |          -0.0022 |          28.8771 |         -21.3837 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |          -0.0165 |          28.7475 |         -22.7179 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |          -0.0255 |          28.4274 |         -23.7595 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |          -0.0295 |          28.2556 |         -24.6971 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |          -0.0351 |          28.1191 |         -25.1489 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |          -0.0380 |          27.8981 |         -26.5125 |
[32m[20221213 12:50:10 @agent_ppo2.py:179][0m |          -0.0417 |          27.8130 |         -26.9021 |
[32m[20221213 12:50:10 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:50:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.42
[32m[20221213 12:50:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.19
[32m[20221213 12:50:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.30
[32m[20221213 12:50:11 @agent_ppo2.py:137][0m Total time:      34.23 min
[32m[20221213 12:50:11 @agent_ppo2.py:139][0m 2351104 total steps have happened
[32m[20221213 12:50:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1148 --------------------------#
[32m[20221213 12:50:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:11 @agent_ppo2.py:179][0m |           0.0747 |          31.1055 |         -24.6761 |
[32m[20221213 12:50:11 @agent_ppo2.py:179][0m |           0.0682 |          29.6985 |         -17.2145 |
[32m[20221213 12:50:11 @agent_ppo2.py:179][0m |           0.0235 |          28.9897 |         -19.2110 |
[32m[20221213 12:50:11 @agent_ppo2.py:179][0m |          -0.0058 |          28.5957 |         -22.8969 |
[32m[20221213 12:50:12 @agent_ppo2.py:179][0m |          -0.0274 |          28.4598 |         -25.0555 |
[32m[20221213 12:50:12 @agent_ppo2.py:179][0m |          -0.0347 |          28.0649 |         -26.2183 |
[32m[20221213 12:50:12 @agent_ppo2.py:179][0m |          -0.0346 |          27.8574 |         -25.6679 |
[32m[20221213 12:50:12 @agent_ppo2.py:179][0m |          -0.0319 |          30.2773 |         -27.9207 |
[32m[20221213 12:50:12 @agent_ppo2.py:179][0m |          -0.0249 |          27.5993 |         -25.7199 |
[32m[20221213 12:50:12 @agent_ppo2.py:179][0m |          -0.0433 |          27.4085 |         -26.9553 |
[32m[20221213 12:50:12 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:50:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.34
[32m[20221213 12:50:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.37
[32m[20221213 12:50:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 382.45
[32m[20221213 12:50:12 @agent_ppo2.py:137][0m Total time:      34.26 min
[32m[20221213 12:50:12 @agent_ppo2.py:139][0m 2353152 total steps have happened
[32m[20221213 12:50:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1149 --------------------------#
[32m[20221213 12:50:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:13 @agent_ppo2.py:179][0m |           0.0498 |          29.8570 |         -26.4031 |
[32m[20221213 12:50:13 @agent_ppo2.py:179][0m |           0.0490 |          28.3067 |         -23.4639 |
[32m[20221213 12:50:13 @agent_ppo2.py:179][0m |           0.0018 |          27.8445 |         -25.8938 |
[32m[20221213 12:50:13 @agent_ppo2.py:179][0m |          -0.0122 |          27.7062 |         -28.5417 |
[32m[20221213 12:50:13 @agent_ppo2.py:179][0m |          -0.0139 |          27.4192 |         -28.0853 |
[32m[20221213 12:50:13 @agent_ppo2.py:179][0m |          -0.0331 |          26.9498 |         -30.1775 |
[32m[20221213 12:50:13 @agent_ppo2.py:179][0m |          -0.0358 |          26.8095 |         -31.6605 |
[32m[20221213 12:50:14 @agent_ppo2.py:179][0m |          -0.0382 |          26.6844 |         -31.8208 |
[32m[20221213 12:50:14 @agent_ppo2.py:179][0m |          -0.0412 |          26.5745 |         -32.1789 |
[32m[20221213 12:50:14 @agent_ppo2.py:179][0m |          -0.0450 |          26.4585 |         -32.9058 |
[32m[20221213 12:50:14 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:50:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 260.30
[32m[20221213 12:50:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.26
[32m[20221213 12:50:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.01
[32m[20221213 12:50:14 @agent_ppo2.py:137][0m Total time:      34.29 min
[32m[20221213 12:50:14 @agent_ppo2.py:139][0m 2355200 total steps have happened
[32m[20221213 12:50:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1150 --------------------------#
[32m[20221213 12:50:14 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:50:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |           0.0825 |          17.7620 |         -18.4075 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |           0.0434 |          16.8369 |          -8.5961 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |           0.0030 |          16.5724 |         -11.0022 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |          -0.0110 |          16.3663 |         -11.8347 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |          -0.0107 |          16.2091 |         -12.4062 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |          -0.0269 |          16.2591 |         -13.0848 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |          -0.0380 |          16.0182 |         -13.7999 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |          -0.0404 |          15.8557 |         -14.1552 |
[32m[20221213 12:50:15 @agent_ppo2.py:179][0m |          -0.0357 |          16.0382 |         -14.4036 |
[32m[20221213 12:50:16 @agent_ppo2.py:179][0m |          -0.0458 |          15.8319 |         -15.4255 |
[32m[20221213 12:50:16 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:50:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.33
[32m[20221213 12:50:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 318.20
[32m[20221213 12:50:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.05
[32m[20221213 12:50:16 @agent_ppo2.py:137][0m Total time:      34.32 min
[32m[20221213 12:50:16 @agent_ppo2.py:139][0m 2357248 total steps have happened
[32m[20221213 12:50:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1151 --------------------------#
[32m[20221213 12:50:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:16 @agent_ppo2.py:179][0m |           0.0510 |          35.4520 |         -26.4499 |
[32m[20221213 12:50:16 @agent_ppo2.py:179][0m |           0.0471 |          34.4131 |         -22.6071 |
[32m[20221213 12:50:16 @agent_ppo2.py:179][0m |           0.0135 |          33.5206 |         -21.8177 |
[32m[20221213 12:50:17 @agent_ppo2.py:179][0m |          -0.0083 |          33.2160 |         -25.4498 |
[32m[20221213 12:50:17 @agent_ppo2.py:179][0m |          -0.0132 |          34.9622 |         -28.3316 |
[32m[20221213 12:50:17 @agent_ppo2.py:179][0m |          -0.0145 |          33.0158 |         -28.1522 |
[32m[20221213 12:50:17 @agent_ppo2.py:179][0m |          -0.0269 |          32.6648 |         -29.3115 |
[32m[20221213 12:50:17 @agent_ppo2.py:179][0m |          -0.0323 |          32.5318 |         -31.2145 |
[32m[20221213 12:50:17 @agent_ppo2.py:179][0m |          -0.0388 |          32.4121 |         -32.4078 |
[32m[20221213 12:50:17 @agent_ppo2.py:179][0m |          -0.0386 |          32.3687 |         -32.9020 |
[32m[20221213 12:50:17 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:50:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 352.00
[32m[20221213 12:50:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.54
[32m[20221213 12:50:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.15
[32m[20221213 12:50:17 @agent_ppo2.py:137][0m Total time:      34.35 min
[32m[20221213 12:50:17 @agent_ppo2.py:139][0m 2359296 total steps have happened
[32m[20221213 12:50:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1152 --------------------------#
[32m[20221213 12:50:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:18 @agent_ppo2.py:179][0m |           0.1207 |          26.2306 |         -24.5521 |
[32m[20221213 12:50:18 @agent_ppo2.py:179][0m |           0.0572 |          23.0721 |         -18.2001 |
[32m[20221213 12:50:18 @agent_ppo2.py:179][0m |           0.0045 |          22.2345 |         -21.3412 |
[32m[20221213 12:50:18 @agent_ppo2.py:179][0m |          -0.0114 |          21.7265 |         -21.7856 |
[32m[20221213 12:50:18 @agent_ppo2.py:179][0m |          -0.0230 |          21.5078 |         -22.8759 |
[32m[20221213 12:50:19 @agent_ppo2.py:179][0m |          -0.0325 |          21.2168 |         -23.8527 |
[32m[20221213 12:50:19 @agent_ppo2.py:179][0m |          -0.0365 |          21.0807 |         -24.5630 |
[32m[20221213 12:50:19 @agent_ppo2.py:179][0m |          -0.0443 |          20.8141 |         -26.4474 |
[32m[20221213 12:50:19 @agent_ppo2.py:179][0m |          -0.0452 |          20.6565 |         -27.1732 |
[32m[20221213 12:50:19 @agent_ppo2.py:179][0m |          -0.0455 |          20.5932 |         -28.1456 |
[32m[20221213 12:50:19 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:50:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 265.48
[32m[20221213 12:50:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.22
[32m[20221213 12:50:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.62
[32m[20221213 12:50:19 @agent_ppo2.py:137][0m Total time:      34.37 min
[32m[20221213 12:50:19 @agent_ppo2.py:139][0m 2361344 total steps have happened
[32m[20221213 12:50:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1153 --------------------------#
[32m[20221213 12:50:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |           0.0731 |          36.0412 |         -30.9800 |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |           0.0575 |          34.9992 |         -24.3244 |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |           0.0046 |          34.4861 |         -29.9054 |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |          -0.0194 |          34.3278 |         -31.5386 |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |          -0.0247 |          34.0309 |         -33.0015 |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |          -0.0254 |          33.7989 |         -33.0024 |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |          -0.0376 |          33.6537 |         -34.7917 |
[32m[20221213 12:50:20 @agent_ppo2.py:179][0m |          -0.0405 |          33.5465 |         -36.0521 |
[32m[20221213 12:50:21 @agent_ppo2.py:179][0m |          -0.0282 |          36.1159 |         -37.2311 |
[32m[20221213 12:50:21 @agent_ppo2.py:179][0m |          -0.0129 |          33.3926 |         -34.0501 |
[32m[20221213 12:50:21 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.68
[32m[20221213 12:50:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.30
[32m[20221213 12:50:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 235.44
[32m[20221213 12:50:21 @agent_ppo2.py:137][0m Total time:      34.40 min
[32m[20221213 12:50:21 @agent_ppo2.py:139][0m 2363392 total steps have happened
[32m[20221213 12:50:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1154 --------------------------#
[32m[20221213 12:50:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:21 @agent_ppo2.py:179][0m |           0.1300 |          29.7690 |         -26.3712 |
[32m[20221213 12:50:21 @agent_ppo2.py:179][0m |           0.0492 |          28.2955 |         -18.2380 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |           0.0076 |          27.6375 |         -22.4743 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |          -0.0124 |          27.3812 |         -26.5736 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |          -0.0174 |          27.3820 |         -28.4742 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |          -0.0158 |          29.4550 |         -28.7995 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |          -0.0353 |          26.7876 |         -30.6981 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |          -0.0387 |          26.5680 |         -31.5082 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |          -0.0387 |          26.8334 |         -32.9816 |
[32m[20221213 12:50:22 @agent_ppo2.py:179][0m |          -0.0416 |          26.3700 |         -33.3105 |
[32m[20221213 12:50:22 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:50:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 277.97
[32m[20221213 12:50:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.77
[32m[20221213 12:50:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.22
[32m[20221213 12:50:23 @agent_ppo2.py:137][0m Total time:      34.43 min
[32m[20221213 12:50:23 @agent_ppo2.py:139][0m 2365440 total steps have happened
[32m[20221213 12:50:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1155 --------------------------#
[32m[20221213 12:50:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:23 @agent_ppo2.py:179][0m |           0.0518 |          27.7007 |         -34.3773 |
[32m[20221213 12:50:23 @agent_ppo2.py:179][0m |           0.0375 |          25.1069 |         -28.8933 |
[32m[20221213 12:50:23 @agent_ppo2.py:179][0m |           0.0021 |          25.1592 |         -26.8002 |
[32m[20221213 12:50:23 @agent_ppo2.py:179][0m |          -0.0267 |          23.8282 |         -26.2261 |
[32m[20221213 12:50:24 @agent_ppo2.py:179][0m |          -0.0393 |          23.3543 |         -29.4324 |
[32m[20221213 12:50:24 @agent_ppo2.py:179][0m |          -0.0426 |          23.4199 |         -30.9233 |
[32m[20221213 12:50:24 @agent_ppo2.py:179][0m |          -0.0488 |          22.6763 |         -31.9788 |
[32m[20221213 12:50:24 @agent_ppo2.py:179][0m |          -0.0564 |          22.3517 |         -33.0799 |
[32m[20221213 12:50:24 @agent_ppo2.py:179][0m |          -0.0559 |          22.1286 |         -33.2963 |
[32m[20221213 12:50:24 @agent_ppo2.py:179][0m |          -0.0609 |          21.9643 |         -34.9075 |
[32m[20221213 12:50:24 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:50:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 254.90
[32m[20221213 12:50:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.27
[32m[20221213 12:50:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.86
[32m[20221213 12:50:24 @agent_ppo2.py:137][0m Total time:      34.46 min
[32m[20221213 12:50:24 @agent_ppo2.py:139][0m 2367488 total steps have happened
[32m[20221213 12:50:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1156 --------------------------#
[32m[20221213 12:50:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:25 @agent_ppo2.py:179][0m |           0.1436 |          35.1759 |         -29.2930 |
[32m[20221213 12:50:25 @agent_ppo2.py:179][0m |           0.0653 |          31.9009 |         -20.7999 |
[32m[20221213 12:50:25 @agent_ppo2.py:179][0m |           0.0122 |          30.7307 |         -27.5682 |
[32m[20221213 12:50:25 @agent_ppo2.py:179][0m |          -0.0094 |          29.9251 |         -31.3249 |
[32m[20221213 12:50:25 @agent_ppo2.py:179][0m |          -0.0295 |          29.4228 |         -33.4748 |
[32m[20221213 12:50:25 @agent_ppo2.py:179][0m |          -0.0443 |          29.1636 |         -36.6147 |
[32m[20221213 12:50:26 @agent_ppo2.py:179][0m |          -0.0354 |          28.8667 |         -37.1401 |
[32m[20221213 12:50:26 @agent_ppo2.py:179][0m |          -0.0418 |          28.5985 |         -38.7430 |
[32m[20221213 12:50:26 @agent_ppo2.py:179][0m |          -0.0488 |          28.4518 |         -40.4082 |
[32m[20221213 12:50:26 @agent_ppo2.py:179][0m |          -0.0524 |          28.4118 |         -41.3125 |
[32m[20221213 12:50:26 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:50:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.99
[32m[20221213 12:50:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.22
[32m[20221213 12:50:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.24
[32m[20221213 12:50:26 @agent_ppo2.py:137][0m Total time:      34.49 min
[32m[20221213 12:50:26 @agent_ppo2.py:139][0m 2369536 total steps have happened
[32m[20221213 12:50:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1157 --------------------------#
[32m[20221213 12:50:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |           0.0658 |          35.6813 |         -29.3952 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |           0.0436 |          34.0304 |         -22.1274 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |           0.0221 |          34.0407 |         -25.8324 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |           0.0035 |          33.1540 |         -26.9643 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |          -0.0130 |          32.7485 |         -27.9820 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |          -0.0264 |          32.5168 |         -30.0947 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |          -0.0326 |          32.4202 |         -31.4577 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |          -0.0360 |          32.2240 |         -32.4304 |
[32m[20221213 12:50:27 @agent_ppo2.py:179][0m |          -0.0363 |          32.1053 |         -32.9276 |
[32m[20221213 12:50:28 @agent_ppo2.py:179][0m |          -0.0376 |          32.0120 |         -33.5935 |
[32m[20221213 12:50:28 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:50:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 365.54
[32m[20221213 12:50:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 377.76
[32m[20221213 12:50:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.76
[32m[20221213 12:50:28 @agent_ppo2.py:137][0m Total time:      34.52 min
[32m[20221213 12:50:28 @agent_ppo2.py:139][0m 2371584 total steps have happened
[32m[20221213 12:50:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1158 --------------------------#
[32m[20221213 12:50:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:28 @agent_ppo2.py:179][0m |           0.0682 |          31.1639 |         -26.5732 |
[32m[20221213 12:50:28 @agent_ppo2.py:179][0m |           0.0436 |          29.5158 |         -24.6670 |
[32m[20221213 12:50:28 @agent_ppo2.py:179][0m |           0.0045 |          29.0267 |         -26.5259 |
[32m[20221213 12:50:29 @agent_ppo2.py:179][0m |          -0.0223 |          28.7291 |         -27.2949 |
[32m[20221213 12:50:29 @agent_ppo2.py:179][0m |          -0.0322 |          28.4099 |         -29.1044 |
[32m[20221213 12:50:29 @agent_ppo2.py:179][0m |          -0.0298 |          30.3727 |         -30.2827 |
[32m[20221213 12:50:29 @agent_ppo2.py:179][0m |          -0.0441 |          27.9264 |         -30.3121 |
[32m[20221213 12:50:29 @agent_ppo2.py:179][0m |          -0.0455 |          27.7415 |         -30.6350 |
[32m[20221213 12:50:29 @agent_ppo2.py:179][0m |          -0.0550 |          27.6117 |         -31.1871 |
[32m[20221213 12:50:29 @agent_ppo2.py:179][0m |          -0.0539 |          27.4536 |         -32.2303 |
[32m[20221213 12:50:29 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:50:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 266.49
[32m[20221213 12:50:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.68
[32m[20221213 12:50:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.69
[32m[20221213 12:50:30 @agent_ppo2.py:137][0m Total time:      34.55 min
[32m[20221213 12:50:30 @agent_ppo2.py:139][0m 2373632 total steps have happened
[32m[20221213 12:50:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1159 --------------------------#
[32m[20221213 12:50:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:30 @agent_ppo2.py:179][0m |           0.0658 |          32.6465 |         -33.1646 |
[32m[20221213 12:50:30 @agent_ppo2.py:179][0m |           0.0619 |          30.1747 |         -24.2649 |
[32m[20221213 12:50:30 @agent_ppo2.py:179][0m |           0.0447 |          29.4225 |         -23.6283 |
[32m[20221213 12:50:30 @agent_ppo2.py:179][0m |           0.0012 |          28.9479 |         -28.1850 |
[32m[20221213 12:50:30 @agent_ppo2.py:179][0m |          -0.0191 |          28.5455 |         -32.5321 |
[32m[20221213 12:50:31 @agent_ppo2.py:179][0m |          -0.0274 |          28.1942 |         -34.0678 |
[32m[20221213 12:50:31 @agent_ppo2.py:179][0m |          -0.0350 |          28.0162 |         -35.2774 |
[32m[20221213 12:50:31 @agent_ppo2.py:179][0m |          -0.0385 |          27.7875 |         -36.6669 |
[32m[20221213 12:50:31 @agent_ppo2.py:179][0m |          -0.0411 |          27.7250 |         -36.8398 |
[32m[20221213 12:50:31 @agent_ppo2.py:179][0m |          -0.0429 |          27.4123 |         -37.7487 |
[32m[20221213 12:50:31 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:50:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.92
[32m[20221213 12:50:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.66
[32m[20221213 12:50:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.84
[32m[20221213 12:50:31 @agent_ppo2.py:137][0m Total time:      34.57 min
[32m[20221213 12:50:31 @agent_ppo2.py:139][0m 2375680 total steps have happened
[32m[20221213 12:50:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1160 --------------------------#
[32m[20221213 12:50:31 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:50:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1784 |           7.8480 |          -7.8208 |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1573 |           6.8375 |          -0.5315 |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1604 |           6.8379 |          -0.6387 |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1472 |           6.7052 |          -0.5850 |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1406 |           6.6765 |          -0.6730 |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1364 |           6.6314 |          -1.0699 |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1433 |           6.6533 |          -0.7377 |
[32m[20221213 12:50:32 @agent_ppo2.py:179][0m |           0.1256 |           6.6159 |          -0.6791 |
[32m[20221213 12:50:33 @agent_ppo2.py:179][0m |           0.1338 |           6.5988 |          -1.2328 |
[32m[20221213 12:50:33 @agent_ppo2.py:179][0m |           0.1277 |           6.5918 |          -0.5008 |
[32m[20221213 12:50:33 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:50:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 24.32
[32m[20221213 12:50:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 25.53
[32m[20221213 12:50:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.72
[32m[20221213 12:50:33 @agent_ppo2.py:137][0m Total time:      34.60 min
[32m[20221213 12:50:33 @agent_ppo2.py:139][0m 2377728 total steps have happened
[32m[20221213 12:50:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1161 --------------------------#
[32m[20221213 12:50:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:33 @agent_ppo2.py:179][0m |           0.0955 |          24.6162 |         -29.3731 |
[32m[20221213 12:50:33 @agent_ppo2.py:179][0m |           0.0400 |          22.8289 |         -24.7300 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |           0.0165 |          22.2842 |         -26.9699 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |           0.0119 |          21.9524 |         -24.6571 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |           0.0211 |          21.7446 |         -20.4747 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |          -0.0317 |          21.5542 |         -21.7551 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |          -0.0463 |          21.4245 |         -23.2160 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |          -0.0377 |          25.1325 |         -24.8381 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |          -0.0438 |          21.3248 |         -25.3198 |
[32m[20221213 12:50:34 @agent_ppo2.py:179][0m |          -0.0435 |          21.0289 |         -26.2172 |
[32m[20221213 12:50:34 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:50:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 197.91
[32m[20221213 12:50:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.36
[32m[20221213 12:50:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 23.15
[32m[20221213 12:50:35 @agent_ppo2.py:137][0m Total time:      34.63 min
[32m[20221213 12:50:35 @agent_ppo2.py:139][0m 2379776 total steps have happened
[32m[20221213 12:50:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1162 --------------------------#
[32m[20221213 12:50:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:35 @agent_ppo2.py:179][0m |           0.0620 |          33.3024 |         -33.7107 |
[32m[20221213 12:50:35 @agent_ppo2.py:179][0m |           0.0458 |          35.0905 |         -26.9469 |
[32m[20221213 12:50:35 @agent_ppo2.py:179][0m |          -0.0039 |          31.0372 |         -30.0504 |
[32m[20221213 12:50:35 @agent_ppo2.py:179][0m |          -0.0063 |          33.9063 |         -32.4431 |
[32m[20221213 12:50:36 @agent_ppo2.py:179][0m |          -0.0322 |          30.2915 |         -34.6014 |
[32m[20221213 12:50:36 @agent_ppo2.py:179][0m |          -0.0338 |          30.0505 |         -36.1063 |
[32m[20221213 12:50:36 @agent_ppo2.py:179][0m |          -0.0333 |          32.0445 |         -38.0743 |
[32m[20221213 12:50:36 @agent_ppo2.py:179][0m |          -0.0362 |          29.9629 |         -38.8918 |
[32m[20221213 12:50:36 @agent_ppo2.py:179][0m |          -0.0375 |          29.4290 |         -38.5195 |
[32m[20221213 12:50:36 @agent_ppo2.py:179][0m |          -0.0486 |          29.3366 |         -40.1938 |
[32m[20221213 12:50:36 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:50:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 306.27
[32m[20221213 12:50:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.54
[32m[20221213 12:50:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.14
[32m[20221213 12:50:36 @agent_ppo2.py:137][0m Total time:      34.66 min
[32m[20221213 12:50:36 @agent_ppo2.py:139][0m 2381824 total steps have happened
[32m[20221213 12:50:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1163 --------------------------#
[32m[20221213 12:50:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:37 @agent_ppo2.py:179][0m |           0.0681 |          28.7078 |         -29.7891 |
[32m[20221213 12:50:37 @agent_ppo2.py:179][0m |           0.0578 |          28.4058 |         -27.4670 |
[32m[20221213 12:50:37 @agent_ppo2.py:179][0m |           0.0063 |          26.2659 |         -31.7482 |
[32m[20221213 12:50:37 @agent_ppo2.py:179][0m |          -0.0087 |          25.8250 |         -33.4912 |
[32m[20221213 12:50:37 @agent_ppo2.py:179][0m |          -0.0120 |          25.6081 |         -33.5716 |
[32m[20221213 12:50:37 @agent_ppo2.py:179][0m |          -0.0177 |          27.8005 |         -35.5096 |
[32m[20221213 12:50:37 @agent_ppo2.py:179][0m |          -0.0256 |          25.2068 |         -28.1747 |
[32m[20221213 12:50:38 @agent_ppo2.py:179][0m |          -0.0409 |          25.0734 |         -28.8180 |
[32m[20221213 12:50:38 @agent_ppo2.py:179][0m |          -0.0409 |          26.0834 |         -29.9100 |
[32m[20221213 12:50:38 @agent_ppo2.py:179][0m |          -0.0391 |          26.9432 |         -30.7428 |
[32m[20221213 12:50:38 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:50:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.81
[32m[20221213 12:50:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.27
[32m[20221213 12:50:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.58
[32m[20221213 12:50:38 @agent_ppo2.py:137][0m Total time:      34.69 min
[32m[20221213 12:50:38 @agent_ppo2.py:139][0m 2383872 total steps have happened
[32m[20221213 12:50:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1164 --------------------------#
[32m[20221213 12:50:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |           0.0977 |          33.5856 |         -26.3568 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |           0.0709 |          32.2982 |         -22.9477 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |           0.0247 |          31.6078 |         -29.2094 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |           0.0037 |          32.3802 |         -31.8555 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |          -0.0148 |          30.8586 |         -32.1031 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |          -0.0131 |          32.6153 |         -34.0697 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |          -0.0176 |          30.3504 |         -33.7380 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |          -0.0302 |          30.2164 |         -35.7510 |
[32m[20221213 12:50:39 @agent_ppo2.py:179][0m |          -0.0347 |          29.9880 |         -36.1129 |
[32m[20221213 12:50:40 @agent_ppo2.py:179][0m |          -0.0359 |          30.0180 |         -37.4995 |
[32m[20221213 12:50:40 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:50:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.60
[32m[20221213 12:50:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.92
[32m[20221213 12:50:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.12
[32m[20221213 12:50:40 @agent_ppo2.py:137][0m Total time:      34.72 min
[32m[20221213 12:50:40 @agent_ppo2.py:139][0m 2385920 total steps have happened
[32m[20221213 12:50:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1165 --------------------------#
[32m[20221213 12:50:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:40 @agent_ppo2.py:179][0m |           0.0695 |          29.9043 |         -22.8884 |
[32m[20221213 12:50:40 @agent_ppo2.py:179][0m |           0.0161 |          28.9913 |         -19.8413 |
[32m[20221213 12:50:40 @agent_ppo2.py:179][0m |          -0.0151 |          28.2640 |         -22.8096 |
[32m[20221213 12:50:41 @agent_ppo2.py:179][0m |          -0.0212 |          27.9745 |         -24.5879 |
[32m[20221213 12:50:41 @agent_ppo2.py:179][0m |          -0.0350 |          27.6394 |         -25.6506 |
[32m[20221213 12:50:41 @agent_ppo2.py:179][0m |          -0.0436 |          27.4045 |         -27.4727 |
[32m[20221213 12:50:41 @agent_ppo2.py:179][0m |          -0.0491 |          27.3020 |         -28.1729 |
[32m[20221213 12:50:41 @agent_ppo2.py:179][0m |          -0.0527 |          27.1900 |         -29.2188 |
[32m[20221213 12:50:41 @agent_ppo2.py:179][0m |          -0.0472 |          27.0345 |         -30.7167 |
[32m[20221213 12:50:41 @agent_ppo2.py:179][0m |          -0.0528 |          26.9221 |         -30.2523 |
[32m[20221213 12:50:41 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:50:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 246.35
[32m[20221213 12:50:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.88
[32m[20221213 12:50:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.99
[32m[20221213 12:50:42 @agent_ppo2.py:137][0m Total time:      34.75 min
[32m[20221213 12:50:42 @agent_ppo2.py:139][0m 2387968 total steps have happened
[32m[20221213 12:50:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1166 --------------------------#
[32m[20221213 12:50:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:42 @agent_ppo2.py:179][0m |           0.0697 |          17.3016 |         -21.5841 |
[32m[20221213 12:50:42 @agent_ppo2.py:179][0m |           0.0380 |          16.2776 |         -12.8997 |
[32m[20221213 12:50:42 @agent_ppo2.py:179][0m |           0.0052 |          15.9751 |         -13.3082 |
[32m[20221213 12:50:42 @agent_ppo2.py:179][0m |          -0.0289 |          15.6912 |         -15.5487 |
[32m[20221213 12:50:42 @agent_ppo2.py:179][0m |          -0.0433 |          15.5633 |         -17.2398 |
[32m[20221213 12:50:43 @agent_ppo2.py:179][0m |          -0.0518 |          15.3978 |         -17.8306 |
[32m[20221213 12:50:43 @agent_ppo2.py:179][0m |          -0.0460 |          19.1692 |         -19.1968 |
[32m[20221213 12:50:43 @agent_ppo2.py:179][0m |          -0.0595 |          15.2679 |         -19.1679 |
[32m[20221213 12:50:43 @agent_ppo2.py:179][0m |          -0.0595 |          15.1062 |         -19.5554 |
[32m[20221213 12:50:43 @agent_ppo2.py:179][0m |          -0.0606 |          15.2256 |         -20.4101 |
[32m[20221213 12:50:43 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:50:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 152.92
[32m[20221213 12:50:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.92
[32m[20221213 12:50:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.23
[32m[20221213 12:50:43 @agent_ppo2.py:137][0m Total time:      34.78 min
[32m[20221213 12:50:43 @agent_ppo2.py:139][0m 2390016 total steps have happened
[32m[20221213 12:50:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1167 --------------------------#
[32m[20221213 12:50:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:44 @agent_ppo2.py:179][0m |           0.0926 |          32.2546 |         -36.5713 |
[32m[20221213 12:50:44 @agent_ppo2.py:179][0m |           0.0683 |          30.1123 |         -24.5098 |
[32m[20221213 12:50:44 @agent_ppo2.py:179][0m |           0.0279 |          29.2008 |         -27.2539 |
[32m[20221213 12:50:44 @agent_ppo2.py:179][0m |           0.0081 |          28.8910 |         -33.2515 |
[32m[20221213 12:50:44 @agent_ppo2.py:179][0m |          -0.0110 |          28.4564 |         -37.6768 |
[32m[20221213 12:50:44 @agent_ppo2.py:179][0m |          -0.0201 |          28.2300 |         -40.0943 |
[32m[20221213 12:50:44 @agent_ppo2.py:179][0m |          -0.0278 |          28.0169 |         -42.4170 |
[32m[20221213 12:50:45 @agent_ppo2.py:179][0m |          -0.0290 |          27.7709 |         -42.2825 |
[32m[20221213 12:50:45 @agent_ppo2.py:179][0m |          -0.0296 |          27.4946 |         -44.3674 |
[32m[20221213 12:50:45 @agent_ppo2.py:179][0m |          -0.0334 |          27.3895 |         -45.3873 |
[32m[20221213 12:50:45 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.67
[32m[20221213 12:50:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.30
[32m[20221213 12:50:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.42
[32m[20221213 12:50:45 @agent_ppo2.py:137][0m Total time:      34.80 min
[32m[20221213 12:50:45 @agent_ppo2.py:139][0m 2392064 total steps have happened
[32m[20221213 12:50:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1168 --------------------------#
[32m[20221213 12:50:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:45 @agent_ppo2.py:179][0m |           0.0961 |          12.0233 |         -18.5780 |
[32m[20221213 12:50:45 @agent_ppo2.py:179][0m |           0.0668 |          10.3091 |          -8.4899 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |           0.0353 |           9.7940 |          -9.8426 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |           0.0112 |           9.4629 |         -11.2277 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |           0.0102 |           9.3164 |         -11.3955 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |          -0.0008 |           9.0470 |         -11.9551 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |          -0.0165 |           9.0404 |         -12.7302 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |          -0.0101 |           8.7433 |         -12.9844 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |          -0.0088 |           8.6153 |         -12.4358 |
[32m[20221213 12:50:46 @agent_ppo2.py:179][0m |          -0.0155 |           8.8110 |         -13.3283 |
[32m[20221213 12:50:46 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 78.67
[32m[20221213 12:50:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 288.45
[32m[20221213 12:50:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 314.22
[32m[20221213 12:50:47 @agent_ppo2.py:137][0m Total time:      34.83 min
[32m[20221213 12:50:47 @agent_ppo2.py:139][0m 2394112 total steps have happened
[32m[20221213 12:50:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1169 --------------------------#
[32m[20221213 12:50:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:47 @agent_ppo2.py:179][0m |           0.0774 |          33.2931 |         -36.6504 |
[32m[20221213 12:50:47 @agent_ppo2.py:179][0m |           0.0299 |          31.5625 |         -33.6873 |
[32m[20221213 12:50:47 @agent_ppo2.py:179][0m |           0.0090 |          30.8819 |         -35.6866 |
[32m[20221213 12:50:47 @agent_ppo2.py:179][0m |          -0.0131 |          30.4393 |         -36.9999 |
[32m[20221213 12:50:48 @agent_ppo2.py:179][0m |          -0.0256 |          30.3671 |         -40.3964 |
[32m[20221213 12:50:48 @agent_ppo2.py:179][0m |          -0.0293 |          30.1304 |         -41.6327 |
[32m[20221213 12:50:48 @agent_ppo2.py:179][0m |          -0.0327 |          29.9531 |         -43.2125 |
[32m[20221213 12:50:48 @agent_ppo2.py:179][0m |          -0.0363 |          29.8048 |         -43.2142 |
[32m[20221213 12:50:48 @agent_ppo2.py:179][0m |          -0.0340 |          29.7922 |         -43.8384 |
[32m[20221213 12:50:48 @agent_ppo2.py:179][0m |          -0.0394 |          29.6456 |         -45.5556 |
[32m[20221213 12:50:48 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:50:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 354.60
[32m[20221213 12:50:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 368.75
[32m[20221213 12:50:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 377.68
[32m[20221213 12:50:48 @agent_ppo2.py:137][0m Total time:      34.86 min
[32m[20221213 12:50:48 @agent_ppo2.py:139][0m 2396160 total steps have happened
[32m[20221213 12:50:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1170 --------------------------#
[32m[20221213 12:50:49 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:50:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:49 @agent_ppo2.py:179][0m |           0.0934 |          17.6529 |         -33.5289 |
[32m[20221213 12:50:49 @agent_ppo2.py:179][0m |           0.0830 |          16.0600 |         -18.2506 |
[32m[20221213 12:50:49 @agent_ppo2.py:179][0m |           0.0289 |          15.5665 |         -19.4045 |
[32m[20221213 12:50:49 @agent_ppo2.py:179][0m |          -0.0004 |          15.1046 |         -22.8947 |
[32m[20221213 12:50:49 @agent_ppo2.py:179][0m |          -0.0161 |          14.7450 |         -24.3823 |
[32m[20221213 12:50:49 @agent_ppo2.py:179][0m |          -0.0334 |          14.4929 |         -26.2367 |
[32m[20221213 12:50:49 @agent_ppo2.py:179][0m |          -0.0312 |          14.2347 |         -27.7800 |
[32m[20221213 12:50:50 @agent_ppo2.py:179][0m |          -0.0382 |          13.9382 |         -29.7212 |
[32m[20221213 12:50:50 @agent_ppo2.py:179][0m |          -0.0420 |          13.7544 |         -31.6738 |
[32m[20221213 12:50:50 @agent_ppo2.py:179][0m |          -0.0446 |          13.5790 |         -30.8717 |
[32m[20221213 12:50:50 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 141.89
[32m[20221213 12:50:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 313.89
[32m[20221213 12:50:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 372.49
[32m[20221213 12:50:50 @agent_ppo2.py:137][0m Total time:      34.89 min
[32m[20221213 12:50:50 @agent_ppo2.py:139][0m 2398208 total steps have happened
[32m[20221213 12:50:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1171 --------------------------#
[32m[20221213 12:50:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:50 @agent_ppo2.py:179][0m |           0.0566 |          34.0300 |         -43.4128 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |           0.0407 |          32.6431 |         -37.8870 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |           0.0072 |          32.0261 |         -39.5253 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |           0.0070 |          34.4231 |         -42.9169 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |          -0.0144 |          31.4967 |         -42.0129 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |          -0.0267 |          31.2865 |         -45.7084 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |          -0.0364 |          31.2367 |         -47.3910 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |          -0.0357 |          31.0000 |         -47.5677 |
[32m[20221213 12:50:51 @agent_ppo2.py:179][0m |          -0.0400 |          30.7773 |         -48.6160 |
[32m[20221213 12:50:52 @agent_ppo2.py:179][0m |          -0.0343 |          31.1486 |         -50.0212 |
[32m[20221213 12:50:52 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:50:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.59
[32m[20221213 12:50:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.78
[32m[20221213 12:50:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.73
[32m[20221213 12:50:52 @agent_ppo2.py:137][0m Total time:      34.92 min
[32m[20221213 12:50:52 @agent_ppo2.py:139][0m 2400256 total steps have happened
[32m[20221213 12:50:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1172 --------------------------#
[32m[20221213 12:50:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:52 @agent_ppo2.py:179][0m |           0.0681 |          33.2187 |         -42.7317 |
[32m[20221213 12:50:52 @agent_ppo2.py:179][0m |           0.1075 |          32.4172 |         -24.0244 |
[32m[20221213 12:50:52 @agent_ppo2.py:179][0m |           0.0352 |          32.3471 |         -28.6082 |
[32m[20221213 12:50:53 @agent_ppo2.py:179][0m |           0.0115 |          31.8757 |         -35.7215 |
[32m[20221213 12:50:53 @agent_ppo2.py:179][0m |          -0.0101 |          31.2848 |         -39.5059 |
[32m[20221213 12:50:53 @agent_ppo2.py:179][0m |          -0.0045 |          32.5897 |         -42.0125 |
[32m[20221213 12:50:53 @agent_ppo2.py:179][0m |          -0.0248 |          31.1816 |         -43.7297 |
[32m[20221213 12:50:53 @agent_ppo2.py:179][0m |          -0.0314 |          30.7945 |         -44.9815 |
[32m[20221213 12:50:53 @agent_ppo2.py:179][0m |          -0.0290 |          30.7214 |         -47.1737 |
[32m[20221213 12:50:53 @agent_ppo2.py:179][0m |          -0.0370 |          30.5632 |         -47.2588 |
[32m[20221213 12:50:53 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:50:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 324.25
[32m[20221213 12:50:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.60
[32m[20221213 12:50:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 24.57
[32m[20221213 12:50:54 @agent_ppo2.py:137][0m Total time:      34.95 min
[32m[20221213 12:50:54 @agent_ppo2.py:139][0m 2402304 total steps have happened
[32m[20221213 12:50:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1173 --------------------------#
[32m[20221213 12:50:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:54 @agent_ppo2.py:179][0m |           0.0690 |          18.2299 |         -26.1668 |
[32m[20221213 12:50:54 @agent_ppo2.py:179][0m |           0.0211 |          17.1641 |         -16.1867 |
[32m[20221213 12:50:54 @agent_ppo2.py:179][0m |          -0.0003 |          20.2169 |         -18.1853 |
[32m[20221213 12:50:54 @agent_ppo2.py:179][0m |          -0.0334 |          16.6053 |         -20.4864 |
[32m[20221213 12:50:54 @agent_ppo2.py:179][0m |          -0.0462 |          16.2877 |         -21.3488 |
[32m[20221213 12:50:55 @agent_ppo2.py:179][0m |          -0.0422 |          16.1510 |         -22.7270 |
[32m[20221213 12:50:55 @agent_ppo2.py:179][0m |          -0.0538 |          15.9487 |         -23.5969 |
[32m[20221213 12:50:55 @agent_ppo2.py:179][0m |          -0.0563 |          15.8768 |         -24.4154 |
[32m[20221213 12:50:55 @agent_ppo2.py:179][0m |          -0.0563 |          15.7450 |         -25.5016 |
[32m[20221213 12:50:55 @agent_ppo2.py:179][0m |          -0.0624 |          15.6022 |         -26.1637 |
[32m[20221213 12:50:55 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 146.86
[32m[20221213 12:50:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.74
[32m[20221213 12:50:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 371.59
[32m[20221213 12:50:55 @agent_ppo2.py:137][0m Total time:      34.98 min
[32m[20221213 12:50:55 @agent_ppo2.py:139][0m 2404352 total steps have happened
[32m[20221213 12:50:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1174 --------------------------#
[32m[20221213 12:50:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:56 @agent_ppo2.py:179][0m |           0.0742 |          30.6137 |         -46.7188 |
[32m[20221213 12:50:56 @agent_ppo2.py:179][0m |           0.0582 |          28.7986 |         -38.4816 |
[32m[20221213 12:50:56 @agent_ppo2.py:179][0m |           0.0221 |          28.1470 |         -45.2304 |
[32m[20221213 12:50:56 @agent_ppo2.py:179][0m |           0.0058 |          27.6549 |         -46.0778 |
[32m[20221213 12:50:56 @agent_ppo2.py:179][0m |          -0.0174 |          27.4957 |         -50.3437 |
[32m[20221213 12:50:56 @agent_ppo2.py:179][0m |          -0.0165 |          26.9556 |         -49.8772 |
[32m[20221213 12:50:56 @agent_ppo2.py:179][0m |          -0.0200 |          26.8383 |         -51.4693 |
[32m[20221213 12:50:57 @agent_ppo2.py:179][0m |          -0.0169 |          26.2581 |         -51.2242 |
[32m[20221213 12:50:57 @agent_ppo2.py:179][0m |          -0.0356 |          26.0730 |         -43.4715 |
[32m[20221213 12:50:57 @agent_ppo2.py:179][0m |          -0.0353 |          29.2077 |         -45.0634 |
[32m[20221213 12:50:57 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:50:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 237.68
[32m[20221213 12:50:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.93
[32m[20221213 12:50:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.00
[32m[20221213 12:50:57 @agent_ppo2.py:137][0m Total time:      35.00 min
[32m[20221213 12:50:57 @agent_ppo2.py:139][0m 2406400 total steps have happened
[32m[20221213 12:50:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1175 --------------------------#
[32m[20221213 12:50:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:57 @agent_ppo2.py:179][0m |           0.0572 |          36.1240 |         -45.3874 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |           0.0225 |          34.3042 |         -39.7284 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0025 |          33.7564 |         -44.6596 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0215 |          33.3422 |         -47.5237 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0308 |          33.0440 |         -47.4652 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0391 |          32.9292 |         -48.9391 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0338 |          32.8052 |         -48.6079 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0423 |          32.4532 |         -50.9784 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0423 |          32.4036 |         -52.0263 |
[32m[20221213 12:50:58 @agent_ppo2.py:179][0m |          -0.0342 |          32.3809 |         -51.3913 |
[32m[20221213 12:50:58 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:50:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.20
[32m[20221213 12:50:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.05
[32m[20221213 12:50:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.68
[32m[20221213 12:50:59 @agent_ppo2.py:137][0m Total time:      35.03 min
[32m[20221213 12:50:59 @agent_ppo2.py:139][0m 2408448 total steps have happened
[32m[20221213 12:50:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1176 --------------------------#
[32m[20221213 12:50:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:50:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:50:59 @agent_ppo2.py:179][0m |           0.0735 |          35.6511 |         -37.7610 |
[32m[20221213 12:50:59 @agent_ppo2.py:179][0m |           0.0691 |          34.2281 |         -25.8203 |
[32m[20221213 12:50:59 @agent_ppo2.py:179][0m |           0.0294 |          34.1140 |         -27.3377 |
[32m[20221213 12:50:59 @agent_ppo2.py:179][0m |          -0.0033 |          33.4970 |         -33.9937 |
[32m[20221213 12:51:00 @agent_ppo2.py:179][0m |          -0.0139 |          33.0921 |         -39.0954 |
[32m[20221213 12:51:00 @agent_ppo2.py:179][0m |          -0.0191 |          32.9043 |         -39.3200 |
[32m[20221213 12:51:00 @agent_ppo2.py:179][0m |          -0.0272 |          32.9163 |         -42.4485 |
[32m[20221213 12:51:00 @agent_ppo2.py:179][0m |          -0.0306 |          32.4505 |         -43.3061 |
[32m[20221213 12:51:00 @agent_ppo2.py:179][0m |          -0.0249 |          35.0163 |         -45.7914 |
[32m[20221213 12:51:00 @agent_ppo2.py:179][0m |          -0.0362 |          32.9734 |         -46.6914 |
[32m[20221213 12:51:00 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:51:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.05
[32m[20221213 12:51:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.27
[32m[20221213 12:51:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 162.81
[32m[20221213 12:51:00 @agent_ppo2.py:137][0m Total time:      35.06 min
[32m[20221213 12:51:00 @agent_ppo2.py:139][0m 2410496 total steps have happened
[32m[20221213 12:51:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1177 --------------------------#
[32m[20221213 12:51:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:01 @agent_ppo2.py:179][0m |           0.1058 |          35.4615 |         -36.8465 |
[32m[20221213 12:51:01 @agent_ppo2.py:179][0m |           0.1137 |          35.1837 |         -20.0936 |
[32m[20221213 12:51:01 @agent_ppo2.py:179][0m |           0.0300 |          32.8822 |         -27.5423 |
[32m[20221213 12:51:01 @agent_ppo2.py:179][0m |           0.0082 |          32.4398 |         -35.5185 |
[32m[20221213 12:51:01 @agent_ppo2.py:179][0m |          -0.0056 |          31.9838 |         -36.4933 |
[32m[20221213 12:51:01 @agent_ppo2.py:179][0m |          -0.0167 |          31.7616 |         -39.2957 |
[32m[20221213 12:51:02 @agent_ppo2.py:179][0m |          -0.0262 |          31.3503 |         -41.5891 |
[32m[20221213 12:51:02 @agent_ppo2.py:179][0m |          -0.0313 |          31.2632 |         -43.4350 |
[32m[20221213 12:51:02 @agent_ppo2.py:179][0m |          -0.0301 |          31.3082 |         -44.6262 |
[32m[20221213 12:51:02 @agent_ppo2.py:179][0m |          -0.0363 |          30.7990 |         -45.5940 |
[32m[20221213 12:51:02 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:51:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.50
[32m[20221213 12:51:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.54
[32m[20221213 12:51:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 25.87
[32m[20221213 12:51:02 @agent_ppo2.py:137][0m Total time:      35.09 min
[32m[20221213 12:51:02 @agent_ppo2.py:139][0m 2412544 total steps have happened
[32m[20221213 12:51:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1178 --------------------------#
[32m[20221213 12:51:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |           0.0464 |          28.9153 |         -39.2134 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |           0.0419 |          26.6876 |         -28.1700 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |           0.0142 |          25.8728 |         -28.6112 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |          -0.0159 |          25.6987 |         -30.7893 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |          -0.0324 |          24.9424 |         -32.5837 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |          -0.0361 |          25.3664 |         -34.1254 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |          -0.0375 |          26.0050 |         -35.8563 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |          -0.0524 |          23.9687 |         -36.3228 |
[32m[20221213 12:51:03 @agent_ppo2.py:179][0m |          -0.0528 |          23.7379 |         -37.4560 |
[32m[20221213 12:51:04 @agent_ppo2.py:179][0m |          -0.0517 |          23.5235 |         -38.3502 |
[32m[20221213 12:51:04 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:51:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 246.33
[32m[20221213 12:51:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.11
[32m[20221213 12:51:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 226.33
[32m[20221213 12:51:04 @agent_ppo2.py:137][0m Total time:      35.12 min
[32m[20221213 12:51:04 @agent_ppo2.py:139][0m 2414592 total steps have happened
[32m[20221213 12:51:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1179 --------------------------#
[32m[20221213 12:51:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:04 @agent_ppo2.py:179][0m |           0.0558 |          34.3451 |         -36.4984 |
[32m[20221213 12:51:04 @agent_ppo2.py:179][0m |           0.0457 |          32.3646 |         -31.1181 |
[32m[20221213 12:51:04 @agent_ppo2.py:179][0m |           0.0348 |          36.9537 |         -33.9547 |
[32m[20221213 12:51:05 @agent_ppo2.py:179][0m |           0.0045 |          33.2107 |         -36.8308 |
[32m[20221213 12:51:05 @agent_ppo2.py:179][0m |          -0.0181 |          31.0104 |         -39.0035 |
[32m[20221213 12:51:05 @agent_ppo2.py:179][0m |          -0.0285 |          30.7656 |         -40.9773 |
[32m[20221213 12:51:05 @agent_ppo2.py:179][0m |          -0.0308 |          30.5742 |         -41.7654 |
[32m[20221213 12:51:05 @agent_ppo2.py:179][0m |          -0.0317 |          30.5514 |         -41.9700 |
[32m[20221213 12:51:05 @agent_ppo2.py:179][0m |          -0.0358 |          30.3019 |         -43.1469 |
[32m[20221213 12:51:05 @agent_ppo2.py:179][0m |          -0.0186 |          34.7586 |         -43.6778 |
[32m[20221213 12:51:05 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:51:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 335.28
[32m[20221213 12:51:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 367.46
[32m[20221213 12:51:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.97
[32m[20221213 12:51:05 @agent_ppo2.py:137][0m Total time:      35.15 min
[32m[20221213 12:51:05 @agent_ppo2.py:139][0m 2416640 total steps have happened
[32m[20221213 12:51:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1180 --------------------------#
[32m[20221213 12:51:06 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:51:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:06 @agent_ppo2.py:179][0m |           0.0687 |          34.3793 |         -32.5359 |
[32m[20221213 12:51:06 @agent_ppo2.py:179][0m |           0.0423 |          32.8515 |         -29.8407 |
[32m[20221213 12:51:06 @agent_ppo2.py:179][0m |           0.0149 |          32.3901 |         -31.9337 |
[32m[20221213 12:51:06 @agent_ppo2.py:179][0m |          -0.0061 |          32.1443 |         -35.5284 |
[32m[20221213 12:51:06 @agent_ppo2.py:179][0m |          -0.0226 |          31.9315 |         -36.5446 |
[32m[20221213 12:51:07 @agent_ppo2.py:179][0m |          -0.0275 |          31.7229 |         -37.3371 |
[32m[20221213 12:51:07 @agent_ppo2.py:179][0m |          -0.0311 |          31.6672 |         -38.6800 |
[32m[20221213 12:51:07 @agent_ppo2.py:179][0m |          -0.0220 |          31.8593 |         -37.3895 |
[32m[20221213 12:51:07 @agent_ppo2.py:179][0m |          -0.0251 |          31.7530 |         -38.9916 |
[32m[20221213 12:51:07 @agent_ppo2.py:179][0m |          -0.0401 |          31.4507 |         -40.3497 |
[32m[20221213 12:51:07 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:51:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.65
[32m[20221213 12:51:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.02
[32m[20221213 12:51:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.83
[32m[20221213 12:51:07 @agent_ppo2.py:137][0m Total time:      35.18 min
[32m[20221213 12:51:07 @agent_ppo2.py:139][0m 2418688 total steps have happened
[32m[20221213 12:51:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1181 --------------------------#
[32m[20221213 12:51:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |           0.0618 |          33.6301 |         -31.7736 |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |           0.0474 |          32.8989 |         -27.9628 |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |           0.0255 |          32.4195 |         -29.8369 |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |          -0.0075 |          32.1834 |         -32.6440 |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |          -0.0183 |          31.9972 |         -34.8143 |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |          -0.0214 |          31.9611 |         -35.2174 |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |          -0.0294 |          31.8414 |         -35.7307 |
[32m[20221213 12:51:08 @agent_ppo2.py:179][0m |          -0.0293 |          31.7566 |         -36.2869 |
[32m[20221213 12:51:09 @agent_ppo2.py:179][0m |          -0.0253 |          31.6056 |         -36.1130 |
[32m[20221213 12:51:09 @agent_ppo2.py:179][0m |          -0.0366 |          31.5598 |         -36.5295 |
[32m[20221213 12:51:09 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:51:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.39
[32m[20221213 12:51:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 367.72
[32m[20221213 12:51:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.38
[32m[20221213 12:51:09 @agent_ppo2.py:137][0m Total time:      35.20 min
[32m[20221213 12:51:09 @agent_ppo2.py:139][0m 2420736 total steps have happened
[32m[20221213 12:51:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1182 --------------------------#
[32m[20221213 12:51:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:09 @agent_ppo2.py:179][0m |           0.0876 |          33.7062 |         -30.9071 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |           0.0786 |          32.9502 |         -18.4755 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |           0.0199 |          32.4556 |         -25.6633 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |           0.0024 |          32.2290 |         -29.3321 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |          -0.0075 |          32.1238 |         -31.7069 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |          -0.0214 |          31.9952 |         -33.4841 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |          -0.0265 |          31.8048 |         -35.3651 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |          -0.0262 |          33.4502 |         -36.5348 |
[32m[20221213 12:51:10 @agent_ppo2.py:179][0m |          -0.0266 |          31.7127 |         -36.1279 |
[32m[20221213 12:51:11 @agent_ppo2.py:179][0m |          -0.0312 |          31.8101 |         -37.5195 |
[32m[20221213 12:51:11 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:51:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.09
[32m[20221213 12:51:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.19
[32m[20221213 12:51:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.14
[32m[20221213 12:51:11 @agent_ppo2.py:137][0m Total time:      35.23 min
[32m[20221213 12:51:11 @agent_ppo2.py:139][0m 2422784 total steps have happened
[32m[20221213 12:51:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1183 --------------------------#
[32m[20221213 12:51:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:11 @agent_ppo2.py:179][0m |           0.0715 |          31.4416 |         -32.9428 |
[32m[20221213 12:51:11 @agent_ppo2.py:179][0m |           0.0351 |          30.0263 |         -31.8485 |
[32m[20221213 12:51:11 @agent_ppo2.py:179][0m |           0.0060 |          29.1618 |         -31.2671 |
[32m[20221213 12:51:12 @agent_ppo2.py:179][0m |          -0.0137 |          28.7016 |         -35.9442 |
[32m[20221213 12:51:12 @agent_ppo2.py:179][0m |          -0.0220 |          28.2135 |         -36.9445 |
[32m[20221213 12:51:12 @agent_ppo2.py:179][0m |          -0.0328 |          27.8075 |         -38.0905 |
[32m[20221213 12:51:12 @agent_ppo2.py:179][0m |          -0.0352 |          27.6211 |         -38.0946 |
[32m[20221213 12:51:12 @agent_ppo2.py:179][0m |          -0.0388 |          27.3975 |         -39.2047 |
[32m[20221213 12:51:12 @agent_ppo2.py:179][0m |          -0.0374 |          27.4086 |         -39.4628 |
[32m[20221213 12:51:12 @agent_ppo2.py:179][0m |          -0.0425 |          27.1023 |         -40.1240 |
[32m[20221213 12:51:12 @agent_ppo2.py:124][0m Policy update time: 1.47 s
[32m[20221213 12:51:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 324.20
[32m[20221213 12:51:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.98
[32m[20221213 12:51:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 318.44
[32m[20221213 12:51:13 @agent_ppo2.py:137][0m Total time:      35.27 min
[32m[20221213 12:51:13 @agent_ppo2.py:139][0m 2424832 total steps have happened
[32m[20221213 12:51:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1184 --------------------------#
[32m[20221213 12:51:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:13 @agent_ppo2.py:179][0m |           0.0615 |          34.2942 |         -30.8205 |
[32m[20221213 12:51:13 @agent_ppo2.py:179][0m |           0.0463 |          33.0960 |         -27.7517 |
[32m[20221213 12:51:13 @agent_ppo2.py:179][0m |          -0.0014 |          31.6098 |         -35.2151 |
[32m[20221213 12:51:13 @agent_ppo2.py:179][0m |          -0.0167 |          31.0123 |         -37.1050 |
[32m[20221213 12:51:14 @agent_ppo2.py:179][0m |          -0.0311 |          30.2168 |         -38.6705 |
[32m[20221213 12:51:14 @agent_ppo2.py:179][0m |          -0.0360 |          29.6581 |         -40.3588 |
[32m[20221213 12:51:14 @agent_ppo2.py:179][0m |          -0.0354 |          29.2463 |         -40.8728 |
[32m[20221213 12:51:14 @agent_ppo2.py:179][0m |          -0.0400 |          29.0950 |         -41.4025 |
[32m[20221213 12:51:14 @agent_ppo2.py:179][0m |          -0.0454 |          28.6817 |         -42.8600 |
[32m[20221213 12:51:14 @agent_ppo2.py:179][0m |          -0.0541 |          28.2439 |         -41.8704 |
[32m[20221213 12:51:14 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:51:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.46
[32m[20221213 12:51:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.64
[32m[20221213 12:51:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.76
[32m[20221213 12:51:14 @agent_ppo2.py:137][0m Total time:      35.29 min
[32m[20221213 12:51:14 @agent_ppo2.py:139][0m 2426880 total steps have happened
[32m[20221213 12:51:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1185 --------------------------#
[32m[20221213 12:51:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:15 @agent_ppo2.py:179][0m |           0.0735 |          34.2926 |         -36.3320 |
[32m[20221213 12:51:15 @agent_ppo2.py:179][0m |           0.0891 |          33.2163 |         -21.4214 |
[32m[20221213 12:51:15 @agent_ppo2.py:179][0m |           0.0335 |          32.3225 |         -30.2132 |
[32m[20221213 12:51:15 @agent_ppo2.py:179][0m |          -0.0008 |          31.9366 |         -36.9611 |
[32m[20221213 12:51:15 @agent_ppo2.py:179][0m |          -0.0167 |          31.5195 |         -39.9567 |
[32m[20221213 12:51:16 @agent_ppo2.py:179][0m |          -0.0241 |          31.3991 |         -39.7904 |
[32m[20221213 12:51:16 @agent_ppo2.py:179][0m |          -0.0270 |          31.1077 |         -42.1586 |
[32m[20221213 12:51:16 @agent_ppo2.py:179][0m |          -0.0358 |          30.9346 |         -42.7659 |
[32m[20221213 12:51:16 @agent_ppo2.py:179][0m |          -0.0322 |          30.8173 |         -43.5240 |
[32m[20221213 12:51:16 @agent_ppo2.py:179][0m |          -0.0423 |          30.5953 |         -45.4997 |
[32m[20221213 12:51:16 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:51:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.73
[32m[20221213 12:51:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.89
[32m[20221213 12:51:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.25
[32m[20221213 12:51:16 @agent_ppo2.py:137][0m Total time:      35.32 min
[32m[20221213 12:51:16 @agent_ppo2.py:139][0m 2428928 total steps have happened
[32m[20221213 12:51:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1186 --------------------------#
[32m[20221213 12:51:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |           0.0557 |          38.3042 |         -33.8882 |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |           0.0257 |          33.2034 |         -28.6399 |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |           0.0003 |          35.0647 |         -30.7416 |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |          -0.0110 |          34.3338 |         -30.8842 |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |          -0.0328 |          31.6589 |         -32.6594 |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |          -0.0323 |          31.2249 |         -31.9991 |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |          -0.0396 |          31.0443 |         -33.0807 |
[32m[20221213 12:51:17 @agent_ppo2.py:179][0m |          -0.0445 |          30.7879 |         -35.1505 |
[32m[20221213 12:51:18 @agent_ppo2.py:179][0m |          -0.0422 |          30.5687 |         -34.8573 |
[32m[20221213 12:51:18 @agent_ppo2.py:179][0m |          -0.0502 |          30.3407 |         -35.6906 |
[32m[20221213 12:51:18 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:51:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 295.39
[32m[20221213 12:51:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.52
[32m[20221213 12:51:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 269.99
[32m[20221213 12:51:18 @agent_ppo2.py:137][0m Total time:      35.35 min
[32m[20221213 12:51:18 @agent_ppo2.py:139][0m 2430976 total steps have happened
[32m[20221213 12:51:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1187 --------------------------#
[32m[20221213 12:51:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:18 @agent_ppo2.py:179][0m |           0.0664 |          34.5981 |         -33.2705 |
[32m[20221213 12:51:18 @agent_ppo2.py:179][0m |           0.0525 |          33.3678 |         -28.2522 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |           0.0222 |          32.9937 |         -31.8928 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |           0.0088 |          35.9628 |         -34.9452 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |          -0.0120 |          32.3635 |         -35.4107 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |          -0.0244 |          32.0934 |         -37.4287 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |          -0.0280 |          31.8624 |         -38.9466 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |          -0.0307 |          31.6941 |         -39.9629 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |          -0.0401 |          31.6399 |         -42.9442 |
[32m[20221213 12:51:19 @agent_ppo2.py:179][0m |          -0.0416 |          31.5079 |         -43.4125 |
[32m[20221213 12:51:19 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:51:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.01
[32m[20221213 12:51:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.63
[32m[20221213 12:51:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.80
[32m[20221213 12:51:20 @agent_ppo2.py:137][0m Total time:      35.38 min
[32m[20221213 12:51:20 @agent_ppo2.py:139][0m 2433024 total steps have happened
[32m[20221213 12:51:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1188 --------------------------#
[32m[20221213 12:51:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:20 @agent_ppo2.py:179][0m |           0.0477 |          33.1292 |         -38.4324 |
[32m[20221213 12:51:20 @agent_ppo2.py:179][0m |           0.0435 |          31.7698 |         -31.0669 |
[32m[20221213 12:51:20 @agent_ppo2.py:179][0m |          -0.0015 |          30.4932 |         -36.5079 |
[32m[20221213 12:51:20 @agent_ppo2.py:179][0m |          -0.0106 |          31.1229 |         -39.1710 |
[32m[20221213 12:51:21 @agent_ppo2.py:179][0m |          -0.0133 |          31.1237 |         -40.3640 |
[32m[20221213 12:51:21 @agent_ppo2.py:179][0m |          -0.0120 |          33.3111 |         -40.6753 |
[32m[20221213 12:51:21 @agent_ppo2.py:179][0m |          -0.0386 |          29.1616 |         -42.6891 |
[32m[20221213 12:51:21 @agent_ppo2.py:179][0m |          -0.0417 |          28.8443 |         -43.9289 |
[32m[20221213 12:51:21 @agent_ppo2.py:179][0m |          -0.0361 |          28.6556 |         -43.8361 |
[32m[20221213 12:51:21 @agent_ppo2.py:179][0m |          -0.0421 |          28.4409 |         -46.5936 |
[32m[20221213 12:51:21 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:51:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 311.19
[32m[20221213 12:51:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.66
[32m[20221213 12:51:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.79
[32m[20221213 12:51:21 @agent_ppo2.py:137][0m Total time:      35.41 min
[32m[20221213 12:51:21 @agent_ppo2.py:139][0m 2435072 total steps have happened
[32m[20221213 12:51:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1189 --------------------------#
[32m[20221213 12:51:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:22 @agent_ppo2.py:179][0m |           0.0578 |          35.6315 |         -39.4310 |
[32m[20221213 12:51:22 @agent_ppo2.py:179][0m |           0.0476 |          33.8407 |         -31.5432 |
[32m[20221213 12:51:22 @agent_ppo2.py:179][0m |           0.0111 |          33.1410 |         -35.8025 |
[32m[20221213 12:51:22 @agent_ppo2.py:179][0m |          -0.0045 |          35.9753 |         -39.3191 |
[32m[20221213 12:51:22 @agent_ppo2.py:179][0m |          -0.0276 |          32.4826 |         -41.0083 |
[32m[20221213 12:51:22 @agent_ppo2.py:179][0m |          -0.0400 |          32.1460 |         -42.0300 |
[32m[20221213 12:51:23 @agent_ppo2.py:179][0m |          -0.0380 |          32.1843 |         -43.0158 |
[32m[20221213 12:51:23 @agent_ppo2.py:179][0m |          -0.0367 |          31.8810 |         -42.9259 |
[32m[20221213 12:51:23 @agent_ppo2.py:179][0m |          -0.0413 |          31.5697 |         -43.8888 |
[32m[20221213 12:51:23 @agent_ppo2.py:179][0m |          -0.0471 |          31.5254 |         -44.4106 |
[32m[20221213 12:51:23 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:51:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 291.88
[32m[20221213 12:51:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.75
[32m[20221213 12:51:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 376.69
[32m[20221213 12:51:23 @agent_ppo2.py:137][0m Total time:      35.44 min
[32m[20221213 12:51:23 @agent_ppo2.py:139][0m 2437120 total steps have happened
[32m[20221213 12:51:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1190 --------------------------#
[32m[20221213 12:51:23 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:51:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |           0.0803 |          35.0298 |         -29.5489 |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |           0.0646 |          36.7661 |         -25.7763 |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |           0.0273 |          33.2147 |         -29.0017 |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |           0.0078 |          32.8887 |         -32.1915 |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |          -0.0103 |          32.6351 |         -34.7914 |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |          -0.0139 |          32.5255 |         -35.6542 |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |          -0.0241 |          32.2908 |         -37.0875 |
[32m[20221213 12:51:24 @agent_ppo2.py:179][0m |          -0.0309 |          32.1663 |         -39.0082 |
[32m[20221213 12:51:25 @agent_ppo2.py:179][0m |          -0.0263 |          33.5735 |         -40.6525 |
[32m[20221213 12:51:25 @agent_ppo2.py:179][0m |          -0.0348 |          32.1407 |         -41.5544 |
[32m[20221213 12:51:25 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:51:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.16
[32m[20221213 12:51:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.62
[32m[20221213 12:51:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.81
[32m[20221213 12:51:25 @agent_ppo2.py:137][0m Total time:      35.47 min
[32m[20221213 12:51:25 @agent_ppo2.py:139][0m 2439168 total steps have happened
[32m[20221213 12:51:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1191 --------------------------#
[32m[20221213 12:51:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:25 @agent_ppo2.py:179][0m |           0.0842 |          36.7794 |         -34.7707 |
[32m[20221213 12:51:25 @agent_ppo2.py:179][0m |           0.0867 |          35.3243 |         -25.8322 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |           0.0168 |          34.7374 |         -31.2967 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |          -0.0143 |          34.2590 |         -34.3182 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |          -0.0215 |          34.0050 |         -35.1709 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |          -0.0317 |          33.7339 |         -36.4957 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |          -0.0288 |          35.5364 |         -36.7981 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |          -0.0370 |          33.4165 |         -37.3938 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |          -0.0426 |          33.1907 |         -38.1129 |
[32m[20221213 12:51:26 @agent_ppo2.py:179][0m |          -0.0450 |          33.0744 |         -38.9559 |
[32m[20221213 12:51:26 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:51:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 282.03
[32m[20221213 12:51:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.01
[32m[20221213 12:51:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.83
[32m[20221213 12:51:27 @agent_ppo2.py:137][0m Total time:      35.50 min
[32m[20221213 12:51:27 @agent_ppo2.py:139][0m 2441216 total steps have happened
[32m[20221213 12:51:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1192 --------------------------#
[32m[20221213 12:51:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:27 @agent_ppo2.py:179][0m |           0.0718 |          34.7245 |         -29.9194 |
[32m[20221213 12:51:27 @agent_ppo2.py:179][0m |           0.0537 |          34.3630 |         -25.1399 |
[32m[20221213 12:51:27 @agent_ppo2.py:179][0m |           0.0354 |          32.7774 |         -26.6039 |
[32m[20221213 12:51:27 @agent_ppo2.py:179][0m |           0.0005 |          32.4107 |         -30.5768 |
[32m[20221213 12:51:27 @agent_ppo2.py:179][0m |          -0.0154 |          31.9713 |         -33.7663 |
[32m[20221213 12:51:28 @agent_ppo2.py:179][0m |          -0.0247 |          31.6708 |         -35.2319 |
[32m[20221213 12:51:28 @agent_ppo2.py:179][0m |          -0.0306 |          31.4757 |         -37.1654 |
[32m[20221213 12:51:28 @agent_ppo2.py:179][0m |          -0.0236 |          31.1529 |         -36.5868 |
[32m[20221213 12:51:28 @agent_ppo2.py:179][0m |          -0.0266 |          31.5164 |         -37.5192 |
[32m[20221213 12:51:28 @agent_ppo2.py:179][0m |          -0.0316 |          32.0223 |         -39.2277 |
[32m[20221213 12:51:28 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:51:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.37
[32m[20221213 12:51:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.36
[32m[20221213 12:51:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.25
[32m[20221213 12:51:28 @agent_ppo2.py:137][0m Total time:      35.53 min
[32m[20221213 12:51:28 @agent_ppo2.py:139][0m 2443264 total steps have happened
[32m[20221213 12:51:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1193 --------------------------#
[32m[20221213 12:51:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:29 @agent_ppo2.py:179][0m |           0.0517 |          35.9194 |         -35.3357 |
[32m[20221213 12:51:29 @agent_ppo2.py:179][0m |           0.0366 |          35.9366 |         -31.7305 |
[32m[20221213 12:51:29 @agent_ppo2.py:179][0m |          -0.0026 |          33.7129 |         -36.1103 |
[32m[20221213 12:51:29 @agent_ppo2.py:179][0m |          -0.0117 |          34.9634 |         -38.0487 |
[32m[20221213 12:51:29 @agent_ppo2.py:179][0m |          -0.0153 |          33.1846 |         -36.6338 |
[32m[20221213 12:51:29 @agent_ppo2.py:179][0m |          -0.0288 |          32.9094 |         -39.0239 |
[32m[20221213 12:51:29 @agent_ppo2.py:179][0m |          -0.0339 |          32.7369 |         -39.2615 |
[32m[20221213 12:51:30 @agent_ppo2.py:179][0m |          -0.0366 |          32.5060 |         -40.9595 |
[32m[20221213 12:51:30 @agent_ppo2.py:179][0m |          -0.0425 |          32.3572 |         -41.8400 |
[32m[20221213 12:51:30 @agent_ppo2.py:179][0m |          -0.0421 |          32.2564 |         -42.3828 |
[32m[20221213 12:51:30 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:51:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.01
[32m[20221213 12:51:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.77
[32m[20221213 12:51:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.45
[32m[20221213 12:51:30 @agent_ppo2.py:137][0m Total time:      35.56 min
[32m[20221213 12:51:30 @agent_ppo2.py:139][0m 2445312 total steps have happened
[32m[20221213 12:51:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1194 --------------------------#
[32m[20221213 12:51:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:30 @agent_ppo2.py:179][0m |           0.0679 |          35.1982 |         -29.4530 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |           0.0807 |          36.1228 |         -23.8326 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |           0.0284 |          33.4350 |         -25.3748 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |           0.0010 |          33.1460 |         -30.9103 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |          -0.0158 |          32.8027 |         -32.6723 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |          -0.0221 |          32.8322 |         -33.1528 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |          -0.0322 |          32.5150 |         -35.2051 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |          -0.0372 |          32.2324 |         -35.5953 |
[32m[20221213 12:51:31 @agent_ppo2.py:179][0m |          -0.0317 |          32.1126 |         -34.9043 |
[32m[20221213 12:51:32 @agent_ppo2.py:179][0m |          -0.0395 |          32.0488 |         -36.5183 |
[32m[20221213 12:51:32 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:51:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.17
[32m[20221213 12:51:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.52
[32m[20221213 12:51:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 270.84
[32m[20221213 12:51:32 @agent_ppo2.py:137][0m Total time:      35.58 min
[32m[20221213 12:51:32 @agent_ppo2.py:139][0m 2447360 total steps have happened
[32m[20221213 12:51:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1195 --------------------------#
[32m[20221213 12:51:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:32 @agent_ppo2.py:179][0m |           0.0464 |          35.0742 |         -29.1762 |
[32m[20221213 12:51:32 @agent_ppo2.py:179][0m |           0.0416 |          34.3944 |         -24.3878 |
[32m[20221213 12:51:32 @agent_ppo2.py:179][0m |           0.0145 |          33.7814 |         -25.1134 |
[32m[20221213 12:51:33 @agent_ppo2.py:179][0m |          -0.0135 |          33.4333 |         -27.4957 |
[32m[20221213 12:51:33 @agent_ppo2.py:179][0m |          -0.0224 |          33.1219 |         -28.3736 |
[32m[20221213 12:51:33 @agent_ppo2.py:179][0m |          -0.0233 |          33.0005 |         -29.2089 |
[32m[20221213 12:51:33 @agent_ppo2.py:179][0m |          -0.0275 |          32.6725 |         -28.8994 |
[32m[20221213 12:51:33 @agent_ppo2.py:179][0m |          -0.0303 |          32.6943 |         -30.6478 |
[32m[20221213 12:51:33 @agent_ppo2.py:179][0m |          -0.0253 |          34.0092 |         -30.7859 |
[32m[20221213 12:51:33 @agent_ppo2.py:179][0m |          -0.0295 |          32.1967 |         -30.0156 |
[32m[20221213 12:51:33 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:51:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.12
[32m[20221213 12:51:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 367.88
[32m[20221213 12:51:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 297.57
[32m[20221213 12:51:34 @agent_ppo2.py:137][0m Total time:      35.61 min
[32m[20221213 12:51:34 @agent_ppo2.py:139][0m 2449408 total steps have happened
[32m[20221213 12:51:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1196 --------------------------#
[32m[20221213 12:51:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:34 @agent_ppo2.py:179][0m |           0.0498 |          36.1544 |         -27.5904 |
[32m[20221213 12:51:34 @agent_ppo2.py:179][0m |           0.0217 |          35.6302 |         -23.9239 |
[32m[20221213 12:51:34 @agent_ppo2.py:179][0m |          -0.0055 |          34.2038 |         -26.2548 |
[32m[20221213 12:51:34 @agent_ppo2.py:179][0m |          -0.0179 |          33.7353 |         -28.0505 |
[32m[20221213 12:51:34 @agent_ppo2.py:179][0m |          -0.0322 |          33.4186 |         -29.0612 |
[32m[20221213 12:51:35 @agent_ppo2.py:179][0m |          -0.0326 |          33.1596 |         -30.3508 |
[32m[20221213 12:51:35 @agent_ppo2.py:179][0m |          -0.0372 |          32.8153 |         -31.3641 |
[32m[20221213 12:51:35 @agent_ppo2.py:179][0m |          -0.0316 |          34.6087 |         -32.0684 |
[32m[20221213 12:51:35 @agent_ppo2.py:179][0m |          -0.0413 |          32.4593 |         -32.1191 |
[32m[20221213 12:51:35 @agent_ppo2.py:179][0m |          -0.0380 |          34.7302 |         -33.7287 |
[32m[20221213 12:51:35 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:51:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.93
[32m[20221213 12:51:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.41
[32m[20221213 12:51:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 312.22
[32m[20221213 12:51:35 @agent_ppo2.py:137][0m Total time:      35.64 min
[32m[20221213 12:51:35 @agent_ppo2.py:139][0m 2451456 total steps have happened
[32m[20221213 12:51:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1197 --------------------------#
[32m[20221213 12:51:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:36 @agent_ppo2.py:179][0m |           0.1127 |          32.3724 |         -22.8134 |
[32m[20221213 12:51:36 @agent_ppo2.py:179][0m |           0.0395 |          30.8313 |         -19.9586 |
[32m[20221213 12:51:36 @agent_ppo2.py:179][0m |          -0.0073 |          32.9481 |         -24.7186 |
[32m[20221213 12:51:36 @agent_ppo2.py:179][0m |          -0.0280 |          30.0122 |         -26.7095 |
[32m[20221213 12:51:36 @agent_ppo2.py:179][0m |          -0.0338 |          29.7891 |         -28.1278 |
[32m[20221213 12:51:36 @agent_ppo2.py:179][0m |          -0.0426 |          29.3846 |         -29.0859 |
[32m[20221213 12:51:36 @agent_ppo2.py:179][0m |          -0.0480 |          29.1221 |         -30.6873 |
[32m[20221213 12:51:37 @agent_ppo2.py:179][0m |          -0.0480 |          29.4332 |         -30.6825 |
[32m[20221213 12:51:37 @agent_ppo2.py:179][0m |          -0.0377 |          33.4225 |         -31.5562 |
[32m[20221213 12:51:37 @agent_ppo2.py:179][0m |          -0.0574 |          28.8148 |         -32.1046 |
[32m[20221213 12:51:37 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:51:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.40
[32m[20221213 12:51:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.04
[32m[20221213 12:51:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.59
[32m[20221213 12:51:37 @agent_ppo2.py:137][0m Total time:      35.67 min
[32m[20221213 12:51:37 @agent_ppo2.py:139][0m 2453504 total steps have happened
[32m[20221213 12:51:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1198 --------------------------#
[32m[20221213 12:51:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:37 @agent_ppo2.py:179][0m |           0.0695 |          37.3930 |         -29.6580 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |           0.0639 |          36.1340 |         -20.4081 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |           0.0084 |          35.2313 |         -29.8952 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |          -0.0101 |          34.7397 |         -32.8720 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |          -0.0284 |          34.2634 |         -34.8943 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |          -0.0318 |          34.1471 |         -36.2168 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |          -0.0312 |          33.8581 |         -37.6969 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |          -0.0344 |          33.4695 |         -36.9891 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |          -0.0356 |          33.4825 |         -38.7464 |
[32m[20221213 12:51:38 @agent_ppo2.py:179][0m |          -0.0332 |          34.4790 |         -39.7199 |
[32m[20221213 12:51:38 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:51:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.41
[32m[20221213 12:51:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.95
[32m[20221213 12:51:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 274.87
[32m[20221213 12:51:39 @agent_ppo2.py:137][0m Total time:      35.70 min
[32m[20221213 12:51:39 @agent_ppo2.py:139][0m 2455552 total steps have happened
[32m[20221213 12:51:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1199 --------------------------#
[32m[20221213 12:51:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:39 @agent_ppo2.py:179][0m |           0.0824 |          36.2438 |         -25.7334 |
[32m[20221213 12:51:39 @agent_ppo2.py:179][0m |           0.0556 |          34.8389 |         -19.5216 |
[32m[20221213 12:51:39 @agent_ppo2.py:179][0m |           0.0108 |          34.2498 |         -25.8827 |
[32m[20221213 12:51:39 @agent_ppo2.py:179][0m |          -0.0102 |          33.9254 |         -27.5569 |
[32m[20221213 12:51:40 @agent_ppo2.py:179][0m |          -0.0215 |          33.6697 |         -29.5849 |
[32m[20221213 12:51:40 @agent_ppo2.py:179][0m |          -0.0259 |          33.4018 |         -30.4102 |
[32m[20221213 12:51:40 @agent_ppo2.py:179][0m |          -0.0193 |          35.2701 |         -30.2099 |
[32m[20221213 12:51:40 @agent_ppo2.py:179][0m |          -0.0360 |          33.1968 |         -31.2852 |
[32m[20221213 12:51:40 @agent_ppo2.py:179][0m |          -0.0296 |          33.5967 |         -32.1041 |
[32m[20221213 12:51:40 @agent_ppo2.py:179][0m |          -0.0367 |          32.8635 |         -32.6465 |
[32m[20221213 12:51:40 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:51:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 300.51
[32m[20221213 12:51:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.03
[32m[20221213 12:51:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.18
[32m[20221213 12:51:40 @agent_ppo2.py:137][0m Total time:      35.73 min
[32m[20221213 12:51:40 @agent_ppo2.py:139][0m 2457600 total steps have happened
[32m[20221213 12:51:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1200 --------------------------#
[32m[20221213 12:51:41 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:51:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:41 @agent_ppo2.py:179][0m |           0.0835 |          36.1360 |         -24.6012 |
[32m[20221213 12:51:41 @agent_ppo2.py:179][0m |           0.0533 |          34.5840 |         -18.4966 |
[32m[20221213 12:51:41 @agent_ppo2.py:179][0m |           0.0078 |          33.8949 |         -22.0280 |
[32m[20221213 12:51:41 @agent_ppo2.py:179][0m |          -0.0178 |          33.3485 |         -25.5742 |
[32m[20221213 12:51:41 @agent_ppo2.py:179][0m |          -0.0211 |          32.9487 |         -26.1161 |
[32m[20221213 12:51:41 @agent_ppo2.py:179][0m |          -0.0298 |          32.6393 |         -26.4517 |
[32m[20221213 12:51:42 @agent_ppo2.py:179][0m |          -0.0334 |          32.5194 |         -27.3992 |
[32m[20221213 12:51:42 @agent_ppo2.py:179][0m |          -0.0359 |          32.3355 |         -28.1554 |
[32m[20221213 12:51:42 @agent_ppo2.py:179][0m |          -0.0397 |          32.1584 |         -28.8594 |
[32m[20221213 12:51:42 @agent_ppo2.py:179][0m |          -0.0416 |          31.9758 |         -29.0612 |
[32m[20221213 12:51:42 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:51:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.81
[32m[20221213 12:51:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.19
[32m[20221213 12:51:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.47
[32m[20221213 12:51:42 @agent_ppo2.py:137][0m Total time:      35.76 min
[32m[20221213 12:51:42 @agent_ppo2.py:139][0m 2459648 total steps have happened
[32m[20221213 12:51:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1201 --------------------------#
[32m[20221213 12:51:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |           0.1252 |          34.1624 |         -26.3360 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |           0.0753 |          35.3975 |         -20.5420 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |           0.0133 |          32.6117 |         -21.4488 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |          -0.0086 |          32.1832 |         -24.7167 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |          -0.0009 |          35.3166 |         -25.6105 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |          -0.0162 |          31.8257 |         -26.9248 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |          -0.0214 |          31.5507 |         -28.1304 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |          -0.0305 |          31.4572 |         -28.7323 |
[32m[20221213 12:51:43 @agent_ppo2.py:179][0m |          -0.0272 |          32.1702 |         -30.4590 |
[32m[20221213 12:51:44 @agent_ppo2.py:179][0m |          -0.0274 |          31.1465 |         -29.6620 |
[32m[20221213 12:51:44 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:51:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.35
[32m[20221213 12:51:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.80
[32m[20221213 12:51:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 294.20
[32m[20221213 12:51:44 @agent_ppo2.py:137][0m Total time:      35.78 min
[32m[20221213 12:51:44 @agent_ppo2.py:139][0m 2461696 total steps have happened
[32m[20221213 12:51:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1202 --------------------------#
[32m[20221213 12:51:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:44 @agent_ppo2.py:179][0m |           0.0455 |          40.2938 |         -26.6936 |
[32m[20221213 12:51:44 @agent_ppo2.py:179][0m |           0.0461 |          34.8391 |         -22.4053 |
[32m[20221213 12:51:44 @agent_ppo2.py:179][0m |           0.0201 |          34.2468 |         -22.4209 |
[32m[20221213 12:51:45 @agent_ppo2.py:179][0m |           0.0027 |          35.2418 |         -23.6326 |
[32m[20221213 12:51:45 @agent_ppo2.py:179][0m |          -0.0182 |          35.1583 |         -27.1011 |
[32m[20221213 12:51:45 @agent_ppo2.py:179][0m |          -0.0091 |          33.5566 |         -26.2415 |
[32m[20221213 12:51:45 @agent_ppo2.py:179][0m |          -0.0245 |          33.6457 |         -28.9050 |
[32m[20221213 12:51:45 @agent_ppo2.py:179][0m |          -0.0326 |          33.1356 |         -29.3512 |
[32m[20221213 12:51:45 @agent_ppo2.py:179][0m |          -0.0395 |          33.0231 |         -31.0298 |
[32m[20221213 12:51:45 @agent_ppo2.py:179][0m |          -0.0419 |          32.8739 |         -30.4851 |
[32m[20221213 12:51:45 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:51:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.89
[32m[20221213 12:51:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.95
[32m[20221213 12:51:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.67
[32m[20221213 12:51:45 @agent_ppo2.py:137][0m Total time:      35.81 min
[32m[20221213 12:51:45 @agent_ppo2.py:139][0m 2463744 total steps have happened
[32m[20221213 12:51:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1203 --------------------------#
[32m[20221213 12:51:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:46 @agent_ppo2.py:179][0m |           0.0655 |          36.1712 |         -22.9047 |
[32m[20221213 12:51:46 @agent_ppo2.py:179][0m |           0.0427 |          34.9885 |         -19.5574 |
[32m[20221213 12:51:46 @agent_ppo2.py:179][0m |           0.0075 |          34.5005 |         -22.9771 |
[32m[20221213 12:51:46 @agent_ppo2.py:179][0m |          -0.0097 |          34.1542 |         -25.3944 |
[32m[20221213 12:51:46 @agent_ppo2.py:179][0m |          -0.0110 |          33.6411 |         -24.7797 |
[32m[20221213 12:51:46 @agent_ppo2.py:179][0m |          -0.0230 |          33.3771 |         -26.8021 |
[32m[20221213 12:51:47 @agent_ppo2.py:179][0m |          -0.0192 |          33.1177 |         -26.3482 |
[32m[20221213 12:51:47 @agent_ppo2.py:179][0m |          -0.0246 |          33.0684 |         -27.7760 |
[32m[20221213 12:51:47 @agent_ppo2.py:179][0m |          -0.0167 |          32.8102 |         -25.5356 |
[32m[20221213 12:51:47 @agent_ppo2.py:179][0m |          -0.0283 |          32.9985 |         -28.1299 |
[32m[20221213 12:51:47 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:51:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.10
[32m[20221213 12:51:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.28
[32m[20221213 12:51:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.91
[32m[20221213 12:51:47 @agent_ppo2.py:137][0m Total time:      35.84 min
[32m[20221213 12:51:47 @agent_ppo2.py:139][0m 2465792 total steps have happened
[32m[20221213 12:51:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1204 --------------------------#
[32m[20221213 12:51:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |           0.0842 |          41.7382 |         -23.7687 |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |           0.0657 |          36.7149 |         -17.2612 |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |           0.0234 |          36.2227 |         -17.2272 |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |           0.0115 |          40.3323 |         -20.0368 |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |          -0.0172 |          35.8524 |         -22.1956 |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |          -0.0259 |          35.5885 |         -23.3828 |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |          -0.0301 |          35.4558 |         -24.4411 |
[32m[20221213 12:51:48 @agent_ppo2.py:179][0m |          -0.0378 |          35.2802 |         -25.6049 |
[32m[20221213 12:51:49 @agent_ppo2.py:179][0m |          -0.0380 |          35.3085 |         -25.8891 |
[32m[20221213 12:51:49 @agent_ppo2.py:179][0m |          -0.0320 |          35.5988 |         -25.8814 |
[32m[20221213 12:51:49 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:51:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.14
[32m[20221213 12:51:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.27
[32m[20221213 12:51:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 369.55
[32m[20221213 12:51:49 @agent_ppo2.py:137][0m Total time:      35.87 min
[32m[20221213 12:51:49 @agent_ppo2.py:139][0m 2467840 total steps have happened
[32m[20221213 12:51:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1205 --------------------------#
[32m[20221213 12:51:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:49 @agent_ppo2.py:179][0m |           0.0834 |          39.4989 |         -20.7383 |
[32m[20221213 12:51:49 @agent_ppo2.py:179][0m |           0.0461 |          35.2711 |         -19.0052 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |           0.0152 |          34.7113 |         -21.0460 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |          -0.0028 |          34.4148 |         -23.6122 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |          -0.0098 |          34.1135 |         -24.8387 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |          -0.0202 |          33.9504 |         -27.0518 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |          -0.0265 |          33.7065 |         -27.2817 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |          -0.0304 |          33.6168 |         -27.5611 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |          -0.0321 |          33.6926 |         -28.5042 |
[32m[20221213 12:51:50 @agent_ppo2.py:179][0m |          -0.0378 |          33.2396 |         -28.8180 |
[32m[20221213 12:51:50 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:51:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.86
[32m[20221213 12:51:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.51
[32m[20221213 12:51:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 395.37
[32m[20221213 12:51:51 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 395.37
[32m[20221213 12:51:51 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 395.37
[32m[20221213 12:51:51 @agent_ppo2.py:137][0m Total time:      35.90 min
[32m[20221213 12:51:51 @agent_ppo2.py:139][0m 2469888 total steps have happened
[32m[20221213 12:51:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1206 --------------------------#
[32m[20221213 12:51:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:51 @agent_ppo2.py:179][0m |           0.0908 |          36.3889 |         -19.0493 |
[32m[20221213 12:51:51 @agent_ppo2.py:179][0m |           0.0964 |          35.5063 |          -8.5506 |
[32m[20221213 12:51:51 @agent_ppo2.py:179][0m |           0.0422 |          35.1648 |         -13.0272 |
[32m[20221213 12:51:51 @agent_ppo2.py:179][0m |           0.0066 |          34.7476 |         -17.7142 |
[32m[20221213 12:51:51 @agent_ppo2.py:179][0m |          -0.0022 |          34.5107 |         -19.5168 |
[32m[20221213 12:51:52 @agent_ppo2.py:179][0m |          -0.0180 |          34.4882 |         -20.5677 |
[32m[20221213 12:51:52 @agent_ppo2.py:179][0m |          -0.0241 |          34.2104 |         -22.6241 |
[32m[20221213 12:51:52 @agent_ppo2.py:179][0m |          -0.0232 |          34.1486 |         -23.0256 |
[32m[20221213 12:51:52 @agent_ppo2.py:179][0m |          -0.0336 |          34.0100 |         -23.9969 |
[32m[20221213 12:51:52 @agent_ppo2.py:179][0m |          -0.0323 |          33.8262 |         -24.7789 |
[32m[20221213 12:51:52 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:51:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.64
[32m[20221213 12:51:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.76
[32m[20221213 12:51:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.22
[32m[20221213 12:51:52 @agent_ppo2.py:137][0m Total time:      35.93 min
[32m[20221213 12:51:52 @agent_ppo2.py:139][0m 2471936 total steps have happened
[32m[20221213 12:51:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1207 --------------------------#
[32m[20221213 12:51:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:53 @agent_ppo2.py:179][0m |           0.0885 |          31.2126 |         -19.8948 |
[32m[20221213 12:51:53 @agent_ppo2.py:179][0m |           0.0396 |          30.2976 |         -14.4052 |
[32m[20221213 12:51:53 @agent_ppo2.py:179][0m |          -0.0018 |          29.9278 |         -17.1880 |
[32m[20221213 12:51:53 @agent_ppo2.py:179][0m |          -0.0125 |          29.6694 |         -18.4224 |
[32m[20221213 12:51:53 @agent_ppo2.py:179][0m |          -0.0170 |          29.4945 |         -18.6164 |
[32m[20221213 12:51:53 @agent_ppo2.py:179][0m |          -0.0257 |          29.7037 |         -20.3980 |
[32m[20221213 12:51:53 @agent_ppo2.py:179][0m |          -0.0189 |          29.1319 |         -19.8618 |
[32m[20221213 12:51:54 @agent_ppo2.py:179][0m |          -0.0337 |          29.3337 |         -21.5671 |
[32m[20221213 12:51:54 @agent_ppo2.py:179][0m |          -0.0358 |          28.9594 |         -21.7212 |
[32m[20221213 12:51:54 @agent_ppo2.py:179][0m |          -0.0425 |          28.7469 |         -22.9874 |
[32m[20221213 12:51:54 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:51:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.01
[32m[20221213 12:51:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.87
[32m[20221213 12:51:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.00
[32m[20221213 12:51:54 @agent_ppo2.py:137][0m Total time:      35.95 min
[32m[20221213 12:51:54 @agent_ppo2.py:139][0m 2473984 total steps have happened
[32m[20221213 12:51:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1208 --------------------------#
[32m[20221213 12:51:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:54 @agent_ppo2.py:179][0m |           0.0580 |          37.0032 |         -20.1244 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |           0.0586 |          35.8631 |         -14.0445 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |           0.0344 |          35.3555 |         -14.3803 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |           0.0095 |          34.9691 |         -17.1419 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |          -0.0138 |          34.7331 |         -19.5074 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |          -0.0201 |          34.5558 |         -20.6993 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |          -0.0238 |          34.3417 |         -22.1347 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |          -0.0271 |          34.2548 |         -23.1688 |
[32m[20221213 12:51:55 @agent_ppo2.py:179][0m |          -0.0321 |          34.1247 |         -23.8228 |
[32m[20221213 12:51:56 @agent_ppo2.py:179][0m |          -0.0357 |          34.0137 |         -24.6953 |
[32m[20221213 12:51:56 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:51:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 363.67
[32m[20221213 12:51:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 377.41
[32m[20221213 12:51:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 384.10
[32m[20221213 12:51:56 @agent_ppo2.py:137][0m Total time:      35.98 min
[32m[20221213 12:51:56 @agent_ppo2.py:139][0m 2476032 total steps have happened
[32m[20221213 12:51:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1209 --------------------------#
[32m[20221213 12:51:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:51:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:56 @agent_ppo2.py:179][0m |           0.0727 |          36.9796 |         -18.8173 |
[32m[20221213 12:51:56 @agent_ppo2.py:179][0m |           0.0474 |          35.7051 |         -15.6557 |
[32m[20221213 12:51:56 @agent_ppo2.py:179][0m |           0.0116 |          35.6087 |         -16.7671 |
[32m[20221213 12:51:57 @agent_ppo2.py:179][0m |          -0.0141 |          35.1197 |         -18.8365 |
[32m[20221213 12:51:57 @agent_ppo2.py:179][0m |          -0.0270 |          34.9952 |         -20.9271 |
[32m[20221213 12:51:57 @agent_ppo2.py:179][0m |          -0.0313 |          34.7024 |         -22.1738 |
[32m[20221213 12:51:57 @agent_ppo2.py:179][0m |          -0.0289 |          34.5671 |         -23.2332 |
[32m[20221213 12:51:57 @agent_ppo2.py:179][0m |          -0.0348 |          34.4014 |         -23.5562 |
[32m[20221213 12:51:57 @agent_ppo2.py:179][0m |          -0.0421 |          34.3635 |         -24.5875 |
[32m[20221213 12:51:57 @agent_ppo2.py:179][0m |          -0.0412 |          34.1703 |         -25.9399 |
[32m[20221213 12:51:57 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:51:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.80
[32m[20221213 12:51:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.85
[32m[20221213 12:51:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.24
[32m[20221213 12:51:57 @agent_ppo2.py:137][0m Total time:      36.01 min
[32m[20221213 12:51:57 @agent_ppo2.py:139][0m 2478080 total steps have happened
[32m[20221213 12:51:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1210 --------------------------#
[32m[20221213 12:51:58 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:51:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:51:58 @agent_ppo2.py:179][0m |           0.0859 |          36.6657 |         -19.9609 |
[32m[20221213 12:51:58 @agent_ppo2.py:179][0m |           0.0347 |          35.0725 |         -15.3620 |
[32m[20221213 12:51:58 @agent_ppo2.py:179][0m |           0.0168 |          34.5853 |         -16.5948 |
[32m[20221213 12:51:58 @agent_ppo2.py:179][0m |          -0.0067 |          34.1114 |         -17.6471 |
[32m[20221213 12:51:58 @agent_ppo2.py:179][0m |          -0.0275 |          33.7272 |         -19.9927 |
[32m[20221213 12:51:59 @agent_ppo2.py:179][0m |          -0.0315 |          33.4268 |         -20.1269 |
[32m[20221213 12:51:59 @agent_ppo2.py:179][0m |          -0.0322 |          33.1744 |         -20.4424 |
[32m[20221213 12:51:59 @agent_ppo2.py:179][0m |          -0.0400 |          32.9069 |         -21.4566 |
[32m[20221213 12:51:59 @agent_ppo2.py:179][0m |          -0.0393 |          32.6939 |         -22.0231 |
[32m[20221213 12:51:59 @agent_ppo2.py:179][0m |          -0.0405 |          32.5336 |         -22.3823 |
[32m[20221213 12:51:59 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:51:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 298.02
[32m[20221213 12:51:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 319.70
[32m[20221213 12:51:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.76
[32m[20221213 12:51:59 @agent_ppo2.py:137][0m Total time:      36.04 min
[32m[20221213 12:51:59 @agent_ppo2.py:139][0m 2480128 total steps have happened
[32m[20221213 12:51:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1211 --------------------------#
[32m[20221213 12:51:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:00 @agent_ppo2.py:179][0m |           0.1106 |          37.3342 |         -14.2956 |
[32m[20221213 12:52:00 @agent_ppo2.py:179][0m |           0.0629 |          35.8380 |         -11.5199 |
[32m[20221213 12:52:00 @agent_ppo2.py:179][0m |           0.0395 |          35.4781 |         -11.1168 |
[32m[20221213 12:52:00 @agent_ppo2.py:179][0m |           0.0015 |          35.2361 |         -13.8369 |
[32m[20221213 12:52:00 @agent_ppo2.py:179][0m |          -0.0163 |          34.9778 |         -16.9564 |
[32m[20221213 12:52:00 @agent_ppo2.py:179][0m |          -0.0227 |          34.8845 |         -17.6142 |
[32m[20221213 12:52:00 @agent_ppo2.py:179][0m |          -0.0202 |          36.8733 |         -18.7696 |
[32m[20221213 12:52:01 @agent_ppo2.py:179][0m |          -0.0322 |          34.6184 |         -18.9347 |
[32m[20221213 12:52:01 @agent_ppo2.py:179][0m |          -0.0350 |          34.5500 |         -20.5203 |
[32m[20221213 12:52:01 @agent_ppo2.py:179][0m |          -0.0391 |          34.4929 |         -21.3229 |
[32m[20221213 12:52:01 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:52:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.04
[32m[20221213 12:52:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.63
[32m[20221213 12:52:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.60
[32m[20221213 12:52:01 @agent_ppo2.py:137][0m Total time:      36.07 min
[32m[20221213 12:52:01 @agent_ppo2.py:139][0m 2482176 total steps have happened
[32m[20221213 12:52:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1212 --------------------------#
[32m[20221213 12:52:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:01 @agent_ppo2.py:179][0m |           0.0804 |          35.9634 |         -15.1716 |
[32m[20221213 12:52:01 @agent_ppo2.py:179][0m |           0.0853 |          34.5023 |          -5.3626 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |           0.0549 |          34.1516 |          -8.6898 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |           0.0169 |          33.9455 |         -11.9266 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |          -0.0011 |          33.7619 |         -14.3730 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |          -0.0114 |          33.6410 |         -16.3700 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |          -0.0216 |          33.4642 |         -17.4849 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |          -0.0203 |          33.5621 |         -18.1350 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |          -0.0275 |          33.2583 |         -19.1513 |
[32m[20221213 12:52:02 @agent_ppo2.py:179][0m |          -0.0210 |          33.1217 |         -19.4856 |
[32m[20221213 12:52:02 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:52:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 369.67
[32m[20221213 12:52:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 381.57
[32m[20221213 12:52:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.77
[32m[20221213 12:52:03 @agent_ppo2.py:137][0m Total time:      36.10 min
[32m[20221213 12:52:03 @agent_ppo2.py:139][0m 2484224 total steps have happened
[32m[20221213 12:52:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1213 --------------------------#
[32m[20221213 12:52:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:03 @agent_ppo2.py:179][0m |           0.0524 |          36.0285 |         -14.2406 |
[32m[20221213 12:52:03 @agent_ppo2.py:179][0m |           0.0925 |          39.7473 |          -5.7556 |
[32m[20221213 12:52:03 @agent_ppo2.py:179][0m |           0.0343 |          34.9282 |          -9.0702 |
[32m[20221213 12:52:03 @agent_ppo2.py:179][0m |           0.0100 |          34.7296 |         -11.6275 |
[32m[20221213 12:52:04 @agent_ppo2.py:179][0m |          -0.0032 |          34.5346 |         -12.4607 |
[32m[20221213 12:52:04 @agent_ppo2.py:179][0m |          -0.0165 |          34.4358 |         -13.9791 |
[32m[20221213 12:52:04 @agent_ppo2.py:179][0m |          -0.0218 |          34.8885 |         -14.6979 |
[32m[20221213 12:52:04 @agent_ppo2.py:179][0m |          -0.0238 |          34.1790 |         -14.9793 |
[32m[20221213 12:52:04 @agent_ppo2.py:179][0m |          -0.0291 |          34.2069 |         -15.4512 |
[32m[20221213 12:52:04 @agent_ppo2.py:179][0m |          -0.0341 |          34.1000 |         -16.5486 |
[32m[20221213 12:52:04 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:52:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 370.02
[32m[20221213 12:52:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 379.44
[32m[20221213 12:52:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 303.98
[32m[20221213 12:52:04 @agent_ppo2.py:137][0m Total time:      36.13 min
[32m[20221213 12:52:04 @agent_ppo2.py:139][0m 2486272 total steps have happened
[32m[20221213 12:52:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1214 --------------------------#
[32m[20221213 12:52:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:05 @agent_ppo2.py:179][0m |           0.0978 |          36.7741 |         -11.2602 |
[32m[20221213 12:52:05 @agent_ppo2.py:179][0m |           0.0366 |          35.4927 |          -8.0382 |
[32m[20221213 12:52:05 @agent_ppo2.py:179][0m |           0.0107 |          34.8662 |          -9.8417 |
[32m[20221213 12:52:05 @agent_ppo2.py:179][0m |          -0.0035 |          34.7879 |         -10.7383 |
[32m[20221213 12:52:05 @agent_ppo2.py:179][0m |          -0.0174 |          34.4566 |         -12.4330 |
[32m[20221213 12:52:05 @agent_ppo2.py:179][0m |          -0.0031 |          42.7937 |         -12.9600 |
[32m[20221213 12:52:06 @agent_ppo2.py:179][0m |          -0.0144 |          36.1653 |         -13.0595 |
[32m[20221213 12:52:06 @agent_ppo2.py:179][0m |          -0.0301 |          34.2729 |         -14.0162 |
[32m[20221213 12:52:06 @agent_ppo2.py:179][0m |          -0.0265 |          34.0178 |         -14.0815 |
[32m[20221213 12:52:06 @agent_ppo2.py:179][0m |          -0.0329 |          33.9238 |         -14.7361 |
[32m[20221213 12:52:06 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:52:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 369.95
[32m[20221213 12:52:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 378.57
[32m[20221213 12:52:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.11
[32m[20221213 12:52:06 @agent_ppo2.py:137][0m Total time:      36.16 min
[32m[20221213 12:52:06 @agent_ppo2.py:139][0m 2488320 total steps have happened
[32m[20221213 12:52:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1215 --------------------------#
[32m[20221213 12:52:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |           0.0680 |          36.8427 |          -8.8466 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |           0.0562 |          36.0615 |          -5.3768 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |           0.0058 |          35.7932 |          -8.4739 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |          -0.0081 |          36.4798 |          -9.7473 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |          -0.0038 |          35.4920 |          -8.7638 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |          -0.0214 |          35.3382 |         -10.1909 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |          -0.0212 |          35.2414 |         -10.1202 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |          -0.0198 |          38.3919 |         -10.6633 |
[32m[20221213 12:52:07 @agent_ppo2.py:179][0m |          -0.0184 |          40.1274 |         -10.9258 |
[32m[20221213 12:52:08 @agent_ppo2.py:179][0m |          -0.0188 |          37.3192 |         -10.5267 |
[32m[20221213 12:52:08 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:52:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.71
[32m[20221213 12:52:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.94
[32m[20221213 12:52:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 380.48
[32m[20221213 12:52:08 @agent_ppo2.py:137][0m Total time:      36.18 min
[32m[20221213 12:52:08 @agent_ppo2.py:139][0m 2490368 total steps have happened
[32m[20221213 12:52:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1216 --------------------------#
[32m[20221213 12:52:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:08 @agent_ppo2.py:179][0m |           0.0554 |          32.2257 |          -7.7590 |
[32m[20221213 12:52:08 @agent_ppo2.py:179][0m |           0.0706 |          31.1588 |          -4.9890 |
[32m[20221213 12:52:08 @agent_ppo2.py:179][0m |           0.0113 |          30.6729 |          -4.5692 |
[32m[20221213 12:52:09 @agent_ppo2.py:179][0m |          -0.0126 |          30.4111 |          -5.6492 |
[32m[20221213 12:52:09 @agent_ppo2.py:179][0m |          -0.0155 |          30.2812 |          -5.7298 |
[32m[20221213 12:52:09 @agent_ppo2.py:179][0m |          -0.0282 |          30.1700 |          -6.2676 |
[32m[20221213 12:52:09 @agent_ppo2.py:179][0m |          -0.0288 |          29.9343 |          -6.7199 |
[32m[20221213 12:52:09 @agent_ppo2.py:179][0m |          -0.0313 |          30.2839 |          -7.6551 |
[32m[20221213 12:52:09 @agent_ppo2.py:179][0m |          -0.0410 |          29.8161 |          -7.5314 |
[32m[20221213 12:52:09 @agent_ppo2.py:179][0m |          -0.0325 |          29.7070 |          -7.4491 |
[32m[20221213 12:52:09 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:52:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 234.96
[32m[20221213 12:52:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.63
[32m[20221213 12:52:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.01
[32m[20221213 12:52:09 @agent_ppo2.py:137][0m Total time:      36.21 min
[32m[20221213 12:52:09 @agent_ppo2.py:139][0m 2492416 total steps have happened
[32m[20221213 12:52:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1217 --------------------------#
[32m[20221213 12:52:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:10 @agent_ppo2.py:179][0m |           0.0606 |          35.2451 |          -7.3304 |
[32m[20221213 12:52:10 @agent_ppo2.py:179][0m |           0.0482 |          34.3041 |          -3.8158 |
[32m[20221213 12:52:10 @agent_ppo2.py:179][0m |           0.0214 |          33.9083 |          -5.6553 |
[32m[20221213 12:52:10 @agent_ppo2.py:179][0m |           0.0065 |          37.3505 |          -7.0859 |
[32m[20221213 12:52:10 @agent_ppo2.py:179][0m |           0.0068 |          33.5817 |          -6.5084 |
[32m[20221213 12:52:10 @agent_ppo2.py:179][0m |          -0.0133 |          33.2744 |          -7.4648 |
[32m[20221213 12:52:11 @agent_ppo2.py:179][0m |          -0.0206 |          33.1402 |          -8.0661 |
[32m[20221213 12:52:11 @agent_ppo2.py:179][0m |          -0.0178 |          34.3353 |          -8.2566 |
[32m[20221213 12:52:11 @agent_ppo2.py:179][0m |          -0.0159 |          33.0371 |          -8.0001 |
[32m[20221213 12:52:11 @agent_ppo2.py:179][0m |          -0.0297 |          32.8503 |          -8.8699 |
[32m[20221213 12:52:11 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:52:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 355.31
[32m[20221213 12:52:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.26
[32m[20221213 12:52:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 385.63
[32m[20221213 12:52:11 @agent_ppo2.py:137][0m Total time:      36.24 min
[32m[20221213 12:52:11 @agent_ppo2.py:139][0m 2494464 total steps have happened
[32m[20221213 12:52:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1218 --------------------------#
[32m[20221213 12:52:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |           0.0998 |          35.5947 |          -6.1946 |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |           0.0653 |          34.4565 |          -2.6529 |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |           0.0202 |          34.8326 |          -4.7832 |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |          -0.0055 |          33.7759 |          -6.6468 |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |          -0.0172 |          33.4813 |          -7.0934 |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |          -0.0274 |          33.3819 |          -7.7536 |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |          -0.0242 |          33.1751 |          -8.1022 |
[32m[20221213 12:52:12 @agent_ppo2.py:179][0m |          -0.0335 |          32.9648 |          -8.4268 |
[32m[20221213 12:52:13 @agent_ppo2.py:179][0m |          -0.0359 |          32.8919 |          -8.8041 |
[32m[20221213 12:52:13 @agent_ppo2.py:179][0m |          -0.0286 |          32.8938 |          -9.0061 |
[32m[20221213 12:52:13 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:52:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 337.93
[32m[20221213 12:52:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.77
[32m[20221213 12:52:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 309.22
[32m[20221213 12:52:13 @agent_ppo2.py:137][0m Total time:      36.27 min
[32m[20221213 12:52:13 @agent_ppo2.py:139][0m 2496512 total steps have happened
[32m[20221213 12:52:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1219 --------------------------#
[32m[20221213 12:52:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:13 @agent_ppo2.py:179][0m |           0.0664 |          36.1237 |          -6.1661 |
[32m[20221213 12:52:13 @agent_ppo2.py:179][0m |           0.0268 |          35.3029 |          -4.3331 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |           0.0068 |          34.9038 |          -5.2657 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |          -0.0116 |          34.7784 |          -6.5889 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |          -0.0170 |          34.5636 |          -6.9348 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |          -0.0235 |          34.3603 |          -7.2112 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |          -0.0293 |          34.2624 |          -8.1579 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |          -0.0291 |          34.2610 |          -8.3697 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |          -0.0352 |          34.1878 |          -8.5847 |
[32m[20221213 12:52:14 @agent_ppo2.py:179][0m |          -0.0298 |          34.2651 |          -9.3819 |
[32m[20221213 12:52:14 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:52:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 355.82
[32m[20221213 12:52:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 377.19
[32m[20221213 12:52:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 383.38
[32m[20221213 12:52:15 @agent_ppo2.py:137][0m Total time:      36.30 min
[32m[20221213 12:52:15 @agent_ppo2.py:139][0m 2498560 total steps have happened
[32m[20221213 12:52:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1220 --------------------------#
[32m[20221213 12:52:15 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:52:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:15 @agent_ppo2.py:179][0m |           0.0571 |          35.6707 |          -6.5354 |
[32m[20221213 12:52:15 @agent_ppo2.py:179][0m |           0.0436 |          34.4351 |          -3.8656 |
[32m[20221213 12:52:15 @agent_ppo2.py:179][0m |           0.0084 |          33.9585 |          -4.6512 |
[32m[20221213 12:52:15 @agent_ppo2.py:179][0m |          -0.0138 |          33.7013 |          -6.2974 |
[32m[20221213 12:52:16 @agent_ppo2.py:179][0m |          -0.0102 |          35.9625 |          -6.6757 |
[32m[20221213 12:52:16 @agent_ppo2.py:179][0m |          -0.0192 |          33.5274 |          -7.1367 |
[32m[20221213 12:52:16 @agent_ppo2.py:179][0m |          -0.0235 |          33.1650 |          -7.9591 |
[32m[20221213 12:52:16 @agent_ppo2.py:179][0m |          -0.0313 |          32.9512 |          -8.3301 |
[32m[20221213 12:52:16 @agent_ppo2.py:179][0m |          -0.0359 |          32.8423 |          -8.3589 |
[32m[20221213 12:52:16 @agent_ppo2.py:179][0m |          -0.0371 |          32.7376 |          -8.3874 |
[32m[20221213 12:52:16 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:52:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.05
[32m[20221213 12:52:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.01
[32m[20221213 12:52:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.01
[32m[20221213 12:52:16 @agent_ppo2.py:137][0m Total time:      36.33 min
[32m[20221213 12:52:16 @agent_ppo2.py:139][0m 2500608 total steps have happened
[32m[20221213 12:52:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1221 --------------------------#
[32m[20221213 12:52:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:17 @agent_ppo2.py:179][0m |           0.0908 |          35.4381 |          -3.3523 |
[32m[20221213 12:52:17 @agent_ppo2.py:179][0m |           0.0698 |          34.4607 |          -0.9979 |
[32m[20221213 12:52:17 @agent_ppo2.py:179][0m |           0.0422 |          34.1460 |          -2.6040 |
[32m[20221213 12:52:17 @agent_ppo2.py:179][0m |           0.0065 |          33.8510 |          -3.3765 |
[32m[20221213 12:52:17 @agent_ppo2.py:179][0m |          -0.0105 |          33.5839 |          -4.1723 |
[32m[20221213 12:52:17 @agent_ppo2.py:179][0m |          -0.0221 |          33.4332 |          -5.1317 |
[32m[20221213 12:52:17 @agent_ppo2.py:179][0m |          -0.0273 |          33.4191 |          -5.4499 |
[32m[20221213 12:52:18 @agent_ppo2.py:179][0m |          -0.0297 |          33.1174 |          -5.5357 |
[32m[20221213 12:52:18 @agent_ppo2.py:179][0m |          -0.0324 |          33.0880 |          -6.0215 |
[32m[20221213 12:52:18 @agent_ppo2.py:179][0m |          -0.0358 |          32.8689 |          -6.2846 |
[32m[20221213 12:52:18 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:52:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.07
[32m[20221213 12:52:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.56
[32m[20221213 12:52:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.66
[32m[20221213 12:52:18 @agent_ppo2.py:137][0m Total time:      36.35 min
[32m[20221213 12:52:18 @agent_ppo2.py:139][0m 2502656 total steps have happened
[32m[20221213 12:52:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1222 --------------------------#
[32m[20221213 12:52:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:18 @agent_ppo2.py:179][0m |           0.0430 |          36.1177 |          -4.7818 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |           0.0192 |          35.2961 |          -4.0253 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |           0.0023 |          34.1609 |          -4.2050 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |           0.0067 |          40.8664 |          -4.6051 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |          -0.0110 |          37.5172 |          -4.9093 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |          -0.0161 |          38.2371 |          -5.7767 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |          -0.0301 |          33.1109 |          -5.8943 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |          -0.0319 |          32.9768 |          -6.2857 |
[32m[20221213 12:52:19 @agent_ppo2.py:179][0m |          -0.0391 |          32.6446 |          -6.9373 |
[32m[20221213 12:52:20 @agent_ppo2.py:179][0m |          -0.0297 |          34.8262 |          -7.5550 |
[32m[20221213 12:52:20 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:52:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.57
[32m[20221213 12:52:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.54
[32m[20221213 12:52:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.58
[32m[20221213 12:52:20 @agent_ppo2.py:137][0m Total time:      36.38 min
[32m[20221213 12:52:20 @agent_ppo2.py:139][0m 2504704 total steps have happened
[32m[20221213 12:52:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1223 --------------------------#
[32m[20221213 12:52:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:20 @agent_ppo2.py:179][0m |           0.0705 |          38.6053 |          -2.5231 |
[32m[20221213 12:52:20 @agent_ppo2.py:179][0m |           0.0325 |          34.4198 |          -0.8972 |
[32m[20221213 12:52:20 @agent_ppo2.py:179][0m |          -0.0017 |          33.7033 |          -3.9639 |
[32m[20221213 12:52:21 @agent_ppo2.py:179][0m |          -0.0104 |          33.2876 |          -4.2731 |
[32m[20221213 12:52:21 @agent_ppo2.py:179][0m |          -0.0172 |          33.0187 |          -4.6390 |
[32m[20221213 12:52:21 @agent_ppo2.py:179][0m |          -0.0257 |          32.7296 |          -5.5222 |
[32m[20221213 12:52:21 @agent_ppo2.py:179][0m |          -0.0280 |          32.5373 |          -6.1689 |
[32m[20221213 12:52:21 @agent_ppo2.py:179][0m |          -0.0246 |          33.3267 |          -6.1114 |
[32m[20221213 12:52:21 @agent_ppo2.py:179][0m |          -0.0380 |          32.2057 |          -6.7220 |
[32m[20221213 12:52:21 @agent_ppo2.py:179][0m |          -0.0357 |          32.1848 |          -6.8960 |
[32m[20221213 12:52:21 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:52:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.01
[32m[20221213 12:52:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.07
[32m[20221213 12:52:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.81
[32m[20221213 12:52:21 @agent_ppo2.py:137][0m Total time:      36.41 min
[32m[20221213 12:52:21 @agent_ppo2.py:139][0m 2506752 total steps have happened
[32m[20221213 12:52:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1224 --------------------------#
[32m[20221213 12:52:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:22 @agent_ppo2.py:179][0m |           0.0552 |          35.4866 |          -4.8482 |
[32m[20221213 12:52:22 @agent_ppo2.py:179][0m |           0.0401 |          34.9875 |          -3.0358 |
[32m[20221213 12:52:22 @agent_ppo2.py:179][0m |           0.0106 |          33.9888 |          -4.5503 |
[32m[20221213 12:52:22 @agent_ppo2.py:179][0m |          -0.0126 |          33.3774 |          -5.7834 |
[32m[20221213 12:52:22 @agent_ppo2.py:179][0m |          -0.0251 |          33.1208 |          -6.3855 |
[32m[20221213 12:52:22 @agent_ppo2.py:179][0m |          -0.0315 |          32.8446 |          -6.3232 |
[32m[20221213 12:52:23 @agent_ppo2.py:179][0m |          -0.0368 |          32.6940 |          -7.1490 |
[32m[20221213 12:52:23 @agent_ppo2.py:179][0m |          -0.0412 |          32.4862 |          -7.7006 |
[32m[20221213 12:52:23 @agent_ppo2.py:179][0m |          -0.0390 |          32.2798 |          -8.0585 |
[32m[20221213 12:52:23 @agent_ppo2.py:179][0m |          -0.0448 |          32.1632 |          -8.2992 |
[32m[20221213 12:52:23 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:52:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.06
[32m[20221213 12:52:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.34
[32m[20221213 12:52:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 367.37
[32m[20221213 12:52:23 @agent_ppo2.py:137][0m Total time:      36.44 min
[32m[20221213 12:52:23 @agent_ppo2.py:139][0m 2508800 total steps have happened
[32m[20221213 12:52:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1225 --------------------------#
[32m[20221213 12:52:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |           0.0710 |          34.3545 |          -5.4958 |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |           0.0453 |          32.6279 |          -2.0136 |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |           0.0278 |          31.9826 |          -3.9109 |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |          -0.0006 |          31.7420 |          -4.9696 |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |          -0.0225 |          30.8828 |          -6.0287 |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |          -0.0313 |          30.5545 |          -6.7245 |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |          -0.0328 |          30.1521 |          -7.0767 |
[32m[20221213 12:52:24 @agent_ppo2.py:179][0m |          -0.0384 |          29.8973 |          -7.8888 |
[32m[20221213 12:52:25 @agent_ppo2.py:179][0m |          -0.0423 |          29.6253 |          -8.6079 |
[32m[20221213 12:52:25 @agent_ppo2.py:179][0m |          -0.0453 |          29.4347 |          -8.8720 |
[32m[20221213 12:52:25 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:52:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 318.73
[32m[20221213 12:52:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.54
[32m[20221213 12:52:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.81
[32m[20221213 12:52:25 @agent_ppo2.py:137][0m Total time:      36.47 min
[32m[20221213 12:52:25 @agent_ppo2.py:139][0m 2510848 total steps have happened
[32m[20221213 12:52:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1226 --------------------------#
[32m[20221213 12:52:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:25 @agent_ppo2.py:179][0m |           0.0533 |          35.9168 |          -6.8233 |
[32m[20221213 12:52:25 @agent_ppo2.py:179][0m |           0.0434 |          35.0504 |          -4.7742 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |           0.0108 |          34.4637 |          -5.7626 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |          -0.0087 |          34.2837 |          -6.8045 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |          -0.0255 |          34.0057 |          -8.1775 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |          -0.0302 |          33.7394 |          -9.0711 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |          -0.0313 |          33.6170 |          -9.1196 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |          -0.0349 |          33.4703 |          -9.5127 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |          -0.0263 |          33.4267 |          -9.2264 |
[32m[20221213 12:52:26 @agent_ppo2.py:179][0m |          -0.0398 |          33.2465 |          -9.9413 |
[32m[20221213 12:52:26 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:52:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 337.83
[32m[20221213 12:52:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.88
[32m[20221213 12:52:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 41.08
[32m[20221213 12:52:27 @agent_ppo2.py:137][0m Total time:      36.50 min
[32m[20221213 12:52:27 @agent_ppo2.py:139][0m 2512896 total steps have happened
[32m[20221213 12:52:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1227 --------------------------#
[32m[20221213 12:52:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:27 @agent_ppo2.py:179][0m |           0.0754 |          28.1109 |          -6.2089 |
[32m[20221213 12:52:27 @agent_ppo2.py:179][0m |           0.0223 |          26.5362 |          -4.3241 |
[32m[20221213 12:52:27 @agent_ppo2.py:179][0m |          -0.0241 |          26.0691 |          -5.9910 |
[32m[20221213 12:52:27 @agent_ppo2.py:179][0m |          -0.0456 |          25.8246 |          -6.4952 |
[32m[20221213 12:52:27 @agent_ppo2.py:179][0m |          -0.0504 |          25.5696 |          -6.8725 |
[32m[20221213 12:52:28 @agent_ppo2.py:179][0m |          -0.0478 |          25.6233 |          -7.8423 |
[32m[20221213 12:52:28 @agent_ppo2.py:179][0m |          -0.0623 |          25.1622 |          -8.2321 |
[32m[20221213 12:52:28 @agent_ppo2.py:179][0m |          -0.0663 |          25.1683 |          -8.6433 |
[32m[20221213 12:52:28 @agent_ppo2.py:179][0m |          -0.0515 |          28.5746 |          -9.1477 |
[32m[20221213 12:52:28 @agent_ppo2.py:179][0m |          -0.0683 |          25.4039 |          -9.3775 |
[32m[20221213 12:52:28 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:52:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 215.03
[32m[20221213 12:52:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.59
[32m[20221213 12:52:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 331.25
[32m[20221213 12:52:28 @agent_ppo2.py:137][0m Total time:      36.53 min
[32m[20221213 12:52:28 @agent_ppo2.py:139][0m 2514944 total steps have happened
[32m[20221213 12:52:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1228 --------------------------#
[32m[20221213 12:52:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:29 @agent_ppo2.py:179][0m |           0.0828 |          36.2202 |         -11.6182 |
[32m[20221213 12:52:29 @agent_ppo2.py:179][0m |           0.1065 |          35.5134 |          -4.1608 |
[32m[20221213 12:52:29 @agent_ppo2.py:179][0m |           0.0437 |          34.7458 |          -7.2526 |
[32m[20221213 12:52:29 @agent_ppo2.py:179][0m |           0.0117 |          36.1615 |         -10.6347 |
[32m[20221213 12:52:29 @agent_ppo2.py:179][0m |          -0.0127 |          34.1572 |         -11.7122 |
[32m[20221213 12:52:29 @agent_ppo2.py:179][0m |          -0.0206 |          34.2213 |         -11.8035 |
[32m[20221213 12:52:29 @agent_ppo2.py:179][0m |          -0.0286 |          33.8016 |         -12.4464 |
[32m[20221213 12:52:30 @agent_ppo2.py:179][0m |          -0.0213 |          36.2843 |         -12.2698 |
[32m[20221213 12:52:30 @agent_ppo2.py:179][0m |          -0.0372 |          33.5748 |         -13.1974 |
[32m[20221213 12:52:30 @agent_ppo2.py:179][0m |          -0.0311 |          33.5132 |         -13.6709 |
[32m[20221213 12:52:30 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:52:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.14
[32m[20221213 12:52:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.07
[32m[20221213 12:52:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.34
[32m[20221213 12:52:30 @agent_ppo2.py:137][0m Total time:      36.55 min
[32m[20221213 12:52:30 @agent_ppo2.py:139][0m 2516992 total steps have happened
[32m[20221213 12:52:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1229 --------------------------#
[32m[20221213 12:52:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:30 @agent_ppo2.py:179][0m |           0.0494 |          37.6786 |         -10.0678 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |           0.0592 |          36.4195 |          -5.6125 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |           0.0268 |          35.9615 |          -8.1141 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |           0.0013 |          35.6326 |          -8.8892 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |          -0.0170 |          35.4335 |         -10.0408 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |          -0.0198 |          35.5844 |         -10.6180 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |          -0.0277 |          35.0377 |         -11.8512 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |          -0.0254 |          35.7273 |         -11.7247 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |          -0.0365 |          34.6781 |         -12.2792 |
[32m[20221213 12:52:31 @agent_ppo2.py:179][0m |          -0.0381 |          34.6129 |         -12.0162 |
[32m[20221213 12:52:31 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:52:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 337.72
[32m[20221213 12:52:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.31
[32m[20221213 12:52:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.61
[32m[20221213 12:52:32 @agent_ppo2.py:137][0m Total time:      36.58 min
[32m[20221213 12:52:32 @agent_ppo2.py:139][0m 2519040 total steps have happened
[32m[20221213 12:52:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1230 --------------------------#
[32m[20221213 12:52:32 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:52:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:32 @agent_ppo2.py:179][0m |           0.0874 |          36.8360 |          -9.7661 |
[32m[20221213 12:52:32 @agent_ppo2.py:179][0m |           0.0366 |          35.3354 |          -7.2757 |
[32m[20221213 12:52:32 @agent_ppo2.py:179][0m |          -0.0006 |          35.5804 |         -10.1306 |
[32m[20221213 12:52:33 @agent_ppo2.py:179][0m |          -0.0159 |          34.3286 |         -10.6251 |
[32m[20221213 12:52:33 @agent_ppo2.py:179][0m |          -0.0288 |          33.8210 |         -11.7990 |
[32m[20221213 12:52:33 @agent_ppo2.py:179][0m |          -0.0344 |          33.5596 |         -12.6414 |
[32m[20221213 12:52:33 @agent_ppo2.py:179][0m |          -0.0195 |          36.9653 |         -12.6184 |
[32m[20221213 12:52:33 @agent_ppo2.py:179][0m |          -0.0232 |          33.2292 |         -12.4084 |
[32m[20221213 12:52:33 @agent_ppo2.py:179][0m |          -0.0344 |          32.8328 |         -13.3467 |
[32m[20221213 12:52:33 @agent_ppo2.py:179][0m |          -0.0405 |          32.5092 |         -14.1156 |
[32m[20221213 12:52:33 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:52:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.26
[32m[20221213 12:52:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.76
[32m[20221213 12:52:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.34
[32m[20221213 12:52:33 @agent_ppo2.py:137][0m Total time:      36.61 min
[32m[20221213 12:52:33 @agent_ppo2.py:139][0m 2521088 total steps have happened
[32m[20221213 12:52:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1231 --------------------------#
[32m[20221213 12:52:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:34 @agent_ppo2.py:179][0m |           0.2531 |          31.7291 |          -9.0636 |
[32m[20221213 12:52:34 @agent_ppo2.py:179][0m |           0.0504 |          30.4621 |          -4.9706 |
[32m[20221213 12:52:34 @agent_ppo2.py:179][0m |           0.0003 |          29.8741 |          -7.9395 |
[32m[20221213 12:52:34 @agent_ppo2.py:179][0m |          -0.0185 |          29.4865 |          -8.9480 |
[32m[20221213 12:52:34 @agent_ppo2.py:179][0m |          -0.0191 |          32.5354 |         -10.3070 |
[32m[20221213 12:52:34 @agent_ppo2.py:179][0m |          -0.0399 |          29.1972 |         -10.9152 |
[32m[20221213 12:52:35 @agent_ppo2.py:179][0m |          -0.0436 |          28.7953 |         -11.3147 |
[32m[20221213 12:52:35 @agent_ppo2.py:179][0m |          -0.0369 |          31.5935 |         -12.2824 |
[32m[20221213 12:52:35 @agent_ppo2.py:179][0m |          -0.0402 |          28.5028 |         -11.8399 |
[32m[20221213 12:52:35 @agent_ppo2.py:179][0m |          -0.0482 |          28.3292 |         -12.9582 |
[32m[20221213 12:52:35 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:52:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 275.05
[32m[20221213 12:52:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.89
[32m[20221213 12:52:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.59
[32m[20221213 12:52:35 @agent_ppo2.py:137][0m Total time:      36.64 min
[32m[20221213 12:52:35 @agent_ppo2.py:139][0m 2523136 total steps have happened
[32m[20221213 12:52:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1232 --------------------------#
[32m[20221213 12:52:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |           0.0425 |          36.2836 |         -11.5736 |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |           0.0354 |          35.2072 |         -10.8414 |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |           0.0214 |          34.6511 |          -9.9997 |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |          -0.0119 |          34.3470 |         -12.9915 |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |          -0.0179 |          34.0758 |         -13.1755 |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |          -0.0256 |          34.0117 |         -13.3914 |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |          -0.0329 |          33.6414 |         -14.6122 |
[32m[20221213 12:52:36 @agent_ppo2.py:179][0m |          -0.0264 |          34.9706 |         -15.4134 |
[32m[20221213 12:52:37 @agent_ppo2.py:179][0m |          -0.0164 |          33.3420 |         -14.4628 |
[32m[20221213 12:52:37 @agent_ppo2.py:179][0m |          -0.0287 |          33.1399 |         -15.1613 |
[32m[20221213 12:52:37 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:52:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 327.53
[32m[20221213 12:52:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.74
[32m[20221213 12:52:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 55.82
[32m[20221213 12:52:37 @agent_ppo2.py:137][0m Total time:      36.67 min
[32m[20221213 12:52:37 @agent_ppo2.py:139][0m 2525184 total steps have happened
[32m[20221213 12:52:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1233 --------------------------#
[32m[20221213 12:52:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:37 @agent_ppo2.py:179][0m |           0.0750 |          36.5872 |         -11.1137 |
[32m[20221213 12:52:37 @agent_ppo2.py:179][0m |           0.0322 |          35.0506 |          -9.5005 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |           0.0016 |          34.5152 |         -11.3503 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |          -0.0091 |          34.1357 |         -12.1114 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |          -0.0186 |          33.9365 |         -11.9765 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |          -0.0258 |          33.8528 |         -12.9122 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |          -0.0307 |          33.6653 |         -13.0506 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |          -0.0353 |          33.5685 |         -13.7742 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |          -0.0323 |          34.3882 |         -14.5245 |
[32m[20221213 12:52:38 @agent_ppo2.py:179][0m |           0.0047 |          33.4283 |         -12.8517 |
[32m[20221213 12:52:38 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:52:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.01
[32m[20221213 12:52:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.34
[32m[20221213 12:52:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.80
[32m[20221213 12:52:39 @agent_ppo2.py:137][0m Total time:      36.70 min
[32m[20221213 12:52:39 @agent_ppo2.py:139][0m 2527232 total steps have happened
[32m[20221213 12:52:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1234 --------------------------#
[32m[20221213 12:52:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:39 @agent_ppo2.py:179][0m |           0.0894 |          35.7937 |         -12.1897 |
[32m[20221213 12:52:39 @agent_ppo2.py:179][0m |           0.0516 |          37.4091 |         -10.2277 |
[32m[20221213 12:52:39 @agent_ppo2.py:179][0m |           0.0042 |          34.6110 |         -12.9807 |
[32m[20221213 12:52:39 @agent_ppo2.py:179][0m |           0.0009 |          34.1534 |         -13.6560 |
[32m[20221213 12:52:40 @agent_ppo2.py:179][0m |          -0.0158 |          33.9961 |         -14.5327 |
[32m[20221213 12:52:40 @agent_ppo2.py:179][0m |          -0.0252 |          33.8351 |         -15.7416 |
[32m[20221213 12:52:40 @agent_ppo2.py:179][0m |          -0.0314 |          33.6623 |         -15.7540 |
[32m[20221213 12:52:40 @agent_ppo2.py:179][0m |          -0.0216 |          37.6915 |         -16.3566 |
[32m[20221213 12:52:40 @agent_ppo2.py:179][0m |          -0.0296 |          33.4092 |         -16.2642 |
[32m[20221213 12:52:40 @agent_ppo2.py:179][0m |          -0.0335 |          33.2962 |         -16.6795 |
[32m[20221213 12:52:40 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:52:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.69
[32m[20221213 12:52:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.40
[32m[20221213 12:52:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.28
[32m[20221213 12:52:40 @agent_ppo2.py:137][0m Total time:      36.73 min
[32m[20221213 12:52:40 @agent_ppo2.py:139][0m 2529280 total steps have happened
[32m[20221213 12:52:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1235 --------------------------#
[32m[20221213 12:52:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:41 @agent_ppo2.py:179][0m |           0.0626 |          35.8171 |          -9.3237 |
[32m[20221213 12:52:41 @agent_ppo2.py:179][0m |           0.0558 |          34.2531 |          -5.8934 |
[32m[20221213 12:52:41 @agent_ppo2.py:179][0m |           0.0102 |          33.4308 |          -8.5166 |
[32m[20221213 12:52:41 @agent_ppo2.py:179][0m |          -0.0074 |          32.9240 |         -10.4995 |
[32m[20221213 12:52:41 @agent_ppo2.py:179][0m |          -0.0118 |          32.9127 |         -10.7626 |
[32m[20221213 12:52:41 @agent_ppo2.py:179][0m |          -0.0195 |          32.7207 |         -11.6198 |
[32m[20221213 12:52:42 @agent_ppo2.py:179][0m |          -0.0289 |          32.1612 |         -12.3054 |
[32m[20221213 12:52:42 @agent_ppo2.py:179][0m |          -0.0264 |          33.0553 |         -12.6088 |
[32m[20221213 12:52:42 @agent_ppo2.py:179][0m |          -0.0296 |          31.8859 |         -12.8086 |
[32m[20221213 12:52:42 @agent_ppo2.py:179][0m |          -0.0231 |          35.2101 |         -13.5607 |
[32m[20221213 12:52:42 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:52:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 337.79
[32m[20221213 12:52:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.75
[32m[20221213 12:52:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.42
[32m[20221213 12:52:42 @agent_ppo2.py:137][0m Total time:      36.76 min
[32m[20221213 12:52:42 @agent_ppo2.py:139][0m 2531328 total steps have happened
[32m[20221213 12:52:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1236 --------------------------#
[32m[20221213 12:52:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |           0.0851 |          39.6706 |          -8.9364 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |           0.1101 |          34.2656 |          -4.6150 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |           0.0323 |          33.4978 |          -7.4938 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |          -0.0032 |          33.1340 |         -10.3517 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |          -0.0130 |          32.8940 |         -11.2543 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |          -0.0247 |          32.6734 |         -12.2158 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |          -0.0353 |          32.5479 |         -12.6684 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |          -0.0277 |          32.7553 |         -12.8492 |
[32m[20221213 12:52:43 @agent_ppo2.py:179][0m |          -0.0379 |          32.2893 |         -13.8046 |
[32m[20221213 12:52:44 @agent_ppo2.py:179][0m |          -0.0301 |          33.5135 |         -13.4256 |
[32m[20221213 12:52:44 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:52:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.80
[32m[20221213 12:52:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.85
[32m[20221213 12:52:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 380.30
[32m[20221213 12:52:44 @agent_ppo2.py:137][0m Total time:      36.78 min
[32m[20221213 12:52:44 @agent_ppo2.py:139][0m 2533376 total steps have happened
[32m[20221213 12:52:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1237 --------------------------#
[32m[20221213 12:52:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:44 @agent_ppo2.py:179][0m |           0.0518 |          33.4276 |          -9.8371 |
[32m[20221213 12:52:44 @agent_ppo2.py:179][0m |           0.0203 |          31.7658 |          -7.1964 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0020 |          31.5964 |          -9.3968 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0189 |          30.5218 |          -9.4931 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0327 |          30.0172 |         -10.6232 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0377 |          29.7449 |         -11.2520 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0398 |          29.4160 |         -11.6209 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0412 |          29.2172 |         -12.4538 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0468 |          28.9300 |         -13.0689 |
[32m[20221213 12:52:45 @agent_ppo2.py:179][0m |          -0.0421 |          28.7798 |         -13.5622 |
[32m[20221213 12:52:45 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:52:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.19
[32m[20221213 12:52:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.80
[32m[20221213 12:52:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 370.22
[32m[20221213 12:52:46 @agent_ppo2.py:137][0m Total time:      36.81 min
[32m[20221213 12:52:46 @agent_ppo2.py:139][0m 2535424 total steps have happened
[32m[20221213 12:52:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1238 --------------------------#
[32m[20221213 12:52:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:46 @agent_ppo2.py:179][0m |           0.0379 |          28.4592 |          -8.4596 |
[32m[20221213 12:52:46 @agent_ppo2.py:179][0m |           0.0047 |          25.7920 |          -7.0424 |
[32m[20221213 12:52:46 @agent_ppo2.py:179][0m |          -0.0259 |          24.7897 |          -8.9503 |
[32m[20221213 12:52:46 @agent_ppo2.py:179][0m |          -0.0388 |          23.8412 |          -9.1675 |
[32m[20221213 12:52:47 @agent_ppo2.py:179][0m |          -0.0427 |          23.1690 |          -9.9713 |
[32m[20221213 12:52:47 @agent_ppo2.py:179][0m |          -0.0463 |          22.9145 |         -10.3701 |
[32m[20221213 12:52:47 @agent_ppo2.py:179][0m |          -0.0567 |          22.3838 |         -11.5384 |
[32m[20221213 12:52:47 @agent_ppo2.py:179][0m |          -0.0592 |          22.0059 |         -11.9933 |
[32m[20221213 12:52:47 @agent_ppo2.py:179][0m |          -0.0664 |          21.7896 |         -13.0564 |
[32m[20221213 12:52:47 @agent_ppo2.py:179][0m |          -0.0590 |          21.5133 |         -12.5890 |
[32m[20221213 12:52:47 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:52:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 244.45
[32m[20221213 12:52:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.82
[32m[20221213 12:52:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 366.87
[32m[20221213 12:52:47 @agent_ppo2.py:137][0m Total time:      36.84 min
[32m[20221213 12:52:47 @agent_ppo2.py:139][0m 2537472 total steps have happened
[32m[20221213 12:52:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1239 --------------------------#
[32m[20221213 12:52:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:48 @agent_ppo2.py:179][0m |           0.0765 |          34.7965 |         -13.3674 |
[32m[20221213 12:52:48 @agent_ppo2.py:179][0m |           0.0589 |          32.9225 |          -8.0400 |
[32m[20221213 12:52:48 @agent_ppo2.py:179][0m |           0.0203 |          32.3213 |          -9.8991 |
[32m[20221213 12:52:48 @agent_ppo2.py:179][0m |          -0.0051 |          32.0446 |         -11.0892 |
[32m[20221213 12:52:48 @agent_ppo2.py:179][0m |          -0.0131 |          31.6594 |         -12.9475 |
[32m[20221213 12:52:48 @agent_ppo2.py:179][0m |          -0.0129 |          33.4299 |         -14.1559 |
[32m[20221213 12:52:49 @agent_ppo2.py:179][0m |          -0.0277 |          31.3302 |         -14.3492 |
[32m[20221213 12:52:49 @agent_ppo2.py:179][0m |          -0.0298 |          31.1870 |         -14.8933 |
[32m[20221213 12:52:49 @agent_ppo2.py:179][0m |          -0.0353 |          30.9667 |         -16.1628 |
[32m[20221213 12:52:49 @agent_ppo2.py:179][0m |          -0.0434 |          30.8294 |         -16.8073 |
[32m[20221213 12:52:49 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:52:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.32
[32m[20221213 12:52:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.36
[32m[20221213 12:52:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.07
[32m[20221213 12:52:49 @agent_ppo2.py:137][0m Total time:      36.87 min
[32m[20221213 12:52:49 @agent_ppo2.py:139][0m 2539520 total steps have happened
[32m[20221213 12:52:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1240 --------------------------#
[32m[20221213 12:52:49 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:52:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |           0.0766 |          34.0521 |         -13.6199 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |           0.0853 |          33.0013 |          -6.6702 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |           0.0332 |          32.6194 |         -10.2067 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |           0.0249 |          36.5806 |         -12.7698 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |          -0.0039 |          32.2437 |         -14.3488 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |          -0.0145 |          32.0652 |         -15.2883 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |          -0.0271 |          31.9116 |         -15.3510 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |          -0.0276 |          31.9856 |         -15.8904 |
[32m[20221213 12:52:50 @agent_ppo2.py:179][0m |          -0.0315 |          31.7110 |         -17.4787 |
[32m[20221213 12:52:51 @agent_ppo2.py:179][0m |          -0.0310 |          31.6507 |         -17.7581 |
[32m[20221213 12:52:51 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:52:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.68
[32m[20221213 12:52:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.06
[32m[20221213 12:52:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.36
[32m[20221213 12:52:51 @agent_ppo2.py:137][0m Total time:      36.90 min
[32m[20221213 12:52:51 @agent_ppo2.py:139][0m 2541568 total steps have happened
[32m[20221213 12:52:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1241 --------------------------#
[32m[20221213 12:52:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:51 @agent_ppo2.py:179][0m |           0.0791 |          33.4354 |         -12.7473 |
[32m[20221213 12:52:51 @agent_ppo2.py:179][0m |           0.0686 |          32.6691 |          -9.5049 |
[32m[20221213 12:52:51 @agent_ppo2.py:179][0m |          -0.0001 |          31.8936 |         -13.5252 |
[32m[20221213 12:52:52 @agent_ppo2.py:179][0m |          -0.0155 |          31.6084 |         -14.4903 |
[32m[20221213 12:52:52 @agent_ppo2.py:179][0m |          -0.0160 |          31.3489 |         -14.9550 |
[32m[20221213 12:52:52 @agent_ppo2.py:179][0m |          -0.0184 |          31.2209 |         -14.6624 |
[32m[20221213 12:52:52 @agent_ppo2.py:179][0m |          -0.0289 |          31.0422 |         -15.1212 |
[32m[20221213 12:52:53 @agent_ppo2.py:179][0m |          -0.0326 |          30.9660 |         -16.6837 |
[32m[20221213 12:52:53 @agent_ppo2.py:179][0m |          -0.0364 |          30.8381 |         -17.0469 |
[32m[20221213 12:52:53 @agent_ppo2.py:179][0m |          -0.0397 |          30.7729 |         -16.9148 |
[32m[20221213 12:52:53 @agent_ppo2.py:124][0m Policy update time: 1.88 s
[32m[20221213 12:52:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.64
[32m[20221213 12:52:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 374.31
[32m[20221213 12:52:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.12
[32m[20221213 12:52:53 @agent_ppo2.py:137][0m Total time:      36.94 min
[32m[20221213 12:52:53 @agent_ppo2.py:139][0m 2543616 total steps have happened
[32m[20221213 12:52:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1242 --------------------------#
[32m[20221213 12:52:53 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:52:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |           0.0737 |          36.9419 |         -14.9844 |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |           0.0669 |          33.0115 |          -8.7632 |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |           0.0174 |          32.0469 |         -11.9618 |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |          -0.0004 |          31.6642 |         -12.7697 |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |          -0.0195 |          31.2678 |         -14.2804 |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |          -0.0262 |          31.0020 |         -15.2722 |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |          -0.0315 |          30.7114 |         -15.9037 |
[32m[20221213 12:52:54 @agent_ppo2.py:179][0m |          -0.0249 |          33.5114 |         -16.7499 |
[32m[20221213 12:52:55 @agent_ppo2.py:179][0m |          -0.0376 |          30.5454 |         -17.5294 |
[32m[20221213 12:52:55 @agent_ppo2.py:179][0m |          -0.0402 |          30.1799 |         -17.8665 |
[32m[20221213 12:52:55 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:52:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.66
[32m[20221213 12:52:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.95
[32m[20221213 12:52:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.97
[32m[20221213 12:52:55 @agent_ppo2.py:137][0m Total time:      36.97 min
[32m[20221213 12:52:55 @agent_ppo2.py:139][0m 2545664 total steps have happened
[32m[20221213 12:52:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1243 --------------------------#
[32m[20221213 12:52:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:55 @agent_ppo2.py:179][0m |           0.0751 |          33.9958 |         -12.1399 |
[32m[20221213 12:52:55 @agent_ppo2.py:179][0m |           0.0455 |          32.9759 |         -10.6097 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |           0.0188 |          32.5363 |         -10.8391 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |          -0.0063 |          32.7910 |         -12.7824 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |           0.0149 |          40.2572 |         -12.3985 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |          -0.0288 |          32.4452 |         -13.2566 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |          -0.0272 |          31.9044 |         -13.4922 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |          -0.0374 |          31.4782 |         -13.8296 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |          -0.0381 |          31.3315 |         -14.3899 |
[32m[20221213 12:52:56 @agent_ppo2.py:179][0m |          -0.0411 |          31.2489 |         -15.2002 |
[32m[20221213 12:52:56 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:52:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.88
[32m[20221213 12:52:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.26
[32m[20221213 12:52:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 324.52
[32m[20221213 12:52:57 @agent_ppo2.py:137][0m Total time:      37.00 min
[32m[20221213 12:52:57 @agent_ppo2.py:139][0m 2547712 total steps have happened
[32m[20221213 12:52:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1244 --------------------------#
[32m[20221213 12:52:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:57 @agent_ppo2.py:179][0m |           0.0597 |          35.1673 |         -10.0832 |
[32m[20221213 12:52:57 @agent_ppo2.py:179][0m |           0.0655 |          34.2462 |          -6.9013 |
[32m[20221213 12:52:57 @agent_ppo2.py:179][0m |           0.0368 |          33.9074 |          -8.0401 |
[32m[20221213 12:52:58 @agent_ppo2.py:179][0m |          -0.0039 |          33.5478 |         -10.2665 |
[32m[20221213 12:52:58 @agent_ppo2.py:179][0m |          -0.0214 |          33.3447 |         -11.1915 |
[32m[20221213 12:52:58 @agent_ppo2.py:179][0m |          -0.0271 |          33.0609 |         -11.7285 |
[32m[20221213 12:52:58 @agent_ppo2.py:179][0m |          -0.0235 |          34.4899 |         -12.0371 |
[32m[20221213 12:52:58 @agent_ppo2.py:179][0m |          -0.0314 |          32.7939 |         -12.4966 |
[32m[20221213 12:52:58 @agent_ppo2.py:179][0m |          -0.0345 |          32.9968 |         -13.3767 |
[32m[20221213 12:52:58 @agent_ppo2.py:179][0m |          -0.0427 |          32.5109 |         -13.6004 |
[32m[20221213 12:52:58 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:52:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.43
[32m[20221213 12:52:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.87
[32m[20221213 12:52:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.86
[32m[20221213 12:52:59 @agent_ppo2.py:137][0m Total time:      37.03 min
[32m[20221213 12:52:59 @agent_ppo2.py:139][0m 2549760 total steps have happened
[32m[20221213 12:52:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1245 --------------------------#
[32m[20221213 12:52:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:52:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:52:59 @agent_ppo2.py:179][0m |           0.0886 |          36.2439 |         -10.5533 |
[32m[20221213 12:52:59 @agent_ppo2.py:179][0m |           0.0738 |          33.7317 |          -3.6357 |
[32m[20221213 12:52:59 @agent_ppo2.py:179][0m |           0.0199 |          33.3758 |          -6.7919 |
[32m[20221213 12:52:59 @agent_ppo2.py:179][0m |           0.0072 |          32.8007 |          -9.0020 |
[32m[20221213 12:53:00 @agent_ppo2.py:179][0m |          -0.0080 |          32.5095 |         -10.2177 |
[32m[20221213 12:53:00 @agent_ppo2.py:179][0m |          -0.0163 |          32.4328 |         -11.3393 |
[32m[20221213 12:53:00 @agent_ppo2.py:179][0m |          -0.0220 |          31.9979 |         -11.4364 |
[32m[20221213 12:53:00 @agent_ppo2.py:179][0m |          -0.0280 |          31.7694 |         -12.6954 |
[32m[20221213 12:53:00 @agent_ppo2.py:179][0m |          -0.0275 |          31.5008 |         -12.5870 |
[32m[20221213 12:53:00 @agent_ppo2.py:179][0m |          -0.0323 |          31.3250 |         -13.6431 |
[32m[20221213 12:53:00 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:53:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.13
[32m[20221213 12:53:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.54
[32m[20221213 12:53:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 383.16
[32m[20221213 12:53:00 @agent_ppo2.py:137][0m Total time:      37.06 min
[32m[20221213 12:53:00 @agent_ppo2.py:139][0m 2551808 total steps have happened
[32m[20221213 12:53:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1246 --------------------------#
[32m[20221213 12:53:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:01 @agent_ppo2.py:179][0m |           0.0794 |          35.6039 |          -7.5190 |
[32m[20221213 12:53:01 @agent_ppo2.py:179][0m |           0.0627 |          34.6570 |          -6.4033 |
[32m[20221213 12:53:01 @agent_ppo2.py:179][0m |           0.0401 |          34.1574 |          -7.8373 |
[32m[20221213 12:53:01 @agent_ppo2.py:179][0m |           0.0129 |          33.9902 |         -10.6603 |
[32m[20221213 12:53:01 @agent_ppo2.py:179][0m |           0.0034 |          34.8282 |         -11.3278 |
[32m[20221213 12:53:01 @agent_ppo2.py:179][0m |          -0.0054 |          35.6971 |         -11.2497 |
[32m[20221213 12:53:02 @agent_ppo2.py:179][0m |          -0.0149 |          33.7393 |         -11.4581 |
[32m[20221213 12:53:02 @agent_ppo2.py:179][0m |          -0.0231 |          33.5876 |         -12.6255 |
[32m[20221213 12:53:02 @agent_ppo2.py:179][0m |          -0.0305 |          33.5636 |         -13.4193 |
[32m[20221213 12:53:02 @agent_ppo2.py:179][0m |          -0.0298 |          33.4663 |         -13.6372 |
[32m[20221213 12:53:02 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:53:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 368.93
[32m[20221213 12:53:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 377.23
[32m[20221213 12:53:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.08
[32m[20221213 12:53:02 @agent_ppo2.py:137][0m Total time:      37.09 min
[32m[20221213 12:53:02 @agent_ppo2.py:139][0m 2553856 total steps have happened
[32m[20221213 12:53:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1247 --------------------------#
[32m[20221213 12:53:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |           0.0630 |          35.2778 |          -9.5511 |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |           0.0532 |          34.3694 |          -6.8958 |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |           0.0185 |          33.9841 |          -6.6805 |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |          -0.0150 |          33.6619 |          -9.0178 |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |          -0.0236 |          33.4464 |          -9.9839 |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |          -0.0338 |          33.2671 |         -10.2762 |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |          -0.0369 |          33.0905 |         -11.0606 |
[32m[20221213 12:53:03 @agent_ppo2.py:179][0m |          -0.0425 |          32.8690 |         -12.0706 |
[32m[20221213 12:53:04 @agent_ppo2.py:179][0m |          -0.0291 |          32.7858 |         -12.3399 |
[32m[20221213 12:53:04 @agent_ppo2.py:179][0m |          -0.0382 |          33.3156 |         -12.7617 |
[32m[20221213 12:53:04 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:53:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 304.44
[32m[20221213 12:53:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.66
[32m[20221213 12:53:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.17
[32m[20221213 12:53:04 @agent_ppo2.py:137][0m Total time:      37.12 min
[32m[20221213 12:53:04 @agent_ppo2.py:139][0m 2555904 total steps have happened
[32m[20221213 12:53:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1248 --------------------------#
[32m[20221213 12:53:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:04 @agent_ppo2.py:179][0m |           0.0419 |          35.2845 |         -11.5489 |
[32m[20221213 12:53:04 @agent_ppo2.py:179][0m |           0.0190 |          34.0460 |         -10.0400 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0053 |          33.4985 |         -10.4434 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0189 |          33.1681 |         -11.2548 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0265 |          32.9155 |         -12.0984 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0326 |          32.7567 |         -12.3376 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0379 |          32.5092 |         -12.9306 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0362 |          32.5274 |         -13.6282 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0404 |          32.2652 |         -13.0037 |
[32m[20221213 12:53:05 @agent_ppo2.py:179][0m |          -0.0388 |          32.2148 |         -13.3855 |
[32m[20221213 12:53:05 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:53:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.23
[32m[20221213 12:53:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.63
[32m[20221213 12:53:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 59.75
[32m[20221213 12:53:06 @agent_ppo2.py:137][0m Total time:      37.15 min
[32m[20221213 12:53:06 @agent_ppo2.py:139][0m 2557952 total steps have happened
[32m[20221213 12:53:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1249 --------------------------#
[32m[20221213 12:53:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:06 @agent_ppo2.py:179][0m |           0.0540 |          35.1964 |          -8.7739 |
[32m[20221213 12:53:06 @agent_ppo2.py:179][0m |           0.0423 |          33.7298 |          -5.5492 |
[32m[20221213 12:53:06 @agent_ppo2.py:179][0m |           0.0010 |          32.9300 |          -7.6694 |
[32m[20221213 12:53:06 @agent_ppo2.py:179][0m |          -0.0202 |          32.3815 |          -8.9868 |
[32m[20221213 12:53:06 @agent_ppo2.py:179][0m |          -0.0218 |          31.9046 |          -9.4466 |
[32m[20221213 12:53:07 @agent_ppo2.py:179][0m |          -0.0284 |          31.5671 |         -10.2750 |
[32m[20221213 12:53:07 @agent_ppo2.py:179][0m |          -0.0365 |          31.1261 |         -10.7633 |
[32m[20221213 12:53:07 @agent_ppo2.py:179][0m |          -0.0367 |          30.8704 |         -11.4573 |
[32m[20221213 12:53:07 @agent_ppo2.py:179][0m |          -0.0439 |          30.7443 |         -11.5962 |
[32m[20221213 12:53:07 @agent_ppo2.py:179][0m |          -0.0392 |          30.5428 |         -11.7471 |
[32m[20221213 12:53:07 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:53:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.40
[32m[20221213 12:53:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.09
[32m[20221213 12:53:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.72
[32m[20221213 12:53:07 @agent_ppo2.py:137][0m Total time:      37.18 min
[32m[20221213 12:53:07 @agent_ppo2.py:139][0m 2560000 total steps have happened
[32m[20221213 12:53:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1250 --------------------------#
[32m[20221213 12:53:07 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:53:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:08 @agent_ppo2.py:179][0m |           0.0738 |          35.8791 |          -8.0786 |
[32m[20221213 12:53:08 @agent_ppo2.py:179][0m |           0.0480 |          34.9007 |          -4.8000 |
[32m[20221213 12:53:08 @agent_ppo2.py:179][0m |           0.0084 |          34.4939 |          -7.5503 |
[32m[20221213 12:53:08 @agent_ppo2.py:179][0m |           0.0025 |          34.1481 |          -7.8255 |
[32m[20221213 12:53:08 @agent_ppo2.py:179][0m |          -0.0088 |          33.9250 |          -8.3689 |
[32m[20221213 12:53:08 @agent_ppo2.py:179][0m |          -0.0189 |          33.9388 |          -8.8159 |
[32m[20221213 12:53:08 @agent_ppo2.py:179][0m |          -0.0221 |          33.9010 |          -9.4245 |
[32m[20221213 12:53:09 @agent_ppo2.py:179][0m |          -0.0292 |          33.5363 |         -10.0453 |
[32m[20221213 12:53:09 @agent_ppo2.py:179][0m |          -0.0317 |          33.4667 |         -10.3343 |
[32m[20221213 12:53:09 @agent_ppo2.py:179][0m |          -0.0381 |          33.4504 |         -10.8703 |
[32m[20221213 12:53:09 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:53:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.74
[32m[20221213 12:53:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.18
[32m[20221213 12:53:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.97
[32m[20221213 12:53:09 @agent_ppo2.py:137][0m Total time:      37.20 min
[32m[20221213 12:53:09 @agent_ppo2.py:139][0m 2562048 total steps have happened
[32m[20221213 12:53:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1251 --------------------------#
[32m[20221213 12:53:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:09 @agent_ppo2.py:179][0m |           0.0843 |          34.6804 |          -3.5024 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |           0.0555 |          33.7026 |          -3.1694 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |           0.0298 |          34.4428 |          -5.7819 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |           0.0085 |          32.7547 |          -6.5492 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |           0.0062 |          32.3997 |          -6.8104 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |          -0.0111 |          32.1346 |          -7.4334 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |          -0.0230 |          31.9129 |          -9.2728 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |          -0.0252 |          31.7494 |          -9.4827 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |          -0.0197 |          33.0917 |          -9.7867 |
[32m[20221213 12:53:10 @agent_ppo2.py:179][0m |          -0.0315 |          31.5987 |         -10.0273 |
[32m[20221213 12:53:10 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:53:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.81
[32m[20221213 12:53:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.24
[32m[20221213 12:53:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 308.58
[32m[20221213 12:53:11 @agent_ppo2.py:137][0m Total time:      37.23 min
[32m[20221213 12:53:11 @agent_ppo2.py:139][0m 2564096 total steps have happened
[32m[20221213 12:53:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1252 --------------------------#
[32m[20221213 12:53:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:11 @agent_ppo2.py:179][0m |           0.0638 |          34.5533 |          -6.2573 |
[32m[20221213 12:53:11 @agent_ppo2.py:179][0m |           0.0532 |          33.5033 |          -3.6563 |
[32m[20221213 12:53:11 @agent_ppo2.py:179][0m |           0.0155 |          33.0426 |          -4.5296 |
[32m[20221213 12:53:11 @agent_ppo2.py:179][0m |           0.0033 |          33.1017 |          -4.6754 |
[32m[20221213 12:53:12 @agent_ppo2.py:179][0m |          -0.0133 |          32.2767 |          -5.6460 |
[32m[20221213 12:53:12 @agent_ppo2.py:179][0m |          -0.0158 |          33.5650 |          -5.8342 |
[32m[20221213 12:53:12 @agent_ppo2.py:179][0m |          -0.0297 |          32.0480 |          -6.4269 |
[32m[20221213 12:53:12 @agent_ppo2.py:179][0m |          -0.0322 |          31.6164 |          -7.0541 |
[32m[20221213 12:53:12 @agent_ppo2.py:179][0m |          -0.0263 |          32.6162 |          -7.3035 |
[32m[20221213 12:53:12 @agent_ppo2.py:179][0m |          -0.0333 |          31.4842 |          -7.4891 |
[32m[20221213 12:53:12 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:53:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 345.38
[32m[20221213 12:53:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.46
[32m[20221213 12:53:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 380.93
[32m[20221213 12:53:12 @agent_ppo2.py:137][0m Total time:      37.26 min
[32m[20221213 12:53:12 @agent_ppo2.py:139][0m 2566144 total steps have happened
[32m[20221213 12:53:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1253 --------------------------#
[32m[20221213 12:53:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:13 @agent_ppo2.py:179][0m |           0.0507 |          31.1736 |          -4.5644 |
[32m[20221213 12:53:13 @agent_ppo2.py:179][0m |           0.0171 |          34.8573 |          -2.3348 |
[32m[20221213 12:53:13 @agent_ppo2.py:179][0m |          -0.0003 |          29.9439 |          -2.6602 |
[32m[20221213 12:53:13 @agent_ppo2.py:179][0m |          -0.0212 |          29.4880 |          -3.6999 |
[32m[20221213 12:53:13 @agent_ppo2.py:179][0m |          -0.0243 |          31.4358 |          -4.3969 |
[32m[20221213 12:53:13 @agent_ppo2.py:179][0m |          -0.0414 |          28.9554 |          -4.6355 |
[32m[20221213 12:53:14 @agent_ppo2.py:179][0m |          -0.0425 |          28.8154 |          -5.0238 |
[32m[20221213 12:53:14 @agent_ppo2.py:179][0m |          -0.0525 |          28.5724 |          -5.6480 |
[32m[20221213 12:53:14 @agent_ppo2.py:179][0m |          -0.0405 |          31.0756 |          -6.2575 |
[32m[20221213 12:53:14 @agent_ppo2.py:179][0m |          -0.0322 |          29.5137 |          -6.0153 |
[32m[20221213 12:53:14 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:53:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 253.82
[32m[20221213 12:53:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 349.10
[32m[20221213 12:53:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.22
[32m[20221213 12:53:14 @agent_ppo2.py:137][0m Total time:      37.29 min
[32m[20221213 12:53:14 @agent_ppo2.py:139][0m 2568192 total steps have happened
[32m[20221213 12:53:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1254 --------------------------#
[32m[20221213 12:53:14 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:15 @agent_ppo2.py:179][0m |           0.0696 |          30.1825 |          -3.4928 |
[32m[20221213 12:53:15 @agent_ppo2.py:179][0m |           0.0152 |          29.4674 |          -3.4025 |
[32m[20221213 12:53:15 @agent_ppo2.py:179][0m |          -0.0106 |          28.5087 |          -4.8853 |
[32m[20221213 12:53:15 @agent_ppo2.py:179][0m |          -0.0259 |          28.2934 |          -4.7372 |
[32m[20221213 12:53:15 @agent_ppo2.py:179][0m |          -0.0413 |          28.0265 |          -5.7709 |
[32m[20221213 12:53:15 @agent_ppo2.py:179][0m |          -0.0470 |          27.8330 |          -6.4008 |
[32m[20221213 12:53:15 @agent_ppo2.py:179][0m |          -0.0497 |          27.6220 |          -6.6553 |
[32m[20221213 12:53:16 @agent_ppo2.py:179][0m |          -0.0469 |          30.3358 |          -7.0166 |
[32m[20221213 12:53:16 @agent_ppo2.py:179][0m |          -0.0572 |          27.4542 |          -7.4993 |
[32m[20221213 12:53:16 @agent_ppo2.py:179][0m |          -0.0598 |          27.2069 |          -8.1297 |
[32m[20221213 12:53:16 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:53:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.26
[32m[20221213 12:53:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.16
[32m[20221213 12:53:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.45
[32m[20221213 12:53:16 @agent_ppo2.py:137][0m Total time:      37.32 min
[32m[20221213 12:53:16 @agent_ppo2.py:139][0m 2570240 total steps have happened
[32m[20221213 12:53:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1255 --------------------------#
[32m[20221213 12:53:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:16 @agent_ppo2.py:179][0m |           0.0927 |          34.9829 |          -6.4318 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |           0.1276 |          33.5599 |          -1.1910 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |           0.0732 |          37.4914 |          -2.6568 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |           0.0374 |          34.2086 |          -4.1461 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |           0.0185 |          32.5189 |          -4.6224 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |          -0.0022 |          32.2850 |          -6.0949 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |          -0.0103 |          32.1537 |          -7.0752 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |          -0.0035 |          32.8789 |          -6.7263 |
[32m[20221213 12:53:17 @agent_ppo2.py:179][0m |          -0.0228 |          31.7995 |          -7.7381 |
[32m[20221213 12:53:18 @agent_ppo2.py:179][0m |          -0.0238 |          31.7012 |          -8.1024 |
[32m[20221213 12:53:18 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:53:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.34
[32m[20221213 12:53:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.71
[32m[20221213 12:53:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.78
[32m[20221213 12:53:18 @agent_ppo2.py:137][0m Total time:      37.35 min
[32m[20221213 12:53:18 @agent_ppo2.py:139][0m 2572288 total steps have happened
[32m[20221213 12:53:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1256 --------------------------#
[32m[20221213 12:53:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:18 @agent_ppo2.py:179][0m |           0.1070 |          38.0060 |          -3.9108 |
[32m[20221213 12:53:18 @agent_ppo2.py:179][0m |           0.0562 |          33.9280 |          -1.9667 |
[32m[20221213 12:53:18 @agent_ppo2.py:179][0m |           0.0209 |          33.4338 |          -3.2243 |
[32m[20221213 12:53:19 @agent_ppo2.py:179][0m |          -0.0031 |          33.5978 |          -4.6754 |
[32m[20221213 12:53:19 @agent_ppo2.py:179][0m |          -0.0184 |          32.9071 |          -5.1963 |
[32m[20221213 12:53:19 @agent_ppo2.py:179][0m |          -0.0118 |          33.4670 |          -4.4486 |
[32m[20221213 12:53:19 @agent_ppo2.py:179][0m |          -0.0260 |          32.5783 |          -4.9609 |
[32m[20221213 12:53:19 @agent_ppo2.py:179][0m |          -0.0262 |          34.2362 |          -5.4451 |
[32m[20221213 12:53:19 @agent_ppo2.py:179][0m |          -0.0358 |          32.3188 |          -5.8016 |
[32m[20221213 12:53:19 @agent_ppo2.py:179][0m |          -0.0321 |          32.1290 |          -5.7523 |
[32m[20221213 12:53:19 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:53:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.50
[32m[20221213 12:53:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.37
[32m[20221213 12:53:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.37
[32m[20221213 12:53:19 @agent_ppo2.py:137][0m Total time:      37.38 min
[32m[20221213 12:53:19 @agent_ppo2.py:139][0m 2574336 total steps have happened
[32m[20221213 12:53:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1257 --------------------------#
[32m[20221213 12:53:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:20 @agent_ppo2.py:179][0m |           0.0936 |          34.7001 |          -3.3649 |
[32m[20221213 12:53:20 @agent_ppo2.py:179][0m |           0.0402 |          33.8028 |          -1.3425 |
[32m[20221213 12:53:20 @agent_ppo2.py:179][0m |           0.0018 |          33.3025 |          -3.3747 |
[32m[20221213 12:53:20 @agent_ppo2.py:179][0m |          -0.0137 |          33.0508 |          -3.9958 |
[32m[20221213 12:53:20 @agent_ppo2.py:179][0m |          -0.0129 |          35.1292 |          -4.8633 |
[32m[20221213 12:53:20 @agent_ppo2.py:179][0m |          -0.0293 |          32.7789 |          -5.1422 |
[32m[20221213 12:53:21 @agent_ppo2.py:179][0m |          -0.0358 |          32.4619 |          -5.6601 |
[32m[20221213 12:53:21 @agent_ppo2.py:179][0m |          -0.0322 |          32.5412 |          -6.5019 |
[32m[20221213 12:53:21 @agent_ppo2.py:179][0m |          -0.0362 |          32.1915 |          -6.4262 |
[32m[20221213 12:53:21 @agent_ppo2.py:179][0m |          -0.0271 |          36.3710 |          -6.3973 |
[32m[20221213 12:53:21 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:53:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.15
[32m[20221213 12:53:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.20
[32m[20221213 12:53:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.81
[32m[20221213 12:53:21 @agent_ppo2.py:137][0m Total time:      37.41 min
[32m[20221213 12:53:21 @agent_ppo2.py:139][0m 2576384 total steps have happened
[32m[20221213 12:53:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1258 --------------------------#
[32m[20221213 12:53:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |           0.0318 |          34.0989 |          -4.8330 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |           0.0293 |          33.0570 |          -2.5190 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |           0.0122 |          32.5266 |          -3.6593 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |          -0.0069 |          32.1886 |          -4.2068 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |          -0.0227 |          31.8994 |          -5.3115 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |          -0.0243 |          31.7275 |          -5.9020 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |          -0.0235 |          33.9583 |          -6.3193 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |          -0.0328 |          31.5080 |          -7.0423 |
[32m[20221213 12:53:22 @agent_ppo2.py:179][0m |          -0.0341 |          31.2098 |          -7.5180 |
[32m[20221213 12:53:23 @agent_ppo2.py:179][0m |          -0.0390 |          31.1191 |          -7.7475 |
[32m[20221213 12:53:23 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:53:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.65
[32m[20221213 12:53:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.89
[32m[20221213 12:53:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.19
[32m[20221213 12:53:23 @agent_ppo2.py:137][0m Total time:      37.43 min
[32m[20221213 12:53:23 @agent_ppo2.py:139][0m 2578432 total steps have happened
[32m[20221213 12:53:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1259 --------------------------#
[32m[20221213 12:53:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:23 @agent_ppo2.py:179][0m |           0.0599 |          34.4273 |          -4.6100 |
[32m[20221213 12:53:23 @agent_ppo2.py:179][0m |           0.0609 |          32.9151 |          -0.6104 |
[32m[20221213 12:53:23 @agent_ppo2.py:179][0m |           0.0117 |          32.4127 |          -2.1412 |
[32m[20221213 12:53:24 @agent_ppo2.py:179][0m |          -0.0121 |          31.8046 |          -3.9375 |
[32m[20221213 12:53:24 @agent_ppo2.py:179][0m |          -0.0213 |          31.3681 |          -4.5256 |
[32m[20221213 12:53:24 @agent_ppo2.py:179][0m |          -0.0222 |          30.9464 |          -5.0316 |
[32m[20221213 12:53:24 @agent_ppo2.py:179][0m |          -0.0260 |          32.7253 |          -5.6145 |
[32m[20221213 12:53:24 @agent_ppo2.py:179][0m |          -0.0298 |          30.7489 |          -5.8883 |
[32m[20221213 12:53:24 @agent_ppo2.py:179][0m |          -0.0386 |          30.1138 |          -6.4359 |
[32m[20221213 12:53:24 @agent_ppo2.py:179][0m |          -0.0422 |          29.7640 |          -7.0537 |
[32m[20221213 12:53:24 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:53:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.82
[32m[20221213 12:53:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.24
[32m[20221213 12:53:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.36
[32m[20221213 12:53:25 @agent_ppo2.py:137][0m Total time:      37.46 min
[32m[20221213 12:53:25 @agent_ppo2.py:139][0m 2580480 total steps have happened
[32m[20221213 12:53:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1260 --------------------------#
[32m[20221213 12:53:25 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:53:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:25 @agent_ppo2.py:179][0m |           0.0777 |          34.7698 |          -5.3470 |
[32m[20221213 12:53:25 @agent_ppo2.py:179][0m |           0.0532 |          32.8378 |          -2.0963 |
[32m[20221213 12:53:25 @agent_ppo2.py:179][0m |           0.0238 |          32.5083 |          -3.7787 |
[32m[20221213 12:53:25 @agent_ppo2.py:179][0m |           0.0097 |          36.3444 |          -4.7396 |
[32m[20221213 12:53:25 @agent_ppo2.py:179][0m |          -0.0133 |          31.5581 |          -5.8295 |
[32m[20221213 12:53:26 @agent_ppo2.py:179][0m |          -0.0154 |          31.0552 |          -5.8546 |
[32m[20221213 12:53:26 @agent_ppo2.py:179][0m |          -0.0173 |          31.5118 |          -6.3049 |
[32m[20221213 12:53:26 @agent_ppo2.py:179][0m |          -0.0293 |          30.4962 |          -6.8996 |
[32m[20221213 12:53:26 @agent_ppo2.py:179][0m |          -0.0215 |          31.8691 |          -7.1039 |
[32m[20221213 12:53:26 @agent_ppo2.py:179][0m |          -0.0330 |          30.1205 |          -7.6302 |
[32m[20221213 12:53:26 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:53:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.85
[32m[20221213 12:53:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 380.26
[32m[20221213 12:53:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 317.46
[32m[20221213 12:53:26 @agent_ppo2.py:137][0m Total time:      37.49 min
[32m[20221213 12:53:26 @agent_ppo2.py:139][0m 2582528 total steps have happened
[32m[20221213 12:53:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1261 --------------------------#
[32m[20221213 12:53:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |           0.0701 |          36.6779 |          -5.8952 |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |           0.0431 |          34.4487 |          -2.9299 |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |           0.0010 |          33.1152 |          -3.7656 |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |          -0.0153 |          32.5867 |          -5.3242 |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |          -0.0247 |          32.0877 |          -5.6657 |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |          -0.0314 |          31.8056 |          -6.5836 |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |          -0.0262 |          31.5916 |          -6.9211 |
[32m[20221213 12:53:27 @agent_ppo2.py:179][0m |          -0.0350 |          31.2934 |          -7.0685 |
[32m[20221213 12:53:28 @agent_ppo2.py:179][0m |          -0.0427 |          31.1500 |          -7.8560 |
[32m[20221213 12:53:28 @agent_ppo2.py:179][0m |          -0.0410 |          30.9611 |          -8.6001 |
[32m[20221213 12:53:28 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:53:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.32
[32m[20221213 12:53:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.85
[32m[20221213 12:53:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 342.60
[32m[20221213 12:53:28 @agent_ppo2.py:137][0m Total time:      37.52 min
[32m[20221213 12:53:28 @agent_ppo2.py:139][0m 2584576 total steps have happened
[32m[20221213 12:53:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1262 --------------------------#
[32m[20221213 12:53:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:28 @agent_ppo2.py:179][0m |           0.0456 |          34.2951 |          -7.6174 |
[32m[20221213 12:53:28 @agent_ppo2.py:179][0m |           0.0098 |          33.5401 |          -6.2670 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0016 |          33.1698 |          -6.8074 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0225 |          32.9521 |          -7.7171 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0205 |          32.7840 |          -8.5600 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0269 |          32.8353 |          -8.9014 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0332 |          32.4979 |          -9.3450 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0310 |          32.4395 |          -9.9374 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0395 |          32.4533 |         -10.5561 |
[32m[20221213 12:53:29 @agent_ppo2.py:179][0m |          -0.0386 |          32.2657 |         -10.5548 |
[32m[20221213 12:53:29 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:53:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.56
[32m[20221213 12:53:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.45
[32m[20221213 12:53:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.16
[32m[20221213 12:53:30 @agent_ppo2.py:137][0m Total time:      37.55 min
[32m[20221213 12:53:30 @agent_ppo2.py:139][0m 2586624 total steps have happened
[32m[20221213 12:53:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1263 --------------------------#
[32m[20221213 12:53:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:30 @agent_ppo2.py:179][0m |           0.0714 |          35.2008 |          -6.9486 |
[32m[20221213 12:53:30 @agent_ppo2.py:179][0m |           0.0625 |          33.9406 |          -4.7480 |
[32m[20221213 12:53:30 @agent_ppo2.py:179][0m |           0.0026 |          33.2000 |          -5.6029 |
[32m[20221213 12:53:30 @agent_ppo2.py:179][0m |          -0.0163 |          32.9140 |          -6.3531 |
[32m[20221213 12:53:30 @agent_ppo2.py:179][0m |          -0.0201 |          32.5466 |          -6.9830 |
[32m[20221213 12:53:31 @agent_ppo2.py:179][0m |          -0.0233 |          32.4188 |          -7.0153 |
[32m[20221213 12:53:31 @agent_ppo2.py:179][0m |          -0.0227 |          32.2648 |          -7.4856 |
[32m[20221213 12:53:31 @agent_ppo2.py:179][0m |          -0.0199 |          32.0911 |          -7.2776 |
[32m[20221213 12:53:31 @agent_ppo2.py:179][0m |          -0.0313 |          32.0375 |          -8.3820 |
[32m[20221213 12:53:31 @agent_ppo2.py:179][0m |          -0.0370 |          32.0272 |          -8.9005 |
[32m[20221213 12:53:31 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:53:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.46
[32m[20221213 12:53:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.90
[32m[20221213 12:53:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 383.35
[32m[20221213 12:53:31 @agent_ppo2.py:137][0m Total time:      37.57 min
[32m[20221213 12:53:31 @agent_ppo2.py:139][0m 2588672 total steps have happened
[32m[20221213 12:53:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1264 --------------------------#
[32m[20221213 12:53:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |           0.0651 |          33.8566 |          -5.9309 |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |           0.0681 |          32.7159 |          -4.1427 |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |           0.0263 |          31.9891 |          -4.2382 |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |          -0.0022 |          31.5471 |          -5.0560 |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |          -0.0028 |          34.3834 |          -5.7005 |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |          -0.0265 |          30.9613 |          -6.0375 |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |          -0.0257 |          30.4750 |          -6.4135 |
[32m[20221213 12:53:32 @agent_ppo2.py:179][0m |          -0.0304 |          30.1521 |          -6.6165 |
[32m[20221213 12:53:33 @agent_ppo2.py:179][0m |          -0.0350 |          29.9175 |          -6.7937 |
[32m[20221213 12:53:33 @agent_ppo2.py:179][0m |          -0.0400 |          29.6284 |          -7.5552 |
[32m[20221213 12:53:33 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:53:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.75
[32m[20221213 12:53:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 317.62
[32m[20221213 12:53:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 378.59
[32m[20221213 12:53:33 @agent_ppo2.py:137][0m Total time:      37.60 min
[32m[20221213 12:53:33 @agent_ppo2.py:139][0m 2590720 total steps have happened
[32m[20221213 12:53:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1265 --------------------------#
[32m[20221213 12:53:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:33 @agent_ppo2.py:179][0m |           0.1369 |          39.8032 |          -4.0858 |
[32m[20221213 12:53:33 @agent_ppo2.py:179][0m |           0.0444 |          35.1854 |          -2.9468 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |           0.0179 |          39.4694 |          -3.7827 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |           0.0029 |          34.2005 |          -3.5637 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |          -0.0152 |          33.8223 |          -3.5192 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |          -0.0057 |          41.0871 |          -4.2348 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |          -0.0328 |          33.8150 |          -4.4335 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |          -0.0334 |          33.3339 |          -4.9695 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |          -0.0424 |          33.2134 |          -5.2834 |
[32m[20221213 12:53:34 @agent_ppo2.py:179][0m |          -0.0408 |          33.2317 |          -5.1330 |
[32m[20221213 12:53:34 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:53:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.05
[32m[20221213 12:53:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.21
[32m[20221213 12:53:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.21
[32m[20221213 12:53:35 @agent_ppo2.py:137][0m Total time:      37.63 min
[32m[20221213 12:53:35 @agent_ppo2.py:139][0m 2592768 total steps have happened
[32m[20221213 12:53:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1266 --------------------------#
[32m[20221213 12:53:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:35 @agent_ppo2.py:179][0m |           0.0632 |          34.4210 |          -3.2309 |
[32m[20221213 12:53:35 @agent_ppo2.py:179][0m |           0.0300 |          33.5939 |          -3.9954 |
[32m[20221213 12:53:35 @agent_ppo2.py:179][0m |           0.0235 |          33.2386 |          -3.5580 |
[32m[20221213 12:53:35 @agent_ppo2.py:179][0m |          -0.0005 |          33.8755 |          -4.8338 |
[32m[20221213 12:53:35 @agent_ppo2.py:179][0m |          -0.0204 |          32.7232 |          -5.6634 |
[32m[20221213 12:53:35 @agent_ppo2.py:179][0m |          -0.0201 |          34.6577 |          -6.0452 |
[32m[20221213 12:53:36 @agent_ppo2.py:179][0m |          -0.0323 |          32.4086 |          -6.6504 |
[32m[20221213 12:53:36 @agent_ppo2.py:179][0m |          -0.0306 |          32.2019 |          -7.2302 |
[32m[20221213 12:53:36 @agent_ppo2.py:179][0m |          -0.0235 |          35.2236 |          -7.2998 |
[32m[20221213 12:53:36 @agent_ppo2.py:179][0m |          -0.0386 |          32.1153 |          -7.8686 |
[32m[20221213 12:53:36 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:53:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 324.70
[32m[20221213 12:53:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.76
[32m[20221213 12:53:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.94
[32m[20221213 12:53:36 @agent_ppo2.py:137][0m Total time:      37.66 min
[32m[20221213 12:53:36 @agent_ppo2.py:139][0m 2594816 total steps have happened
[32m[20221213 12:53:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1267 --------------------------#
[32m[20221213 12:53:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |           0.0446 |          34.6302 |          -1.4057 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |           0.0171 |          33.0952 |          -1.0149 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |           0.0252 |          35.7976 |          -1.3416 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |           0.0038 |          32.2787 |          -0.8344 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |          -0.0184 |          31.7842 |          -2.2741 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |          -0.0264 |          31.5343 |          -3.0892 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |          -0.0305 |          31.3127 |          -3.3256 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |          -0.0237 |          31.0494 |          -3.5721 |
[32m[20221213 12:53:37 @agent_ppo2.py:179][0m |          -0.0328 |          30.9542 |          -4.3909 |
[32m[20221213 12:53:38 @agent_ppo2.py:179][0m |          -0.0323 |          30.8695 |          -4.3372 |
[32m[20221213 12:53:38 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:53:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.75
[32m[20221213 12:53:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.98
[32m[20221213 12:53:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 300.50
[32m[20221213 12:53:38 @agent_ppo2.py:137][0m Total time:      37.68 min
[32m[20221213 12:53:38 @agent_ppo2.py:139][0m 2596864 total steps have happened
[32m[20221213 12:53:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1268 --------------------------#
[32m[20221213 12:53:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:38 @agent_ppo2.py:179][0m |           0.0749 |          34.7263 |          -1.9121 |
[32m[20221213 12:53:38 @agent_ppo2.py:179][0m |           0.0646 |          33.4047 |           0.4704 |
[32m[20221213 12:53:38 @agent_ppo2.py:179][0m |           0.0093 |          32.9979 |          -2.1220 |
[32m[20221213 12:53:39 @agent_ppo2.py:179][0m |          -0.0089 |          32.7406 |          -2.7479 |
[32m[20221213 12:53:39 @agent_ppo2.py:179][0m |          -0.0152 |          32.6683 |          -2.6483 |
[32m[20221213 12:53:39 @agent_ppo2.py:179][0m |          -0.0222 |          32.4882 |          -3.1860 |
[32m[20221213 12:53:39 @agent_ppo2.py:179][0m |          -0.0256 |          32.3081 |          -3.2167 |
[32m[20221213 12:53:39 @agent_ppo2.py:179][0m |          -0.0314 |          32.2165 |          -3.8700 |
[32m[20221213 12:53:39 @agent_ppo2.py:179][0m |          -0.0316 |          32.1149 |          -4.0740 |
[32m[20221213 12:53:39 @agent_ppo2.py:179][0m |          -0.0383 |          32.0627 |          -4.4816 |
[32m[20221213 12:53:39 @agent_ppo2.py:124][0m Policy update time: 1.18 s
[32m[20221213 12:53:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 347.98
[32m[20221213 12:53:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.23
[32m[20221213 12:53:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.72
[32m[20221213 12:53:39 @agent_ppo2.py:137][0m Total time:      37.71 min
[32m[20221213 12:53:39 @agent_ppo2.py:139][0m 2598912 total steps have happened
[32m[20221213 12:53:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1269 --------------------------#
[32m[20221213 12:53:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:40 @agent_ppo2.py:179][0m |           0.0976 |          34.7584 |          -0.6117 |
[32m[20221213 12:53:40 @agent_ppo2.py:179][0m |           0.0813 |          33.8575 |           0.8380 |
[32m[20221213 12:53:40 @agent_ppo2.py:179][0m |           0.0262 |          33.4842 |          -0.2194 |
[32m[20221213 12:53:40 @agent_ppo2.py:179][0m |           0.0015 |          33.1924 |          -0.7754 |
[32m[20221213 12:53:40 @agent_ppo2.py:179][0m |          -0.0170 |          32.9653 |          -1.1540 |
[32m[20221213 12:53:40 @agent_ppo2.py:179][0m |          -0.0249 |          32.9273 |          -1.4915 |
[32m[20221213 12:53:40 @agent_ppo2.py:179][0m |          -0.0235 |          33.4578 |          -1.7496 |
[32m[20221213 12:53:41 @agent_ppo2.py:179][0m |          -0.0243 |          33.1717 |          -2.1459 |
[32m[20221213 12:53:41 @agent_ppo2.py:179][0m |          -0.0346 |          32.2650 |          -2.1437 |
[32m[20221213 12:53:41 @agent_ppo2.py:179][0m |          -0.0381 |          32.1561 |          -2.7632 |
[32m[20221213 12:53:41 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:53:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.34
[32m[20221213 12:53:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.74
[32m[20221213 12:53:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.63
[32m[20221213 12:53:41 @agent_ppo2.py:137][0m Total time:      37.74 min
[32m[20221213 12:53:41 @agent_ppo2.py:139][0m 2600960 total steps have happened
[32m[20221213 12:53:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1270 --------------------------#
[32m[20221213 12:53:41 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:53:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:41 @agent_ppo2.py:179][0m |           0.0629 |          33.9785 |          -0.5748 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |           0.0401 |          33.1497 |           0.4665 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |           0.0034 |          32.7203 |          -1.8184 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |          -0.0113 |          32.3210 |          -1.9430 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |          -0.0254 |          32.0310 |          -2.6500 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |          -0.0196 |          33.7137 |          -3.0717 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |          -0.0144 |          36.7605 |          -3.5442 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |          -0.0294 |          31.9868 |          -3.6710 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |          -0.0318 |          31.4809 |          -3.9020 |
[32m[20221213 12:53:42 @agent_ppo2.py:179][0m |          -0.0390 |          31.2457 |          -4.5219 |
[32m[20221213 12:53:42 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:53:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.80
[32m[20221213 12:53:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.85
[32m[20221213 12:53:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 321.29
[32m[20221213 12:53:43 @agent_ppo2.py:137][0m Total time:      37.77 min
[32m[20221213 12:53:43 @agent_ppo2.py:139][0m 2603008 total steps have happened
[32m[20221213 12:53:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1271 --------------------------#
[32m[20221213 12:53:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:43 @agent_ppo2.py:179][0m |           0.0412 |          33.8387 |          -3.1125 |
[32m[20221213 12:53:43 @agent_ppo2.py:179][0m |           0.0353 |          32.7221 |          -1.7551 |
[32m[20221213 12:53:43 @agent_ppo2.py:179][0m |           0.0007 |          32.0072 |          -1.8471 |
[32m[20221213 12:53:43 @agent_ppo2.py:179][0m |          -0.0137 |          32.8111 |          -2.5812 |
[32m[20221213 12:53:44 @agent_ppo2.py:179][0m |          -0.0308 |          31.2593 |          -2.6876 |
[32m[20221213 12:53:44 @agent_ppo2.py:179][0m |          -0.0348 |          30.9684 |          -2.9967 |
[32m[20221213 12:53:44 @agent_ppo2.py:179][0m |          -0.0426 |          30.6604 |          -3.4768 |
[32m[20221213 12:53:44 @agent_ppo2.py:179][0m |          -0.0426 |          30.4600 |          -4.1865 |
[32m[20221213 12:53:44 @agent_ppo2.py:179][0m |          -0.0458 |          30.3051 |          -4.3155 |
[32m[20221213 12:53:44 @agent_ppo2.py:179][0m |          -0.0429 |          30.1269 |          -4.5959 |
[32m[20221213 12:53:44 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:53:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.13
[32m[20221213 12:53:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.21
[32m[20221213 12:53:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.50
[32m[20221213 12:53:44 @agent_ppo2.py:137][0m Total time:      37.79 min
[32m[20221213 12:53:44 @agent_ppo2.py:139][0m 2605056 total steps have happened
[32m[20221213 12:53:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1272 --------------------------#
[32m[20221213 12:53:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:45 @agent_ppo2.py:179][0m |           0.0972 |          33.5441 |          -1.9280 |
[32m[20221213 12:53:45 @agent_ppo2.py:179][0m |           0.0603 |          32.1620 |          -0.6970 |
[32m[20221213 12:53:45 @agent_ppo2.py:179][0m |           0.0164 |          31.4945 |          -1.4015 |
[32m[20221213 12:53:45 @agent_ppo2.py:179][0m |          -0.0053 |          31.0867 |          -1.8828 |
[32m[20221213 12:53:45 @agent_ppo2.py:179][0m |          -0.0222 |          30.8026 |          -3.0781 |
[32m[20221213 12:53:45 @agent_ppo2.py:179][0m |          -0.0230 |          30.4236 |          -3.4518 |
[32m[20221213 12:53:45 @agent_ppo2.py:179][0m |          -0.0346 |          30.1586 |          -3.9337 |
[32m[20221213 12:53:46 @agent_ppo2.py:179][0m |          -0.0376 |          29.9203 |          -3.9582 |
[32m[20221213 12:53:46 @agent_ppo2.py:179][0m |          -0.0398 |          29.8055 |          -4.4940 |
[32m[20221213 12:53:46 @agent_ppo2.py:179][0m |          -0.0444 |          29.6259 |          -4.7872 |
[32m[20221213 12:53:46 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:53:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.97
[32m[20221213 12:53:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.91
[32m[20221213 12:53:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.16
[32m[20221213 12:53:46 @agent_ppo2.py:137][0m Total time:      37.82 min
[32m[20221213 12:53:46 @agent_ppo2.py:139][0m 2607104 total steps have happened
[32m[20221213 12:53:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1273 --------------------------#
[32m[20221213 12:53:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:46 @agent_ppo2.py:179][0m |           0.0560 |          33.9626 |          -2.2112 |
[32m[20221213 12:53:46 @agent_ppo2.py:179][0m |           0.0678 |          32.9544 |           0.1246 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |           0.0086 |          32.4882 |          -1.5314 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |          -0.0123 |          32.1819 |          -1.9862 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |          -0.0262 |          31.9961 |          -2.6479 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |          -0.0212 |          32.7690 |          -3.1415 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |          -0.0251 |          31.4816 |          -3.3557 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |          -0.0382 |          31.1252 |          -3.6978 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |          -0.0431 |          30.8974 |          -4.1014 |
[32m[20221213 12:53:47 @agent_ppo2.py:179][0m |          -0.0391 |          30.9628 |          -4.4706 |
[32m[20221213 12:53:47 @agent_ppo2.py:124][0m Policy update time: 1.19 s
[32m[20221213 12:53:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.83
[32m[20221213 12:53:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.54
[32m[20221213 12:53:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 342.87
[32m[20221213 12:53:48 @agent_ppo2.py:137][0m Total time:      37.85 min
[32m[20221213 12:53:48 @agent_ppo2.py:139][0m 2609152 total steps have happened
[32m[20221213 12:53:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1274 --------------------------#
[32m[20221213 12:53:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:48 @agent_ppo2.py:179][0m |           0.0525 |          34.0685 |          -3.3504 |
[32m[20221213 12:53:48 @agent_ppo2.py:179][0m |           0.0416 |          32.7571 |          -1.4290 |
[32m[20221213 12:53:48 @agent_ppo2.py:179][0m |           0.0100 |          32.5730 |          -2.1634 |
[32m[20221213 12:53:48 @agent_ppo2.py:179][0m |          -0.0179 |          31.6611 |          -2.8714 |
[32m[20221213 12:53:48 @agent_ppo2.py:179][0m |          -0.0193 |          31.3671 |          -3.3038 |
[32m[20221213 12:53:49 @agent_ppo2.py:179][0m |          -0.0285 |          30.9912 |          -3.7343 |
[32m[20221213 12:53:49 @agent_ppo2.py:179][0m |          -0.0371 |          30.7281 |          -4.3450 |
[32m[20221213 12:53:49 @agent_ppo2.py:179][0m |          -0.0404 |          30.5039 |          -5.1371 |
[32m[20221213 12:53:49 @agent_ppo2.py:179][0m |          -0.0404 |          30.3244 |          -5.0661 |
[32m[20221213 12:53:49 @agent_ppo2.py:179][0m |          -0.0359 |          30.2414 |          -5.3249 |
[32m[20221213 12:53:49 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:53:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.94
[32m[20221213 12:53:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.75
[32m[20221213 12:53:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.38
[32m[20221213 12:53:49 @agent_ppo2.py:137][0m Total time:      37.88 min
[32m[20221213 12:53:49 @agent_ppo2.py:139][0m 2611200 total steps have happened
[32m[20221213 12:53:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1275 --------------------------#
[32m[20221213 12:53:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:50 @agent_ppo2.py:179][0m |           0.0535 |          33.9267 |          -3.4305 |
[32m[20221213 12:53:50 @agent_ppo2.py:179][0m |           0.0195 |          32.8140 |          -2.3907 |
[32m[20221213 12:53:50 @agent_ppo2.py:179][0m |          -0.0057 |          32.3350 |          -2.2847 |
[32m[20221213 12:53:50 @agent_ppo2.py:179][0m |          -0.0208 |          32.0862 |          -2.9896 |
[32m[20221213 12:53:50 @agent_ppo2.py:179][0m |          -0.0291 |          31.8407 |          -3.5501 |
[32m[20221213 12:53:50 @agent_ppo2.py:179][0m |          -0.0363 |          31.6290 |          -4.0670 |
[32m[20221213 12:53:50 @agent_ppo2.py:179][0m |          -0.0352 |          31.5510 |          -4.3254 |
[32m[20221213 12:53:51 @agent_ppo2.py:179][0m |          -0.0429 |          31.4830 |          -4.8002 |
[32m[20221213 12:53:51 @agent_ppo2.py:179][0m |          -0.0403 |          31.3026 |          -5.0995 |
[32m[20221213 12:53:51 @agent_ppo2.py:179][0m |          -0.0406 |          31.1810 |          -5.5451 |
[32m[20221213 12:53:51 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:53:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.70
[32m[20221213 12:53:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.89
[32m[20221213 12:53:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.62
[32m[20221213 12:53:51 @agent_ppo2.py:137][0m Total time:      37.90 min
[32m[20221213 12:53:51 @agent_ppo2.py:139][0m 2613248 total steps have happened
[32m[20221213 12:53:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1276 --------------------------#
[32m[20221213 12:53:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:51 @agent_ppo2.py:179][0m |           0.0845 |          34.0120 |          -3.0609 |
[32m[20221213 12:53:51 @agent_ppo2.py:179][0m |           0.0584 |          33.0131 |          -2.1214 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |           0.0214 |          32.5232 |          -2.5704 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |          -0.0068 |          32.1432 |          -4.2505 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |          -0.0119 |          31.9617 |          -4.9073 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |          -0.0264 |          31.5831 |          -5.5579 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |          -0.0308 |          31.7232 |          -5.9308 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |          -0.0356 |          31.2039 |          -6.4794 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |          -0.0288 |          33.9845 |          -6.9225 |
[32m[20221213 12:53:52 @agent_ppo2.py:179][0m |          -0.0415 |          30.9973 |          -6.9765 |
[32m[20221213 12:53:52 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:53:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.04
[32m[20221213 12:53:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 301.06
[32m[20221213 12:53:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.74
[32m[20221213 12:53:53 @agent_ppo2.py:137][0m Total time:      37.93 min
[32m[20221213 12:53:53 @agent_ppo2.py:139][0m 2615296 total steps have happened
[32m[20221213 12:53:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1277 --------------------------#
[32m[20221213 12:53:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:53 @agent_ppo2.py:179][0m |           0.0866 |          35.5313 |          -3.6806 |
[32m[20221213 12:53:53 @agent_ppo2.py:179][0m |           0.0403 |          32.0893 |          -2.4643 |
[32m[20221213 12:53:53 @agent_ppo2.py:179][0m |           0.0075 |          31.5929 |          -4.8680 |
[32m[20221213 12:53:53 @agent_ppo2.py:179][0m |           0.0097 |          37.7783 |          -5.2011 |
[32m[20221213 12:53:53 @agent_ppo2.py:179][0m |          -0.0134 |          31.3468 |          -5.3060 |
[32m[20221213 12:53:54 @agent_ppo2.py:179][0m |          -0.0272 |          30.8473 |          -6.0803 |
[32m[20221213 12:53:54 @agent_ppo2.py:179][0m |          -0.0317 |          30.7449 |          -6.4229 |
[32m[20221213 12:53:54 @agent_ppo2.py:179][0m |          -0.0346 |          30.6036 |          -6.6220 |
[32m[20221213 12:53:54 @agent_ppo2.py:179][0m |          -0.0427 |          30.3972 |          -7.2277 |
[32m[20221213 12:53:54 @agent_ppo2.py:179][0m |          -0.0407 |          30.2249 |          -7.5577 |
[32m[20221213 12:53:54 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:53:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.82
[32m[20221213 12:53:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.33
[32m[20221213 12:53:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.88
[32m[20221213 12:53:54 @agent_ppo2.py:137][0m Total time:      37.96 min
[32m[20221213 12:53:54 @agent_ppo2.py:139][0m 2617344 total steps have happened
[32m[20221213 12:53:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1278 --------------------------#
[32m[20221213 12:53:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |           0.0461 |          33.9832 |          -3.8617 |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |           0.0660 |          33.5687 |          -1.6349 |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |           0.0153 |          36.1436 |          -1.8396 |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |          -0.0203 |          31.5439 |          -3.0362 |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |          -0.0283 |          31.0462 |          -3.8063 |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |          -0.0374 |          30.8239 |          -4.2902 |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |          -0.0243 |          33.5303 |          -4.9752 |
[32m[20221213 12:53:55 @agent_ppo2.py:179][0m |          -0.0395 |          30.3504 |          -5.0811 |
[32m[20221213 12:53:56 @agent_ppo2.py:179][0m |          -0.0406 |          30.0271 |          -5.5028 |
[32m[20221213 12:53:56 @agent_ppo2.py:179][0m |          -0.0258 |          29.8778 |          -5.5721 |
[32m[20221213 12:53:56 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:53:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.39
[32m[20221213 12:53:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.65
[32m[20221213 12:53:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.49
[32m[20221213 12:53:56 @agent_ppo2.py:137][0m Total time:      37.99 min
[32m[20221213 12:53:56 @agent_ppo2.py:139][0m 2619392 total steps have happened
[32m[20221213 12:53:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1279 --------------------------#
[32m[20221213 12:53:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:53:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:56 @agent_ppo2.py:179][0m |           0.0648 |          33.9187 |          -4.5959 |
[32m[20221213 12:53:56 @agent_ppo2.py:179][0m |           0.0724 |          34.3338 |          -1.3045 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |           0.0212 |          32.6145 |          -3.2404 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |          -0.0002 |          32.8636 |          -4.2406 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |          -0.0055 |          35.3073 |          -4.4122 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |          -0.0145 |          32.0235 |          -4.3181 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |          -0.0256 |          31.7117 |          -5.5588 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |          -0.0169 |          35.9817 |          -5.4164 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |          -0.0139 |          31.5992 |          -4.1986 |
[32m[20221213 12:53:57 @agent_ppo2.py:179][0m |          -0.0189 |          31.2952 |          -4.8889 |
[32m[20221213 12:53:57 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:53:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 357.36
[32m[20221213 12:53:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.17
[32m[20221213 12:53:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 371.99
[32m[20221213 12:53:58 @agent_ppo2.py:137][0m Total time:      38.01 min
[32m[20221213 12:53:58 @agent_ppo2.py:139][0m 2621440 total steps have happened
[32m[20221213 12:53:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1280 --------------------------#
[32m[20221213 12:53:58 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:53:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:53:58 @agent_ppo2.py:179][0m |           0.0475 |          33.7410 |          -2.8987 |
[32m[20221213 12:53:58 @agent_ppo2.py:179][0m |           0.0099 |          33.1448 |          -3.4759 |
[32m[20221213 12:53:58 @agent_ppo2.py:179][0m |          -0.0112 |          32.8849 |          -5.0992 |
[32m[20221213 12:53:58 @agent_ppo2.py:179][0m |          -0.0131 |          35.7285 |          -4.8001 |
[32m[20221213 12:53:58 @agent_ppo2.py:179][0m |          -0.0277 |          32.7004 |          -5.3585 |
[32m[20221213 12:53:59 @agent_ppo2.py:179][0m |          -0.0329 |          32.3361 |          -5.7538 |
[32m[20221213 12:53:59 @agent_ppo2.py:179][0m |          -0.0378 |          32.1881 |          -5.9879 |
[32m[20221213 12:53:59 @agent_ppo2.py:179][0m |          -0.0290 |          33.4835 |          -6.5640 |
[32m[20221213 12:53:59 @agent_ppo2.py:179][0m |          -0.0260 |          32.0643 |          -6.2732 |
[32m[20221213 12:53:59 @agent_ppo2.py:179][0m |          -0.0360 |          31.8782 |          -7.1399 |
[32m[20221213 12:53:59 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:53:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.30
[32m[20221213 12:53:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.37
[32m[20221213 12:53:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.00
[32m[20221213 12:53:59 @agent_ppo2.py:137][0m Total time:      38.04 min
[32m[20221213 12:53:59 @agent_ppo2.py:139][0m 2623488 total steps have happened
[32m[20221213 12:53:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1281 --------------------------#
[32m[20221213 12:53:59 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:53:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |           0.0629 |          32.8999 |          -2.7280 |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |           0.0227 |          31.4229 |          -1.1358 |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |          -0.0023 |          30.7124 |          -3.3621 |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |          -0.0211 |          30.0757 |          -3.6960 |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |          -0.0224 |          29.6826 |          -4.0689 |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |          -0.0264 |          29.9805 |          -4.7562 |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |          -0.0328 |          29.1418 |          -4.9314 |
[32m[20221213 12:54:00 @agent_ppo2.py:179][0m |          -0.0371 |          28.8111 |          -5.3092 |
[32m[20221213 12:54:01 @agent_ppo2.py:179][0m |          -0.0381 |          28.5545 |          -5.2290 |
[32m[20221213 12:54:01 @agent_ppo2.py:179][0m |          -0.0381 |          28.4609 |          -5.3338 |
[32m[20221213 12:54:01 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.92
[32m[20221213 12:54:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.50
[32m[20221213 12:54:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 327.16
[32m[20221213 12:54:01 @agent_ppo2.py:137][0m Total time:      38.07 min
[32m[20221213 12:54:01 @agent_ppo2.py:139][0m 2625536 total steps have happened
[32m[20221213 12:54:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1282 --------------------------#
[32m[20221213 12:54:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:01 @agent_ppo2.py:179][0m |           0.0733 |          34.7670 |          -3.3020 |
[32m[20221213 12:54:01 @agent_ppo2.py:179][0m |           0.0256 |          33.1392 |          -1.5619 |
[32m[20221213 12:54:01 @agent_ppo2.py:179][0m |          -0.0123 |          32.4636 |          -4.2626 |
[32m[20221213 12:54:02 @agent_ppo2.py:179][0m |          -0.0255 |          32.2095 |          -4.7919 |
[32m[20221213 12:54:02 @agent_ppo2.py:179][0m |          -0.0344 |          31.8605 |          -5.4217 |
[32m[20221213 12:54:02 @agent_ppo2.py:179][0m |          -0.0415 |          31.5999 |          -6.2167 |
[32m[20221213 12:54:02 @agent_ppo2.py:179][0m |          -0.0482 |          31.3641 |          -6.4914 |
[32m[20221213 12:54:02 @agent_ppo2.py:179][0m |          -0.0454 |          31.3234 |          -7.0102 |
[32m[20221213 12:54:02 @agent_ppo2.py:179][0m |          -0.0454 |          30.9931 |          -7.3090 |
[32m[20221213 12:54:02 @agent_ppo2.py:179][0m |          -0.0504 |          30.7968 |          -7.9191 |
[32m[20221213 12:54:02 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:54:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 282.57
[32m[20221213 12:54:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 294.93
[32m[20221213 12:54:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.32
[32m[20221213 12:54:02 @agent_ppo2.py:137][0m Total time:      38.10 min
[32m[20221213 12:54:02 @agent_ppo2.py:139][0m 2627584 total steps have happened
[32m[20221213 12:54:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1283 --------------------------#
[32m[20221213 12:54:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:03 @agent_ppo2.py:179][0m |           0.0712 |          29.7472 |          -4.1387 |
[32m[20221213 12:54:03 @agent_ppo2.py:179][0m |           0.0536 |          28.4518 |          -2.8626 |
[32m[20221213 12:54:03 @agent_ppo2.py:179][0m |          -0.0021 |          27.8978 |          -3.8141 |
[32m[20221213 12:54:03 @agent_ppo2.py:179][0m |          -0.0132 |          29.5998 |          -3.9462 |
[32m[20221213 12:54:03 @agent_ppo2.py:179][0m |          -0.0379 |          27.4203 |          -4.8463 |
[32m[20221213 12:54:03 @agent_ppo2.py:179][0m |          -0.0389 |          26.9326 |          -4.9989 |
[32m[20221213 12:54:04 @agent_ppo2.py:179][0m |          -0.0512 |          26.8501 |          -5.5230 |
[32m[20221213 12:54:04 @agent_ppo2.py:179][0m |          -0.0540 |          26.4987 |          -6.0224 |
[32m[20221213 12:54:04 @agent_ppo2.py:179][0m |          -0.0553 |          26.3341 |          -6.5842 |
[32m[20221213 12:54:04 @agent_ppo2.py:179][0m |          -0.0573 |          26.2797 |          -7.2387 |
[32m[20221213 12:54:04 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:54:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 239.45
[32m[20221213 12:54:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.50
[32m[20221213 12:54:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.44
[32m[20221213 12:54:04 @agent_ppo2.py:137][0m Total time:      38.12 min
[32m[20221213 12:54:04 @agent_ppo2.py:139][0m 2629632 total steps have happened
[32m[20221213 12:54:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1284 --------------------------#
[32m[20221213 12:54:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |           0.0885 |          34.1542 |          -6.7859 |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |           0.0599 |          32.8501 |          -4.0074 |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |           0.0106 |          32.1613 |          -5.1269 |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |          -0.0107 |          32.2065 |          -5.6854 |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |          -0.0284 |          31.5849 |          -5.9039 |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |          -0.0366 |          31.3582 |          -6.5380 |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |          -0.0395 |          31.1057 |          -6.5292 |
[32m[20221213 12:54:05 @agent_ppo2.py:179][0m |          -0.0403 |          31.0593 |          -6.9725 |
[32m[20221213 12:54:06 @agent_ppo2.py:179][0m |          -0.0336 |          30.8483 |          -6.6682 |
[32m[20221213 12:54:06 @agent_ppo2.py:179][0m |          -0.0449 |          30.6995 |          -7.4641 |
[32m[20221213 12:54:06 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:54:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.92
[32m[20221213 12:54:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.35
[32m[20221213 12:54:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.53
[32m[20221213 12:54:06 @agent_ppo2.py:137][0m Total time:      38.15 min
[32m[20221213 12:54:06 @agent_ppo2.py:139][0m 2631680 total steps have happened
[32m[20221213 12:54:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1285 --------------------------#
[32m[20221213 12:54:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:06 @agent_ppo2.py:179][0m |           0.0830 |          33.9613 |          -4.3617 |
[32m[20221213 12:54:06 @agent_ppo2.py:179][0m |           0.0412 |          31.7905 |          -3.0529 |
[32m[20221213 12:54:06 @agent_ppo2.py:179][0m |           0.0012 |          31.4055 |          -5.0982 |
[32m[20221213 12:54:07 @agent_ppo2.py:179][0m |          -0.0049 |          35.3022 |          -5.2203 |
[32m[20221213 12:54:07 @agent_ppo2.py:179][0m |          -0.0155 |          33.6724 |          -6.0476 |
[32m[20221213 12:54:07 @agent_ppo2.py:179][0m |          -0.0279 |          30.9535 |          -6.5153 |
[32m[20221213 12:54:07 @agent_ppo2.py:179][0m |          -0.0330 |          30.4023 |          -6.9336 |
[32m[20221213 12:54:07 @agent_ppo2.py:179][0m |          -0.0394 |          30.3122 |          -7.3922 |
[32m[20221213 12:54:07 @agent_ppo2.py:179][0m |          -0.0422 |          30.1779 |          -8.0976 |
[32m[20221213 12:54:07 @agent_ppo2.py:179][0m |          -0.0365 |          30.1569 |          -8.0865 |
[32m[20221213 12:54:07 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:54:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.74
[32m[20221213 12:54:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.03
[32m[20221213 12:54:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 384.27
[32m[20221213 12:54:07 @agent_ppo2.py:137][0m Total time:      38.18 min
[32m[20221213 12:54:07 @agent_ppo2.py:139][0m 2633728 total steps have happened
[32m[20221213 12:54:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1286 --------------------------#
[32m[20221213 12:54:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:08 @agent_ppo2.py:179][0m |           0.1066 |          31.5652 |          -4.9403 |
[32m[20221213 12:54:08 @agent_ppo2.py:179][0m |           0.0819 |          30.9939 |           0.3506 |
[32m[20221213 12:54:08 @agent_ppo2.py:179][0m |           0.0316 |          30.7983 |          -1.9531 |
[32m[20221213 12:54:08 @agent_ppo2.py:179][0m |           0.0072 |          30.8076 |          -3.2158 |
[32m[20221213 12:54:08 @agent_ppo2.py:179][0m |          -0.0087 |          30.4760 |          -4.7873 |
[32m[20221213 12:54:08 @agent_ppo2.py:179][0m |          -0.0153 |          30.3713 |          -5.7811 |
[32m[20221213 12:54:09 @agent_ppo2.py:179][0m |          -0.0179 |          30.2820 |          -6.7380 |
[32m[20221213 12:54:09 @agent_ppo2.py:179][0m |          -0.0193 |          30.2433 |          -7.0609 |
[32m[20221213 12:54:09 @agent_ppo2.py:179][0m |          -0.0267 |          30.1453 |          -7.9559 |
[32m[20221213 12:54:09 @agent_ppo2.py:179][0m |          -0.0239 |          30.0884 |          -8.4401 |
[32m[20221213 12:54:09 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.85
[32m[20221213 12:54:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.82
[32m[20221213 12:54:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.20
[32m[20221213 12:54:09 @agent_ppo2.py:137][0m Total time:      38.21 min
[32m[20221213 12:54:09 @agent_ppo2.py:139][0m 2635776 total steps have happened
[32m[20221213 12:54:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1287 --------------------------#
[32m[20221213 12:54:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |           0.0582 |          32.8751 |          -5.0432 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |           0.0335 |          31.6021 |          -1.9024 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |          -0.0028 |          30.8840 |          -4.5233 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |          -0.0116 |          30.4593 |          -4.3440 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |          -0.0201 |          30.0784 |          -4.4290 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |          -0.0146 |          34.4403 |          -4.8153 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |          -0.0405 |          29.7777 |          -5.2495 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |          -0.0405 |          29.5832 |          -5.5557 |
[32m[20221213 12:54:10 @agent_ppo2.py:179][0m |          -0.0322 |          33.2937 |          -5.9090 |
[32m[20221213 12:54:11 @agent_ppo2.py:179][0m |          -0.0468 |          29.0989 |          -6.4375 |
[32m[20221213 12:54:11 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:54:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 300.19
[32m[20221213 12:54:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 327.12
[32m[20221213 12:54:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.12
[32m[20221213 12:54:11 @agent_ppo2.py:137][0m Total time:      38.23 min
[32m[20221213 12:54:11 @agent_ppo2.py:139][0m 2637824 total steps have happened
[32m[20221213 12:54:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1288 --------------------------#
[32m[20221213 12:54:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:11 @agent_ppo2.py:179][0m |           0.0983 |          32.1206 |          -3.9243 |
[32m[20221213 12:54:11 @agent_ppo2.py:179][0m |           0.0482 |          31.0082 |          -1.1897 |
[32m[20221213 12:54:11 @agent_ppo2.py:179][0m |           0.0116 |          30.9609 |          -2.6803 |
[32m[20221213 12:54:12 @agent_ppo2.py:179][0m |          -0.0128 |          30.2604 |          -3.4217 |
[32m[20221213 12:54:12 @agent_ppo2.py:179][0m |          -0.0187 |          31.0912 |          -4.4430 |
[32m[20221213 12:54:12 @agent_ppo2.py:179][0m |          -0.0083 |          31.2190 |          -3.6538 |
[32m[20221213 12:54:12 @agent_ppo2.py:179][0m |          -0.0255 |          29.6641 |          -4.3268 |
[32m[20221213 12:54:12 @agent_ppo2.py:179][0m |          -0.0236 |          30.4556 |          -5.0313 |
[32m[20221213 12:54:12 @agent_ppo2.py:179][0m |          -0.0356 |          29.3550 |          -5.2562 |
[32m[20221213 12:54:12 @agent_ppo2.py:179][0m |          -0.0329 |          29.1620 |          -5.6956 |
[32m[20221213 12:54:12 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:54:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.95
[32m[20221213 12:54:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.03
[32m[20221213 12:54:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.26
[32m[20221213 12:54:12 @agent_ppo2.py:137][0m Total time:      38.26 min
[32m[20221213 12:54:12 @agent_ppo2.py:139][0m 2639872 total steps have happened
[32m[20221213 12:54:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1289 --------------------------#
[32m[20221213 12:54:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:13 @agent_ppo2.py:179][0m |           0.0747 |          33.2281 |          -3.0810 |
[32m[20221213 12:54:13 @agent_ppo2.py:179][0m |           0.0597 |          31.9021 |          -1.4771 |
[32m[20221213 12:54:13 @agent_ppo2.py:179][0m |           0.0221 |          35.6085 |          -2.5189 |
[32m[20221213 12:54:13 @agent_ppo2.py:179][0m |          -0.0166 |          30.8836 |          -3.4429 |
[32m[20221213 12:54:13 @agent_ppo2.py:179][0m |          -0.0249 |          30.6782 |          -3.6660 |
[32m[20221213 12:54:13 @agent_ppo2.py:179][0m |          -0.0296 |          30.5346 |          -4.3720 |
[32m[20221213 12:54:13 @agent_ppo2.py:179][0m |          -0.0297 |          30.1935 |          -4.5495 |
[32m[20221213 12:54:14 @agent_ppo2.py:179][0m |          -0.0385 |          30.0904 |          -5.0436 |
[32m[20221213 12:54:14 @agent_ppo2.py:179][0m |          -0.0429 |          29.9214 |          -5.2503 |
[32m[20221213 12:54:14 @agent_ppo2.py:179][0m |          -0.0376 |          29.8708 |          -5.7570 |
[32m[20221213 12:54:14 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 301.02
[32m[20221213 12:54:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.16
[32m[20221213 12:54:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.14
[32m[20221213 12:54:14 @agent_ppo2.py:137][0m Total time:      38.29 min
[32m[20221213 12:54:14 @agent_ppo2.py:139][0m 2641920 total steps have happened
[32m[20221213 12:54:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1290 --------------------------#
[32m[20221213 12:54:14 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:54:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:14 @agent_ppo2.py:179][0m |           0.0726 |          32.5868 |          -3.8461 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |           0.0372 |          32.0595 |          -0.8852 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |           0.0067 |          31.4785 |          -2.0800 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |          -0.0116 |          31.2990 |          -3.5507 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |          -0.0188 |          31.0411 |          -4.4997 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |          -0.0168 |          32.2198 |          -4.8065 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |          -0.0274 |          30.7497 |          -4.9442 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |          -0.0321 |          30.6410 |          -5.1539 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |          -0.0359 |          30.4780 |          -5.7298 |
[32m[20221213 12:54:15 @agent_ppo2.py:179][0m |          -0.0250 |          33.6062 |          -6.0277 |
[32m[20221213 12:54:15 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:54:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.94
[32m[20221213 12:54:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.18
[32m[20221213 12:54:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.59
[32m[20221213 12:54:16 @agent_ppo2.py:137][0m Total time:      38.32 min
[32m[20221213 12:54:16 @agent_ppo2.py:139][0m 2643968 total steps have happened
[32m[20221213 12:54:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1291 --------------------------#
[32m[20221213 12:54:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:16 @agent_ppo2.py:179][0m |           0.0712 |          32.1856 |          -2.7396 |
[32m[20221213 12:54:16 @agent_ppo2.py:179][0m |           0.0808 |          35.4834 |           0.0384 |
[32m[20221213 12:54:16 @agent_ppo2.py:179][0m |           0.0300 |          34.0258 |          -1.4919 |
[32m[20221213 12:54:16 @agent_ppo2.py:179][0m |           0.0139 |          31.1950 |          -1.3061 |
[32m[20221213 12:54:17 @agent_ppo2.py:179][0m |           0.0100 |          33.7627 |          -2.3473 |
[32m[20221213 12:54:17 @agent_ppo2.py:179][0m |          -0.0095 |          30.9961 |          -3.4324 |
[32m[20221213 12:54:17 @agent_ppo2.py:179][0m |          -0.0152 |          30.7920 |          -3.7155 |
[32m[20221213 12:54:17 @agent_ppo2.py:179][0m |          -0.0247 |          30.7737 |          -4.4291 |
[32m[20221213 12:54:17 @agent_ppo2.py:179][0m |          -0.0237 |          32.8384 |          -5.0081 |
[32m[20221213 12:54:17 @agent_ppo2.py:179][0m |          -0.0317 |          30.5554 |          -5.3100 |
[32m[20221213 12:54:17 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:54:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 344.35
[32m[20221213 12:54:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.15
[32m[20221213 12:54:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.91
[32m[20221213 12:54:17 @agent_ppo2.py:137][0m Total time:      38.34 min
[32m[20221213 12:54:17 @agent_ppo2.py:139][0m 2646016 total steps have happened
[32m[20221213 12:54:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1292 --------------------------#
[32m[20221213 12:54:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:18 @agent_ppo2.py:179][0m |           0.0552 |          36.2247 |          -2.2097 |
[32m[20221213 12:54:18 @agent_ppo2.py:179][0m |           0.0211 |          32.2386 |          -1.2927 |
[32m[20221213 12:54:18 @agent_ppo2.py:179][0m |           0.0015 |          31.7620 |          -1.2207 |
[32m[20221213 12:54:18 @agent_ppo2.py:179][0m |          -0.0182 |          31.4487 |          -1.8670 |
[32m[20221213 12:54:18 @agent_ppo2.py:179][0m |          -0.0270 |          31.2001 |          -1.9190 |
[32m[20221213 12:54:18 @agent_ppo2.py:179][0m |          -0.0333 |          31.0655 |          -2.1089 |
[32m[20221213 12:54:18 @agent_ppo2.py:179][0m |          -0.0351 |          30.8977 |          -2.3523 |
[32m[20221213 12:54:19 @agent_ppo2.py:179][0m |          -0.0410 |          30.7556 |          -2.3845 |
[32m[20221213 12:54:19 @agent_ppo2.py:179][0m |          -0.0394 |          30.7275 |          -2.7447 |
[32m[20221213 12:54:19 @agent_ppo2.py:179][0m |          -0.0420 |          30.5548 |          -3.0461 |
[32m[20221213 12:54:19 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:54:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 302.08
[32m[20221213 12:54:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 308.88
[32m[20221213 12:54:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.14
[32m[20221213 12:54:19 @agent_ppo2.py:137][0m Total time:      38.37 min
[32m[20221213 12:54:19 @agent_ppo2.py:139][0m 2648064 total steps have happened
[32m[20221213 12:54:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1293 --------------------------#
[32m[20221213 12:54:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:19 @agent_ppo2.py:179][0m |           0.0660 |          32.5175 |          -0.8522 |
[32m[20221213 12:54:19 @agent_ppo2.py:179][0m |           0.0487 |          31.5336 |           0.9159 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |           0.0106 |          30.6647 |          -0.0953 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |           0.0009 |          32.9405 |          -0.8790 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |          -0.0041 |          32.6794 |          -1.3246 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |          -0.0217 |          30.0880 |          -1.8496 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |          -0.0273 |          29.8711 |          -2.3381 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |          -0.0307 |          29.8195 |          -2.6843 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |          -0.0296 |          29.6565 |          -2.8059 |
[32m[20221213 12:54:20 @agent_ppo2.py:179][0m |          -0.0296 |          29.5657 |          -3.4167 |
[32m[20221213 12:54:20 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 360.26
[32m[20221213 12:54:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.56
[32m[20221213 12:54:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 382.24
[32m[20221213 12:54:21 @agent_ppo2.py:137][0m Total time:      38.40 min
[32m[20221213 12:54:21 @agent_ppo2.py:139][0m 2650112 total steps have happened
[32m[20221213 12:54:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1294 --------------------------#
[32m[20221213 12:54:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:21 @agent_ppo2.py:179][0m |           0.0315 |          32.3632 |          -1.3080 |
[32m[20221213 12:54:21 @agent_ppo2.py:179][0m |           0.0091 |          31.6276 |           0.1064 |
[32m[20221213 12:54:21 @agent_ppo2.py:179][0m |           0.0004 |          31.3038 |          -0.0381 |
[32m[20221213 12:54:21 @agent_ppo2.py:179][0m |          -0.0220 |          31.1180 |           0.0072 |
[32m[20221213 12:54:21 @agent_ppo2.py:179][0m |          -0.0327 |          30.9599 |          -0.4352 |
[32m[20221213 12:54:22 @agent_ppo2.py:179][0m |          -0.0413 |          30.7502 |          -0.8850 |
[32m[20221213 12:54:22 @agent_ppo2.py:179][0m |          -0.0443 |          30.6258 |          -1.1341 |
[32m[20221213 12:54:22 @agent_ppo2.py:179][0m |          -0.0423 |          30.8625 |          -1.2843 |
[32m[20221213 12:54:22 @agent_ppo2.py:179][0m |          -0.0483 |          30.4477 |          -1.4949 |
[32m[20221213 12:54:22 @agent_ppo2.py:179][0m |          -0.0430 |          30.6794 |          -1.7663 |
[32m[20221213 12:54:22 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.11
[32m[20221213 12:54:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.31
[32m[20221213 12:54:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 288.27
[32m[20221213 12:54:22 @agent_ppo2.py:137][0m Total time:      38.43 min
[32m[20221213 12:54:22 @agent_ppo2.py:139][0m 2652160 total steps have happened
[32m[20221213 12:54:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1295 --------------------------#
[32m[20221213 12:54:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |           0.0608 |          31.7170 |           0.8449 |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |           0.0389 |          31.0771 |           2.6667 |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |           0.0002 |          30.7300 |           0.5199 |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |           0.0046 |          34.7908 |           0.8191 |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |          -0.0194 |          30.7204 |           0.4923 |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |          -0.0161 |          30.2457 |           0.2231 |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |          -0.0303 |          30.1624 |          -0.1903 |
[32m[20221213 12:54:23 @agent_ppo2.py:179][0m |          -0.0336 |          30.0428 |          -0.2911 |
[32m[20221213 12:54:24 @agent_ppo2.py:179][0m |          -0.0342 |          30.0300 |          -0.3993 |
[32m[20221213 12:54:24 @agent_ppo2.py:179][0m |          -0.0382 |          29.8521 |          -1.2714 |
[32m[20221213 12:54:24 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:54:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 311.68
[32m[20221213 12:54:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 321.52
[32m[20221213 12:54:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 60.02
[32m[20221213 12:54:24 @agent_ppo2.py:137][0m Total time:      38.45 min
[32m[20221213 12:54:24 @agent_ppo2.py:139][0m 2654208 total steps have happened
[32m[20221213 12:54:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1296 --------------------------#
[32m[20221213 12:54:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:24 @agent_ppo2.py:179][0m |           0.0765 |          31.6295 |           0.5780 |
[32m[20221213 12:54:24 @agent_ppo2.py:179][0m |           0.0500 |          30.9400 |           2.6428 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |           0.0332 |          30.6471 |           1.9172 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |           0.0101 |          30.4471 |           1.5829 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |          -0.0075 |          30.1595 |           0.9249 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |          -0.0215 |          30.0671 |           0.3415 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |          -0.0175 |          29.8920 |          -0.3918 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |          -0.0118 |          31.6473 |          -0.5629 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |          -0.0284 |          29.7356 |          -0.8735 |
[32m[20221213 12:54:25 @agent_ppo2.py:179][0m |          -0.0294 |          29.6226 |          -1.1601 |
[32m[20221213 12:54:25 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:54:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 348.07
[32m[20221213 12:54:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.90
[32m[20221213 12:54:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.07
[32m[20221213 12:54:26 @agent_ppo2.py:137][0m Total time:      38.48 min
[32m[20221213 12:54:26 @agent_ppo2.py:139][0m 2656256 total steps have happened
[32m[20221213 12:54:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1297 --------------------------#
[32m[20221213 12:54:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:26 @agent_ppo2.py:179][0m |           0.0361 |          32.7380 |           0.6154 |
[32m[20221213 12:54:26 @agent_ppo2.py:179][0m |           0.0167 |          31.5970 |           0.8562 |
[32m[20221213 12:54:26 @agent_ppo2.py:179][0m |          -0.0056 |          31.1379 |           1.2693 |
[32m[20221213 12:54:26 @agent_ppo2.py:179][0m |          -0.0231 |          30.7900 |           0.9833 |
[32m[20221213 12:54:26 @agent_ppo2.py:179][0m |          -0.0281 |          30.6088 |           0.4179 |
[32m[20221213 12:54:27 @agent_ppo2.py:179][0m |          -0.0327 |          30.3715 |          -0.0182 |
[32m[20221213 12:54:27 @agent_ppo2.py:179][0m |          -0.0343 |          30.2310 |          -0.0200 |
[32m[20221213 12:54:27 @agent_ppo2.py:179][0m |          -0.0331 |          30.1445 |           0.0011 |
[32m[20221213 12:54:27 @agent_ppo2.py:179][0m |          -0.0349 |          30.1521 |          -0.5794 |
[32m[20221213 12:54:27 @agent_ppo2.py:179][0m |          -0.0404 |          29.8994 |          -0.8468 |
[32m[20221213 12:54:27 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:54:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.54
[32m[20221213 12:54:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.46
[32m[20221213 12:54:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.81
[32m[20221213 12:54:27 @agent_ppo2.py:137][0m Total time:      38.51 min
[32m[20221213 12:54:27 @agent_ppo2.py:139][0m 2658304 total steps have happened
[32m[20221213 12:54:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1298 --------------------------#
[32m[20221213 12:54:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |           0.0809 |          32.6241 |           0.7756 |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |           0.0401 |          31.2149 |           2.3625 |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |           0.0150 |          30.5978 |           1.9608 |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |          -0.0132 |          30.0301 |           1.3376 |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |          -0.0230 |          29.6778 |           0.9033 |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |          -0.0302 |          30.1034 |           0.3576 |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |          -0.0325 |          28.9579 |           0.0159 |
[32m[20221213 12:54:28 @agent_ppo2.py:179][0m |          -0.0374 |          28.8141 |          -0.3201 |
[32m[20221213 12:54:29 @agent_ppo2.py:179][0m |          -0.0315 |          29.2567 |          -0.4484 |
[32m[20221213 12:54:29 @agent_ppo2.py:179][0m |          -0.0298 |          28.4358 |          -0.5170 |
[32m[20221213 12:54:29 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:54:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.60
[32m[20221213 12:54:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.17
[32m[20221213 12:54:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 310.66
[32m[20221213 12:54:29 @agent_ppo2.py:137][0m Total time:      38.54 min
[32m[20221213 12:54:29 @agent_ppo2.py:139][0m 2660352 total steps have happened
[32m[20221213 12:54:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1299 --------------------------#
[32m[20221213 12:54:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:29 @agent_ppo2.py:179][0m |           0.0728 |          34.3073 |           1.7039 |
[32m[20221213 12:54:29 @agent_ppo2.py:179][0m |           0.0396 |          32.2484 |           3.2402 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |           0.0161 |          31.3333 |           2.8106 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |          -0.0155 |          30.8056 |           1.9069 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |          -0.0243 |          30.3117 |           1.2129 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |          -0.0248 |          30.0181 |           0.9243 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |          -0.0330 |          29.6776 |           0.3581 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |          -0.0377 |          29.3255 |           0.0492 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |          -0.0419 |          29.0457 |          -0.3319 |
[32m[20221213 12:54:30 @agent_ppo2.py:179][0m |          -0.0437 |          28.8146 |          -0.4960 |
[32m[20221213 12:54:30 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.24
[32m[20221213 12:54:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.30
[32m[20221213 12:54:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 294.24
[32m[20221213 12:54:30 @agent_ppo2.py:137][0m Total time:      38.56 min
[32m[20221213 12:54:30 @agent_ppo2.py:139][0m 2662400 total steps have happened
[32m[20221213 12:54:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1300 --------------------------#
[32m[20221213 12:54:31 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:54:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:31 @agent_ppo2.py:179][0m |           0.0778 |          33.5279 |           1.7678 |
[32m[20221213 12:54:31 @agent_ppo2.py:179][0m |           0.0711 |          32.3873 |           2.2917 |
[32m[20221213 12:54:31 @agent_ppo2.py:179][0m |           0.0283 |          31.9589 |           0.8259 |
[32m[20221213 12:54:31 @agent_ppo2.py:179][0m |           0.0025 |          32.3118 |           0.6909 |
[32m[20221213 12:54:31 @agent_ppo2.py:179][0m |          -0.0033 |          34.0608 |           0.1340 |
[32m[20221213 12:54:31 @agent_ppo2.py:179][0m |          -0.0244 |          31.3336 |          -0.2361 |
[32m[20221213 12:54:32 @agent_ppo2.py:179][0m |          -0.0342 |          31.1339 |          -0.6893 |
[32m[20221213 12:54:32 @agent_ppo2.py:179][0m |          -0.0344 |          30.9298 |          -0.9450 |
[32m[20221213 12:54:32 @agent_ppo2.py:179][0m |          -0.0320 |          30.7080 |          -1.0529 |
[32m[20221213 12:54:32 @agent_ppo2.py:179][0m |          -0.0350 |          30.7191 |          -1.5067 |
[32m[20221213 12:54:32 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:54:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.12
[32m[20221213 12:54:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.19
[32m[20221213 12:54:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.41
[32m[20221213 12:54:32 @agent_ppo2.py:137][0m Total time:      38.59 min
[32m[20221213 12:54:32 @agent_ppo2.py:139][0m 2664448 total steps have happened
[32m[20221213 12:54:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1301 --------------------------#
[32m[20221213 12:54:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |           0.0592 |          34.7039 |           0.3777 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |           0.0492 |          33.3424 |           1.2399 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |           0.0079 |          32.8788 |           0.4869 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |          -0.0228 |          32.4821 |           0.0087 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |          -0.0195 |          36.9548 |          -0.4795 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |          -0.0301 |          32.3199 |          -0.8689 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |          -0.0315 |          31.9621 |          -1.0203 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |          -0.0401 |          31.8539 |          -0.9231 |
[32m[20221213 12:54:33 @agent_ppo2.py:179][0m |          -0.0466 |          31.7053 |          -1.6416 |
[32m[20221213 12:54:34 @agent_ppo2.py:179][0m |          -0.0505 |          31.5923 |          -2.2063 |
[32m[20221213 12:54:34 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:54:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.82
[32m[20221213 12:54:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.54
[32m[20221213 12:54:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 314.88
[32m[20221213 12:54:34 @agent_ppo2.py:137][0m Total time:      38.62 min
[32m[20221213 12:54:34 @agent_ppo2.py:139][0m 2666496 total steps have happened
[32m[20221213 12:54:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1302 --------------------------#
[32m[20221213 12:54:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:34 @agent_ppo2.py:179][0m |           0.0817 |          34.1662 |           0.3275 |
[32m[20221213 12:54:34 @agent_ppo2.py:179][0m |           0.0311 |          33.0437 |           1.7111 |
[32m[20221213 12:54:34 @agent_ppo2.py:179][0m |           0.0029 |          32.5942 |           0.3617 |
[32m[20221213 12:54:35 @agent_ppo2.py:179][0m |          -0.0158 |          32.4605 |          -0.0112 |
[32m[20221213 12:54:35 @agent_ppo2.py:179][0m |          -0.0226 |          32.3053 |          -0.5454 |
[32m[20221213 12:54:35 @agent_ppo2.py:179][0m |          -0.0138 |          34.4768 |          -0.8029 |
[32m[20221213 12:54:35 @agent_ppo2.py:179][0m |          -0.0249 |          32.4609 |          -0.9920 |
[32m[20221213 12:54:35 @agent_ppo2.py:179][0m |          -0.0101 |          31.8713 |           0.2281 |
[32m[20221213 12:54:35 @agent_ppo2.py:179][0m |          -0.0307 |          31.8475 |          -0.6129 |
[32m[20221213 12:54:35 @agent_ppo2.py:179][0m |          -0.0340 |          31.7549 |          -0.9546 |
[32m[20221213 12:54:35 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:54:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 354.33
[32m[20221213 12:54:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 375.77
[32m[20221213 12:54:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.69
[32m[20221213 12:54:35 @agent_ppo2.py:137][0m Total time:      38.64 min
[32m[20221213 12:54:35 @agent_ppo2.py:139][0m 2668544 total steps have happened
[32m[20221213 12:54:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1303 --------------------------#
[32m[20221213 12:54:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:36 @agent_ppo2.py:179][0m |           0.0599 |          33.6438 |           0.1363 |
[32m[20221213 12:54:36 @agent_ppo2.py:179][0m |           0.0607 |          33.0514 |           3.7878 |
[32m[20221213 12:54:36 @agent_ppo2.py:179][0m |           0.0173 |          32.7974 |           2.5739 |
[32m[20221213 12:54:36 @agent_ppo2.py:179][0m |           0.0043 |          32.8200 |           1.7234 |
[32m[20221213 12:54:36 @agent_ppo2.py:179][0m |          -0.0160 |          32.6931 |           1.1694 |
[32m[20221213 12:54:36 @agent_ppo2.py:179][0m |          -0.0227 |          32.3828 |           1.0415 |
[32m[20221213 12:54:37 @agent_ppo2.py:179][0m |          -0.0275 |          32.2775 |           0.7398 |
[32m[20221213 12:54:37 @agent_ppo2.py:179][0m |          -0.0292 |          32.5065 |           0.4189 |
[32m[20221213 12:54:37 @agent_ppo2.py:179][0m |          -0.0293 |          32.0912 |           0.1275 |
[32m[20221213 12:54:37 @agent_ppo2.py:179][0m |          -0.0310 |          32.1777 |          -0.5094 |
[32m[20221213 12:54:37 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:54:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.15
[32m[20221213 12:54:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.62
[32m[20221213 12:54:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.30
[32m[20221213 12:54:37 @agent_ppo2.py:137][0m Total time:      38.67 min
[32m[20221213 12:54:37 @agent_ppo2.py:139][0m 2670592 total steps have happened
[32m[20221213 12:54:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1304 --------------------------#
[32m[20221213 12:54:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |           0.0516 |          33.6914 |           2.1120 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |           0.0503 |          33.0431 |           2.5105 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |           0.0130 |          32.6811 |           2.5298 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |          -0.0088 |          33.0956 |           1.9686 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |          -0.0052 |          32.4264 |           2.5691 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |          -0.0149 |          33.0259 |           1.7635 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |          -0.0307 |          32.1416 |           1.6618 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |          -0.0345 |          32.0586 |           1.1779 |
[32m[20221213 12:54:38 @agent_ppo2.py:179][0m |          -0.0296 |          33.7946 |           0.7461 |
[32m[20221213 12:54:39 @agent_ppo2.py:179][0m |          -0.0368 |          31.8319 |           0.5086 |
[32m[20221213 12:54:39 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:54:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.29
[32m[20221213 12:54:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.19
[32m[20221213 12:54:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.33
[32m[20221213 12:54:39 @agent_ppo2.py:137][0m Total time:      38.70 min
[32m[20221213 12:54:39 @agent_ppo2.py:139][0m 2672640 total steps have happened
[32m[20221213 12:54:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1305 --------------------------#
[32m[20221213 12:54:39 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:39 @agent_ppo2.py:179][0m |           0.0728 |          30.8633 |           2.9255 |
[32m[20221213 12:54:39 @agent_ppo2.py:179][0m |           0.0212 |          28.4636 |           3.7933 |
[32m[20221213 12:54:39 @agent_ppo2.py:179][0m |          -0.0084 |          28.7289 |           3.2247 |
[32m[20221213 12:54:39 @agent_ppo2.py:179][0m |          -0.0241 |          27.6529 |           3.3580 |
[32m[20221213 12:54:40 @agent_ppo2.py:179][0m |          -0.0329 |          27.3251 |           3.0740 |
[32m[20221213 12:54:40 @agent_ppo2.py:179][0m |          -0.0291 |          27.1212 |           2.8303 |
[32m[20221213 12:54:40 @agent_ppo2.py:179][0m |          -0.0464 |          26.9772 |           2.3253 |
[32m[20221213 12:54:40 @agent_ppo2.py:179][0m |          -0.0518 |          26.8487 |           1.9880 |
[32m[20221213 12:54:40 @agent_ppo2.py:179][0m |          -0.0552 |          26.8697 |           1.8816 |
[32m[20221213 12:54:40 @agent_ppo2.py:179][0m |          -0.0589 |          26.5160 |           1.8317 |
[32m[20221213 12:54:40 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:54:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.46
[32m[20221213 12:54:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.09
[32m[20221213 12:54:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 380.60
[32m[20221213 12:54:40 @agent_ppo2.py:137][0m Total time:      38.73 min
[32m[20221213 12:54:40 @agent_ppo2.py:139][0m 2674688 total steps have happened
[32m[20221213 12:54:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1306 --------------------------#
[32m[20221213 12:54:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:41 @agent_ppo2.py:179][0m |           0.0297 |          30.3098 |           1.7764 |
[32m[20221213 12:54:41 @agent_ppo2.py:179][0m |           0.0166 |          27.7649 |           3.3393 |
[32m[20221213 12:54:41 @agent_ppo2.py:179][0m |          -0.0139 |          27.1830 |           2.7019 |
[32m[20221213 12:54:41 @agent_ppo2.py:179][0m |          -0.0209 |          26.9389 |           2.3874 |
[32m[20221213 12:54:41 @agent_ppo2.py:179][0m |          -0.0301 |          26.8516 |           2.2314 |
[32m[20221213 12:54:41 @agent_ppo2.py:179][0m |          -0.0283 |          26.8372 |           2.4541 |
[32m[20221213 12:54:41 @agent_ppo2.py:179][0m |          -0.0296 |          26.5027 |           2.0958 |
[32m[20221213 12:54:42 @agent_ppo2.py:179][0m |          -0.0275 |          26.3866 |           2.3643 |
[32m[20221213 12:54:42 @agent_ppo2.py:179][0m |          -0.0269 |          27.0399 |           1.7785 |
[32m[20221213 12:54:42 @agent_ppo2.py:179][0m |          -0.0365 |          26.2512 |           1.4584 |
[32m[20221213 12:54:42 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:54:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.99
[32m[20221213 12:54:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.52
[32m[20221213 12:54:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 386.23
[32m[20221213 12:54:42 @agent_ppo2.py:137][0m Total time:      38.76 min
[32m[20221213 12:54:42 @agent_ppo2.py:139][0m 2676736 total steps have happened
[32m[20221213 12:54:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1307 --------------------------#
[32m[20221213 12:54:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:42 @agent_ppo2.py:179][0m |           0.0451 |          36.6157 |           1.4740 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |           0.0099 |          34.8221 |           1.4612 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0178 |          34.1836 |           1.0721 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0297 |          34.0378 |           0.6508 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0350 |          33.5590 |           0.6052 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0401 |          33.3931 |           0.4799 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0352 |          34.1567 |           0.0729 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0363 |          36.2793 |           0.0304 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0385 |          32.7132 |           0.3668 |
[32m[20221213 12:54:43 @agent_ppo2.py:179][0m |          -0.0531 |          32.5384 |          -0.4850 |
[32m[20221213 12:54:43 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 306.16
[32m[20221213 12:54:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 373.59
[32m[20221213 12:54:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.64
[32m[20221213 12:54:44 @agent_ppo2.py:137][0m Total time:      38.78 min
[32m[20221213 12:54:44 @agent_ppo2.py:139][0m 2678784 total steps have happened
[32m[20221213 12:54:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1308 --------------------------#
[32m[20221213 12:54:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:44 @agent_ppo2.py:179][0m |           0.0601 |          35.5263 |           1.6963 |
[32m[20221213 12:54:44 @agent_ppo2.py:179][0m |           0.0227 |          33.7923 |           2.0743 |
[32m[20221213 12:54:44 @agent_ppo2.py:179][0m |           0.0014 |          34.8912 |           0.7462 |
[32m[20221213 12:54:44 @agent_ppo2.py:179][0m |          -0.0285 |          32.4919 |           0.2544 |
[32m[20221213 12:54:45 @agent_ppo2.py:179][0m |          -0.0328 |          32.0926 |           0.0587 |
[32m[20221213 12:54:45 @agent_ppo2.py:179][0m |          -0.0408 |          31.7764 |          -0.5148 |
[32m[20221213 12:54:45 @agent_ppo2.py:179][0m |          -0.0395 |          31.5373 |          -0.9441 |
[32m[20221213 12:54:45 @agent_ppo2.py:179][0m |          -0.0331 |          32.2466 |          -0.9723 |
[32m[20221213 12:54:45 @agent_ppo2.py:179][0m |          -0.0393 |          31.0589 |          -0.9984 |
[32m[20221213 12:54:45 @agent_ppo2.py:179][0m |          -0.0419 |          32.3844 |          -1.7777 |
[32m[20221213 12:54:45 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:54:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 299.84
[32m[20221213 12:54:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.15
[32m[20221213 12:54:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.80
[32m[20221213 12:54:45 @agent_ppo2.py:137][0m Total time:      38.81 min
[32m[20221213 12:54:45 @agent_ppo2.py:139][0m 2680832 total steps have happened
[32m[20221213 12:54:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1309 --------------------------#
[32m[20221213 12:54:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:46 @agent_ppo2.py:179][0m |           0.0796 |          34.4721 |           1.0244 |
[32m[20221213 12:54:46 @agent_ppo2.py:179][0m |           0.1797 |          37.3167 |           2.9983 |
[32m[20221213 12:54:46 @agent_ppo2.py:179][0m |           0.0500 |          32.9525 |           3.4082 |
[32m[20221213 12:54:46 @agent_ppo2.py:179][0m |           0.0041 |          32.6668 |           1.2723 |
[32m[20221213 12:54:46 @agent_ppo2.py:179][0m |          -0.0121 |          32.5537 |           0.1250 |
[32m[20221213 12:54:46 @agent_ppo2.py:179][0m |          -0.0270 |          32.1045 |          -0.4024 |
[32m[20221213 12:54:46 @agent_ppo2.py:179][0m |          -0.0160 |          37.1693 |          -0.7563 |
[32m[20221213 12:54:47 @agent_ppo2.py:179][0m |          -0.0297 |          33.5965 |          -1.2209 |
[32m[20221213 12:54:47 @agent_ppo2.py:179][0m |          -0.0387 |          33.2131 |          -1.5080 |
[32m[20221213 12:54:47 @agent_ppo2.py:179][0m |          -0.0349 |          31.5598 |          -1.4016 |
[32m[20221213 12:54:47 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:54:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 314.82
[32m[20221213 12:54:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.49
[32m[20221213 12:54:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.19
[32m[20221213 12:54:47 @agent_ppo2.py:137][0m Total time:      38.84 min
[32m[20221213 12:54:47 @agent_ppo2.py:139][0m 2682880 total steps have happened
[32m[20221213 12:54:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1310 --------------------------#
[32m[20221213 12:54:47 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:54:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:47 @agent_ppo2.py:179][0m |           0.0787 |          35.7904 |           0.4389 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |           0.0213 |          34.6262 |           1.2805 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |           0.0002 |          36.8551 |          -0.8852 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |          -0.0227 |          33.7422 |          -0.9603 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |          -0.0295 |          33.2517 |          -0.8768 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |          -0.0275 |          35.9194 |          -1.2469 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |          -0.0387 |          32.7833 |          -1.7025 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |          -0.0457 |          32.5550 |          -1.8442 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |          -0.0401 |          32.7637 |          -2.0649 |
[32m[20221213 12:54:48 @agent_ppo2.py:179][0m |          -0.0176 |          32.3905 |          -1.7270 |
[32m[20221213 12:54:48 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:54:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.82
[32m[20221213 12:54:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.17
[32m[20221213 12:54:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 382.83
[32m[20221213 12:54:49 @agent_ppo2.py:137][0m Total time:      38.87 min
[32m[20221213 12:54:49 @agent_ppo2.py:139][0m 2684928 total steps have happened
[32m[20221213 12:54:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1311 --------------------------#
[32m[20221213 12:54:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:49 @agent_ppo2.py:179][0m |           0.0374 |          36.0590 |           0.5060 |
[32m[20221213 12:54:49 @agent_ppo2.py:179][0m |           0.0116 |          34.5094 |           1.1453 |
[32m[20221213 12:54:49 @agent_ppo2.py:179][0m |          -0.0105 |          34.4517 |           0.9213 |
[32m[20221213 12:54:49 @agent_ppo2.py:179][0m |          -0.0244 |          33.5958 |           0.1561 |
[32m[20221213 12:54:50 @agent_ppo2.py:179][0m |          -0.0331 |          33.3987 |          -0.1162 |
[32m[20221213 12:54:50 @agent_ppo2.py:179][0m |          -0.0420 |          33.2302 |          -0.1790 |
[32m[20221213 12:54:50 @agent_ppo2.py:179][0m |          -0.0253 |          37.7336 |          -0.1870 |
[32m[20221213 12:54:50 @agent_ppo2.py:179][0m |          -0.0419 |          33.0399 |          -0.7518 |
[32m[20221213 12:54:50 @agent_ppo2.py:179][0m |          -0.0475 |          32.7711 |          -0.7586 |
[32m[20221213 12:54:50 @agent_ppo2.py:179][0m |          -0.0448 |          32.6319 |          -0.7207 |
[32m[20221213 12:54:50 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 324.08
[32m[20221213 12:54:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.37
[32m[20221213 12:54:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.62
[32m[20221213 12:54:50 @agent_ppo2.py:137][0m Total time:      38.89 min
[32m[20221213 12:54:50 @agent_ppo2.py:139][0m 2686976 total steps have happened
[32m[20221213 12:54:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1312 --------------------------#
[32m[20221213 12:54:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |           0.0580 |          34.9632 |           0.8024 |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |           0.0378 |          33.5685 |           1.6392 |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |           0.0030 |          32.7883 |           0.3273 |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |          -0.0178 |          32.1473 |           0.4557 |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |          -0.0240 |          31.7998 |           0.1980 |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |          -0.0310 |          32.9621 |          -0.4296 |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |          -0.0401 |          31.1863 |          -0.9480 |
[32m[20221213 12:54:51 @agent_ppo2.py:179][0m |          -0.0318 |          31.0466 |          -1.0921 |
[32m[20221213 12:54:52 @agent_ppo2.py:179][0m |          -0.0344 |          30.7300 |          -1.5441 |
[32m[20221213 12:54:52 @agent_ppo2.py:179][0m |          -0.0428 |          30.6735 |          -2.2147 |
[32m[20221213 12:54:52 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 302.50
[32m[20221213 12:54:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.17
[32m[20221213 12:54:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.73
[32m[20221213 12:54:52 @agent_ppo2.py:137][0m Total time:      38.92 min
[32m[20221213 12:54:52 @agent_ppo2.py:139][0m 2689024 total steps have happened
[32m[20221213 12:54:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1313 --------------------------#
[32m[20221213 12:54:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:52 @agent_ppo2.py:179][0m |           0.0802 |          37.2473 |          -0.6909 |
[32m[20221213 12:54:52 @agent_ppo2.py:179][0m |           0.0488 |          33.5239 |           1.0054 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |           0.0001 |          32.6156 |          -0.6488 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |          -0.0113 |          32.3395 |          -1.4321 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |          -0.0308 |          31.1890 |          -2.0584 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |          -0.0301 |          31.0815 |          -2.4107 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |          -0.0339 |          30.3648 |          -2.3746 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |          -0.0373 |          29.9853 |          -2.6419 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |          -0.0394 |          29.6917 |          -2.7484 |
[32m[20221213 12:54:53 @agent_ppo2.py:179][0m |          -0.0444 |          29.4223 |          -3.0886 |
[32m[20221213 12:54:53 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.13
[32m[20221213 12:54:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.70
[32m[20221213 12:54:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 318.70
[32m[20221213 12:54:54 @agent_ppo2.py:137][0m Total time:      38.95 min
[32m[20221213 12:54:54 @agent_ppo2.py:139][0m 2691072 total steps have happened
[32m[20221213 12:54:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1314 --------------------------#
[32m[20221213 12:54:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:54 @agent_ppo2.py:179][0m |           0.0447 |          36.1799 |          -0.6192 |
[32m[20221213 12:54:54 @agent_ppo2.py:179][0m |           0.0450 |          33.2336 |           1.9125 |
[32m[20221213 12:54:54 @agent_ppo2.py:179][0m |          -0.0004 |          32.6206 |           0.3388 |
[32m[20221213 12:54:54 @agent_ppo2.py:179][0m |          -0.0185 |          32.2109 |          -0.6742 |
[32m[20221213 12:54:54 @agent_ppo2.py:179][0m |          -0.0229 |          32.4356 |          -0.9093 |
[32m[20221213 12:54:55 @agent_ppo2.py:179][0m |          -0.0284 |          31.6540 |          -1.1973 |
[32m[20221213 12:54:55 @agent_ppo2.py:179][0m |          -0.0332 |          31.7694 |          -1.8599 |
[32m[20221213 12:54:55 @agent_ppo2.py:179][0m |          -0.0382 |          31.3059 |          -2.2288 |
[32m[20221213 12:54:55 @agent_ppo2.py:179][0m |          -0.0410 |          31.0986 |          -2.6252 |
[32m[20221213 12:54:55 @agent_ppo2.py:179][0m |          -0.0374 |          30.8885 |          -2.8992 |
[32m[20221213 12:54:55 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:54:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.60
[32m[20221213 12:54:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.10
[32m[20221213 12:54:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.06
[32m[20221213 12:54:55 @agent_ppo2.py:137][0m Total time:      38.97 min
[32m[20221213 12:54:55 @agent_ppo2.py:139][0m 2693120 total steps have happened
[32m[20221213 12:54:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1315 --------------------------#
[32m[20221213 12:54:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |           0.0673 |          36.4071 |          -1.8772 |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |           0.0683 |          34.5801 |           0.1036 |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |           0.0110 |          33.7700 |          -1.6389 |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |          -0.0162 |          33.4690 |          -2.7854 |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |          -0.0327 |          32.9493 |          -3.1312 |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |          -0.0373 |          32.5483 |          -3.2401 |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |          -0.0385 |          34.3597 |          -3.4837 |
[32m[20221213 12:54:56 @agent_ppo2.py:179][0m |          -0.0456 |          32.0611 |          -3.6456 |
[32m[20221213 12:54:57 @agent_ppo2.py:179][0m |          -0.0438 |          31.7979 |          -3.6832 |
[32m[20221213 12:54:57 @agent_ppo2.py:179][0m |          -0.0475 |          31.5160 |          -3.9942 |
[32m[20221213 12:54:57 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:54:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 274.93
[32m[20221213 12:54:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.03
[32m[20221213 12:54:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.28
[32m[20221213 12:54:57 @agent_ppo2.py:137][0m Total time:      39.00 min
[32m[20221213 12:54:57 @agent_ppo2.py:139][0m 2695168 total steps have happened
[32m[20221213 12:54:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1316 --------------------------#
[32m[20221213 12:54:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:57 @agent_ppo2.py:179][0m |           0.0695 |          37.5666 |          -1.0671 |
[32m[20221213 12:54:57 @agent_ppo2.py:179][0m |           0.0430 |          35.5425 |           0.7248 |
[32m[20221213 12:54:57 @agent_ppo2.py:179][0m |          -0.0009 |          34.9611 |          -0.3883 |
[32m[20221213 12:54:58 @agent_ppo2.py:179][0m |          -0.0254 |          33.9177 |          -1.2051 |
[32m[20221213 12:54:58 @agent_ppo2.py:179][0m |          -0.0376 |          33.5976 |          -1.3805 |
[32m[20221213 12:54:58 @agent_ppo2.py:179][0m |          -0.0456 |          33.2260 |          -1.6771 |
[32m[20221213 12:54:58 @agent_ppo2.py:179][0m |          -0.0483 |          32.8448 |          -2.2140 |
[32m[20221213 12:54:58 @agent_ppo2.py:179][0m |          -0.0491 |          32.6147 |          -2.3359 |
[32m[20221213 12:54:58 @agent_ppo2.py:179][0m |          -0.0563 |          32.3469 |          -2.6825 |
[32m[20221213 12:54:58 @agent_ppo2.py:179][0m |          -0.0560 |          32.6259 |          -3.1218 |
[32m[20221213 12:54:58 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:54:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 248.43
[32m[20221213 12:54:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 322.04
[32m[20221213 12:54:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.11
[32m[20221213 12:54:58 @agent_ppo2.py:137][0m Total time:      39.03 min
[32m[20221213 12:54:58 @agent_ppo2.py:139][0m 2697216 total steps have happened
[32m[20221213 12:54:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1317 --------------------------#
[32m[20221213 12:54:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:54:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:54:59 @agent_ppo2.py:179][0m |           0.1032 |          36.5937 |           0.6499 |
[32m[20221213 12:54:59 @agent_ppo2.py:179][0m |           0.0967 |          35.1630 |           3.2748 |
[32m[20221213 12:54:59 @agent_ppo2.py:179][0m |           0.0468 |          34.5005 |           2.2656 |
[32m[20221213 12:54:59 @agent_ppo2.py:179][0m |           0.0191 |          33.9619 |           1.8960 |
[32m[20221213 12:54:59 @agent_ppo2.py:179][0m |          -0.0019 |          33.5543 |           0.9442 |
[32m[20221213 12:54:59 @agent_ppo2.py:179][0m |          -0.0184 |          33.3340 |           0.4490 |
[32m[20221213 12:55:00 @agent_ppo2.py:179][0m |          -0.0232 |          33.0673 |           0.2589 |
[32m[20221213 12:55:00 @agent_ppo2.py:179][0m |          -0.0274 |          32.7652 |          -0.0801 |
[32m[20221213 12:55:00 @agent_ppo2.py:179][0m |          -0.0342 |          32.6895 |          -0.4202 |
[32m[20221213 12:55:00 @agent_ppo2.py:179][0m |          -0.0362 |          32.3174 |          -0.4584 |
[32m[20221213 12:55:00 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:55:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 306.05
[32m[20221213 12:55:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.87
[32m[20221213 12:55:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.89
[32m[20221213 12:55:00 @agent_ppo2.py:137][0m Total time:      39.06 min
[32m[20221213 12:55:00 @agent_ppo2.py:139][0m 2699264 total steps have happened
[32m[20221213 12:55:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1318 --------------------------#
[32m[20221213 12:55:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |           0.0840 |          36.0393 |           1.1122 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |           0.0638 |          33.1167 |           2.8644 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |           0.0218 |          32.5896 |           2.2547 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |          -0.0096 |          32.2453 |           1.1178 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |          -0.0222 |          31.9790 |           1.0119 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |          -0.0247 |          31.6808 |           0.8327 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |          -0.0334 |          31.5312 |           0.4123 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |          -0.0347 |          31.3780 |           0.2589 |
[32m[20221213 12:55:01 @agent_ppo2.py:179][0m |          -0.0390 |          31.3343 |           0.0098 |
[32m[20221213 12:55:02 @agent_ppo2.py:179][0m |          -0.0410 |          31.2062 |          -0.1619 |
[32m[20221213 12:55:02 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:55:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.42
[32m[20221213 12:55:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.97
[32m[20221213 12:55:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 300.14
[32m[20221213 12:55:02 @agent_ppo2.py:137][0m Total time:      39.08 min
[32m[20221213 12:55:02 @agent_ppo2.py:139][0m 2701312 total steps have happened
[32m[20221213 12:55:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1319 --------------------------#
[32m[20221213 12:55:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:02 @agent_ppo2.py:179][0m |           0.0509 |          33.9249 |           1.0975 |
[32m[20221213 12:55:02 @agent_ppo2.py:179][0m |           0.0322 |          32.9875 |           2.5731 |
[32m[20221213 12:55:02 @agent_ppo2.py:179][0m |          -0.0028 |          32.6590 |           1.3426 |
[32m[20221213 12:55:03 @agent_ppo2.py:179][0m |          -0.0126 |          32.3897 |           1.3632 |
[32m[20221213 12:55:03 @agent_ppo2.py:179][0m |          -0.0170 |          33.1533 |           0.2310 |
[32m[20221213 12:55:03 @agent_ppo2.py:179][0m |          -0.0292 |          32.0675 |          -0.0782 |
[32m[20221213 12:55:03 @agent_ppo2.py:179][0m |          -0.0237 |          32.5191 |          -0.1756 |
[32m[20221213 12:55:03 @agent_ppo2.py:179][0m |          -0.0308 |          31.9724 |          -0.7967 |
[32m[20221213 12:55:03 @agent_ppo2.py:179][0m |          -0.0311 |          31.9642 |          -0.7513 |
[32m[20221213 12:55:03 @agent_ppo2.py:179][0m |          -0.0357 |          31.7364 |          -1.1103 |
[32m[20221213 12:55:03 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:55:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 361.13
[32m[20221213 12:55:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.86
[32m[20221213 12:55:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.72
[32m[20221213 12:55:03 @agent_ppo2.py:137][0m Total time:      39.11 min
[32m[20221213 12:55:03 @agent_ppo2.py:139][0m 2703360 total steps have happened
[32m[20221213 12:55:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1320 --------------------------#
[32m[20221213 12:55:04 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:55:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:04 @agent_ppo2.py:179][0m |           0.0865 |          33.7932 |           0.8388 |
[32m[20221213 12:55:04 @agent_ppo2.py:179][0m |           0.0458 |          31.7606 |           1.8264 |
[32m[20221213 12:55:04 @agent_ppo2.py:179][0m |           0.0096 |          30.9053 |           0.8881 |
[32m[20221213 12:55:04 @agent_ppo2.py:179][0m |          -0.0081 |          30.2966 |           0.2064 |
[32m[20221213 12:55:04 @agent_ppo2.py:179][0m |          -0.0142 |          30.8303 |           0.6502 |
[32m[20221213 12:55:04 @agent_ppo2.py:179][0m |          -0.0282 |          29.7506 |           0.2340 |
[32m[20221213 12:55:05 @agent_ppo2.py:179][0m |          -0.0334 |          29.5544 |           0.2006 |
[32m[20221213 12:55:05 @agent_ppo2.py:179][0m |          -0.0381 |          29.4108 |          -0.1026 |
[32m[20221213 12:55:05 @agent_ppo2.py:179][0m |          -0.0370 |          29.0786 |          -0.1481 |
[32m[20221213 12:55:05 @agent_ppo2.py:179][0m |          -0.0424 |          28.9849 |          -0.4359 |
[32m[20221213 12:55:05 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:55:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 314.04
[32m[20221213 12:55:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.68
[32m[20221213 12:55:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.16
[32m[20221213 12:55:05 @agent_ppo2.py:137][0m Total time:      39.14 min
[32m[20221213 12:55:05 @agent_ppo2.py:139][0m 2705408 total steps have happened
[32m[20221213 12:55:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1321 --------------------------#
[32m[20221213 12:55:05 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:55:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:05 @agent_ppo2.py:179][0m |           0.0522 |          36.1675 |           1.2959 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |           0.0545 |          35.0960 |           2.7559 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |           0.0132 |          34.6524 |           1.7052 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |          -0.0091 |          34.1909 |           1.4350 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |          -0.0225 |          33.9703 |           1.4061 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |          -0.0341 |          33.8112 |           1.4148 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |          -0.0340 |          33.5584 |           1.2839 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |          -0.0389 |          33.3897 |           1.1904 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |          -0.0359 |          33.3125 |           0.5679 |
[32m[20221213 12:55:06 @agent_ppo2.py:179][0m |          -0.0447 |          33.1663 |           0.6304 |
[32m[20221213 12:55:06 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:55:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 300.73
[32m[20221213 12:55:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.06
[32m[20221213 12:55:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 322.56
[32m[20221213 12:55:07 @agent_ppo2.py:137][0m Total time:      39.17 min
[32m[20221213 12:55:07 @agent_ppo2.py:139][0m 2707456 total steps have happened
[32m[20221213 12:55:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1322 --------------------------#
[32m[20221213 12:55:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:07 @agent_ppo2.py:179][0m |           0.0837 |          34.5766 |           2.3565 |
[32m[20221213 12:55:07 @agent_ppo2.py:179][0m |           0.0495 |          33.5442 |           4.0484 |
[32m[20221213 12:55:07 @agent_ppo2.py:179][0m |           0.0090 |          33.1355 |           2.8217 |
[32m[20221213 12:55:07 @agent_ppo2.py:179][0m |          -0.0111 |          32.9619 |           2.4180 |
[32m[20221213 12:55:08 @agent_ppo2.py:179][0m |          -0.0212 |          32.7638 |           2.0012 |
[32m[20221213 12:55:08 @agent_ppo2.py:179][0m |          -0.0278 |          32.6237 |           1.6310 |
[32m[20221213 12:55:08 @agent_ppo2.py:179][0m |          -0.0308 |          32.4631 |           1.1965 |
[32m[20221213 12:55:08 @agent_ppo2.py:179][0m |          -0.0304 |          32.2859 |           1.0498 |
[32m[20221213 12:55:08 @agent_ppo2.py:179][0m |          -0.0285 |          33.2751 |           0.7311 |
[32m[20221213 12:55:08 @agent_ppo2.py:179][0m |          -0.0394 |          32.1339 |           0.3743 |
[32m[20221213 12:55:08 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:55:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 361.80
[32m[20221213 12:55:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 368.85
[32m[20221213 12:55:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.15
[32m[20221213 12:55:08 @agent_ppo2.py:137][0m Total time:      39.19 min
[32m[20221213 12:55:08 @agent_ppo2.py:139][0m 2709504 total steps have happened
[32m[20221213 12:55:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1323 --------------------------#
[32m[20221213 12:55:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:09 @agent_ppo2.py:179][0m |           0.0824 |          35.1804 |           3.3789 |
[32m[20221213 12:55:09 @agent_ppo2.py:179][0m |           0.0479 |          33.9619 |           4.2725 |
[32m[20221213 12:55:09 @agent_ppo2.py:179][0m |           0.0148 |          35.2080 |           3.2662 |
[32m[20221213 12:55:09 @agent_ppo2.py:179][0m |          -0.0049 |          32.9512 |           3.3043 |
[32m[20221213 12:55:09 @agent_ppo2.py:179][0m |          -0.0200 |          33.0778 |           2.8763 |
[32m[20221213 12:55:09 @agent_ppo2.py:179][0m |          -0.0302 |          32.3240 |           2.6122 |
[32m[20221213 12:55:09 @agent_ppo2.py:179][0m |          -0.0256 |          35.2006 |           2.4685 |
[32m[20221213 12:55:10 @agent_ppo2.py:179][0m |          -0.0276 |          36.3278 |           2.1865 |
[32m[20221213 12:55:10 @agent_ppo2.py:179][0m |          -0.0386 |          31.8413 |           1.8865 |
[32m[20221213 12:55:10 @agent_ppo2.py:179][0m |          -0.0479 |          31.6322 |           1.6872 |
[32m[20221213 12:55:10 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:55:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 311.87
[32m[20221213 12:55:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 328.70
[32m[20221213 12:55:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.86
[32m[20221213 12:55:10 @agent_ppo2.py:137][0m Total time:      39.22 min
[32m[20221213 12:55:10 @agent_ppo2.py:139][0m 2711552 total steps have happened
[32m[20221213 12:55:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1324 --------------------------#
[32m[20221213 12:55:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:10 @agent_ppo2.py:179][0m |           0.0643 |          30.4174 |           2.6246 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |           0.0011 |          29.4423 |           1.8021 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0108 |          28.9668 |           1.7693 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0297 |          28.5162 |           1.2873 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0413 |          28.1835 |           0.7876 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0509 |          27.8699 |           0.2593 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0533 |          27.6145 |           0.1942 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0479 |          27.7419 |          -0.2195 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0514 |          27.0337 |          -0.4052 |
[32m[20221213 12:55:11 @agent_ppo2.py:179][0m |          -0.0599 |          26.8632 |          -0.7602 |
[32m[20221213 12:55:11 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:55:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 283.28
[32m[20221213 12:55:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.10
[32m[20221213 12:55:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.83
[32m[20221213 12:55:12 @agent_ppo2.py:137][0m Total time:      39.25 min
[32m[20221213 12:55:12 @agent_ppo2.py:139][0m 2713600 total steps have happened
[32m[20221213 12:55:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1325 --------------------------#
[32m[20221213 12:55:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:12 @agent_ppo2.py:179][0m |           0.0540 |          35.3502 |           1.4447 |
[32m[20221213 12:55:12 @agent_ppo2.py:179][0m |           0.0314 |          33.9101 |           3.1403 |
[32m[20221213 12:55:12 @agent_ppo2.py:179][0m |          -0.0059 |          32.8132 |           1.7319 |
[32m[20221213 12:55:12 @agent_ppo2.py:179][0m |          -0.0229 |          32.2484 |           1.0598 |
[32m[20221213 12:55:12 @agent_ppo2.py:179][0m |          -0.0222 |          31.6773 |           0.9488 |
[32m[20221213 12:55:13 @agent_ppo2.py:179][0m |          -0.0243 |          33.6420 |           0.7473 |
[32m[20221213 12:55:13 @agent_ppo2.py:179][0m |          -0.0337 |          31.0406 |           0.2284 |
[32m[20221213 12:55:13 @agent_ppo2.py:179][0m |          -0.0380 |          30.6388 |           0.2430 |
[32m[20221213 12:55:13 @agent_ppo2.py:179][0m |          -0.0401 |          30.3191 |           0.0880 |
[32m[20221213 12:55:13 @agent_ppo2.py:179][0m |          -0.0363 |          30.7086 |          -0.3600 |
[32m[20221213 12:55:13 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:55:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.79
[32m[20221213 12:55:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.27
[32m[20221213 12:55:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 321.19
[32m[20221213 12:55:13 @agent_ppo2.py:137][0m Total time:      39.28 min
[32m[20221213 12:55:13 @agent_ppo2.py:139][0m 2715648 total steps have happened
[32m[20221213 12:55:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1326 --------------------------#
[32m[20221213 12:55:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:14 @agent_ppo2.py:179][0m |           0.1049 |          35.3594 |           2.4878 |
[32m[20221213 12:55:14 @agent_ppo2.py:179][0m |           0.0479 |          34.3078 |           3.2441 |
[32m[20221213 12:55:14 @agent_ppo2.py:179][0m |           0.0175 |          34.0327 |           2.0111 |
[32m[20221213 12:55:14 @agent_ppo2.py:179][0m |          -0.0026 |          33.7944 |           1.6443 |
[32m[20221213 12:55:14 @agent_ppo2.py:179][0m |          -0.0156 |          33.6287 |           0.4463 |
[32m[20221213 12:55:14 @agent_ppo2.py:179][0m |          -0.0210 |          33.5798 |           0.5919 |
[32m[20221213 12:55:14 @agent_ppo2.py:179][0m |          -0.0268 |          33.4348 |           0.1813 |
[32m[20221213 12:55:15 @agent_ppo2.py:179][0m |          -0.0294 |          33.3558 |          -0.1197 |
[32m[20221213 12:55:15 @agent_ppo2.py:179][0m |          -0.0157 |          38.0134 |          -0.4260 |
[32m[20221213 12:55:15 @agent_ppo2.py:179][0m |          -0.0353 |          33.3635 |          -0.7817 |
[32m[20221213 12:55:15 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:55:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 367.09
[32m[20221213 12:55:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 376.50
[32m[20221213 12:55:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 366.80
[32m[20221213 12:55:15 @agent_ppo2.py:137][0m Total time:      39.30 min
[32m[20221213 12:55:15 @agent_ppo2.py:139][0m 2717696 total steps have happened
[32m[20221213 12:55:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1327 --------------------------#
[32m[20221213 12:55:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:15 @agent_ppo2.py:179][0m |           0.0900 |          35.8133 |           0.2168 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |           0.0488 |          34.1636 |           2.0365 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |           0.0112 |          33.6608 |           1.2135 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |           0.0035 |          35.1657 |           0.4129 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |          -0.0135 |          33.0131 |          -0.2389 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |          -0.0258 |          32.7725 |          -0.4151 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |          -0.0288 |          32.5873 |          -1.1802 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |          -0.0349 |          32.3871 |          -1.3868 |
[32m[20221213 12:55:16 @agent_ppo2.py:179][0m |          -0.0358 |          32.5156 |          -1.7209 |
[32m[20221213 12:55:17 @agent_ppo2.py:179][0m |          -0.0414 |          32.1010 |          -1.7789 |
[32m[20221213 12:55:17 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:55:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.32
[32m[20221213 12:55:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.90
[32m[20221213 12:55:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.92
[32m[20221213 12:55:17 @agent_ppo2.py:137][0m Total time:      39.33 min
[32m[20221213 12:55:17 @agent_ppo2.py:139][0m 2719744 total steps have happened
[32m[20221213 12:55:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1328 --------------------------#
[32m[20221213 12:55:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:17 @agent_ppo2.py:179][0m |           0.0532 |          34.8696 |           0.2635 |
[32m[20221213 12:55:17 @agent_ppo2.py:179][0m |           0.0464 |          33.0755 |           2.4175 |
[32m[20221213 12:55:17 @agent_ppo2.py:179][0m |          -0.0055 |          32.2347 |           1.4319 |
[32m[20221213 12:55:18 @agent_ppo2.py:179][0m |          -0.0201 |          31.6053 |           1.1103 |
[32m[20221213 12:55:18 @agent_ppo2.py:179][0m |          -0.0249 |          31.5881 |           0.4005 |
[32m[20221213 12:55:18 @agent_ppo2.py:179][0m |          -0.0329 |          31.0409 |           0.0812 |
[32m[20221213 12:55:18 @agent_ppo2.py:179][0m |          -0.0423 |          30.7163 |          -0.0590 |
[32m[20221213 12:55:18 @agent_ppo2.py:179][0m |          -0.0380 |          31.7474 |          -0.3462 |
[32m[20221213 12:55:18 @agent_ppo2.py:179][0m |          -0.0510 |          30.3061 |          -0.9764 |
[32m[20221213 12:55:18 @agent_ppo2.py:179][0m |          -0.0405 |          30.4298 |          -0.7625 |
[32m[20221213 12:55:18 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:55:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 294.04
[32m[20221213 12:55:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.47
[32m[20221213 12:55:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 304.37
[32m[20221213 12:55:19 @agent_ppo2.py:137][0m Total time:      39.36 min
[32m[20221213 12:55:19 @agent_ppo2.py:139][0m 2721792 total steps have happened
[32m[20221213 12:55:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1329 --------------------------#
[32m[20221213 12:55:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:19 @agent_ppo2.py:179][0m |           0.0533 |          35.4830 |          -0.4144 |
[32m[20221213 12:55:19 @agent_ppo2.py:179][0m |           0.0408 |          33.7410 |           0.5544 |
[32m[20221213 12:55:19 @agent_ppo2.py:179][0m |           0.0013 |          33.1828 |          -0.0827 |
[32m[20221213 12:55:19 @agent_ppo2.py:179][0m |          -0.0134 |          32.8452 |          -0.5749 |
[32m[20221213 12:55:19 @agent_ppo2.py:179][0m |          -0.0148 |          32.9463 |          -0.8836 |
[32m[20221213 12:55:20 @agent_ppo2.py:179][0m |          -0.0189 |          32.3608 |          -0.7405 |
[32m[20221213 12:55:20 @agent_ppo2.py:179][0m |          -0.0290 |          32.0523 |          -1.4270 |
[32m[20221213 12:55:20 @agent_ppo2.py:179][0m |          -0.0329 |          32.0022 |          -1.3661 |
[32m[20221213 12:55:20 @agent_ppo2.py:179][0m |          -0.0269 |          35.5961 |          -1.9924 |
[32m[20221213 12:55:20 @agent_ppo2.py:179][0m |          -0.0387 |          31.7390 |          -2.2753 |
[32m[20221213 12:55:20 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:55:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 364.34
[32m[20221213 12:55:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 382.50
[32m[20221213 12:55:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.95
[32m[20221213 12:55:20 @agent_ppo2.py:137][0m Total time:      39.39 min
[32m[20221213 12:55:20 @agent_ppo2.py:139][0m 2723840 total steps have happened
[32m[20221213 12:55:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1330 --------------------------#
[32m[20221213 12:55:21 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:55:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:21 @agent_ppo2.py:179][0m |           0.0772 |          35.8171 |           0.4627 |
[32m[20221213 12:55:21 @agent_ppo2.py:179][0m |           0.0488 |          34.5071 |           2.4133 |
[32m[20221213 12:55:21 @agent_ppo2.py:179][0m |           0.0167 |          34.5163 |           0.8966 |
[32m[20221213 12:55:21 @agent_ppo2.py:179][0m |          -0.0093 |          33.5164 |           0.4377 |
[32m[20221213 12:55:21 @agent_ppo2.py:179][0m |          -0.0239 |          33.2142 |           0.1150 |
[32m[20221213 12:55:21 @agent_ppo2.py:179][0m |          -0.0292 |          32.9559 |          -0.3372 |
[32m[20221213 12:55:22 @agent_ppo2.py:179][0m |          -0.0326 |          32.8380 |          -0.4964 |
[32m[20221213 12:55:22 @agent_ppo2.py:179][0m |          -0.0371 |          32.6114 |          -0.7396 |
[32m[20221213 12:55:22 @agent_ppo2.py:179][0m |          -0.0309 |          34.0391 |          -0.9757 |
[32m[20221213 12:55:22 @agent_ppo2.py:179][0m |          -0.0208 |          32.3940 |          -0.5731 |
[32m[20221213 12:55:22 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:55:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.95
[32m[20221213 12:55:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.80
[32m[20221213 12:55:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 371.33
[32m[20221213 12:55:22 @agent_ppo2.py:137][0m Total time:      39.42 min
[32m[20221213 12:55:22 @agent_ppo2.py:139][0m 2725888 total steps have happened
[32m[20221213 12:55:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1331 --------------------------#
[32m[20221213 12:55:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |           0.0655 |          34.7790 |          -0.5557 |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |           0.0796 |          35.1887 |           1.6413 |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |           0.0320 |          33.5197 |           1.0896 |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |           0.0027 |          32.8002 |           0.7296 |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |          -0.0178 |          32.6874 |          -0.0994 |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |          -0.0212 |          32.3432 |          -0.6032 |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |          -0.0212 |          32.7132 |          -0.8051 |
[32m[20221213 12:55:23 @agent_ppo2.py:179][0m |          -0.0280 |          31.9982 |          -1.0727 |
[32m[20221213 12:55:24 @agent_ppo2.py:179][0m |          -0.0261 |          32.0469 |          -1.3807 |
[32m[20221213 12:55:24 @agent_ppo2.py:179][0m |          -0.0271 |          31.8135 |          -1.5365 |
[32m[20221213 12:55:24 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:55:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.33
[32m[20221213 12:55:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.35
[32m[20221213 12:55:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 302.21
[32m[20221213 12:55:24 @agent_ppo2.py:137][0m Total time:      39.45 min
[32m[20221213 12:55:24 @agent_ppo2.py:139][0m 2727936 total steps have happened
[32m[20221213 12:55:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1332 --------------------------#
[32m[20221213 12:55:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:24 @agent_ppo2.py:179][0m |           0.0396 |          34.9951 |           0.6623 |
[32m[20221213 12:55:24 @agent_ppo2.py:179][0m |           0.0301 |          33.8013 |           2.0521 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0012 |          33.2944 |           1.1592 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0103 |          34.9988 |           0.8094 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0175 |          32.7372 |           0.5605 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0330 |          32.5747 |           0.1079 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0306 |          32.3321 |          -0.0528 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0306 |          32.2490 |          -0.4486 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0365 |          32.1078 |          -0.5379 |
[32m[20221213 12:55:25 @agent_ppo2.py:179][0m |          -0.0429 |          31.9180 |          -1.1027 |
[32m[20221213 12:55:25 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:55:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.04
[32m[20221213 12:55:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.58
[32m[20221213 12:55:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.25
[32m[20221213 12:55:26 @agent_ppo2.py:137][0m Total time:      39.48 min
[32m[20221213 12:55:26 @agent_ppo2.py:139][0m 2729984 total steps have happened
[32m[20221213 12:55:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1333 --------------------------#
[32m[20221213 12:55:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:26 @agent_ppo2.py:179][0m |           0.0506 |          33.7537 |          -0.0001 |
[32m[20221213 12:55:26 @agent_ppo2.py:179][0m |           0.0350 |          32.6414 |           1.6042 |
[32m[20221213 12:55:26 @agent_ppo2.py:179][0m |          -0.0082 |          32.1897 |           0.8092 |
[32m[20221213 12:55:26 @agent_ppo2.py:179][0m |          -0.0152 |          32.7892 |           0.2822 |
[32m[20221213 12:55:27 @agent_ppo2.py:179][0m |          -0.0172 |          31.6964 |           0.3167 |
[32m[20221213 12:55:27 @agent_ppo2.py:179][0m |          -0.0191 |          33.7896 |          -0.2862 |
[32m[20221213 12:55:27 @agent_ppo2.py:179][0m |          -0.0288 |          33.0689 |          -1.1428 |
[32m[20221213 12:55:27 @agent_ppo2.py:179][0m |          -0.0265 |          31.1858 |          -1.1005 |
[32m[20221213 12:55:27 @agent_ppo2.py:179][0m |          -0.0388 |          30.8925 |          -1.6287 |
[32m[20221213 12:55:27 @agent_ppo2.py:179][0m |          -0.0425 |          30.7491 |          -1.9149 |
[32m[20221213 12:55:27 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:55:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 314.20
[32m[20221213 12:55:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.99
[32m[20221213 12:55:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.74
[32m[20221213 12:55:27 @agent_ppo2.py:137][0m Total time:      39.51 min
[32m[20221213 12:55:27 @agent_ppo2.py:139][0m 2732032 total steps have happened
[32m[20221213 12:55:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1334 --------------------------#
[32m[20221213 12:55:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:28 @agent_ppo2.py:179][0m |           0.0612 |          34.4435 |          -0.0053 |
[32m[20221213 12:55:28 @agent_ppo2.py:179][0m |           0.0453 |          33.1040 |           0.8903 |
[32m[20221213 12:55:28 @agent_ppo2.py:179][0m |          -0.0004 |          32.5998 |           0.9364 |
[32m[20221213 12:55:28 @agent_ppo2.py:179][0m |          -0.0182 |          32.3552 |           0.1088 |
[32m[20221213 12:55:28 @agent_ppo2.py:179][0m |          -0.0283 |          32.1528 |           0.0263 |
[32m[20221213 12:55:28 @agent_ppo2.py:179][0m |          -0.0312 |          32.3427 |          -0.4733 |
[32m[20221213 12:55:29 @agent_ppo2.py:179][0m |          -0.0292 |          33.6860 |          -0.9651 |
[32m[20221213 12:55:29 @agent_ppo2.py:179][0m |          -0.0229 |          31.6290 |          -0.7059 |
[32m[20221213 12:55:29 @agent_ppo2.py:179][0m |          -0.0377 |          31.5050 |          -0.8871 |
[32m[20221213 12:55:29 @agent_ppo2.py:179][0m |          -0.0375 |          31.3943 |          -1.5534 |
[32m[20221213 12:55:29 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:55:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.69
[32m[20221213 12:55:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.25
[32m[20221213 12:55:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.15
[32m[20221213 12:55:29 @agent_ppo2.py:137][0m Total time:      39.54 min
[32m[20221213 12:55:29 @agent_ppo2.py:139][0m 2734080 total steps have happened
[32m[20221213 12:55:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1335 --------------------------#
[32m[20221213 12:55:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |           0.0526 |          33.8181 |          -1.2448 |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |           0.0344 |          33.3015 |          -0.2444 |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |          -0.0044 |          33.2239 |          -0.9731 |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |          -0.0073 |          32.4919 |          -1.1933 |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |          -0.0117 |          32.5821 |          -0.9737 |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |          -0.0229 |          31.9558 |          -1.1645 |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |          -0.0289 |          31.7231 |          -1.7293 |
[32m[20221213 12:55:30 @agent_ppo2.py:179][0m |          -0.0342 |          31.5315 |          -2.0514 |
[32m[20221213 12:55:31 @agent_ppo2.py:179][0m |          -0.0301 |          31.5763 |          -2.1469 |
[32m[20221213 12:55:31 @agent_ppo2.py:179][0m |          -0.0359 |          31.2457 |          -2.6006 |
[32m[20221213 12:55:31 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:55:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 352.17
[32m[20221213 12:55:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.40
[32m[20221213 12:55:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 219.93
[32m[20221213 12:55:31 @agent_ppo2.py:137][0m Total time:      39.57 min
[32m[20221213 12:55:31 @agent_ppo2.py:139][0m 2736128 total steps have happened
[32m[20221213 12:55:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1336 --------------------------#
[32m[20221213 12:55:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:31 @agent_ppo2.py:179][0m |           0.0557 |          36.3912 |          -1.2931 |
[32m[20221213 12:55:31 @agent_ppo2.py:179][0m |           0.0518 |          33.3214 |           1.2344 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0034 |          31.9323 |          -0.7551 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0247 |          31.4404 |          -1.2868 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0268 |          32.8572 |          -1.2200 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0398 |          30.8114 |          -1.6995 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0391 |          30.3164 |          -1.7102 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0350 |          31.7701 |          -2.1426 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0493 |          29.7474 |          -2.4236 |
[32m[20221213 12:55:32 @agent_ppo2.py:179][0m |          -0.0518 |          29.4866 |          -2.6564 |
[32m[20221213 12:55:32 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:55:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.33
[32m[20221213 12:55:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.48
[32m[20221213 12:55:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 358.36
[32m[20221213 12:55:33 @agent_ppo2.py:137][0m Total time:      39.60 min
[32m[20221213 12:55:33 @agent_ppo2.py:139][0m 2738176 total steps have happened
[32m[20221213 12:55:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1337 --------------------------#
[32m[20221213 12:55:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:33 @agent_ppo2.py:179][0m |           0.0599 |          35.9815 |          -0.2359 |
[32m[20221213 12:55:33 @agent_ppo2.py:179][0m |           0.0280 |          34.0548 |           0.8528 |
[32m[20221213 12:55:33 @agent_ppo2.py:179][0m |          -0.0099 |          33.1236 |           0.0281 |
[32m[20221213 12:55:33 @agent_ppo2.py:179][0m |          -0.0273 |          32.6157 |          -0.8499 |
[32m[20221213 12:55:34 @agent_ppo2.py:179][0m |          -0.0383 |          32.1432 |          -1.1954 |
[32m[20221213 12:55:34 @agent_ppo2.py:179][0m |          -0.0391 |          31.7879 |          -1.7163 |
[32m[20221213 12:55:34 @agent_ppo2.py:179][0m |          -0.0374 |          32.4054 |          -2.1981 |
[32m[20221213 12:55:34 @agent_ppo2.py:179][0m |          -0.0459 |          31.2884 |          -2.5102 |
[32m[20221213 12:55:34 @agent_ppo2.py:179][0m |          -0.0493 |          31.1081 |          -2.4853 |
[32m[20221213 12:55:34 @agent_ppo2.py:179][0m |          -0.0533 |          30.8595 |          -3.1483 |
[32m[20221213 12:55:34 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:55:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 297.13
[32m[20221213 12:55:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 334.35
[32m[20221213 12:55:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 334.52
[32m[20221213 12:55:34 @agent_ppo2.py:137][0m Total time:      39.63 min
[32m[20221213 12:55:34 @agent_ppo2.py:139][0m 2740224 total steps have happened
[32m[20221213 12:55:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1338 --------------------------#
[32m[20221213 12:55:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:35 @agent_ppo2.py:179][0m |           0.1109 |          34.4717 |           0.5487 |
[32m[20221213 12:55:35 @agent_ppo2.py:179][0m |           0.0540 |          32.8101 |           1.9157 |
[32m[20221213 12:55:35 @agent_ppo2.py:179][0m |           0.0047 |          31.9520 |           0.9767 |
[32m[20221213 12:55:35 @agent_ppo2.py:179][0m |          -0.0152 |          31.4875 |          -0.0009 |
[32m[20221213 12:55:35 @agent_ppo2.py:179][0m |          -0.0213 |          31.0231 |          -0.0371 |
[32m[20221213 12:55:35 @agent_ppo2.py:179][0m |          -0.0316 |          30.7304 |          -1.0624 |
[32m[20221213 12:55:36 @agent_ppo2.py:179][0m |          -0.0366 |          30.4472 |          -1.5009 |
[32m[20221213 12:55:36 @agent_ppo2.py:179][0m |          -0.0309 |          30.2591 |          -1.6999 |
[32m[20221213 12:55:36 @agent_ppo2.py:179][0m |          -0.0403 |          30.1143 |          -2.1513 |
[32m[20221213 12:55:36 @agent_ppo2.py:179][0m |          -0.0362 |          29.9137 |          -2.3385 |
[32m[20221213 12:55:36 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:55:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.93
[32m[20221213 12:55:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.33
[32m[20221213 12:55:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 333.78
[32m[20221213 12:55:36 @agent_ppo2.py:137][0m Total time:      39.66 min
[32m[20221213 12:55:36 @agent_ppo2.py:139][0m 2742272 total steps have happened
[32m[20221213 12:55:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1339 --------------------------#
[32m[20221213 12:55:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |           0.0375 |          33.4413 |          -1.8027 |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |           0.0113 |          31.2738 |          -0.5133 |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |          -0.0135 |          30.1625 |          -1.1104 |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |          -0.0278 |          29.3602 |          -1.7527 |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |          -0.0361 |          28.8649 |          -1.8066 |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |          -0.0388 |          28.5355 |          -2.0241 |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |          -0.0432 |          28.2078 |          -2.3956 |
[32m[20221213 12:55:37 @agent_ppo2.py:179][0m |          -0.0408 |          28.3756 |          -2.6970 |
[32m[20221213 12:55:38 @agent_ppo2.py:179][0m |          -0.0501 |          27.5318 |          -3.2267 |
[32m[20221213 12:55:38 @agent_ppo2.py:179][0m |          -0.0489 |          27.2518 |          -3.5624 |
[32m[20221213 12:55:38 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:55:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.85
[32m[20221213 12:55:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.22
[32m[20221213 12:55:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.31
[32m[20221213 12:55:38 @agent_ppo2.py:137][0m Total time:      39.69 min
[32m[20221213 12:55:38 @agent_ppo2.py:139][0m 2744320 total steps have happened
[32m[20221213 12:55:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1340 --------------------------#
[32m[20221213 12:55:38 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:55:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:38 @agent_ppo2.py:179][0m |           0.0769 |          34.6126 |          -1.7472 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |           0.0651 |          33.3502 |           1.0680 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |           0.0159 |          32.6864 |          -0.3402 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |          -0.0046 |          32.1430 |          -1.0094 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |          -0.0174 |          31.8483 |          -1.2970 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |          -0.0221 |          32.2046 |          -2.2554 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |          -0.0326 |          31.2102 |          -2.4759 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |          -0.0390 |          30.9742 |          -2.9923 |
[32m[20221213 12:55:39 @agent_ppo2.py:179][0m |          -0.0373 |          30.7058 |          -3.6240 |
[32m[20221213 12:55:40 @agent_ppo2.py:179][0m |          -0.0414 |          30.6597 |          -3.8778 |
[32m[20221213 12:55:40 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:55:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.64
[32m[20221213 12:55:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.52
[32m[20221213 12:55:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.10
[32m[20221213 12:55:40 @agent_ppo2.py:137][0m Total time:      39.72 min
[32m[20221213 12:55:40 @agent_ppo2.py:139][0m 2746368 total steps have happened
[32m[20221213 12:55:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1341 --------------------------#
[32m[20221213 12:55:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:40 @agent_ppo2.py:179][0m |           0.0700 |          35.4932 |          -2.3848 |
[32m[20221213 12:55:40 @agent_ppo2.py:179][0m |           0.0481 |          34.1852 |          -0.6860 |
[32m[20221213 12:55:40 @agent_ppo2.py:179][0m |           0.0158 |          33.5883 |          -0.5834 |
[32m[20221213 12:55:41 @agent_ppo2.py:179][0m |          -0.0160 |          33.3638 |          -2.0680 |
[32m[20221213 12:55:41 @agent_ppo2.py:179][0m |          -0.0207 |          33.1393 |          -2.8443 |
[32m[20221213 12:55:41 @agent_ppo2.py:179][0m |          -0.0320 |          32.8313 |          -2.5157 |
[32m[20221213 12:55:41 @agent_ppo2.py:179][0m |          -0.0382 |          32.6292 |          -3.1351 |
[32m[20221213 12:55:41 @agent_ppo2.py:179][0m |          -0.0403 |          32.5116 |          -3.7588 |
[32m[20221213 12:55:41 @agent_ppo2.py:179][0m |          -0.0348 |          33.5654 |          -3.7144 |
[32m[20221213 12:55:41 @agent_ppo2.py:179][0m |          -0.0356 |          32.9212 |          -3.8790 |
[32m[20221213 12:55:41 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:55:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 327.13
[32m[20221213 12:55:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.36
[32m[20221213 12:55:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.49
[32m[20221213 12:55:42 @agent_ppo2.py:137][0m Total time:      39.75 min
[32m[20221213 12:55:42 @agent_ppo2.py:139][0m 2748416 total steps have happened
[32m[20221213 12:55:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1342 --------------------------#
[32m[20221213 12:55:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:42 @agent_ppo2.py:179][0m |           0.0727 |          34.9297 |          -0.2784 |
[32m[20221213 12:55:42 @agent_ppo2.py:179][0m |           0.0393 |          33.6010 |           1.2483 |
[32m[20221213 12:55:42 @agent_ppo2.py:179][0m |           0.0013 |          33.0802 |           0.2753 |
[32m[20221213 12:55:42 @agent_ppo2.py:179][0m |          -0.0123 |          33.0091 |          -0.0163 |
[32m[20221213 12:55:42 @agent_ppo2.py:179][0m |          -0.0232 |          32.3810 |          -0.2268 |
[32m[20221213 12:55:43 @agent_ppo2.py:179][0m |          -0.0335 |          32.0512 |          -0.8874 |
[32m[20221213 12:55:43 @agent_ppo2.py:179][0m |          -0.0320 |          33.1501 |          -0.8454 |
[32m[20221213 12:55:43 @agent_ppo2.py:179][0m |          -0.0385 |          31.7236 |          -1.3375 |
[32m[20221213 12:55:43 @agent_ppo2.py:179][0m |          -0.0397 |          31.4733 |          -1.6540 |
[32m[20221213 12:55:43 @agent_ppo2.py:179][0m |          -0.0473 |          31.3330 |          -1.7624 |
[32m[20221213 12:55:43 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:55:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.32
[32m[20221213 12:55:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.29
[32m[20221213 12:55:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.51
[32m[20221213 12:55:43 @agent_ppo2.py:137][0m Total time:      39.78 min
[32m[20221213 12:55:43 @agent_ppo2.py:139][0m 2750464 total steps have happened
[32m[20221213 12:55:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1343 --------------------------#
[32m[20221213 12:55:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:44 @agent_ppo2.py:179][0m |           0.1236 |          29.8524 |           0.2716 |
[32m[20221213 12:55:44 @agent_ppo2.py:179][0m |           0.0377 |          26.0526 |           2.1543 |
[32m[20221213 12:55:44 @agent_ppo2.py:179][0m |          -0.0128 |          24.6306 |           0.5701 |
[32m[20221213 12:55:44 @agent_ppo2.py:179][0m |          -0.0233 |          23.6932 |           0.1527 |
[32m[20221213 12:55:44 @agent_ppo2.py:179][0m |          -0.0349 |          22.9924 |          -0.2040 |
[32m[20221213 12:55:44 @agent_ppo2.py:179][0m |          -0.0376 |          22.4676 |          -0.7456 |
[32m[20221213 12:55:44 @agent_ppo2.py:179][0m |          -0.0336 |          23.2124 |          -0.9869 |
[32m[20221213 12:55:45 @agent_ppo2.py:179][0m |          -0.0431 |          21.8064 |          -1.1573 |
[32m[20221213 12:55:45 @agent_ppo2.py:179][0m |          -0.0474 |          21.5125 |          -1.4888 |
[32m[20221213 12:55:45 @agent_ppo2.py:179][0m |          -0.0482 |          21.2867 |          -1.7505 |
[32m[20221213 12:55:45 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:55:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 349.91
[32m[20221213 12:55:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 373.61
[32m[20221213 12:55:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.23
[32m[20221213 12:55:45 @agent_ppo2.py:137][0m Total time:      39.81 min
[32m[20221213 12:55:45 @agent_ppo2.py:139][0m 2752512 total steps have happened
[32m[20221213 12:55:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1344 --------------------------#
[32m[20221213 12:55:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:45 @agent_ppo2.py:179][0m |           0.0773 |          37.9675 |          -0.6183 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |           0.0165 |          35.4360 |          -0.5727 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |          -0.0133 |          34.4082 |          -1.0492 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |          -0.0261 |          33.8997 |          -1.1324 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |          -0.0389 |          33.4160 |          -1.4822 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |          -0.0456 |          33.1505 |          -1.6999 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |          -0.0477 |          32.8870 |          -2.1267 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |          -0.0493 |          32.6889 |          -2.3394 |
[32m[20221213 12:55:46 @agent_ppo2.py:179][0m |          -0.0557 |          32.4446 |          -2.6742 |
[32m[20221213 12:55:47 @agent_ppo2.py:179][0m |          -0.0556 |          32.2596 |          -2.5891 |
[32m[20221213 12:55:47 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:55:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.41
[32m[20221213 12:55:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.84
[32m[20221213 12:55:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.83
[32m[20221213 12:55:47 @agent_ppo2.py:137][0m Total time:      39.83 min
[32m[20221213 12:55:47 @agent_ppo2.py:139][0m 2754560 total steps have happened
[32m[20221213 12:55:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1345 --------------------------#
[32m[20221213 12:55:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:47 @agent_ppo2.py:179][0m |           0.0438 |          36.9247 |          -1.3996 |
[32m[20221213 12:55:47 @agent_ppo2.py:179][0m |           0.0317 |          35.4890 |          -0.1910 |
[32m[20221213 12:55:47 @agent_ppo2.py:179][0m |           0.0045 |          34.9887 |          -0.8007 |
[32m[20221213 12:55:48 @agent_ppo2.py:179][0m |          -0.0115 |          34.4254 |          -0.6532 |
[32m[20221213 12:55:48 @agent_ppo2.py:179][0m |          -0.0313 |          34.1204 |          -1.6490 |
[32m[20221213 12:55:48 @agent_ppo2.py:179][0m |          -0.0352 |          33.7913 |          -2.4146 |
[32m[20221213 12:55:48 @agent_ppo2.py:179][0m |          -0.0406 |          33.6013 |          -2.6229 |
[32m[20221213 12:55:48 @agent_ppo2.py:179][0m |          -0.0398 |          33.4492 |          -2.9674 |
[32m[20221213 12:55:48 @agent_ppo2.py:179][0m |          -0.0438 |          33.2309 |          -3.3868 |
[32m[20221213 12:55:48 @agent_ppo2.py:179][0m |          -0.0501 |          32.9401 |          -3.7749 |
[32m[20221213 12:55:48 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:55:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.94
[32m[20221213 12:55:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.38
[32m[20221213 12:55:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.54
[32m[20221213 12:55:49 @agent_ppo2.py:137][0m Total time:      39.86 min
[32m[20221213 12:55:49 @agent_ppo2.py:139][0m 2756608 total steps have happened
[32m[20221213 12:55:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1346 --------------------------#
[32m[20221213 12:55:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:49 @agent_ppo2.py:179][0m |           0.0430 |          32.3654 |          -1.4930 |
[32m[20221213 12:55:49 @agent_ppo2.py:179][0m |           0.0034 |          30.5216 |          -1.0632 |
[32m[20221213 12:55:49 @agent_ppo2.py:179][0m |          -0.0162 |          29.7378 |          -2.5978 |
[32m[20221213 12:55:49 @agent_ppo2.py:179][0m |          -0.0368 |          29.2293 |          -2.9220 |
[32m[20221213 12:55:49 @agent_ppo2.py:179][0m |          -0.0448 |          28.8368 |          -3.6547 |
[32m[20221213 12:55:50 @agent_ppo2.py:179][0m |          -0.0450 |          28.6043 |          -3.7616 |
[32m[20221213 12:55:50 @agent_ppo2.py:179][0m |          -0.0451 |          28.4200 |          -4.2657 |
[32m[20221213 12:55:50 @agent_ppo2.py:179][0m |          -0.0574 |          28.2451 |          -4.7464 |
[32m[20221213 12:55:50 @agent_ppo2.py:179][0m |          -0.0561 |          28.0444 |          -5.2161 |
[32m[20221213 12:55:50 @agent_ppo2.py:179][0m |          -0.0624 |          27.8493 |          -5.6676 |
[32m[20221213 12:55:50 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:55:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 270.99
[32m[20221213 12:55:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 370.58
[32m[20221213 12:55:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 392.84
[32m[20221213 12:55:50 @agent_ppo2.py:137][0m Total time:      39.89 min
[32m[20221213 12:55:50 @agent_ppo2.py:139][0m 2758656 total steps have happened
[32m[20221213 12:55:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1347 --------------------------#
[32m[20221213 12:55:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:51 @agent_ppo2.py:179][0m |           0.0544 |          39.6491 |          -2.8280 |
[32m[20221213 12:55:51 @agent_ppo2.py:179][0m |           0.0244 |          37.2389 |          -1.8156 |
[32m[20221213 12:55:51 @agent_ppo2.py:179][0m |          -0.0031 |          36.5847 |          -2.3726 |
[32m[20221213 12:55:51 @agent_ppo2.py:179][0m |          -0.0243 |          36.2987 |          -3.4552 |
[32m[20221213 12:55:51 @agent_ppo2.py:179][0m |          -0.0403 |          35.7617 |          -4.1087 |
[32m[20221213 12:55:51 @agent_ppo2.py:179][0m |          -0.0389 |          35.6076 |          -4.2490 |
[32m[20221213 12:55:52 @agent_ppo2.py:179][0m |          -0.0480 |          35.2316 |          -4.6942 |
[32m[20221213 12:55:52 @agent_ppo2.py:179][0m |          -0.0409 |          35.6174 |          -4.7874 |
[32m[20221213 12:55:52 @agent_ppo2.py:179][0m |          -0.0509 |          34.8143 |          -5.3594 |
[32m[20221213 12:55:52 @agent_ppo2.py:179][0m |          -0.0534 |          34.7035 |          -5.8334 |
[32m[20221213 12:55:52 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:55:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.80
[32m[20221213 12:55:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.81
[32m[20221213 12:55:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 302.53
[32m[20221213 12:55:52 @agent_ppo2.py:137][0m Total time:      39.92 min
[32m[20221213 12:55:52 @agent_ppo2.py:139][0m 2760704 total steps have happened
[32m[20221213 12:55:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1348 --------------------------#
[32m[20221213 12:55:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |           0.0887 |          37.7008 |          -4.4917 |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |           0.0401 |          36.0056 |          -2.8161 |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |           0.0143 |          36.7778 |          -4.7618 |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |          -0.0114 |          35.2012 |          -5.7073 |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |          -0.0199 |          35.6032 |          -6.0558 |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |          -0.0286 |          34.3972 |          -6.3289 |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |          -0.0348 |          34.0837 |          -7.3240 |
[32m[20221213 12:55:53 @agent_ppo2.py:179][0m |          -0.0403 |          33.8126 |          -7.8985 |
[32m[20221213 12:55:54 @agent_ppo2.py:179][0m |          -0.0411 |          33.6898 |          -7.9760 |
[32m[20221213 12:55:54 @agent_ppo2.py:179][0m |          -0.0404 |          33.8033 |          -8.3693 |
[32m[20221213 12:55:54 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:55:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.37
[32m[20221213 12:55:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.13
[32m[20221213 12:55:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 395.74
[32m[20221213 12:55:54 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 395.74
[32m[20221213 12:55:54 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 395.74
[32m[20221213 12:55:54 @agent_ppo2.py:137][0m Total time:      39.95 min
[32m[20221213 12:55:54 @agent_ppo2.py:139][0m 2762752 total steps have happened
[32m[20221213 12:55:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1349 --------------------------#
[32m[20221213 12:55:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:54 @agent_ppo2.py:179][0m |           0.0504 |          36.9902 |          -5.0874 |
[32m[20221213 12:55:54 @agent_ppo2.py:179][0m |           0.0199 |          34.5028 |          -3.4356 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0080 |          33.7576 |          -4.8711 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0252 |          32.9721 |          -4.9504 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0268 |          33.5085 |          -5.6697 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0301 |          32.4783 |          -6.0164 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0414 |          31.9359 |          -6.0731 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0386 |          31.7754 |          -6.9007 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0447 |          31.5350 |          -6.9264 |
[32m[20221213 12:55:55 @agent_ppo2.py:179][0m |          -0.0442 |          31.3021 |          -7.4118 |
[32m[20221213 12:55:55 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:55:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 335.52
[32m[20221213 12:55:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.51
[32m[20221213 12:55:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.14
[32m[20221213 12:55:56 @agent_ppo2.py:137][0m Total time:      39.98 min
[32m[20221213 12:55:56 @agent_ppo2.py:139][0m 2764800 total steps have happened
[32m[20221213 12:55:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1350 --------------------------#
[32m[20221213 12:55:56 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:55:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:56 @agent_ppo2.py:179][0m |           0.0442 |          37.6808 |          -3.7164 |
[32m[20221213 12:55:56 @agent_ppo2.py:179][0m |           0.0260 |          35.8860 |          -1.5989 |
[32m[20221213 12:55:56 @agent_ppo2.py:179][0m |          -0.0050 |          35.1231 |          -1.6632 |
[32m[20221213 12:55:56 @agent_ppo2.py:179][0m |          -0.0252 |          34.6157 |          -2.3629 |
[32m[20221213 12:55:57 @agent_ppo2.py:179][0m |          -0.0348 |          34.3237 |          -2.7235 |
[32m[20221213 12:55:57 @agent_ppo2.py:179][0m |          -0.0380 |          33.8708 |          -3.1988 |
[32m[20221213 12:55:57 @agent_ppo2.py:179][0m |          -0.0435 |          33.6949 |          -3.3523 |
[32m[20221213 12:55:57 @agent_ppo2.py:179][0m |          -0.0397 |          33.5164 |          -3.0566 |
[32m[20221213 12:55:57 @agent_ppo2.py:179][0m |          -0.0516 |          33.3162 |          -4.0868 |
[32m[20221213 12:55:57 @agent_ppo2.py:179][0m |          -0.0473 |          33.0808 |          -4.5906 |
[32m[20221213 12:55:57 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:55:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.49
[32m[20221213 12:55:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 339.73
[32m[20221213 12:55:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.53
[32m[20221213 12:55:57 @agent_ppo2.py:137][0m Total time:      40.01 min
[32m[20221213 12:55:57 @agent_ppo2.py:139][0m 2766848 total steps have happened
[32m[20221213 12:55:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1351 --------------------------#
[32m[20221213 12:55:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:55:58 @agent_ppo2.py:179][0m |           0.0798 |          32.6537 |          -2.3815 |
[32m[20221213 12:55:58 @agent_ppo2.py:179][0m |           0.0111 |          30.9980 |          -0.8781 |
[32m[20221213 12:55:58 @agent_ppo2.py:179][0m |          -0.0279 |          30.2685 |          -1.9472 |
[32m[20221213 12:55:58 @agent_ppo2.py:179][0m |          -0.0403 |          29.3497 |          -2.3867 |
[32m[20221213 12:55:58 @agent_ppo2.py:179][0m |          -0.0381 |          33.0192 |          -3.4115 |
[32m[20221213 12:55:58 @agent_ppo2.py:179][0m |          -0.0456 |          29.3262 |          -3.6608 |
[32m[20221213 12:55:58 @agent_ppo2.py:179][0m |          -0.0553 |          27.8893 |          -4.0620 |
[32m[20221213 12:55:59 @agent_ppo2.py:179][0m |          -0.0579 |          27.7319 |          -4.7322 |
[32m[20221213 12:55:59 @agent_ppo2.py:179][0m |          -0.0620 |          27.3776 |          -5.0185 |
[32m[20221213 12:55:59 @agent_ppo2.py:179][0m |          -0.0610 |          27.1457 |          -5.8620 |
[32m[20221213 12:55:59 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:55:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 269.09
[32m[20221213 12:55:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.46
[32m[20221213 12:55:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 376.73
[32m[20221213 12:55:59 @agent_ppo2.py:137][0m Total time:      40.04 min
[32m[20221213 12:55:59 @agent_ppo2.py:139][0m 2768896 total steps have happened
[32m[20221213 12:55:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1352 --------------------------#
[32m[20221213 12:55:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:55:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |           0.1126 |          29.5802 |          -2.1367 |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |           0.0555 |          27.4780 |          -0.3157 |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |          -0.0185 |          26.5195 |          -2.4441 |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |          -0.0327 |          26.0416 |          -3.6413 |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |          -0.0403 |          25.7183 |          -4.0290 |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |          -0.0509 |          25.3999 |          -4.6297 |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |          -0.0515 |          25.2315 |          -5.0105 |
[32m[20221213 12:56:00 @agent_ppo2.py:179][0m |          -0.0464 |          26.5737 |          -5.5541 |
[32m[20221213 12:56:01 @agent_ppo2.py:179][0m |          -0.0623 |          24.9150 |          -5.9055 |
[32m[20221213 12:56:01 @agent_ppo2.py:179][0m |          -0.0634 |          24.7441 |          -6.6977 |
[32m[20221213 12:56:01 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:56:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 222.01
[32m[20221213 12:56:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.78
[32m[20221213 12:56:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 385.67
[32m[20221213 12:56:01 @agent_ppo2.py:137][0m Total time:      40.07 min
[32m[20221213 12:56:01 @agent_ppo2.py:139][0m 2770944 total steps have happened
[32m[20221213 12:56:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1353 --------------------------#
[32m[20221213 12:56:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:01 @agent_ppo2.py:179][0m |           0.0758 |          31.0013 |          -7.0260 |
[32m[20221213 12:56:01 @agent_ppo2.py:179][0m |           0.0464 |          29.7904 |          -5.6536 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |           0.0029 |          29.5083 |          -7.0987 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |          -0.0218 |          29.0238 |          -8.3269 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |          -0.0313 |          28.9304 |          -9.2135 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |          -0.0400 |          28.6309 |         -10.4042 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |          -0.0454 |          28.4309 |         -10.8031 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |          -0.0473 |          29.2049 |         -11.9289 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |          -0.0528 |          28.2280 |         -12.7668 |
[32m[20221213 12:56:02 @agent_ppo2.py:179][0m |          -0.0511 |          28.1483 |         -13.2124 |
[32m[20221213 12:56:02 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:56:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.24
[32m[20221213 12:56:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.17
[32m[20221213 12:56:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 374.47
[32m[20221213 12:56:03 @agent_ppo2.py:137][0m Total time:      40.10 min
[32m[20221213 12:56:03 @agent_ppo2.py:139][0m 2772992 total steps have happened
[32m[20221213 12:56:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1354 --------------------------#
[32m[20221213 12:56:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:03 @agent_ppo2.py:179][0m |           0.0821 |          30.0258 |         -11.7128 |
[32m[20221213 12:56:03 @agent_ppo2.py:179][0m |           0.0516 |          28.6506 |          -6.0625 |
[32m[20221213 12:56:03 @agent_ppo2.py:179][0m |           0.0109 |          27.8841 |          -7.3836 |
[32m[20221213 12:56:03 @agent_ppo2.py:179][0m |          -0.0207 |          27.4912 |          -9.2363 |
[32m[20221213 12:56:04 @agent_ppo2.py:179][0m |          -0.0288 |          27.0561 |         -10.9449 |
[32m[20221213 12:56:04 @agent_ppo2.py:179][0m |          -0.0453 |          26.8707 |         -11.6599 |
[32m[20221213 12:56:04 @agent_ppo2.py:179][0m |          -0.0475 |          27.1359 |         -13.1549 |
[32m[20221213 12:56:04 @agent_ppo2.py:179][0m |          -0.0579 |          26.3772 |         -13.5192 |
[32m[20221213 12:56:04 @agent_ppo2.py:179][0m |          -0.0549 |          26.8186 |         -13.6474 |
[32m[20221213 12:56:04 @agent_ppo2.py:179][0m |          -0.0587 |          26.0546 |         -14.3256 |
[32m[20221213 12:56:04 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:56:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.03
[32m[20221213 12:56:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 355.32
[32m[20221213 12:56:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.49
[32m[20221213 12:56:04 @agent_ppo2.py:137][0m Total time:      40.13 min
[32m[20221213 12:56:04 @agent_ppo2.py:139][0m 2775040 total steps have happened
[32m[20221213 12:56:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1355 --------------------------#
[32m[20221213 12:56:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:05 @agent_ppo2.py:179][0m |           0.0744 |          13.8155 |          -4.4046 |
[32m[20221213 12:56:05 @agent_ppo2.py:179][0m |           0.0206 |          12.2513 |          -3.2497 |
[32m[20221213 12:56:05 @agent_ppo2.py:179][0m |           0.0451 |          11.9228 |          -2.7018 |
[32m[20221213 12:56:05 @agent_ppo2.py:179][0m |          -0.0020 |          11.8512 |          -4.7644 |
[32m[20221213 12:56:05 @agent_ppo2.py:179][0m |          -0.0202 |          11.6394 |          -5.3945 |
[32m[20221213 12:56:05 @agent_ppo2.py:179][0m |          -0.0211 |          11.5258 |          -5.8551 |
[32m[20221213 12:56:06 @agent_ppo2.py:179][0m |          -0.0223 |          11.5269 |          -6.0427 |
[32m[20221213 12:56:06 @agent_ppo2.py:179][0m |          -0.0251 |          11.4556 |          -7.0433 |
[32m[20221213 12:56:06 @agent_ppo2.py:179][0m |          -0.0265 |          11.6945 |          -7.5884 |
[32m[20221213 12:56:06 @agent_ppo2.py:179][0m |          -0.0369 |          11.3627 |          -7.7443 |
[32m[20221213 12:56:06 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:56:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 66.61
[32m[20221213 12:56:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 192.19
[32m[20221213 12:56:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.66
[32m[20221213 12:56:06 @agent_ppo2.py:137][0m Total time:      40.16 min
[32m[20221213 12:56:06 @agent_ppo2.py:139][0m 2777088 total steps have happened
[32m[20221213 12:56:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1356 --------------------------#
[32m[20221213 12:56:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |           0.1340 |          36.4226 |         -16.0610 |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |           0.0823 |          30.5331 |         -11.1971 |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |           0.0205 |          30.1441 |         -13.1041 |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |          -0.0128 |          29.6984 |         -16.7336 |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |          -0.0280 |          29.4553 |         -18.3793 |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |          -0.0402 |          29.3884 |         -19.3978 |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |          -0.0448 |          29.1208 |         -20.7996 |
[32m[20221213 12:56:07 @agent_ppo2.py:179][0m |          -0.0418 |          29.3034 |         -21.7208 |
[32m[20221213 12:56:08 @agent_ppo2.py:179][0m |          -0.0374 |          29.2812 |         -21.6456 |
[32m[20221213 12:56:08 @agent_ppo2.py:179][0m |          -0.0493 |          28.7339 |         -22.9594 |
[32m[20221213 12:56:08 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:56:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 275.02
[32m[20221213 12:56:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.56
[32m[20221213 12:56:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 370.96
[32m[20221213 12:56:08 @agent_ppo2.py:137][0m Total time:      40.19 min
[32m[20221213 12:56:08 @agent_ppo2.py:139][0m 2779136 total steps have happened
[32m[20221213 12:56:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1357 --------------------------#
[32m[20221213 12:56:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:08 @agent_ppo2.py:179][0m |           0.0895 |          29.4432 |         -21.7737 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |           0.0309 |          26.2588 |         -16.7453 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |          -0.0093 |          24.9754 |         -18.9426 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |          -0.0215 |          24.0607 |         -19.2709 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |          -0.0421 |          23.4247 |         -20.3567 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |          -0.0446 |          22.8757 |         -20.6092 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |          -0.0523 |          22.3742 |         -21.8763 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |          -0.0529 |          22.0467 |         -22.7563 |
[32m[20221213 12:56:09 @agent_ppo2.py:179][0m |          -0.0609 |          21.7481 |         -23.7894 |
[32m[20221213 12:56:10 @agent_ppo2.py:179][0m |          -0.0609 |          21.3326 |         -24.1458 |
[32m[20221213 12:56:10 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:56:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.02
[32m[20221213 12:56:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.02
[32m[20221213 12:56:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 397.77
[32m[20221213 12:56:10 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 397.77
[32m[20221213 12:56:10 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 397.77
[32m[20221213 12:56:10 @agent_ppo2.py:137][0m Total time:      40.22 min
[32m[20221213 12:56:10 @agent_ppo2.py:139][0m 2781184 total steps have happened
[32m[20221213 12:56:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1358 --------------------------#
[32m[20221213 12:56:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:10 @agent_ppo2.py:179][0m |           0.0574 |          30.7712 |         -26.7254 |
[32m[20221213 12:56:10 @agent_ppo2.py:179][0m |           0.0859 |          29.2000 |         -16.7884 |
[32m[20221213 12:56:10 @agent_ppo2.py:179][0m |           0.0141 |          28.6475 |         -19.1759 |
[32m[20221213 12:56:11 @agent_ppo2.py:179][0m |          -0.0151 |          28.1774 |         -22.7070 |
[32m[20221213 12:56:11 @agent_ppo2.py:179][0m |          -0.0283 |          28.0534 |         -23.5763 |
[32m[20221213 12:56:11 @agent_ppo2.py:179][0m |          -0.0344 |          27.6071 |         -25.7514 |
[32m[20221213 12:56:11 @agent_ppo2.py:179][0m |          -0.0240 |          27.3467 |         -24.0970 |
[32m[20221213 12:56:11 @agent_ppo2.py:179][0m |          -0.0362 |          27.1211 |         -26.3866 |
[32m[20221213 12:56:11 @agent_ppo2.py:179][0m |          -0.0408 |          27.0457 |         -27.9238 |
[32m[20221213 12:56:11 @agent_ppo2.py:179][0m |          -0.0521 |          26.8201 |         -28.8022 |
[32m[20221213 12:56:11 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:56:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 288.60
[32m[20221213 12:56:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 374.29
[32m[20221213 12:56:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.05
[32m[20221213 12:56:11 @agent_ppo2.py:137][0m Total time:      40.25 min
[32m[20221213 12:56:11 @agent_ppo2.py:139][0m 2783232 total steps have happened
[32m[20221213 12:56:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1359 --------------------------#
[32m[20221213 12:56:12 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:12 @agent_ppo2.py:179][0m |           0.0513 |          35.6929 |         -28.9202 |
[32m[20221213 12:56:12 @agent_ppo2.py:179][0m |           0.0405 |          34.1197 |         -26.6453 |
[32m[20221213 12:56:12 @agent_ppo2.py:179][0m |           0.0117 |          32.1563 |         -27.2622 |
[32m[20221213 12:56:12 @agent_ppo2.py:179][0m |          -0.0081 |          32.1324 |         -30.0595 |
[32m[20221213 12:56:12 @agent_ppo2.py:179][0m |          -0.0162 |          32.3004 |         -31.3958 |
[32m[20221213 12:56:13 @agent_ppo2.py:179][0m |          -0.0131 |          31.3008 |         -30.6718 |
[32m[20221213 12:56:13 @agent_ppo2.py:179][0m |          -0.0309 |          30.7069 |         -33.1345 |
[32m[20221213 12:56:13 @agent_ppo2.py:179][0m |          -0.0366 |          30.1674 |         -32.7460 |
[32m[20221213 12:56:13 @agent_ppo2.py:179][0m |          -0.0386 |          30.8180 |         -34.0999 |
[32m[20221213 12:56:13 @agent_ppo2.py:179][0m |          -0.0382 |          32.6140 |         -35.5296 |
[32m[20221213 12:56:13 @agent_ppo2.py:124][0m Policy update time: 1.39 s
[32m[20221213 12:56:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.85
[32m[20221213 12:56:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.99
[32m[20221213 12:56:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.57
[32m[20221213 12:56:13 @agent_ppo2.py:137][0m Total time:      40.28 min
[32m[20221213 12:56:13 @agent_ppo2.py:139][0m 2785280 total steps have happened
[32m[20221213 12:56:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1360 --------------------------#
[32m[20221213 12:56:14 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:56:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:14 @agent_ppo2.py:179][0m |           0.0577 |          37.5437 |         -25.2445 |
[32m[20221213 12:56:14 @agent_ppo2.py:179][0m |           0.0667 |          35.7095 |         -19.8864 |
[32m[20221213 12:56:14 @agent_ppo2.py:179][0m |           0.0448 |          35.8788 |         -19.4627 |
[32m[20221213 12:56:14 @agent_ppo2.py:179][0m |           0.0129 |          36.5371 |         -24.7668 |
[32m[20221213 12:56:14 @agent_ppo2.py:179][0m |          -0.0201 |          34.2053 |         -27.3801 |
[32m[20221213 12:56:14 @agent_ppo2.py:179][0m |          -0.0289 |          33.8741 |         -29.7648 |
[32m[20221213 12:56:14 @agent_ppo2.py:179][0m |          -0.0260 |          33.8817 |         -29.8185 |
[32m[20221213 12:56:15 @agent_ppo2.py:179][0m |          -0.0230 |          33.4018 |         -28.9943 |
[32m[20221213 12:56:15 @agent_ppo2.py:179][0m |          -0.0408 |          33.0962 |         -31.8808 |
[32m[20221213 12:56:15 @agent_ppo2.py:179][0m |          -0.0398 |          33.0783 |         -32.0832 |
[32m[20221213 12:56:15 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:56:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.94
[32m[20221213 12:56:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.01
[32m[20221213 12:56:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 317.83
[32m[20221213 12:56:15 @agent_ppo2.py:137][0m Total time:      40.31 min
[32m[20221213 12:56:15 @agent_ppo2.py:139][0m 2787328 total steps have happened
[32m[20221213 12:56:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1361 --------------------------#
[32m[20221213 12:56:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:15 @agent_ppo2.py:179][0m |           0.0995 |          32.8075 |         -25.6143 |
[32m[20221213 12:56:16 @agent_ppo2.py:179][0m |           0.0639 |          29.1373 |         -16.9256 |
[32m[20221213 12:56:16 @agent_ppo2.py:179][0m |           0.0127 |          27.8063 |         -19.6031 |
[32m[20221213 12:56:16 @agent_ppo2.py:179][0m |          -0.0099 |          26.5604 |         -22.8576 |
[32m[20221213 12:56:16 @agent_ppo2.py:179][0m |          -0.0342 |          25.7892 |         -24.4407 |
[32m[20221213 12:56:16 @agent_ppo2.py:179][0m |          -0.0176 |          25.2694 |         -22.8299 |
[32m[20221213 12:56:16 @agent_ppo2.py:179][0m |          -0.0293 |          25.7115 |         -25.5788 |
[32m[20221213 12:56:16 @agent_ppo2.py:179][0m |          -0.0462 |          24.8205 |         -27.1267 |
[32m[20221213 12:56:17 @agent_ppo2.py:179][0m |          -0.0331 |          23.3879 |         -27.0637 |
[32m[20221213 12:56:17 @agent_ppo2.py:179][0m |          -0.0507 |          22.9576 |         -28.7403 |
[32m[20221213 12:56:17 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:56:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 189.41
[32m[20221213 12:56:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 332.45
[32m[20221213 12:56:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 382.81
[32m[20221213 12:56:17 @agent_ppo2.py:137][0m Total time:      40.34 min
[32m[20221213 12:56:17 @agent_ppo2.py:139][0m 2789376 total steps have happened
[32m[20221213 12:56:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1362 --------------------------#
[32m[20221213 12:56:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:17 @agent_ppo2.py:179][0m |           0.0641 |          39.4672 |         -30.8894 |
[32m[20221213 12:56:17 @agent_ppo2.py:179][0m |           0.0410 |          37.2255 |         -24.5575 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |           0.0131 |          36.5040 |         -25.6758 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |          -0.0054 |          36.0604 |         -27.8178 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |          -0.0193 |          36.9313 |         -29.8929 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |          -0.0242 |          35.5539 |         -29.1347 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |          -0.0302 |          36.8623 |         -31.8278 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |          -0.0364 |          35.0935 |         -32.7217 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |          -0.0430 |          34.8135 |         -33.6512 |
[32m[20221213 12:56:18 @agent_ppo2.py:179][0m |          -0.0460 |          34.9234 |         -34.7273 |
[32m[20221213 12:56:18 @agent_ppo2.py:124][0m Policy update time: 1.27 s
[32m[20221213 12:56:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.18
[32m[20221213 12:56:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.80
[32m[20221213 12:56:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 383.74
[32m[20221213 12:56:19 @agent_ppo2.py:137][0m Total time:      40.36 min
[32m[20221213 12:56:19 @agent_ppo2.py:139][0m 2791424 total steps have happened
[32m[20221213 12:56:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1363 --------------------------#
[32m[20221213 12:56:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:19 @agent_ppo2.py:179][0m |           0.0480 |          37.8887 |         -25.9200 |
[32m[20221213 12:56:19 @agent_ppo2.py:179][0m |           0.0195 |          38.0505 |         -24.5198 |
[32m[20221213 12:56:19 @agent_ppo2.py:179][0m |          -0.0160 |          35.8127 |         -26.5027 |
[32m[20221213 12:56:19 @agent_ppo2.py:179][0m |          -0.0324 |          35.2698 |         -28.4507 |
[32m[20221213 12:56:20 @agent_ppo2.py:179][0m |          -0.0337 |          35.0957 |         -29.2809 |
[32m[20221213 12:56:20 @agent_ppo2.py:179][0m |          -0.0428 |          34.5781 |         -29.7667 |
[32m[20221213 12:56:20 @agent_ppo2.py:179][0m |          -0.0347 |          34.2975 |         -30.1401 |
[32m[20221213 12:56:20 @agent_ppo2.py:179][0m |          -0.0412 |          34.1049 |         -30.8679 |
[32m[20221213 12:56:20 @agent_ppo2.py:179][0m |          -0.0469 |          33.9051 |         -31.5287 |
[32m[20221213 12:56:20 @agent_ppo2.py:179][0m |          -0.0419 |          36.0497 |         -32.2436 |
[32m[20221213 12:56:20 @agent_ppo2.py:124][0m Policy update time: 1.41 s
[32m[20221213 12:56:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.25
[32m[20221213 12:56:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.32
[32m[20221213 12:56:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.87
[32m[20221213 12:56:20 @agent_ppo2.py:137][0m Total time:      40.39 min
[32m[20221213 12:56:20 @agent_ppo2.py:139][0m 2793472 total steps have happened
[32m[20221213 12:56:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1364 --------------------------#
[32m[20221213 12:56:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:21 @agent_ppo2.py:179][0m |           0.0811 |          37.5967 |         -26.0577 |
[32m[20221213 12:56:21 @agent_ppo2.py:179][0m |           0.0548 |          36.5055 |         -15.5001 |
[32m[20221213 12:56:21 @agent_ppo2.py:179][0m |           0.0203 |          35.9978 |         -22.0166 |
[32m[20221213 12:56:21 @agent_ppo2.py:179][0m |          -0.0032 |          35.5500 |         -25.9490 |
[32m[20221213 12:56:21 @agent_ppo2.py:179][0m |          -0.0156 |          35.2095 |         -26.6432 |
[32m[20221213 12:56:21 @agent_ppo2.py:179][0m |          -0.0161 |          38.2987 |         -27.4666 |
[32m[20221213 12:56:22 @agent_ppo2.py:179][0m |          -0.0280 |          34.8915 |         -28.8189 |
[32m[20221213 12:56:22 @agent_ppo2.py:179][0m |          -0.0345 |          34.6716 |         -29.6451 |
[32m[20221213 12:56:22 @agent_ppo2.py:179][0m |          -0.0352 |          34.5411 |         -30.4299 |
[32m[20221213 12:56:22 @agent_ppo2.py:179][0m |          -0.0331 |          35.3033 |         -32.3130 |
[32m[20221213 12:56:22 @agent_ppo2.py:124][0m Policy update time: 1.30 s
[32m[20221213 12:56:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 341.36
[32m[20221213 12:56:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.79
[32m[20221213 12:56:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 304.76
[32m[20221213 12:56:22 @agent_ppo2.py:137][0m Total time:      40.42 min
[32m[20221213 12:56:22 @agent_ppo2.py:139][0m 2795520 total steps have happened
[32m[20221213 12:56:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1365 --------------------------#
[32m[20221213 12:56:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |           0.0601 |          37.8127 |         -25.3974 |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |           0.0573 |          38.3869 |         -21.5332 |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |           0.0063 |          36.0244 |         -24.3783 |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |          -0.0087 |          35.6036 |         -27.2732 |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |          -0.0239 |          35.3790 |         -28.4326 |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |          -0.0326 |          35.6888 |         -29.4477 |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |          -0.0423 |          34.9078 |         -30.2214 |
[32m[20221213 12:56:23 @agent_ppo2.py:179][0m |          -0.0380 |          34.8015 |         -31.6998 |
[32m[20221213 12:56:24 @agent_ppo2.py:179][0m |          -0.0368 |          34.4729 |         -32.2585 |
[32m[20221213 12:56:24 @agent_ppo2.py:179][0m |          -0.0443 |          34.2758 |         -33.7699 |
[32m[20221213 12:56:24 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:56:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.56
[32m[20221213 12:56:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.80
[32m[20221213 12:56:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 366.29
[32m[20221213 12:56:24 @agent_ppo2.py:137][0m Total time:      40.45 min
[32m[20221213 12:56:24 @agent_ppo2.py:139][0m 2797568 total steps have happened
[32m[20221213 12:56:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1366 --------------------------#
[32m[20221213 12:56:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:24 @agent_ppo2.py:179][0m |           0.0870 |          37.3866 |         -26.4893 |
[32m[20221213 12:56:24 @agent_ppo2.py:179][0m |           0.0604 |          36.0578 |         -21.6006 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |           0.0244 |          35.4886 |         -24.4175 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |          -0.0059 |          35.3548 |         -26.8866 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |          -0.0079 |          35.8517 |         -26.9217 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |          -0.0259 |          34.3476 |         -28.5722 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |          -0.0298 |          34.1779 |         -29.4676 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |          -0.0357 |          33.9476 |         -30.6492 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |          -0.0352 |          33.8218 |         -32.3733 |
[32m[20221213 12:56:25 @agent_ppo2.py:179][0m |          -0.0368 |          34.6452 |         -32.7015 |
[32m[20221213 12:56:25 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:56:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 327.47
[32m[20221213 12:56:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.08
[32m[20221213 12:56:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.32
[32m[20221213 12:56:26 @agent_ppo2.py:137][0m Total time:      40.48 min
[32m[20221213 12:56:26 @agent_ppo2.py:139][0m 2799616 total steps have happened
[32m[20221213 12:56:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1367 --------------------------#
[32m[20221213 12:56:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:26 @agent_ppo2.py:179][0m |           0.0751 |          37.3941 |         -24.7580 |
[32m[20221213 12:56:26 @agent_ppo2.py:179][0m |           0.0965 |          35.8661 |         -16.7605 |
[32m[20221213 12:56:26 @agent_ppo2.py:179][0m |           0.0582 |          34.4125 |         -16.3892 |
[32m[20221213 12:56:27 @agent_ppo2.py:179][0m |           0.0028 |          33.8378 |         -20.9281 |
[32m[20221213 12:56:27 @agent_ppo2.py:179][0m |          -0.0114 |          33.4567 |         -22.4942 |
[32m[20221213 12:56:27 @agent_ppo2.py:179][0m |          -0.0185 |          33.2167 |         -24.8660 |
[32m[20221213 12:56:27 @agent_ppo2.py:179][0m |          -0.0230 |          32.7518 |         -25.7682 |
[32m[20221213 12:56:27 @agent_ppo2.py:179][0m |          -0.0354 |          32.4870 |         -26.8312 |
[32m[20221213 12:56:27 @agent_ppo2.py:179][0m |          -0.0374 |          32.1995 |         -27.2982 |
[32m[20221213 12:56:27 @agent_ppo2.py:179][0m |          -0.0292 |          32.3942 |         -28.4837 |
[32m[20221213 12:56:27 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:56:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 280.91
[32m[20221213 12:56:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.57
[32m[20221213 12:56:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 390.58
[32m[20221213 12:56:27 @agent_ppo2.py:137][0m Total time:      40.51 min
[32m[20221213 12:56:27 @agent_ppo2.py:139][0m 2801664 total steps have happened
[32m[20221213 12:56:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1368 --------------------------#
[32m[20221213 12:56:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:28 @agent_ppo2.py:179][0m |           0.0607 |          36.5849 |         -26.2916 |
[32m[20221213 12:56:28 @agent_ppo2.py:179][0m |           0.0413 |          34.0214 |         -24.3858 |
[32m[20221213 12:56:28 @agent_ppo2.py:179][0m |           0.0068 |          33.1777 |         -24.3285 |
[32m[20221213 12:56:28 @agent_ppo2.py:179][0m |          -0.0092 |          33.6717 |         -27.5781 |
[32m[20221213 12:56:28 @agent_ppo2.py:179][0m |          -0.0162 |          32.4476 |         -26.6909 |
[32m[20221213 12:56:29 @agent_ppo2.py:179][0m |          -0.0300 |          31.9685 |         -28.5070 |
[32m[20221213 12:56:29 @agent_ppo2.py:179][0m |          -0.0252 |          35.4783 |         -30.5943 |
[32m[20221213 12:56:29 @agent_ppo2.py:179][0m |          -0.0406 |          31.6169 |         -32.3563 |
[32m[20221213 12:56:29 @agent_ppo2.py:179][0m |          -0.0384 |          31.1660 |         -31.9182 |
[32m[20221213 12:56:29 @agent_ppo2.py:179][0m |          -0.0364 |          32.8461 |         -33.7143 |
[32m[20221213 12:56:29 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:56:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 341.88
[32m[20221213 12:56:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.34
[32m[20221213 12:56:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 368.79
[32m[20221213 12:56:29 @agent_ppo2.py:137][0m Total time:      40.54 min
[32m[20221213 12:56:29 @agent_ppo2.py:139][0m 2803712 total steps have happened
[32m[20221213 12:56:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1369 --------------------------#
[32m[20221213 12:56:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:30 @agent_ppo2.py:179][0m |           0.0524 |          35.2898 |         -26.5004 |
[32m[20221213 12:56:30 @agent_ppo2.py:179][0m |           0.0454 |          32.7504 |         -16.4974 |
[32m[20221213 12:56:30 @agent_ppo2.py:179][0m |           0.0089 |          31.8357 |         -23.5127 |
[32m[20221213 12:56:30 @agent_ppo2.py:179][0m |           0.0001 |          31.2470 |         -24.7905 |
[32m[20221213 12:56:30 @agent_ppo2.py:179][0m |          -0.0054 |          30.8856 |         -24.9538 |
[32m[20221213 12:56:30 @agent_ppo2.py:179][0m |          -0.0146 |          34.5397 |         -28.5803 |
[32m[20221213 12:56:30 @agent_ppo2.py:179][0m |          -0.0302 |          30.4505 |         -30.4783 |
[32m[20221213 12:56:31 @agent_ppo2.py:179][0m |          -0.0366 |          30.0440 |         -32.2446 |
[32m[20221213 12:56:31 @agent_ppo2.py:179][0m |          -0.0416 |          29.8431 |         -33.8774 |
[32m[20221213 12:56:31 @agent_ppo2.py:179][0m |          -0.0392 |          29.6061 |         -33.9601 |
[32m[20221213 12:56:31 @agent_ppo2.py:124][0m Policy update time: 1.37 s
[32m[20221213 12:56:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.38
[32m[20221213 12:56:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.45
[32m[20221213 12:56:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 37.71
[32m[20221213 12:56:31 @agent_ppo2.py:137][0m Total time:      40.57 min
[32m[20221213 12:56:31 @agent_ppo2.py:139][0m 2805760 total steps have happened
[32m[20221213 12:56:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1370 --------------------------#
[32m[20221213 12:56:31 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:56:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:31 @agent_ppo2.py:179][0m |           0.0738 |          36.3691 |         -27.5860 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |           0.0742 |          35.0794 |         -19.1941 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |           0.0309 |          34.3855 |         -24.2156 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |          -0.0040 |          33.9544 |         -29.6764 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |          -0.0156 |          33.4778 |         -32.0795 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |          -0.0255 |          33.0570 |         -33.9626 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |          -0.0303 |          32.8816 |         -34.3275 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |          -0.0327 |          32.4776 |         -35.0954 |
[32m[20221213 12:56:32 @agent_ppo2.py:179][0m |          -0.0295 |          32.3088 |         -36.6988 |
[32m[20221213 12:56:33 @agent_ppo2.py:179][0m |          -0.0385 |          32.0976 |         -37.5969 |
[32m[20221213 12:56:33 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:56:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.16
[32m[20221213 12:56:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.26
[32m[20221213 12:56:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.54
[32m[20221213 12:56:33 @agent_ppo2.py:137][0m Total time:      40.60 min
[32m[20221213 12:56:33 @agent_ppo2.py:139][0m 2807808 total steps have happened
[32m[20221213 12:56:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1371 --------------------------#
[32m[20221213 12:56:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:33 @agent_ppo2.py:179][0m |           0.0600 |          36.0717 |         -28.6855 |
[32m[20221213 12:56:33 @agent_ppo2.py:179][0m |           0.0539 |          34.9275 |         -19.8804 |
[32m[20221213 12:56:33 @agent_ppo2.py:179][0m |           0.0035 |          34.4432 |         -26.5323 |
[32m[20221213 12:56:34 @agent_ppo2.py:179][0m |          -0.0103 |          35.1203 |         -28.8450 |
[32m[20221213 12:56:34 @agent_ppo2.py:179][0m |          -0.0263 |          34.0813 |         -30.3514 |
[32m[20221213 12:56:34 @agent_ppo2.py:179][0m |          -0.0267 |          33.9579 |         -31.7934 |
[32m[20221213 12:56:34 @agent_ppo2.py:179][0m |          -0.0319 |          33.6842 |         -32.0746 |
[32m[20221213 12:56:34 @agent_ppo2.py:179][0m |          -0.0366 |          33.5845 |         -33.7829 |
[32m[20221213 12:56:34 @agent_ppo2.py:179][0m |          -0.0277 |          35.9133 |         -33.9792 |
[32m[20221213 12:56:34 @agent_ppo2.py:179][0m |          -0.0408 |          33.4030 |         -34.9457 |
[32m[20221213 12:56:34 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:56:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.57
[32m[20221213 12:56:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.03
[32m[20221213 12:56:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.29
[32m[20221213 12:56:35 @agent_ppo2.py:137][0m Total time:      40.63 min
[32m[20221213 12:56:35 @agent_ppo2.py:139][0m 2809856 total steps have happened
[32m[20221213 12:56:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1372 --------------------------#
[32m[20221213 12:56:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:35 @agent_ppo2.py:179][0m |           0.0674 |          36.5628 |         -27.6722 |
[32m[20221213 12:56:35 @agent_ppo2.py:179][0m |           0.0742 |          34.1679 |         -17.0422 |
[32m[20221213 12:56:35 @agent_ppo2.py:179][0m |           0.0201 |          33.6162 |         -22.5878 |
[32m[20221213 12:56:35 @agent_ppo2.py:179][0m |           0.0022 |          33.1881 |         -24.0612 |
[32m[20221213 12:56:35 @agent_ppo2.py:179][0m |          -0.0154 |          33.2100 |         -27.4121 |
[32m[20221213 12:56:36 @agent_ppo2.py:179][0m |          -0.0307 |          32.7501 |         -28.0013 |
[32m[20221213 12:56:36 @agent_ppo2.py:179][0m |          -0.0347 |          32.5521 |         -29.6288 |
[32m[20221213 12:56:36 @agent_ppo2.py:179][0m |          -0.0376 |          32.4003 |         -30.3683 |
[32m[20221213 12:56:36 @agent_ppo2.py:179][0m |          -0.0401 |          32.1472 |         -31.1815 |
[32m[20221213 12:56:36 @agent_ppo2.py:179][0m |          -0.0405 |          32.0429 |         -32.4263 |
[32m[20221213 12:56:36 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:56:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.79
[32m[20221213 12:56:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.09
[32m[20221213 12:56:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 74.85
[32m[20221213 12:56:36 @agent_ppo2.py:137][0m Total time:      40.66 min
[32m[20221213 12:56:36 @agent_ppo2.py:139][0m 2811904 total steps have happened
[32m[20221213 12:56:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1373 --------------------------#
[32m[20221213 12:56:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:37 @agent_ppo2.py:179][0m |           0.0719 |          25.5105 |         -23.7987 |
[32m[20221213 12:56:37 @agent_ppo2.py:179][0m |           0.1066 |          24.2854 |         -14.9121 |
[32m[20221213 12:56:37 @agent_ppo2.py:179][0m |           0.0510 |          23.4336 |         -13.7953 |
[32m[20221213 12:56:37 @agent_ppo2.py:179][0m |           0.0021 |          23.0755 |         -13.9282 |
[32m[20221213 12:56:37 @agent_ppo2.py:179][0m |          -0.0153 |          22.8773 |         -16.0352 |
[32m[20221213 12:56:37 @agent_ppo2.py:179][0m |          -0.0309 |          22.6672 |         -17.2360 |
[32m[20221213 12:56:37 @agent_ppo2.py:179][0m |          -0.0343 |          22.5369 |         -18.3948 |
[32m[20221213 12:56:38 @agent_ppo2.py:179][0m |          -0.0412 |          22.3934 |         -19.2014 |
[32m[20221213 12:56:38 @agent_ppo2.py:179][0m |          -0.0433 |          22.3651 |         -19.7314 |
[32m[20221213 12:56:38 @agent_ppo2.py:179][0m |          -0.0488 |          22.2563 |         -20.7632 |
[32m[20221213 12:56:38 @agent_ppo2.py:124][0m Policy update time: 1.28 s
[32m[20221213 12:56:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 204.66
[32m[20221213 12:56:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 324.69
[32m[20221213 12:56:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 335.34
[32m[20221213 12:56:38 @agent_ppo2.py:137][0m Total time:      40.69 min
[32m[20221213 12:56:38 @agent_ppo2.py:139][0m 2813952 total steps have happened
[32m[20221213 12:56:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1374 --------------------------#
[32m[20221213 12:56:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:38 @agent_ppo2.py:179][0m |           0.0719 |          38.0924 |         -24.1179 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |           0.0717 |          36.5087 |         -15.8582 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |           0.0126 |          35.8193 |         -21.6647 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |          -0.0018 |          36.0239 |         -24.7926 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |          -0.0192 |          35.3098 |         -25.4091 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |          -0.0252 |          34.7634 |         -26.9394 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |          -0.0206 |          34.8112 |         -27.2819 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |          -0.0147 |          38.9146 |         -27.0298 |
[32m[20221213 12:56:39 @agent_ppo2.py:179][0m |          -0.0316 |          34.3692 |         -28.2740 |
[32m[20221213 12:56:40 @agent_ppo2.py:179][0m |          -0.0347 |          34.1164 |         -29.5873 |
[32m[20221213 12:56:40 @agent_ppo2.py:124][0m Policy update time: 1.32 s
[32m[20221213 12:56:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.01
[32m[20221213 12:56:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.60
[32m[20221213 12:56:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.99
[32m[20221213 12:56:40 @agent_ppo2.py:137][0m Total time:      40.72 min
[32m[20221213 12:56:40 @agent_ppo2.py:139][0m 2816000 total steps have happened
[32m[20221213 12:56:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1375 --------------------------#
[32m[20221213 12:56:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:40 @agent_ppo2.py:179][0m |           0.0552 |          36.9297 |         -22.4298 |
[32m[20221213 12:56:40 @agent_ppo2.py:179][0m |           0.0527 |          35.7178 |         -15.6962 |
[32m[20221213 12:56:40 @agent_ppo2.py:179][0m |           0.0219 |          35.0286 |         -20.3797 |
[32m[20221213 12:56:41 @agent_ppo2.py:179][0m |          -0.0087 |          34.6233 |         -24.5260 |
[32m[20221213 12:56:41 @agent_ppo2.py:179][0m |          -0.0168 |          34.4035 |         -26.4519 |
[32m[20221213 12:56:41 @agent_ppo2.py:179][0m |          -0.0198 |          35.8205 |         -27.8258 |
[32m[20221213 12:56:41 @agent_ppo2.py:179][0m |          -0.0304 |          33.8862 |         -27.5201 |
[32m[20221213 12:56:41 @agent_ppo2.py:179][0m |          -0.0343 |          33.7483 |         -28.9862 |
[32m[20221213 12:56:41 @agent_ppo2.py:179][0m |          -0.0322 |          33.7054 |         -29.4182 |
[32m[20221213 12:56:41 @agent_ppo2.py:179][0m |          -0.0363 |          33.4165 |         -29.9225 |
[32m[20221213 12:56:41 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:56:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.92
[32m[20221213 12:56:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.01
[32m[20221213 12:56:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 372.61
[32m[20221213 12:56:42 @agent_ppo2.py:137][0m Total time:      40.75 min
[32m[20221213 12:56:42 @agent_ppo2.py:139][0m 2818048 total steps have happened
[32m[20221213 12:56:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1376 --------------------------#
[32m[20221213 12:56:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:42 @agent_ppo2.py:179][0m |           0.0523 |          31.0402 |         -24.6457 |
[32m[20221213 12:56:42 @agent_ppo2.py:179][0m |           0.0844 |          36.4274 |         -18.6104 |
[32m[20221213 12:56:42 @agent_ppo2.py:179][0m |           0.0236 |          28.7766 |         -19.6915 |
[32m[20221213 12:56:42 @agent_ppo2.py:179][0m |          -0.0043 |          27.6164 |         -19.4639 |
[32m[20221213 12:56:42 @agent_ppo2.py:179][0m |          -0.0319 |          27.2295 |         -22.0871 |
[32m[20221213 12:56:43 @agent_ppo2.py:179][0m |          -0.0360 |          26.8051 |         -22.8517 |
[32m[20221213 12:56:43 @agent_ppo2.py:179][0m |          -0.0444 |          26.4382 |         -23.6568 |
[32m[20221213 12:56:43 @agent_ppo2.py:179][0m |          -0.0379 |          26.3343 |         -25.2802 |
[32m[20221213 12:56:43 @agent_ppo2.py:179][0m |          -0.0516 |          25.9975 |         -26.2342 |
[32m[20221213 12:56:43 @agent_ppo2.py:179][0m |          -0.0540 |          25.8206 |         -26.7954 |
[32m[20221213 12:56:43 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:56:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.92
[32m[20221213 12:56:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.51
[32m[20221213 12:56:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 36.40
[32m[20221213 12:56:43 @agent_ppo2.py:137][0m Total time:      40.78 min
[32m[20221213 12:56:43 @agent_ppo2.py:139][0m 2820096 total steps have happened
[32m[20221213 12:56:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1377 --------------------------#
[32m[20221213 12:56:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:44 @agent_ppo2.py:179][0m |           0.0802 |          32.5174 |         -25.6688 |
[32m[20221213 12:56:44 @agent_ppo2.py:179][0m |           0.0505 |          30.9361 |         -22.0898 |
[32m[20221213 12:56:44 @agent_ppo2.py:179][0m |           0.0131 |          30.5846 |         -23.5596 |
[32m[20221213 12:56:44 @agent_ppo2.py:179][0m |          -0.0119 |          30.3480 |         -24.9037 |
[32m[20221213 12:56:44 @agent_ppo2.py:179][0m |          -0.0264 |          30.0829 |         -27.1809 |
[32m[20221213 12:56:44 @agent_ppo2.py:179][0m |          -0.0345 |          30.0465 |         -28.7581 |
[32m[20221213 12:56:44 @agent_ppo2.py:179][0m |          -0.0241 |          30.8433 |         -29.4964 |
[32m[20221213 12:56:45 @agent_ppo2.py:179][0m |          -0.0355 |          29.7412 |         -30.1770 |
[32m[20221213 12:56:45 @agent_ppo2.py:179][0m |          -0.0214 |          34.3981 |         -30.3085 |
[32m[20221213 12:56:45 @agent_ppo2.py:179][0m |          -0.0337 |          33.8740 |         -31.6999 |
[32m[20221213 12:56:45 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:56:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 275.01
[32m[20221213 12:56:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.81
[32m[20221213 12:56:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 391.49
[32m[20221213 12:56:45 @agent_ppo2.py:137][0m Total time:      40.81 min
[32m[20221213 12:56:45 @agent_ppo2.py:139][0m 2822144 total steps have happened
[32m[20221213 12:56:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1378 --------------------------#
[32m[20221213 12:56:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |           0.0810 |          32.3173 |         -25.9788 |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |           0.0668 |          31.1895 |         -22.0065 |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |           0.0266 |          30.8563 |         -24.0708 |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |           0.0036 |          30.7393 |         -25.8116 |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |          -0.0153 |          30.5026 |         -26.9475 |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |          -0.0232 |          30.3931 |         -27.4393 |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |          -0.0330 |          30.2625 |         -28.7563 |
[32m[20221213 12:56:46 @agent_ppo2.py:179][0m |          -0.0319 |          30.2432 |         -28.6271 |
[32m[20221213 12:56:47 @agent_ppo2.py:179][0m |          -0.0395 |          30.1608 |         -29.4451 |
[32m[20221213 12:56:47 @agent_ppo2.py:179][0m |          -0.0406 |          29.9945 |         -32.1405 |
[32m[20221213 12:56:47 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:56:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.33
[32m[20221213 12:56:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.16
[32m[20221213 12:56:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 369.86
[32m[20221213 12:56:47 @agent_ppo2.py:137][0m Total time:      40.84 min
[32m[20221213 12:56:47 @agent_ppo2.py:139][0m 2824192 total steps have happened
[32m[20221213 12:56:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1379 --------------------------#
[32m[20221213 12:56:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:47 @agent_ppo2.py:179][0m |           0.0764 |          38.3663 |         -30.0637 |
[32m[20221213 12:56:47 @agent_ppo2.py:179][0m |           0.0604 |          37.8145 |         -18.0021 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |           0.0153 |          36.9472 |         -24.2446 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |           0.0040 |          37.3134 |         -26.2375 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |          -0.0159 |          36.1899 |         -29.4065 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |          -0.0206 |          35.9496 |         -29.0895 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |          -0.0202 |          36.2962 |         -31.1025 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |          -0.0306 |          35.6079 |         -31.9929 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |          -0.0133 |          42.1312 |         -33.8959 |
[32m[20221213 12:56:48 @agent_ppo2.py:179][0m |          -0.0360 |          35.6441 |         -33.9979 |
[32m[20221213 12:56:48 @agent_ppo2.py:124][0m Policy update time: 1.40 s
[32m[20221213 12:56:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.81
[32m[20221213 12:56:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.90
[32m[20221213 12:56:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.91
[32m[20221213 12:56:49 @agent_ppo2.py:137][0m Total time:      40.87 min
[32m[20221213 12:56:49 @agent_ppo2.py:139][0m 2826240 total steps have happened
[32m[20221213 12:56:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1380 --------------------------#
[32m[20221213 12:56:49 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:56:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:49 @agent_ppo2.py:179][0m |           0.0680 |          37.7459 |         -31.8598 |
[32m[20221213 12:56:49 @agent_ppo2.py:179][0m |           0.0427 |          35.5047 |         -27.3124 |
[32m[20221213 12:56:49 @agent_ppo2.py:179][0m |           0.0059 |          34.7722 |         -31.2043 |
[32m[20221213 12:56:49 @agent_ppo2.py:179][0m |          -0.0077 |          34.1851 |         -30.8477 |
[32m[20221213 12:56:50 @agent_ppo2.py:179][0m |          -0.0177 |          33.8790 |         -32.6195 |
[32m[20221213 12:56:50 @agent_ppo2.py:179][0m |          -0.0235 |          33.5037 |         -32.3749 |
[32m[20221213 12:56:50 @agent_ppo2.py:179][0m |          -0.0256 |          37.2884 |         -35.3117 |
[32m[20221213 12:56:50 @agent_ppo2.py:179][0m |          -0.0245 |          37.0766 |         -35.9187 |
[32m[20221213 12:56:50 @agent_ppo2.py:179][0m |          -0.0386 |          32.8958 |         -36.8499 |
[32m[20221213 12:56:50 @agent_ppo2.py:179][0m |          -0.0373 |          33.3625 |         -36.6529 |
[32m[20221213 12:56:50 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:56:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 264.62
[32m[20221213 12:56:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.03
[32m[20221213 12:56:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 328.03
[32m[20221213 12:56:50 @agent_ppo2.py:137][0m Total time:      40.89 min
[32m[20221213 12:56:50 @agent_ppo2.py:139][0m 2828288 total steps have happened
[32m[20221213 12:56:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1381 --------------------------#
[32m[20221213 12:56:51 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:51 @agent_ppo2.py:179][0m |           0.0577 |          37.6051 |         -28.0137 |
[32m[20221213 12:56:51 @agent_ppo2.py:179][0m |           0.0617 |          36.6306 |         -24.6519 |
[32m[20221213 12:56:51 @agent_ppo2.py:179][0m |           0.0171 |          36.1536 |         -28.4580 |
[32m[20221213 12:56:51 @agent_ppo2.py:179][0m |           0.0039 |          38.4588 |         -31.0165 |
[32m[20221213 12:56:51 @agent_ppo2.py:179][0m |          -0.0237 |          35.9700 |         -33.7560 |
[32m[20221213 12:56:51 @agent_ppo2.py:179][0m |          -0.0351 |          35.4277 |         -35.6858 |
[32m[20221213 12:56:52 @agent_ppo2.py:179][0m |          -0.0366 |          35.2708 |         -36.7951 |
[32m[20221213 12:56:52 @agent_ppo2.py:179][0m |          -0.0402 |          35.1374 |         -37.4911 |
[32m[20221213 12:56:52 @agent_ppo2.py:179][0m |          -0.0413 |          34.9498 |         -37.8464 |
[32m[20221213 12:56:52 @agent_ppo2.py:179][0m |          -0.0372 |          34.8657 |         -37.8416 |
[32m[20221213 12:56:52 @agent_ppo2.py:124][0m Policy update time: 1.36 s
[32m[20221213 12:56:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.40
[32m[20221213 12:56:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.24
[32m[20221213 12:56:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.57
[32m[20221213 12:56:52 @agent_ppo2.py:137][0m Total time:      40.92 min
[32m[20221213 12:56:52 @agent_ppo2.py:139][0m 2830336 total steps have happened
[32m[20221213 12:56:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1382 --------------------------#
[32m[20221213 12:56:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |           0.0682 |          26.7033 |         -30.2862 |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |           0.0740 |          24.4865 |         -21.4137 |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |           0.0768 |          24.0484 |         -13.2213 |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |           0.0126 |          22.6976 |         -19.2296 |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |          -0.0190 |          22.1953 |         -22.2881 |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |          -0.0275 |          21.7961 |         -24.3295 |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |          -0.0262 |          22.3402 |         -25.7304 |
[32m[20221213 12:56:53 @agent_ppo2.py:179][0m |          -0.0339 |          21.1531 |         -28.0373 |
[32m[20221213 12:56:54 @agent_ppo2.py:179][0m |          -0.0398 |          20.7168 |         -28.1442 |
[32m[20221213 12:56:54 @agent_ppo2.py:179][0m |          -0.0457 |          20.4960 |         -29.3304 |
[32m[20221213 12:56:54 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:56:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 162.04
[32m[20221213 12:56:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.50
[32m[20221213 12:56:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.16
[32m[20221213 12:56:54 @agent_ppo2.py:137][0m Total time:      40.95 min
[32m[20221213 12:56:54 @agent_ppo2.py:139][0m 2832384 total steps have happened
[32m[20221213 12:56:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1383 --------------------------#
[32m[20221213 12:56:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:54 @agent_ppo2.py:179][0m |           0.0554 |          21.2981 |         -21.9328 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |           0.0045 |          19.8901 |         -16.4455 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |          -0.0227 |          18.9420 |         -19.0535 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |          -0.0375 |          18.5020 |         -20.2976 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |          -0.0400 |          17.9312 |         -20.7944 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |          -0.0453 |          17.5220 |         -21.8513 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |          -0.0552 |          17.2029 |         -22.5711 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |          -0.0534 |          18.0095 |         -23.0780 |
[32m[20221213 12:56:55 @agent_ppo2.py:179][0m |          -0.0570 |          16.8197 |         -24.2837 |
[32m[20221213 12:56:56 @agent_ppo2.py:179][0m |          -0.0668 |          16.5224 |         -23.9755 |
[32m[20221213 12:56:56 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:56:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.69
[32m[20221213 12:56:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.91
[32m[20221213 12:56:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 366.08
[32m[20221213 12:56:56 @agent_ppo2.py:137][0m Total time:      40.98 min
[32m[20221213 12:56:56 @agent_ppo2.py:139][0m 2834432 total steps have happened
[32m[20221213 12:56:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1384 --------------------------#
[32m[20221213 12:56:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:56:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:56 @agent_ppo2.py:179][0m |           0.1294 |          39.1851 |         -30.2300 |
[32m[20221213 12:56:56 @agent_ppo2.py:179][0m |           0.1513 |          37.3827 |          -9.8334 |
[32m[20221213 12:56:56 @agent_ppo2.py:179][0m |           0.0938 |          36.3670 |         -11.8300 |
[32m[20221213 12:56:57 @agent_ppo2.py:179][0m |           0.0594 |          35.7557 |         -19.7083 |
[32m[20221213 12:56:57 @agent_ppo2.py:179][0m |           0.0488 |          35.5542 |         -25.1384 |
[32m[20221213 12:56:57 @agent_ppo2.py:179][0m |           0.0265 |          35.2224 |         -29.9687 |
[32m[20221213 12:56:57 @agent_ppo2.py:179][0m |           0.0026 |          35.0031 |         -33.0674 |
[32m[20221213 12:56:57 @agent_ppo2.py:179][0m |          -0.0031 |          34.7078 |         -35.1605 |
[32m[20221213 12:56:57 @agent_ppo2.py:179][0m |          -0.0069 |          36.1924 |         -36.1881 |
[32m[20221213 12:56:57 @agent_ppo2.py:179][0m |          -0.0133 |          35.5927 |         -38.3325 |
[32m[20221213 12:56:57 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:56:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 373.42
[32m[20221213 12:56:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 379.22
[32m[20221213 12:56:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 353.38
[32m[20221213 12:56:58 @agent_ppo2.py:137][0m Total time:      41.01 min
[32m[20221213 12:56:58 @agent_ppo2.py:139][0m 2836480 total steps have happened
[32m[20221213 12:56:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1385 --------------------------#
[32m[20221213 12:56:58 @agent_ppo2.py:121][0m Sampling time: 0.22 s by 5 slaves
[32m[20221213 12:56:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:56:58 @agent_ppo2.py:179][0m |           0.0719 |          21.5306 |         -19.5691 |
[32m[20221213 12:56:58 @agent_ppo2.py:179][0m |           0.0110 |          19.4870 |         -13.2885 |
[32m[20221213 12:56:58 @agent_ppo2.py:179][0m |          -0.0218 |          19.1214 |         -15.2997 |
[32m[20221213 12:56:58 @agent_ppo2.py:179][0m |          -0.0408 |          18.8012 |         -16.8090 |
[32m[20221213 12:56:58 @agent_ppo2.py:179][0m |          -0.0482 |          18.6360 |         -18.0761 |
[32m[20221213 12:56:59 @agent_ppo2.py:179][0m |          -0.0585 |          18.5567 |         -18.8424 |
[32m[20221213 12:56:59 @agent_ppo2.py:179][0m |          -0.0470 |          18.4016 |         -18.6387 |
[32m[20221213 12:56:59 @agent_ppo2.py:179][0m |          -0.0476 |          18.8655 |         -19.6055 |
[32m[20221213 12:56:59 @agent_ppo2.py:179][0m |          -0.0603 |          18.2805 |         -20.7490 |
[32m[20221213 12:56:59 @agent_ppo2.py:179][0m |          -0.0654 |          18.1289 |         -21.6724 |
[32m[20221213 12:56:59 @agent_ppo2.py:124][0m Policy update time: 1.31 s
[32m[20221213 12:56:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 163.34
[32m[20221213 12:56:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 370.06
[32m[20221213 12:56:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.23
[32m[20221213 12:56:59 @agent_ppo2.py:137][0m Total time:      41.04 min
[32m[20221213 12:56:59 @agent_ppo2.py:139][0m 2838528 total steps have happened
[32m[20221213 12:56:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1386 --------------------------#
[32m[20221213 12:56:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:00 @agent_ppo2.py:179][0m |           0.0729 |          39.3747 |         -34.7701 |
[32m[20221213 12:57:00 @agent_ppo2.py:179][0m |           0.0537 |          37.9389 |         -26.6401 |
[32m[20221213 12:57:00 @agent_ppo2.py:179][0m |           0.0384 |          37.3804 |         -27.3266 |
[32m[20221213 12:57:00 @agent_ppo2.py:179][0m |          -0.0006 |          37.0770 |         -34.1918 |
[32m[20221213 12:57:00 @agent_ppo2.py:179][0m |          -0.0127 |          36.7648 |         -36.6280 |
[32m[20221213 12:57:00 @agent_ppo2.py:179][0m |          -0.0223 |          36.6505 |         -38.0041 |
[32m[20221213 12:57:00 @agent_ppo2.py:179][0m |          -0.0283 |          36.3904 |         -39.5493 |
[32m[20221213 12:57:01 @agent_ppo2.py:179][0m |          -0.0280 |          36.4585 |         -40.1369 |
[32m[20221213 12:57:01 @agent_ppo2.py:179][0m |          -0.0267 |          36.9308 |         -40.0951 |
[32m[20221213 12:57:01 @agent_ppo2.py:179][0m |          -0.0389 |          36.0882 |         -42.5465 |
[32m[20221213 12:57:01 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:57:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 365.78
[32m[20221213 12:57:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 368.85
[32m[20221213 12:57:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.92
[32m[20221213 12:57:01 @agent_ppo2.py:137][0m Total time:      41.07 min
[32m[20221213 12:57:01 @agent_ppo2.py:139][0m 2840576 total steps have happened
[32m[20221213 12:57:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1387 --------------------------#
[32m[20221213 12:57:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:01 @agent_ppo2.py:179][0m |           0.1023 |          36.7960 |         -28.8435 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |           0.0903 |          35.9303 |         -15.2197 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |           0.0553 |          35.4613 |         -18.8561 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |           0.0335 |          35.5267 |         -21.2003 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |           0.0034 |          35.0910 |         -28.4140 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |          -0.0037 |          35.0401 |         -30.0988 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |          -0.0127 |          34.8512 |         -31.1433 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |          -0.0093 |          36.7332 |         -33.1162 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |          -0.0154 |          34.6876 |         -32.9978 |
[32m[20221213 12:57:02 @agent_ppo2.py:179][0m |          -0.0197 |          35.3910 |         -35.5521 |
[32m[20221213 12:57:02 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:57:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 351.19
[32m[20221213 12:57:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.58
[32m[20221213 12:57:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.36
[32m[20221213 12:57:03 @agent_ppo2.py:137][0m Total time:      41.10 min
[32m[20221213 12:57:03 @agent_ppo2.py:139][0m 2842624 total steps have happened
[32m[20221213 12:57:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1388 --------------------------#
[32m[20221213 12:57:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:03 @agent_ppo2.py:179][0m |           0.1524 |          38.5011 |         -21.0727 |
[32m[20221213 12:57:03 @agent_ppo2.py:179][0m |           0.1078 |          37.2718 |          -6.7868 |
[32m[20221213 12:57:03 @agent_ppo2.py:179][0m |           0.0591 |          36.9023 |         -11.2511 |
[32m[20221213 12:57:03 @agent_ppo2.py:179][0m |           0.0410 |          36.6540 |         -15.1892 |
[32m[20221213 12:57:04 @agent_ppo2.py:179][0m |           0.0146 |          36.4761 |         -21.1057 |
[32m[20221213 12:57:04 @agent_ppo2.py:179][0m |           0.0017 |          36.3893 |         -25.0634 |
[32m[20221213 12:57:04 @agent_ppo2.py:179][0m |          -0.0005 |          36.1487 |         -25.2442 |
[32m[20221213 12:57:04 @agent_ppo2.py:179][0m |          -0.0109 |          36.0477 |         -27.5016 |
[32m[20221213 12:57:04 @agent_ppo2.py:179][0m |          -0.0247 |          36.0515 |         -28.9784 |
[32m[20221213 12:57:04 @agent_ppo2.py:179][0m |          -0.0175 |          41.0862 |         -30.8615 |
[32m[20221213 12:57:04 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 355.03
[32m[20221213 12:57:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 372.65
[32m[20221213 12:57:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 352.00
[32m[20221213 12:57:04 @agent_ppo2.py:137][0m Total time:      41.13 min
[32m[20221213 12:57:04 @agent_ppo2.py:139][0m 2844672 total steps have happened
[32m[20221213 12:57:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1389 --------------------------#
[32m[20221213 12:57:04 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:57:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |           0.0593 |          25.3573 |         -20.3171 |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |          -0.0039 |          23.4972 |         -14.9521 |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |          -0.0202 |          24.9034 |         -17.3120 |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |          -0.0378 |          21.9691 |         -18.2695 |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |          -0.0450 |          21.4033 |         -19.9595 |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |          -0.0524 |          21.0458 |         -21.0817 |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |          -0.0516 |          20.7667 |         -21.8791 |
[32m[20221213 12:57:05 @agent_ppo2.py:179][0m |          -0.0389 |          23.8133 |         -22.0280 |
[32m[20221213 12:57:06 @agent_ppo2.py:179][0m |          -0.0471 |          20.2661 |         -22.0690 |
[32m[20221213 12:57:06 @agent_ppo2.py:179][0m |          -0.0442 |          20.8816 |         -23.0842 |
[32m[20221213 12:57:06 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:57:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 218.89
[32m[20221213 12:57:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.28
[32m[20221213 12:57:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 42.12
[32m[20221213 12:57:06 @agent_ppo2.py:137][0m Total time:      41.15 min
[32m[20221213 12:57:06 @agent_ppo2.py:139][0m 2846720 total steps have happened
[32m[20221213 12:57:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1390 --------------------------#
[32m[20221213 12:57:06 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:57:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:06 @agent_ppo2.py:179][0m |           0.0812 |          14.0830 |         -11.7175 |
[32m[20221213 12:57:06 @agent_ppo2.py:179][0m |           0.0361 |          13.1022 |          -2.7791 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |           0.0087 |          12.5910 |          -4.1641 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |          -0.0136 |          12.2056 |          -5.8835 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |          -0.0145 |          12.0593 |          -8.6547 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |          -0.0132 |          12.0036 |          -9.1699 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |          -0.0142 |          11.7462 |          -7.3474 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |          -0.0297 |          11.6293 |          -8.2105 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |          -0.0286 |          11.5019 |          -8.3999 |
[32m[20221213 12:57:07 @agent_ppo2.py:179][0m |          -0.0319 |          11.3861 |          -8.6664 |
[32m[20221213 12:57:07 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:57:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 100.15
[32m[20221213 12:57:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.85
[32m[20221213 12:57:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.91
[32m[20221213 12:57:08 @agent_ppo2.py:137][0m Total time:      41.18 min
[32m[20221213 12:57:08 @agent_ppo2.py:139][0m 2848768 total steps have happened
[32m[20221213 12:57:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1391 --------------------------#
[32m[20221213 12:57:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:08 @agent_ppo2.py:179][0m |           0.0746 |          19.7699 |         -28.9165 |
[32m[20221213 12:57:08 @agent_ppo2.py:179][0m |           0.0442 |          18.5603 |         -12.3773 |
[32m[20221213 12:57:08 @agent_ppo2.py:179][0m |           0.0028 |          18.1182 |         -14.4861 |
[32m[20221213 12:57:08 @agent_ppo2.py:179][0m |          -0.0177 |          17.7271 |         -14.9147 |
[32m[20221213 12:57:08 @agent_ppo2.py:179][0m |          -0.0281 |          17.7501 |         -16.4438 |
[32m[20221213 12:57:09 @agent_ppo2.py:179][0m |          -0.0395 |          17.4297 |         -17.9581 |
[32m[20221213 12:57:09 @agent_ppo2.py:179][0m |          -0.0445 |          17.2770 |         -18.2779 |
[32m[20221213 12:57:09 @agent_ppo2.py:179][0m |          -0.0437 |          17.6133 |         -20.0668 |
[32m[20221213 12:57:09 @agent_ppo2.py:179][0m |          -0.0412 |          18.0568 |         -19.9704 |
[32m[20221213 12:57:09 @agent_ppo2.py:179][0m |          -0.0485 |          17.0972 |         -20.6714 |
[32m[20221213 12:57:09 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 165.86
[32m[20221213 12:57:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.34
[32m[20221213 12:57:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 365.13
[32m[20221213 12:57:09 @agent_ppo2.py:137][0m Total time:      41.21 min
[32m[20221213 12:57:09 @agent_ppo2.py:139][0m 2850816 total steps have happened
[32m[20221213 12:57:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1392 --------------------------#
[32m[20221213 12:57:09 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |           0.0542 |          38.2098 |         -37.6806 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |           0.0546 |          36.1071 |         -33.3040 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |           0.0336 |          36.3521 |         -35.1551 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |           0.0102 |          34.8471 |         -36.5715 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |          -0.0147 |          34.2968 |         -40.3136 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |          -0.0234 |          34.0395 |         -42.4564 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |          -0.0336 |          33.7682 |         -44.7021 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |          -0.0330 |          33.5348 |         -44.5836 |
[32m[20221213 12:57:10 @agent_ppo2.py:179][0m |          -0.0399 |          33.3006 |         -45.5935 |
[32m[20221213 12:57:11 @agent_ppo2.py:179][0m |          -0.0339 |          33.0707 |         -45.1382 |
[32m[20221213 12:57:11 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:57:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 358.32
[32m[20221213 12:57:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.68
[32m[20221213 12:57:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 40.07
[32m[20221213 12:57:11 @agent_ppo2.py:137][0m Total time:      41.23 min
[32m[20221213 12:57:11 @agent_ppo2.py:139][0m 2852864 total steps have happened
[32m[20221213 12:57:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1393 --------------------------#
[32m[20221213 12:57:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:11 @agent_ppo2.py:179][0m |           0.0813 |          38.2134 |         -38.4729 |
[32m[20221213 12:57:11 @agent_ppo2.py:179][0m |           0.0404 |          36.6050 |         -34.8203 |
[32m[20221213 12:57:11 @agent_ppo2.py:179][0m |           0.0104 |          36.0199 |         -36.9345 |
[32m[20221213 12:57:12 @agent_ppo2.py:179][0m |          -0.0104 |          35.4644 |         -37.7855 |
[32m[20221213 12:57:12 @agent_ppo2.py:179][0m |          -0.0260 |          35.4613 |         -39.9623 |
[32m[20221213 12:57:12 @agent_ppo2.py:179][0m |          -0.0354 |          34.7922 |         -40.4568 |
[32m[20221213 12:57:12 @agent_ppo2.py:179][0m |          -0.0405 |          34.6273 |         -42.6382 |
[32m[20221213 12:57:12 @agent_ppo2.py:179][0m |          -0.0434 |          34.2890 |         -43.1452 |
[32m[20221213 12:57:12 @agent_ppo2.py:179][0m |          -0.0434 |          34.0836 |         -44.4333 |
[32m[20221213 12:57:12 @agent_ppo2.py:179][0m |          -0.0393 |          34.5108 |         -45.1339 |
[32m[20221213 12:57:12 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:57:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 318.76
[32m[20221213 12:57:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.67
[32m[20221213 12:57:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.51
[32m[20221213 12:57:12 @agent_ppo2.py:137][0m Total time:      41.26 min
[32m[20221213 12:57:12 @agent_ppo2.py:139][0m 2854912 total steps have happened
[32m[20221213 12:57:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1394 --------------------------#
[32m[20221213 12:57:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:13 @agent_ppo2.py:179][0m |           0.0785 |          32.9244 |         -31.4941 |
[32m[20221213 12:57:13 @agent_ppo2.py:179][0m |           0.0645 |          31.4444 |         -23.3573 |
[32m[20221213 12:57:13 @agent_ppo2.py:179][0m |           0.0131 |          30.9722 |         -26.1952 |
[32m[20221213 12:57:13 @agent_ppo2.py:179][0m |          -0.0021 |          31.6543 |         -27.7670 |
[32m[20221213 12:57:13 @agent_ppo2.py:179][0m |          -0.0198 |          30.4262 |         -29.3816 |
[32m[20221213 12:57:13 @agent_ppo2.py:179][0m |          -0.0292 |          30.1539 |         -29.7760 |
[32m[20221213 12:57:14 @agent_ppo2.py:179][0m |          -0.0348 |          29.9773 |         -30.9298 |
[32m[20221213 12:57:14 @agent_ppo2.py:179][0m |          -0.0411 |          29.8531 |         -32.1221 |
[32m[20221213 12:57:14 @agent_ppo2.py:179][0m |          -0.0426 |          29.7563 |         -34.0034 |
[32m[20221213 12:57:14 @agent_ppo2.py:179][0m |          -0.0314 |          30.0046 |         -34.8015 |
[32m[20221213 12:57:14 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.32
[32m[20221213 12:57:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.79
[32m[20221213 12:57:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 384.68
[32m[20221213 12:57:14 @agent_ppo2.py:137][0m Total time:      41.29 min
[32m[20221213 12:57:14 @agent_ppo2.py:139][0m 2856960 total steps have happened
[32m[20221213 12:57:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1395 --------------------------#
[32m[20221213 12:57:14 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:57:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |           0.0592 |          28.2287 |         -33.0558 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |           0.0475 |          26.5001 |         -19.5907 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |           0.0084 |          26.6779 |         -19.9428 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |          -0.0035 |          25.8371 |         -19.6606 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |          -0.0161 |          25.3316 |         -20.7532 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |          -0.0257 |          25.2941 |         -22.1682 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |          -0.0352 |          25.1249 |         -23.2853 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |          -0.0405 |          24.9202 |         -24.2393 |
[32m[20221213 12:57:15 @agent_ppo2.py:179][0m |          -0.0297 |          28.8573 |         -25.3572 |
[32m[20221213 12:57:16 @agent_ppo2.py:179][0m |          -0.0471 |          24.9604 |         -26.5179 |
[32m[20221213 12:57:16 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:57:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 233.29
[32m[20221213 12:57:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 377.78
[32m[20221213 12:57:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 356.28
[32m[20221213 12:57:16 @agent_ppo2.py:137][0m Total time:      41.32 min
[32m[20221213 12:57:16 @agent_ppo2.py:139][0m 2859008 total steps have happened
[32m[20221213 12:57:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1396 --------------------------#
[32m[20221213 12:57:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:16 @agent_ppo2.py:179][0m |           0.0718 |          40.0280 |         -31.9192 |
[32m[20221213 12:57:16 @agent_ppo2.py:179][0m |           0.0429 |          37.9792 |         -27.3103 |
[32m[20221213 12:57:16 @agent_ppo2.py:179][0m |           0.0284 |          37.8173 |         -27.6888 |
[32m[20221213 12:57:17 @agent_ppo2.py:179][0m |           0.0004 |          37.3247 |         -32.3304 |
[32m[20221213 12:57:17 @agent_ppo2.py:179][0m |          -0.0080 |          37.0152 |         -34.6437 |
[32m[20221213 12:57:17 @agent_ppo2.py:179][0m |          -0.0129 |          36.9159 |         -35.5345 |
[32m[20221213 12:57:17 @agent_ppo2.py:179][0m |          -0.0245 |          36.7511 |         -38.2242 |
[32m[20221213 12:57:17 @agent_ppo2.py:179][0m |          -0.0323 |          36.6595 |         -40.0028 |
[32m[20221213 12:57:17 @agent_ppo2.py:179][0m |          -0.0260 |          36.5254 |         -40.5224 |
[32m[20221213 12:57:17 @agent_ppo2.py:179][0m |          -0.0315 |          36.5289 |         -41.5304 |
[32m[20221213 12:57:17 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 358.46
[32m[20221213 12:57:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 376.65
[32m[20221213 12:57:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.52
[32m[20221213 12:57:17 @agent_ppo2.py:137][0m Total time:      41.34 min
[32m[20221213 12:57:17 @agent_ppo2.py:139][0m 2861056 total steps have happened
[32m[20221213 12:57:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1397 --------------------------#
[32m[20221213 12:57:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:18 @agent_ppo2.py:179][0m |           0.0896 |          32.8699 |         -29.9716 |
[32m[20221213 12:57:18 @agent_ppo2.py:179][0m |           0.0627 |          32.1064 |         -19.8549 |
[32m[20221213 12:57:18 @agent_ppo2.py:179][0m |           0.0138 |          31.5664 |         -24.6893 |
[32m[20221213 12:57:18 @agent_ppo2.py:179][0m |          -0.0072 |          31.3026 |         -27.9725 |
[32m[20221213 12:57:18 @agent_ppo2.py:179][0m |          -0.0150 |          35.0238 |         -30.6571 |
[32m[20221213 12:57:18 @agent_ppo2.py:179][0m |          -0.0346 |          31.4237 |         -32.2688 |
[32m[20221213 12:57:19 @agent_ppo2.py:179][0m |          -0.0323 |          31.0095 |         -33.2779 |
[32m[20221213 12:57:19 @agent_ppo2.py:179][0m |          -0.0386 |          30.8494 |         -33.9898 |
[32m[20221213 12:57:19 @agent_ppo2.py:179][0m |          -0.0459 |          30.7769 |         -35.1820 |
[32m[20221213 12:57:19 @agent_ppo2.py:179][0m |          -0.0431 |          30.8004 |         -35.4880 |
[32m[20221213 12:57:19 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:57:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 281.06
[32m[20221213 12:57:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 369.09
[32m[20221213 12:57:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.83
[32m[20221213 12:57:19 @agent_ppo2.py:137][0m Total time:      41.37 min
[32m[20221213 12:57:19 @agent_ppo2.py:139][0m 2863104 total steps have happened
[32m[20221213 12:57:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1398 --------------------------#
[32m[20221213 12:57:19 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:19 @agent_ppo2.py:179][0m |           0.0864 |          38.0738 |         -30.1367 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |           0.1071 |          36.1710 |         -21.1366 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |           0.0488 |          36.5658 |         -27.3121 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |           0.0131 |          35.0077 |         -29.6194 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |          -0.0115 |          34.6827 |         -33.7717 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |          -0.0135 |          35.1371 |         -34.9483 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |          -0.0228 |          34.1373 |         -36.2981 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |          -0.0290 |          33.9429 |         -38.0217 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |          -0.0330 |          33.7669 |         -38.8737 |
[32m[20221213 12:57:20 @agent_ppo2.py:179][0m |          -0.0385 |          33.5529 |         -40.3839 |
[32m[20221213 12:57:20 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 283.34
[32m[20221213 12:57:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.47
[32m[20221213 12:57:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 28.93
[32m[20221213 12:57:21 @agent_ppo2.py:137][0m Total time:      41.40 min
[32m[20221213 12:57:21 @agent_ppo2.py:139][0m 2865152 total steps have happened
[32m[20221213 12:57:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1399 --------------------------#
[32m[20221213 12:57:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:21 @agent_ppo2.py:179][0m |           0.0730 |          39.4665 |         -27.8728 |
[32m[20221213 12:57:21 @agent_ppo2.py:179][0m |           0.0465 |          37.4901 |         -21.2574 |
[32m[20221213 12:57:21 @agent_ppo2.py:179][0m |           0.0125 |          36.7315 |         -24.4156 |
[32m[20221213 12:57:21 @agent_ppo2.py:179][0m |          -0.0041 |          36.3224 |         -26.8589 |
[32m[20221213 12:57:22 @agent_ppo2.py:179][0m |          -0.0183 |          36.1008 |         -28.7377 |
[32m[20221213 12:57:22 @agent_ppo2.py:179][0m |          -0.0243 |          35.9087 |         -30.5801 |
[32m[20221213 12:57:22 @agent_ppo2.py:179][0m |          -0.0244 |          35.6023 |         -31.0464 |
[32m[20221213 12:57:22 @agent_ppo2.py:179][0m |          -0.0353 |          35.4836 |         -32.9046 |
[32m[20221213 12:57:22 @agent_ppo2.py:179][0m |          -0.0307 |          35.3781 |         -32.2045 |
[32m[20221213 12:57:22 @agent_ppo2.py:179][0m |          -0.0318 |          35.3128 |         -33.7443 |
[32m[20221213 12:57:22 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:57:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.28
[32m[20221213 12:57:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.63
[32m[20221213 12:57:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.46
[32m[20221213 12:57:22 @agent_ppo2.py:137][0m Total time:      41.43 min
[32m[20221213 12:57:22 @agent_ppo2.py:139][0m 2867200 total steps have happened
[32m[20221213 12:57:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1400 --------------------------#
[32m[20221213 12:57:23 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:57:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:23 @agent_ppo2.py:179][0m |           0.0452 |          37.4889 |         -29.9242 |
[32m[20221213 12:57:23 @agent_ppo2.py:179][0m |           0.0115 |          35.7383 |         -29.2761 |
[32m[20221213 12:57:23 @agent_ppo2.py:179][0m |           0.0015 |          35.1108 |         -28.2167 |
[32m[20221213 12:57:23 @agent_ppo2.py:179][0m |          -0.0151 |          34.5712 |         -32.9286 |
[32m[20221213 12:57:23 @agent_ppo2.py:179][0m |          -0.0225 |          34.1487 |         -34.2547 |
[32m[20221213 12:57:23 @agent_ppo2.py:179][0m |          -0.0271 |          33.7985 |         -36.1844 |
[32m[20221213 12:57:23 @agent_ppo2.py:179][0m |          -0.0182 |          36.6262 |         -36.4590 |
[32m[20221213 12:57:24 @agent_ppo2.py:179][0m |          -0.0193 |          35.9090 |         -37.9975 |
[32m[20221213 12:57:24 @agent_ppo2.py:179][0m |          -0.0189 |          33.1903 |         -35.2479 |
[32m[20221213 12:57:24 @agent_ppo2.py:179][0m |          -0.0247 |          33.0732 |         -36.2866 |
[32m[20221213 12:57:24 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:57:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 362.23
[32m[20221213 12:57:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 371.33
[32m[20221213 12:57:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 378.22
[32m[20221213 12:57:24 @agent_ppo2.py:137][0m Total time:      41.45 min
[32m[20221213 12:57:24 @agent_ppo2.py:139][0m 2869248 total steps have happened
[32m[20221213 12:57:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1401 --------------------------#
[32m[20221213 12:57:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:24 @agent_ppo2.py:179][0m |           0.1187 |          38.8871 |         -26.0939 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |           0.0935 |          37.4241 |          -8.2390 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |           0.0357 |          37.1113 |         -17.6354 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |           0.0076 |          36.7582 |         -22.6117 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |          -0.0067 |          36.6131 |         -26.9656 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |          -0.0167 |          36.7531 |         -28.6011 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |          -0.0120 |          40.2679 |         -30.6207 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |          -0.0201 |          36.9680 |         -30.9762 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |          -0.0263 |          36.1334 |         -32.1577 |
[32m[20221213 12:57:25 @agent_ppo2.py:179][0m |          -0.0345 |          35.9719 |         -34.7497 |
[32m[20221213 12:57:25 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:57:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.46
[32m[20221213 12:57:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.02
[32m[20221213 12:57:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 325.38
[32m[20221213 12:57:26 @agent_ppo2.py:137][0m Total time:      41.48 min
[32m[20221213 12:57:26 @agent_ppo2.py:139][0m 2871296 total steps have happened
[32m[20221213 12:57:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1402 --------------------------#
[32m[20221213 12:57:26 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:26 @agent_ppo2.py:179][0m |           0.0379 |          34.7141 |         -23.9119 |
[32m[20221213 12:57:26 @agent_ppo2.py:179][0m |           0.0506 |          32.4312 |         -19.2568 |
[32m[20221213 12:57:26 @agent_ppo2.py:179][0m |           0.0076 |          31.1620 |         -22.8747 |
[32m[20221213 12:57:26 @agent_ppo2.py:179][0m |          -0.0107 |          30.5829 |         -25.4361 |
[32m[20221213 12:57:26 @agent_ppo2.py:179][0m |          -0.0169 |          30.2460 |         -26.7919 |
[32m[20221213 12:57:27 @agent_ppo2.py:179][0m |          -0.0242 |          29.8270 |         -28.1516 |
[32m[20221213 12:57:27 @agent_ppo2.py:179][0m |          -0.0254 |          29.3579 |         -28.4493 |
[32m[20221213 12:57:27 @agent_ppo2.py:179][0m |          -0.0334 |          29.0593 |         -29.2550 |
[32m[20221213 12:57:27 @agent_ppo2.py:179][0m |          -0.0390 |          28.7688 |         -31.0317 |
[32m[20221213 12:57:27 @agent_ppo2.py:179][0m |          -0.0341 |          29.8713 |         -32.1166 |
[32m[20221213 12:57:27 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:57:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 364.81
[32m[20221213 12:57:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 379.82
[32m[20221213 12:57:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 392.43
[32m[20221213 12:57:27 @agent_ppo2.py:137][0m Total time:      41.51 min
[32m[20221213 12:57:27 @agent_ppo2.py:139][0m 2873344 total steps have happened
[32m[20221213 12:57:27 @agent_ppo2.py:115][0m #------------------------ Iteration 1403 --------------------------#
[32m[20221213 12:57:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |           0.0681 |          33.7148 |         -27.6842 |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |           0.0406 |          32.0229 |         -21.6489 |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |           0.0120 |          31.3667 |         -26.4223 |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |          -0.0065 |          30.9531 |         -26.8864 |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |          -0.0180 |          30.6628 |         -26.8108 |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |          -0.0255 |          30.6983 |         -28.5954 |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |          -0.0226 |          34.8818 |         -29.6464 |
[32m[20221213 12:57:28 @agent_ppo2.py:179][0m |          -0.0395 |          30.0877 |         -30.0957 |
[32m[20221213 12:57:29 @agent_ppo2.py:179][0m |          -0.0424 |          29.8965 |         -31.3583 |
[32m[20221213 12:57:29 @agent_ppo2.py:179][0m |          -0.0382 |          29.6587 |         -30.5261 |
[32m[20221213 12:57:29 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 269.40
[32m[20221213 12:57:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.35
[32m[20221213 12:57:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.87
[32m[20221213 12:57:29 @agent_ppo2.py:137][0m Total time:      41.54 min
[32m[20221213 12:57:29 @agent_ppo2.py:139][0m 2875392 total steps have happened
[32m[20221213 12:57:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1404 --------------------------#
[32m[20221213 12:57:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:29 @agent_ppo2.py:179][0m |           0.0965 |          38.8821 |         -21.8734 |
[32m[20221213 12:57:29 @agent_ppo2.py:179][0m |           0.0747 |          37.9587 |         -11.4193 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |           0.0382 |          40.7251 |         -16.1014 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |           0.0056 |          37.1502 |         -19.7537 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |          -0.0056 |          36.9101 |         -21.0400 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |          -0.0186 |          36.7728 |         -23.2786 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |          -0.0241 |          36.5346 |         -23.8977 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |          -0.0308 |          36.4664 |         -25.4797 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |          -0.0365 |          36.1963 |         -26.7123 |
[32m[20221213 12:57:30 @agent_ppo2.py:179][0m |          -0.0388 |          36.1997 |         -27.9665 |
[32m[20221213 12:57:30 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 350.48
[32m[20221213 12:57:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.89
[32m[20221213 12:57:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 383.34
[32m[20221213 12:57:31 @agent_ppo2.py:137][0m Total time:      41.56 min
[32m[20221213 12:57:31 @agent_ppo2.py:139][0m 2877440 total steps have happened
[32m[20221213 12:57:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1405 --------------------------#
[32m[20221213 12:57:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:31 @agent_ppo2.py:179][0m |           0.0636 |          38.7368 |         -20.0125 |
[32m[20221213 12:57:31 @agent_ppo2.py:179][0m |           0.0322 |          36.9559 |         -16.0762 |
[32m[20221213 12:57:31 @agent_ppo2.py:179][0m |           0.0174 |          36.1972 |         -17.0181 |
[32m[20221213 12:57:31 @agent_ppo2.py:179][0m |           0.0027 |          37.3374 |         -19.1255 |
[32m[20221213 12:57:31 @agent_ppo2.py:179][0m |          -0.0163 |          35.1108 |         -20.9368 |
[32m[20221213 12:57:32 @agent_ppo2.py:179][0m |          -0.0158 |          37.6846 |         -22.3982 |
[32m[20221213 12:57:32 @agent_ppo2.py:179][0m |          -0.0226 |          34.6980 |         -22.8165 |
[32m[20221213 12:57:32 @agent_ppo2.py:179][0m |          -0.0343 |          34.3321 |         -24.4931 |
[32m[20221213 12:57:32 @agent_ppo2.py:179][0m |          -0.0404 |          34.2202 |         -25.1630 |
[32m[20221213 12:57:32 @agent_ppo2.py:179][0m |          -0.0401 |          34.0121 |         -25.7715 |
[32m[20221213 12:57:32 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:57:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.01
[32m[20221213 12:57:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 374.71
[32m[20221213 12:57:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 393.38
[32m[20221213 12:57:32 @agent_ppo2.py:137][0m Total time:      41.59 min
[32m[20221213 12:57:32 @agent_ppo2.py:139][0m 2879488 total steps have happened
[32m[20221213 12:57:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1406 --------------------------#
[32m[20221213 12:57:32 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |           0.0829 |          12.5505 |         -10.3414 |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |           0.0587 |          11.8200 |          -3.0195 |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |           0.0188 |          11.5114 |          -3.5363 |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |           0.0084 |          11.3253 |          -4.3924 |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |          -0.0033 |          11.2071 |          -4.5676 |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |          -0.0078 |          11.0703 |          -5.5341 |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |          -0.0136 |          10.9929 |          -5.6226 |
[32m[20221213 12:57:33 @agent_ppo2.py:179][0m |          -0.0166 |          11.0307 |          -6.0717 |
[32m[20221213 12:57:34 @agent_ppo2.py:179][0m |          -0.0241 |          10.8224 |          -6.4573 |
[32m[20221213 12:57:34 @agent_ppo2.py:179][0m |          -0.0126 |          11.2208 |          -6.5339 |
[32m[20221213 12:57:34 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:57:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 85.86
[32m[20221213 12:57:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 336.04
[32m[20221213 12:57:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 381.64
[32m[20221213 12:57:34 @agent_ppo2.py:137][0m Total time:      41.62 min
[32m[20221213 12:57:34 @agent_ppo2.py:139][0m 2881536 total steps have happened
[32m[20221213 12:57:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1407 --------------------------#
[32m[20221213 12:57:34 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:34 @agent_ppo2.py:179][0m |           0.0864 |          39.3367 |         -17.8622 |
[32m[20221213 12:57:34 @agent_ppo2.py:179][0m |           0.0505 |          37.9525 |         -13.3384 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |           0.0325 |          37.5197 |         -15.1637 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |           0.0025 |          37.2425 |         -20.5246 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |          -0.0073 |          36.9273 |         -21.6735 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |          -0.0240 |          36.7293 |         -22.8383 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |          -0.0312 |          36.5080 |         -23.7787 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |          -0.0322 |          36.3723 |         -24.3832 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |          -0.0350 |          36.6372 |         -25.4692 |
[32m[20221213 12:57:35 @agent_ppo2.py:179][0m |          -0.0387 |          36.5962 |         -26.8121 |
[32m[20221213 12:57:35 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 344.24
[32m[20221213 12:57:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.49
[32m[20221213 12:57:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 27.85
[32m[20221213 12:57:36 @agent_ppo2.py:137][0m Total time:      41.65 min
[32m[20221213 12:57:36 @agent_ppo2.py:139][0m 2883584 total steps have happened
[32m[20221213 12:57:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1408 --------------------------#
[32m[20221213 12:57:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:36 @agent_ppo2.py:179][0m |           0.0419 |          34.1084 |         -17.9350 |
[32m[20221213 12:57:36 @agent_ppo2.py:179][0m |           0.0324 |          33.2548 |         -14.1762 |
[32m[20221213 12:57:36 @agent_ppo2.py:179][0m |          -0.0051 |          32.9748 |         -16.7548 |
[32m[20221213 12:57:36 @agent_ppo2.py:179][0m |          -0.0221 |          32.7646 |         -18.0654 |
[32m[20221213 12:57:36 @agent_ppo2.py:179][0m |          -0.0204 |          34.2127 |         -19.3866 |
[32m[20221213 12:57:36 @agent_ppo2.py:179][0m |          -0.0251 |          33.3129 |         -19.8906 |
[32m[20221213 12:57:37 @agent_ppo2.py:179][0m |          -0.0360 |          32.3274 |         -19.1226 |
[32m[20221213 12:57:37 @agent_ppo2.py:179][0m |          -0.0479 |          32.2734 |         -21.6914 |
[32m[20221213 12:57:37 @agent_ppo2.py:179][0m |          -0.0554 |          32.1792 |         -22.2521 |
[32m[20221213 12:57:37 @agent_ppo2.py:179][0m |          -0.0439 |          32.0951 |         -22.3604 |
[32m[20221213 12:57:37 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:57:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 281.17
[32m[20221213 12:57:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.49
[32m[20221213 12:57:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 386.46
[32m[20221213 12:57:37 @agent_ppo2.py:137][0m Total time:      41.67 min
[32m[20221213 12:57:37 @agent_ppo2.py:139][0m 2885632 total steps have happened
[32m[20221213 12:57:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1409 --------------------------#
[32m[20221213 12:57:37 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |           0.0965 |          32.1291 |         -16.6561 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |           0.0424 |          30.9326 |         -10.9221 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |           0.0172 |          30.4226 |         -12.9440 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |          -0.0116 |          30.2067 |         -16.1247 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |          -0.0246 |          29.9119 |         -18.0108 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |          -0.0304 |          29.7548 |         -19.1070 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |          -0.0345 |          29.5986 |         -20.3025 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |          -0.0371 |          29.4942 |         -20.9358 |
[32m[20221213 12:57:38 @agent_ppo2.py:179][0m |          -0.0411 |          29.4196 |         -22.1027 |
[32m[20221213 12:57:39 @agent_ppo2.py:179][0m |          -0.0374 |          29.4840 |         -22.6000 |
[32m[20221213 12:57:39 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:57:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 299.96
[32m[20221213 12:57:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 373.23
[32m[20221213 12:57:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.50
[32m[20221213 12:57:39 @agent_ppo2.py:137][0m Total time:      41.70 min
[32m[20221213 12:57:39 @agent_ppo2.py:139][0m 2887680 total steps have happened
[32m[20221213 12:57:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1410 --------------------------#
[32m[20221213 12:57:39 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:57:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:39 @agent_ppo2.py:179][0m |           0.1214 |          13.5231 |          -8.1308 |
[32m[20221213 12:57:39 @agent_ppo2.py:179][0m |           0.0651 |          12.2437 |          -1.9609 |
[32m[20221213 12:57:39 @agent_ppo2.py:179][0m |           0.0241 |          11.7599 |          -3.5497 |
[32m[20221213 12:57:40 @agent_ppo2.py:179][0m |           0.0243 |          11.3945 |          -4.4967 |
[32m[20221213 12:57:40 @agent_ppo2.py:179][0m |           0.0025 |          11.1472 |          -5.0906 |
[32m[20221213 12:57:40 @agent_ppo2.py:179][0m |          -0.0144 |          10.9164 |          -6.2308 |
[32m[20221213 12:57:40 @agent_ppo2.py:179][0m |          -0.0181 |          10.7539 |          -6.3925 |
[32m[20221213 12:57:40 @agent_ppo2.py:179][0m |          -0.0264 |          10.7546 |          -7.1917 |
[32m[20221213 12:57:40 @agent_ppo2.py:179][0m |          -0.0266 |          10.4988 |          -7.7779 |
[32m[20221213 12:57:40 @agent_ppo2.py:179][0m |          -0.0278 |          10.3688 |          -8.2070 |
[32m[20221213 12:57:40 @agent_ppo2.py:124][0m Policy update time: 1.18 s
[32m[20221213 12:57:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 83.64
[32m[20221213 12:57:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 302.45
[32m[20221213 12:57:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 357.23
[32m[20221213 12:57:40 @agent_ppo2.py:137][0m Total time:      41.73 min
[32m[20221213 12:57:40 @agent_ppo2.py:139][0m 2889728 total steps have happened
[32m[20221213 12:57:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1411 --------------------------#
[32m[20221213 12:57:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:41 @agent_ppo2.py:179][0m |           0.0692 |          38.4819 |         -19.8519 |
[32m[20221213 12:57:41 @agent_ppo2.py:179][0m |           0.0689 |          35.9423 |         -15.7602 |
[32m[20221213 12:57:41 @agent_ppo2.py:179][0m |           0.0250 |          40.3349 |         -19.6154 |
[32m[20221213 12:57:41 @agent_ppo2.py:179][0m |          -0.0037 |          34.8170 |         -20.1552 |
[32m[20221213 12:57:41 @agent_ppo2.py:179][0m |          -0.0159 |          34.2522 |         -22.1743 |
[32m[20221213 12:57:41 @agent_ppo2.py:179][0m |          -0.0269 |          33.9040 |         -23.3584 |
[32m[20221213 12:57:41 @agent_ppo2.py:179][0m |          -0.0340 |          33.5735 |         -23.8161 |
[32m[20221213 12:57:42 @agent_ppo2.py:179][0m |          -0.0379 |          33.3913 |         -24.5800 |
[32m[20221213 12:57:42 @agent_ppo2.py:179][0m |          -0.0371 |          35.1630 |         -25.4147 |
[32m[20221213 12:57:42 @agent_ppo2.py:179][0m |          -0.0353 |          33.0240 |         -24.1906 |
[32m[20221213 12:57:42 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:57:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 348.04
[32m[20221213 12:57:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.43
[32m[20221213 12:57:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.08
[32m[20221213 12:57:42 @agent_ppo2.py:137][0m Total time:      41.75 min
[32m[20221213 12:57:42 @agent_ppo2.py:139][0m 2891776 total steps have happened
[32m[20221213 12:57:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1412 --------------------------#
[32m[20221213 12:57:42 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:42 @agent_ppo2.py:179][0m |           0.0466 |          38.7141 |         -19.8770 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |           0.0705 |          41.9865 |         -15.9502 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |           0.0247 |          36.4928 |         -16.3507 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |          -0.0068 |          35.9769 |         -18.0818 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |          -0.0253 |          35.7521 |         -20.3306 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |          -0.0345 |          35.4878 |         -21.2152 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |          -0.0392 |          35.2441 |         -21.2035 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |          -0.0373 |          35.1863 |         -21.5134 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |          -0.0392 |          34.9881 |         -22.0426 |
[32m[20221213 12:57:43 @agent_ppo2.py:179][0m |          -0.0391 |          34.8730 |         -22.6531 |
[32m[20221213 12:57:43 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:57:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.61
[32m[20221213 12:57:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.59
[32m[20221213 12:57:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.28
[32m[20221213 12:57:44 @agent_ppo2.py:137][0m Total time:      41.78 min
[32m[20221213 12:57:44 @agent_ppo2.py:139][0m 2893824 total steps have happened
[32m[20221213 12:57:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1413 --------------------------#
[32m[20221213 12:57:44 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:44 @agent_ppo2.py:179][0m |           0.0721 |          36.2277 |         -22.5297 |
[32m[20221213 12:57:44 @agent_ppo2.py:179][0m |           0.0483 |          35.0005 |         -18.3887 |
[32m[20221213 12:57:44 @agent_ppo2.py:179][0m |           0.0076 |          34.1211 |         -21.6699 |
[32m[20221213 12:57:44 @agent_ppo2.py:179][0m |          -0.0127 |          33.8104 |         -22.2306 |
[32m[20221213 12:57:45 @agent_ppo2.py:179][0m |          -0.0247 |          33.5697 |         -23.8113 |
[32m[20221213 12:57:45 @agent_ppo2.py:179][0m |          -0.0271 |          33.5775 |         -23.3882 |
[32m[20221213 12:57:45 @agent_ppo2.py:179][0m |          -0.0351 |          33.0143 |         -24.6793 |
[32m[20221213 12:57:45 @agent_ppo2.py:179][0m |          -0.0369 |          32.8718 |         -25.1801 |
[32m[20221213 12:57:45 @agent_ppo2.py:179][0m |          -0.0315 |          37.0023 |         -25.2989 |
[32m[20221213 12:57:45 @agent_ppo2.py:179][0m |          -0.0399 |          33.4197 |         -26.0169 |
[32m[20221213 12:57:45 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.49
[32m[20221213 12:57:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.40
[32m[20221213 12:57:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.08
[32m[20221213 12:57:45 @agent_ppo2.py:137][0m Total time:      41.81 min
[32m[20221213 12:57:45 @agent_ppo2.py:139][0m 2895872 total steps have happened
[32m[20221213 12:57:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1414 --------------------------#
[32m[20221213 12:57:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:46 @agent_ppo2.py:179][0m |           0.0697 |          32.6474 |         -18.5333 |
[32m[20221213 12:57:46 @agent_ppo2.py:179][0m |           0.1572 |          30.9154 |          -5.6662 |
[32m[20221213 12:57:46 @agent_ppo2.py:179][0m |           0.0437 |          30.2967 |          -9.3003 |
[32m[20221213 12:57:46 @agent_ppo2.py:179][0m |           0.0085 |          29.7188 |         -11.7055 |
[32m[20221213 12:57:46 @agent_ppo2.py:179][0m |          -0.0089 |          29.2845 |         -13.1597 |
[32m[20221213 12:57:46 @agent_ppo2.py:179][0m |          -0.0229 |          29.0802 |         -15.0624 |
[32m[20221213 12:57:46 @agent_ppo2.py:179][0m |          -0.0207 |          31.7051 |         -16.2150 |
[32m[20221213 12:57:47 @agent_ppo2.py:179][0m |          -0.0321 |          28.6547 |         -17.4706 |
[32m[20221213 12:57:47 @agent_ppo2.py:179][0m |          -0.0383 |          28.3113 |         -17.3793 |
[32m[20221213 12:57:47 @agent_ppo2.py:179][0m |          -0.0446 |          28.1693 |         -18.9667 |
[32m[20221213 12:57:47 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 249.35
[32m[20221213 12:57:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.82
[32m[20221213 12:57:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 370.33
[32m[20221213 12:57:47 @agent_ppo2.py:137][0m Total time:      41.84 min
[32m[20221213 12:57:47 @agent_ppo2.py:139][0m 2897920 total steps have happened
[32m[20221213 12:57:47 @agent_ppo2.py:115][0m #------------------------ Iteration 1415 --------------------------#
[32m[20221213 12:57:47 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:47 @agent_ppo2.py:179][0m |           0.0766 |          35.7970 |         -19.1075 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |           0.0354 |          34.6960 |         -14.2283 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |           0.0043 |          34.2921 |         -17.3882 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |          -0.0086 |          33.9903 |         -18.8343 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |          -0.0267 |          33.8644 |         -19.9880 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |          -0.0314 |          33.6776 |         -21.2751 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |          -0.0369 |          33.5699 |         -22.0639 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |          -0.0406 |          33.4064 |         -22.3932 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |          -0.0420 |          33.3183 |         -23.7567 |
[32m[20221213 12:57:48 @agent_ppo2.py:179][0m |          -0.0464 |          33.1784 |         -24.4810 |
[32m[20221213 12:57:48 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:57:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 319.40
[32m[20221213 12:57:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 349.70
[32m[20221213 12:57:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.10
[32m[20221213 12:57:49 @agent_ppo2.py:137][0m Total time:      41.87 min
[32m[20221213 12:57:49 @agent_ppo2.py:139][0m 2899968 total steps have happened
[32m[20221213 12:57:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1416 --------------------------#
[32m[20221213 12:57:49 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:49 @agent_ppo2.py:179][0m |           0.0707 |          36.5411 |         -16.4731 |
[32m[20221213 12:57:49 @agent_ppo2.py:179][0m |           0.0531 |          35.8862 |         -14.8002 |
[32m[20221213 12:57:49 @agent_ppo2.py:179][0m |           0.0084 |          34.7636 |         -16.3453 |
[32m[20221213 12:57:49 @agent_ppo2.py:179][0m |          -0.0078 |          36.8662 |         -19.8633 |
[32m[20221213 12:57:49 @agent_ppo2.py:179][0m |          -0.0162 |          36.7549 |         -20.5878 |
[32m[20221213 12:57:50 @agent_ppo2.py:179][0m |          -0.0302 |          33.9270 |         -20.9116 |
[32m[20221213 12:57:50 @agent_ppo2.py:179][0m |          -0.0233 |          35.0491 |         -21.2676 |
[32m[20221213 12:57:50 @agent_ppo2.py:179][0m |          -0.0368 |          33.5835 |         -22.1911 |
[32m[20221213 12:57:50 @agent_ppo2.py:179][0m |          -0.0330 |          33.3609 |         -22.6597 |
[32m[20221213 12:57:50 @agent_ppo2.py:179][0m |          -0.0380 |          33.2066 |         -23.1590 |
[32m[20221213 12:57:50 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:57:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 316.58
[32m[20221213 12:57:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.41
[32m[20221213 12:57:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.23
[32m[20221213 12:57:50 @agent_ppo2.py:137][0m Total time:      41.89 min
[32m[20221213 12:57:50 @agent_ppo2.py:139][0m 2902016 total steps have happened
[32m[20221213 12:57:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1417 --------------------------#
[32m[20221213 12:57:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |           0.1069 |          36.1661 |         -18.7669 |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |           0.0578 |          33.8469 |         -12.6657 |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |           0.0276 |          33.9841 |         -15.9077 |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |           0.0078 |          32.3768 |         -16.6061 |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |          -0.0116 |          31.9589 |         -19.3216 |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |          -0.0215 |          31.6189 |         -20.0314 |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |          -0.0263 |          31.2809 |         -21.7916 |
[32m[20221213 12:57:51 @agent_ppo2.py:179][0m |          -0.0270 |          31.0378 |         -22.0608 |
[32m[20221213 12:57:52 @agent_ppo2.py:179][0m |          -0.0354 |          30.7487 |         -23.1290 |
[32m[20221213 12:57:52 @agent_ppo2.py:179][0m |          -0.0389 |          30.4532 |         -24.2918 |
[32m[20221213 12:57:52 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.40
[32m[20221213 12:57:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 326.19
[32m[20221213 12:57:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 30.18
[32m[20221213 12:57:52 @agent_ppo2.py:137][0m Total time:      41.92 min
[32m[20221213 12:57:52 @agent_ppo2.py:139][0m 2904064 total steps have happened
[32m[20221213 12:57:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1418 --------------------------#
[32m[20221213 12:57:52 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:52 @agent_ppo2.py:179][0m |           0.0736 |          36.4650 |         -15.0725 |
[32m[20221213 12:57:52 @agent_ppo2.py:179][0m |           0.0868 |          34.8035 |          -9.2966 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |           0.0291 |          36.6830 |         -12.5957 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |          -0.0004 |          35.1638 |         -14.5076 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |          -0.0206 |          33.6248 |         -15.2821 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |          -0.0179 |          35.0845 |         -16.2799 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |          -0.0268 |          35.5072 |         -16.9296 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |          -0.0362 |          33.1608 |         -16.9698 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |          -0.0312 |          32.8118 |         -16.4246 |
[32m[20221213 12:57:53 @agent_ppo2.py:179][0m |          -0.0430 |          32.6969 |         -17.6153 |
[32m[20221213 12:57:53 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:57:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 304.98
[32m[20221213 12:57:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.64
[32m[20221213 12:57:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.87
[32m[20221213 12:57:54 @agent_ppo2.py:137][0m Total time:      41.95 min
[32m[20221213 12:57:54 @agent_ppo2.py:139][0m 2906112 total steps have happened
[32m[20221213 12:57:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1419 --------------------------#
[32m[20221213 12:57:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:54 @agent_ppo2.py:179][0m |           0.0642 |          31.0826 |         -18.1346 |
[32m[20221213 12:57:54 @agent_ppo2.py:179][0m |           0.0589 |          29.3338 |         -13.3666 |
[32m[20221213 12:57:54 @agent_ppo2.py:179][0m |           0.0033 |          27.9012 |         -16.1775 |
[32m[20221213 12:57:54 @agent_ppo2.py:179][0m |          -0.0096 |          26.8465 |         -17.8008 |
[32m[20221213 12:57:54 @agent_ppo2.py:179][0m |          -0.0224 |          26.2416 |         -18.4716 |
[32m[20221213 12:57:55 @agent_ppo2.py:179][0m |          -0.0288 |          25.8859 |         -18.9923 |
[32m[20221213 12:57:55 @agent_ppo2.py:179][0m |          -0.0415 |          25.4964 |         -20.5476 |
[32m[20221213 12:57:55 @agent_ppo2.py:179][0m |          -0.0458 |          25.0777 |         -20.9002 |
[32m[20221213 12:57:55 @agent_ppo2.py:179][0m |          -0.0500 |          24.8714 |         -21.5640 |
[32m[20221213 12:57:55 @agent_ppo2.py:179][0m |          -0.0478 |          24.5154 |         -21.7512 |
[32m[20221213 12:57:55 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:57:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 287.13
[32m[20221213 12:57:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.16
[32m[20221213 12:57:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.12
[32m[20221213 12:57:55 @agent_ppo2.py:137][0m Total time:      41.97 min
[32m[20221213 12:57:55 @agent_ppo2.py:139][0m 2908160 total steps have happened
[32m[20221213 12:57:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1420 --------------------------#
[32m[20221213 12:57:55 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:57:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |           0.0636 |          39.1189 |         -20.5073 |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |           0.0265 |          37.4808 |         -19.0192 |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |          -0.0050 |          36.8392 |         -20.9825 |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |          -0.0179 |          36.4057 |         -22.9390 |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |          -0.0240 |          36.0662 |         -23.8231 |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |          -0.0377 |          35.6269 |         -24.8419 |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |          -0.0429 |          35.4661 |         -25.8587 |
[32m[20221213 12:57:56 @agent_ppo2.py:179][0m |          -0.0392 |          38.3724 |         -25.8649 |
[32m[20221213 12:57:57 @agent_ppo2.py:179][0m |          -0.0501 |          35.2846 |         -27.2700 |
[32m[20221213 12:57:57 @agent_ppo2.py:179][0m |          -0.0500 |          35.0760 |         -27.4909 |
[32m[20221213 12:57:57 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:57:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.39
[32m[20221213 12:57:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 330.00
[32m[20221213 12:57:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 345.01
[32m[20221213 12:57:57 @agent_ppo2.py:137][0m Total time:      42.00 min
[32m[20221213 12:57:57 @agent_ppo2.py:139][0m 2910208 total steps have happened
[32m[20221213 12:57:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1421 --------------------------#
[32m[20221213 12:57:57 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:57 @agent_ppo2.py:179][0m |           0.0834 |          29.9791 |         -15.7589 |
[32m[20221213 12:57:57 @agent_ppo2.py:179][0m |           0.0465 |          28.5928 |         -10.1677 |
[32m[20221213 12:57:57 @agent_ppo2.py:179][0m |           0.0173 |          28.4867 |         -12.7174 |
[32m[20221213 12:57:58 @agent_ppo2.py:179][0m |          -0.0085 |          28.5843 |         -15.7791 |
[32m[20221213 12:57:58 @agent_ppo2.py:179][0m |          -0.0255 |          27.6284 |         -17.3135 |
[32m[20221213 12:57:58 @agent_ppo2.py:179][0m |          -0.0378 |          27.2071 |         -18.7613 |
[32m[20221213 12:57:58 @agent_ppo2.py:179][0m |          -0.0396 |          27.3156 |         -19.6564 |
[32m[20221213 12:57:58 @agent_ppo2.py:179][0m |          -0.0466 |          26.8996 |         -20.6882 |
[32m[20221213 12:57:58 @agent_ppo2.py:179][0m |          -0.0513 |          26.6785 |         -21.1764 |
[32m[20221213 12:57:58 @agent_ppo2.py:179][0m |          -0.0530 |          26.5015 |         -21.8283 |
[32m[20221213 12:57:58 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:57:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 252.12
[32m[20221213 12:57:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 342.27
[32m[20221213 12:57:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 193.44
[32m[20221213 12:57:58 @agent_ppo2.py:137][0m Total time:      42.03 min
[32m[20221213 12:57:58 @agent_ppo2.py:139][0m 2912256 total steps have happened
[32m[20221213 12:57:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1422 --------------------------#
[32m[20221213 12:57:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:57:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:57:59 @agent_ppo2.py:179][0m |           0.0946 |          37.0179 |         -19.9404 |
[32m[20221213 12:57:59 @agent_ppo2.py:179][0m |           0.0685 |          35.6082 |         -15.8485 |
[32m[20221213 12:57:59 @agent_ppo2.py:179][0m |           0.0190 |          34.5619 |         -18.4654 |
[32m[20221213 12:57:59 @agent_ppo2.py:179][0m |          -0.0082 |          34.0827 |         -21.7107 |
[32m[20221213 12:57:59 @agent_ppo2.py:179][0m |          -0.0211 |          33.6695 |         -22.9600 |
[32m[20221213 12:57:59 @agent_ppo2.py:179][0m |          -0.0327 |          33.4005 |         -24.0355 |
[32m[20221213 12:58:00 @agent_ppo2.py:179][0m |          -0.0337 |          33.0385 |         -24.6563 |
[32m[20221213 12:58:00 @agent_ppo2.py:179][0m |          -0.0374 |          32.8366 |         -25.6805 |
[32m[20221213 12:58:00 @agent_ppo2.py:179][0m |          -0.0430 |          32.7142 |         -26.4158 |
[32m[20221213 12:58:00 @agent_ppo2.py:179][0m |          -0.0419 |          32.6002 |         -27.6303 |
[32m[20221213 12:58:00 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.84
[32m[20221213 12:58:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.32
[32m[20221213 12:58:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 22.91
[32m[20221213 12:58:00 @agent_ppo2.py:137][0m Total time:      42.06 min
[32m[20221213 12:58:00 @agent_ppo2.py:139][0m 2914304 total steps have happened
[32m[20221213 12:58:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1423 --------------------------#
[32m[20221213 12:58:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |           0.0939 |          34.2845 |         -21.3512 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |           0.0574 |          32.9554 |         -16.7155 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |           0.0262 |          32.8291 |         -19.2008 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |          -0.0068 |          32.1519 |         -23.1172 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |          -0.0209 |          31.9396 |         -24.4789 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |          -0.0259 |          31.6430 |         -25.2538 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |          -0.0294 |          31.5383 |         -26.7579 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |          -0.0269 |          33.6845 |         -27.4573 |
[32m[20221213 12:58:01 @agent_ppo2.py:179][0m |          -0.0352 |          31.2341 |         -28.1375 |
[32m[20221213 12:58:02 @agent_ppo2.py:179][0m |          -0.0375 |          31.0612 |         -29.0191 |
[32m[20221213 12:58:02 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:58:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.65
[32m[20221213 12:58:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.50
[32m[20221213 12:58:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 31.94
[32m[20221213 12:58:02 @agent_ppo2.py:137][0m Total time:      42.08 min
[32m[20221213 12:58:02 @agent_ppo2.py:139][0m 2916352 total steps have happened
[32m[20221213 12:58:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1424 --------------------------#
[32m[20221213 12:58:02 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:02 @agent_ppo2.py:179][0m |           0.0923 |          33.8587 |         -21.7158 |
[32m[20221213 12:58:02 @agent_ppo2.py:179][0m |           0.0388 |          32.6475 |         -20.7386 |
[32m[20221213 12:58:02 @agent_ppo2.py:179][0m |           0.0125 |          32.0511 |         -23.0969 |
[32m[20221213 12:58:03 @agent_ppo2.py:179][0m |          -0.0105 |          31.6946 |         -24.8972 |
[32m[20221213 12:58:03 @agent_ppo2.py:179][0m |          -0.0129 |          33.2084 |         -26.2376 |
[32m[20221213 12:58:03 @agent_ppo2.py:179][0m |          -0.0275 |          31.3051 |         -26.8504 |
[32m[20221213 12:58:03 @agent_ppo2.py:179][0m |          -0.0290 |          30.8895 |         -27.7673 |
[32m[20221213 12:58:03 @agent_ppo2.py:179][0m |          -0.0324 |          30.6623 |         -28.0393 |
[32m[20221213 12:58:03 @agent_ppo2.py:179][0m |          -0.0393 |          30.5112 |         -29.0998 |
[32m[20221213 12:58:03 @agent_ppo2.py:179][0m |          -0.0382 |          30.3581 |         -29.6200 |
[32m[20221213 12:58:03 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.49
[32m[20221213 12:58:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 338.04
[32m[20221213 12:58:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.96
[32m[20221213 12:58:03 @agent_ppo2.py:137][0m Total time:      42.11 min
[32m[20221213 12:58:03 @agent_ppo2.py:139][0m 2918400 total steps have happened
[32m[20221213 12:58:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1425 --------------------------#
[32m[20221213 12:58:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:04 @agent_ppo2.py:179][0m |           0.0570 |          24.9081 |         -16.2711 |
[32m[20221213 12:58:04 @agent_ppo2.py:179][0m |           0.0266 |          23.6558 |         -14.0828 |
[32m[20221213 12:58:04 @agent_ppo2.py:179][0m |           0.0018 |          23.2443 |         -14.2672 |
[32m[20221213 12:58:04 @agent_ppo2.py:179][0m |          -0.0275 |          22.8884 |         -16.4397 |
[32m[20221213 12:58:04 @agent_ppo2.py:179][0m |          -0.0330 |          22.9505 |         -17.4622 |
[32m[20221213 12:58:04 @agent_ppo2.py:179][0m |          -0.0463 |          22.4201 |         -18.7529 |
[32m[20221213 12:58:04 @agent_ppo2.py:179][0m |          -0.0513 |          22.2057 |         -19.8895 |
[32m[20221213 12:58:05 @agent_ppo2.py:179][0m |          -0.0494 |          22.0084 |         -20.7745 |
[32m[20221213 12:58:05 @agent_ppo2.py:179][0m |          -0.0541 |          21.8549 |         -21.0459 |
[32m[20221213 12:58:05 @agent_ppo2.py:179][0m |          -0.0594 |          21.7723 |         -21.2303 |
[32m[20221213 12:58:05 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 198.53
[32m[20221213 12:58:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 331.09
[32m[20221213 12:58:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 291.74
[32m[20221213 12:58:05 @agent_ppo2.py:137][0m Total time:      42.14 min
[32m[20221213 12:58:05 @agent_ppo2.py:139][0m 2920448 total steps have happened
[32m[20221213 12:58:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1426 --------------------------#
[32m[20221213 12:58:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:05 @agent_ppo2.py:179][0m |           0.0907 |          20.0418 |         -15.0310 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |           0.0665 |          20.5878 |          -6.7639 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |           0.0027 |          18.0969 |          -8.8996 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |          -0.0276 |          17.7192 |         -11.5684 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |          -0.0402 |          17.5519 |         -12.8317 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |          -0.0488 |          17.2935 |         -13.9708 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |          -0.0523 |          17.1411 |         -14.5350 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |          -0.0595 |          17.0643 |         -15.2907 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |          -0.0619 |          16.8224 |         -15.7227 |
[32m[20221213 12:58:06 @agent_ppo2.py:179][0m |          -0.0635 |          16.7946 |         -15.8473 |
[32m[20221213 12:58:06 @agent_ppo2.py:124][0m Policy update time: 1.19 s
[32m[20221213 12:58:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 159.39
[32m[20221213 12:58:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.66
[32m[20221213 12:58:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 326.54
[32m[20221213 12:58:07 @agent_ppo2.py:137][0m Total time:      42.17 min
[32m[20221213 12:58:07 @agent_ppo2.py:139][0m 2922496 total steps have happened
[32m[20221213 12:58:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1427 --------------------------#
[32m[20221213 12:58:07 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:07 @agent_ppo2.py:179][0m |           0.0702 |          34.3315 |         -24.6704 |
[32m[20221213 12:58:07 @agent_ppo2.py:179][0m |           0.0379 |          32.8297 |         -21.2097 |
[32m[20221213 12:58:07 @agent_ppo2.py:179][0m |           0.0087 |          32.3581 |         -25.0926 |
[32m[20221213 12:58:07 @agent_ppo2.py:179][0m |          -0.0120 |          32.0723 |         -26.1646 |
[32m[20221213 12:58:08 @agent_ppo2.py:179][0m |          -0.0223 |          31.9219 |         -27.4498 |
[32m[20221213 12:58:08 @agent_ppo2.py:179][0m |          -0.0275 |          31.7154 |         -28.8846 |
[32m[20221213 12:58:08 @agent_ppo2.py:179][0m |          -0.0271 |          31.5960 |         -29.3587 |
[32m[20221213 12:58:08 @agent_ppo2.py:179][0m |          -0.0208 |          31.5196 |         -28.4802 |
[32m[20221213 12:58:08 @agent_ppo2.py:179][0m |          -0.0314 |          32.1518 |         -30.0943 |
[32m[20221213 12:58:08 @agent_ppo2.py:179][0m |          -0.0410 |          31.3376 |         -31.6969 |
[32m[20221213 12:58:08 @agent_ppo2.py:124][0m Policy update time: 1.19 s
[32m[20221213 12:58:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 327.10
[32m[20221213 12:58:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.13
[32m[20221213 12:58:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.13
[32m[20221213 12:58:08 @agent_ppo2.py:137][0m Total time:      42.19 min
[32m[20221213 12:58:08 @agent_ppo2.py:139][0m 2924544 total steps have happened
[32m[20221213 12:58:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1428 --------------------------#
[32m[20221213 12:58:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |           0.0727 |          31.2057 |         -20.3491 |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |           0.0319 |          29.8209 |         -18.9361 |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |           0.0028 |          28.8900 |         -19.6265 |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |          -0.0130 |          28.4013 |         -22.0081 |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |          -0.0268 |          27.9674 |         -24.1458 |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |          -0.0369 |          27.7022 |         -25.0226 |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |          -0.0385 |          27.3499 |         -26.4343 |
[32m[20221213 12:58:09 @agent_ppo2.py:179][0m |          -0.0464 |          27.3133 |         -27.6404 |
[32m[20221213 12:58:10 @agent_ppo2.py:179][0m |          -0.0477 |          26.8871 |         -28.7397 |
[32m[20221213 12:58:10 @agent_ppo2.py:179][0m |          -0.0519 |          26.7139 |         -29.7701 |
[32m[20221213 12:58:10 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 277.00
[32m[20221213 12:58:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.60
[32m[20221213 12:58:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.44
[32m[20221213 12:58:10 @agent_ppo2.py:137][0m Total time:      42.22 min
[32m[20221213 12:58:10 @agent_ppo2.py:139][0m 2926592 total steps have happened
[32m[20221213 12:58:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1429 --------------------------#
[32m[20221213 12:58:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:10 @agent_ppo2.py:179][0m |           0.1115 |          33.9640 |         -28.6600 |
[32m[20221213 12:58:10 @agent_ppo2.py:179][0m |           0.1029 |          32.0901 |         -19.3431 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |           0.0253 |          31.3389 |         -24.5939 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |          -0.0014 |          30.8264 |         -27.8695 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |          -0.0187 |          30.3744 |         -29.8442 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |          -0.0174 |          30.4574 |         -31.5243 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |          -0.0289 |          29.8056 |         -32.5882 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |          -0.0308 |          30.3961 |         -33.4240 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |          -0.0397 |          29.3141 |         -34.3032 |
[32m[20221213 12:58:11 @agent_ppo2.py:179][0m |          -0.0443 |          29.2360 |         -35.4172 |
[32m[20221213 12:58:11 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:58:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.77
[32m[20221213 12:58:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.23
[32m[20221213 12:58:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.46
[32m[20221213 12:58:12 @agent_ppo2.py:137][0m Total time:      42.25 min
[32m[20221213 12:58:12 @agent_ppo2.py:139][0m 2928640 total steps have happened
[32m[20221213 12:58:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1430 --------------------------#
[32m[20221213 12:58:12 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:58:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:12 @agent_ppo2.py:179][0m |           0.0827 |          34.7282 |         -25.0122 |
[32m[20221213 12:58:12 @agent_ppo2.py:179][0m |           0.0457 |          33.2807 |         -20.5540 |
[32m[20221213 12:58:12 @agent_ppo2.py:179][0m |           0.0229 |          32.6098 |         -25.3394 |
[32m[20221213 12:58:12 @agent_ppo2.py:179][0m |           0.0038 |          34.2217 |         -27.3826 |
[32m[20221213 12:58:12 @agent_ppo2.py:179][0m |          -0.0189 |          32.0351 |         -29.0101 |
[32m[20221213 12:58:13 @agent_ppo2.py:179][0m |          -0.0269 |          31.8667 |         -30.8970 |
[32m[20221213 12:58:13 @agent_ppo2.py:179][0m |          -0.0330 |          31.5986 |         -31.8883 |
[32m[20221213 12:58:13 @agent_ppo2.py:179][0m |          -0.0363 |          31.4840 |         -33.0812 |
[32m[20221213 12:58:13 @agent_ppo2.py:179][0m |          -0.0391 |          31.3182 |         -34.5069 |
[32m[20221213 12:58:13 @agent_ppo2.py:179][0m |          -0.0414 |          31.2584 |         -34.9296 |
[32m[20221213 12:58:13 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:58:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 335.55
[32m[20221213 12:58:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 357.77
[32m[20221213 12:58:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 388.76
[32m[20221213 12:58:13 @agent_ppo2.py:137][0m Total time:      42.27 min
[32m[20221213 12:58:13 @agent_ppo2.py:139][0m 2930688 total steps have happened
[32m[20221213 12:58:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1431 --------------------------#
[32m[20221213 12:58:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |           0.0553 |          35.6104 |         -28.4717 |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |           0.0817 |          34.3577 |         -15.6470 |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |           0.0388 |          33.8147 |         -17.0577 |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |          -0.0006 |          33.2475 |         -25.6125 |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |          -0.0095 |          34.8592 |         -28.9902 |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |          -0.0271 |          32.6624 |         -30.8467 |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |          -0.0325 |          32.3217 |         -32.2957 |
[32m[20221213 12:58:14 @agent_ppo2.py:179][0m |          -0.0376 |          32.1675 |         -33.4522 |
[32m[20221213 12:58:15 @agent_ppo2.py:179][0m |          -0.0367 |          32.6697 |         -34.3823 |
[32m[20221213 12:58:15 @agent_ppo2.py:179][0m |          -0.0428 |          31.8079 |         -35.2856 |
[32m[20221213 12:58:15 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:58:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 339.46
[32m[20221213 12:58:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.43
[32m[20221213 12:58:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 368.50
[32m[20221213 12:58:15 @agent_ppo2.py:137][0m Total time:      42.30 min
[32m[20221213 12:58:15 @agent_ppo2.py:139][0m 2932736 total steps have happened
[32m[20221213 12:58:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1432 --------------------------#
[32m[20221213 12:58:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:15 @agent_ppo2.py:179][0m |           0.0928 |          33.4290 |         -26.0612 |
[32m[20221213 12:58:15 @agent_ppo2.py:179][0m |           0.0636 |          34.3976 |         -19.9421 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |           0.0185 |          30.0966 |         -24.2741 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |           0.0014 |          29.4036 |         -25.9549 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |          -0.0084 |          28.7354 |         -27.7052 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |          -0.0207 |          28.4332 |         -31.3430 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |          -0.0293 |          27.9695 |         -32.6732 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |          -0.0341 |          27.6626 |         -33.6702 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |          -0.0396 |          27.4420 |         -34.3668 |
[32m[20221213 12:58:16 @agent_ppo2.py:179][0m |          -0.0389 |          27.2042 |         -35.6064 |
[32m[20221213 12:58:16 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 344.53
[32m[20221213 12:58:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.62
[32m[20221213 12:58:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 218.79
[32m[20221213 12:58:16 @agent_ppo2.py:137][0m Total time:      42.33 min
[32m[20221213 12:58:16 @agent_ppo2.py:139][0m 2934784 total steps have happened
[32m[20221213 12:58:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1433 --------------------------#
[32m[20221213 12:58:17 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:17 @agent_ppo2.py:179][0m |           0.0600 |          35.7203 |         -24.1723 |
[32m[20221213 12:58:17 @agent_ppo2.py:179][0m |           0.0555 |          34.1520 |         -17.6042 |
[32m[20221213 12:58:17 @agent_ppo2.py:179][0m |           0.0308 |          33.7450 |         -19.4585 |
[32m[20221213 12:58:17 @agent_ppo2.py:179][0m |          -0.0083 |          33.3425 |         -22.7270 |
[32m[20221213 12:58:17 @agent_ppo2.py:179][0m |          -0.0231 |          33.1668 |         -24.1505 |
[32m[20221213 12:58:17 @agent_ppo2.py:179][0m |          -0.0257 |          32.8799 |         -25.5860 |
[32m[20221213 12:58:18 @agent_ppo2.py:179][0m |          -0.0403 |          32.6892 |         -26.6870 |
[32m[20221213 12:58:18 @agent_ppo2.py:179][0m |          -0.0340 |          32.4135 |         -27.4894 |
[32m[20221213 12:58:18 @agent_ppo2.py:179][0m |          -0.0365 |          32.2354 |         -27.8280 |
[32m[20221213 12:58:18 @agent_ppo2.py:179][0m |          -0.0405 |          32.1518 |         -29.5967 |
[32m[20221213 12:58:18 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 350.50
[32m[20221213 12:58:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.28
[32m[20221213 12:58:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.51
[32m[20221213 12:58:18 @agent_ppo2.py:137][0m Total time:      42.36 min
[32m[20221213 12:58:18 @agent_ppo2.py:139][0m 2936832 total steps have happened
[32m[20221213 12:58:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1434 --------------------------#
[32m[20221213 12:58:18 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |           0.0613 |          36.0977 |         -23.5438 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |           0.0515 |          35.4465 |         -16.0807 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |           0.0107 |          34.2004 |         -19.5839 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |          -0.0056 |          34.7611 |         -22.3405 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |          -0.0164 |          33.7607 |         -25.6143 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |          -0.0194 |          33.4518 |         -25.4306 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |          -0.0332 |          33.3546 |         -27.5553 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |          -0.0361 |          33.1662 |         -28.9344 |
[32m[20221213 12:58:19 @agent_ppo2.py:179][0m |          -0.0268 |          34.9912 |         -29.4943 |
[32m[20221213 12:58:20 @agent_ppo2.py:179][0m |          -0.0379 |          32.9176 |         -30.8323 |
[32m[20221213 12:58:20 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:58:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 347.84
[32m[20221213 12:58:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 370.49
[32m[20221213 12:58:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.75
[32m[20221213 12:58:20 @agent_ppo2.py:137][0m Total time:      42.38 min
[32m[20221213 12:58:20 @agent_ppo2.py:139][0m 2938880 total steps have happened
[32m[20221213 12:58:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1435 --------------------------#
[32m[20221213 12:58:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:20 @agent_ppo2.py:179][0m |           0.0638 |          35.5806 |         -19.0569 |
[32m[20221213 12:58:20 @agent_ppo2.py:179][0m |           0.0490 |          34.7228 |         -17.8372 |
[32m[20221213 12:58:20 @agent_ppo2.py:179][0m |           0.0094 |          34.2364 |         -19.9880 |
[32m[20221213 12:58:21 @agent_ppo2.py:179][0m |          -0.0099 |          33.5773 |         -20.7418 |
[32m[20221213 12:58:21 @agent_ppo2.py:179][0m |          -0.0175 |          33.3576 |         -22.6602 |
[32m[20221213 12:58:21 @agent_ppo2.py:179][0m |          -0.0221 |          33.2982 |         -23.6993 |
[32m[20221213 12:58:21 @agent_ppo2.py:179][0m |          -0.0264 |          32.9973 |         -24.5343 |
[32m[20221213 12:58:21 @agent_ppo2.py:179][0m |          -0.0278 |          32.9669 |         -25.1681 |
[32m[20221213 12:58:21 @agent_ppo2.py:179][0m |          -0.0247 |          32.7862 |         -26.3412 |
[32m[20221213 12:58:21 @agent_ppo2.py:179][0m |          -0.0375 |          32.6626 |         -26.8942 |
[32m[20221213 12:58:21 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 349.30
[32m[20221213 12:58:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.31
[32m[20221213 12:58:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.93
[32m[20221213 12:58:21 @agent_ppo2.py:137][0m Total time:      42.41 min
[32m[20221213 12:58:21 @agent_ppo2.py:139][0m 2940928 total steps have happened
[32m[20221213 12:58:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1436 --------------------------#
[32m[20221213 12:58:22 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:22 @agent_ppo2.py:179][0m |           0.0458 |          36.4921 |         -26.0897 |
[32m[20221213 12:58:22 @agent_ppo2.py:179][0m |           0.0424 |          35.5015 |         -20.6310 |
[32m[20221213 12:58:22 @agent_ppo2.py:179][0m |           0.0214 |          35.1720 |         -22.1330 |
[32m[20221213 12:58:22 @agent_ppo2.py:179][0m |          -0.0014 |          36.1265 |         -25.1286 |
[32m[20221213 12:58:22 @agent_ppo2.py:179][0m |          -0.0081 |          38.9780 |         -26.8923 |
[32m[20221213 12:58:22 @agent_ppo2.py:179][0m |          -0.0120 |          38.8561 |         -26.0288 |
[32m[20221213 12:58:23 @agent_ppo2.py:179][0m |          -0.0274 |          34.5357 |         -27.9683 |
[32m[20221213 12:58:23 @agent_ppo2.py:179][0m |          -0.0327 |          34.3831 |         -29.2239 |
[32m[20221213 12:58:23 @agent_ppo2.py:179][0m |          -0.0225 |          34.2643 |         -28.7850 |
[32m[20221213 12:58:23 @agent_ppo2.py:179][0m |          -0.0387 |          34.1774 |         -30.1188 |
[32m[20221213 12:58:23 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.06
[32m[20221213 12:58:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.72
[32m[20221213 12:58:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.92
[32m[20221213 12:58:23 @agent_ppo2.py:137][0m Total time:      42.44 min
[32m[20221213 12:58:23 @agent_ppo2.py:139][0m 2942976 total steps have happened
[32m[20221213 12:58:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1437 --------------------------#
[32m[20221213 12:58:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |           0.0429 |          37.0002 |         -20.3480 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |           0.0398 |          36.1476 |         -17.6617 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |           0.0149 |          35.7181 |         -20.1409 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |          -0.0105 |          35.4425 |         -21.9193 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |          -0.0211 |          35.2800 |         -23.2934 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |          -0.0201 |          35.1221 |         -23.4666 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |          -0.0286 |          34.9689 |         -24.6333 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |          -0.0315 |          34.8500 |         -25.3715 |
[32m[20221213 12:58:24 @agent_ppo2.py:179][0m |          -0.0364 |          34.7102 |         -25.2579 |
[32m[20221213 12:58:25 @agent_ppo2.py:179][0m |          -0.0317 |          35.3643 |         -25.3519 |
[32m[20221213 12:58:25 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 347.48
[32m[20221213 12:58:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.04
[32m[20221213 12:58:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.90
[32m[20221213 12:58:25 @agent_ppo2.py:137][0m Total time:      42.47 min
[32m[20221213 12:58:25 @agent_ppo2.py:139][0m 2945024 total steps have happened
[32m[20221213 12:58:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1438 --------------------------#
[32m[20221213 12:58:25 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:25 @agent_ppo2.py:179][0m |           0.0673 |          36.6521 |         -16.5046 |
[32m[20221213 12:58:25 @agent_ppo2.py:179][0m |           0.0442 |          35.7392 |         -12.7653 |
[32m[20221213 12:58:25 @agent_ppo2.py:179][0m |           0.0456 |          34.4258 |         -13.5311 |
[32m[20221213 12:58:25 @agent_ppo2.py:179][0m |           0.0101 |          33.9970 |         -13.2986 |
[32m[20221213 12:58:26 @agent_ppo2.py:179][0m |          -0.0100 |          33.6126 |         -16.4482 |
[32m[20221213 12:58:26 @agent_ppo2.py:179][0m |          -0.0229 |          33.3932 |         -17.8675 |
[32m[20221213 12:58:26 @agent_ppo2.py:179][0m |          -0.0240 |          33.0462 |         -18.4063 |
[32m[20221213 12:58:26 @agent_ppo2.py:179][0m |          -0.0265 |          33.0324 |         -20.4876 |
[32m[20221213 12:58:26 @agent_ppo2.py:179][0m |          -0.0346 |          32.7664 |         -21.6088 |
[32m[20221213 12:58:26 @agent_ppo2.py:179][0m |          -0.0400 |          32.6019 |         -22.1734 |
[32m[20221213 12:58:26 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:58:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 321.40
[32m[20221213 12:58:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.26
[32m[20221213 12:58:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 366.38
[32m[20221213 12:58:26 @agent_ppo2.py:137][0m Total time:      42.49 min
[32m[20221213 12:58:26 @agent_ppo2.py:139][0m 2947072 total steps have happened
[32m[20221213 12:58:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1439 --------------------------#
[32m[20221213 12:58:27 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:27 @agent_ppo2.py:179][0m |           0.0964 |          36.2225 |         -14.6948 |
[32m[20221213 12:58:27 @agent_ppo2.py:179][0m |           0.0438 |          34.4118 |         -11.1856 |
[32m[20221213 12:58:27 @agent_ppo2.py:179][0m |           0.0132 |          36.3091 |         -12.8301 |
[32m[20221213 12:58:27 @agent_ppo2.py:179][0m |          -0.0089 |          32.9544 |         -14.6080 |
[32m[20221213 12:58:27 @agent_ppo2.py:179][0m |           0.0067 |          32.3618 |         -13.2575 |
[32m[20221213 12:58:27 @agent_ppo2.py:179][0m |          -0.0129 |          32.1511 |         -15.4208 |
[32m[20221213 12:58:27 @agent_ppo2.py:179][0m |          -0.0258 |          31.3002 |         -16.4104 |
[32m[20221213 12:58:28 @agent_ppo2.py:179][0m |          -0.0359 |          30.9557 |         -17.3068 |
[32m[20221213 12:58:28 @agent_ppo2.py:179][0m |          -0.0286 |          31.4049 |         -17.8080 |
[32m[20221213 12:58:28 @agent_ppo2.py:179][0m |          -0.0275 |          30.0713 |         -17.4965 |
[32m[20221213 12:58:28 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 308.70
[32m[20221213 12:58:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.23
[32m[20221213 12:58:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.88
[32m[20221213 12:58:28 @agent_ppo2.py:137][0m Total time:      42.52 min
[32m[20221213 12:58:28 @agent_ppo2.py:139][0m 2949120 total steps have happened
[32m[20221213 12:58:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1440 --------------------------#
[32m[20221213 12:58:28 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:58:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:28 @agent_ppo2.py:179][0m |           0.2151 |          39.0547 |         -14.5434 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |           0.0283 |          37.2709 |         -13.9807 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0044 |          36.5320 |         -15.8892 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0202 |          36.0547 |         -17.2001 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0312 |          35.6296 |         -18.0826 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0368 |          35.3911 |         -18.6625 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0421 |          35.0014 |         -19.3551 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0443 |          34.9729 |         -19.6415 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0467 |          34.5921 |         -20.5668 |
[32m[20221213 12:58:29 @agent_ppo2.py:179][0m |          -0.0469 |          34.3796 |         -20.6734 |
[32m[20221213 12:58:29 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.32
[32m[20221213 12:58:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.07
[32m[20221213 12:58:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 374.85
[32m[20221213 12:58:30 @agent_ppo2.py:137][0m Total time:      42.55 min
[32m[20221213 12:58:30 @agent_ppo2.py:139][0m 2951168 total steps have happened
[32m[20221213 12:58:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1441 --------------------------#
[32m[20221213 12:58:30 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:30 @agent_ppo2.py:179][0m |           0.1596 |          38.1636 |         -10.2213 |
[32m[20221213 12:58:30 @agent_ppo2.py:179][0m |           0.0807 |          37.0597 |          -5.0897 |
[32m[20221213 12:58:30 @agent_ppo2.py:179][0m |           0.0306 |          36.1093 |         -10.8447 |
[32m[20221213 12:58:30 @agent_ppo2.py:179][0m |           0.0034 |          35.8033 |         -14.2299 |
[32m[20221213 12:58:31 @agent_ppo2.py:179][0m |          -0.0012 |          36.3481 |         -15.2402 |
[32m[20221213 12:58:31 @agent_ppo2.py:179][0m |          -0.0002 |          35.3657 |         -14.3464 |
[32m[20221213 12:58:31 @agent_ppo2.py:179][0m |          -0.0228 |          35.1492 |         -16.5277 |
[32m[20221213 12:58:31 @agent_ppo2.py:179][0m |          -0.0310 |          34.9829 |         -17.8239 |
[32m[20221213 12:58:31 @agent_ppo2.py:179][0m |          -0.0311 |          34.9196 |         -18.2415 |
[32m[20221213 12:58:31 @agent_ppo2.py:179][0m |          -0.0375 |          34.7465 |         -18.7312 |
[32m[20221213 12:58:31 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.13
[32m[20221213 12:58:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.11
[32m[20221213 12:58:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 387.42
[32m[20221213 12:58:31 @agent_ppo2.py:137][0m Total time:      42.58 min
[32m[20221213 12:58:31 @agent_ppo2.py:139][0m 2953216 total steps have happened
[32m[20221213 12:58:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1442 --------------------------#
[32m[20221213 12:58:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:32 @agent_ppo2.py:179][0m |           0.0621 |          36.8472 |         -16.3737 |
[32m[20221213 12:58:32 @agent_ppo2.py:179][0m |           0.0254 |          35.7550 |         -14.3104 |
[32m[20221213 12:58:32 @agent_ppo2.py:179][0m |          -0.0066 |          35.1930 |         -15.3624 |
[32m[20221213 12:58:32 @agent_ppo2.py:179][0m |          -0.0236 |          34.9869 |         -16.3669 |
[32m[20221213 12:58:32 @agent_ppo2.py:179][0m |          -0.0335 |          34.7024 |         -16.9848 |
[32m[20221213 12:58:32 @agent_ppo2.py:179][0m |          -0.0342 |          34.4550 |         -18.0279 |
[32m[20221213 12:58:32 @agent_ppo2.py:179][0m |          -0.0407 |          34.3384 |         -17.9290 |
[32m[20221213 12:58:33 @agent_ppo2.py:179][0m |          -0.0419 |          34.2854 |         -18.5405 |
[32m[20221213 12:58:33 @agent_ppo2.py:179][0m |          -0.0378 |          37.1080 |         -18.9478 |
[32m[20221213 12:58:33 @agent_ppo2.py:179][0m |          -0.0451 |          34.0952 |         -19.5431 |
[32m[20221213 12:58:33 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 320.12
[32m[20221213 12:58:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 346.46
[32m[20221213 12:58:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.27
[32m[20221213 12:58:33 @agent_ppo2.py:137][0m Total time:      42.60 min
[32m[20221213 12:58:33 @agent_ppo2.py:139][0m 2955264 total steps have happened
[32m[20221213 12:58:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1443 --------------------------#
[32m[20221213 12:58:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:33 @agent_ppo2.py:179][0m |           0.0555 |          36.1559 |         -11.5545 |
[32m[20221213 12:58:33 @agent_ppo2.py:179][0m |           0.0439 |          34.7040 |          -8.8318 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |           0.0091 |          34.1760 |         -10.9517 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |          -0.0059 |          35.3044 |         -12.7379 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |          -0.0257 |          33.5558 |         -13.2989 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |          -0.0270 |          34.8606 |         -13.8071 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |          -0.0325 |          33.4071 |         -14.4131 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |          -0.0401 |          32.9372 |         -14.7642 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |          -0.0387 |          32.8655 |         -15.7048 |
[32m[20221213 12:58:34 @agent_ppo2.py:179][0m |          -0.0335 |          32.8836 |         -15.9500 |
[32m[20221213 12:58:34 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.17
[32m[20221213 12:58:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.71
[32m[20221213 12:58:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 369.51
[32m[20221213 12:58:35 @agent_ppo2.py:137][0m Total time:      42.63 min
[32m[20221213 12:58:35 @agent_ppo2.py:139][0m 2957312 total steps have happened
[32m[20221213 12:58:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1444 --------------------------#
[32m[20221213 12:58:35 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:35 @agent_ppo2.py:179][0m |           0.1100 |          37.1491 |          -9.5618 |
[32m[20221213 12:58:35 @agent_ppo2.py:179][0m |           0.0905 |          37.0590 |          -7.6232 |
[32m[20221213 12:58:35 @agent_ppo2.py:179][0m |           0.0282 |          35.9407 |         -10.6814 |
[32m[20221213 12:58:35 @agent_ppo2.py:179][0m |           0.0081 |          35.6106 |         -14.6584 |
[32m[20221213 12:58:35 @agent_ppo2.py:179][0m |           0.0029 |          35.6572 |         -15.4463 |
[32m[20221213 12:58:36 @agent_ppo2.py:179][0m |          -0.0118 |          35.2441 |         -15.3106 |
[32m[20221213 12:58:36 @agent_ppo2.py:179][0m |          -0.0211 |          35.1488 |         -16.7898 |
[32m[20221213 12:58:36 @agent_ppo2.py:179][0m |          -0.0231 |          35.0234 |         -17.4963 |
[32m[20221213 12:58:36 @agent_ppo2.py:179][0m |          -0.0274 |          34.9105 |         -17.0316 |
[32m[20221213 12:58:36 @agent_ppo2.py:179][0m |          -0.0310 |          34.7814 |         -18.5038 |
[32m[20221213 12:58:36 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:58:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 358.42
[32m[20221213 12:58:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 371.65
[32m[20221213 12:58:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 375.92
[32m[20221213 12:58:36 @agent_ppo2.py:137][0m Total time:      42.66 min
[32m[20221213 12:58:36 @agent_ppo2.py:139][0m 2959360 total steps have happened
[32m[20221213 12:58:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1445 --------------------------#
[32m[20221213 12:58:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |           0.0522 |          37.1833 |         -12.9643 |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |           0.0546 |          36.2717 |         -10.6168 |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |           0.0131 |          34.5153 |         -10.6660 |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |          -0.0076 |          34.0370 |         -13.0106 |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |          -0.0179 |          35.9637 |         -14.1598 |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |          -0.0309 |          33.3550 |         -15.2039 |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |          -0.0358 |          32.9977 |         -15.7893 |
[32m[20221213 12:58:37 @agent_ppo2.py:179][0m |          -0.0422 |          32.6857 |         -16.3964 |
[32m[20221213 12:58:38 @agent_ppo2.py:179][0m |          -0.0407 |          32.4571 |         -16.5996 |
[32m[20221213 12:58:38 @agent_ppo2.py:179][0m |          -0.0383 |          32.7618 |         -16.7028 |
[32m[20221213 12:58:38 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:58:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.49
[32m[20221213 12:58:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.08
[32m[20221213 12:58:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.90
[32m[20221213 12:58:38 @agent_ppo2.py:137][0m Total time:      42.69 min
[32m[20221213 12:58:38 @agent_ppo2.py:139][0m 2961408 total steps have happened
[32m[20221213 12:58:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1446 --------------------------#
[32m[20221213 12:58:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:38 @agent_ppo2.py:179][0m |           0.0784 |          38.1917 |         -12.0643 |
[32m[20221213 12:58:38 @agent_ppo2.py:179][0m |           0.0630 |          36.6864 |          -9.6392 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |           0.0260 |          36.2603 |         -10.7148 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |          -0.0001 |          36.7283 |         -11.6859 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |          -0.0127 |          35.8179 |         -11.7585 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |          -0.0268 |          35.3550 |         -12.6852 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |          -0.0356 |          35.1921 |         -13.7993 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |          -0.0371 |          35.0419 |         -14.3334 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |          -0.0406 |          35.0400 |         -14.6024 |
[32m[20221213 12:58:39 @agent_ppo2.py:179][0m |          -0.0454 |          34.7839 |         -14.9725 |
[32m[20221213 12:58:39 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:58:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 300.59
[32m[20221213 12:58:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 314.83
[32m[20221213 12:58:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 374.84
[32m[20221213 12:58:40 @agent_ppo2.py:137][0m Total time:      42.71 min
[32m[20221213 12:58:40 @agent_ppo2.py:139][0m 2963456 total steps have happened
[32m[20221213 12:58:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1447 --------------------------#
[32m[20221213 12:58:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:40 @agent_ppo2.py:179][0m |           0.0756 |          36.1797 |         -11.8295 |
[32m[20221213 12:58:40 @agent_ppo2.py:179][0m |           0.0522 |          34.1873 |          -7.8075 |
[32m[20221213 12:58:40 @agent_ppo2.py:179][0m |           0.0203 |          33.4907 |          -8.9600 |
[32m[20221213 12:58:40 @agent_ppo2.py:179][0m |          -0.0023 |          33.2007 |         -11.4578 |
[32m[20221213 12:58:40 @agent_ppo2.py:179][0m |          -0.0178 |          32.8605 |         -11.9801 |
[32m[20221213 12:58:41 @agent_ppo2.py:179][0m |          -0.0197 |          34.1323 |         -12.7125 |
[32m[20221213 12:58:41 @agent_ppo2.py:179][0m |          -0.0217 |          32.8812 |         -13.1398 |
[32m[20221213 12:58:41 @agent_ppo2.py:179][0m |          -0.0104 |          32.3766 |         -11.3598 |
[32m[20221213 12:58:41 @agent_ppo2.py:179][0m |          -0.0249 |          32.0584 |         -12.9919 |
[32m[20221213 12:58:41 @agent_ppo2.py:179][0m |          -0.0297 |          33.0607 |         -14.5805 |
[32m[20221213 12:58:41 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.56
[32m[20221213 12:58:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.34
[32m[20221213 12:58:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 401.01
[32m[20221213 12:58:41 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 401.01
[32m[20221213 12:58:41 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 401.01
[32m[20221213 12:58:41 @agent_ppo2.py:137][0m Total time:      42.74 min
[32m[20221213 12:58:41 @agent_ppo2.py:139][0m 2965504 total steps have happened
[32m[20221213 12:58:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1448 --------------------------#
[32m[20221213 12:58:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |           0.0424 |          33.9876 |         -12.1078 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |           0.0141 |          32.2367 |          -9.1591 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |          -0.0143 |          31.5912 |         -10.8272 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |          -0.0253 |          31.0666 |         -11.1047 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |          -0.0302 |          30.6698 |         -11.5684 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |          -0.0340 |          30.4145 |         -12.3567 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |          -0.0377 |          30.0441 |         -12.9220 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |          -0.0357 |          29.7929 |         -13.3678 |
[32m[20221213 12:58:42 @agent_ppo2.py:179][0m |          -0.0388 |          29.7032 |         -14.1116 |
[32m[20221213 12:58:43 @agent_ppo2.py:179][0m |          -0.0345 |          29.3426 |         -13.6718 |
[32m[20221213 12:58:43 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:58:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.54
[32m[20221213 12:58:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.61
[32m[20221213 12:58:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.74
[32m[20221213 12:58:43 @agent_ppo2.py:137][0m Total time:      42.77 min
[32m[20221213 12:58:43 @agent_ppo2.py:139][0m 2967552 total steps have happened
[32m[20221213 12:58:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1449 --------------------------#
[32m[20221213 12:58:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:43 @agent_ppo2.py:179][0m |           0.0707 |          34.9093 |         -11.7787 |
[32m[20221213 12:58:43 @agent_ppo2.py:179][0m |           0.0737 |          33.7703 |          -8.3202 |
[32m[20221213 12:58:43 @agent_ppo2.py:179][0m |           0.0229 |          33.2875 |         -10.5342 |
[32m[20221213 12:58:44 @agent_ppo2.py:179][0m |          -0.0035 |          33.1179 |         -10.7293 |
[32m[20221213 12:58:44 @agent_ppo2.py:179][0m |          -0.0097 |          32.9685 |         -11.7873 |
[32m[20221213 12:58:44 @agent_ppo2.py:179][0m |          -0.0221 |          32.5906 |         -12.4231 |
[32m[20221213 12:58:44 @agent_ppo2.py:179][0m |          -0.0282 |          32.4567 |         -13.1960 |
[32m[20221213 12:58:44 @agent_ppo2.py:179][0m |          -0.0319 |          32.2031 |         -13.5907 |
[32m[20221213 12:58:44 @agent_ppo2.py:179][0m |          -0.0352 |          32.0560 |         -13.9093 |
[32m[20221213 12:58:44 @agent_ppo2.py:179][0m |          -0.0394 |          31.9807 |         -14.3007 |
[32m[20221213 12:58:44 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.62
[32m[20221213 12:58:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.14
[32m[20221213 12:58:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 394.19
[32m[20221213 12:58:44 @agent_ppo2.py:137][0m Total time:      42.79 min
[32m[20221213 12:58:44 @agent_ppo2.py:139][0m 2969600 total steps have happened
[32m[20221213 12:58:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1450 --------------------------#
[32m[20221213 12:58:45 @agent_ppo2.py:121][0m Sampling time: 0.23 s by 5 slaves
[32m[20221213 12:58:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:45 @agent_ppo2.py:179][0m |           0.0467 |          35.9157 |          -8.6398 |
[32m[20221213 12:58:45 @agent_ppo2.py:179][0m |           0.0559 |          34.6576 |          -5.6272 |
[32m[20221213 12:58:45 @agent_ppo2.py:179][0m |           0.0168 |          33.2012 |          -5.4401 |
[32m[20221213 12:58:45 @agent_ppo2.py:179][0m |          -0.0105 |          32.4998 |          -7.1689 |
[32m[20221213 12:58:45 @agent_ppo2.py:179][0m |          -0.0238 |          32.0991 |          -8.0236 |
[32m[20221213 12:58:45 @agent_ppo2.py:179][0m |          -0.0326 |          31.6674 |          -8.5473 |
[32m[20221213 12:58:46 @agent_ppo2.py:179][0m |          -0.0390 |          31.3492 |          -8.7184 |
[32m[20221213 12:58:46 @agent_ppo2.py:179][0m |          -0.0409 |          31.1229 |          -8.8668 |
[32m[20221213 12:58:46 @agent_ppo2.py:179][0m |          -0.0432 |          30.9531 |          -9.2024 |
[32m[20221213 12:58:46 @agent_ppo2.py:179][0m |          -0.0415 |          31.0492 |          -9.2881 |
[32m[20221213 12:58:46 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:58:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.97
[32m[20221213 12:58:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.13
[32m[20221213 12:58:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 385.90
[32m[20221213 12:58:46 @agent_ppo2.py:137][0m Total time:      42.82 min
[32m[20221213 12:58:46 @agent_ppo2.py:139][0m 2971648 total steps have happened
[32m[20221213 12:58:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1451 --------------------------#
[32m[20221213 12:58:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:46 @agent_ppo2.py:179][0m |           0.0550 |          37.3545 |          -7.3746 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |           0.0571 |          36.1856 |          -3.2928 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |           0.0017 |          35.7315 |          -5.9560 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |          -0.0248 |          35.6649 |          -7.0759 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |          -0.0227 |          35.3790 |          -7.6929 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |          -0.0349 |          35.2119 |          -7.9801 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |          -0.0419 |          35.0382 |          -8.2605 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |          -0.0387 |          34.9439 |          -8.5776 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |          -0.0319 |          36.3605 |          -8.8010 |
[32m[20221213 12:58:47 @agent_ppo2.py:179][0m |          -0.0361 |          36.6308 |          -9.0989 |
[32m[20221213 12:58:47 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.48
[32m[20221213 12:58:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.13
[32m[20221213 12:58:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 368.14
[32m[20221213 12:58:48 @agent_ppo2.py:137][0m Total time:      42.85 min
[32m[20221213 12:58:48 @agent_ppo2.py:139][0m 2973696 total steps have happened
[32m[20221213 12:58:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1452 --------------------------#
[32m[20221213 12:58:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:48 @agent_ppo2.py:179][0m |           0.0434 |          37.4124 |          -6.5061 |
[32m[20221213 12:58:48 @agent_ppo2.py:179][0m |           0.0277 |          39.4686 |          -5.8788 |
[32m[20221213 12:58:48 @agent_ppo2.py:179][0m |           0.0000 |          35.4215 |          -6.2399 |
[32m[20221213 12:58:48 @agent_ppo2.py:179][0m |          -0.0091 |          35.1154 |          -6.4842 |
[32m[20221213 12:58:49 @agent_ppo2.py:179][0m |          -0.0265 |          34.9239 |          -7.4565 |
[32m[20221213 12:58:49 @agent_ppo2.py:179][0m |          -0.0348 |          34.7704 |          -7.7260 |
[32m[20221213 12:58:49 @agent_ppo2.py:179][0m |          -0.0399 |          34.9946 |          -8.1891 |
[32m[20221213 12:58:49 @agent_ppo2.py:179][0m |          -0.0404 |          34.4588 |          -8.9742 |
[32m[20221213 12:58:49 @agent_ppo2.py:179][0m |          -0.0439 |          34.3290 |          -9.4744 |
[32m[20221213 12:58:49 @agent_ppo2.py:179][0m |          -0.0422 |          34.1517 |          -9.6791 |
[32m[20221213 12:58:49 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:58:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.54
[32m[20221213 12:58:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 362.04
[32m[20221213 12:58:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 367.62
[32m[20221213 12:58:49 @agent_ppo2.py:137][0m Total time:      42.88 min
[32m[20221213 12:58:49 @agent_ppo2.py:139][0m 2975744 total steps have happened
[32m[20221213 12:58:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1453 --------------------------#
[32m[20221213 12:58:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:50 @agent_ppo2.py:179][0m |           0.1388 |          34.5915 |          -4.9782 |
[32m[20221213 12:58:50 @agent_ppo2.py:179][0m |           0.0620 |          32.9520 |          -4.6289 |
[32m[20221213 12:58:50 @agent_ppo2.py:179][0m |           0.0351 |          33.5766 |          -4.0866 |
[32m[20221213 12:58:50 @agent_ppo2.py:179][0m |          -0.0001 |          31.7453 |          -6.8523 |
[32m[20221213 12:58:50 @agent_ppo2.py:179][0m |          -0.0142 |          31.4433 |          -8.1686 |
[32m[20221213 12:58:50 @agent_ppo2.py:179][0m |          -0.0205 |          31.2305 |          -8.8671 |
[32m[20221213 12:58:50 @agent_ppo2.py:179][0m |          -0.0267 |          30.9256 |          -8.7905 |
[32m[20221213 12:58:51 @agent_ppo2.py:179][0m |          -0.0334 |          30.7593 |          -9.7488 |
[32m[20221213 12:58:51 @agent_ppo2.py:179][0m |          -0.0348 |          30.5390 |          -9.9703 |
[32m[20221213 12:58:51 @agent_ppo2.py:179][0m |          -0.0327 |          30.2728 |         -10.4601 |
[32m[20221213 12:58:51 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 332.31
[32m[20221213 12:58:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 350.15
[32m[20221213 12:58:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.89
[32m[20221213 12:58:51 @agent_ppo2.py:137][0m Total time:      42.91 min
[32m[20221213 12:58:51 @agent_ppo2.py:139][0m 2977792 total steps have happened
[32m[20221213 12:58:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1454 --------------------------#
[32m[20221213 12:58:51 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:58:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:51 @agent_ppo2.py:179][0m |           0.0578 |          36.5395 |          -7.1264 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |           0.0399 |          34.9882 |          -4.4091 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |           0.0123 |          34.4230 |          -6.2587 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |          -0.0116 |          34.0349 |          -7.3909 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |          -0.0210 |          33.9831 |          -8.3235 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |          -0.0246 |          33.9599 |          -8.3354 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |          -0.0253 |          33.5310 |          -8.0424 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |          -0.0318 |          33.4469 |          -8.1914 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |          -0.0383 |          33.3403 |          -9.0454 |
[32m[20221213 12:58:52 @agent_ppo2.py:179][0m |          -0.0355 |          33.4030 |          -9.1113 |
[32m[20221213 12:58:52 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 360.79
[32m[20221213 12:58:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 376.37
[32m[20221213 12:58:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 408.32
[32m[20221213 12:58:53 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 408.32
[32m[20221213 12:58:53 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 408.32
[32m[20221213 12:58:53 @agent_ppo2.py:137][0m Total time:      42.93 min
[32m[20221213 12:58:53 @agent_ppo2.py:139][0m 2979840 total steps have happened
[32m[20221213 12:58:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1455 --------------------------#
[32m[20221213 12:58:53 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:58:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:53 @agent_ppo2.py:179][0m |           0.0592 |          35.3500 |          -6.2882 |
[32m[20221213 12:58:53 @agent_ppo2.py:179][0m |           0.0816 |          34.5523 |          -1.9183 |
[32m[20221213 12:58:53 @agent_ppo2.py:179][0m |           0.0496 |          34.0535 |          -3.4122 |
[32m[20221213 12:58:53 @agent_ppo2.py:179][0m |           0.0040 |          34.6251 |          -5.9493 |
[32m[20221213 12:58:54 @agent_ppo2.py:179][0m |          -0.0146 |          33.6326 |          -7.1259 |
[32m[20221213 12:58:54 @agent_ppo2.py:179][0m |          -0.0223 |          33.3554 |          -8.0871 |
[32m[20221213 12:58:54 @agent_ppo2.py:179][0m |          -0.0315 |          33.2351 |          -8.7488 |
[32m[20221213 12:58:54 @agent_ppo2.py:179][0m |          -0.0324 |          33.0626 |          -9.3395 |
[32m[20221213 12:58:54 @agent_ppo2.py:179][0m |          -0.0366 |          33.0069 |         -10.0261 |
[32m[20221213 12:58:54 @agent_ppo2.py:179][0m |          -0.0369 |          32.9065 |         -10.7534 |
[32m[20221213 12:58:54 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:58:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.56
[32m[20221213 12:58:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.96
[32m[20221213 12:58:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 375.65
[32m[20221213 12:58:54 @agent_ppo2.py:137][0m Total time:      42.96 min
[32m[20221213 12:58:54 @agent_ppo2.py:139][0m 2981888 total steps have happened
[32m[20221213 12:58:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1456 --------------------------#
[32m[20221213 12:58:55 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:55 @agent_ppo2.py:179][0m |           0.0922 |          36.6323 |          -7.3993 |
[32m[20221213 12:58:55 @agent_ppo2.py:179][0m |           0.0731 |          38.6194 |          -3.7249 |
[32m[20221213 12:58:55 @agent_ppo2.py:179][0m |           0.0296 |          39.3313 |          -5.5948 |
[32m[20221213 12:58:55 @agent_ppo2.py:179][0m |          -0.0043 |          34.4564 |          -7.4148 |
[32m[20221213 12:58:55 @agent_ppo2.py:179][0m |          -0.0151 |          34.1425 |          -7.9339 |
[32m[20221213 12:58:55 @agent_ppo2.py:179][0m |          -0.0170 |          34.6855 |          -8.4797 |
[32m[20221213 12:58:55 @agent_ppo2.py:179][0m |          -0.0254 |          33.8016 |          -9.5413 |
[32m[20221213 12:58:56 @agent_ppo2.py:179][0m |          -0.0205 |          33.5137 |          -9.2435 |
[32m[20221213 12:58:56 @agent_ppo2.py:179][0m |          -0.0324 |          33.5185 |         -10.4018 |
[32m[20221213 12:58:56 @agent_ppo2.py:179][0m |          -0.0376 |          33.4261 |         -11.0491 |
[32m[20221213 12:58:56 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:58:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.23
[32m[20221213 12:58:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.98
[32m[20221213 12:58:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 404.07
[32m[20221213 12:58:56 @agent_ppo2.py:137][0m Total time:      42.99 min
[32m[20221213 12:58:56 @agent_ppo2.py:139][0m 2983936 total steps have happened
[32m[20221213 12:58:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1457 --------------------------#
[32m[20221213 12:58:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:56 @agent_ppo2.py:179][0m |           0.0980 |          34.8764 |          -5.1672 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |           0.0869 |          34.1955 |          -1.2861 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |           0.0316 |          33.8994 |          -2.6302 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |           0.0170 |          33.7404 |          -3.8134 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |           0.0019 |          33.5737 |          -5.1011 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |          -0.0066 |          33.5011 |          -6.3085 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |          -0.0163 |          33.4299 |          -7.0381 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |          -0.0161 |          33.7064 |          -7.4740 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |          -0.0241 |          33.2054 |          -7.7563 |
[32m[20221213 12:58:57 @agent_ppo2.py:179][0m |          -0.0253 |          33.1586 |          -8.4176 |
[32m[20221213 12:58:57 @agent_ppo2.py:124][0m Policy update time: 1.34 s
[32m[20221213 12:58:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 367.11
[32m[20221213 12:58:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 369.58
[32m[20221213 12:58:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 80.64
[32m[20221213 12:58:58 @agent_ppo2.py:137][0m Total time:      43.02 min
[32m[20221213 12:58:58 @agent_ppo2.py:139][0m 2985984 total steps have happened
[32m[20221213 12:58:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1458 --------------------------#
[32m[20221213 12:58:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:58:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:58:58 @agent_ppo2.py:179][0m |           0.0659 |          36.3592 |          -5.1512 |
[32m[20221213 12:58:58 @agent_ppo2.py:179][0m |           0.0526 |          35.4431 |          -3.1173 |
[32m[20221213 12:58:58 @agent_ppo2.py:179][0m |           0.0137 |          35.1203 |          -5.0112 |
[32m[20221213 12:58:58 @agent_ppo2.py:179][0m |          -0.0116 |          34.8905 |          -6.2706 |
[32m[20221213 12:58:59 @agent_ppo2.py:179][0m |          -0.0204 |          34.7216 |          -6.8440 |
[32m[20221213 12:58:59 @agent_ppo2.py:179][0m |          -0.0294 |          34.5571 |          -7.0282 |
[32m[20221213 12:58:59 @agent_ppo2.py:179][0m |          -0.0316 |          34.4504 |          -7.6180 |
[32m[20221213 12:58:59 @agent_ppo2.py:179][0m |          -0.0311 |          34.3318 |          -7.7695 |
[32m[20221213 12:58:59 @agent_ppo2.py:179][0m |          -0.0350 |          34.2205 |          -8.1329 |
[32m[20221213 12:58:59 @agent_ppo2.py:179][0m |          -0.0379 |          34.1614 |          -8.2689 |
[32m[20221213 12:58:59 @agent_ppo2.py:124][0m Policy update time: 1.38 s
[32m[20221213 12:58:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.35
[32m[20221213 12:58:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.22
[32m[20221213 12:58:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 397.03
[32m[20221213 12:58:59 @agent_ppo2.py:137][0m Total time:      43.05 min
[32m[20221213 12:58:59 @agent_ppo2.py:139][0m 2988032 total steps have happened
[32m[20221213 12:58:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1459 --------------------------#
[32m[20221213 12:59:00 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:00 @agent_ppo2.py:179][0m |           0.0624 |          34.9561 |          -5.7113 |
[32m[20221213 12:59:00 @agent_ppo2.py:179][0m |           0.0423 |          33.9386 |          -3.4118 |
[32m[20221213 12:59:00 @agent_ppo2.py:179][0m |           0.0093 |          33.1237 |          -3.8565 |
[32m[20221213 12:59:00 @agent_ppo2.py:179][0m |          -0.0070 |          33.1551 |          -4.9844 |
[32m[20221213 12:59:00 @agent_ppo2.py:179][0m |          -0.0234 |          32.1131 |          -5.7392 |
[32m[20221213 12:59:01 @agent_ppo2.py:179][0m |          -0.0324 |          31.6857 |          -6.1352 |
[32m[20221213 12:59:01 @agent_ppo2.py:179][0m |          -0.0357 |          31.1720 |          -6.6061 |
[32m[20221213 12:59:01 @agent_ppo2.py:179][0m |          -0.0299 |          32.5145 |          -7.1460 |
[32m[20221213 12:59:01 @agent_ppo2.py:179][0m |          -0.0371 |          30.6632 |          -7.5730 |
[32m[20221213 12:59:01 @agent_ppo2.py:179][0m |          -0.0267 |          30.9518 |          -7.2122 |
[32m[20221213 12:59:01 @agent_ppo2.py:124][0m Policy update time: 1.33 s
[32m[20221213 12:59:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 310.33
[32m[20221213 12:59:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 354.27
[32m[20221213 12:59:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 394.02
[32m[20221213 12:59:01 @agent_ppo2.py:137][0m Total time:      43.08 min
[32m[20221213 12:59:01 @agent_ppo2.py:139][0m 2990080 total steps have happened
[32m[20221213 12:59:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1460 --------------------------#
[32m[20221213 12:59:01 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:59:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:02 @agent_ppo2.py:179][0m |           0.0548 |          36.3242 |          -6.3331 |
[32m[20221213 12:59:02 @agent_ppo2.py:179][0m |           0.0282 |          34.5564 |          -5.5178 |
[32m[20221213 12:59:02 @agent_ppo2.py:179][0m |           0.0037 |          33.9997 |          -6.1264 |
[32m[20221213 12:59:02 @agent_ppo2.py:179][0m |          -0.0148 |          33.6042 |          -6.9169 |
[32m[20221213 12:59:02 @agent_ppo2.py:179][0m |          -0.0273 |          33.3882 |          -8.2312 |
[32m[20221213 12:59:02 @agent_ppo2.py:179][0m |          -0.0311 |          33.7652 |          -8.4573 |
[32m[20221213 12:59:02 @agent_ppo2.py:179][0m |          -0.0363 |          32.9625 |          -8.7921 |
[32m[20221213 12:59:03 @agent_ppo2.py:179][0m |          -0.0369 |          32.7499 |          -8.9534 |
[32m[20221213 12:59:03 @agent_ppo2.py:179][0m |          -0.0412 |          32.5789 |         -10.0053 |
[32m[20221213 12:59:03 @agent_ppo2.py:179][0m |          -0.0432 |          32.4496 |         -10.1699 |
[32m[20221213 12:59:03 @agent_ppo2.py:124][0m Policy update time: 1.29 s
[32m[20221213 12:59:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 350.08
[32m[20221213 12:59:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 381.85
[32m[20221213 12:59:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 392.08
[32m[20221213 12:59:03 @agent_ppo2.py:137][0m Total time:      43.10 min
[32m[20221213 12:59:03 @agent_ppo2.py:139][0m 2992128 total steps have happened
[32m[20221213 12:59:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1461 --------------------------#
[32m[20221213 12:59:03 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:03 @agent_ppo2.py:179][0m |           0.0830 |          39.1541 |          -5.3108 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |           0.0768 |          36.8658 |          -2.7918 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |           0.0304 |          36.5610 |          -3.5897 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |           0.0025 |          34.5319 |          -4.7119 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |          -0.0101 |          34.1851 |          -5.8187 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |          -0.0187 |          34.1063 |          -6.8141 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |          -0.0251 |          33.9744 |          -7.2688 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |          -0.0175 |          33.8848 |          -7.5606 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |          -0.0191 |          36.5818 |          -8.2822 |
[32m[20221213 12:59:04 @agent_ppo2.py:179][0m |          -0.0334 |          33.9033 |          -8.7295 |
[32m[20221213 12:59:04 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:59:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 384.79
[32m[20221213 12:59:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 397.27
[32m[20221213 12:59:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 366.91
[32m[20221213 12:59:05 @agent_ppo2.py:137][0m Total time:      43.13 min
[32m[20221213 12:59:05 @agent_ppo2.py:139][0m 2994176 total steps have happened
[32m[20221213 12:59:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1462 --------------------------#
[32m[20221213 12:59:05 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:05 @agent_ppo2.py:179][0m |           0.0534 |          36.2881 |          -6.1364 |
[32m[20221213 12:59:05 @agent_ppo2.py:179][0m |           0.0935 |          35.4226 |          -1.3693 |
[32m[20221213 12:59:05 @agent_ppo2.py:179][0m |           0.0201 |          34.9939 |          -2.1724 |
[32m[20221213 12:59:05 @agent_ppo2.py:179][0m |          -0.0028 |          35.0678 |          -4.4882 |
[32m[20221213 12:59:05 @agent_ppo2.py:179][0m |          -0.0205 |          34.5837 |          -5.7644 |
[32m[20221213 12:59:06 @agent_ppo2.py:179][0m |          -0.0279 |          34.4780 |          -6.4821 |
[32m[20221213 12:59:06 @agent_ppo2.py:179][0m |          -0.0205 |          39.8105 |          -7.0930 |
[32m[20221213 12:59:06 @agent_ppo2.py:179][0m |          -0.0311 |          34.3403 |          -7.6437 |
[32m[20221213 12:59:06 @agent_ppo2.py:179][0m |          -0.0264 |          34.5220 |          -7.3591 |
[32m[20221213 12:59:06 @agent_ppo2.py:179][0m |          -0.0353 |          34.0457 |          -8.5783 |
[32m[20221213 12:59:06 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 353.13
[32m[20221213 12:59:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.76
[32m[20221213 12:59:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 375.14
[32m[20221213 12:59:06 @agent_ppo2.py:137][0m Total time:      43.16 min
[32m[20221213 12:59:06 @agent_ppo2.py:139][0m 2996224 total steps have happened
[32m[20221213 12:59:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1463 --------------------------#
[32m[20221213 12:59:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |           0.0514 |          37.5303 |          -5.4902 |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |           0.0342 |          36.6584 |          -2.8340 |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |          -0.0031 |          36.0960 |          -4.4540 |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |          -0.0140 |          35.8272 |          -4.5985 |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |          -0.0185 |          36.1351 |          -4.9401 |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |          -0.0152 |          35.3464 |          -3.8882 |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |          -0.0247 |          35.2877 |          -5.0726 |
[32m[20221213 12:59:07 @agent_ppo2.py:179][0m |          -0.0366 |          35.0608 |          -5.7932 |
[32m[20221213 12:59:08 @agent_ppo2.py:179][0m |          -0.0345 |          34.9841 |          -5.9220 |
[32m[20221213 12:59:08 @agent_ppo2.py:179][0m |          -0.0337 |          34.8704 |          -5.8981 |
[32m[20221213 12:59:08 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.11
[32m[20221213 12:59:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.13
[32m[20221213 12:59:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.62
[32m[20221213 12:59:08 @agent_ppo2.py:137][0m Total time:      43.19 min
[32m[20221213 12:59:08 @agent_ppo2.py:139][0m 2998272 total steps have happened
[32m[20221213 12:59:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1464 --------------------------#
[32m[20221213 12:59:08 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:08 @agent_ppo2.py:179][0m |           0.1073 |          37.3752 |          -4.1672 |
[32m[20221213 12:59:08 @agent_ppo2.py:179][0m |           0.0686 |          37.9607 |          -1.5855 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |           0.0146 |          36.4322 |          -4.3580 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |          -0.0121 |          36.1551 |          -5.1936 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |          -0.0177 |          35.9582 |          -5.1798 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |          -0.0226 |          35.8681 |          -5.6713 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |          -0.0284 |          35.7382 |          -6.3774 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |          -0.0348 |          35.6071 |          -7.0939 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |          -0.0388 |          35.5201 |          -7.2269 |
[32m[20221213 12:59:09 @agent_ppo2.py:179][0m |          -0.0366 |          35.5332 |          -7.4134 |
[32m[20221213 12:59:09 @agent_ppo2.py:124][0m Policy update time: 1.19 s
[32m[20221213 12:59:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 337.10
[32m[20221213 12:59:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.62
[32m[20221213 12:59:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 402.31
[32m[20221213 12:59:10 @agent_ppo2.py:137][0m Total time:      43.21 min
[32m[20221213 12:59:10 @agent_ppo2.py:139][0m 3000320 total steps have happened
[32m[20221213 12:59:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1465 --------------------------#
[32m[20221213 12:59:10 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:10 @agent_ppo2.py:179][0m |           0.0550 |          37.1721 |          -5.3355 |
[32m[20221213 12:59:10 @agent_ppo2.py:179][0m |           0.0578 |          37.2094 |          -3.6754 |
[32m[20221213 12:59:10 @agent_ppo2.py:179][0m |           0.0093 |          35.8681 |          -4.2886 |
[32m[20221213 12:59:10 @agent_ppo2.py:179][0m |          -0.0098 |          35.6101 |          -5.1745 |
[32m[20221213 12:59:10 @agent_ppo2.py:179][0m |          -0.0182 |          35.3637 |          -5.4175 |
[32m[20221213 12:59:10 @agent_ppo2.py:179][0m |          -0.0237 |          35.2175 |          -5.8349 |
[32m[20221213 12:59:11 @agent_ppo2.py:179][0m |          -0.0320 |          35.0969 |          -6.3898 |
[32m[20221213 12:59:11 @agent_ppo2.py:179][0m |          -0.0249 |          34.9370 |          -6.4431 |
[32m[20221213 12:59:11 @agent_ppo2.py:179][0m |          -0.0223 |          39.6429 |          -6.3036 |
[32m[20221213 12:59:11 @agent_ppo2.py:179][0m |          -0.0343 |          34.9035 |          -6.4005 |
[32m[20221213 12:59:11 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:59:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.72
[32m[20221213 12:59:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.15
[32m[20221213 12:59:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 367.18
[32m[20221213 12:59:11 @agent_ppo2.py:137][0m Total time:      43.24 min
[32m[20221213 12:59:11 @agent_ppo2.py:139][0m 3002368 total steps have happened
[32m[20221213 12:59:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1466 --------------------------#
[32m[20221213 12:59:11 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |           0.0855 |          38.3997 |          -3.2499 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |           0.0564 |          37.0768 |           0.2570 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |           0.0155 |          34.4569 |          -1.1896 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |          -0.0010 |          33.7618 |          -2.0133 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |          -0.0142 |          33.2217 |          -2.3182 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |          -0.0196 |          32.8679 |          -3.1951 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |          -0.0250 |          32.7217 |          -3.8061 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |          -0.0326 |          32.2262 |          -4.2162 |
[32m[20221213 12:59:12 @agent_ppo2.py:179][0m |          -0.0348 |          32.0536 |          -4.7420 |
[32m[20221213 12:59:13 @agent_ppo2.py:179][0m |          -0.0349 |          31.8785 |          -4.9566 |
[32m[20221213 12:59:13 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:59:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 369.89
[32m[20221213 12:59:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 389.98
[32m[20221213 12:59:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.89
[32m[20221213 12:59:13 @agent_ppo2.py:137][0m Total time:      43.27 min
[32m[20221213 12:59:13 @agent_ppo2.py:139][0m 3004416 total steps have happened
[32m[20221213 12:59:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1467 --------------------------#
[32m[20221213 12:59:13 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:13 @agent_ppo2.py:179][0m |           0.1140 |          40.5587 |          -2.7864 |
[32m[20221213 12:59:13 @agent_ppo2.py:179][0m |           0.0793 |          40.3811 |          -1.7772 |
[32m[20221213 12:59:13 @agent_ppo2.py:179][0m |           0.0335 |          37.2623 |          -2.7512 |
[32m[20221213 12:59:14 @agent_ppo2.py:179][0m |           0.0119 |          38.9088 |          -4.3566 |
[32m[20221213 12:59:14 @agent_ppo2.py:179][0m |          -0.0068 |          36.4915 |          -5.4420 |
[32m[20221213 12:59:14 @agent_ppo2.py:179][0m |          -0.0207 |          36.2403 |          -5.2688 |
[32m[20221213 12:59:14 @agent_ppo2.py:179][0m |          -0.0279 |          36.0558 |          -5.8559 |
[32m[20221213 12:59:14 @agent_ppo2.py:179][0m |          -0.0298 |          35.8768 |          -5.8820 |
[32m[20221213 12:59:14 @agent_ppo2.py:179][0m |          -0.0325 |          35.7932 |          -6.1681 |
[32m[20221213 12:59:14 @agent_ppo2.py:179][0m |          -0.0313 |          35.6199 |          -6.9514 |
[32m[20221213 12:59:14 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 345.27
[32m[20221213 12:59:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 361.58
[32m[20221213 12:59:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 397.96
[32m[20221213 12:59:14 @agent_ppo2.py:137][0m Total time:      43.30 min
[32m[20221213 12:59:14 @agent_ppo2.py:139][0m 3006464 total steps have happened
[32m[20221213 12:59:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1468 --------------------------#
[32m[20221213 12:59:15 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:15 @agent_ppo2.py:179][0m |           0.0587 |          37.9524 |          -3.8050 |
[32m[20221213 12:59:15 @agent_ppo2.py:179][0m |           0.0695 |          37.2002 |          -1.1473 |
[32m[20221213 12:59:15 @agent_ppo2.py:179][0m |           0.0064 |          36.6691 |          -3.1556 |
[32m[20221213 12:59:15 @agent_ppo2.py:179][0m |          -0.0141 |          36.4515 |          -3.9489 |
[32m[20221213 12:59:15 @agent_ppo2.py:179][0m |          -0.0139 |          38.8888 |          -4.8479 |
[32m[20221213 12:59:15 @agent_ppo2.py:179][0m |          -0.0330 |          35.9951 |          -5.1751 |
[32m[20221213 12:59:16 @agent_ppo2.py:179][0m |          -0.0355 |          35.8828 |          -5.7632 |
[32m[20221213 12:59:16 @agent_ppo2.py:179][0m |          -0.0364 |          35.6525 |          -6.0689 |
[32m[20221213 12:59:16 @agent_ppo2.py:179][0m |          -0.0374 |          35.5278 |          -6.5588 |
[32m[20221213 12:59:16 @agent_ppo2.py:179][0m |          -0.0406 |          35.5084 |          -6.7027 |
[32m[20221213 12:59:16 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.83
[32m[20221213 12:59:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 351.21
[32m[20221213 12:59:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 371.48
[32m[20221213 12:59:16 @agent_ppo2.py:137][0m Total time:      43.32 min
[32m[20221213 12:59:16 @agent_ppo2.py:139][0m 3008512 total steps have happened
[32m[20221213 12:59:16 @agent_ppo2.py:115][0m #------------------------ Iteration 1469 --------------------------#
[32m[20221213 12:59:16 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |           0.0502 |          38.2063 |          -4.8062 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |           0.0678 |          37.2575 |          -1.2922 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |           0.0183 |          36.8080 |          -2.9581 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |           0.0105 |          36.6520 |          -3.5116 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |           0.0021 |          41.2136 |          -3.6233 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |          -0.0208 |          36.3910 |          -4.4272 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |          -0.0267 |          36.1252 |          -5.2341 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |          -0.0282 |          36.0337 |          -5.9993 |
[32m[20221213 12:59:17 @agent_ppo2.py:179][0m |          -0.0293 |          35.8811 |          -6.4739 |
[32m[20221213 12:59:18 @agent_ppo2.py:179][0m |          -0.0289 |          35.7528 |          -6.5279 |
[32m[20221213 12:59:18 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 359.60
[32m[20221213 12:59:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 382.53
[32m[20221213 12:59:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 320.92
[32m[20221213 12:59:18 @agent_ppo2.py:137][0m Total time:      43.35 min
[32m[20221213 12:59:18 @agent_ppo2.py:139][0m 3010560 total steps have happened
[32m[20221213 12:59:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1470 --------------------------#
[32m[20221213 12:59:18 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:59:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:18 @agent_ppo2.py:179][0m |           0.0719 |          38.6007 |          -2.7635 |
[32m[20221213 12:59:18 @agent_ppo2.py:179][0m |           0.0454 |          36.9193 |          -1.9774 |
[32m[20221213 12:59:18 @agent_ppo2.py:179][0m |           0.0075 |          36.3663 |          -2.6226 |
[32m[20221213 12:59:19 @agent_ppo2.py:179][0m |          -0.0178 |          35.8739 |          -4.1067 |
[32m[20221213 12:59:19 @agent_ppo2.py:179][0m |          -0.0268 |          35.4956 |          -4.4828 |
[32m[20221213 12:59:19 @agent_ppo2.py:179][0m |          -0.0347 |          35.4290 |          -5.0792 |
[32m[20221213 12:59:19 @agent_ppo2.py:179][0m |          -0.0355 |          35.1632 |          -5.2918 |
[32m[20221213 12:59:19 @agent_ppo2.py:179][0m |          -0.0306 |          39.1582 |          -5.8408 |
[32m[20221213 12:59:19 @agent_ppo2.py:179][0m |          -0.0419 |          34.9309 |          -5.9668 |
[32m[20221213 12:59:19 @agent_ppo2.py:179][0m |          -0.0445 |          34.6762 |          -6.2436 |
[32m[20221213 12:59:19 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.27
[32m[20221213 12:59:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 376.42
[32m[20221213 12:59:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 385.35
[32m[20221213 12:59:19 @agent_ppo2.py:137][0m Total time:      43.38 min
[32m[20221213 12:59:19 @agent_ppo2.py:139][0m 3012608 total steps have happened
[32m[20221213 12:59:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1471 --------------------------#
[32m[20221213 12:59:20 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:20 @agent_ppo2.py:179][0m |           0.0875 |          37.5677 |          -3.2622 |
[32m[20221213 12:59:20 @agent_ppo2.py:179][0m |           0.0404 |          36.6558 |          -1.6009 |
[32m[20221213 12:59:20 @agent_ppo2.py:179][0m |           0.0120 |          37.0153 |          -2.1856 |
[32m[20221213 12:59:20 @agent_ppo2.py:179][0m |          -0.0147 |          36.1622 |          -3.6690 |
[32m[20221213 12:59:20 @agent_ppo2.py:179][0m |          -0.0132 |          39.9635 |          -3.8647 |
[32m[20221213 12:59:20 @agent_ppo2.py:179][0m |          -0.0313 |          35.8821 |          -4.2947 |
[32m[20221213 12:59:20 @agent_ppo2.py:179][0m |          -0.0311 |          35.6623 |          -4.3647 |
[32m[20221213 12:59:21 @agent_ppo2.py:179][0m |          -0.0344 |          35.5750 |          -4.9868 |
[32m[20221213 12:59:21 @agent_ppo2.py:179][0m |          -0.0299 |          35.8596 |          -5.2735 |
[32m[20221213 12:59:21 @agent_ppo2.py:179][0m |          -0.0332 |          35.3753 |          -5.4738 |
[32m[20221213 12:59:21 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 347.66
[32m[20221213 12:59:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 375.42
[32m[20221213 12:59:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.90
[32m[20221213 12:59:21 @agent_ppo2.py:137][0m Total time:      43.40 min
[32m[20221213 12:59:21 @agent_ppo2.py:139][0m 3014656 total steps have happened
[32m[20221213 12:59:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1472 --------------------------#
[32m[20221213 12:59:21 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:21 @agent_ppo2.py:179][0m |           0.1083 |          37.7440 |          -2.9786 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |           0.0512 |          37.4332 |          -0.3246 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |           0.0087 |          36.5534 |          -2.9305 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |          -0.0055 |          36.3677 |          -3.3063 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |          -0.0149 |          36.1030 |          -3.8350 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |          -0.0276 |          35.9655 |          -4.5084 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |          -0.0309 |          35.8714 |          -5.0663 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |          -0.0345 |          35.7209 |          -5.2571 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |          -0.0281 |          37.6411 |          -6.0946 |
[32m[20221213 12:59:22 @agent_ppo2.py:179][0m |          -0.0404 |          35.5625 |          -6.4805 |
[32m[20221213 12:59:22 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.69
[32m[20221213 12:59:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 352.46
[32m[20221213 12:59:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 337.96
[32m[20221213 12:59:23 @agent_ppo2.py:137][0m Total time:      43.43 min
[32m[20221213 12:59:23 @agent_ppo2.py:139][0m 3016704 total steps have happened
[32m[20221213 12:59:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1473 --------------------------#
[32m[20221213 12:59:23 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:23 @agent_ppo2.py:179][0m |           0.0737 |          37.9544 |          -3.5951 |
[32m[20221213 12:59:23 @agent_ppo2.py:179][0m |           0.0622 |          36.9111 |           0.1373 |
[32m[20221213 12:59:23 @agent_ppo2.py:179][0m |           0.0188 |          36.1146 |          -2.4169 |
[32m[20221213 12:59:23 @agent_ppo2.py:179][0m |          -0.0021 |          35.8607 |          -3.8221 |
[32m[20221213 12:59:23 @agent_ppo2.py:179][0m |          -0.0176 |          35.5750 |          -4.5912 |
[32m[20221213 12:59:24 @agent_ppo2.py:179][0m |          -0.0097 |          35.3754 |          -4.6712 |
[32m[20221213 12:59:24 @agent_ppo2.py:179][0m |          -0.0221 |          35.2339 |          -4.8989 |
[32m[20221213 12:59:24 @agent_ppo2.py:179][0m |          -0.0297 |          35.1406 |          -5.9125 |
[32m[20221213 12:59:24 @agent_ppo2.py:179][0m |          -0.0304 |          35.0838 |          -6.4408 |
[32m[20221213 12:59:24 @agent_ppo2.py:179][0m |          -0.0342 |          34.8347 |          -6.5266 |
[32m[20221213 12:59:24 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:59:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 348.97
[32m[20221213 12:59:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.87
[32m[20221213 12:59:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.98
[32m[20221213 12:59:24 @agent_ppo2.py:137][0m Total time:      43.46 min
[32m[20221213 12:59:24 @agent_ppo2.py:139][0m 3018752 total steps have happened
[32m[20221213 12:59:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1474 --------------------------#
[32m[20221213 12:59:24 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:25 @agent_ppo2.py:179][0m |           0.0575 |          37.8952 |          -2.3534 |
[32m[20221213 12:59:25 @agent_ppo2.py:179][0m |           0.0524 |          38.0516 |          -0.9661 |
[32m[20221213 12:59:25 @agent_ppo2.py:179][0m |           0.0082 |          36.4452 |          -1.6348 |
[32m[20221213 12:59:25 @agent_ppo2.py:179][0m |          -0.0204 |          36.0285 |          -1.9592 |
[32m[20221213 12:59:25 @agent_ppo2.py:179][0m |          -0.0299 |          35.7167 |          -2.2147 |
[32m[20221213 12:59:25 @agent_ppo2.py:179][0m |          -0.0368 |          35.5820 |          -2.8651 |
[32m[20221213 12:59:25 @agent_ppo2.py:179][0m |          -0.0356 |          35.3311 |          -3.2011 |
[32m[20221213 12:59:26 @agent_ppo2.py:179][0m |          -0.0293 |          35.1621 |          -3.0214 |
[32m[20221213 12:59:26 @agent_ppo2.py:179][0m |          -0.0359 |          35.4934 |          -3.5806 |
[32m[20221213 12:59:26 @agent_ppo2.py:179][0m |          -0.0474 |          34.8272 |          -3.6795 |
[32m[20221213 12:59:26 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:59:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.27
[32m[20221213 12:59:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.29
[32m[20221213 12:59:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.46
[32m[20221213 12:59:26 @agent_ppo2.py:137][0m Total time:      43.49 min
[32m[20221213 12:59:26 @agent_ppo2.py:139][0m 3020800 total steps have happened
[32m[20221213 12:59:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1475 --------------------------#
[32m[20221213 12:59:26 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 12:59:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:26 @agent_ppo2.py:179][0m |           0.0455 |          38.2221 |          -3.3715 |
[32m[20221213 12:59:26 @agent_ppo2.py:179][0m |           0.0287 |          37.8807 |          -2.3240 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0063 |          37.7628 |          -3.6064 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0161 |          35.9696 |          -3.7065 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0149 |          35.4500 |          -3.5052 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0315 |          35.1797 |          -4.6180 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0376 |          34.8911 |          -5.2243 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0443 |          34.6043 |          -5.5218 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0462 |          34.3248 |          -6.4477 |
[32m[20221213 12:59:27 @agent_ppo2.py:179][0m |          -0.0458 |          34.1011 |          -6.5336 |
[32m[20221213 12:59:27 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:59:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 300.58
[32m[20221213 12:59:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.27
[32m[20221213 12:59:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 370.70
[32m[20221213 12:59:28 @agent_ppo2.py:137][0m Total time:      43.51 min
[32m[20221213 12:59:28 @agent_ppo2.py:139][0m 3022848 total steps have happened
[32m[20221213 12:59:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1476 --------------------------#
[32m[20221213 12:59:28 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:28 @agent_ppo2.py:179][0m |           0.0798 |          34.6285 |          -2.5387 |
[32m[20221213 12:59:28 @agent_ppo2.py:179][0m |           0.0446 |          32.1254 |          -0.7638 |
[32m[20221213 12:59:28 @agent_ppo2.py:179][0m |           0.0027 |          30.9966 |          -1.5124 |
[32m[20221213 12:59:28 @agent_ppo2.py:179][0m |          -0.0137 |          30.3471 |          -2.2780 |
[32m[20221213 12:59:28 @agent_ppo2.py:179][0m |          -0.0260 |          29.9228 |          -2.9255 |
[32m[20221213 12:59:29 @agent_ppo2.py:179][0m |          -0.0320 |          29.6292 |          -3.9045 |
[32m[20221213 12:59:29 @agent_ppo2.py:179][0m |          -0.0296 |          30.0832 |          -3.9128 |
[32m[20221213 12:59:29 @agent_ppo2.py:179][0m |          -0.0379 |          29.0713 |          -4.2939 |
[32m[20221213 12:59:29 @agent_ppo2.py:179][0m |          -0.0460 |          28.8889 |          -5.0049 |
[32m[20221213 12:59:29 @agent_ppo2.py:179][0m |          -0.0494 |          28.7831 |          -5.7094 |
[32m[20221213 12:59:29 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:59:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.96
[32m[20221213 12:59:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 353.70
[32m[20221213 12:59:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 377.95
[32m[20221213 12:59:29 @agent_ppo2.py:137][0m Total time:      43.54 min
[32m[20221213 12:59:29 @agent_ppo2.py:139][0m 3024896 total steps have happened
[32m[20221213 12:59:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1477 --------------------------#
[32m[20221213 12:59:29 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |           0.0927 |          38.0848 |          -4.6406 |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |           0.0522 |          35.9560 |          -1.3703 |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |          -0.0022 |          35.3360 |          -3.3283 |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |          -0.0186 |          34.8495 |          -4.1783 |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |          -0.0308 |          34.5138 |          -4.8967 |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |          -0.0340 |          34.2744 |          -4.9204 |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |          -0.0378 |          34.2012 |          -5.2126 |
[32m[20221213 12:59:30 @agent_ppo2.py:179][0m |          -0.0436 |          33.9445 |          -5.8980 |
[32m[20221213 12:59:31 @agent_ppo2.py:179][0m |          -0.0436 |          33.7666 |          -6.1760 |
[32m[20221213 12:59:31 @agent_ppo2.py:179][0m |          -0.0407 |          35.7725 |          -6.5984 |
[32m[20221213 12:59:31 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:59:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 328.29
[32m[20221213 12:59:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.50
[32m[20221213 12:59:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.88
[32m[20221213 12:59:31 @agent_ppo2.py:137][0m Total time:      43.57 min
[32m[20221213 12:59:31 @agent_ppo2.py:139][0m 3026944 total steps have happened
[32m[20221213 12:59:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1478 --------------------------#
[32m[20221213 12:59:31 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:31 @agent_ppo2.py:179][0m |           0.0584 |          37.7561 |          -2.7251 |
[32m[20221213 12:59:31 @agent_ppo2.py:179][0m |           0.0613 |          35.8714 |          -1.0275 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |           0.0185 |          35.5021 |          -1.7012 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |          -0.0072 |          35.3675 |          -2.9975 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |          -0.0177 |          34.9775 |          -3.5756 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |          -0.0183 |          35.5629 |          -3.7205 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |          -0.0137 |          36.9522 |          -3.6984 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |          -0.0297 |          34.5285 |          -4.6662 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |          -0.0324 |          34.4514 |          -4.5522 |
[32m[20221213 12:59:32 @agent_ppo2.py:179][0m |          -0.0320 |          34.3327 |          -5.1450 |
[32m[20221213 12:59:32 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:59:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 376.44
[32m[20221213 12:59:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 386.16
[32m[20221213 12:59:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.33
[32m[20221213 12:59:33 @agent_ppo2.py:137][0m Total time:      43.60 min
[32m[20221213 12:59:33 @agent_ppo2.py:139][0m 3028992 total steps have happened
[32m[20221213 12:59:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1479 --------------------------#
[32m[20221213 12:59:33 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:33 @agent_ppo2.py:179][0m |           0.0758 |          38.4859 |          -2.2920 |
[32m[20221213 12:59:33 @agent_ppo2.py:179][0m |           0.0510 |          36.7302 |          -0.1824 |
[32m[20221213 12:59:33 @agent_ppo2.py:179][0m |           0.0119 |          36.0477 |          -0.5428 |
[32m[20221213 12:59:33 @agent_ppo2.py:179][0m |          -0.0162 |          35.6373 |          -1.9739 |
[32m[20221213 12:59:33 @agent_ppo2.py:179][0m |          -0.0230 |          35.3040 |          -2.8391 |
[32m[20221213 12:59:34 @agent_ppo2.py:179][0m |          -0.0340 |          34.9694 |          -3.6602 |
[32m[20221213 12:59:34 @agent_ppo2.py:179][0m |          -0.0366 |          34.7409 |          -3.9830 |
[32m[20221213 12:59:34 @agent_ppo2.py:179][0m |          -0.0387 |          34.6067 |          -4.4828 |
[32m[20221213 12:59:34 @agent_ppo2.py:179][0m |          -0.0415 |          34.4436 |          -5.0965 |
[32m[20221213 12:59:34 @agent_ppo2.py:179][0m |          -0.0388 |          35.7358 |          -5.4805 |
[32m[20221213 12:59:34 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 340.05
[32m[20221213 12:59:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 358.99
[32m[20221213 12:59:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.93
[32m[20221213 12:59:34 @agent_ppo2.py:137][0m Total time:      43.62 min
[32m[20221213 12:59:34 @agent_ppo2.py:139][0m 3031040 total steps have happened
[32m[20221213 12:59:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1480 --------------------------#
[32m[20221213 12:59:34 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:59:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |           0.0810 |          37.3610 |          -4.0236 |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |           0.0529 |          33.7288 |           0.0789 |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |           0.0018 |          32.2320 |          -2.6426 |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |          -0.0127 |          31.3038 |          -3.0716 |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |          -0.0198 |          31.0262 |          -4.1525 |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |          -0.0330 |          30.5219 |          -4.8864 |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |          -0.0245 |          33.9722 |          -5.2878 |
[32m[20221213 12:59:35 @agent_ppo2.py:179][0m |          -0.0389 |          30.2635 |          -5.9334 |
[32m[20221213 12:59:36 @agent_ppo2.py:179][0m |          -0.0377 |          30.0466 |          -5.5784 |
[32m[20221213 12:59:36 @agent_ppo2.py:179][0m |          -0.0466 |          29.8358 |          -6.5343 |
[32m[20221213 12:59:36 @agent_ppo2.py:124][0m Policy update time: 1.20 s
[32m[20221213 12:59:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.63
[32m[20221213 12:59:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 376.41
[32m[20221213 12:59:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.31
[32m[20221213 12:59:36 @agent_ppo2.py:137][0m Total time:      43.65 min
[32m[20221213 12:59:36 @agent_ppo2.py:139][0m 3033088 total steps have happened
[32m[20221213 12:59:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1481 --------------------------#
[32m[20221213 12:59:36 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:36 @agent_ppo2.py:179][0m |           0.0598 |          42.3123 |          -3.3420 |
[32m[20221213 12:59:36 @agent_ppo2.py:179][0m |           0.0106 |          39.0326 |          -2.9684 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0131 |          37.7525 |          -3.2883 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0277 |          37.2093 |          -4.2883 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0392 |          36.5356 |          -4.6552 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0478 |          36.0801 |          -4.8722 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0522 |          35.6996 |          -5.4215 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0564 |          35.4295 |          -6.1055 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0536 |          35.2632 |          -5.7845 |
[32m[20221213 12:59:37 @agent_ppo2.py:179][0m |          -0.0623 |          35.0379 |          -6.6179 |
[32m[20221213 12:59:37 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:59:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.58
[32m[20221213 12:59:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.27
[32m[20221213 12:59:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.10
[32m[20221213 12:59:38 @agent_ppo2.py:137][0m Total time:      43.68 min
[32m[20221213 12:59:38 @agent_ppo2.py:139][0m 3035136 total steps have happened
[32m[20221213 12:59:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1482 --------------------------#
[32m[20221213 12:59:38 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:38 @agent_ppo2.py:179][0m |           0.0871 |          40.2965 |          -3.7774 |
[32m[20221213 12:59:38 @agent_ppo2.py:179][0m |           0.0381 |          38.8307 |          -3.1701 |
[32m[20221213 12:59:38 @agent_ppo2.py:179][0m |           0.0108 |          38.5042 |          -3.4787 |
[32m[20221213 12:59:38 @agent_ppo2.py:179][0m |          -0.0104 |          37.8098 |          -3.8727 |
[32m[20221213 12:59:39 @agent_ppo2.py:179][0m |          -0.0272 |          37.5353 |          -4.7630 |
[32m[20221213 12:59:39 @agent_ppo2.py:179][0m |          -0.0296 |          41.6658 |          -5.0165 |
[32m[20221213 12:59:39 @agent_ppo2.py:179][0m |          -0.0387 |          37.0951 |          -5.3277 |
[32m[20221213 12:59:39 @agent_ppo2.py:179][0m |          -0.0466 |          36.8793 |          -5.4168 |
[32m[20221213 12:59:39 @agent_ppo2.py:179][0m |          -0.0446 |          39.0594 |          -6.1162 |
[32m[20221213 12:59:39 @agent_ppo2.py:179][0m |          -0.0447 |          36.6598 |          -6.1585 |
[32m[20221213 12:59:39 @agent_ppo2.py:124][0m Policy update time: 1.35 s
[32m[20221213 12:59:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.05
[32m[20221213 12:59:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.64
[32m[20221213 12:59:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 393.30
[32m[20221213 12:59:39 @agent_ppo2.py:137][0m Total time:      43.71 min
[32m[20221213 12:59:39 @agent_ppo2.py:139][0m 3037184 total steps have happened
[32m[20221213 12:59:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1483 --------------------------#
[32m[20221213 12:59:40 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:40 @agent_ppo2.py:179][0m |           0.0353 |          38.5063 |          -5.4142 |
[32m[20221213 12:59:40 @agent_ppo2.py:179][0m |           0.0170 |          37.4871 |          -3.8289 |
[32m[20221213 12:59:40 @agent_ppo2.py:179][0m |          -0.0119 |          37.0047 |          -4.7526 |
[32m[20221213 12:59:40 @agent_ppo2.py:179][0m |          -0.0301 |          36.6870 |          -5.7377 |
[32m[20221213 12:59:40 @agent_ppo2.py:179][0m |          -0.0334 |          36.4384 |          -6.5285 |
[32m[20221213 12:59:40 @agent_ppo2.py:179][0m |          -0.0386 |          36.2694 |          -7.2662 |
[32m[20221213 12:59:40 @agent_ppo2.py:179][0m |          -0.0375 |          36.1656 |          -7.1323 |
[32m[20221213 12:59:41 @agent_ppo2.py:179][0m |          -0.0335 |          36.0581 |          -7.0451 |
[32m[20221213 12:59:41 @agent_ppo2.py:179][0m |          -0.0375 |          35.9363 |          -7.1456 |
[32m[20221213 12:59:41 @agent_ppo2.py:179][0m |          -0.0422 |          35.7771 |          -7.3634 |
[32m[20221213 12:59:41 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:59:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.26
[32m[20221213 12:59:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.66
[32m[20221213 12:59:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.20
[32m[20221213 12:59:41 @agent_ppo2.py:137][0m Total time:      43.74 min
[32m[20221213 12:59:41 @agent_ppo2.py:139][0m 3039232 total steps have happened
[32m[20221213 12:59:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1484 --------------------------#
[32m[20221213 12:59:41 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:41 @agent_ppo2.py:179][0m |           0.0529 |          37.8099 |          -4.8770 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |           0.0650 |          36.4129 |          -1.6524 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |           0.0270 |          35.7093 |          -2.0719 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |           0.0052 |          38.3829 |          -3.3376 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |          -0.0098 |          39.4783 |          -4.4110 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |          -0.0261 |          35.2759 |          -5.3755 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |          -0.0328 |          35.0365 |          -6.0970 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |          -0.0335 |          34.9243 |          -6.4564 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |          -0.0359 |          34.8307 |          -6.7722 |
[32m[20221213 12:59:42 @agent_ppo2.py:179][0m |          -0.0282 |          37.7781 |          -6.9928 |
[32m[20221213 12:59:42 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.11
[32m[20221213 12:59:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.19
[32m[20221213 12:59:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.21
[32m[20221213 12:59:43 @agent_ppo2.py:137][0m Total time:      43.77 min
[32m[20221213 12:59:43 @agent_ppo2.py:139][0m 3041280 total steps have happened
[32m[20221213 12:59:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1485 --------------------------#
[32m[20221213 12:59:43 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:43 @agent_ppo2.py:179][0m |           0.0796 |          37.0768 |          -3.2755 |
[32m[20221213 12:59:43 @agent_ppo2.py:179][0m |           0.0629 |          34.7608 |          -0.7163 |
[32m[20221213 12:59:43 @agent_ppo2.py:179][0m |           0.0135 |          34.3892 |          -2.6359 |
[32m[20221213 12:59:43 @agent_ppo2.py:179][0m |          -0.0152 |          33.0871 |          -3.6247 |
[32m[20221213 12:59:44 @agent_ppo2.py:179][0m |          -0.0251 |          32.2990 |          -4.3942 |
[32m[20221213 12:59:44 @agent_ppo2.py:179][0m |          -0.0311 |          32.3356 |          -4.6668 |
[32m[20221213 12:59:44 @agent_ppo2.py:179][0m |          -0.0403 |          31.5720 |          -5.8986 |
[32m[20221213 12:59:44 @agent_ppo2.py:179][0m |          -0.0383 |          31.2563 |          -6.1288 |
[32m[20221213 12:59:44 @agent_ppo2.py:179][0m |          -0.0394 |          32.8153 |          -6.4683 |
[32m[20221213 12:59:44 @agent_ppo2.py:179][0m |          -0.0495 |          30.6639 |          -6.7696 |
[32m[20221213 12:59:44 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 12:59:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.69
[32m[20221213 12:59:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 365.09
[32m[20221213 12:59:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 386.64
[32m[20221213 12:59:44 @agent_ppo2.py:137][0m Total time:      43.79 min
[32m[20221213 12:59:44 @agent_ppo2.py:139][0m 3043328 total steps have happened
[32m[20221213 12:59:44 @agent_ppo2.py:115][0m #------------------------ Iteration 1486 --------------------------#
[32m[20221213 12:59:45 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:45 @agent_ppo2.py:179][0m |           0.0438 |          37.4246 |          -6.6434 |
[32m[20221213 12:59:45 @agent_ppo2.py:179][0m |           0.0307 |          35.6038 |          -3.6019 |
[32m[20221213 12:59:45 @agent_ppo2.py:179][0m |          -0.0134 |          34.9750 |          -5.3704 |
[32m[20221213 12:59:45 @agent_ppo2.py:179][0m |          -0.0268 |          34.3675 |          -6.4115 |
[32m[20221213 12:59:45 @agent_ppo2.py:179][0m |          -0.0254 |          34.6630 |          -7.4118 |
[32m[20221213 12:59:45 @agent_ppo2.py:179][0m |          -0.0395 |          33.7649 |          -7.5623 |
[32m[20221213 12:59:45 @agent_ppo2.py:179][0m |          -0.0479 |          33.5252 |          -8.3885 |
[32m[20221213 12:59:46 @agent_ppo2.py:179][0m |          -0.0488 |          33.2373 |          -8.8949 |
[32m[20221213 12:59:46 @agent_ppo2.py:179][0m |          -0.0498 |          33.0316 |          -9.5110 |
[32m[20221213 12:59:46 @agent_ppo2.py:179][0m |          -0.0490 |          33.4826 |          -9.7465 |
[32m[20221213 12:59:46 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:59:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.59
[32m[20221213 12:59:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 333.34
[32m[20221213 12:59:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 171.60
[32m[20221213 12:59:46 @agent_ppo2.py:137][0m Total time:      43.82 min
[32m[20221213 12:59:46 @agent_ppo2.py:139][0m 3045376 total steps have happened
[32m[20221213 12:59:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1487 --------------------------#
[32m[20221213 12:59:46 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:46 @agent_ppo2.py:179][0m |           0.0983 |          36.8810 |          -7.5948 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |           0.0662 |          35.4424 |          -4.4637 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |           0.0177 |          34.9106 |          -7.2834 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |          -0.0085 |          34.8098 |          -7.9834 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |          -0.0146 |          34.9006 |          -8.6121 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |          -0.0234 |          34.4021 |          -9.5423 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |          -0.0295 |          34.1715 |          -9.4955 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |          -0.0340 |          34.0245 |         -10.4287 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |          -0.0365 |          33.9688 |         -10.7606 |
[32m[20221213 12:59:47 @agent_ppo2.py:179][0m |          -0.0465 |          33.8001 |         -11.3018 |
[32m[20221213 12:59:47 @agent_ppo2.py:124][0m Policy update time: 1.25 s
[32m[20221213 12:59:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.63
[32m[20221213 12:59:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.35
[32m[20221213 12:59:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.25
[32m[20221213 12:59:48 @agent_ppo2.py:137][0m Total time:      43.85 min
[32m[20221213 12:59:48 @agent_ppo2.py:139][0m 3047424 total steps have happened
[32m[20221213 12:59:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1488 --------------------------#
[32m[20221213 12:59:48 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:48 @agent_ppo2.py:179][0m |           0.0718 |          33.0478 |          -7.6393 |
[32m[20221213 12:59:48 @agent_ppo2.py:179][0m |           0.0326 |          31.2164 |          -5.5366 |
[32m[20221213 12:59:48 @agent_ppo2.py:179][0m |          -0.0020 |          30.3489 |          -6.7377 |
[32m[20221213 12:59:48 @agent_ppo2.py:179][0m |          -0.0037 |          29.5858 |          -6.6070 |
[32m[20221213 12:59:49 @agent_ppo2.py:179][0m |          -0.0331 |          29.1446 |          -7.8540 |
[32m[20221213 12:59:49 @agent_ppo2.py:179][0m |          -0.0405 |          28.8059 |          -8.6545 |
[32m[20221213 12:59:49 @agent_ppo2.py:179][0m |          -0.0305 |          29.1367 |          -9.1450 |
[32m[20221213 12:59:49 @agent_ppo2.py:179][0m |          -0.0406 |          28.1144 |          -9.2709 |
[32m[20221213 12:59:49 @agent_ppo2.py:179][0m |          -0.0331 |          32.5732 |         -10.1231 |
[32m[20221213 12:59:49 @agent_ppo2.py:179][0m |          -0.0481 |          27.7705 |         -10.7759 |
[32m[20221213 12:59:49 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 12:59:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 295.39
[32m[20221213 12:59:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.56
[32m[20221213 12:59:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 360.33
[32m[20221213 12:59:49 @agent_ppo2.py:137][0m Total time:      43.88 min
[32m[20221213 12:59:49 @agent_ppo2.py:139][0m 3049472 total steps have happened
[32m[20221213 12:59:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1489 --------------------------#
[32m[20221213 12:59:50 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:50 @agent_ppo2.py:179][0m |           0.0578 |          38.1130 |          -8.3307 |
[32m[20221213 12:59:50 @agent_ppo2.py:179][0m |           0.0398 |          35.9309 |          -5.3840 |
[32m[20221213 12:59:50 @agent_ppo2.py:179][0m |           0.0144 |          35.4746 |          -6.6582 |
[32m[20221213 12:59:50 @agent_ppo2.py:179][0m |          -0.0149 |          35.1790 |          -7.6689 |
[32m[20221213 12:59:50 @agent_ppo2.py:179][0m |          -0.0280 |          34.8640 |          -8.3561 |
[32m[20221213 12:59:50 @agent_ppo2.py:179][0m |          -0.0282 |          34.7609 |          -9.1045 |
[32m[20221213 12:59:50 @agent_ppo2.py:179][0m |          -0.0334 |          34.7793 |          -9.3369 |
[32m[20221213 12:59:51 @agent_ppo2.py:179][0m |          -0.0364 |          34.6206 |          -9.5904 |
[32m[20221213 12:59:51 @agent_ppo2.py:179][0m |          -0.0388 |          34.5357 |         -10.2911 |
[32m[20221213 12:59:51 @agent_ppo2.py:179][0m |          -0.0353 |          35.8206 |         -10.1275 |
[32m[20221213 12:59:51 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 12:59:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.21
[32m[20221213 12:59:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 335.06
[32m[20221213 12:59:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 300.24
[32m[20221213 12:59:51 @agent_ppo2.py:137][0m Total time:      43.90 min
[32m[20221213 12:59:51 @agent_ppo2.py:139][0m 3051520 total steps have happened
[32m[20221213 12:59:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1490 --------------------------#
[32m[20221213 12:59:51 @agent_ppo2.py:121][0m Sampling time: 0.24 s by 5 slaves
[32m[20221213 12:59:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:51 @agent_ppo2.py:179][0m |           0.0739 |          35.2756 |          -6.9085 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |           0.0250 |          33.7993 |          -5.2711 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0091 |          33.0844 |          -7.7172 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0158 |          32.3948 |          -8.6376 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0197 |          32.2213 |          -9.7860 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0354 |          31.2772 |         -10.7642 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0329 |          31.2869 |         -11.2183 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0352 |          30.8147 |         -11.8182 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0362 |          30.3630 |         -12.5433 |
[32m[20221213 12:59:52 @agent_ppo2.py:179][0m |          -0.0396 |          30.0840 |         -13.1274 |
[32m[20221213 12:59:52 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:59:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 348.02
[32m[20221213 12:59:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 369.10
[32m[20221213 12:59:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.98
[32m[20221213 12:59:53 @agent_ppo2.py:137][0m Total time:      43.93 min
[32m[20221213 12:59:53 @agent_ppo2.py:139][0m 3053568 total steps have happened
[32m[20221213 12:59:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1491 --------------------------#
[32m[20221213 12:59:53 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:53 @agent_ppo2.py:179][0m |           0.0705 |          39.0341 |          -7.8008 |
[32m[20221213 12:59:53 @agent_ppo2.py:179][0m |           0.0419 |          36.3080 |          -5.3546 |
[32m[20221213 12:59:53 @agent_ppo2.py:179][0m |           0.0001 |          35.2544 |          -7.4056 |
[32m[20221213 12:59:53 @agent_ppo2.py:179][0m |          -0.0203 |          34.9327 |          -8.0835 |
[32m[20221213 12:59:53 @agent_ppo2.py:179][0m |          -0.0285 |          34.3653 |          -8.3621 |
[32m[20221213 12:59:54 @agent_ppo2.py:179][0m |          -0.0380 |          34.3778 |          -8.8810 |
[32m[20221213 12:59:54 @agent_ppo2.py:179][0m |          -0.0412 |          34.0892 |          -9.2221 |
[32m[20221213 12:59:54 @agent_ppo2.py:179][0m |          -0.0447 |          34.0967 |          -9.7288 |
[32m[20221213 12:59:54 @agent_ppo2.py:179][0m |          -0.0390 |          34.0445 |          -9.4965 |
[32m[20221213 12:59:54 @agent_ppo2.py:179][0m |          -0.0474 |          33.8542 |         -10.4063 |
[32m[20221213 12:59:54 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:59:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.54
[32m[20221213 12:59:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 366.87
[32m[20221213 12:59:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.03
[32m[20221213 12:59:54 @agent_ppo2.py:137][0m Total time:      43.96 min
[32m[20221213 12:59:54 @agent_ppo2.py:139][0m 3055616 total steps have happened
[32m[20221213 12:59:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1492 --------------------------#
[32m[20221213 12:59:54 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |           0.0942 |          39.5387 |          -7.1847 |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |           0.0745 |          36.0101 |          -0.6217 |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |           0.0349 |          35.5337 |          -3.6155 |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |           0.0064 |          35.2190 |          -5.8153 |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |          -0.0087 |          34.9885 |          -6.8259 |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |          -0.0247 |          34.7638 |          -7.9312 |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |          -0.0307 |          34.5870 |          -8.7377 |
[32m[20221213 12:59:55 @agent_ppo2.py:179][0m |          -0.0296 |          34.4506 |          -9.3057 |
[32m[20221213 12:59:56 @agent_ppo2.py:179][0m |          -0.0233 |          38.1167 |          -9.4314 |
[32m[20221213 12:59:56 @agent_ppo2.py:179][0m |          -0.0330 |          34.4349 |         -10.2349 |
[32m[20221213 12:59:56 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:59:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 346.32
[32m[20221213 12:59:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 363.58
[32m[20221213 12:59:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 364.71
[32m[20221213 12:59:56 @agent_ppo2.py:137][0m Total time:      43.99 min
[32m[20221213 12:59:56 @agent_ppo2.py:139][0m 3057664 total steps have happened
[32m[20221213 12:59:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1493 --------------------------#
[32m[20221213 12:59:56 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:56 @agent_ppo2.py:179][0m |           0.0635 |          36.8164 |          -5.0163 |
[32m[20221213 12:59:56 @agent_ppo2.py:179][0m |           0.0630 |          40.9326 |          -3.0673 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |           0.0057 |          35.4804 |          -4.4475 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |          -0.0099 |          35.6847 |          -5.1589 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |          -0.0245 |          34.9989 |          -5.7750 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |          -0.0231 |          35.6658 |          -5.9878 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |          -0.0333 |          34.6777 |          -6.5855 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |          -0.0326 |          34.7958 |          -6.8548 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |          -0.0267 |          34.5243 |          -6.8405 |
[32m[20221213 12:59:57 @agent_ppo2.py:179][0m |          -0.0379 |          34.3589 |          -7.4645 |
[32m[20221213 12:59:57 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 12:59:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 330.74
[32m[20221213 12:59:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 341.10
[32m[20221213 12:59:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 329.37
[32m[20221213 12:59:58 @agent_ppo2.py:137][0m Total time:      44.01 min
[32m[20221213 12:59:58 @agent_ppo2.py:139][0m 3059712 total steps have happened
[32m[20221213 12:59:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1494 --------------------------#
[32m[20221213 12:59:58 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 12:59:58 @agent_ppo2.py:179][0m |           0.0628 |          36.9483 |          -3.6662 |
[32m[20221213 12:59:58 @agent_ppo2.py:179][0m |           0.0266 |          35.8659 |          -3.2564 |
[32m[20221213 12:59:58 @agent_ppo2.py:179][0m |           0.0069 |          35.2915 |          -4.3489 |
[32m[20221213 12:59:58 @agent_ppo2.py:179][0m |          -0.0012 |          35.1259 |          -4.7595 |
[32m[20221213 12:59:58 @agent_ppo2.py:179][0m |          -0.0127 |          34.6591 |          -5.1812 |
[32m[20221213 12:59:59 @agent_ppo2.py:179][0m |          -0.0155 |          34.5044 |          -4.9597 |
[32m[20221213 12:59:59 @agent_ppo2.py:179][0m |          -0.0269 |          34.3291 |          -6.5192 |
[32m[20221213 12:59:59 @agent_ppo2.py:179][0m |          -0.0357 |          34.1820 |          -7.1348 |
[32m[20221213 12:59:59 @agent_ppo2.py:179][0m |          -0.0303 |          34.8306 |          -7.2100 |
[32m[20221213 12:59:59 @agent_ppo2.py:179][0m |          -0.0261 |          36.3353 |          -7.6911 |
[32m[20221213 12:59:59 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 12:59:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 350.66
[32m[20221213 12:59:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 368.39
[32m[20221213 12:59:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 365.47
[32m[20221213 12:59:59 @agent_ppo2.py:137][0m Total time:      44.04 min
[32m[20221213 12:59:59 @agent_ppo2.py:139][0m 3061760 total steps have happened
[32m[20221213 12:59:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1495 --------------------------#
[32m[20221213 12:59:59 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 12:59:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |           0.0543 |          37.7620 |          -6.6234 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |           0.0308 |          40.7196 |          -4.8447 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |          -0.0036 |          35.9150 |          -6.0972 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |          -0.0204 |          35.3764 |          -7.1249 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |          -0.0285 |          35.1795 |          -7.6760 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |          -0.0341 |          35.1049 |          -7.8850 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |          -0.0327 |          35.9125 |          -7.8012 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |          -0.0317 |          37.8868 |          -8.5515 |
[32m[20221213 13:00:00 @agent_ppo2.py:179][0m |          -0.0399 |          34.7313 |          -8.8581 |
[32m[20221213 13:00:01 @agent_ppo2.py:179][0m |          -0.0355 |          35.4793 |          -8.7425 |
[32m[20221213 13:00:01 @agent_ppo2.py:124][0m Policy update time: 1.22 s
[32m[20221213 13:00:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.84
[32m[20221213 13:00:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 344.31
[32m[20221213 13:00:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.82
[32m[20221213 13:00:01 @agent_ppo2.py:137][0m Total time:      44.07 min
[32m[20221213 13:00:01 @agent_ppo2.py:139][0m 3063808 total steps have happened
[32m[20221213 13:00:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1496 --------------------------#
[32m[20221213 13:00:01 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 13:00:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 13:00:01 @agent_ppo2.py:179][0m |           0.0614 |          36.9216 |          -5.2238 |
[32m[20221213 13:00:01 @agent_ppo2.py:179][0m |           0.0435 |          36.1740 |          -2.6523 |
[32m[20221213 13:00:01 @agent_ppo2.py:179][0m |           0.0323 |          40.1198 |          -3.5579 |
[32m[20221213 13:00:02 @agent_ppo2.py:179][0m |          -0.0081 |          36.1559 |          -5.0269 |
[32m[20221213 13:00:02 @agent_ppo2.py:179][0m |          -0.0191 |          35.3190 |          -5.5869 |
[32m[20221213 13:00:02 @agent_ppo2.py:179][0m |          -0.0284 |          35.1920 |          -5.4932 |
[32m[20221213 13:00:02 @agent_ppo2.py:179][0m |          -0.0300 |          35.0379 |          -5.9947 |
[32m[20221213 13:00:02 @agent_ppo2.py:179][0m |          -0.0326 |          34.9853 |          -6.5477 |
[32m[20221213 13:00:02 @agent_ppo2.py:179][0m |          -0.0200 |          34.8651 |          -6.3132 |
[32m[20221213 13:00:02 @agent_ppo2.py:179][0m |          -0.0276 |          34.8128 |          -6.6590 |
[32m[20221213 13:00:02 @agent_ppo2.py:124][0m Policy update time: 1.23 s
[32m[20221213 13:00:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.27
[32m[20221213 13:00:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 347.16
[32m[20221213 13:00:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.22
[32m[20221213 13:00:02 @agent_ppo2.py:137][0m Total time:      44.10 min
[32m[20221213 13:00:02 @agent_ppo2.py:139][0m 3065856 total steps have happened
[32m[20221213 13:00:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1497 --------------------------#
[32m[20221213 13:00:03 @agent_ppo2.py:121][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 13:00:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 13:00:03 @agent_ppo2.py:179][0m |           0.0623 |          36.5098 |          -3.9420 |
[32m[20221213 13:00:03 @agent_ppo2.py:179][0m |           0.0621 |          36.0055 |          -0.9967 |
[32m[20221213 13:00:03 @agent_ppo2.py:179][0m |           0.0432 |          35.6910 |          -0.1551 |
[32m[20221213 13:00:03 @agent_ppo2.py:179][0m |           0.0232 |          35.6337 |          -1.5175 |
[32m[20221213 13:00:03 @agent_ppo2.py:179][0m |           0.0080 |          38.1753 |          -3.0190 |
[32m[20221213 13:00:03 @agent_ppo2.py:179][0m |          -0.0101 |          36.6080 |          -3.7589 |
[32m[20221213 13:00:04 @agent_ppo2.py:179][0m |          -0.0188 |          35.3518 |          -4.6972 |
[32m[20221213 13:00:04 @agent_ppo2.py:179][0m |          -0.0235 |          35.2166 |          -4.7183 |
[32m[20221213 13:00:04 @agent_ppo2.py:179][0m |          -0.0310 |          35.0813 |          -5.4597 |
[32m[20221213 13:00:04 @agent_ppo2.py:179][0m |          -0.0322 |          35.6101 |          -5.9521 |
[32m[20221213 13:00:04 @agent_ppo2.py:124][0m Policy update time: 1.21 s
[32m[20221213 13:00:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 325.46
[32m[20221213 13:00:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 360.84
[32m[20221213 13:00:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 363.08
[32m[20221213 13:00:04 @agent_ppo2.py:137][0m Total time:      44.12 min
[32m[20221213 13:00:04 @agent_ppo2.py:139][0m 3067904 total steps have happened
[32m[20221213 13:00:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1498 --------------------------#
[32m[20221213 13:00:04 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 13:00:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |           0.0668 |          36.3951 |          -2.8788 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |           0.0802 |          35.2814 |          -0.5074 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |           0.0178 |          34.9211 |          -1.5172 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |          -0.0056 |          34.5950 |          -2.0288 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |          -0.0140 |          34.4229 |          -2.4218 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |          -0.0260 |          34.2698 |          -2.6816 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |          -0.0105 |          42.0438 |          -3.1769 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |          -0.0327 |          34.1917 |          -3.2775 |
[32m[20221213 13:00:05 @agent_ppo2.py:179][0m |          -0.0372 |          33.9734 |          -3.9267 |
[32m[20221213 13:00:06 @agent_ppo2.py:179][0m |          -0.0364 |          33.8161 |          -3.6196 |
[32m[20221213 13:00:06 @agent_ppo2.py:124][0m Policy update time: 1.24 s
[32m[20221213 13:00:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 323.87
[32m[20221213 13:00:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.99
[32m[20221213 13:00:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.99
[32m[20221213 13:00:06 @agent_ppo2.py:137][0m Total time:      44.15 min
[32m[20221213 13:00:06 @agent_ppo2.py:139][0m 3069952 total steps have happened
[32m[20221213 13:00:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1499 --------------------------#
[32m[20221213 13:00:06 @agent_ppo2.py:121][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 13:00:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 13:00:06 @agent_ppo2.py:179][0m |           0.0372 |          35.6408 |          -3.1985 |
[32m[20221213 13:00:06 @agent_ppo2.py:179][0m |           0.0255 |          34.9228 |          -1.6088 |
[32m[20221213 13:00:06 @agent_ppo2.py:179][0m |          -0.0010 |          35.4430 |          -2.4570 |
[32m[20221213 13:00:07 @agent_ppo2.py:179][0m |          -0.0180 |          33.8343 |          -2.2590 |
[32m[20221213 13:00:07 @agent_ppo2.py:179][0m |          -0.0227 |          33.8244 |          -3.3361 |
[32m[20221213 13:00:07 @agent_ppo2.py:179][0m |          -0.0240 |          37.4330 |          -3.6359 |
[32m[20221213 13:00:07 @agent_ppo2.py:179][0m |          -0.0300 |          33.4255 |          -3.9053 |
[32m[20221213 13:00:07 @agent_ppo2.py:179][0m |          -0.0272 |          32.7538 |          -3.9675 |
[32m[20221213 13:00:07 @agent_ppo2.py:179][0m |          -0.0372 |          32.4727 |          -4.4740 |
[32m[20221213 13:00:07 @agent_ppo2.py:179][0m |          -0.0402 |          32.3638 |          -5.0128 |
[32m[20221213 13:00:07 @agent_ppo2.py:124][0m Policy update time: 1.26 s
[32m[20221213 13:00:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 341.42
[32m[20221213 13:00:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 359.80
[32m[20221213 13:00:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 367.13
[32m[20221213 13:00:07 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 408.32
[32m[20221213 13:00:07 @agent_ppo2.py:137][0m Total time:      44.18 min
[32m[20221213 13:00:07 @agent_ppo2.py:139][0m 3072000 total steps have happened
[32m[20221213 13:00:07 @train.py:58][0m [4m[34mCRITICAL[0m Training completed!
