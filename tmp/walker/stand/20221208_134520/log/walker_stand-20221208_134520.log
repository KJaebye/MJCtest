[32m[20221208 13:45:20 @logger.py:105][0m Log file set to ./tmp/walker/stand/20221208_134520/log/walker_stand-20221208_134520.log
[32m[20221208 13:45:20 @agent_ppo2.py:115][0m #------------------------ Iteration 0 --------------------------#
[32m[20221208 13:45:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0028 |           5.2204 |           0.2305 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0151 |           2.5989 |           0.2297 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0173 |           0.7092 |           0.2302 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0200 |           0.5221 |           0.2300 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0211 |           0.4924 |           0.2297 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0224 |           0.4781 |           0.2296 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0247 |           0.4691 |           0.2294 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0244 |           0.4604 |           0.2294 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0251 |           0.4537 |           0.2292 |
[32m[20221208 13:45:21 @agent_ppo2.py:179][0m |          -0.0264 |           0.4470 |           0.2287 |
[32m[20221208 13:45:21 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:45:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 133.52
[32m[20221208 13:45:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 144.01
[32m[20221208 13:45:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 206.22
[32m[20221208 13:45:22 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 206.22
[32m[20221208 13:45:22 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 206.22
[32m[20221208 13:45:22 @agent_ppo2.py:137][0m Total time:       0.03 min
[32m[20221208 13:45:22 @agent_ppo2.py:139][0m 2048 total steps have happened
[32m[20221208 13:45:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1 --------------------------#
[32m[20221208 13:45:22 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:45:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:22 @agent_ppo2.py:179][0m |           0.0002 |           1.6001 |           0.2332 |
[32m[20221208 13:45:22 @agent_ppo2.py:179][0m |          -0.0125 |           0.7650 |           0.2330 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0151 |           0.7056 |           0.2335 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0163 |           0.6838 |           0.2337 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0184 |           0.6671 |           0.2340 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0198 |           0.6578 |           0.2343 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0206 |           0.6478 |           0.2345 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0224 |           0.6435 |           0.2349 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0230 |           0.6267 |           0.2351 |
[32m[20221208 13:45:23 @agent_ppo2.py:179][0m |          -0.0258 |           0.6179 |           0.2357 |
[32m[20221208 13:45:23 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:45:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 129.13
[32m[20221208 13:45:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 152.70
[32m[20221208 13:45:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 143.38
[32m[20221208 13:45:23 @agent_ppo2.py:137][0m Total time:       0.05 min
[32m[20221208 13:45:23 @agent_ppo2.py:139][0m 4096 total steps have happened
[32m[20221208 13:45:23 @agent_ppo2.py:115][0m #------------------------ Iteration 2 --------------------------#
[32m[20221208 13:45:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |           0.0002 |           1.9427 |           0.2361 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0114 |           1.1736 |           0.2362 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0166 |           1.1392 |           0.2360 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0185 |           1.1195 |           0.2363 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0202 |           1.0860 |           0.2369 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0225 |           1.0662 |           0.2368 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0222 |           1.0390 |           0.2373 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0250 |           1.0098 |           0.2376 |
[32m[20221208 13:45:24 @agent_ppo2.py:179][0m |          -0.0261 |           0.9817 |           0.2372 |
[32m[20221208 13:45:25 @agent_ppo2.py:179][0m |          -0.0268 |           0.9712 |           0.2374 |
[32m[20221208 13:45:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.56
[32m[20221208 13:45:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 157.63
[32m[20221208 13:45:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 171.68
[32m[20221208 13:45:25 @agent_ppo2.py:137][0m Total time:       0.08 min
[32m[20221208 13:45:25 @agent_ppo2.py:139][0m 6144 total steps have happened
[32m[20221208 13:45:25 @agent_ppo2.py:115][0m #------------------------ Iteration 3 --------------------------#
[32m[20221208 13:45:25 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:45:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:25 @agent_ppo2.py:179][0m |           0.0040 |           1.6412 |           0.2425 |
[32m[20221208 13:45:25 @agent_ppo2.py:179][0m |          -0.0095 |           1.2101 |           0.2417 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0133 |           1.1642 |           0.2411 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0161 |           1.1499 |           0.2409 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0161 |           1.1304 |           0.2406 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0199 |           1.1147 |           0.2404 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0212 |           1.0915 |           0.2397 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0223 |           1.0750 |           0.2393 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0230 |           1.0650 |           0.2393 |
[32m[20221208 13:45:26 @agent_ppo2.py:179][0m |          -0.0256 |           1.0444 |           0.2396 |
[32m[20221208 13:45:26 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 139.69
[32m[20221208 13:45:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 151.76
[32m[20221208 13:45:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 186.72
[32m[20221208 13:45:26 @agent_ppo2.py:137][0m Total time:       0.10 min
[32m[20221208 13:45:26 @agent_ppo2.py:139][0m 8192 total steps have happened
[32m[20221208 13:45:26 @agent_ppo2.py:115][0m #------------------------ Iteration 4 --------------------------#
[32m[20221208 13:45:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |           0.0007 |           1.9451 |           0.2388 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0113 |           1.4779 |           0.2370 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0137 |           1.4309 |           0.2373 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0168 |           1.3988 |           0.2368 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0186 |           1.3489 |           0.2367 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0205 |           1.3235 |           0.2368 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0226 |           1.2985 |           0.2369 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0229 |           1.2829 |           0.2368 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0253 |           1.2547 |           0.2373 |
[32m[20221208 13:45:27 @agent_ppo2.py:179][0m |          -0.0250 |           1.2338 |           0.2367 |
[32m[20221208 13:45:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 149.81
[32m[20221208 13:45:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 169.73
[32m[20221208 13:45:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.74
[32m[20221208 13:45:28 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 260.74
[32m[20221208 13:45:28 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 260.74
[32m[20221208 13:45:28 @agent_ppo2.py:137][0m Total time:       0.12 min
[32m[20221208 13:45:28 @agent_ppo2.py:139][0m 10240 total steps have happened
[32m[20221208 13:45:28 @agent_ppo2.py:115][0m #------------------------ Iteration 5 --------------------------#
[32m[20221208 13:45:28 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:45:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:28 @agent_ppo2.py:179][0m |           0.0011 |           2.6453 |           0.2420 |
[32m[20221208 13:45:28 @agent_ppo2.py:179][0m |          -0.0132 |           2.0719 |           0.2411 |
[32m[20221208 13:45:28 @agent_ppo2.py:179][0m |          -0.0171 |           1.9857 |           0.2407 |
[32m[20221208 13:45:29 @agent_ppo2.py:179][0m |          -0.0195 |           1.9014 |           0.2410 |
[32m[20221208 13:45:29 @agent_ppo2.py:179][0m |          -0.0212 |           1.8527 |           0.2410 |
[32m[20221208 13:45:29 @agent_ppo2.py:179][0m |          -0.0246 |           1.7652 |           0.2401 |
[32m[20221208 13:45:29 @agent_ppo2.py:179][0m |          -0.0255 |           1.7144 |           0.2406 |
[32m[20221208 13:45:29 @agent_ppo2.py:179][0m |          -0.0266 |           1.6893 |           0.2402 |
[32m[20221208 13:45:29 @agent_ppo2.py:179][0m |          -0.0288 |           1.6272 |           0.2400 |
[32m[20221208 13:45:29 @agent_ppo2.py:179][0m |          -0.0276 |           1.6112 |           0.2396 |
[32m[20221208 13:45:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:45:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 155.51
[32m[20221208 13:45:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 184.73
[32m[20221208 13:45:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 259.37
[32m[20221208 13:45:29 @agent_ppo2.py:137][0m Total time:       0.15 min
[32m[20221208 13:45:29 @agent_ppo2.py:139][0m 12288 total steps have happened
[32m[20221208 13:45:29 @agent_ppo2.py:115][0m #------------------------ Iteration 6 --------------------------#
[32m[20221208 13:45:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |           0.0006 |           2.6646 |           0.2417 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0095 |           2.2858 |           0.2407 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0139 |           2.2080 |           0.2408 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0159 |           2.1631 |           0.2413 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0178 |           2.1384 |           0.2410 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0189 |           2.1127 |           0.2412 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0192 |           2.1290 |           0.2410 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0221 |           2.0863 |           0.2410 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0217 |           2.0515 |           0.2404 |
[32m[20221208 13:45:30 @agent_ppo2.py:179][0m |          -0.0235 |           2.0362 |           0.2405 |
[32m[20221208 13:45:30 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:45:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 158.06
[32m[20221208 13:45:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 165.41
[32m[20221208 13:45:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 175.27
[32m[20221208 13:45:31 @agent_ppo2.py:137][0m Total time:       0.17 min
[32m[20221208 13:45:31 @agent_ppo2.py:139][0m 14336 total steps have happened
[32m[20221208 13:45:31 @agent_ppo2.py:115][0m #------------------------ Iteration 7 --------------------------#
[32m[20221208 13:45:31 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:45:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:31 @agent_ppo2.py:179][0m |          -0.0010 |           3.7520 |           0.2342 |
[32m[20221208 13:45:31 @agent_ppo2.py:179][0m |          -0.0119 |           3.0572 |           0.2341 |
[32m[20221208 13:45:31 @agent_ppo2.py:179][0m |          -0.0161 |           2.9323 |           0.2336 |
[32m[20221208 13:45:32 @agent_ppo2.py:179][0m |          -0.0174 |           2.8666 |           0.2345 |
[32m[20221208 13:45:32 @agent_ppo2.py:179][0m |          -0.0207 |           2.7602 |           0.2341 |
[32m[20221208 13:45:32 @agent_ppo2.py:179][0m |          -0.0230 |           2.6967 |           0.2345 |
[32m[20221208 13:45:32 @agent_ppo2.py:179][0m |          -0.0237 |           2.6543 |           0.2344 |
[32m[20221208 13:45:32 @agent_ppo2.py:179][0m |          -0.0249 |           2.6022 |           0.2346 |
[32m[20221208 13:45:32 @agent_ppo2.py:179][0m |          -0.0270 |           2.5892 |           0.2347 |
[32m[20221208 13:45:32 @agent_ppo2.py:179][0m |          -0.0271 |           2.5396 |           0.2344 |
[32m[20221208 13:45:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:45:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 172.50
[32m[20221208 13:45:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 195.23
[32m[20221208 13:45:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 226.88
[32m[20221208 13:45:32 @agent_ppo2.py:137][0m Total time:       0.20 min
[32m[20221208 13:45:32 @agent_ppo2.py:139][0m 16384 total steps have happened
[32m[20221208 13:45:32 @agent_ppo2.py:115][0m #------------------------ Iteration 8 --------------------------#
[32m[20221208 13:45:33 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:45:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |           0.0045 |           3.7275 |           0.2371 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0102 |           3.2953 |           0.2376 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0154 |           3.2558 |           0.2374 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0156 |           3.1737 |           0.2370 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0191 |           3.1398 |           0.2374 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0178 |           3.1137 |           0.2362 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0190 |           3.0794 |           0.2372 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0216 |           3.0548 |           0.2372 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0217 |           3.0083 |           0.2374 |
[32m[20221208 13:45:33 @agent_ppo2.py:179][0m |          -0.0234 |           2.9848 |           0.2379 |
[32m[20221208 13:45:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 183.63
[32m[20221208 13:45:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 199.41
[32m[20221208 13:45:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 197.30
[32m[20221208 13:45:34 @agent_ppo2.py:137][0m Total time:       0.22 min
[32m[20221208 13:45:34 @agent_ppo2.py:139][0m 18432 total steps have happened
[32m[20221208 13:45:34 @agent_ppo2.py:115][0m #------------------------ Iteration 9 --------------------------#
[32m[20221208 13:45:34 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:45:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:34 @agent_ppo2.py:179][0m |          -0.0027 |           4.2178 |           0.2496 |
[32m[20221208 13:45:34 @agent_ppo2.py:179][0m |          -0.0134 |           3.7228 |           0.2493 |
[32m[20221208 13:45:34 @agent_ppo2.py:179][0m |          -0.0171 |           3.6400 |           0.2500 |
[32m[20221208 13:45:34 @agent_ppo2.py:179][0m |          -0.0192 |           3.5998 |           0.2497 |
[32m[20221208 13:45:34 @agent_ppo2.py:179][0m |          -0.0207 |           3.5461 |           0.2506 |
[32m[20221208 13:45:35 @agent_ppo2.py:179][0m |          -0.0214 |           3.5209 |           0.2510 |
[32m[20221208 13:45:35 @agent_ppo2.py:179][0m |          -0.0222 |           3.4667 |           0.2507 |
[32m[20221208 13:45:35 @agent_ppo2.py:179][0m |          -0.0215 |           3.4675 |           0.2507 |
[32m[20221208 13:45:35 @agent_ppo2.py:179][0m |          -0.0240 |           3.4114 |           0.2508 |
[32m[20221208 13:45:35 @agent_ppo2.py:179][0m |          -0.0242 |           3.4009 |           0.2513 |
[32m[20221208 13:45:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 191.18
[32m[20221208 13:45:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 215.37
[32m[20221208 13:45:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 233.77
[32m[20221208 13:45:35 @agent_ppo2.py:137][0m Total time:       0.25 min
[32m[20221208 13:45:35 @agent_ppo2.py:139][0m 20480 total steps have happened
[32m[20221208 13:45:35 @agent_ppo2.py:115][0m #------------------------ Iteration 10 --------------------------#
[32m[20221208 13:45:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:45:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |           0.0021 |           5.1554 |           0.2481 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0093 |           4.6169 |           0.2481 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0131 |           4.5414 |           0.2464 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0127 |           4.5341 |           0.2463 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0150 |           4.4571 |           0.2468 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0178 |           4.4000 |           0.2473 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0197 |           4.3837 |           0.2474 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0211 |           4.3544 |           0.2478 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0229 |           4.3219 |           0.2481 |
[32m[20221208 13:45:36 @agent_ppo2.py:179][0m |          -0.0226 |           4.2746 |           0.2474 |
[32m[20221208 13:45:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:45:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 206.66
[32m[20221208 13:45:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 243.57
[32m[20221208 13:45:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 194.78
[32m[20221208 13:45:37 @agent_ppo2.py:137][0m Total time:       0.27 min
[32m[20221208 13:45:37 @agent_ppo2.py:139][0m 22528 total steps have happened
[32m[20221208 13:45:37 @agent_ppo2.py:115][0m #------------------------ Iteration 11 --------------------------#
[32m[20221208 13:45:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:37 @agent_ppo2.py:179][0m |           0.0021 |           5.3676 |           0.2582 |
[32m[20221208 13:45:37 @agent_ppo2.py:179][0m |          -0.0072 |           4.7427 |           0.2563 |
[32m[20221208 13:45:37 @agent_ppo2.py:179][0m |          -0.0117 |           4.6582 |           0.2557 |
[32m[20221208 13:45:37 @agent_ppo2.py:179][0m |          -0.0155 |           4.6440 |           0.2545 |
[32m[20221208 13:45:38 @agent_ppo2.py:179][0m |          -0.0167 |           4.5934 |           0.2545 |
[32m[20221208 13:45:38 @agent_ppo2.py:179][0m |          -0.0172 |           4.5798 |           0.2549 |
[32m[20221208 13:45:38 @agent_ppo2.py:179][0m |          -0.0189 |           4.5363 |           0.2538 |
[32m[20221208 13:45:38 @agent_ppo2.py:179][0m |          -0.0191 |           4.5579 |           0.2545 |
[32m[20221208 13:45:38 @agent_ppo2.py:179][0m |          -0.0195 |           4.4944 |           0.2535 |
[32m[20221208 13:45:38 @agent_ppo2.py:179][0m |          -0.0202 |           4.5019 |           0.2544 |
[32m[20221208 13:45:38 @agent_ppo2.py:124][0m Policy update time: 0.84 s
[32m[20221208 13:45:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 222.98
[32m[20221208 13:45:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 253.82
[32m[20221208 13:45:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 260.54
[32m[20221208 13:45:38 @agent_ppo2.py:137][0m Total time:       0.30 min
[32m[20221208 13:45:38 @agent_ppo2.py:139][0m 24576 total steps have happened
[32m[20221208 13:45:38 @agent_ppo2.py:115][0m #------------------------ Iteration 12 --------------------------#
[32m[20221208 13:45:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |           0.0014 |           6.9880 |           0.2565 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0105 |           6.3226 |           0.2568 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0156 |           6.2322 |           0.2567 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0165 |           6.1516 |           0.2579 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0187 |           6.0970 |           0.2580 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0174 |           6.0980 |           0.2581 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0209 |           6.0363 |           0.2576 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0213 |           5.9938 |           0.2580 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0205 |           5.9630 |           0.2582 |
[32m[20221208 13:45:39 @agent_ppo2.py:179][0m |          -0.0211 |           5.9262 |           0.2585 |
[32m[20221208 13:45:39 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:45:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 221.18
[32m[20221208 13:45:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 258.57
[32m[20221208 13:45:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 227.65
[32m[20221208 13:45:40 @agent_ppo2.py:137][0m Total time:       0.32 min
[32m[20221208 13:45:40 @agent_ppo2.py:139][0m 26624 total steps have happened
[32m[20221208 13:45:40 @agent_ppo2.py:115][0m #------------------------ Iteration 13 --------------------------#
[32m[20221208 13:45:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:40 @agent_ppo2.py:179][0m |           0.0059 |           7.6672 |           0.2650 |
[32m[20221208 13:45:40 @agent_ppo2.py:179][0m |          -0.0041 |           6.7582 |           0.2649 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0066 |           6.5677 |           0.2648 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0089 |           6.4851 |           0.2648 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0093 |           6.4067 |           0.2650 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0089 |           6.3672 |           0.2642 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0147 |           6.3206 |           0.2659 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0145 |           6.2611 |           0.2659 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0151 |           6.2466 |           0.2660 |
[32m[20221208 13:45:41 @agent_ppo2.py:179][0m |          -0.0162 |           6.2752 |           0.2659 |
[32m[20221208 13:45:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:45:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 219.38
[32m[20221208 13:45:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.41
[32m[20221208 13:45:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 164.19
[32m[20221208 13:45:41 @agent_ppo2.py:137][0m Total time:       0.35 min
[32m[20221208 13:45:41 @agent_ppo2.py:139][0m 28672 total steps have happened
[32m[20221208 13:45:41 @agent_ppo2.py:115][0m #------------------------ Iteration 14 --------------------------#
[32m[20221208 13:45:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |           0.0053 |           4.4796 |           0.2617 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0093 |           4.1917 |           0.2619 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0111 |           4.1351 |           0.2611 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0153 |           4.1046 |           0.2622 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0147 |           4.0808 |           0.2616 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0179 |           4.0735 |           0.2633 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0208 |           4.0288 |           0.2633 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0175 |           4.0409 |           0.2633 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0219 |           4.0072 |           0.2639 |
[32m[20221208 13:45:42 @agent_ppo2.py:179][0m |          -0.0229 |           3.9850 |           0.2642 |
[32m[20221208 13:45:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 156.17
[32m[20221208 13:45:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 163.08
[32m[20221208 13:45:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 269.10
[32m[20221208 13:45:43 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 269.10
[32m[20221208 13:45:43 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 269.10
[32m[20221208 13:45:43 @agent_ppo2.py:137][0m Total time:       0.37 min
[32m[20221208 13:45:43 @agent_ppo2.py:139][0m 30720 total steps have happened
[32m[20221208 13:45:43 @agent_ppo2.py:115][0m #------------------------ Iteration 15 --------------------------#
[32m[20221208 13:45:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:43 @agent_ppo2.py:179][0m |           0.0029 |           5.7805 |           0.2714 |
[32m[20221208 13:45:43 @agent_ppo2.py:179][0m |          -0.0049 |           5.4951 |           0.2712 |
[32m[20221208 13:45:43 @agent_ppo2.py:179][0m |          -0.0101 |           5.4074 |           0.2701 |
[32m[20221208 13:45:44 @agent_ppo2.py:179][0m |          -0.0121 |           5.3827 |           0.2703 |
[32m[20221208 13:45:44 @agent_ppo2.py:179][0m |          -0.0130 |           5.3662 |           0.2702 |
[32m[20221208 13:45:44 @agent_ppo2.py:179][0m |          -0.0131 |           5.3463 |           0.2697 |
[32m[20221208 13:45:44 @agent_ppo2.py:179][0m |          -0.0149 |           5.3205 |           0.2695 |
[32m[20221208 13:45:44 @agent_ppo2.py:179][0m |          -0.0153 |           5.3219 |           0.2702 |
[32m[20221208 13:45:44 @agent_ppo2.py:179][0m |          -0.0171 |           5.3447 |           0.2695 |
[32m[20221208 13:45:44 @agent_ppo2.py:179][0m |          -0.0155 |           5.2852 |           0.2702 |
[32m[20221208 13:45:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:45:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 181.77
[32m[20221208 13:45:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 255.39
[32m[20221208 13:45:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 277.09
[32m[20221208 13:45:44 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 277.09
[32m[20221208 13:45:44 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 277.09
[32m[20221208 13:45:44 @agent_ppo2.py:137][0m Total time:       0.40 min
[32m[20221208 13:45:44 @agent_ppo2.py:139][0m 32768 total steps have happened
[32m[20221208 13:45:44 @agent_ppo2.py:115][0m #------------------------ Iteration 16 --------------------------#
[32m[20221208 13:45:45 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:45:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |           0.0003 |           8.7064 |           0.2770 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0125 |           7.9734 |           0.2755 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0147 |           7.7891 |           0.2760 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0160 |           7.6875 |           0.2761 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0173 |           7.6273 |           0.2763 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0195 |           7.5696 |           0.2758 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0216 |           7.5117 |           0.2758 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0211 |           7.4749 |           0.2761 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0219 |           7.4747 |           0.2762 |
[32m[20221208 13:45:45 @agent_ppo2.py:179][0m |          -0.0214 |           7.4808 |           0.2760 |
[32m[20221208 13:45:45 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:45:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 253.27
[32m[20221208 13:45:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 280.12
[32m[20221208 13:45:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 229.80
[32m[20221208 13:45:46 @agent_ppo2.py:137][0m Total time:       0.42 min
[32m[20221208 13:45:46 @agent_ppo2.py:139][0m 34816 total steps have happened
[32m[20221208 13:45:46 @agent_ppo2.py:115][0m #------------------------ Iteration 17 --------------------------#
[32m[20221208 13:45:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:46 @agent_ppo2.py:179][0m |           0.0034 |           6.4621 |           0.2792 |
[32m[20221208 13:45:46 @agent_ppo2.py:179][0m |          -0.0088 |           6.1178 |           0.2805 |
[32m[20221208 13:45:46 @agent_ppo2.py:179][0m |          -0.0106 |           5.9915 |           0.2803 |
[32m[20221208 13:45:47 @agent_ppo2.py:179][0m |          -0.0149 |           5.9439 |           0.2817 |
[32m[20221208 13:45:47 @agent_ppo2.py:179][0m |          -0.0129 |           5.9147 |           0.2812 |
[32m[20221208 13:45:47 @agent_ppo2.py:179][0m |          -0.0171 |           5.8463 |           0.2824 |
[32m[20221208 13:45:47 @agent_ppo2.py:179][0m |          -0.0178 |           5.8430 |           0.2829 |
[32m[20221208 13:45:47 @agent_ppo2.py:179][0m |          -0.0189 |           5.8315 |           0.2830 |
[32m[20221208 13:45:47 @agent_ppo2.py:179][0m |          -0.0197 |           5.7901 |           0.2833 |
[32m[20221208 13:45:47 @agent_ppo2.py:179][0m |          -0.0198 |           5.7430 |           0.2833 |
[32m[20221208 13:45:47 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:45:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 188.53
[32m[20221208 13:45:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.71
[32m[20221208 13:45:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 295.50
[32m[20221208 13:45:47 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 295.50
[32m[20221208 13:45:47 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 295.50
[32m[20221208 13:45:47 @agent_ppo2.py:137][0m Total time:       0.45 min
[32m[20221208 13:45:47 @agent_ppo2.py:139][0m 36864 total steps have happened
[32m[20221208 13:45:47 @agent_ppo2.py:115][0m #------------------------ Iteration 18 --------------------------#
[32m[20221208 13:45:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |           0.0008 |           8.8573 |           0.3059 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0095 |           8.5221 |           0.3052 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0129 |           8.4152 |           0.3047 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0136 |           8.3516 |           0.3055 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0162 |           8.3397 |           0.3053 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0152 |           8.2691 |           0.3053 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0171 |           8.2737 |           0.3059 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0183 |           8.2835 |           0.3058 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0180 |           8.2393 |           0.3064 |
[32m[20221208 13:45:48 @agent_ppo2.py:179][0m |          -0.0201 |           8.2356 |           0.3058 |
[32m[20221208 13:45:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 226.73
[32m[20221208 13:45:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 271.74
[32m[20221208 13:45:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 304.77
[32m[20221208 13:45:49 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 304.77
[32m[20221208 13:45:49 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 304.77
[32m[20221208 13:45:49 @agent_ppo2.py:137][0m Total time:       0.47 min
[32m[20221208 13:45:49 @agent_ppo2.py:139][0m 38912 total steps have happened
[32m[20221208 13:45:49 @agent_ppo2.py:115][0m #------------------------ Iteration 19 --------------------------#
[32m[20221208 13:45:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:49 @agent_ppo2.py:179][0m |           0.0060 |           9.8855 |           0.3034 |
[32m[20221208 13:45:49 @agent_ppo2.py:179][0m |          -0.0066 |           9.4654 |           0.3031 |
[32m[20221208 13:45:49 @agent_ppo2.py:179][0m |          -0.0120 |           9.3215 |           0.3017 |
[32m[20221208 13:45:49 @agent_ppo2.py:179][0m |          -0.0171 |           9.2253 |           0.3023 |
[32m[20221208 13:45:50 @agent_ppo2.py:179][0m |          -0.0184 |           9.1868 |           0.3018 |
[32m[20221208 13:45:50 @agent_ppo2.py:179][0m |          -0.0182 |           9.1425 |           0.3011 |
[32m[20221208 13:45:50 @agent_ppo2.py:179][0m |          -0.0174 |           9.0975 |           0.3007 |
[32m[20221208 13:45:50 @agent_ppo2.py:179][0m |          -0.0192 |           9.1082 |           0.3011 |
[32m[20221208 13:45:50 @agent_ppo2.py:179][0m |          -0.0213 |           9.1461 |           0.3011 |
[32m[20221208 13:45:50 @agent_ppo2.py:179][0m |          -0.0222 |           9.0194 |           0.3007 |
[32m[20221208 13:45:50 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:45:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 245.31
[32m[20221208 13:45:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 265.15
[32m[20221208 13:45:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 294.77
[32m[20221208 13:45:50 @agent_ppo2.py:137][0m Total time:       0.50 min
[32m[20221208 13:45:50 @agent_ppo2.py:139][0m 40960 total steps have happened
[32m[20221208 13:45:50 @agent_ppo2.py:115][0m #------------------------ Iteration 20 --------------------------#
[32m[20221208 13:45:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |           0.0042 |          13.5370 |           0.2896 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0095 |          12.4139 |           0.2907 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0146 |          11.5043 |           0.2915 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0190 |          10.9904 |           0.2912 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0200 |          10.5389 |           0.2907 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0214 |          10.3800 |           0.2911 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0224 |          10.2065 |           0.2911 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0237 |          10.0244 |           0.2913 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0247 |           9.9275 |           0.2911 |
[32m[20221208 13:45:51 @agent_ppo2.py:179][0m |          -0.0265 |           9.8397 |           0.2910 |
[32m[20221208 13:45:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 269.38
[32m[20221208 13:45:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 272.81
[32m[20221208 13:45:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.20
[32m[20221208 13:45:52 @agent_ppo2.py:137][0m Total time:       0.52 min
[32m[20221208 13:45:52 @agent_ppo2.py:139][0m 43008 total steps have happened
[32m[20221208 13:45:52 @agent_ppo2.py:115][0m #------------------------ Iteration 21 --------------------------#
[32m[20221208 13:45:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:45:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:52 @agent_ppo2.py:179][0m |           0.0041 |          12.7831 |           0.3105 |
[32m[20221208 13:45:52 @agent_ppo2.py:179][0m |          -0.0093 |          11.5826 |           0.3101 |
[32m[20221208 13:45:52 @agent_ppo2.py:179][0m |          -0.0098 |          11.2684 |           0.3101 |
[32m[20221208 13:45:52 @agent_ppo2.py:179][0m |          -0.0161 |          11.1182 |           0.3111 |
[32m[20221208 13:45:52 @agent_ppo2.py:179][0m |          -0.0161 |          10.9355 |           0.3108 |
[32m[20221208 13:45:53 @agent_ppo2.py:179][0m |          -0.0179 |          10.8526 |           0.3115 |
[32m[20221208 13:45:53 @agent_ppo2.py:179][0m |          -0.0193 |          10.7760 |           0.3117 |
[32m[20221208 13:45:53 @agent_ppo2.py:179][0m |          -0.0229 |          10.6937 |           0.3117 |
[32m[20221208 13:45:53 @agent_ppo2.py:179][0m |          -0.0201 |          10.6285 |           0.3115 |
[32m[20221208 13:45:53 @agent_ppo2.py:179][0m |          -0.0247 |          10.6197 |           0.3119 |
[32m[20221208 13:45:53 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:45:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 228.50
[32m[20221208 13:45:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 276.05
[32m[20221208 13:45:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 228.78
[32m[20221208 13:45:53 @agent_ppo2.py:137][0m Total time:       0.55 min
[32m[20221208 13:45:53 @agent_ppo2.py:139][0m 45056 total steps have happened
[32m[20221208 13:45:53 @agent_ppo2.py:115][0m #------------------------ Iteration 22 --------------------------#
[32m[20221208 13:45:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |           0.0005 |          13.4265 |           0.3035 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0054 |          13.1305 |           0.3031 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0105 |          12.8846 |           0.3019 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0104 |          12.7760 |           0.2995 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0122 |          12.6703 |           0.3007 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0137 |          12.6702 |           0.3005 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0156 |          12.5033 |           0.2998 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0142 |          12.5699 |           0.2981 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0167 |          12.4603 |           0.2982 |
[32m[20221208 13:45:54 @agent_ppo2.py:179][0m |          -0.0180 |          12.3929 |           0.2983 |
[32m[20221208 13:45:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 258.68
[32m[20221208 13:45:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 286.73
[32m[20221208 13:45:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 293.21
[32m[20221208 13:45:55 @agent_ppo2.py:137][0m Total time:       0.57 min
[32m[20221208 13:45:55 @agent_ppo2.py:139][0m 47104 total steps have happened
[32m[20221208 13:45:55 @agent_ppo2.py:115][0m #------------------------ Iteration 23 --------------------------#
[32m[20221208 13:45:55 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:55 @agent_ppo2.py:179][0m |           0.0050 |          15.7906 |           0.2979 |
[32m[20221208 13:45:55 @agent_ppo2.py:179][0m |          -0.0070 |          15.1852 |           0.2981 |
[32m[20221208 13:45:55 @agent_ppo2.py:179][0m |          -0.0111 |          14.8118 |           0.2977 |
[32m[20221208 13:45:55 @agent_ppo2.py:179][0m |          -0.0127 |          14.5869 |           0.2973 |
[32m[20221208 13:45:55 @agent_ppo2.py:179][0m |          -0.0138 |          14.4141 |           0.2971 |
[32m[20221208 13:45:55 @agent_ppo2.py:179][0m |          -0.0151 |          14.3668 |           0.2973 |
[32m[20221208 13:45:55 @agent_ppo2.py:179][0m |          -0.0171 |          14.2555 |           0.2973 |
[32m[20221208 13:45:56 @agent_ppo2.py:179][0m |          -0.0201 |          14.0167 |           0.2968 |
[32m[20221208 13:45:56 @agent_ppo2.py:179][0m |          -0.0203 |          13.8832 |           0.2984 |
[32m[20221208 13:45:56 @agent_ppo2.py:179][0m |          -0.0211 |          13.7607 |           0.2980 |
[32m[20221208 13:45:56 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 242.55
[32m[20221208 13:45:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 263.29
[32m[20221208 13:45:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 209.53
[32m[20221208 13:45:56 @agent_ppo2.py:137][0m Total time:       0.60 min
[32m[20221208 13:45:56 @agent_ppo2.py:139][0m 49152 total steps have happened
[32m[20221208 13:45:56 @agent_ppo2.py:115][0m #------------------------ Iteration 24 --------------------------#
[32m[20221208 13:45:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:45:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |           0.0053 |          13.4354 |           0.2925 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0046 |          13.2248 |           0.2935 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0110 |          13.1600 |           0.2942 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0137 |          12.9999 |           0.2944 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0165 |          12.9645 |           0.2934 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0148 |          12.9556 |           0.2946 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0178 |          12.8089 |           0.2944 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0199 |          12.8360 |           0.2938 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0198 |          12.6460 |           0.2942 |
[32m[20221208 13:45:57 @agent_ppo2.py:179][0m |          -0.0200 |          12.5795 |           0.2941 |
[32m[20221208 13:45:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:45:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 263.52
[32m[20221208 13:45:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 269.43
[32m[20221208 13:45:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 219.57
[32m[20221208 13:45:58 @agent_ppo2.py:137][0m Total time:       0.62 min
[32m[20221208 13:45:58 @agent_ppo2.py:139][0m 51200 total steps have happened
[32m[20221208 13:45:58 @agent_ppo2.py:115][0m #------------------------ Iteration 25 --------------------------#
[32m[20221208 13:45:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |           0.0044 |          13.7565 |           0.3072 |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |          -0.0030 |          13.0734 |           0.3079 |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |          -0.0110 |          12.9290 |           0.3076 |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |          -0.0128 |          12.7663 |           0.3078 |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |          -0.0180 |          12.6633 |           0.3089 |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |          -0.0200 |          12.6071 |           0.3099 |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |          -0.0232 |          12.4773 |           0.3102 |
[32m[20221208 13:45:58 @agent_ppo2.py:179][0m |          -0.0225 |          12.3872 |           0.3118 |
[32m[20221208 13:45:59 @agent_ppo2.py:179][0m |          -0.0254 |          12.3381 |           0.3110 |
[32m[20221208 13:45:59 @agent_ppo2.py:179][0m |          -0.0259 |          12.2358 |           0.3123 |
[32m[20221208 13:45:59 @agent_ppo2.py:124][0m Policy update time: 0.62 s
[32m[20221208 13:45:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 242.02
[32m[20221208 13:45:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 281.33
[32m[20221208 13:45:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 226.17
[32m[20221208 13:45:59 @agent_ppo2.py:137][0m Total time:       0.64 min
[32m[20221208 13:45:59 @agent_ppo2.py:139][0m 53248 total steps have happened
[32m[20221208 13:45:59 @agent_ppo2.py:115][0m #------------------------ Iteration 26 --------------------------#
[32m[20221208 13:45:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:45:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:45:59 @agent_ppo2.py:179][0m |           0.0042 |          13.7291 |           0.3082 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0134 |          12.8058 |           0.3065 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0159 |          12.3687 |           0.3053 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0196 |          12.1799 |           0.3065 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0214 |          11.9169 |           0.3047 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0227 |          11.6815 |           0.3039 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0237 |          11.5646 |           0.3043 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0265 |          11.4114 |           0.3032 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0291 |          11.3477 |           0.3044 |
[32m[20221208 13:46:00 @agent_ppo2.py:179][0m |          -0.0306 |          11.2207 |           0.3040 |
[32m[20221208 13:46:00 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 222.25
[32m[20221208 13:46:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 304.13
[32m[20221208 13:46:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 243.30
[32m[20221208 13:46:00 @agent_ppo2.py:137][0m Total time:       0.67 min
[32m[20221208 13:46:00 @agent_ppo2.py:139][0m 55296 total steps have happened
[32m[20221208 13:46:00 @agent_ppo2.py:115][0m #------------------------ Iteration 27 --------------------------#
[32m[20221208 13:46:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |           0.0063 |          16.3031 |           0.3114 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0048 |          15.2298 |           0.3109 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0095 |          14.8695 |           0.3104 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0114 |          14.7064 |           0.3109 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0119 |          14.6232 |           0.3108 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0125 |          14.5278 |           0.3103 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0158 |          14.4517 |           0.3115 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0165 |          14.4092 |           0.3112 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0168 |          14.3201 |           0.3117 |
[32m[20221208 13:46:01 @agent_ppo2.py:179][0m |          -0.0186 |          14.2641 |           0.3122 |
[32m[20221208 13:46:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 279.82
[32m[20221208 13:46:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 293.23
[32m[20221208 13:46:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 339.37
[32m[20221208 13:46:02 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 339.37
[32m[20221208 13:46:02 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 339.37
[32m[20221208 13:46:02 @agent_ppo2.py:137][0m Total time:       0.69 min
[32m[20221208 13:46:02 @agent_ppo2.py:139][0m 57344 total steps have happened
[32m[20221208 13:46:02 @agent_ppo2.py:115][0m #------------------------ Iteration 28 --------------------------#
[32m[20221208 13:46:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:02 @agent_ppo2.py:179][0m |           0.0046 |          14.4719 |           0.3115 |
[32m[20221208 13:46:02 @agent_ppo2.py:179][0m |          -0.0083 |          13.5208 |           0.3107 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0136 |          13.2426 |           0.3104 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0159 |          13.1008 |           0.3109 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0161 |          12.9927 |           0.3113 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0177 |          12.8562 |           0.3097 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0199 |          12.7800 |           0.3093 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0209 |          12.7794 |           0.3098 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0218 |          12.6471 |           0.3103 |
[32m[20221208 13:46:03 @agent_ppo2.py:179][0m |          -0.0227 |          12.6127 |           0.3094 |
[32m[20221208 13:46:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 250.64
[32m[20221208 13:46:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 329.40
[32m[20221208 13:46:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 254.76
[32m[20221208 13:46:03 @agent_ppo2.py:137][0m Total time:       0.72 min
[32m[20221208 13:46:03 @agent_ppo2.py:139][0m 59392 total steps have happened
[32m[20221208 13:46:03 @agent_ppo2.py:115][0m #------------------------ Iteration 29 --------------------------#
[32m[20221208 13:46:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |           0.0029 |          21.3768 |           0.3177 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0079 |          20.1297 |           0.3171 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0097 |          19.7524 |           0.3180 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0127 |          19.5638 |           0.3179 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0155 |          19.4321 |           0.3190 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0162 |          19.3513 |           0.3195 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0191 |          19.1950 |           0.3205 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0189 |          19.1715 |           0.3209 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0209 |          19.0332 |           0.3207 |
[32m[20221208 13:46:04 @agent_ppo2.py:179][0m |          -0.0250 |          19.0296 |           0.3217 |
[32m[20221208 13:46:04 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.60
[32m[20221208 13:46:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 343.68
[32m[20221208 13:46:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 241.93
[32m[20221208 13:46:05 @agent_ppo2.py:137][0m Total time:       0.74 min
[32m[20221208 13:46:05 @agent_ppo2.py:139][0m 61440 total steps have happened
[32m[20221208 13:46:05 @agent_ppo2.py:115][0m #------------------------ Iteration 30 --------------------------#
[32m[20221208 13:46:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:05 @agent_ppo2.py:179][0m |           0.0064 |          16.2929 |           0.3242 |
[32m[20221208 13:46:05 @agent_ppo2.py:179][0m |          -0.0086 |          15.6187 |           0.3220 |
[32m[20221208 13:46:05 @agent_ppo2.py:179][0m |          -0.0108 |          15.4092 |           0.3214 |
[32m[20221208 13:46:06 @agent_ppo2.py:179][0m |          -0.0142 |          15.2326 |           0.3217 |
[32m[20221208 13:46:06 @agent_ppo2.py:179][0m |          -0.0112 |          15.1201 |           0.3205 |
[32m[20221208 13:46:06 @agent_ppo2.py:179][0m |          -0.0154 |          15.1445 |           0.3207 |
[32m[20221208 13:46:06 @agent_ppo2.py:179][0m |          -0.0181 |          15.0772 |           0.3208 |
[32m[20221208 13:46:06 @agent_ppo2.py:179][0m |          -0.0191 |          15.0201 |           0.3209 |
[32m[20221208 13:46:06 @agent_ppo2.py:179][0m |          -0.0198 |          15.0081 |           0.3216 |
[32m[20221208 13:46:06 @agent_ppo2.py:179][0m |          -0.0220 |          14.9624 |           0.3221 |
[32m[20221208 13:46:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:46:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 255.53
[32m[20221208 13:46:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.13
[32m[20221208 13:46:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 196.07
[32m[20221208 13:46:06 @agent_ppo2.py:137][0m Total time:       0.77 min
[32m[20221208 13:46:06 @agent_ppo2.py:139][0m 63488 total steps have happened
[32m[20221208 13:46:06 @agent_ppo2.py:115][0m #------------------------ Iteration 31 --------------------------#
[32m[20221208 13:46:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |           0.0028 |          21.3850 |           0.3196 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0105 |          20.3599 |           0.3187 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0130 |          20.2539 |           0.3176 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0181 |          19.9782 |           0.3173 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0220 |          19.8363 |           0.3170 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0242 |          19.6295 |           0.3168 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0241 |          19.6697 |           0.3182 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0265 |          19.3744 |           0.3172 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0280 |          19.2215 |           0.3173 |
[32m[20221208 13:46:07 @agent_ppo2.py:179][0m |          -0.0271 |          19.2388 |           0.3174 |
[32m[20221208 13:46:07 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 313.88
[32m[20221208 13:46:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 383.51
[32m[20221208 13:46:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 263.79
[32m[20221208 13:46:08 @agent_ppo2.py:137][0m Total time:       0.79 min
[32m[20221208 13:46:08 @agent_ppo2.py:139][0m 65536 total steps have happened
[32m[20221208 13:46:08 @agent_ppo2.py:115][0m #------------------------ Iteration 32 --------------------------#
[32m[20221208 13:46:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:08 @agent_ppo2.py:179][0m |           0.0037 |          21.6463 |           0.3169 |
[32m[20221208 13:46:08 @agent_ppo2.py:179][0m |          -0.0083 |          17.2118 |           0.3147 |
[32m[20221208 13:46:08 @agent_ppo2.py:179][0m |          -0.0128 |          15.3897 |           0.3158 |
[32m[20221208 13:46:08 @agent_ppo2.py:179][0m |          -0.0192 |          14.8538 |           0.3150 |
[32m[20221208 13:46:09 @agent_ppo2.py:179][0m |          -0.0201 |          14.5564 |           0.3148 |
[32m[20221208 13:46:09 @agent_ppo2.py:179][0m |          -0.0216 |          14.0568 |           0.3154 |
[32m[20221208 13:46:09 @agent_ppo2.py:179][0m |          -0.0213 |          13.7989 |           0.3154 |
[32m[20221208 13:46:09 @agent_ppo2.py:179][0m |          -0.0229 |          13.5348 |           0.3150 |
[32m[20221208 13:46:09 @agent_ppo2.py:179][0m |          -0.0243 |          13.2917 |           0.3154 |
[32m[20221208 13:46:09 @agent_ppo2.py:179][0m |          -0.0261 |          13.1073 |           0.3150 |
[32m[20221208 13:46:09 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 312.22
[32m[20221208 13:46:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 375.11
[32m[20221208 13:46:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 246.22
[32m[20221208 13:46:09 @agent_ppo2.py:137][0m Total time:       0.81 min
[32m[20221208 13:46:09 @agent_ppo2.py:139][0m 67584 total steps have happened
[32m[20221208 13:46:09 @agent_ppo2.py:115][0m #------------------------ Iteration 33 --------------------------#
[32m[20221208 13:46:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |           0.0046 |          25.5239 |           0.3186 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0063 |          23.8105 |           0.3168 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0154 |          23.2431 |           0.3174 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0189 |          22.8622 |           0.3165 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0211 |          22.6026 |           0.3167 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0235 |          22.4085 |           0.3163 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0274 |          22.1185 |           0.3163 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0275 |          21.8190 |           0.3161 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0285 |          21.5745 |           0.3158 |
[32m[20221208 13:46:10 @agent_ppo2.py:179][0m |          -0.0290 |          21.2907 |           0.3155 |
[32m[20221208 13:46:10 @agent_ppo2.py:124][0m Policy update time: 0.86 s
[32m[20221208 13:46:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 324.55
[32m[20221208 13:46:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 337.28
[32m[20221208 13:46:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 280.05
[32m[20221208 13:46:11 @agent_ppo2.py:137][0m Total time:       0.84 min
[32m[20221208 13:46:11 @agent_ppo2.py:139][0m 69632 total steps have happened
[32m[20221208 13:46:11 @agent_ppo2.py:115][0m #------------------------ Iteration 34 --------------------------#
[32m[20221208 13:46:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:46:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:11 @agent_ppo2.py:179][0m |          -0.0003 |          16.5142 |           0.3218 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0103 |          15.6658 |           0.3205 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0152 |          15.5112 |           0.3205 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0187 |          15.4000 |           0.3207 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0220 |          15.1780 |           0.3205 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0245 |          15.0330 |           0.3199 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0242 |          14.9418 |           0.3198 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0252 |          14.8804 |           0.3216 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0283 |          14.8076 |           0.3210 |
[32m[20221208 13:46:12 @agent_ppo2.py:179][0m |          -0.0272 |          14.7411 |           0.3207 |
[32m[20221208 13:46:12 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 13:46:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 315.78
[32m[20221208 13:46:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 345.85
[32m[20221208 13:46:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 343.83
[32m[20221208 13:46:13 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 343.83
[32m[20221208 13:46:13 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 343.83
[32m[20221208 13:46:13 @agent_ppo2.py:137][0m Total time:       0.87 min
[32m[20221208 13:46:13 @agent_ppo2.py:139][0m 71680 total steps have happened
[32m[20221208 13:46:13 @agent_ppo2.py:115][0m #------------------------ Iteration 35 --------------------------#
[32m[20221208 13:46:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:46:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:13 @agent_ppo2.py:179][0m |           0.0065 |          15.1360 |           0.3225 |
[32m[20221208 13:46:13 @agent_ppo2.py:179][0m |          -0.0075 |          14.4154 |           0.3246 |
[32m[20221208 13:46:13 @agent_ppo2.py:179][0m |          -0.0144 |          14.2158 |           0.3236 |
[32m[20221208 13:46:13 @agent_ppo2.py:179][0m |          -0.0165 |          14.1606 |           0.3226 |
[32m[20221208 13:46:13 @agent_ppo2.py:179][0m |          -0.0182 |          14.1305 |           0.3220 |
[32m[20221208 13:46:13 @agent_ppo2.py:179][0m |          -0.0194 |          13.9917 |           0.3215 |
[32m[20221208 13:46:13 @agent_ppo2.py:179][0m |          -0.0216 |          13.9379 |           0.3212 |
[32m[20221208 13:46:14 @agent_ppo2.py:179][0m |          -0.0224 |          13.8929 |           0.3214 |
[32m[20221208 13:46:14 @agent_ppo2.py:179][0m |          -0.0229 |          13.9202 |           0.3203 |
[32m[20221208 13:46:14 @agent_ppo2.py:179][0m |          -0.0261 |          13.7606 |           0.3214 |
[32m[20221208 13:46:14 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:46:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.79
[32m[20221208 13:46:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 349.80
[32m[20221208 13:46:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 318.99
[32m[20221208 13:46:14 @agent_ppo2.py:137][0m Total time:       0.90 min
[32m[20221208 13:46:14 @agent_ppo2.py:139][0m 73728 total steps have happened
[32m[20221208 13:46:14 @agent_ppo2.py:115][0m #------------------------ Iteration 36 --------------------------#
[32m[20221208 13:46:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:46:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |           0.0025 |          16.7346 |           0.3106 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0091 |          16.0132 |           0.3118 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0141 |          15.6590 |           0.3108 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0173 |          15.2539 |           0.3094 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0172 |          14.9991 |           0.3106 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0206 |          14.8726 |           0.3110 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0230 |          14.6224 |           0.3107 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0247 |          14.4954 |           0.3119 |
[32m[20221208 13:46:15 @agent_ppo2.py:179][0m |          -0.0236 |          14.3919 |           0.3114 |
[32m[20221208 13:46:16 @agent_ppo2.py:179][0m |          -0.0265 |          14.3433 |           0.3128 |
[32m[20221208 13:46:16 @agent_ppo2.py:124][0m Policy update time: 1.01 s
[32m[20221208 13:46:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.67
[32m[20221208 13:46:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 294.98
[32m[20221208 13:46:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 261.35
[32m[20221208 13:46:16 @agent_ppo2.py:137][0m Total time:       0.93 min
[32m[20221208 13:46:16 @agent_ppo2.py:139][0m 75776 total steps have happened
[32m[20221208 13:46:16 @agent_ppo2.py:115][0m #------------------------ Iteration 37 --------------------------#
[32m[20221208 13:46:16 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:46:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0001 |          19.8609 |           0.3173 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0149 |          19.1015 |           0.3149 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0183 |          18.6294 |           0.3163 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0198 |          18.1830 |           0.3166 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0234 |          17.6404 |           0.3160 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0209 |          17.7239 |           0.3164 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0257 |          16.8886 |           0.3165 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0256 |          16.8252 |           0.3175 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0277 |          16.4985 |           0.3175 |
[32m[20221208 13:46:17 @agent_ppo2.py:179][0m |          -0.0264 |          16.2384 |           0.3179 |
[32m[20221208 13:46:17 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:46:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.32
[32m[20221208 13:46:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 356.57
[32m[20221208 13:46:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 284.74
[32m[20221208 13:46:18 @agent_ppo2.py:137][0m Total time:       0.95 min
[32m[20221208 13:46:18 @agent_ppo2.py:139][0m 77824 total steps have happened
[32m[20221208 13:46:18 @agent_ppo2.py:115][0m #------------------------ Iteration 38 --------------------------#
[32m[20221208 13:46:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:46:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:18 @agent_ppo2.py:179][0m |          -0.0027 |          20.7790 |           0.3235 |
[32m[20221208 13:46:18 @agent_ppo2.py:179][0m |          -0.0126 |          19.8878 |           0.3223 |
[32m[20221208 13:46:18 @agent_ppo2.py:179][0m |          -0.0151 |          19.4823 |           0.3220 |
[32m[20221208 13:46:18 @agent_ppo2.py:179][0m |          -0.0191 |          19.1938 |           0.3224 |
[32m[20221208 13:46:18 @agent_ppo2.py:179][0m |          -0.0193 |          18.8592 |           0.3215 |
[32m[20221208 13:46:18 @agent_ppo2.py:179][0m |          -0.0222 |          18.5990 |           0.3211 |
[32m[20221208 13:46:19 @agent_ppo2.py:179][0m |          -0.0235 |          18.4414 |           0.3213 |
[32m[20221208 13:46:19 @agent_ppo2.py:179][0m |          -0.0265 |          18.2166 |           0.3208 |
[32m[20221208 13:46:19 @agent_ppo2.py:179][0m |          -0.0269 |          18.1245 |           0.3206 |
[32m[20221208 13:46:19 @agent_ppo2.py:179][0m |          -0.0271 |          17.9285 |           0.3206 |
[32m[20221208 13:46:19 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 13:46:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 265.35
[32m[20221208 13:46:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 280.81
[32m[20221208 13:46:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 313.97
[32m[20221208 13:46:19 @agent_ppo2.py:137][0m Total time:       0.98 min
[32m[20221208 13:46:19 @agent_ppo2.py:139][0m 79872 total steps have happened
[32m[20221208 13:46:19 @agent_ppo2.py:115][0m #------------------------ Iteration 39 --------------------------#
[32m[20221208 13:46:20 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 13:46:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |           0.0023 |          20.6586 |           0.3218 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0096 |          20.1213 |           0.3227 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0121 |          19.6423 |           0.3210 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0140 |          19.4488 |           0.3217 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0175 |          19.1327 |           0.3211 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0197 |          18.9462 |           0.3207 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0196 |          18.7205 |           0.3203 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0223 |          18.6181 |           0.3214 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0216 |          18.7183 |           0.3207 |
[32m[20221208 13:46:20 @agent_ppo2.py:179][0m |          -0.0245 |          18.3894 |           0.3214 |
[32m[20221208 13:46:20 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 13:46:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 298.87
[32m[20221208 13:46:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 373.42
[32m[20221208 13:46:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 307.49
[32m[20221208 13:46:21 @agent_ppo2.py:137][0m Total time:       1.01 min
[32m[20221208 13:46:21 @agent_ppo2.py:139][0m 81920 total steps have happened
[32m[20221208 13:46:21 @agent_ppo2.py:115][0m #------------------------ Iteration 40 --------------------------#
[32m[20221208 13:46:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:46:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:21 @agent_ppo2.py:179][0m |           0.0059 |          22.6289 |           0.3280 |
[32m[20221208 13:46:21 @agent_ppo2.py:179][0m |          -0.0089 |          21.9038 |           0.3280 |
[32m[20221208 13:46:21 @agent_ppo2.py:179][0m |          -0.0112 |          21.6657 |           0.3288 |
[32m[20221208 13:46:21 @agent_ppo2.py:179][0m |          -0.0134 |          21.4764 |           0.3280 |
[32m[20221208 13:46:22 @agent_ppo2.py:179][0m |          -0.0155 |          21.2444 |           0.3280 |
[32m[20221208 13:46:22 @agent_ppo2.py:179][0m |          -0.0191 |          21.0933 |           0.3274 |
[32m[20221208 13:46:22 @agent_ppo2.py:179][0m |          -0.0211 |          20.9940 |           0.3289 |
[32m[20221208 13:46:22 @agent_ppo2.py:179][0m |          -0.0206 |          21.0183 |           0.3291 |
[32m[20221208 13:46:22 @agent_ppo2.py:179][0m |          -0.0225 |          20.9807 |           0.3296 |
[32m[20221208 13:46:22 @agent_ppo2.py:179][0m |          -0.0242 |          20.7265 |           0.3289 |
[32m[20221208 13:46:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:46:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.88
[32m[20221208 13:46:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 364.04
[32m[20221208 13:46:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 229.42
[32m[20221208 13:46:22 @agent_ppo2.py:137][0m Total time:       1.03 min
[32m[20221208 13:46:22 @agent_ppo2.py:139][0m 83968 total steps have happened
[32m[20221208 13:46:22 @agent_ppo2.py:115][0m #------------------------ Iteration 41 --------------------------#
[32m[20221208 13:46:23 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 13:46:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |           0.0070 |          21.4321 |           0.3105 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0074 |          21.0302 |           0.3119 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0127 |          20.8358 |           0.3097 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0148 |          20.6700 |           0.3115 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0151 |          20.6251 |           0.3100 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0185 |          20.4947 |           0.3104 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0216 |          20.5025 |           0.3084 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0193 |          20.4104 |           0.3083 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0218 |          20.3726 |           0.3089 |
[32m[20221208 13:46:23 @agent_ppo2.py:179][0m |          -0.0224 |          20.3802 |           0.3090 |
[32m[20221208 13:46:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:46:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.04
[32m[20221208 13:46:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 383.84
[32m[20221208 13:46:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 315.49
[32m[20221208 13:46:24 @agent_ppo2.py:137][0m Total time:       1.06 min
[32m[20221208 13:46:24 @agent_ppo2.py:139][0m 86016 total steps have happened
[32m[20221208 13:46:24 @agent_ppo2.py:115][0m #------------------------ Iteration 42 --------------------------#
[32m[20221208 13:46:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:24 @agent_ppo2.py:179][0m |           0.0055 |          21.7551 |           0.3071 |
[32m[20221208 13:46:24 @agent_ppo2.py:179][0m |          -0.0078 |          21.4801 |           0.3075 |
[32m[20221208 13:46:24 @agent_ppo2.py:179][0m |          -0.0114 |          21.1779 |           0.3064 |
[32m[20221208 13:46:24 @agent_ppo2.py:179][0m |          -0.0148 |          21.1832 |           0.3084 |
[32m[20221208 13:46:25 @agent_ppo2.py:179][0m |          -0.0179 |          20.9699 |           0.3079 |
[32m[20221208 13:46:25 @agent_ppo2.py:179][0m |          -0.0209 |          20.8937 |           0.3082 |
[32m[20221208 13:46:25 @agent_ppo2.py:179][0m |          -0.0190 |          20.7855 |           0.3078 |
[32m[20221208 13:46:25 @agent_ppo2.py:179][0m |          -0.0194 |          20.7346 |           0.3076 |
[32m[20221208 13:46:25 @agent_ppo2.py:179][0m |          -0.0210 |          20.6958 |           0.3093 |
[32m[20221208 13:46:25 @agent_ppo2.py:179][0m |          -0.0240 |          20.6554 |           0.3090 |
[32m[20221208 13:46:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:46:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 309.75
[32m[20221208 13:46:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 379.02
[32m[20221208 13:46:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 269.97
[32m[20221208 13:46:25 @agent_ppo2.py:137][0m Total time:       1.08 min
[32m[20221208 13:46:25 @agent_ppo2.py:139][0m 88064 total steps have happened
[32m[20221208 13:46:25 @agent_ppo2.py:115][0m #------------------------ Iteration 43 --------------------------#
[32m[20221208 13:46:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |           0.0099 |          27.1667 |           0.2946 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0067 |          26.8395 |           0.2938 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0131 |          26.7749 |           0.2923 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0158 |          26.6701 |           0.2936 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0199 |          26.4730 |           0.2943 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0208 |          26.4272 |           0.2932 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0244 |          26.3139 |           0.2938 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0212 |          26.2944 |           0.2935 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0253 |          26.1713 |           0.2946 |
[32m[20221208 13:46:26 @agent_ppo2.py:179][0m |          -0.0241 |          26.1696 |           0.2944 |
[32m[20221208 13:46:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:46:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 324.46
[32m[20221208 13:46:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 383.30
[32m[20221208 13:46:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.01
[32m[20221208 13:46:27 @agent_ppo2.py:137][0m Total time:       1.11 min
[32m[20221208 13:46:27 @agent_ppo2.py:139][0m 90112 total steps have happened
[32m[20221208 13:46:27 @agent_ppo2.py:115][0m #------------------------ Iteration 44 --------------------------#
[32m[20221208 13:46:27 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 13:46:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:27 @agent_ppo2.py:179][0m |           0.0035 |          15.3859 |           0.3205 |
[32m[20221208 13:46:27 @agent_ppo2.py:179][0m |          -0.0041 |          14.9336 |           0.3187 |
[32m[20221208 13:46:27 @agent_ppo2.py:179][0m |          -0.0085 |          14.7714 |           0.3197 |
[32m[20221208 13:46:28 @agent_ppo2.py:179][0m |          -0.0117 |          14.6642 |           0.3190 |
[32m[20221208 13:46:28 @agent_ppo2.py:179][0m |          -0.0134 |          14.5552 |           0.3185 |
[32m[20221208 13:46:28 @agent_ppo2.py:179][0m |          -0.0151 |          14.5244 |           0.3187 |
[32m[20221208 13:46:28 @agent_ppo2.py:179][0m |          -0.0144 |          14.4766 |           0.3174 |
[32m[20221208 13:46:28 @agent_ppo2.py:179][0m |          -0.0181 |          14.5053 |           0.3163 |
[32m[20221208 13:46:28 @agent_ppo2.py:179][0m |          -0.0164 |          14.4813 |           0.3157 |
[32m[20221208 13:46:28 @agent_ppo2.py:179][0m |          -0.0163 |          14.4546 |           0.3161 |
[32m[20221208 13:46:28 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 13:46:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 232.15
[32m[20221208 13:46:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 260.97
[32m[20221208 13:46:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 218.63
[32m[20221208 13:46:28 @agent_ppo2.py:137][0m Total time:       1.13 min
[32m[20221208 13:46:28 @agent_ppo2.py:139][0m 92160 total steps have happened
[32m[20221208 13:46:28 @agent_ppo2.py:115][0m #------------------------ Iteration 45 --------------------------#
[32m[20221208 13:46:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:29 @agent_ppo2.py:179][0m |           0.0041 |          33.9180 |           0.3126 |
[32m[20221208 13:46:29 @agent_ppo2.py:179][0m |          -0.0062 |          32.9755 |           0.3107 |
[32m[20221208 13:46:29 @agent_ppo2.py:179][0m |          -0.0110 |          32.8484 |           0.3108 |
[32m[20221208 13:46:29 @agent_ppo2.py:179][0m |          -0.0174 |          32.6322 |           0.3118 |
[32m[20221208 13:46:29 @agent_ppo2.py:179][0m |          -0.0180 |          32.3794 |           0.3105 |
[32m[20221208 13:46:29 @agent_ppo2.py:179][0m |          -0.0200 |          32.2625 |           0.3115 |
[32m[20221208 13:46:29 @agent_ppo2.py:179][0m |          -0.0230 |          32.2064 |           0.3114 |
[32m[20221208 13:46:30 @agent_ppo2.py:179][0m |          -0.0251 |          32.0548 |           0.3116 |
[32m[20221208 13:46:30 @agent_ppo2.py:179][0m |          -0.0244 |          31.9667 |           0.3118 |
[32m[20221208 13:46:30 @agent_ppo2.py:179][0m |          -0.0273 |          31.8078 |           0.3114 |
[32m[20221208 13:46:30 @agent_ppo2.py:124][0m Policy update time: 0.82 s
[32m[20221208 13:46:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 350.02
[32m[20221208 13:46:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 378.00
[32m[20221208 13:46:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 359.82
[32m[20221208 13:46:30 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 359.82
[32m[20221208 13:46:30 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 359.82
[32m[20221208 13:46:30 @agent_ppo2.py:137][0m Total time:       1.16 min
[32m[20221208 13:46:30 @agent_ppo2.py:139][0m 94208 total steps have happened
[32m[20221208 13:46:30 @agent_ppo2.py:115][0m #------------------------ Iteration 46 --------------------------#
[32m[20221208 13:46:31 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 13:46:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |           0.0042 |          21.6499 |           0.3052 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0108 |          21.2866 |           0.3042 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0110 |          21.1652 |           0.3040 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0138 |          21.0752 |           0.3027 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0184 |          21.0493 |           0.3029 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0194 |          20.9783 |           0.3038 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0188 |          20.9279 |           0.3033 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0211 |          20.8294 |           0.3033 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0184 |          20.9492 |           0.3026 |
[32m[20221208 13:46:31 @agent_ppo2.py:179][0m |          -0.0198 |          20.9208 |           0.3038 |
[32m[20221208 13:46:31 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:46:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 295.06
[32m[20221208 13:46:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 372.69
[32m[20221208 13:46:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 258.16
[32m[20221208 13:46:32 @agent_ppo2.py:137][0m Total time:       1.19 min
[32m[20221208 13:46:32 @agent_ppo2.py:139][0m 96256 total steps have happened
[32m[20221208 13:46:32 @agent_ppo2.py:115][0m #------------------------ Iteration 47 --------------------------#
[32m[20221208 13:46:32 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:46:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:32 @agent_ppo2.py:179][0m |           0.0065 |          24.6407 |           0.3074 |
[32m[20221208 13:46:32 @agent_ppo2.py:179][0m |          -0.0074 |          23.6861 |           0.3061 |
[32m[20221208 13:46:32 @agent_ppo2.py:179][0m |          -0.0149 |          23.2061 |           0.3065 |
[32m[20221208 13:46:33 @agent_ppo2.py:179][0m |          -0.0196 |          22.5842 |           0.3055 |
[32m[20221208 13:46:33 @agent_ppo2.py:179][0m |          -0.0222 |          22.3237 |           0.3052 |
[32m[20221208 13:46:33 @agent_ppo2.py:179][0m |          -0.0235 |          22.1015 |           0.3052 |
[32m[20221208 13:46:33 @agent_ppo2.py:179][0m |          -0.0241 |          21.9180 |           0.3047 |
[32m[20221208 13:46:33 @agent_ppo2.py:179][0m |          -0.0266 |          21.7549 |           0.3047 |
[32m[20221208 13:46:33 @agent_ppo2.py:179][0m |          -0.0266 |          21.5107 |           0.3044 |
[32m[20221208 13:46:33 @agent_ppo2.py:179][0m |          -0.0286 |          21.4376 |           0.3043 |
[32m[20221208 13:46:33 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 13:46:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.94
[32m[20221208 13:46:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 379.95
[32m[20221208 13:46:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 280.98
[32m[20221208 13:46:33 @agent_ppo2.py:137][0m Total time:       1.22 min
[32m[20221208 13:46:33 @agent_ppo2.py:139][0m 98304 total steps have happened
[32m[20221208 13:46:33 @agent_ppo2.py:115][0m #------------------------ Iteration 48 --------------------------#
[32m[20221208 13:46:34 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 13:46:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |           0.0060 |          28.9112 |           0.2992 |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |          -0.0081 |          28.3600 |           0.2989 |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |          -0.0133 |          28.0850 |           0.2987 |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |          -0.0131 |          27.9690 |           0.2975 |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |          -0.0168 |          27.7520 |           0.2959 |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |          -0.0143 |          27.5914 |           0.2975 |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |          -0.0211 |          27.4668 |           0.2961 |
[32m[20221208 13:46:34 @agent_ppo2.py:179][0m |          -0.0201 |          27.4017 |           0.2968 |
[32m[20221208 13:46:35 @agent_ppo2.py:179][0m |          -0.0224 |          27.3058 |           0.2967 |
[32m[20221208 13:46:35 @agent_ppo2.py:179][0m |          -0.0232 |          27.2892 |           0.2950 |
[32m[20221208 13:46:35 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 13:46:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.76
[32m[20221208 13:46:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 376.30
[32m[20221208 13:46:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 285.93
[32m[20221208 13:46:35 @agent_ppo2.py:137][0m Total time:       1.24 min
[32m[20221208 13:46:35 @agent_ppo2.py:139][0m 100352 total steps have happened
[32m[20221208 13:46:35 @agent_ppo2.py:115][0m #------------------------ Iteration 49 --------------------------#
[32m[20221208 13:46:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |           0.0027 |          35.8306 |           0.2966 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0159 |          35.0841 |           0.2950 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0182 |          34.4747 |           0.2941 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0203 |          34.2694 |           0.2923 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0225 |          33.9297 |           0.2930 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0245 |          33.8585 |           0.2927 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0230 |          33.5641 |           0.2913 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0244 |          33.3591 |           0.2902 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0259 |          33.0833 |           0.2916 |
[32m[20221208 13:46:36 @agent_ppo2.py:179][0m |          -0.0277 |          32.8738 |           0.2923 |
[32m[20221208 13:46:36 @agent_ppo2.py:124][0m Policy update time: 0.77 s
[32m[20221208 13:46:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 399.26
[32m[20221208 13:46:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 407.86
[32m[20221208 13:46:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.07
[32m[20221208 13:46:37 @agent_ppo2.py:137][0m Total time:       1.27 min
[32m[20221208 13:46:37 @agent_ppo2.py:139][0m 102400 total steps have happened
[32m[20221208 13:46:37 @agent_ppo2.py:115][0m #------------------------ Iteration 50 --------------------------#
[32m[20221208 13:46:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:46:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:37 @agent_ppo2.py:179][0m |           0.0066 |          30.3549 |           0.2867 |
[32m[20221208 13:46:37 @agent_ppo2.py:179][0m |          -0.0079 |          29.3636 |           0.2839 |
[32m[20221208 13:46:37 @agent_ppo2.py:179][0m |          -0.0139 |          29.1874 |           0.2833 |
[32m[20221208 13:46:37 @agent_ppo2.py:179][0m |          -0.0168 |          28.9373 |           0.2825 |
[32m[20221208 13:46:37 @agent_ppo2.py:179][0m |          -0.0164 |          28.8513 |           0.2813 |
[32m[20221208 13:46:37 @agent_ppo2.py:179][0m |          -0.0189 |          28.7393 |           0.2808 |
[32m[20221208 13:46:38 @agent_ppo2.py:179][0m |          -0.0202 |          28.5990 |           0.2801 |
[32m[20221208 13:46:38 @agent_ppo2.py:179][0m |          -0.0226 |          28.5166 |           0.2792 |
[32m[20221208 13:46:38 @agent_ppo2.py:179][0m |          -0.0230 |          28.4628 |           0.2785 |
[32m[20221208 13:46:38 @agent_ppo2.py:179][0m |          -0.0241 |          28.4271 |           0.2778 |
[32m[20221208 13:46:38 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:46:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.74
[32m[20221208 13:46:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 374.37
[32m[20221208 13:46:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 291.05
[32m[20221208 13:46:38 @agent_ppo2.py:137][0m Total time:       1.30 min
[32m[20221208 13:46:38 @agent_ppo2.py:139][0m 104448 total steps have happened
[32m[20221208 13:46:38 @agent_ppo2.py:115][0m #------------------------ Iteration 51 --------------------------#
[32m[20221208 13:46:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |           0.0045 |          19.5600 |           0.2686 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0089 |          18.4849 |           0.2681 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0124 |          18.1685 |           0.2674 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0191 |          17.8643 |           0.2678 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0223 |          17.8780 |           0.2689 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0254 |          17.6492 |           0.2686 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0261 |          17.5381 |           0.2698 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0264 |          17.6223 |           0.2700 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0277 |          17.2841 |           0.2695 |
[32m[20221208 13:46:39 @agent_ppo2.py:179][0m |          -0.0305 |          17.1348 |           0.2708 |
[32m[20221208 13:46:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 304.03
[32m[20221208 13:46:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 379.34
[32m[20221208 13:46:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 287.95
[32m[20221208 13:46:40 @agent_ppo2.py:137][0m Total time:       1.32 min
[32m[20221208 13:46:40 @agent_ppo2.py:139][0m 106496 total steps have happened
[32m[20221208 13:46:40 @agent_ppo2.py:115][0m #------------------------ Iteration 52 --------------------------#
[32m[20221208 13:46:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:40 @agent_ppo2.py:179][0m |           0.0027 |          26.0460 |           0.2651 |
[32m[20221208 13:46:40 @agent_ppo2.py:179][0m |          -0.0109 |          24.0872 |           0.2656 |
[32m[20221208 13:46:40 @agent_ppo2.py:179][0m |          -0.0164 |          23.2550 |           0.2655 |
[32m[20221208 13:46:40 @agent_ppo2.py:179][0m |          -0.0201 |          22.8018 |           0.2656 |
[32m[20221208 13:46:40 @agent_ppo2.py:179][0m |          -0.0219 |          22.2774 |           0.2657 |
[32m[20221208 13:46:40 @agent_ppo2.py:179][0m |          -0.0261 |          21.6531 |           0.2667 |
[32m[20221208 13:46:40 @agent_ppo2.py:179][0m |          -0.0262 |          21.2087 |           0.2677 |
[32m[20221208 13:46:41 @agent_ppo2.py:179][0m |          -0.0283 |          20.2687 |           0.2675 |
[32m[20221208 13:46:41 @agent_ppo2.py:179][0m |          -0.0301 |          19.9092 |           0.2678 |
[32m[20221208 13:46:41 @agent_ppo2.py:179][0m |          -0.0298 |          19.2769 |           0.2680 |
[32m[20221208 13:46:41 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 286.63
[32m[20221208 13:46:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 348.74
[32m[20221208 13:46:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 309.97
[32m[20221208 13:46:41 @agent_ppo2.py:137][0m Total time:       1.35 min
[32m[20221208 13:46:41 @agent_ppo2.py:139][0m 108544 total steps have happened
[32m[20221208 13:46:41 @agent_ppo2.py:115][0m #------------------------ Iteration 53 --------------------------#
[32m[20221208 13:46:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |           0.0078 |          18.8153 |           0.2697 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0087 |          17.9646 |           0.2706 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0161 |          17.6565 |           0.2685 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0176 |          17.6305 |           0.2690 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0192 |          17.4878 |           0.2695 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0236 |          17.4103 |           0.2703 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0241 |          17.3497 |           0.2693 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0247 |          17.3600 |           0.2703 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0255 |          17.3414 |           0.2705 |
[32m[20221208 13:46:42 @agent_ppo2.py:179][0m |          -0.0266 |          17.2412 |           0.2709 |
[32m[20221208 13:46:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 243.37
[32m[20221208 13:46:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 247.19
[32m[20221208 13:46:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 319.21
[32m[20221208 13:46:43 @agent_ppo2.py:137][0m Total time:       1.37 min
[32m[20221208 13:46:43 @agent_ppo2.py:139][0m 110592 total steps have happened
[32m[20221208 13:46:43 @agent_ppo2.py:115][0m #------------------------ Iteration 54 --------------------------#
[32m[20221208 13:46:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0006 |          31.7294 |           0.2912 |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0116 |          30.5461 |           0.2910 |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0179 |          30.2823 |           0.2905 |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0217 |          30.1881 |           0.2901 |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0228 |          30.0918 |           0.2902 |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0240 |          29.9693 |           0.2907 |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0269 |          29.7433 |           0.2902 |
[32m[20221208 13:46:43 @agent_ppo2.py:179][0m |          -0.0268 |          29.6895 |           0.2900 |
[32m[20221208 13:46:44 @agent_ppo2.py:179][0m |          -0.0266 |          29.8405 |           0.2898 |
[32m[20221208 13:46:44 @agent_ppo2.py:179][0m |          -0.0298 |          29.4432 |           0.2899 |
[32m[20221208 13:46:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.73
[32m[20221208 13:46:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 389.99
[32m[20221208 13:46:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 354.24
[32m[20221208 13:46:44 @agent_ppo2.py:137][0m Total time:       1.39 min
[32m[20221208 13:46:44 @agent_ppo2.py:139][0m 112640 total steps have happened
[32m[20221208 13:46:44 @agent_ppo2.py:115][0m #------------------------ Iteration 55 --------------------------#
[32m[20221208 13:46:44 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |           0.0063 |          26.6138 |           0.2779 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0117 |          24.8779 |           0.2769 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0148 |          23.9560 |           0.2756 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0178 |          23.3016 |           0.2750 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0198 |          22.9239 |           0.2748 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0229 |          22.5336 |           0.2748 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0255 |          22.0603 |           0.2742 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0263 |          21.7149 |           0.2749 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0256 |          21.3296 |           0.2748 |
[32m[20221208 13:46:45 @agent_ppo2.py:179][0m |          -0.0302 |          20.5903 |           0.2750 |
[32m[20221208 13:46:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 338.59
[32m[20221208 13:46:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 398.61
[32m[20221208 13:46:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 306.68
[32m[20221208 13:46:45 @agent_ppo2.py:137][0m Total time:       1.42 min
[32m[20221208 13:46:45 @agent_ppo2.py:139][0m 114688 total steps have happened
[32m[20221208 13:46:45 @agent_ppo2.py:115][0m #------------------------ Iteration 56 --------------------------#
[32m[20221208 13:46:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |           0.0048 |          21.0039 |           0.2700 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0078 |          19.1784 |           0.2709 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0133 |          18.8779 |           0.2702 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0154 |          18.7137 |           0.2712 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0207 |          18.6728 |           0.2717 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0223 |          18.6692 |           0.2725 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0266 |          18.4663 |           0.2729 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0280 |          18.4405 |           0.2731 |
[32m[20221208 13:46:46 @agent_ppo2.py:179][0m |          -0.0280 |          18.3937 |           0.2725 |
[32m[20221208 13:46:47 @agent_ppo2.py:179][0m |          -0.0310 |          18.3643 |           0.2737 |
[32m[20221208 13:46:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:46:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 251.01
[32m[20221208 13:46:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 264.31
[32m[20221208 13:46:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 259.78
[32m[20221208 13:46:47 @agent_ppo2.py:137][0m Total time:       1.44 min
[32m[20221208 13:46:47 @agent_ppo2.py:139][0m 116736 total steps have happened
[32m[20221208 13:46:47 @agent_ppo2.py:115][0m #------------------------ Iteration 57 --------------------------#
[32m[20221208 13:46:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:47 @agent_ppo2.py:179][0m |           0.0018 |          26.4739 |           0.2753 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0118 |          24.4687 |           0.2753 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0182 |          23.7156 |           0.2765 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0230 |          23.2540 |           0.2778 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0255 |          22.6918 |           0.2769 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0295 |          22.4671 |           0.2775 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0320 |          22.0778 |           0.2781 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0335 |          21.9444 |           0.2782 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0339 |          21.6009 |           0.2778 |
[32m[20221208 13:46:48 @agent_ppo2.py:179][0m |          -0.0380 |          21.3336 |           0.2787 |
[32m[20221208 13:46:48 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 341.29
[32m[20221208 13:46:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 405.86
[32m[20221208 13:46:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 273.10
[32m[20221208 13:46:48 @agent_ppo2.py:137][0m Total time:       1.47 min
[32m[20221208 13:46:48 @agent_ppo2.py:139][0m 118784 total steps have happened
[32m[20221208 13:46:48 @agent_ppo2.py:115][0m #------------------------ Iteration 58 --------------------------#
[32m[20221208 13:46:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |           0.0067 |          42.1692 |           0.2786 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0017 |          40.3092 |           0.2771 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0128 |          39.4586 |           0.2779 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0194 |          39.1861 |           0.2805 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0252 |          38.7499 |           0.2809 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0267 |          38.5494 |           0.2822 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0269 |          38.3823 |           0.2820 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0317 |          38.1487 |           0.2828 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0317 |          37.9072 |           0.2832 |
[32m[20221208 13:46:49 @agent_ppo2.py:179][0m |          -0.0320 |          37.7697 |           0.2844 |
[32m[20221208 13:46:49 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 412.03
[32m[20221208 13:46:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 420.65
[32m[20221208 13:46:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 240.87
[32m[20221208 13:46:50 @agent_ppo2.py:137][0m Total time:       1.49 min
[32m[20221208 13:46:50 @agent_ppo2.py:139][0m 120832 total steps have happened
[32m[20221208 13:46:50 @agent_ppo2.py:115][0m #------------------------ Iteration 59 --------------------------#
[32m[20221208 13:46:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:50 @agent_ppo2.py:179][0m |           0.0041 |          26.5942 |           0.2866 |
[32m[20221208 13:46:50 @agent_ppo2.py:179][0m |          -0.0106 |          23.1488 |           0.2859 |
[32m[20221208 13:46:50 @agent_ppo2.py:179][0m |          -0.0134 |          20.6804 |           0.2858 |
[32m[20221208 13:46:51 @agent_ppo2.py:179][0m |          -0.0173 |          19.7894 |           0.2862 |
[32m[20221208 13:46:51 @agent_ppo2.py:179][0m |          -0.0178 |          19.4213 |           0.2858 |
[32m[20221208 13:46:51 @agent_ppo2.py:179][0m |          -0.0192 |          19.0588 |           0.2855 |
[32m[20221208 13:46:51 @agent_ppo2.py:179][0m |          -0.0241 |          18.9113 |           0.2857 |
[32m[20221208 13:46:51 @agent_ppo2.py:179][0m |          -0.0250 |          18.8103 |           0.2860 |
[32m[20221208 13:46:51 @agent_ppo2.py:179][0m |          -0.0272 |          18.6165 |           0.2869 |
[32m[20221208 13:46:51 @agent_ppo2.py:179][0m |          -0.0306 |          18.5044 |           0.2876 |
[32m[20221208 13:46:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 357.29
[32m[20221208 13:46:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 399.21
[32m[20221208 13:46:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.86
[32m[20221208 13:46:51 @agent_ppo2.py:137][0m Total time:       1.52 min
[32m[20221208 13:46:51 @agent_ppo2.py:139][0m 122880 total steps have happened
[32m[20221208 13:46:51 @agent_ppo2.py:115][0m #------------------------ Iteration 60 --------------------------#
[32m[20221208 13:46:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |           0.0046 |          22.6090 |           0.3053 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0105 |          21.4624 |           0.3068 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0179 |          20.9499 |           0.3067 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0200 |          20.6161 |           0.3077 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0235 |          20.3434 |           0.3087 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0261 |          20.1966 |           0.3087 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0225 |          20.0084 |           0.3098 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0263 |          19.9448 |           0.3091 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0286 |          19.8107 |           0.3104 |
[32m[20221208 13:46:52 @agent_ppo2.py:179][0m |          -0.0315 |          19.7024 |           0.3109 |
[32m[20221208 13:46:52 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:46:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 342.09
[32m[20221208 13:46:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 397.90
[32m[20221208 13:46:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 347.46
[32m[20221208 13:46:53 @agent_ppo2.py:137][0m Total time:       1.54 min
[32m[20221208 13:46:53 @agent_ppo2.py:139][0m 124928 total steps have happened
[32m[20221208 13:46:53 @agent_ppo2.py:115][0m #------------------------ Iteration 61 --------------------------#
[32m[20221208 13:46:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:46:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:53 @agent_ppo2.py:179][0m |           0.0065 |          26.3151 |           0.3042 |
[32m[20221208 13:46:53 @agent_ppo2.py:179][0m |          -0.0076 |          25.4251 |           0.3055 |
[32m[20221208 13:46:53 @agent_ppo2.py:179][0m |          -0.0143 |          24.6915 |           0.3036 |
[32m[20221208 13:46:54 @agent_ppo2.py:179][0m |          -0.0173 |          24.2638 |           0.3067 |
[32m[20221208 13:46:54 @agent_ppo2.py:179][0m |          -0.0215 |          24.1274 |           0.3072 |
[32m[20221208 13:46:54 @agent_ppo2.py:179][0m |          -0.0233 |          23.9721 |           0.3069 |
[32m[20221208 13:46:54 @agent_ppo2.py:179][0m |          -0.0255 |          23.8567 |           0.3069 |
[32m[20221208 13:46:54 @agent_ppo2.py:179][0m |          -0.0256 |          23.7605 |           0.3082 |
[32m[20221208 13:46:54 @agent_ppo2.py:179][0m |          -0.0272 |          23.6963 |           0.3079 |
[32m[20221208 13:46:54 @agent_ppo2.py:179][0m |          -0.0297 |          23.6274 |           0.3087 |
[32m[20221208 13:46:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.77
[32m[20221208 13:46:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 424.30
[32m[20221208 13:46:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 323.58
[32m[20221208 13:46:54 @agent_ppo2.py:137][0m Total time:       1.56 min
[32m[20221208 13:46:54 @agent_ppo2.py:139][0m 126976 total steps have happened
[32m[20221208 13:46:54 @agent_ppo2.py:115][0m #------------------------ Iteration 62 --------------------------#
[32m[20221208 13:46:55 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:46:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |           0.0061 |          29.4042 |           0.3190 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0050 |          27.4864 |           0.3196 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0109 |          26.6672 |           0.3208 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0179 |          26.2653 |           0.3210 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0203 |          26.0062 |           0.3217 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0239 |          25.6467 |           0.3219 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0252 |          25.4615 |           0.3227 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0291 |          25.2558 |           0.3234 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0301 |          24.9646 |           0.3239 |
[32m[20221208 13:46:55 @agent_ppo2.py:179][0m |          -0.0322 |          24.6533 |           0.3236 |
[32m[20221208 13:46:55 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 381.40
[32m[20221208 13:46:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 420.30
[32m[20221208 13:46:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 344.97
[32m[20221208 13:46:56 @agent_ppo2.py:137][0m Total time:       1.59 min
[32m[20221208 13:46:56 @agent_ppo2.py:139][0m 129024 total steps have happened
[32m[20221208 13:46:56 @agent_ppo2.py:115][0m #------------------------ Iteration 63 --------------------------#
[32m[20221208 13:46:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:46:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:56 @agent_ppo2.py:179][0m |           0.0006 |          28.1594 |           0.3194 |
[32m[20221208 13:46:56 @agent_ppo2.py:179][0m |          -0.0130 |          26.4481 |           0.3184 |
[32m[20221208 13:46:56 @agent_ppo2.py:179][0m |          -0.0168 |          25.3443 |           0.3192 |
[32m[20221208 13:46:56 @agent_ppo2.py:179][0m |          -0.0193 |          24.8396 |           0.3190 |
[32m[20221208 13:46:56 @agent_ppo2.py:179][0m |          -0.0223 |          24.0272 |           0.3179 |
[32m[20221208 13:46:57 @agent_ppo2.py:179][0m |          -0.0268 |          23.1872 |           0.3188 |
[32m[20221208 13:46:57 @agent_ppo2.py:179][0m |          -0.0255 |          22.8633 |           0.3183 |
[32m[20221208 13:46:57 @agent_ppo2.py:179][0m |          -0.0300 |          22.3090 |           0.3179 |
[32m[20221208 13:46:57 @agent_ppo2.py:179][0m |          -0.0325 |          21.9404 |           0.3189 |
[32m[20221208 13:46:57 @agent_ppo2.py:179][0m |          -0.0333 |          21.5626 |           0.3193 |
[32m[20221208 13:46:57 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:46:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 356.88
[32m[20221208 13:46:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 431.80
[32m[20221208 13:46:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.58
[32m[20221208 13:46:57 @agent_ppo2.py:137][0m Total time:       1.61 min
[32m[20221208 13:46:57 @agent_ppo2.py:139][0m 131072 total steps have happened
[32m[20221208 13:46:57 @agent_ppo2.py:115][0m #------------------------ Iteration 64 --------------------------#
[32m[20221208 13:46:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |           0.0024 |          28.0687 |           0.3118 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0077 |          26.6114 |           0.3091 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0153 |          25.8564 |           0.3065 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0170 |          25.4682 |           0.3080 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0197 |          25.1603 |           0.3082 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0227 |          24.9716 |           0.3075 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0244 |          24.7524 |           0.3070 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0287 |          24.3442 |           0.3076 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0301 |          24.1648 |           0.3076 |
[32m[20221208 13:46:58 @agent_ppo2.py:179][0m |          -0.0288 |          24.0182 |           0.3076 |
[32m[20221208 13:46:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:46:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 331.79
[32m[20221208 13:46:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 370.91
[32m[20221208 13:46:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 315.32
[32m[20221208 13:46:59 @agent_ppo2.py:137][0m Total time:       1.64 min
[32m[20221208 13:46:59 @agent_ppo2.py:139][0m 133120 total steps have happened
[32m[20221208 13:46:59 @agent_ppo2.py:115][0m #------------------------ Iteration 65 --------------------------#
[32m[20221208 13:46:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:46:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:46:59 @agent_ppo2.py:179][0m |           0.0043 |          16.3855 |           0.3035 |
[32m[20221208 13:46:59 @agent_ppo2.py:179][0m |          -0.0124 |          15.9804 |           0.3038 |
[32m[20221208 13:46:59 @agent_ppo2.py:179][0m |          -0.0155 |          15.7339 |           0.3044 |
[32m[20221208 13:46:59 @agent_ppo2.py:179][0m |          -0.0200 |          15.7179 |           0.3059 |
[32m[20221208 13:46:59 @agent_ppo2.py:179][0m |          -0.0189 |          15.5441 |           0.3037 |
[32m[20221208 13:46:59 @agent_ppo2.py:179][0m |          -0.0240 |          15.3492 |           0.3045 |
[32m[20221208 13:46:59 @agent_ppo2.py:179][0m |          -0.0275 |          15.2570 |           0.3047 |
[32m[20221208 13:47:00 @agent_ppo2.py:179][0m |          -0.0291 |          15.2424 |           0.3047 |
[32m[20221208 13:47:00 @agent_ppo2.py:179][0m |          -0.0293 |          15.1157 |           0.3043 |
[32m[20221208 13:47:00 @agent_ppo2.py:179][0m |          -0.0278 |          15.0308 |           0.3038 |
[32m[20221208 13:47:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 276.98
[32m[20221208 13:47:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 315.28
[32m[20221208 13:47:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 271.32
[32m[20221208 13:47:00 @agent_ppo2.py:137][0m Total time:       1.66 min
[32m[20221208 13:47:00 @agent_ppo2.py:139][0m 135168 total steps have happened
[32m[20221208 13:47:00 @agent_ppo2.py:115][0m #------------------------ Iteration 66 --------------------------#
[32m[20221208 13:47:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |           0.0025 |          45.4634 |           0.3150 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0089 |          43.5429 |           0.3149 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0163 |          42.7439 |           0.3155 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0179 |          41.9910 |           0.3141 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0229 |          41.5103 |           0.3145 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0239 |          41.1352 |           0.3134 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0252 |          40.6312 |           0.3140 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0265 |          40.3424 |           0.3147 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0290 |          40.3867 |           0.3141 |
[32m[20221208 13:47:01 @agent_ppo2.py:179][0m |          -0.0289 |          40.1547 |           0.3153 |
[32m[20221208 13:47:01 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:47:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 377.01
[32m[20221208 13:47:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 388.16
[32m[20221208 13:47:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 332.47
[32m[20221208 13:47:02 @agent_ppo2.py:137][0m Total time:       1.69 min
[32m[20221208 13:47:02 @agent_ppo2.py:139][0m 137216 total steps have happened
[32m[20221208 13:47:02 @agent_ppo2.py:115][0m #------------------------ Iteration 67 --------------------------#
[32m[20221208 13:47:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |           0.0030 |          35.4775 |           0.2970 |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |          -0.0128 |          33.3353 |           0.2947 |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |          -0.0176 |          32.0771 |           0.2942 |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |          -0.0216 |          31.4279 |           0.2937 |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |          -0.0246 |          31.0286 |           0.2939 |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |          -0.0261 |          30.0412 |           0.2934 |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |          -0.0280 |          29.2171 |           0.2935 |
[32m[20221208 13:47:02 @agent_ppo2.py:179][0m |          -0.0302 |          27.9474 |           0.2942 |
[32m[20221208 13:47:03 @agent_ppo2.py:179][0m |          -0.0305 |          27.2673 |           0.2938 |
[32m[20221208 13:47:03 @agent_ppo2.py:179][0m |          -0.0327 |          26.6771 |           0.2944 |
[32m[20221208 13:47:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 357.12
[32m[20221208 13:47:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 421.15
[32m[20221208 13:47:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 280.42
[32m[20221208 13:47:03 @agent_ppo2.py:137][0m Total time:       1.71 min
[32m[20221208 13:47:03 @agent_ppo2.py:139][0m 139264 total steps have happened
[32m[20221208 13:47:03 @agent_ppo2.py:115][0m #------------------------ Iteration 68 --------------------------#
[32m[20221208 13:47:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |           0.0057 |          38.1041 |           0.2924 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0083 |          35.7494 |           0.2922 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0183 |          35.0330 |           0.2925 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0214 |          34.4457 |           0.2922 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0233 |          34.4231 |           0.2916 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0277 |          34.2036 |           0.2916 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0280 |          33.9395 |           0.2909 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0303 |          33.7809 |           0.2908 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0323 |          33.7119 |           0.2909 |
[32m[20221208 13:47:04 @agent_ppo2.py:179][0m |          -0.0312 |          33.8463 |           0.2902 |
[32m[20221208 13:47:04 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:47:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 388.64
[32m[20221208 13:47:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 406.19
[32m[20221208 13:47:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 315.92
[32m[20221208 13:47:04 @agent_ppo2.py:137][0m Total time:       1.73 min
[32m[20221208 13:47:04 @agent_ppo2.py:139][0m 141312 total steps have happened
[32m[20221208 13:47:04 @agent_ppo2.py:115][0m #------------------------ Iteration 69 --------------------------#
[32m[20221208 13:47:05 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |           0.0045 |          45.8456 |           0.2861 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0075 |          44.7275 |           0.2844 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0103 |          44.4444 |           0.2835 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0170 |          43.8858 |           0.2837 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0229 |          43.8678 |           0.2838 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0222 |          43.5461 |           0.2837 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0246 |          43.4608 |           0.2837 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0276 |          43.1779 |           0.2852 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0279 |          42.8637 |           0.2847 |
[32m[20221208 13:47:05 @agent_ppo2.py:179][0m |          -0.0291 |          43.0603 |           0.2850 |
[32m[20221208 13:47:05 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 404.26
[32m[20221208 13:47:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 420.49
[32m[20221208 13:47:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 311.59
[32m[20221208 13:47:06 @agent_ppo2.py:137][0m Total time:       1.76 min
[32m[20221208 13:47:06 @agent_ppo2.py:139][0m 143360 total steps have happened
[32m[20221208 13:47:06 @agent_ppo2.py:115][0m #------------------------ Iteration 70 --------------------------#
[32m[20221208 13:47:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:06 @agent_ppo2.py:179][0m |           0.0083 |          28.2730 |           0.2827 |
[32m[20221208 13:47:06 @agent_ppo2.py:179][0m |          -0.0115 |          27.0838 |           0.2814 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0212 |          26.4734 |           0.2827 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0258 |          26.1557 |           0.2822 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0275 |          25.7670 |           0.2815 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0310 |          25.4288 |           0.2819 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0323 |          25.1346 |           0.2823 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0345 |          24.9605 |           0.2825 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0343 |          24.6079 |           0.2820 |
[32m[20221208 13:47:07 @agent_ppo2.py:179][0m |          -0.0368 |          24.3252 |           0.2827 |
[32m[20221208 13:47:07 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:47:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 326.84
[32m[20221208 13:47:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 440.96
[32m[20221208 13:47:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 252.13
[32m[20221208 13:47:07 @agent_ppo2.py:137][0m Total time:       1.78 min
[32m[20221208 13:47:07 @agent_ppo2.py:139][0m 145408 total steps have happened
[32m[20221208 13:47:07 @agent_ppo2.py:115][0m #------------------------ Iteration 71 --------------------------#
[32m[20221208 13:47:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |           0.0093 |          29.8418 |           0.2940 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0099 |          28.7034 |           0.2912 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0166 |          28.1538 |           0.2928 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0199 |          27.6488 |           0.2920 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0249 |          27.2542 |           0.2930 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0283 |          26.9093 |           0.2935 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0292 |          26.5308 |           0.2939 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0323 |          26.2277 |           0.2949 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0335 |          25.9987 |           0.2947 |
[32m[20221208 13:47:08 @agent_ppo2.py:179][0m |          -0.0356 |          25.6166 |           0.2954 |
[32m[20221208 13:47:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:47:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 322.73
[32m[20221208 13:47:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 427.74
[32m[20221208 13:47:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 330.64
[32m[20221208 13:47:09 @agent_ppo2.py:137][0m Total time:       1.81 min
[32m[20221208 13:47:09 @agent_ppo2.py:139][0m 147456 total steps have happened
[32m[20221208 13:47:09 @agent_ppo2.py:115][0m #------------------------ Iteration 72 --------------------------#
[32m[20221208 13:47:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:09 @agent_ppo2.py:179][0m |           0.0101 |          32.6404 |           0.2913 |
[32m[20221208 13:47:09 @agent_ppo2.py:179][0m |          -0.0113 |          30.1002 |           0.2908 |
[32m[20221208 13:47:09 @agent_ppo2.py:179][0m |          -0.0161 |          29.0693 |           0.2897 |
[32m[20221208 13:47:10 @agent_ppo2.py:179][0m |          -0.0219 |          28.1172 |           0.2903 |
[32m[20221208 13:47:10 @agent_ppo2.py:179][0m |          -0.0266 |          27.6196 |           0.2907 |
[32m[20221208 13:47:10 @agent_ppo2.py:179][0m |          -0.0288 |          27.3767 |           0.2911 |
[32m[20221208 13:47:10 @agent_ppo2.py:179][0m |          -0.0323 |          27.0334 |           0.2920 |
[32m[20221208 13:47:10 @agent_ppo2.py:179][0m |          -0.0342 |          26.6691 |           0.2916 |
[32m[20221208 13:47:10 @agent_ppo2.py:179][0m |          -0.0376 |          26.3714 |           0.2919 |
[32m[20221208 13:47:10 @agent_ppo2.py:179][0m |          -0.0374 |          26.2812 |           0.2926 |
[32m[20221208 13:47:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 307.07
[32m[20221208 13:47:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 340.86
[32m[20221208 13:47:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 314.15
[32m[20221208 13:47:10 @agent_ppo2.py:137][0m Total time:       1.83 min
[32m[20221208 13:47:10 @agent_ppo2.py:139][0m 149504 total steps have happened
[32m[20221208 13:47:10 @agent_ppo2.py:115][0m #------------------------ Iteration 73 --------------------------#
[32m[20221208 13:47:11 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |           0.0078 |          31.9549 |           0.2897 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0112 |          29.0839 |           0.2896 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0156 |          27.9930 |           0.2879 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0239 |          27.5426 |           0.2881 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0297 |          26.8922 |           0.2872 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0329 |          26.4595 |           0.2867 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0345 |          26.1683 |           0.2870 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0381 |          25.6306 |           0.2877 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0410 |          25.5113 |           0.2874 |
[32m[20221208 13:47:11 @agent_ppo2.py:179][0m |          -0.0416 |          25.2680 |           0.2878 |
[32m[20221208 13:47:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 392.24
[32m[20221208 13:47:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 463.23
[32m[20221208 13:47:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 305.90
[32m[20221208 13:47:12 @agent_ppo2.py:137][0m Total time:       1.86 min
[32m[20221208 13:47:12 @agent_ppo2.py:139][0m 151552 total steps have happened
[32m[20221208 13:47:12 @agent_ppo2.py:115][0m #------------------------ Iteration 74 --------------------------#
[32m[20221208 13:47:12 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:12 @agent_ppo2.py:179][0m |           0.0003 |          39.3079 |           0.2959 |
[32m[20221208 13:47:12 @agent_ppo2.py:179][0m |          -0.0116 |          37.4645 |           0.2935 |
[32m[20221208 13:47:12 @agent_ppo2.py:179][0m |          -0.0199 |          36.5599 |           0.2914 |
[32m[20221208 13:47:12 @agent_ppo2.py:179][0m |          -0.0245 |          35.9470 |           0.2921 |
[32m[20221208 13:47:12 @agent_ppo2.py:179][0m |          -0.0277 |          35.3604 |           0.2927 |
[32m[20221208 13:47:13 @agent_ppo2.py:179][0m |          -0.0306 |          35.0284 |           0.2923 |
[32m[20221208 13:47:13 @agent_ppo2.py:179][0m |          -0.0339 |          34.5856 |           0.2937 |
[32m[20221208 13:47:13 @agent_ppo2.py:179][0m |          -0.0353 |          34.3073 |           0.2938 |
[32m[20221208 13:47:13 @agent_ppo2.py:179][0m |          -0.0362 |          34.0373 |           0.2936 |
[32m[20221208 13:47:13 @agent_ppo2.py:179][0m |          -0.0376 |          33.9464 |           0.2941 |
[32m[20221208 13:47:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 374.72
[32m[20221208 13:47:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 440.46
[32m[20221208 13:47:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 301.97
[32m[20221208 13:47:13 @agent_ppo2.py:137][0m Total time:       1.88 min
[32m[20221208 13:47:13 @agent_ppo2.py:139][0m 153600 total steps have happened
[32m[20221208 13:47:13 @agent_ppo2.py:115][0m #------------------------ Iteration 75 --------------------------#
[32m[20221208 13:47:14 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |           0.0056 |          50.6268 |           0.2779 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0103 |          48.5608 |           0.2790 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0158 |          47.0162 |           0.2797 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0218 |          45.9257 |           0.2795 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0277 |          45.4772 |           0.2810 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0281 |          44.6040 |           0.2799 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0315 |          44.2689 |           0.2805 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0342 |          43.6834 |           0.2809 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0342 |          43.3359 |           0.2818 |
[32m[20221208 13:47:14 @agent_ppo2.py:179][0m |          -0.0350 |          42.7424 |           0.2818 |
[32m[20221208 13:47:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:47:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 448.68
[32m[20221208 13:47:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 470.45
[32m[20221208 13:47:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 429.23
[32m[20221208 13:47:15 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 429.23
[32m[20221208 13:47:15 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 429.23
[32m[20221208 13:47:15 @agent_ppo2.py:137][0m Total time:       1.90 min
[32m[20221208 13:47:15 @agent_ppo2.py:139][0m 155648 total steps have happened
[32m[20221208 13:47:15 @agent_ppo2.py:115][0m #------------------------ Iteration 76 --------------------------#
[32m[20221208 13:47:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:15 @agent_ppo2.py:179][0m |           0.0080 |          31.9440 |           0.2861 |
[32m[20221208 13:47:15 @agent_ppo2.py:179][0m |          -0.0139 |          29.4546 |           0.2854 |
[32m[20221208 13:47:15 @agent_ppo2.py:179][0m |          -0.0198 |          28.4981 |           0.2849 |
[32m[20221208 13:47:15 @agent_ppo2.py:179][0m |          -0.0275 |          27.9635 |           0.2856 |
[32m[20221208 13:47:15 @agent_ppo2.py:179][0m |          -0.0305 |          27.6598 |           0.2870 |
[32m[20221208 13:47:15 @agent_ppo2.py:179][0m |          -0.0334 |          27.1502 |           0.2871 |
[32m[20221208 13:47:16 @agent_ppo2.py:179][0m |          -0.0358 |          26.8195 |           0.2876 |
[32m[20221208 13:47:16 @agent_ppo2.py:179][0m |          -0.0361 |          26.5861 |           0.2878 |
[32m[20221208 13:47:16 @agent_ppo2.py:179][0m |          -0.0390 |          26.5163 |           0.2881 |
[32m[20221208 13:47:16 @agent_ppo2.py:179][0m |          -0.0380 |          26.3151 |           0.2886 |
[32m[20221208 13:47:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 370.87
[32m[20221208 13:47:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 445.17
[32m[20221208 13:47:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 367.78
[32m[20221208 13:47:16 @agent_ppo2.py:137][0m Total time:       1.93 min
[32m[20221208 13:47:16 @agent_ppo2.py:139][0m 157696 total steps have happened
[32m[20221208 13:47:16 @agent_ppo2.py:115][0m #------------------------ Iteration 77 --------------------------#
[32m[20221208 13:47:17 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |           0.0053 |          49.9921 |           0.3091 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0157 |          47.3443 |           0.3094 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0186 |          44.7152 |           0.3098 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0225 |          42.2825 |           0.3110 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0226 |          42.0211 |           0.3113 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0290 |          40.9744 |           0.3123 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0309 |          39.6279 |           0.3130 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0319 |          38.9535 |           0.3129 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0343 |          38.2644 |           0.3147 |
[32m[20221208 13:47:17 @agent_ppo2.py:179][0m |          -0.0369 |          37.9136 |           0.3158 |
[32m[20221208 13:47:17 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 386.22
[32m[20221208 13:47:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 399.87
[32m[20221208 13:47:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 346.83
[32m[20221208 13:47:18 @agent_ppo2.py:137][0m Total time:       1.95 min
[32m[20221208 13:47:18 @agent_ppo2.py:139][0m 159744 total steps have happened
[32m[20221208 13:47:18 @agent_ppo2.py:115][0m #------------------------ Iteration 78 --------------------------#
[32m[20221208 13:47:18 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |           0.0036 |          32.0966 |           0.2999 |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |          -0.0108 |          27.0709 |           0.2999 |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |          -0.0211 |          25.5395 |           0.3001 |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |          -0.0227 |          24.9074 |           0.3004 |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |          -0.0280 |          24.3131 |           0.3008 |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |          -0.0299 |          23.8734 |           0.3022 |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |          -0.0337 |          23.6439 |           0.3021 |
[32m[20221208 13:47:18 @agent_ppo2.py:179][0m |          -0.0352 |          23.3703 |           0.3029 |
[32m[20221208 13:47:19 @agent_ppo2.py:179][0m |          -0.0367 |          23.0198 |           0.3035 |
[32m[20221208 13:47:19 @agent_ppo2.py:179][0m |          -0.0379 |          22.7816 |           0.3036 |
[32m[20221208 13:47:19 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 343.94
[32m[20221208 13:47:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 458.41
[32m[20221208 13:47:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 255.88
[32m[20221208 13:47:19 @agent_ppo2.py:137][0m Total time:       1.98 min
[32m[20221208 13:47:19 @agent_ppo2.py:139][0m 161792 total steps have happened
[32m[20221208 13:47:19 @agent_ppo2.py:115][0m #------------------------ Iteration 79 --------------------------#
[32m[20221208 13:47:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |           0.0102 |          40.1032 |           0.3125 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0090 |          37.2221 |           0.3112 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0167 |          36.7124 |           0.3113 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0236 |          35.8592 |           0.3134 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0251 |          35.4621 |           0.3134 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0271 |          34.8911 |           0.3134 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0266 |          34.3879 |           0.3138 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0306 |          34.0993 |           0.3153 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0333 |          33.9620 |           0.3162 |
[32m[20221208 13:47:20 @agent_ppo2.py:179][0m |          -0.0332 |          33.2130 |           0.3156 |
[32m[20221208 13:47:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 372.28
[32m[20221208 13:47:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 426.43
[32m[20221208 13:47:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 165.06
[32m[20221208 13:47:20 @agent_ppo2.py:137][0m Total time:       2.00 min
[32m[20221208 13:47:20 @agent_ppo2.py:139][0m 163840 total steps have happened
[32m[20221208 13:47:20 @agent_ppo2.py:115][0m #------------------------ Iteration 80 --------------------------#
[32m[20221208 13:47:21 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |           0.0036 |          40.5379 |           0.3017 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0164 |          39.3447 |           0.3024 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0184 |          38.7967 |           0.2998 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0241 |          38.3005 |           0.3013 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0269 |          37.8201 |           0.3016 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0287 |          37.4176 |           0.3007 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0305 |          37.2766 |           0.3013 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0308 |          36.9596 |           0.3014 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0341 |          36.4577 |           0.3019 |
[32m[20221208 13:47:21 @agent_ppo2.py:179][0m |          -0.0362 |          36.2650 |           0.3024 |
[32m[20221208 13:47:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 373.72
[32m[20221208 13:47:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 449.36
[32m[20221208 13:47:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 395.93
[32m[20221208 13:47:22 @agent_ppo2.py:137][0m Total time:       2.03 min
[32m[20221208 13:47:22 @agent_ppo2.py:139][0m 165888 total steps have happened
[32m[20221208 13:47:22 @agent_ppo2.py:115][0m #------------------------ Iteration 81 --------------------------#
[32m[20221208 13:47:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:47:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:22 @agent_ppo2.py:179][0m |           0.0085 |          29.0978 |           0.3049 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0066 |          26.9575 |           0.3036 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0148 |          26.4202 |           0.3025 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0170 |          26.1152 |           0.3026 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0176 |          25.8014 |           0.3025 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0219 |          25.4945 |           0.3008 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0229 |          25.5104 |           0.3003 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0220 |          25.1626 |           0.3006 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0265 |          24.8778 |           0.3004 |
[32m[20221208 13:47:23 @agent_ppo2.py:179][0m |          -0.0280 |          24.7707 |           0.3010 |
[32m[20221208 13:47:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.98
[32m[20221208 13:47:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 434.96
[32m[20221208 13:47:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 184.30
[32m[20221208 13:47:23 @agent_ppo2.py:137][0m Total time:       2.05 min
[32m[20221208 13:47:23 @agent_ppo2.py:139][0m 167936 total steps have happened
[32m[20221208 13:47:23 @agent_ppo2.py:115][0m #------------------------ Iteration 82 --------------------------#
[32m[20221208 13:47:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |           0.0089 |          31.9670 |           0.2930 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0093 |          29.9968 |           0.2927 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0182 |          29.3359 |           0.2935 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0227 |          28.8020 |           0.2937 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0273 |          28.1508 |           0.2931 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0304 |          27.8491 |           0.2942 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0325 |          27.5691 |           0.2946 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0343 |          27.3084 |           0.2938 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0380 |          27.1992 |           0.2947 |
[32m[20221208 13:47:24 @agent_ppo2.py:179][0m |          -0.0407 |          26.8296 |           0.2947 |
[32m[20221208 13:47:24 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 349.71
[32m[20221208 13:47:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 468.65
[32m[20221208 13:47:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 424.07
[32m[20221208 13:47:25 @agent_ppo2.py:137][0m Total time:       2.07 min
[32m[20221208 13:47:25 @agent_ppo2.py:139][0m 169984 total steps have happened
[32m[20221208 13:47:25 @agent_ppo2.py:115][0m #------------------------ Iteration 83 --------------------------#
[32m[20221208 13:47:25 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:25 @agent_ppo2.py:179][0m |           0.0085 |          32.6731 |           0.3076 |
[32m[20221208 13:47:25 @agent_ppo2.py:179][0m |          -0.0108 |          30.8818 |           0.3056 |
[32m[20221208 13:47:25 @agent_ppo2.py:179][0m |          -0.0226 |          29.9323 |           0.3083 |
[32m[20221208 13:47:26 @agent_ppo2.py:179][0m |          -0.0284 |          29.2818 |           0.3087 |
[32m[20221208 13:47:26 @agent_ppo2.py:179][0m |          -0.0318 |          28.9014 |           0.3092 |
[32m[20221208 13:47:26 @agent_ppo2.py:179][0m |          -0.0355 |          28.3550 |           0.3104 |
[32m[20221208 13:47:26 @agent_ppo2.py:179][0m |          -0.0359 |          27.9964 |           0.3108 |
[32m[20221208 13:47:26 @agent_ppo2.py:179][0m |          -0.0387 |          27.7622 |           0.3113 |
[32m[20221208 13:47:26 @agent_ppo2.py:179][0m |          -0.0399 |          27.5921 |           0.3121 |
[32m[20221208 13:47:26 @agent_ppo2.py:179][0m |          -0.0427 |          27.0245 |           0.3123 |
[32m[20221208 13:47:26 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:47:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 387.61
[32m[20221208 13:47:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 458.29
[32m[20221208 13:47:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 272.18
[32m[20221208 13:47:26 @agent_ppo2.py:137][0m Total time:       2.10 min
[32m[20221208 13:47:26 @agent_ppo2.py:139][0m 172032 total steps have happened
[32m[20221208 13:47:26 @agent_ppo2.py:115][0m #------------------------ Iteration 84 --------------------------#
[32m[20221208 13:47:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |           0.0110 |          36.7678 |           0.2996 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0055 |          33.8218 |           0.3022 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0072 |          33.0882 |           0.3014 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0135 |          32.7326 |           0.3029 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0197 |          32.2295 |           0.3016 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0185 |          31.9426 |           0.3043 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0268 |          31.6360 |           0.3060 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0275 |          31.4105 |           0.3040 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0313 |          31.2610 |           0.3056 |
[32m[20221208 13:47:27 @agent_ppo2.py:179][0m |          -0.0268 |          31.1010 |           0.3066 |
[32m[20221208 13:47:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 223.65
[32m[20221208 13:47:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 446.39
[32m[20221208 13:47:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 390.31
[32m[20221208 13:47:28 @agent_ppo2.py:137][0m Total time:       2.12 min
[32m[20221208 13:47:28 @agent_ppo2.py:139][0m 174080 total steps have happened
[32m[20221208 13:47:28 @agent_ppo2.py:115][0m #------------------------ Iteration 85 --------------------------#
[32m[20221208 13:47:28 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:28 @agent_ppo2.py:179][0m |           0.0080 |          44.7854 |           0.3173 |
[32m[20221208 13:47:28 @agent_ppo2.py:179][0m |          -0.0109 |          41.9408 |           0.3159 |
[32m[20221208 13:47:28 @agent_ppo2.py:179][0m |          -0.0210 |          40.5123 |           0.3139 |
[32m[20221208 13:47:28 @agent_ppo2.py:179][0m |          -0.0250 |          39.3263 |           0.3150 |
[32m[20221208 13:47:28 @agent_ppo2.py:179][0m |          -0.0296 |          38.3088 |           0.3144 |
[32m[20221208 13:47:29 @agent_ppo2.py:179][0m |          -0.0330 |          37.6499 |           0.3136 |
[32m[20221208 13:47:29 @agent_ppo2.py:179][0m |          -0.0370 |          36.9468 |           0.3148 |
[32m[20221208 13:47:29 @agent_ppo2.py:179][0m |          -0.0403 |          36.2952 |           0.3153 |
[32m[20221208 13:47:29 @agent_ppo2.py:179][0m |          -0.0416 |          36.0006 |           0.3142 |
[32m[20221208 13:47:29 @agent_ppo2.py:179][0m |          -0.0443 |          34.9538 |           0.3146 |
[32m[20221208 13:47:29 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 336.12
[32m[20221208 13:47:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 463.00
[32m[20221208 13:47:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 221.23
[32m[20221208 13:47:29 @agent_ppo2.py:137][0m Total time:       2.15 min
[32m[20221208 13:47:29 @agent_ppo2.py:139][0m 176128 total steps have happened
[32m[20221208 13:47:29 @agent_ppo2.py:115][0m #------------------------ Iteration 86 --------------------------#
[32m[20221208 13:47:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |           0.0038 |          33.8458 |           0.3055 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0192 |          30.6653 |           0.3027 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0259 |          29.2240 |           0.3041 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0318 |          28.5144 |           0.3044 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0351 |          27.7520 |           0.3042 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0380 |          27.4222 |           0.3043 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0405 |          26.8639 |           0.3044 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0436 |          26.3808 |           0.3045 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0455 |          25.9622 |           0.3039 |
[32m[20221208 13:47:30 @agent_ppo2.py:179][0m |          -0.0486 |          26.3988 |           0.3050 |
[32m[20221208 13:47:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:47:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.45
[32m[20221208 13:47:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 372.61
[32m[20221208 13:47:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 434.45
[32m[20221208 13:47:31 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 434.45
[32m[20221208 13:47:31 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 434.45
[32m[20221208 13:47:31 @agent_ppo2.py:137][0m Total time:       2.17 min
[32m[20221208 13:47:31 @agent_ppo2.py:139][0m 178176 total steps have happened
[32m[20221208 13:47:31 @agent_ppo2.py:115][0m #------------------------ Iteration 87 --------------------------#
[32m[20221208 13:47:31 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:31 @agent_ppo2.py:179][0m |           0.0059 |          43.7148 |           0.2987 |
[32m[20221208 13:47:31 @agent_ppo2.py:179][0m |          -0.0124 |          40.4827 |           0.2974 |
[32m[20221208 13:47:31 @agent_ppo2.py:179][0m |          -0.0195 |          39.3484 |           0.2974 |
[32m[20221208 13:47:31 @agent_ppo2.py:179][0m |          -0.0245 |          38.6650 |           0.2959 |
[32m[20221208 13:47:31 @agent_ppo2.py:179][0m |          -0.0293 |          38.3724 |           0.2960 |
[32m[20221208 13:47:31 @agent_ppo2.py:179][0m |          -0.0324 |          37.7072 |           0.2962 |
[32m[20221208 13:47:32 @agent_ppo2.py:179][0m |          -0.0356 |          37.2625 |           0.2948 |
[32m[20221208 13:47:32 @agent_ppo2.py:179][0m |          -0.0385 |          37.0332 |           0.2954 |
[32m[20221208 13:47:32 @agent_ppo2.py:179][0m |          -0.0408 |          36.6758 |           0.2954 |
[32m[20221208 13:47:32 @agent_ppo2.py:179][0m |          -0.0424 |          36.3612 |           0.2962 |
[32m[20221208 13:47:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 371.84
[32m[20221208 13:47:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 455.69
[32m[20221208 13:47:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 243.05
[32m[20221208 13:47:32 @agent_ppo2.py:137][0m Total time:       2.20 min
[32m[20221208 13:47:32 @agent_ppo2.py:139][0m 180224 total steps have happened
[32m[20221208 13:47:32 @agent_ppo2.py:115][0m #------------------------ Iteration 88 --------------------------#
[32m[20221208 13:47:32 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |           0.0057 |          42.6004 |           0.3033 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0070 |          40.7090 |           0.3022 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0166 |          39.9234 |           0.3054 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0164 |          39.4918 |           0.3032 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0200 |          39.1130 |           0.3045 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0259 |          38.8659 |           0.3061 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0281 |          38.3423 |           0.3066 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0261 |          37.9735 |           0.3078 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0275 |          37.6489 |           0.3078 |
[32m[20221208 13:47:33 @agent_ppo2.py:179][0m |          -0.0335 |          37.0733 |           0.3080 |
[32m[20221208 13:47:33 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:47:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 317.44
[32m[20221208 13:47:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 464.75
[32m[20221208 13:47:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.41
[32m[20221208 13:47:34 @agent_ppo2.py:137][0m Total time:       2.22 min
[32m[20221208 13:47:34 @agent_ppo2.py:139][0m 182272 total steps have happened
[32m[20221208 13:47:34 @agent_ppo2.py:115][0m #------------------------ Iteration 89 --------------------------#
[32m[20221208 13:47:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |           0.0062 |          41.3960 |           0.2894 |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |          -0.0128 |          39.8351 |           0.2883 |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |          -0.0197 |          38.9596 |           0.2885 |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |          -0.0220 |          38.6231 |           0.2869 |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |          -0.0277 |          38.0224 |           0.2873 |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |          -0.0314 |          37.6504 |           0.2877 |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |          -0.0350 |          37.3200 |           0.2867 |
[32m[20221208 13:47:34 @agent_ppo2.py:179][0m |          -0.0357 |          36.8882 |           0.2878 |
[32m[20221208 13:47:35 @agent_ppo2.py:179][0m |          -0.0380 |          36.6756 |           0.2878 |
[32m[20221208 13:47:35 @agent_ppo2.py:179][0m |          -0.0419 |          36.2367 |           0.2872 |
[32m[20221208 13:47:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 353.72
[32m[20221208 13:47:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 466.08
[32m[20221208 13:47:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 371.49
[32m[20221208 13:47:35 @agent_ppo2.py:137][0m Total time:       2.24 min
[32m[20221208 13:47:35 @agent_ppo2.py:139][0m 184320 total steps have happened
[32m[20221208 13:47:35 @agent_ppo2.py:115][0m #------------------------ Iteration 90 --------------------------#
[32m[20221208 13:47:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |           0.0077 |          25.1454 |           0.3001 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0085 |          22.7705 |           0.2976 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0187 |          21.9372 |           0.2995 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0250 |          21.3557 |           0.3003 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0286 |          20.9980 |           0.2997 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0315 |          20.5092 |           0.3011 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0337 |          20.2866 |           0.3008 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0377 |          19.9563 |           0.3007 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0404 |          19.7635 |           0.3031 |
[32m[20221208 13:47:36 @agent_ppo2.py:179][0m |          -0.0447 |          19.4631 |           0.3038 |
[32m[20221208 13:47:36 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:47:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 261.86
[32m[20221208 13:47:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 388.44
[32m[20221208 13:47:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 154.75
[32m[20221208 13:47:36 @agent_ppo2.py:137][0m Total time:       2.27 min
[32m[20221208 13:47:36 @agent_ppo2.py:139][0m 186368 total steps have happened
[32m[20221208 13:47:36 @agent_ppo2.py:115][0m #------------------------ Iteration 91 --------------------------#
[32m[20221208 13:47:37 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |           0.0037 |          52.3737 |           0.3053 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0108 |          49.8631 |           0.3042 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0225 |          48.9630 |           0.3055 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0286 |          48.6743 |           0.3062 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0311 |          47.9597 |           0.3064 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0338 |          47.4199 |           0.3050 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0374 |          46.9874 |           0.3065 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0391 |          46.6916 |           0.3065 |
[32m[20221208 13:47:37 @agent_ppo2.py:179][0m |          -0.0398 |          46.2673 |           0.3065 |
[32m[20221208 13:47:38 @agent_ppo2.py:179][0m |          -0.0420 |          45.8329 |           0.3060 |
[32m[20221208 13:47:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 363.09
[32m[20221208 13:47:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 451.39
[32m[20221208 13:47:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 369.28
[32m[20221208 13:47:38 @agent_ppo2.py:137][0m Total time:       2.29 min
[32m[20221208 13:47:38 @agent_ppo2.py:139][0m 188416 total steps have happened
[32m[20221208 13:47:38 @agent_ppo2.py:115][0m #------------------------ Iteration 92 --------------------------#
[32m[20221208 13:47:38 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:38 @agent_ppo2.py:179][0m |           0.0055 |          52.3135 |           0.3070 |
[32m[20221208 13:47:38 @agent_ppo2.py:179][0m |          -0.0105 |          50.3583 |           0.3062 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0201 |          49.5118 |           0.3083 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0247 |          49.3375 |           0.3079 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0271 |          48.8319 |           0.3080 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0317 |          48.3634 |           0.3085 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0346 |          48.1081 |           0.3087 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0350 |          47.8655 |           0.3088 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0377 |          47.8284 |           0.3091 |
[32m[20221208 13:47:39 @agent_ppo2.py:179][0m |          -0.0398 |          47.6252 |           0.3096 |
[32m[20221208 13:47:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 427.56
[32m[20221208 13:47:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 450.16
[32m[20221208 13:47:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 376.38
[32m[20221208 13:47:39 @agent_ppo2.py:137][0m Total time:       2.32 min
[32m[20221208 13:47:39 @agent_ppo2.py:139][0m 190464 total steps have happened
[32m[20221208 13:47:39 @agent_ppo2.py:115][0m #------------------------ Iteration 93 --------------------------#
[32m[20221208 13:47:40 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |           0.0051 |          50.3381 |           0.3139 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0102 |          48.3280 |           0.3123 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0188 |          47.2967 |           0.3140 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0208 |          46.4750 |           0.3144 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0248 |          45.8225 |           0.3143 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0271 |          45.2134 |           0.3153 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0282 |          44.8969 |           0.3150 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0288 |          44.5716 |           0.3155 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0319 |          44.0254 |           0.3167 |
[32m[20221208 13:47:40 @agent_ppo2.py:179][0m |          -0.0321 |          43.5145 |           0.3165 |
[32m[20221208 13:47:40 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:47:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 459.20
[32m[20221208 13:47:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 468.16
[32m[20221208 13:47:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 393.91
[32m[20221208 13:47:41 @agent_ppo2.py:137][0m Total time:       2.34 min
[32m[20221208 13:47:41 @agent_ppo2.py:139][0m 192512 total steps have happened
[32m[20221208 13:47:41 @agent_ppo2.py:115][0m #------------------------ Iteration 94 --------------------------#
[32m[20221208 13:47:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:47:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:41 @agent_ppo2.py:179][0m |           0.0104 |          40.2709 |           0.3041 |
[32m[20221208 13:47:41 @agent_ppo2.py:179][0m |          -0.0049 |          37.7211 |           0.3062 |
[32m[20221208 13:47:41 @agent_ppo2.py:179][0m |          -0.0065 |          37.0858 |           0.3044 |
[32m[20221208 13:47:42 @agent_ppo2.py:179][0m |          -0.0124 |          37.0978 |           0.3066 |
[32m[20221208 13:47:42 @agent_ppo2.py:179][0m |          -0.0189 |          36.3507 |           0.3067 |
[32m[20221208 13:47:42 @agent_ppo2.py:179][0m |          -0.0201 |          36.2625 |           0.3071 |
[32m[20221208 13:47:42 @agent_ppo2.py:179][0m |          -0.0183 |          36.2305 |           0.3055 |
[32m[20221208 13:47:42 @agent_ppo2.py:179][0m |          -0.0252 |          35.9257 |           0.3082 |
[32m[20221208 13:47:42 @agent_ppo2.py:179][0m |          -0.0246 |          35.8695 |           0.3083 |
[32m[20221208 13:47:42 @agent_ppo2.py:179][0m |          -0.0240 |          35.6420 |           0.3074 |
[32m[20221208 13:47:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 257.74
[32m[20221208 13:47:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 426.80
[32m[20221208 13:47:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 429.36
[32m[20221208 13:47:42 @agent_ppo2.py:137][0m Total time:       2.36 min
[32m[20221208 13:47:42 @agent_ppo2.py:139][0m 194560 total steps have happened
[32m[20221208 13:47:42 @agent_ppo2.py:115][0m #------------------------ Iteration 95 --------------------------#
[32m[20221208 13:47:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |           0.0083 |          52.7291 |           0.2971 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0120 |          51.3448 |           0.2948 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0173 |          50.7022 |           0.2947 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0238 |          50.2639 |           0.2955 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0269 |          50.1029 |           0.2962 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0292 |          49.6279 |           0.2968 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0330 |          49.5015 |           0.2976 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0339 |          49.1796 |           0.2969 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0346 |          48.9804 |           0.2976 |
[32m[20221208 13:47:43 @agent_ppo2.py:179][0m |          -0.0368 |          48.7584 |           0.2982 |
[32m[20221208 13:47:43 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:47:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 410.82
[32m[20221208 13:47:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 474.41
[32m[20221208 13:47:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 181.41
[32m[20221208 13:47:44 @agent_ppo2.py:137][0m Total time:       2.39 min
[32m[20221208 13:47:44 @agent_ppo2.py:139][0m 196608 total steps have happened
[32m[20221208 13:47:44 @agent_ppo2.py:115][0m #------------------------ Iteration 96 --------------------------#
[32m[20221208 13:47:44 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:44 @agent_ppo2.py:179][0m |           0.0117 |          37.4688 |           0.3114 |
[32m[20221208 13:47:44 @agent_ppo2.py:179][0m |           0.0002 |          31.8902 |           0.3117 |
[32m[20221208 13:47:44 @agent_ppo2.py:179][0m |          -0.0034 |          30.7330 |           0.3109 |
[32m[20221208 13:47:44 @agent_ppo2.py:179][0m |          -0.0158 |          30.1696 |           0.3141 |
[32m[20221208 13:47:44 @agent_ppo2.py:179][0m |          -0.0190 |          29.6880 |           0.3124 |
[32m[20221208 13:47:45 @agent_ppo2.py:179][0m |          -0.0220 |          29.2687 |           0.3140 |
[32m[20221208 13:47:45 @agent_ppo2.py:179][0m |          -0.0219 |          29.0810 |           0.3149 |
[32m[20221208 13:47:45 @agent_ppo2.py:179][0m |          -0.0230 |          28.8224 |           0.3135 |
[32m[20221208 13:47:45 @agent_ppo2.py:179][0m |          -0.0259 |          28.5025 |           0.3131 |
[32m[20221208 13:47:45 @agent_ppo2.py:179][0m |          -0.0287 |          28.5906 |           0.3144 |
[32m[20221208 13:47:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:47:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 290.34
[32m[20221208 13:47:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 414.46
[32m[20221208 13:47:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 187.37
[32m[20221208 13:47:45 @agent_ppo2.py:137][0m Total time:       2.41 min
[32m[20221208 13:47:45 @agent_ppo2.py:139][0m 198656 total steps have happened
[32m[20221208 13:47:45 @agent_ppo2.py:115][0m #------------------------ Iteration 97 --------------------------#
[32m[20221208 13:47:46 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |           0.0105 |          54.3261 |           0.3086 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0088 |          50.8968 |           0.3065 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0187 |          49.3440 |           0.3069 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0251 |          47.5241 |           0.3072 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0281 |          46.6126 |           0.3076 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0304 |          45.2948 |           0.3091 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0335 |          44.3857 |           0.3085 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0373 |          43.4821 |           0.3093 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0391 |          42.3289 |           0.3095 |
[32m[20221208 13:47:46 @agent_ppo2.py:179][0m |          -0.0388 |          41.2107 |           0.3101 |
[32m[20221208 13:47:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 453.59
[32m[20221208 13:47:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 464.32
[32m[20221208 13:47:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 217.34
[32m[20221208 13:47:47 @agent_ppo2.py:137][0m Total time:       2.44 min
[32m[20221208 13:47:47 @agent_ppo2.py:139][0m 200704 total steps have happened
[32m[20221208 13:47:47 @agent_ppo2.py:115][0m #------------------------ Iteration 98 --------------------------#
[32m[20221208 13:47:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:47 @agent_ppo2.py:179][0m |           0.0092 |          38.2141 |           0.3161 |
[32m[20221208 13:47:47 @agent_ppo2.py:179][0m |          -0.0058 |          33.3990 |           0.3145 |
[32m[20221208 13:47:47 @agent_ppo2.py:179][0m |          -0.0154 |          31.6866 |           0.3161 |
[32m[20221208 13:47:47 @agent_ppo2.py:179][0m |          -0.0225 |          30.8529 |           0.3162 |
[32m[20221208 13:47:47 @agent_ppo2.py:179][0m |          -0.0233 |          30.3136 |           0.3167 |
[32m[20221208 13:47:47 @agent_ppo2.py:179][0m |          -0.0239 |          29.8133 |           0.3166 |
[32m[20221208 13:47:48 @agent_ppo2.py:179][0m |          -0.0323 |          29.1949 |           0.3175 |
[32m[20221208 13:47:48 @agent_ppo2.py:179][0m |          -0.0313 |          28.5934 |           0.3185 |
[32m[20221208 13:47:48 @agent_ppo2.py:179][0m |          -0.0355 |          28.4484 |           0.3184 |
[32m[20221208 13:47:48 @agent_ppo2.py:179][0m |          -0.0311 |          28.1262 |           0.3188 |
[32m[20221208 13:47:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 259.85
[32m[20221208 13:47:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 460.70
[32m[20221208 13:47:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.21
[32m[20221208 13:47:48 @agent_ppo2.py:137][0m Total time:       2.46 min
[32m[20221208 13:47:48 @agent_ppo2.py:139][0m 202752 total steps have happened
[32m[20221208 13:47:48 @agent_ppo2.py:115][0m #------------------------ Iteration 99 --------------------------#
[32m[20221208 13:47:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:47:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |           0.0056 |          46.7096 |           0.3188 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0135 |          44.0624 |           0.3185 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0225 |          43.1283 |           0.3182 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0285 |          42.6485 |           0.3197 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0306 |          42.2002 |           0.3199 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0341 |          41.8070 |           0.3208 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0364 |          41.3388 |           0.3197 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0400 |          41.1999 |           0.3216 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0420 |          40.9294 |           0.3211 |
[32m[20221208 13:47:49 @agent_ppo2.py:179][0m |          -0.0427 |          40.7432 |           0.3223 |
[32m[20221208 13:47:49 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 376.67
[32m[20221208 13:47:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 467.25
[32m[20221208 13:47:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 439.81
[32m[20221208 13:47:50 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 439.81
[32m[20221208 13:47:50 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 439.81
[32m[20221208 13:47:50 @agent_ppo2.py:137][0m Total time:       2.49 min
[32m[20221208 13:47:50 @agent_ppo2.py:139][0m 204800 total steps have happened
[32m[20221208 13:47:50 @agent_ppo2.py:115][0m #------------------------ Iteration 100 --------------------------#
[32m[20221208 13:47:50 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |           0.0082 |          51.8374 |           0.3126 |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |          -0.0081 |          51.3832 |           0.3096 |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |          -0.0189 |          50.8328 |           0.3110 |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |          -0.0250 |          50.4264 |           0.3121 |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |          -0.0284 |          50.2235 |           0.3131 |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |          -0.0327 |          50.0605 |           0.3128 |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |          -0.0325 |          50.0813 |           0.3139 |
[32m[20221208 13:47:50 @agent_ppo2.py:179][0m |          -0.0362 |          49.7294 |           0.3138 |
[32m[20221208 13:47:51 @agent_ppo2.py:179][0m |          -0.0384 |          49.6570 |           0.3146 |
[32m[20221208 13:47:51 @agent_ppo2.py:179][0m |          -0.0399 |          49.3651 |           0.3147 |
[32m[20221208 13:47:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 444.27
[32m[20221208 13:47:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 470.65
[32m[20221208 13:47:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 167.99
[32m[20221208 13:47:51 @agent_ppo2.py:137][0m Total time:       2.51 min
[32m[20221208 13:47:51 @agent_ppo2.py:139][0m 206848 total steps have happened
[32m[20221208 13:47:51 @agent_ppo2.py:115][0m #------------------------ Iteration 101 --------------------------#
[32m[20221208 13:47:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:47:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |           0.0041 |          52.8416 |           0.3262 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0128 |          51.1302 |           0.3273 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0213 |          50.5657 |           0.3280 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0264 |          50.1269 |           0.3301 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0286 |          49.6487 |           0.3297 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0319 |          49.4197 |           0.3310 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0348 |          48.6395 |           0.3319 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0378 |          47.6424 |           0.3325 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0396 |          47.1681 |           0.3328 |
[32m[20221208 13:47:52 @agent_ppo2.py:179][0m |          -0.0412 |          46.0282 |           0.3339 |
[32m[20221208 13:47:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:47:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 395.86
[32m[20221208 13:47:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 490.14
[32m[20221208 13:47:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 529.36
[32m[20221208 13:47:52 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 529.36
[32m[20221208 13:47:52 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 529.36
[32m[20221208 13:47:52 @agent_ppo2.py:137][0m Total time:       2.54 min
[32m[20221208 13:47:52 @agent_ppo2.py:139][0m 208896 total steps have happened
[32m[20221208 13:47:52 @agent_ppo2.py:115][0m #------------------------ Iteration 102 --------------------------#
[32m[20221208 13:47:53 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |           0.0062 |          40.2681 |           0.3324 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0157 |          36.2401 |           0.3345 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0229 |          34.6535 |           0.3345 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0283 |          33.6227 |           0.3363 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0355 |          32.7154 |           0.3386 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0378 |          31.9174 |           0.3377 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0409 |          31.5376 |           0.3371 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0456 |          30.8675 |           0.3385 |
[32m[20221208 13:47:53 @agent_ppo2.py:179][0m |          -0.0462 |          30.4195 |           0.3387 |
[32m[20221208 13:47:54 @agent_ppo2.py:179][0m |          -0.0501 |          29.9739 |           0.3395 |
[32m[20221208 13:47:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 334.52
[32m[20221208 13:47:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 466.56
[32m[20221208 13:47:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 423.54
[32m[20221208 13:47:54 @agent_ppo2.py:137][0m Total time:       2.56 min
[32m[20221208 13:47:54 @agent_ppo2.py:139][0m 210944 total steps have happened
[32m[20221208 13:47:54 @agent_ppo2.py:115][0m #------------------------ Iteration 103 --------------------------#
[32m[20221208 13:47:54 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:54 @agent_ppo2.py:179][0m |           0.0083 |          44.3398 |           0.3426 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0070 |          42.3322 |           0.3425 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0152 |          41.6798 |           0.3431 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0199 |          41.4693 |           0.3437 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0252 |          40.9711 |           0.3442 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0288 |          40.8499 |           0.3461 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0289 |          40.2610 |           0.3456 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0331 |          39.9267 |           0.3465 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0354 |          39.6668 |           0.3476 |
[32m[20221208 13:47:55 @agent_ppo2.py:179][0m |          -0.0361 |          39.3494 |           0.3469 |
[32m[20221208 13:47:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 329.01
[32m[20221208 13:47:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 467.66
[32m[20221208 13:47:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 79.07
[32m[20221208 13:47:55 @agent_ppo2.py:137][0m Total time:       2.58 min
[32m[20221208 13:47:55 @agent_ppo2.py:139][0m 212992 total steps have happened
[32m[20221208 13:47:55 @agent_ppo2.py:115][0m #------------------------ Iteration 104 --------------------------#
[32m[20221208 13:47:56 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |           0.0070 |          60.1386 |           0.3441 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0111 |          56.2785 |           0.3407 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0196 |          54.9725 |           0.3425 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0288 |          53.7978 |           0.3444 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0347 |          53.2959 |           0.3440 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0368 |          52.7111 |           0.3440 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0375 |          52.3138 |           0.3444 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0422 |          51.7321 |           0.3453 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0441 |          51.3671 |           0.3449 |
[32m[20221208 13:47:56 @agent_ppo2.py:179][0m |          -0.0461 |          51.1215 |           0.3471 |
[32m[20221208 13:47:56 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:47:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 426.24
[32m[20221208 13:47:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 490.13
[32m[20221208 13:47:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 441.37
[32m[20221208 13:47:57 @agent_ppo2.py:137][0m Total time:       2.61 min
[32m[20221208 13:47:57 @agent_ppo2.py:139][0m 215040 total steps have happened
[32m[20221208 13:47:57 @agent_ppo2.py:115][0m #------------------------ Iteration 105 --------------------------#
[32m[20221208 13:47:57 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:57 @agent_ppo2.py:179][0m |           0.0096 |          50.7956 |           0.3468 |
[32m[20221208 13:47:57 @agent_ppo2.py:179][0m |          -0.0092 |          45.2705 |           0.3454 |
[32m[20221208 13:47:57 @agent_ppo2.py:179][0m |          -0.0140 |          43.8350 |           0.3459 |
[32m[20221208 13:47:58 @agent_ppo2.py:179][0m |          -0.0200 |          43.2644 |           0.3462 |
[32m[20221208 13:47:58 @agent_ppo2.py:179][0m |          -0.0271 |          42.6565 |           0.3474 |
[32m[20221208 13:47:58 @agent_ppo2.py:179][0m |          -0.0321 |          42.1457 |           0.3481 |
[32m[20221208 13:47:58 @agent_ppo2.py:179][0m |          -0.0337 |          41.7474 |           0.3485 |
[32m[20221208 13:47:58 @agent_ppo2.py:179][0m |          -0.0382 |          41.0442 |           0.3482 |
[32m[20221208 13:47:58 @agent_ppo2.py:179][0m |          -0.0383 |          40.5045 |           0.3506 |
[32m[20221208 13:47:58 @agent_ppo2.py:179][0m |          -0.0392 |          40.1145 |           0.3496 |
[32m[20221208 13:47:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:47:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 352.05
[32m[20221208 13:47:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 448.54
[32m[20221208 13:47:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 518.54
[32m[20221208 13:47:58 @agent_ppo2.py:137][0m Total time:       2.63 min
[32m[20221208 13:47:58 @agent_ppo2.py:139][0m 217088 total steps have happened
[32m[20221208 13:47:58 @agent_ppo2.py:115][0m #------------------------ Iteration 106 --------------------------#
[32m[20221208 13:47:59 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:47:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |           0.0111 |          52.7172 |           0.3555 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0137 |          48.1637 |           0.3517 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0204 |          46.5469 |           0.3511 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0286 |          45.2856 |           0.3526 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0326 |          44.1523 |           0.3518 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0357 |          43.2731 |           0.3519 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0379 |          42.5988 |           0.3525 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0385 |          41.9422 |           0.3525 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0411 |          41.5946 |           0.3533 |
[32m[20221208 13:47:59 @agent_ppo2.py:179][0m |          -0.0427 |          41.1411 |           0.3541 |
[32m[20221208 13:47:59 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 428.88
[32m[20221208 13:48:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 474.37
[32m[20221208 13:48:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 179.82
[32m[20221208 13:48:00 @agent_ppo2.py:137][0m Total time:       2.66 min
[32m[20221208 13:48:00 @agent_ppo2.py:139][0m 219136 total steps have happened
[32m[20221208 13:48:00 @agent_ppo2.py:115][0m #------------------------ Iteration 107 --------------------------#
[32m[20221208 13:48:00 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:00 @agent_ppo2.py:179][0m |           0.0059 |          56.8209 |           0.3512 |
[32m[20221208 13:48:00 @agent_ppo2.py:179][0m |          -0.0169 |          47.3928 |           0.3485 |
[32m[20221208 13:48:00 @agent_ppo2.py:179][0m |          -0.0242 |          45.4142 |           0.3487 |
[32m[20221208 13:48:00 @agent_ppo2.py:179][0m |          -0.0362 |          43.9106 |           0.3504 |
[32m[20221208 13:48:01 @agent_ppo2.py:179][0m |          -0.0379 |          42.9108 |           0.3519 |
[32m[20221208 13:48:01 @agent_ppo2.py:179][0m |          -0.0431 |          42.2083 |           0.3509 |
[32m[20221208 13:48:01 @agent_ppo2.py:179][0m |          -0.0440 |          41.5475 |           0.3512 |
[32m[20221208 13:48:01 @agent_ppo2.py:179][0m |          -0.0485 |          41.0825 |           0.3517 |
[32m[20221208 13:48:01 @agent_ppo2.py:179][0m |          -0.0498 |          40.4486 |           0.3523 |
[32m[20221208 13:48:01 @agent_ppo2.py:179][0m |          -0.0521 |          40.1051 |           0.3527 |
[32m[20221208 13:48:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 414.18
[32m[20221208 13:48:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 486.66
[32m[20221208 13:48:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 199.97
[32m[20221208 13:48:01 @agent_ppo2.py:137][0m Total time:       2.68 min
[32m[20221208 13:48:01 @agent_ppo2.py:139][0m 221184 total steps have happened
[32m[20221208 13:48:01 @agent_ppo2.py:115][0m #------------------------ Iteration 108 --------------------------#
[32m[20221208 13:48:02 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |           0.0039 |          47.9029 |           0.3586 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0098 |          44.8544 |           0.3534 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0227 |          43.5012 |           0.3560 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0294 |          42.8868 |           0.3565 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0315 |          42.4041 |           0.3581 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0361 |          41.6818 |           0.3577 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0372 |          41.1622 |           0.3569 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0402 |          40.8128 |           0.3592 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0419 |          40.5195 |           0.3586 |
[32m[20221208 13:48:02 @agent_ppo2.py:179][0m |          -0.0432 |          40.2280 |           0.3594 |
[32m[20221208 13:48:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 437.78
[32m[20221208 13:48:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 485.36
[32m[20221208 13:48:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 457.03
[32m[20221208 13:48:03 @agent_ppo2.py:137][0m Total time:       2.70 min
[32m[20221208 13:48:03 @agent_ppo2.py:139][0m 223232 total steps have happened
[32m[20221208 13:48:03 @agent_ppo2.py:115][0m #------------------------ Iteration 109 --------------------------#
[32m[20221208 13:48:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:03 @agent_ppo2.py:179][0m |           0.0113 |          53.5214 |           0.3655 |
[32m[20221208 13:48:03 @agent_ppo2.py:179][0m |          -0.0116 |          50.5011 |           0.3626 |
[32m[20221208 13:48:03 @agent_ppo2.py:179][0m |          -0.0187 |          49.2667 |           0.3619 |
[32m[20221208 13:48:03 @agent_ppo2.py:179][0m |          -0.0258 |          48.8242 |           0.3611 |
[32m[20221208 13:48:03 @agent_ppo2.py:179][0m |          -0.0324 |          48.6032 |           0.3630 |
[32m[20221208 13:48:03 @agent_ppo2.py:179][0m |          -0.0324 |          48.0552 |           0.3628 |
[32m[20221208 13:48:04 @agent_ppo2.py:179][0m |          -0.0362 |          48.2639 |           0.3631 |
[32m[20221208 13:48:04 @agent_ppo2.py:179][0m |          -0.0403 |          47.6878 |           0.3627 |
[32m[20221208 13:48:04 @agent_ppo2.py:179][0m |          -0.0409 |          48.0928 |           0.3641 |
[32m[20221208 13:48:04 @agent_ppo2.py:179][0m |          -0.0435 |          47.2163 |           0.3647 |
[32m[20221208 13:48:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 362.57
[32m[20221208 13:48:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 460.11
[32m[20221208 13:48:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 169.25
[32m[20221208 13:48:04 @agent_ppo2.py:137][0m Total time:       2.73 min
[32m[20221208 13:48:04 @agent_ppo2.py:139][0m 225280 total steps have happened
[32m[20221208 13:48:04 @agent_ppo2.py:115][0m #------------------------ Iteration 110 --------------------------#
[32m[20221208 13:48:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |           0.0112 |          51.5140 |           0.3579 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0116 |          47.3081 |           0.3569 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0191 |          46.3867 |           0.3562 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0261 |          45.6073 |           0.3568 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0306 |          45.0937 |           0.3543 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0356 |          44.6834 |           0.3581 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0367 |          44.6850 |           0.3584 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0417 |          44.0457 |           0.3584 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0428 |          44.0095 |           0.3589 |
[32m[20221208 13:48:05 @agent_ppo2.py:179][0m |          -0.0462 |          43.5934 |           0.3602 |
[32m[20221208 13:48:05 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:48:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 333.66
[32m[20221208 13:48:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 474.55
[32m[20221208 13:48:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 453.54
[32m[20221208 13:48:06 @agent_ppo2.py:137][0m Total time:       2.75 min
[32m[20221208 13:48:06 @agent_ppo2.py:139][0m 227328 total steps have happened
[32m[20221208 13:48:06 @agent_ppo2.py:115][0m #------------------------ Iteration 111 --------------------------#
[32m[20221208 13:48:06 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:06 @agent_ppo2.py:179][0m |           0.0136 |          56.3030 |           0.3570 |
[32m[20221208 13:48:06 @agent_ppo2.py:179][0m |          -0.0087 |          54.7430 |           0.3543 |
[32m[20221208 13:48:06 @agent_ppo2.py:179][0m |          -0.0197 |          53.8027 |           0.3554 |
[32m[20221208 13:48:06 @agent_ppo2.py:179][0m |          -0.0254 |          52.9584 |           0.3542 |
[32m[20221208 13:48:06 @agent_ppo2.py:179][0m |          -0.0288 |          52.2196 |           0.3548 |
[32m[20221208 13:48:06 @agent_ppo2.py:179][0m |          -0.0319 |          51.3954 |           0.3543 |
[32m[20221208 13:48:06 @agent_ppo2.py:179][0m |          -0.0332 |          50.8777 |           0.3551 |
[32m[20221208 13:48:07 @agent_ppo2.py:179][0m |          -0.0349 |          50.4594 |           0.3550 |
[32m[20221208 13:48:07 @agent_ppo2.py:179][0m |          -0.0369 |          49.8931 |           0.3562 |
[32m[20221208 13:48:07 @agent_ppo2.py:179][0m |          -0.0392 |          49.0520 |           0.3564 |
[32m[20221208 13:48:07 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 475.41
[32m[20221208 13:48:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 499.78
[32m[20221208 13:48:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 350.21
[32m[20221208 13:48:07 @agent_ppo2.py:137][0m Total time:       2.78 min
[32m[20221208 13:48:07 @agent_ppo2.py:139][0m 229376 total steps have happened
[32m[20221208 13:48:07 @agent_ppo2.py:115][0m #------------------------ Iteration 112 --------------------------#
[32m[20221208 13:48:07 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |           0.0157 |          57.9601 |           0.3508 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0037 |          51.3797 |           0.3504 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0222 |          48.7494 |           0.3507 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0265 |          47.2891 |           0.3526 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0289 |          45.1079 |           0.3515 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0346 |          43.6687 |           0.3520 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0372 |          42.5570 |           0.3531 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0392 |          41.7032 |           0.3544 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0422 |          40.9243 |           0.3546 |
[32m[20221208 13:48:08 @agent_ppo2.py:179][0m |          -0.0419 |          40.0769 |           0.3537 |
[32m[20221208 13:48:08 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 467.54
[32m[20221208 13:48:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 473.82
[32m[20221208 13:48:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 483.20
[32m[20221208 13:48:08 @agent_ppo2.py:137][0m Total time:       2.80 min
[32m[20221208 13:48:08 @agent_ppo2.py:139][0m 231424 total steps have happened
[32m[20221208 13:48:08 @agent_ppo2.py:115][0m #------------------------ Iteration 113 --------------------------#
[32m[20221208 13:48:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |           0.0121 |          39.3692 |           0.3594 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0064 |          35.0192 |           0.3611 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0156 |          33.4153 |           0.3617 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0248 |          32.4635 |           0.3623 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0274 |          31.9556 |           0.3624 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0330 |          31.4093 |           0.3639 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0335 |          30.9707 |           0.3652 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0369 |          30.5313 |           0.3655 |
[32m[20221208 13:48:09 @agent_ppo2.py:179][0m |          -0.0405 |          30.2343 |           0.3644 |
[32m[20221208 13:48:10 @agent_ppo2.py:179][0m |          -0.0422 |          30.0560 |           0.3662 |
[32m[20221208 13:48:10 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:48:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 303.92
[32m[20221208 13:48:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 434.76
[32m[20221208 13:48:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.46
[32m[20221208 13:48:10 @agent_ppo2.py:137][0m Total time:       2.83 min
[32m[20221208 13:48:10 @agent_ppo2.py:139][0m 233472 total steps have happened
[32m[20221208 13:48:10 @agent_ppo2.py:115][0m #------------------------ Iteration 114 --------------------------#
[32m[20221208 13:48:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:10 @agent_ppo2.py:179][0m |           0.0058 |          67.0229 |           0.3458 |
[32m[20221208 13:48:10 @agent_ppo2.py:179][0m |          -0.0151 |          62.0635 |           0.3437 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0183 |          60.1299 |           0.3439 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0281 |          58.2378 |           0.3447 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0298 |          56.6914 |           0.3442 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0358 |          55.0762 |           0.3446 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0389 |          53.9327 |           0.3443 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0406 |          52.7415 |           0.3463 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0447 |          51.6700 |           0.3475 |
[32m[20221208 13:48:11 @agent_ppo2.py:179][0m |          -0.0454 |          50.3774 |           0.3472 |
[32m[20221208 13:48:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 364.42
[32m[20221208 13:48:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 531.92
[32m[20221208 13:48:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 316.77
[32m[20221208 13:48:11 @agent_ppo2.py:137][0m Total time:       2.85 min
[32m[20221208 13:48:11 @agent_ppo2.py:139][0m 235520 total steps have happened
[32m[20221208 13:48:11 @agent_ppo2.py:115][0m #------------------------ Iteration 115 --------------------------#
[32m[20221208 13:48:12 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |           0.0107 |          69.7296 |           0.3445 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0154 |          62.3211 |           0.3442 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0244 |          59.2608 |           0.3442 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0340 |          57.0060 |           0.3430 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0365 |          55.2983 |           0.3432 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0431 |          53.4140 |           0.3449 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0457 |          52.1875 |           0.3449 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0493 |          50.9371 |           0.3454 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0509 |          49.6727 |           0.3466 |
[32m[20221208 13:48:12 @agent_ppo2.py:179][0m |          -0.0503 |          48.8995 |           0.3463 |
[32m[20221208 13:48:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 417.53
[32m[20221208 13:48:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 466.70
[32m[20221208 13:48:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 349.16
[32m[20221208 13:48:13 @agent_ppo2.py:137][0m Total time:       2.87 min
[32m[20221208 13:48:13 @agent_ppo2.py:139][0m 237568 total steps have happened
[32m[20221208 13:48:13 @agent_ppo2.py:115][0m #------------------------ Iteration 116 --------------------------#
[32m[20221208 13:48:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:13 @agent_ppo2.py:179][0m |           0.0093 |          50.2529 |           0.3684 |
[32m[20221208 13:48:13 @agent_ppo2.py:179][0m |          -0.0154 |          46.4196 |           0.3684 |
[32m[20221208 13:48:13 @agent_ppo2.py:179][0m |          -0.0236 |          45.2073 |           0.3697 |
[32m[20221208 13:48:14 @agent_ppo2.py:179][0m |          -0.0255 |          44.1354 |           0.3700 |
[32m[20221208 13:48:14 @agent_ppo2.py:179][0m |          -0.0311 |          43.6226 |           0.3698 |
[32m[20221208 13:48:14 @agent_ppo2.py:179][0m |          -0.0340 |          42.9599 |           0.3713 |
[32m[20221208 13:48:14 @agent_ppo2.py:179][0m |          -0.0368 |          42.4766 |           0.3715 |
[32m[20221208 13:48:14 @agent_ppo2.py:179][0m |          -0.0382 |          42.2321 |           0.3721 |
[32m[20221208 13:48:14 @agent_ppo2.py:179][0m |          -0.0404 |          41.7903 |           0.3738 |
[32m[20221208 13:48:14 @agent_ppo2.py:179][0m |          -0.0425 |          41.6704 |           0.3735 |
[32m[20221208 13:48:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:48:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 419.45
[32m[20221208 13:48:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 458.09
[32m[20221208 13:48:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 230.48
[32m[20221208 13:48:14 @agent_ppo2.py:137][0m Total time:       2.90 min
[32m[20221208 13:48:14 @agent_ppo2.py:139][0m 239616 total steps have happened
[32m[20221208 13:48:14 @agent_ppo2.py:115][0m #------------------------ Iteration 117 --------------------------#
[32m[20221208 13:48:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |           0.0135 |          53.1233 |           0.3521 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0067 |          50.1883 |           0.3512 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0131 |          49.2885 |           0.3492 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0159 |          49.1037 |           0.3496 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0245 |          48.3154 |           0.3504 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0279 |          47.9546 |           0.3500 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0303 |          47.9184 |           0.3500 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0329 |          47.8971 |           0.3500 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0358 |          47.3490 |           0.3517 |
[32m[20221208 13:48:15 @agent_ppo2.py:179][0m |          -0.0350 |          47.1558 |           0.3513 |
[32m[20221208 13:48:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 372.48
[32m[20221208 13:48:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 505.57
[32m[20221208 13:48:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 210.37
[32m[20221208 13:48:16 @agent_ppo2.py:137][0m Total time:       2.92 min
[32m[20221208 13:48:16 @agent_ppo2.py:139][0m 241664 total steps have happened
[32m[20221208 13:48:16 @agent_ppo2.py:115][0m #------------------------ Iteration 118 --------------------------#
[32m[20221208 13:48:16 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:16 @agent_ppo2.py:179][0m |           0.0113 |          63.3430 |           0.3518 |
[32m[20221208 13:48:16 @agent_ppo2.py:179][0m |          -0.0154 |          61.0774 |           0.3516 |
[32m[20221208 13:48:16 @agent_ppo2.py:179][0m |          -0.0246 |          59.9173 |           0.3519 |
[32m[20221208 13:48:16 @agent_ppo2.py:179][0m |          -0.0288 |          59.6173 |           0.3520 |
[32m[20221208 13:48:17 @agent_ppo2.py:179][0m |          -0.0320 |          59.3305 |           0.3541 |
[32m[20221208 13:48:17 @agent_ppo2.py:179][0m |          -0.0344 |          58.5094 |           0.3548 |
[32m[20221208 13:48:17 @agent_ppo2.py:179][0m |          -0.0357 |          58.2463 |           0.3561 |
[32m[20221208 13:48:17 @agent_ppo2.py:179][0m |          -0.0400 |          57.9969 |           0.3572 |
[32m[20221208 13:48:17 @agent_ppo2.py:179][0m |          -0.0419 |          57.8115 |           0.3581 |
[32m[20221208 13:48:17 @agent_ppo2.py:179][0m |          -0.0424 |          57.8609 |           0.3594 |
[32m[20221208 13:48:17 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 498.95
[32m[20221208 13:48:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 503.40
[32m[20221208 13:48:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.50
[32m[20221208 13:48:17 @agent_ppo2.py:137][0m Total time:       2.95 min
[32m[20221208 13:48:17 @agent_ppo2.py:139][0m 243712 total steps have happened
[32m[20221208 13:48:17 @agent_ppo2.py:115][0m #------------------------ Iteration 119 --------------------------#
[32m[20221208 13:48:18 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |           0.0075 |          66.0198 |           0.3638 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0155 |          64.1292 |           0.3598 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0207 |          63.3909 |           0.3610 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0245 |          62.8105 |           0.3608 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0301 |          62.1625 |           0.3613 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0337 |          61.7298 |           0.3604 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0361 |          61.2323 |           0.3610 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0384 |          60.8826 |           0.3596 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0377 |          60.4174 |           0.3600 |
[32m[20221208 13:48:18 @agent_ppo2.py:179][0m |          -0.0403 |          59.5354 |           0.3597 |
[32m[20221208 13:48:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 451.83
[32m[20221208 13:48:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 516.80
[32m[20221208 13:48:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 536.00
[32m[20221208 13:48:19 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 536.00
[32m[20221208 13:48:19 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 536.00
[32m[20221208 13:48:19 @agent_ppo2.py:137][0m Total time:       2.97 min
[32m[20221208 13:48:19 @agent_ppo2.py:139][0m 245760 total steps have happened
[32m[20221208 13:48:19 @agent_ppo2.py:115][0m #------------------------ Iteration 120 --------------------------#
[32m[20221208 13:48:19 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:19 @agent_ppo2.py:179][0m |           0.0105 |          68.3883 |           0.3560 |
[32m[20221208 13:48:19 @agent_ppo2.py:179][0m |          -0.0082 |          62.8891 |           0.3549 |
[32m[20221208 13:48:19 @agent_ppo2.py:179][0m |          -0.0156 |          60.7115 |           0.3543 |
[32m[20221208 13:48:19 @agent_ppo2.py:179][0m |          -0.0296 |          59.7365 |           0.3558 |
[32m[20221208 13:48:19 @agent_ppo2.py:179][0m |          -0.0314 |          58.6710 |           0.3563 |
[32m[20221208 13:48:19 @agent_ppo2.py:179][0m |          -0.0359 |          58.2886 |           0.3560 |
[32m[20221208 13:48:20 @agent_ppo2.py:179][0m |          -0.0420 |          57.6149 |           0.3565 |
[32m[20221208 13:48:20 @agent_ppo2.py:179][0m |          -0.0448 |          57.5260 |           0.3576 |
[32m[20221208 13:48:20 @agent_ppo2.py:179][0m |          -0.0447 |          56.7723 |           0.3579 |
[32m[20221208 13:48:20 @agent_ppo2.py:179][0m |          -0.0487 |          56.4617 |           0.3584 |
[32m[20221208 13:48:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:48:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 418.47
[32m[20221208 13:48:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 458.39
[32m[20221208 13:48:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 536.19
[32m[20221208 13:48:20 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 536.19
[32m[20221208 13:48:20 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 536.19
[32m[20221208 13:48:20 @agent_ppo2.py:137][0m Total time:       3.00 min
[32m[20221208 13:48:20 @agent_ppo2.py:139][0m 247808 total steps have happened
[32m[20221208 13:48:20 @agent_ppo2.py:115][0m #------------------------ Iteration 121 --------------------------#
[32m[20221208 13:48:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:48:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |           0.0052 |          57.0840 |           0.3413 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0090 |          54.8876 |           0.3405 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0168 |          54.0420 |           0.3432 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0210 |          53.2947 |           0.3420 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0260 |          52.4444 |           0.3419 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0296 |          52.1626 |           0.3413 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0314 |          51.9122 |           0.3438 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0350 |          51.6826 |           0.3441 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0344 |          51.3799 |           0.3452 |
[32m[20221208 13:48:21 @agent_ppo2.py:179][0m |          -0.0344 |          51.0687 |           0.3455 |
[32m[20221208 13:48:21 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:48:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 357.05
[32m[20221208 13:48:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 460.02
[32m[20221208 13:48:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 473.76
[32m[20221208 13:48:22 @agent_ppo2.py:137][0m Total time:       3.02 min
[32m[20221208 13:48:22 @agent_ppo2.py:139][0m 249856 total steps have happened
[32m[20221208 13:48:22 @agent_ppo2.py:115][0m #------------------------ Iteration 122 --------------------------#
[32m[20221208 13:48:22 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:22 @agent_ppo2.py:179][0m |           0.0043 |          65.8069 |           0.3547 |
[32m[20221208 13:48:22 @agent_ppo2.py:179][0m |          -0.0191 |          62.7291 |           0.3546 |
[32m[20221208 13:48:22 @agent_ppo2.py:179][0m |          -0.0225 |          61.4049 |           0.3565 |
[32m[20221208 13:48:22 @agent_ppo2.py:179][0m |          -0.0289 |          60.1623 |           0.3556 |
[32m[20221208 13:48:22 @agent_ppo2.py:179][0m |          -0.0296 |          59.2394 |           0.3563 |
[32m[20221208 13:48:22 @agent_ppo2.py:179][0m |          -0.0345 |          58.5979 |           0.3566 |
[32m[20221208 13:48:22 @agent_ppo2.py:179][0m |          -0.0363 |          57.8183 |           0.3570 |
[32m[20221208 13:48:23 @agent_ppo2.py:179][0m |          -0.0401 |          57.1928 |           0.3585 |
[32m[20221208 13:48:23 @agent_ppo2.py:179][0m |          -0.0420 |          56.6103 |           0.3596 |
[32m[20221208 13:48:23 @agent_ppo2.py:179][0m |          -0.0442 |          56.0175 |           0.3598 |
[32m[20221208 13:48:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 456.38
[32m[20221208 13:48:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 496.28
[32m[20221208 13:48:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.43
[32m[20221208 13:48:23 @agent_ppo2.py:137][0m Total time:       3.04 min
[32m[20221208 13:48:23 @agent_ppo2.py:139][0m 251904 total steps have happened
[32m[20221208 13:48:23 @agent_ppo2.py:115][0m #------------------------ Iteration 123 --------------------------#
[32m[20221208 13:48:23 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |           0.0091 |          61.7415 |           0.3493 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0155 |          51.0809 |           0.3464 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0239 |          47.4826 |           0.3487 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0279 |          45.2340 |           0.3500 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0336 |          43.2394 |           0.3510 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0387 |          41.6993 |           0.3513 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0403 |          40.5254 |           0.3514 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0431 |          39.7988 |           0.3508 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0439 |          38.9078 |           0.3516 |
[32m[20221208 13:48:24 @agent_ppo2.py:179][0m |          -0.0467 |          37.8251 |           0.3521 |
[32m[20221208 13:48:24 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 435.07
[32m[20221208 13:48:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 498.51
[32m[20221208 13:48:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 340.17
[32m[20221208 13:48:24 @agent_ppo2.py:137][0m Total time:       3.07 min
[32m[20221208 13:48:24 @agent_ppo2.py:139][0m 253952 total steps have happened
[32m[20221208 13:48:24 @agent_ppo2.py:115][0m #------------------------ Iteration 124 --------------------------#
[32m[20221208 13:48:25 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |           0.0137 |          66.2517 |           0.3588 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0042 |          62.5624 |           0.3580 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0228 |          61.5779 |           0.3598 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0280 |          61.4057 |           0.3602 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0329 |          60.7420 |           0.3608 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0354 |          60.1202 |           0.3608 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0385 |          59.8493 |           0.3603 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0398 |          59.5612 |           0.3618 |
[32m[20221208 13:48:25 @agent_ppo2.py:179][0m |          -0.0406 |          59.3858 |           0.3619 |
[32m[20221208 13:48:26 @agent_ppo2.py:179][0m |          -0.0435 |          59.1551 |           0.3624 |
[32m[20221208 13:48:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 470.24
[32m[20221208 13:48:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 505.55
[32m[20221208 13:48:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.28
[32m[20221208 13:48:26 @agent_ppo2.py:137][0m Total time:       3.09 min
[32m[20221208 13:48:26 @agent_ppo2.py:139][0m 256000 total steps have happened
[32m[20221208 13:48:26 @agent_ppo2.py:115][0m #------------------------ Iteration 125 --------------------------#
[32m[20221208 13:48:26 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:26 @agent_ppo2.py:179][0m |           0.0066 |          61.2760 |           0.3642 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0116 |          59.8681 |           0.3627 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0204 |          58.9980 |           0.3652 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0305 |          58.6440 |           0.3654 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0333 |          58.0887 |           0.3668 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0347 |          57.7457 |           0.3665 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0368 |          57.5577 |           0.3687 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0392 |          57.0546 |           0.3697 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0399 |          56.7810 |           0.3701 |
[32m[20221208 13:48:27 @agent_ppo2.py:179][0m |          -0.0407 |          56.4505 |           0.3712 |
[32m[20221208 13:48:27 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 494.84
[32m[20221208 13:48:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 512.69
[32m[20221208 13:48:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 382.18
[32m[20221208 13:48:27 @agent_ppo2.py:137][0m Total time:       3.12 min
[32m[20221208 13:48:27 @agent_ppo2.py:139][0m 258048 total steps have happened
[32m[20221208 13:48:27 @agent_ppo2.py:115][0m #------------------------ Iteration 126 --------------------------#
[32m[20221208 13:48:28 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |           0.0087 |          62.2276 |           0.3660 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0101 |          61.0441 |           0.3667 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0132 |          60.8694 |           0.3658 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0219 |          60.2466 |           0.3681 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0279 |          59.8507 |           0.3685 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0307 |          59.5647 |           0.3685 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0340 |          59.4123 |           0.3686 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0358 |          59.5275 |           0.3689 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0360 |          59.1970 |           0.3682 |
[32m[20221208 13:48:28 @agent_ppo2.py:179][0m |          -0.0405 |          58.9236 |           0.3696 |
[32m[20221208 13:48:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 488.73
[32m[20221208 13:48:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 509.36
[32m[20221208 13:48:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 321.13
[32m[20221208 13:48:29 @agent_ppo2.py:137][0m Total time:       3.14 min
[32m[20221208 13:48:29 @agent_ppo2.py:139][0m 260096 total steps have happened
[32m[20221208 13:48:29 @agent_ppo2.py:115][0m #------------------------ Iteration 127 --------------------------#
[32m[20221208 13:48:29 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:29 @agent_ppo2.py:179][0m |           0.0120 |          62.7584 |           0.3606 |
[32m[20221208 13:48:29 @agent_ppo2.py:179][0m |          -0.0111 |          60.5248 |           0.3603 |
[32m[20221208 13:48:29 @agent_ppo2.py:179][0m |          -0.0173 |          58.8568 |           0.3603 |
[32m[20221208 13:48:30 @agent_ppo2.py:179][0m |          -0.0233 |          57.4157 |           0.3603 |
[32m[20221208 13:48:30 @agent_ppo2.py:179][0m |          -0.0282 |          56.2236 |           0.3608 |
[32m[20221208 13:48:30 @agent_ppo2.py:179][0m |          -0.0309 |          55.0034 |           0.3618 |
[32m[20221208 13:48:30 @agent_ppo2.py:179][0m |          -0.0315 |          53.6113 |           0.3610 |
[32m[20221208 13:48:30 @agent_ppo2.py:179][0m |          -0.0334 |          52.6235 |           0.3636 |
[32m[20221208 13:48:30 @agent_ppo2.py:179][0m |          -0.0369 |          51.9403 |           0.3639 |
[32m[20221208 13:48:30 @agent_ppo2.py:179][0m |          -0.0360 |          51.7942 |           0.3657 |
[32m[20221208 13:48:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 508.20
[32m[20221208 13:48:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 536.32
[32m[20221208 13:48:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 190.89
[32m[20221208 13:48:30 @agent_ppo2.py:137][0m Total time:       3.17 min
[32m[20221208 13:48:30 @agent_ppo2.py:139][0m 262144 total steps have happened
[32m[20221208 13:48:30 @agent_ppo2.py:115][0m #------------------------ Iteration 128 --------------------------#
[32m[20221208 13:48:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |           0.0107 |          53.5153 |           0.3663 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0077 |          50.6243 |           0.3657 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0206 |          49.4328 |           0.3632 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0264 |          48.8397 |           0.3644 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0289 |          48.1483 |           0.3658 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0319 |          47.6703 |           0.3677 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0362 |          47.4232 |           0.3686 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0385 |          46.7687 |           0.3695 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0395 |          46.4780 |           0.3710 |
[32m[20221208 13:48:31 @agent_ppo2.py:179][0m |          -0.0420 |          46.4110 |           0.3718 |
[32m[20221208 13:48:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 356.14
[32m[20221208 13:48:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 513.78
[32m[20221208 13:48:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 285.25
[32m[20221208 13:48:32 @agent_ppo2.py:137][0m Total time:       3.19 min
[32m[20221208 13:48:32 @agent_ppo2.py:139][0m 264192 total steps have happened
[32m[20221208 13:48:32 @agent_ppo2.py:115][0m #------------------------ Iteration 129 --------------------------#
[32m[20221208 13:48:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:32 @agent_ppo2.py:179][0m |           0.0087 |          68.8118 |           0.3712 |
[32m[20221208 13:48:32 @agent_ppo2.py:179][0m |          -0.0088 |          65.5509 |           0.3677 |
[32m[20221208 13:48:32 @agent_ppo2.py:179][0m |          -0.0169 |          64.0195 |           0.3689 |
[32m[20221208 13:48:32 @agent_ppo2.py:179][0m |          -0.0266 |          62.9426 |           0.3696 |
[32m[20221208 13:48:33 @agent_ppo2.py:179][0m |          -0.0313 |          61.9900 |           0.3694 |
[32m[20221208 13:48:33 @agent_ppo2.py:179][0m |          -0.0355 |          61.1587 |           0.3693 |
[32m[20221208 13:48:33 @agent_ppo2.py:179][0m |          -0.0409 |          60.5428 |           0.3707 |
[32m[20221208 13:48:33 @agent_ppo2.py:179][0m |          -0.0399 |          59.6348 |           0.3711 |
[32m[20221208 13:48:33 @agent_ppo2.py:179][0m |          -0.0385 |          58.5669 |           0.3710 |
[32m[20221208 13:48:33 @agent_ppo2.py:179][0m |          -0.0445 |          57.9424 |           0.3727 |
[32m[20221208 13:48:33 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:48:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 443.10
[32m[20221208 13:48:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 490.96
[32m[20221208 13:48:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 201.01
[32m[20221208 13:48:33 @agent_ppo2.py:137][0m Total time:       3.22 min
[32m[20221208 13:48:33 @agent_ppo2.py:139][0m 266240 total steps have happened
[32m[20221208 13:48:33 @agent_ppo2.py:115][0m #------------------------ Iteration 130 --------------------------#
[32m[20221208 13:48:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:48:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |           0.0094 |          51.4398 |           0.3727 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |           0.0009 |          44.4079 |           0.3758 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0106 |          42.6547 |           0.3780 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0148 |          41.6903 |           0.3764 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0243 |          41.0263 |           0.3810 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0342 |          40.0660 |           0.3823 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0357 |          39.8266 |           0.3819 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0363 |          39.3315 |           0.3818 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0328 |          38.9242 |           0.3841 |
[32m[20221208 13:48:34 @agent_ppo2.py:179][0m |          -0.0387 |          38.7051 |           0.3844 |
[32m[20221208 13:48:34 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:48:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 296.21
[32m[20221208 13:48:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 535.25
[32m[20221208 13:48:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 193.44
[32m[20221208 13:48:35 @agent_ppo2.py:137][0m Total time:       3.24 min
[32m[20221208 13:48:35 @agent_ppo2.py:139][0m 268288 total steps have happened
[32m[20221208 13:48:35 @agent_ppo2.py:115][0m #------------------------ Iteration 131 --------------------------#
[32m[20221208 13:48:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:48:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:35 @agent_ppo2.py:179][0m |           0.0058 |          74.0572 |           0.3868 |
[32m[20221208 13:48:35 @agent_ppo2.py:179][0m |          -0.0146 |          68.7562 |           0.3876 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0264 |          67.1324 |           0.3907 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0339 |          65.8975 |           0.3898 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0352 |          64.9777 |           0.3905 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0391 |          63.9409 |           0.3911 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0443 |          62.7401 |           0.3913 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0470 |          62.1927 |           0.3912 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0491 |          61.4987 |           0.3916 |
[32m[20221208 13:48:36 @agent_ppo2.py:179][0m |          -0.0523 |          60.6973 |           0.3933 |
[32m[20221208 13:48:36 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:48:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 440.59
[32m[20221208 13:48:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 553.91
[32m[20221208 13:48:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 460.59
[32m[20221208 13:48:36 @agent_ppo2.py:137][0m Total time:       3.27 min
[32m[20221208 13:48:36 @agent_ppo2.py:139][0m 270336 total steps have happened
[32m[20221208 13:48:36 @agent_ppo2.py:115][0m #------------------------ Iteration 132 --------------------------#
[32m[20221208 13:48:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |           0.0066 |          59.5596 |           0.3931 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0084 |          54.0161 |           0.3921 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0223 |          50.5652 |           0.3938 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0263 |          48.4119 |           0.3944 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0310 |          47.6596 |           0.3947 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0353 |          45.5890 |           0.3949 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0365 |          45.5031 |           0.3959 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0388 |          43.4786 |           0.3952 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0395 |          43.2028 |           0.3970 |
[32m[20221208 13:48:37 @agent_ppo2.py:179][0m |          -0.0419 |          42.3314 |           0.3965 |
[32m[20221208 13:48:37 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:48:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 362.64
[32m[20221208 13:48:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 520.93
[32m[20221208 13:48:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 341.21
[32m[20221208 13:48:38 @agent_ppo2.py:137][0m Total time:       3.29 min
[32m[20221208 13:48:38 @agent_ppo2.py:139][0m 272384 total steps have happened
[32m[20221208 13:48:38 @agent_ppo2.py:115][0m #------------------------ Iteration 133 --------------------------#
[32m[20221208 13:48:38 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:38 @agent_ppo2.py:179][0m |           0.0075 |          71.7079 |           0.3884 |
[32m[20221208 13:48:38 @agent_ppo2.py:179][0m |          -0.0160 |          66.5710 |           0.3867 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0300 |          63.9320 |           0.3886 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0310 |          62.2057 |           0.3885 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0353 |          60.8393 |           0.3902 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0401 |          59.7961 |           0.3899 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0450 |          58.7438 |           0.3916 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0459 |          57.9961 |           0.3927 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0494 |          56.9686 |           0.3934 |
[32m[20221208 13:48:39 @agent_ppo2.py:179][0m |          -0.0537 |          56.4201 |           0.3948 |
[32m[20221208 13:48:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 514.51
[32m[20221208 13:48:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 551.43
[32m[20221208 13:48:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 220.58
[32m[20221208 13:48:39 @agent_ppo2.py:137][0m Total time:       3.32 min
[32m[20221208 13:48:39 @agent_ppo2.py:139][0m 274432 total steps have happened
[32m[20221208 13:48:39 @agent_ppo2.py:115][0m #------------------------ Iteration 134 --------------------------#
[32m[20221208 13:48:40 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |           0.0049 |          65.6655 |           0.3962 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0151 |          59.7908 |           0.3961 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0220 |          57.4076 |           0.3975 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0316 |          56.0075 |           0.3980 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0331 |          55.0352 |           0.3993 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0398 |          54.1166 |           0.4009 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0397 |          53.4409 |           0.4012 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0478 |          52.5586 |           0.4013 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0484 |          52.5350 |           0.4027 |
[32m[20221208 13:48:40 @agent_ppo2.py:179][0m |          -0.0499 |          51.4621 |           0.4038 |
[32m[20221208 13:48:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 488.80
[32m[20221208 13:48:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 526.01
[32m[20221208 13:48:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 299.05
[32m[20221208 13:48:41 @agent_ppo2.py:137][0m Total time:       3.34 min
[32m[20221208 13:48:41 @agent_ppo2.py:139][0m 276480 total steps have happened
[32m[20221208 13:48:41 @agent_ppo2.py:115][0m #------------------------ Iteration 135 --------------------------#
[32m[20221208 13:48:41 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:41 @agent_ppo2.py:179][0m |           0.0091 |          78.0281 |           0.4258 |
[32m[20221208 13:48:41 @agent_ppo2.py:179][0m |          -0.0169 |          69.8815 |           0.4262 |
[32m[20221208 13:48:41 @agent_ppo2.py:179][0m |          -0.0270 |          66.6181 |           0.4277 |
[32m[20221208 13:48:41 @agent_ppo2.py:179][0m |          -0.0345 |          64.0251 |           0.4279 |
[32m[20221208 13:48:42 @agent_ppo2.py:179][0m |          -0.0409 |          62.5327 |           0.4301 |
[32m[20221208 13:48:42 @agent_ppo2.py:179][0m |          -0.0455 |          61.1180 |           0.4331 |
[32m[20221208 13:48:42 @agent_ppo2.py:179][0m |          -0.0466 |          59.5576 |           0.4350 |
[32m[20221208 13:48:42 @agent_ppo2.py:179][0m |          -0.0504 |          58.7284 |           0.4361 |
[32m[20221208 13:48:42 @agent_ppo2.py:179][0m |          -0.0536 |          57.0247 |           0.4374 |
[32m[20221208 13:48:42 @agent_ppo2.py:179][0m |          -0.0546 |          56.2455 |           0.4391 |
[32m[20221208 13:48:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 481.74
[32m[20221208 13:48:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 494.72
[32m[20221208 13:48:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 250.04
[32m[20221208 13:48:42 @agent_ppo2.py:137][0m Total time:       3.36 min
[32m[20221208 13:48:42 @agent_ppo2.py:139][0m 278528 total steps have happened
[32m[20221208 13:48:42 @agent_ppo2.py:115][0m #------------------------ Iteration 136 --------------------------#
[32m[20221208 13:48:43 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |           0.0104 |          76.3174 |           0.4301 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0068 |          67.1482 |           0.4271 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0192 |          62.9576 |           0.4273 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0268 |          61.0215 |           0.4258 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0322 |          59.7716 |           0.4283 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0349 |          58.4185 |           0.4287 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0414 |          57.4212 |           0.4291 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0443 |          56.8620 |           0.4301 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0465 |          55.8606 |           0.4316 |
[32m[20221208 13:48:43 @agent_ppo2.py:179][0m |          -0.0488 |          55.2558 |           0.4323 |
[32m[20221208 13:48:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 462.81
[32m[20221208 13:48:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 515.43
[32m[20221208 13:48:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 490.97
[32m[20221208 13:48:44 @agent_ppo2.py:137][0m Total time:       3.39 min
[32m[20221208 13:48:44 @agent_ppo2.py:139][0m 280576 total steps have happened
[32m[20221208 13:48:44 @agent_ppo2.py:115][0m #------------------------ Iteration 137 --------------------------#
[32m[20221208 13:48:44 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:44 @agent_ppo2.py:179][0m |           0.0091 |          66.5461 |           0.4236 |
[32m[20221208 13:48:44 @agent_ppo2.py:179][0m |          -0.0190 |          57.7609 |           0.4191 |
[32m[20221208 13:48:44 @agent_ppo2.py:179][0m |          -0.0307 |          53.5144 |           0.4202 |
[32m[20221208 13:48:44 @agent_ppo2.py:179][0m |          -0.0367 |          51.0299 |           0.4202 |
[32m[20221208 13:48:44 @agent_ppo2.py:179][0m |          -0.0436 |          49.0593 |           0.4208 |
[32m[20221208 13:48:44 @agent_ppo2.py:179][0m |          -0.0455 |          47.3337 |           0.4199 |
[32m[20221208 13:48:45 @agent_ppo2.py:179][0m |          -0.0491 |          46.0244 |           0.4207 |
[32m[20221208 13:48:45 @agent_ppo2.py:179][0m |          -0.0504 |          44.4658 |           0.4215 |
[32m[20221208 13:48:45 @agent_ppo2.py:179][0m |          -0.0523 |          43.3638 |           0.4227 |
[32m[20221208 13:48:45 @agent_ppo2.py:179][0m |          -0.0537 |          42.3472 |           0.4233 |
[32m[20221208 13:48:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 488.87
[32m[20221208 13:48:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 554.52
[32m[20221208 13:48:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 362.20
[32m[20221208 13:48:45 @agent_ppo2.py:137][0m Total time:       3.41 min
[32m[20221208 13:48:45 @agent_ppo2.py:139][0m 282624 total steps have happened
[32m[20221208 13:48:45 @agent_ppo2.py:115][0m #------------------------ Iteration 138 --------------------------#
[32m[20221208 13:48:46 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |           0.0154 |          78.6853 |           0.4204 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0118 |          72.7532 |           0.4189 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0232 |          69.6047 |           0.4204 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0297 |          67.2727 |           0.4205 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0352 |          65.0845 |           0.4217 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0393 |          62.9473 |           0.4229 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0425 |          61.6690 |           0.4239 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0461 |          59.9438 |           0.4266 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0489 |          59.1274 |           0.4271 |
[32m[20221208 13:48:46 @agent_ppo2.py:179][0m |          -0.0499 |          57.8074 |           0.4271 |
[32m[20221208 13:48:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 507.82
[32m[20221208 13:48:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 536.43
[32m[20221208 13:48:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 468.48
[32m[20221208 13:48:47 @agent_ppo2.py:137][0m Total time:       3.44 min
[32m[20221208 13:48:47 @agent_ppo2.py:139][0m 284672 total steps have happened
[32m[20221208 13:48:47 @agent_ppo2.py:115][0m #------------------------ Iteration 139 --------------------------#
[32m[20221208 13:48:47 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:47 @agent_ppo2.py:179][0m |           0.0115 |          74.0022 |           0.4163 |
[32m[20221208 13:48:47 @agent_ppo2.py:179][0m |          -0.0062 |          68.1155 |           0.4165 |
[32m[20221208 13:48:47 @agent_ppo2.py:179][0m |          -0.0210 |          65.6300 |           0.4176 |
[32m[20221208 13:48:47 @agent_ppo2.py:179][0m |          -0.0267 |          64.4338 |           0.4188 |
[32m[20221208 13:48:47 @agent_ppo2.py:179][0m |          -0.0326 |          63.3682 |           0.4183 |
[32m[20221208 13:48:47 @agent_ppo2.py:179][0m |          -0.0364 |          62.7272 |           0.4201 |
[32m[20221208 13:48:47 @agent_ppo2.py:179][0m |          -0.0372 |          61.8222 |           0.4218 |
[32m[20221208 13:48:48 @agent_ppo2.py:179][0m |          -0.0408 |          61.1221 |           0.4234 |
[32m[20221208 13:48:48 @agent_ppo2.py:179][0m |          -0.0414 |          60.7337 |           0.4242 |
[32m[20221208 13:48:48 @agent_ppo2.py:179][0m |          -0.0427 |          60.1268 |           0.4243 |
[32m[20221208 13:48:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 501.02
[32m[20221208 13:48:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 550.44
[32m[20221208 13:48:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 435.51
[32m[20221208 13:48:48 @agent_ppo2.py:137][0m Total time:       3.46 min
[32m[20221208 13:48:48 @agent_ppo2.py:139][0m 286720 total steps have happened
[32m[20221208 13:48:48 @agent_ppo2.py:115][0m #------------------------ Iteration 140 --------------------------#
[32m[20221208 13:48:48 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |           0.0090 |          74.4915 |           0.4592 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0175 |          68.8011 |           0.4544 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0284 |          66.1357 |           0.4546 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0362 |          64.2013 |           0.4568 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0391 |          62.9375 |           0.4563 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0407 |          61.9516 |           0.4563 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0441 |          60.5559 |           0.4564 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0475 |          60.0276 |           0.4577 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0496 |          58.9339 |           0.4597 |
[32m[20221208 13:48:49 @agent_ppo2.py:179][0m |          -0.0536 |          58.3512 |           0.4614 |
[32m[20221208 13:48:49 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 480.86
[32m[20221208 13:48:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 582.08
[32m[20221208 13:48:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 474.17
[32m[20221208 13:48:50 @agent_ppo2.py:137][0m Total time:       3.49 min
[32m[20221208 13:48:50 @agent_ppo2.py:139][0m 288768 total steps have happened
[32m[20221208 13:48:50 @agent_ppo2.py:115][0m #------------------------ Iteration 141 --------------------------#
[32m[20221208 13:48:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:48:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |           0.0074 |          80.7013 |           0.4627 |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |          -0.0179 |          72.6126 |           0.4608 |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |          -0.0260 |          68.7146 |           0.4593 |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |          -0.0316 |          65.6904 |           0.4604 |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |          -0.0380 |          63.5005 |           0.4618 |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |          -0.0397 |          60.5862 |           0.4607 |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |          -0.0445 |          58.3262 |           0.4634 |
[32m[20221208 13:48:50 @agent_ppo2.py:179][0m |          -0.0474 |          56.7967 |           0.4652 |
[32m[20221208 13:48:51 @agent_ppo2.py:179][0m |          -0.0490 |          55.2805 |           0.4668 |
[32m[20221208 13:48:51 @agent_ppo2.py:179][0m |          -0.0513 |          54.2292 |           0.4664 |
[32m[20221208 13:48:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:48:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 471.90
[32m[20221208 13:48:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 527.41
[32m[20221208 13:48:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 476.58
[32m[20221208 13:48:51 @agent_ppo2.py:137][0m Total time:       3.51 min
[32m[20221208 13:48:51 @agent_ppo2.py:139][0m 290816 total steps have happened
[32m[20221208 13:48:51 @agent_ppo2.py:115][0m #------------------------ Iteration 142 --------------------------#
[32m[20221208 13:48:51 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:51 @agent_ppo2.py:179][0m |           0.0026 |          63.7923 |           0.4471 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0155 |          55.2435 |           0.4429 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0240 |          51.6429 |           0.4440 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0308 |          50.1299 |           0.4452 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0372 |          48.5180 |           0.4446 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0404 |          47.5792 |           0.4448 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0436 |          46.9140 |           0.4467 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0450 |          45.8373 |           0.4484 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0478 |          45.1314 |           0.4485 |
[32m[20221208 13:48:52 @agent_ppo2.py:179][0m |          -0.0515 |          44.4668 |           0.4495 |
[32m[20221208 13:48:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:48:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 439.56
[32m[20221208 13:48:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 570.34
[32m[20221208 13:48:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 515.42
[32m[20221208 13:48:52 @agent_ppo2.py:137][0m Total time:       3.53 min
[32m[20221208 13:48:52 @agent_ppo2.py:139][0m 292864 total steps have happened
[32m[20221208 13:48:52 @agent_ppo2.py:115][0m #------------------------ Iteration 143 --------------------------#
[32m[20221208 13:48:53 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |           0.0075 |          93.3236 |           0.4448 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0164 |          87.7365 |           0.4444 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0257 |          84.9922 |           0.4442 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0337 |          83.4665 |           0.4456 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0383 |          82.3498 |           0.4446 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0418 |          81.3945 |           0.4471 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0436 |          80.9880 |           0.4474 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0485 |          79.9291 |           0.4490 |
[32m[20221208 13:48:53 @agent_ppo2.py:179][0m |          -0.0522 |          79.3604 |           0.4486 |
[32m[20221208 13:48:54 @agent_ppo2.py:179][0m |          -0.0545 |          79.1893 |           0.4487 |
[32m[20221208 13:48:54 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:48:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 547.19
[32m[20221208 13:48:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 571.77
[32m[20221208 13:48:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 256.68
[32m[20221208 13:48:54 @agent_ppo2.py:137][0m Total time:       3.56 min
[32m[20221208 13:48:54 @agent_ppo2.py:139][0m 294912 total steps have happened
[32m[20221208 13:48:54 @agent_ppo2.py:115][0m #------------------------ Iteration 144 --------------------------#
[32m[20221208 13:48:54 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:54 @agent_ppo2.py:179][0m |           0.0109 |          82.4201 |           0.4530 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |           0.0066 |          78.3903 |           0.4540 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0245 |          75.8342 |           0.4529 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0329 |          74.0785 |           0.4537 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0335 |          72.9831 |           0.4545 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0393 |          72.0425 |           0.4547 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0433 |          71.3521 |           0.4550 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0434 |          70.4504 |           0.4555 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0481 |          69.7803 |           0.4563 |
[32m[20221208 13:48:55 @agent_ppo2.py:179][0m |          -0.0496 |          69.2531 |           0.4567 |
[32m[20221208 13:48:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 495.59
[32m[20221208 13:48:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 606.67
[32m[20221208 13:48:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 565.42
[32m[20221208 13:48:55 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 565.42
[32m[20221208 13:48:55 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 565.42
[32m[20221208 13:48:55 @agent_ppo2.py:137][0m Total time:       3.58 min
[32m[20221208 13:48:55 @agent_ppo2.py:139][0m 296960 total steps have happened
[32m[20221208 13:48:55 @agent_ppo2.py:115][0m #------------------------ Iteration 145 --------------------------#
[32m[20221208 13:48:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |           0.0081 |          59.2530 |           0.4537 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0134 |          52.5715 |           0.4536 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0292 |          50.1619 |           0.4537 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0338 |          47.9561 |           0.4545 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0348 |          46.8886 |           0.4543 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0451 |          45.8773 |           0.4562 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0475 |          44.9368 |           0.4563 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0497 |          44.3055 |           0.4575 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0519 |          43.4213 |           0.4593 |
[32m[20221208 13:48:56 @agent_ppo2.py:179][0m |          -0.0539 |          42.5692 |           0.4602 |
[32m[20221208 13:48:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:48:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 502.40
[32m[20221208 13:48:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 530.18
[32m[20221208 13:48:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 477.47
[32m[20221208 13:48:57 @agent_ppo2.py:137][0m Total time:       3.61 min
[32m[20221208 13:48:57 @agent_ppo2.py:139][0m 299008 total steps have happened
[32m[20221208 13:48:57 @agent_ppo2.py:115][0m #------------------------ Iteration 146 --------------------------#
[32m[20221208 13:48:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:48:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:57 @agent_ppo2.py:179][0m |           0.0088 |          93.3514 |           0.4653 |
[32m[20221208 13:48:57 @agent_ppo2.py:179][0m |          -0.0194 |          87.5451 |           0.4687 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0240 |          83.7775 |           0.4688 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0340 |          81.0292 |           0.4699 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0404 |          78.5524 |           0.4721 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0460 |          76.6448 |           0.4732 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0510 |          73.9860 |           0.4733 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0528 |          72.3235 |           0.4741 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0578 |          71.0997 |           0.4769 |
[32m[20221208 13:48:58 @agent_ppo2.py:179][0m |          -0.0581 |          69.8897 |           0.4777 |
[32m[20221208 13:48:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:48:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 509.50
[32m[20221208 13:48:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 608.56
[32m[20221208 13:48:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 504.83
[32m[20221208 13:48:58 @agent_ppo2.py:137][0m Total time:       3.63 min
[32m[20221208 13:48:58 @agent_ppo2.py:139][0m 301056 total steps have happened
[32m[20221208 13:48:58 @agent_ppo2.py:115][0m #------------------------ Iteration 147 --------------------------#
[32m[20221208 13:48:59 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:48:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |           0.0074 |          84.4805 |           0.4770 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0195 |          76.6506 |           0.4739 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0311 |          73.9251 |           0.4755 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0358 |          72.4048 |           0.4771 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0407 |          70.2042 |           0.4778 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0447 |          69.2797 |           0.4772 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0505 |          67.9710 |           0.4782 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0545 |          66.9100 |           0.4797 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0531 |          66.3313 |           0.4799 |
[32m[20221208 13:48:59 @agent_ppo2.py:179][0m |          -0.0587 |          65.0159 |           0.4789 |
[32m[20221208 13:48:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 535.99
[32m[20221208 13:49:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 648.71
[32m[20221208 13:49:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 487.62
[32m[20221208 13:49:00 @agent_ppo2.py:137][0m Total time:       3.66 min
[32m[20221208 13:49:00 @agent_ppo2.py:139][0m 303104 total steps have happened
[32m[20221208 13:49:00 @agent_ppo2.py:115][0m #------------------------ Iteration 148 --------------------------#
[32m[20221208 13:49:00 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:00 @agent_ppo2.py:179][0m |           0.0083 |          92.4182 |           0.4968 |
[32m[20221208 13:49:00 @agent_ppo2.py:179][0m |          -0.0128 |          81.5127 |           0.4922 |
[32m[20221208 13:49:00 @agent_ppo2.py:179][0m |          -0.0294 |          75.4342 |           0.4932 |
[32m[20221208 13:49:01 @agent_ppo2.py:179][0m |          -0.0404 |          71.1950 |           0.4935 |
[32m[20221208 13:49:01 @agent_ppo2.py:179][0m |          -0.0475 |          67.9676 |           0.4969 |
[32m[20221208 13:49:01 @agent_ppo2.py:179][0m |          -0.0514 |          65.8808 |           0.4975 |
[32m[20221208 13:49:01 @agent_ppo2.py:179][0m |          -0.0560 |          63.9096 |           0.4979 |
[32m[20221208 13:49:01 @agent_ppo2.py:179][0m |          -0.0580 |          62.1069 |           0.4983 |
[32m[20221208 13:49:01 @agent_ppo2.py:179][0m |          -0.0623 |          60.5751 |           0.5004 |
[32m[20221208 13:49:01 @agent_ppo2.py:179][0m |          -0.0648 |          59.4421 |           0.5020 |
[32m[20221208 13:49:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 541.32
[32m[20221208 13:49:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 630.61
[32m[20221208 13:49:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 338.84
[32m[20221208 13:49:01 @agent_ppo2.py:137][0m Total time:       3.68 min
[32m[20221208 13:49:01 @agent_ppo2.py:139][0m 305152 total steps have happened
[32m[20221208 13:49:01 @agent_ppo2.py:115][0m #------------------------ Iteration 149 --------------------------#
[32m[20221208 13:49:02 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |           0.0100 |          71.5415 |           0.4934 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0201 |          61.5773 |           0.4923 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0327 |          56.0862 |           0.4909 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0418 |          52.3606 |           0.4909 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0477 |          49.8254 |           0.4894 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0542 |          47.7177 |           0.4911 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0570 |          46.0858 |           0.4913 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0585 |          44.6557 |           0.4919 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0625 |          43.5207 |           0.4909 |
[32m[20221208 13:49:02 @agent_ppo2.py:179][0m |          -0.0637 |          42.5017 |           0.4919 |
[32m[20221208 13:49:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 464.69
[32m[20221208 13:49:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 596.28
[32m[20221208 13:49:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 440.91
[32m[20221208 13:49:03 @agent_ppo2.py:137][0m Total time:       3.71 min
[32m[20221208 13:49:03 @agent_ppo2.py:139][0m 307200 total steps have happened
[32m[20221208 13:49:03 @agent_ppo2.py:115][0m #------------------------ Iteration 150 --------------------------#
[32m[20221208 13:49:03 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:03 @agent_ppo2.py:179][0m |           0.0107 |          84.5079 |           0.4948 |
[32m[20221208 13:49:03 @agent_ppo2.py:179][0m |          -0.0157 |          72.3035 |           0.4960 |
[32m[20221208 13:49:03 @agent_ppo2.py:179][0m |          -0.0265 |          68.1990 |           0.4947 |
[32m[20221208 13:49:03 @agent_ppo2.py:179][0m |          -0.0355 |          65.7651 |           0.4976 |
[32m[20221208 13:49:04 @agent_ppo2.py:179][0m |          -0.0433 |          63.8047 |           0.4983 |
[32m[20221208 13:49:04 @agent_ppo2.py:179][0m |          -0.0479 |          62.5632 |           0.4980 |
[32m[20221208 13:49:04 @agent_ppo2.py:179][0m |          -0.0502 |          61.3521 |           0.4993 |
[32m[20221208 13:49:04 @agent_ppo2.py:179][0m |          -0.0571 |          60.1550 |           0.5014 |
[32m[20221208 13:49:04 @agent_ppo2.py:179][0m |          -0.0539 |          59.1952 |           0.5028 |
[32m[20221208 13:49:04 @agent_ppo2.py:179][0m |          -0.0581 |          58.2733 |           0.5043 |
[32m[20221208 13:49:04 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:49:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 472.69
[32m[20221208 13:49:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 600.13
[32m[20221208 13:49:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 491.46
[32m[20221208 13:49:04 @agent_ppo2.py:137][0m Total time:       3.73 min
[32m[20221208 13:49:04 @agent_ppo2.py:139][0m 309248 total steps have happened
[32m[20221208 13:49:04 @agent_ppo2.py:115][0m #------------------------ Iteration 151 --------------------------#
[32m[20221208 13:49:05 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |           0.0153 |          92.3219 |           0.5099 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0032 |          86.2113 |           0.5086 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0237 |          83.5944 |           0.5084 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0305 |          81.7732 |           0.5076 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0365 |          80.5787 |           0.5083 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0365 |          79.3535 |           0.5094 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0457 |          78.4300 |           0.5099 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0434 |          77.8007 |           0.5105 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0467 |          76.7961 |           0.5107 |
[32m[20221208 13:49:05 @agent_ppo2.py:179][0m |          -0.0490 |          76.1284 |           0.5128 |
[32m[20221208 13:49:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 527.36
[32m[20221208 13:49:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 603.22
[32m[20221208 13:49:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 289.25
[32m[20221208 13:49:06 @agent_ppo2.py:137][0m Total time:       3.76 min
[32m[20221208 13:49:06 @agent_ppo2.py:139][0m 311296 total steps have happened
[32m[20221208 13:49:06 @agent_ppo2.py:115][0m #------------------------ Iteration 152 --------------------------#
[32m[20221208 13:49:06 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:06 @agent_ppo2.py:179][0m |           0.0086 |         101.3865 |           0.4966 |
[32m[20221208 13:49:06 @agent_ppo2.py:179][0m |          -0.0162 |          86.6217 |           0.4936 |
[32m[20221208 13:49:06 @agent_ppo2.py:179][0m |          -0.0300 |          80.6074 |           0.4955 |
[32m[20221208 13:49:06 @agent_ppo2.py:179][0m |          -0.0356 |          77.3802 |           0.4936 |
[32m[20221208 13:49:06 @agent_ppo2.py:179][0m |          -0.0392 |          74.7091 |           0.4934 |
[32m[20221208 13:49:07 @agent_ppo2.py:179][0m |          -0.0457 |          72.7841 |           0.4938 |
[32m[20221208 13:49:07 @agent_ppo2.py:179][0m |          -0.0499 |          71.1546 |           0.4928 |
[32m[20221208 13:49:07 @agent_ppo2.py:179][0m |          -0.0505 |          70.1278 |           0.4930 |
[32m[20221208 13:49:07 @agent_ppo2.py:179][0m |          -0.0519 |          68.6511 |           0.4936 |
[32m[20221208 13:49:07 @agent_ppo2.py:179][0m |          -0.0527 |          67.3292 |           0.4931 |
[32m[20221208 13:49:07 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 508.62
[32m[20221208 13:49:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 547.84
[32m[20221208 13:49:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 381.63
[32m[20221208 13:49:07 @agent_ppo2.py:137][0m Total time:       3.78 min
[32m[20221208 13:49:07 @agent_ppo2.py:139][0m 313344 total steps have happened
[32m[20221208 13:49:07 @agent_ppo2.py:115][0m #------------------------ Iteration 153 --------------------------#
[32m[20221208 13:49:08 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |           0.0106 |         101.0308 |           0.4898 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0134 |          91.2392 |           0.4918 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0261 |          85.6948 |           0.4931 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0324 |          82.0245 |           0.4938 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0373 |          80.0626 |           0.4949 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0393 |          77.2614 |           0.4953 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0428 |          75.6861 |           0.4961 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0463 |          73.7439 |           0.4985 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0513 |          72.5788 |           0.5003 |
[32m[20221208 13:49:08 @agent_ppo2.py:179][0m |          -0.0539 |          70.7735 |           0.5013 |
[32m[20221208 13:49:08 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:49:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 585.46
[32m[20221208 13:49:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 591.09
[32m[20221208 13:49:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 483.41
[32m[20221208 13:49:09 @agent_ppo2.py:137][0m Total time:       3.80 min
[32m[20221208 13:49:09 @agent_ppo2.py:139][0m 315392 total steps have happened
[32m[20221208 13:49:09 @agent_ppo2.py:115][0m #------------------------ Iteration 154 --------------------------#
[32m[20221208 13:49:09 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:09 @agent_ppo2.py:179][0m |           0.0154 |         109.7712 |           0.5249 |
[32m[20221208 13:49:09 @agent_ppo2.py:179][0m |          -0.0185 |         100.8705 |           0.5247 |
[32m[20221208 13:49:09 @agent_ppo2.py:179][0m |          -0.0323 |          96.8801 |           0.5240 |
[32m[20221208 13:49:09 @agent_ppo2.py:179][0m |          -0.0354 |          93.3628 |           0.5250 |
[32m[20221208 13:49:09 @agent_ppo2.py:179][0m |          -0.0428 |          90.5934 |           0.5276 |
[32m[20221208 13:49:09 @agent_ppo2.py:179][0m |          -0.0465 |          88.8503 |           0.5280 |
[32m[20221208 13:49:10 @agent_ppo2.py:179][0m |          -0.0465 |          86.8408 |           0.5287 |
[32m[20221208 13:49:10 @agent_ppo2.py:179][0m |          -0.0513 |          84.9873 |           0.5319 |
[32m[20221208 13:49:10 @agent_ppo2.py:179][0m |          -0.0519 |          83.1358 |           0.5321 |
[32m[20221208 13:49:10 @agent_ppo2.py:179][0m |          -0.0557 |          81.6729 |           0.5332 |
[32m[20221208 13:49:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 615.50
[32m[20221208 13:49:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 635.08
[32m[20221208 13:49:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 429.28
[32m[20221208 13:49:10 @agent_ppo2.py:137][0m Total time:       3.83 min
[32m[20221208 13:49:10 @agent_ppo2.py:139][0m 317440 total steps have happened
[32m[20221208 13:49:10 @agent_ppo2.py:115][0m #------------------------ Iteration 155 --------------------------#
[32m[20221208 13:49:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |           0.0074 |         121.2849 |           0.5205 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0144 |         114.1336 |           0.5188 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0268 |         109.3698 |           0.5171 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0352 |         106.0968 |           0.5184 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0350 |         102.7523 |           0.5191 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0429 |         100.3250 |           0.5200 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0448 |          97.9716 |           0.5210 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0467 |          96.5485 |           0.5208 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0488 |          94.7556 |           0.5210 |
[32m[20221208 13:49:11 @agent_ppo2.py:179][0m |          -0.0523 |          93.6038 |           0.5227 |
[32m[20221208 13:49:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 585.79
[32m[20221208 13:49:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 636.78
[32m[20221208 13:49:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 671.79
[32m[20221208 13:49:12 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 671.79
[32m[20221208 13:49:12 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 671.79
[32m[20221208 13:49:12 @agent_ppo2.py:137][0m Total time:       3.85 min
[32m[20221208 13:49:12 @agent_ppo2.py:139][0m 319488 total steps have happened
[32m[20221208 13:49:12 @agent_ppo2.py:115][0m #------------------------ Iteration 156 --------------------------#
[32m[20221208 13:49:12 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |           0.0086 |         101.0278 |           0.5221 |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |          -0.0153 |          88.9850 |           0.5228 |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |          -0.0306 |          84.2021 |           0.5250 |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |          -0.0330 |          80.8868 |           0.5243 |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |          -0.0403 |          77.9898 |           0.5251 |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |          -0.0428 |          75.2984 |           0.5240 |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |          -0.0471 |          72.4204 |           0.5254 |
[32m[20221208 13:49:12 @agent_ppo2.py:179][0m |          -0.0485 |          70.3441 |           0.5277 |
[32m[20221208 13:49:13 @agent_ppo2.py:179][0m |          -0.0513 |          68.3366 |           0.5274 |
[32m[20221208 13:49:13 @agent_ppo2.py:179][0m |          -0.0552 |          66.7581 |           0.5294 |
[32m[20221208 13:49:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 576.46
[32m[20221208 13:49:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 667.16
[32m[20221208 13:49:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 527.33
[32m[20221208 13:49:13 @agent_ppo2.py:137][0m Total time:       3.88 min
[32m[20221208 13:49:13 @agent_ppo2.py:139][0m 321536 total steps have happened
[32m[20221208 13:49:13 @agent_ppo2.py:115][0m #------------------------ Iteration 157 --------------------------#
[32m[20221208 13:49:13 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |           0.0106 |          96.6514 |           0.5314 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0171 |          82.4142 |           0.5288 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0301 |          76.1806 |           0.5304 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0385 |          72.6263 |           0.5299 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0461 |          69.7671 |           0.5316 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0508 |          67.2013 |           0.5327 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0547 |          65.6441 |           0.5334 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0570 |          63.7320 |           0.5349 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0596 |          62.3666 |           0.5345 |
[32m[20221208 13:49:14 @agent_ppo2.py:179][0m |          -0.0625 |          60.9684 |           0.5359 |
[32m[20221208 13:49:14 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 613.11
[32m[20221208 13:49:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 631.35
[32m[20221208 13:49:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 494.39
[32m[20221208 13:49:14 @agent_ppo2.py:137][0m Total time:       3.90 min
[32m[20221208 13:49:14 @agent_ppo2.py:139][0m 323584 total steps have happened
[32m[20221208 13:49:14 @agent_ppo2.py:115][0m #------------------------ Iteration 158 --------------------------#
[32m[20221208 13:49:15 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |           0.0056 |         101.8890 |           0.5412 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0137 |          91.4708 |           0.5421 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0263 |          86.7626 |           0.5427 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0329 |          84.0236 |           0.5422 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0389 |          81.9980 |           0.5440 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0421 |          80.4133 |           0.5440 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0395 |          78.8997 |           0.5452 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0502 |          78.0266 |           0.5473 |
[32m[20221208 13:49:15 @agent_ppo2.py:179][0m |          -0.0529 |          76.5741 |           0.5477 |
[32m[20221208 13:49:16 @agent_ppo2.py:179][0m |          -0.0550 |          75.6495 |           0.5492 |
[32m[20221208 13:49:16 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 510.37
[32m[20221208 13:49:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 591.72
[32m[20221208 13:49:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 642.61
[32m[20221208 13:49:16 @agent_ppo2.py:137][0m Total time:       3.93 min
[32m[20221208 13:49:16 @agent_ppo2.py:139][0m 325632 total steps have happened
[32m[20221208 13:49:16 @agent_ppo2.py:115][0m #------------------------ Iteration 159 --------------------------#
[32m[20221208 13:49:16 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:16 @agent_ppo2.py:179][0m |           0.0028 |         110.7100 |           0.5603 |
[32m[20221208 13:49:16 @agent_ppo2.py:179][0m |          -0.0197 |          99.7692 |           0.5588 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0242 |          94.5469 |           0.5629 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0321 |          91.4626 |           0.5608 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0430 |          89.6617 |           0.5651 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0450 |          86.2598 |           0.5668 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0490 |          83.7763 |           0.5679 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0522 |          82.4154 |           0.5694 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0543 |          80.3399 |           0.5698 |
[32m[20221208 13:49:17 @agent_ppo2.py:179][0m |          -0.0571 |          79.0607 |           0.5733 |
[32m[20221208 13:49:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 616.65
[32m[20221208 13:49:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 654.02
[32m[20221208 13:49:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 386.34
[32m[20221208 13:49:17 @agent_ppo2.py:137][0m Total time:       3.95 min
[32m[20221208 13:49:17 @agent_ppo2.py:139][0m 327680 total steps have happened
[32m[20221208 13:49:17 @agent_ppo2.py:115][0m #------------------------ Iteration 160 --------------------------#
[32m[20221208 13:49:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |           0.0085 |         111.9029 |           0.5862 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0228 |         102.1550 |           0.5835 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0324 |          97.5084 |           0.5850 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0387 |          94.9971 |           0.5856 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0414 |          92.5639 |           0.5877 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0449 |          90.7626 |           0.5892 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0485 |          88.3853 |           0.5892 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0510 |          86.3335 |           0.5926 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0523 |          84.9750 |           0.5912 |
[32m[20221208 13:49:18 @agent_ppo2.py:179][0m |          -0.0541 |          83.8075 |           0.5928 |
[32m[20221208 13:49:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 668.03
[32m[20221208 13:49:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 723.45
[32m[20221208 13:49:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 457.51
[32m[20221208 13:49:19 @agent_ppo2.py:137][0m Total time:       3.97 min
[32m[20221208 13:49:19 @agent_ppo2.py:139][0m 329728 total steps have happened
[32m[20221208 13:49:19 @agent_ppo2.py:115][0m #------------------------ Iteration 161 --------------------------#
[32m[20221208 13:49:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:49:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:19 @agent_ppo2.py:179][0m |           0.0157 |         102.3025 |           0.5969 |
[32m[20221208 13:49:19 @agent_ppo2.py:179][0m |          -0.0135 |          91.1002 |           0.5946 |
[32m[20221208 13:49:19 @agent_ppo2.py:179][0m |          -0.0287 |          85.8882 |           0.5956 |
[32m[20221208 13:49:20 @agent_ppo2.py:179][0m |          -0.0320 |          82.1868 |           0.5963 |
[32m[20221208 13:49:20 @agent_ppo2.py:179][0m |          -0.0403 |          79.1682 |           0.5976 |
[32m[20221208 13:49:20 @agent_ppo2.py:179][0m |          -0.0433 |          76.5935 |           0.6003 |
[32m[20221208 13:49:20 @agent_ppo2.py:179][0m |          -0.0458 |          74.7662 |           0.5994 |
[32m[20221208 13:49:20 @agent_ppo2.py:179][0m |          -0.0498 |          71.6398 |           0.5993 |
[32m[20221208 13:49:20 @agent_ppo2.py:179][0m |          -0.0511 |          69.0689 |           0.6006 |
[32m[20221208 13:49:20 @agent_ppo2.py:179][0m |          -0.0535 |          66.5631 |           0.6021 |
[32m[20221208 13:49:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 660.27
[32m[20221208 13:49:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 666.10
[32m[20221208 13:49:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 288.85
[32m[20221208 13:49:20 @agent_ppo2.py:137][0m Total time:       4.00 min
[32m[20221208 13:49:20 @agent_ppo2.py:139][0m 331776 total steps have happened
[32m[20221208 13:49:20 @agent_ppo2.py:115][0m #------------------------ Iteration 162 --------------------------#
[32m[20221208 13:49:21 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |           0.0092 |         121.6261 |           0.5950 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0112 |         108.3173 |           0.5925 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0263 |         103.1310 |           0.5955 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0305 |         100.3289 |           0.5986 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0354 |          98.5208 |           0.5989 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0395 |          96.8525 |           0.5992 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0426 |          94.1179 |           0.6014 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0467 |          92.5264 |           0.6024 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0476 |          91.7457 |           0.6040 |
[32m[20221208 13:49:21 @agent_ppo2.py:179][0m |          -0.0526 |          90.0745 |           0.6050 |
[32m[20221208 13:49:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 631.40
[32m[20221208 13:49:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 713.61
[32m[20221208 13:49:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 616.30
[32m[20221208 13:49:22 @agent_ppo2.py:137][0m Total time:       4.02 min
[32m[20221208 13:49:22 @agent_ppo2.py:139][0m 333824 total steps have happened
[32m[20221208 13:49:22 @agent_ppo2.py:115][0m #------------------------ Iteration 163 --------------------------#
[32m[20221208 13:49:22 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:22 @agent_ppo2.py:179][0m |           0.0163 |         115.8435 |           0.6277 |
[32m[20221208 13:49:22 @agent_ppo2.py:179][0m |          -0.0076 |         106.1562 |           0.6249 |
[32m[20221208 13:49:22 @agent_ppo2.py:179][0m |          -0.0235 |         102.6627 |           0.6304 |
[32m[20221208 13:49:22 @agent_ppo2.py:179][0m |          -0.0314 |         100.4571 |           0.6324 |
[32m[20221208 13:49:23 @agent_ppo2.py:179][0m |          -0.0367 |          98.6027 |           0.6316 |
[32m[20221208 13:49:23 @agent_ppo2.py:179][0m |          -0.0387 |          97.0612 |           0.6324 |
[32m[20221208 13:49:23 @agent_ppo2.py:179][0m |          -0.0408 |          95.9720 |           0.6336 |
[32m[20221208 13:49:23 @agent_ppo2.py:179][0m |          -0.0453 |          94.9193 |           0.6341 |
[32m[20221208 13:49:23 @agent_ppo2.py:179][0m |          -0.0458 |          93.9612 |           0.6345 |
[32m[20221208 13:49:23 @agent_ppo2.py:179][0m |          -0.0485 |          93.1138 |           0.6356 |
[32m[20221208 13:49:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 671.02
[32m[20221208 13:49:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 691.89
[32m[20221208 13:49:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 844.27
[32m[20221208 13:49:23 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 844.27
[32m[20221208 13:49:23 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 844.27
[32m[20221208 13:49:23 @agent_ppo2.py:137][0m Total time:       4.05 min
[32m[20221208 13:49:23 @agent_ppo2.py:139][0m 335872 total steps have happened
[32m[20221208 13:49:23 @agent_ppo2.py:115][0m #------------------------ Iteration 164 --------------------------#
[32m[20221208 13:49:24 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |           0.0093 |         127.6482 |           0.6390 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0124 |         119.8090 |           0.6383 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0234 |         114.9516 |           0.6354 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0295 |         112.3885 |           0.6366 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0356 |         110.3598 |           0.6422 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0404 |         109.4095 |           0.6442 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0412 |         107.4581 |           0.6457 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0446 |         106.2437 |           0.6448 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0466 |         105.1214 |           0.6468 |
[32m[20221208 13:49:24 @agent_ppo2.py:179][0m |          -0.0484 |         104.5753 |           0.6506 |
[32m[20221208 13:49:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:49:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 632.68
[32m[20221208 13:49:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 714.03
[32m[20221208 13:49:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 711.98
[32m[20221208 13:49:25 @agent_ppo2.py:137][0m Total time:       4.07 min
[32m[20221208 13:49:25 @agent_ppo2.py:139][0m 337920 total steps have happened
[32m[20221208 13:49:25 @agent_ppo2.py:115][0m #------------------------ Iteration 165 --------------------------#
[32m[20221208 13:49:25 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:25 @agent_ppo2.py:179][0m |           0.0087 |         128.9343 |           0.6430 |
[32m[20221208 13:49:25 @agent_ppo2.py:179][0m |          -0.0081 |         123.3554 |           0.6362 |
[32m[20221208 13:49:25 @agent_ppo2.py:179][0m |          -0.0213 |         120.7354 |           0.6393 |
[32m[20221208 13:49:25 @agent_ppo2.py:179][0m |          -0.0293 |         119.2442 |           0.6418 |
[32m[20221208 13:49:25 @agent_ppo2.py:179][0m |          -0.0279 |         118.8149 |           0.6426 |
[32m[20221208 13:49:25 @agent_ppo2.py:179][0m |          -0.0326 |         117.5822 |           0.6455 |
[32m[20221208 13:49:26 @agent_ppo2.py:179][0m |          -0.0354 |         117.1618 |           0.6448 |
[32m[20221208 13:49:26 @agent_ppo2.py:179][0m |          -0.0386 |         116.7253 |           0.6502 |
[32m[20221208 13:49:26 @agent_ppo2.py:179][0m |          -0.0398 |         116.1293 |           0.6492 |
[32m[20221208 13:49:26 @agent_ppo2.py:179][0m |          -0.0381 |         116.3132 |           0.6493 |
[32m[20221208 13:49:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 676.99
[32m[20221208 13:49:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 706.76
[32m[20221208 13:49:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 822.16
[32m[20221208 13:49:26 @agent_ppo2.py:137][0m Total time:       4.10 min
[32m[20221208 13:49:26 @agent_ppo2.py:139][0m 339968 total steps have happened
[32m[20221208 13:49:26 @agent_ppo2.py:115][0m #------------------------ Iteration 166 --------------------------#
[32m[20221208 13:49:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |           0.0135 |          89.2440 |           0.6616 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |           0.0022 |          84.2523 |           0.6660 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0017 |          81.8646 |           0.6608 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0155 |          80.7255 |           0.6710 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0172 |          79.6619 |           0.6712 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0219 |          79.2115 |           0.6761 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0290 |          78.3981 |           0.6797 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0310 |          78.0231 |           0.6806 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0321 |          78.1166 |           0.6810 |
[32m[20221208 13:49:27 @agent_ppo2.py:179][0m |          -0.0339 |          77.3488 |           0.6809 |
[32m[20221208 13:49:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 453.36
[32m[20221208 13:49:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 634.13
[32m[20221208 13:49:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 566.09
[32m[20221208 13:49:28 @agent_ppo2.py:137][0m Total time:       4.12 min
[32m[20221208 13:49:28 @agent_ppo2.py:139][0m 342016 total steps have happened
[32m[20221208 13:49:28 @agent_ppo2.py:115][0m #------------------------ Iteration 167 --------------------------#
[32m[20221208 13:49:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:28 @agent_ppo2.py:179][0m |           0.0066 |         120.1264 |           0.7037 |
[32m[20221208 13:49:28 @agent_ppo2.py:179][0m |          -0.0004 |         113.8701 |           0.6964 |
[32m[20221208 13:49:28 @agent_ppo2.py:179][0m |          -0.0192 |         110.8785 |           0.7046 |
[32m[20221208 13:49:28 @agent_ppo2.py:179][0m |          -0.0231 |         108.0243 |           0.7052 |
[32m[20221208 13:49:28 @agent_ppo2.py:179][0m |          -0.0294 |         105.9499 |           0.7079 |
[32m[20221208 13:49:28 @agent_ppo2.py:179][0m |          -0.0321 |         104.0441 |           0.7066 |
[32m[20221208 13:49:28 @agent_ppo2.py:179][0m |          -0.0299 |         103.6474 |           0.7078 |
[32m[20221208 13:49:29 @agent_ppo2.py:179][0m |          -0.0342 |         102.1566 |           0.7093 |
[32m[20221208 13:49:29 @agent_ppo2.py:179][0m |          -0.0380 |         100.3374 |           0.7100 |
[32m[20221208 13:49:29 @agent_ppo2.py:179][0m |          -0.0370 |         100.1507 |           0.7093 |
[32m[20221208 13:49:29 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 741.63
[32m[20221208 13:49:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 774.48
[32m[20221208 13:49:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 584.90
[32m[20221208 13:49:29 @agent_ppo2.py:137][0m Total time:       4.14 min
[32m[20221208 13:49:29 @agent_ppo2.py:139][0m 344064 total steps have happened
[32m[20221208 13:49:29 @agent_ppo2.py:115][0m #------------------------ Iteration 168 --------------------------#
[32m[20221208 13:49:29 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |           0.0082 |         113.9823 |           0.6883 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0138 |         106.4867 |           0.6841 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0191 |         103.2660 |           0.6824 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0272 |         101.6364 |           0.6804 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0298 |          99.7246 |           0.6833 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0340 |          98.2767 |           0.6831 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0392 |          97.4716 |           0.6825 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0357 |          96.8768 |           0.6809 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0392 |          95.5292 |           0.6845 |
[32m[20221208 13:49:30 @agent_ppo2.py:179][0m |          -0.0422 |          94.7397 |           0.6850 |
[32m[20221208 13:49:30 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:49:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 711.96
[32m[20221208 13:49:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 752.40
[32m[20221208 13:49:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 403.22
[32m[20221208 13:49:31 @agent_ppo2.py:137][0m Total time:       4.17 min
[32m[20221208 13:49:31 @agent_ppo2.py:139][0m 346112 total steps have happened
[32m[20221208 13:49:31 @agent_ppo2.py:115][0m #------------------------ Iteration 169 --------------------------#
[32m[20221208 13:49:31 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |           0.0088 |         127.0820 |           0.6866 |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |          -0.0143 |         116.9646 |           0.6832 |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |          -0.0271 |         106.2995 |           0.6871 |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |          -0.0304 |          97.8397 |           0.6873 |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |          -0.0368 |          92.1316 |           0.6914 |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |          -0.0425 |          88.3068 |           0.6940 |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |          -0.0444 |          85.9322 |           0.6943 |
[32m[20221208 13:49:31 @agent_ppo2.py:179][0m |          -0.0469 |          84.0708 |           0.6974 |
[32m[20221208 13:49:32 @agent_ppo2.py:179][0m |          -0.0485 |          82.8975 |           0.6987 |
[32m[20221208 13:49:32 @agent_ppo2.py:179][0m |          -0.0495 |          81.8880 |           0.6988 |
[32m[20221208 13:49:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 599.64
[32m[20221208 13:49:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 655.83
[32m[20221208 13:49:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 573.44
[32m[20221208 13:49:32 @agent_ppo2.py:137][0m Total time:       4.19 min
[32m[20221208 13:49:32 @agent_ppo2.py:139][0m 348160 total steps have happened
[32m[20221208 13:49:32 @agent_ppo2.py:115][0m #------------------------ Iteration 170 --------------------------#
[32m[20221208 13:49:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |           0.0138 |         103.0321 |           0.6960 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0115 |          89.9992 |           0.6879 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0236 |          84.9623 |           0.6952 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0258 |          80.6559 |           0.6972 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0322 |          76.7068 |           0.7002 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0362 |          73.2412 |           0.7023 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0400 |          71.1427 |           0.7032 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0399 |          67.7085 |           0.7021 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0420 |          65.2761 |           0.7044 |
[32m[20221208 13:49:33 @agent_ppo2.py:179][0m |          -0.0439 |          63.6001 |           0.7047 |
[32m[20221208 13:49:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 425.66
[32m[20221208 13:49:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 675.76
[32m[20221208 13:49:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 729.14
[32m[20221208 13:49:33 @agent_ppo2.py:137][0m Total time:       4.22 min
[32m[20221208 13:49:33 @agent_ppo2.py:139][0m 350208 total steps have happened
[32m[20221208 13:49:33 @agent_ppo2.py:115][0m #------------------------ Iteration 171 --------------------------#
[32m[20221208 13:49:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |           0.0143 |         144.5737 |           0.6963 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0083 |         133.5490 |           0.6967 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0220 |         127.9831 |           0.6960 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0329 |         123.5685 |           0.6993 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0381 |         120.0640 |           0.7026 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0395 |         117.8049 |           0.7011 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0404 |         115.7125 |           0.7038 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0460 |         113.7465 |           0.7045 |
[32m[20221208 13:49:34 @agent_ppo2.py:179][0m |          -0.0457 |         112.2492 |           0.7061 |
[32m[20221208 13:49:35 @agent_ppo2.py:179][0m |          -0.0510 |         110.7493 |           0.7049 |
[32m[20221208 13:49:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 636.06
[32m[20221208 13:49:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 740.48
[32m[20221208 13:49:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 852.85
[32m[20221208 13:49:35 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 852.85
[32m[20221208 13:49:35 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 852.85
[32m[20221208 13:49:35 @agent_ppo2.py:137][0m Total time:       4.24 min
[32m[20221208 13:49:35 @agent_ppo2.py:139][0m 352256 total steps have happened
[32m[20221208 13:49:35 @agent_ppo2.py:115][0m #------------------------ Iteration 172 --------------------------#
[32m[20221208 13:49:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:35 @agent_ppo2.py:179][0m |           0.0187 |          95.1364 |           0.7071 |
[32m[20221208 13:49:35 @agent_ppo2.py:179][0m |          -0.0059 |          83.4781 |           0.7059 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0275 |          78.5869 |           0.7111 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0345 |          74.4674 |           0.7123 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0370 |          72.3587 |           0.7124 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0418 |          70.6102 |           0.7176 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0503 |          69.6296 |           0.7200 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0468 |          68.2966 |           0.7188 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0499 |          67.4903 |           0.7217 |
[32m[20221208 13:49:36 @agent_ppo2.py:179][0m |          -0.0544 |          66.8395 |           0.7221 |
[32m[20221208 13:49:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:49:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 500.45
[32m[20221208 13:49:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 700.13
[32m[20221208 13:49:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 258.60
[32m[20221208 13:49:36 @agent_ppo2.py:137][0m Total time:       4.27 min
[32m[20221208 13:49:36 @agent_ppo2.py:139][0m 354304 total steps have happened
[32m[20221208 13:49:36 @agent_ppo2.py:115][0m #------------------------ Iteration 173 --------------------------#
[32m[20221208 13:49:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |           0.0205 |         112.0468 |           0.7759 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0105 |          98.1592 |           0.7716 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0287 |          90.0746 |           0.7765 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0374 |          84.3063 |           0.7805 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0403 |          80.5355 |           0.7839 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0423 |          77.4529 |           0.7843 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0449 |          74.2635 |           0.7860 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0483 |          71.9020 |           0.7878 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0504 |          70.9068 |           0.7917 |
[32m[20221208 13:49:37 @agent_ppo2.py:179][0m |          -0.0506 |          67.9697 |           0.7914 |
[32m[20221208 13:49:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 739.24
[32m[20221208 13:49:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 779.89
[32m[20221208 13:49:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 879.00
[32m[20221208 13:49:38 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 879.00
[32m[20221208 13:49:38 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 879.00
[32m[20221208 13:49:38 @agent_ppo2.py:137][0m Total time:       4.29 min
[32m[20221208 13:49:38 @agent_ppo2.py:139][0m 356352 total steps have happened
[32m[20221208 13:49:38 @agent_ppo2.py:115][0m #------------------------ Iteration 174 --------------------------#
[32m[20221208 13:49:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:38 @agent_ppo2.py:179][0m |           0.0116 |         117.0701 |           0.7689 |
[32m[20221208 13:49:38 @agent_ppo2.py:179][0m |          -0.0102 |         108.3921 |           0.7671 |
[32m[20221208 13:49:38 @agent_ppo2.py:179][0m |          -0.0224 |         104.8004 |           0.7693 |
[32m[20221208 13:49:39 @agent_ppo2.py:179][0m |          -0.0233 |         102.8070 |           0.7696 |
[32m[20221208 13:49:39 @agent_ppo2.py:179][0m |          -0.0229 |         100.7387 |           0.7663 |
[32m[20221208 13:49:39 @agent_ppo2.py:179][0m |          -0.0300 |          99.4438 |           0.7644 |
[32m[20221208 13:49:39 @agent_ppo2.py:179][0m |          -0.0345 |          97.8562 |           0.7669 |
[32m[20221208 13:49:39 @agent_ppo2.py:179][0m |          -0.0344 |          96.7426 |           0.7712 |
[32m[20221208 13:49:39 @agent_ppo2.py:179][0m |          -0.0406 |          95.9793 |           0.7687 |
[32m[20221208 13:49:39 @agent_ppo2.py:179][0m |          -0.0382 |          94.9054 |           0.7687 |
[32m[20221208 13:49:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 732.92
[32m[20221208 13:49:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 821.53
[32m[20221208 13:49:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 809.15
[32m[20221208 13:49:39 @agent_ppo2.py:137][0m Total time:       4.32 min
[32m[20221208 13:49:39 @agent_ppo2.py:139][0m 358400 total steps have happened
[32m[20221208 13:49:39 @agent_ppo2.py:115][0m #------------------------ Iteration 175 --------------------------#
[32m[20221208 13:49:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |           0.0109 |         140.3025 |           0.7910 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0147 |         128.6538 |           0.7880 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0220 |         122.6590 |           0.7905 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0288 |         118.4107 |           0.7955 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0301 |         115.8876 |           0.8027 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0348 |         113.7661 |           0.8027 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0378 |         112.0376 |           0.8058 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0321 |         111.3750 |           0.8047 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0389 |         109.8207 |           0.8081 |
[32m[20221208 13:49:40 @agent_ppo2.py:179][0m |          -0.0424 |         109.0857 |           0.8127 |
[32m[20221208 13:49:40 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 763.42
[32m[20221208 13:49:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 785.52
[32m[20221208 13:49:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 598.27
[32m[20221208 13:49:41 @agent_ppo2.py:137][0m Total time:       4.34 min
[32m[20221208 13:49:41 @agent_ppo2.py:139][0m 360448 total steps have happened
[32m[20221208 13:49:41 @agent_ppo2.py:115][0m #------------------------ Iteration 176 --------------------------#
[32m[20221208 13:49:41 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:41 @agent_ppo2.py:179][0m |           0.0060 |         155.0012 |           0.8476 |
[32m[20221208 13:49:41 @agent_ppo2.py:179][0m |          -0.0076 |         145.3013 |           0.8400 |
[32m[20221208 13:49:41 @agent_ppo2.py:179][0m |          -0.0166 |         143.3166 |           0.8500 |
[32m[20221208 13:49:41 @agent_ppo2.py:179][0m |          -0.0266 |         140.9980 |           0.8467 |
[32m[20221208 13:49:42 @agent_ppo2.py:179][0m |          -0.0292 |         139.0667 |           0.8503 |
[32m[20221208 13:49:42 @agent_ppo2.py:179][0m |          -0.0318 |         137.7941 |           0.8525 |
[32m[20221208 13:49:42 @agent_ppo2.py:179][0m |          -0.0362 |         137.5715 |           0.8562 |
[32m[20221208 13:49:42 @agent_ppo2.py:179][0m |          -0.0380 |         136.6762 |           0.8594 |
[32m[20221208 13:49:42 @agent_ppo2.py:179][0m |          -0.0395 |         135.7480 |           0.8569 |
[32m[20221208 13:49:42 @agent_ppo2.py:179][0m |          -0.0401 |         134.6232 |           0.8602 |
[32m[20221208 13:49:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 740.21
[32m[20221208 13:49:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 778.15
[32m[20221208 13:49:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 726.28
[32m[20221208 13:49:42 @agent_ppo2.py:137][0m Total time:       4.36 min
[32m[20221208 13:49:42 @agent_ppo2.py:139][0m 362496 total steps have happened
[32m[20221208 13:49:42 @agent_ppo2.py:115][0m #------------------------ Iteration 177 --------------------------#
[32m[20221208 13:49:43 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:49:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |           0.0171 |         140.3096 |           0.8748 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0019 |         127.2859 |           0.8754 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0199 |         123.0415 |           0.8753 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0232 |         120.7903 |           0.8769 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0263 |         119.8893 |           0.8799 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0302 |         117.6492 |           0.8820 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0310 |         116.4271 |           0.8842 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0349 |         115.3065 |           0.8830 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0410 |         113.8286 |           0.8869 |
[32m[20221208 13:49:43 @agent_ppo2.py:179][0m |          -0.0407 |         112.9580 |           0.8906 |
[32m[20221208 13:49:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 728.55
[32m[20221208 13:49:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 820.87
[32m[20221208 13:49:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 856.32
[32m[20221208 13:49:44 @agent_ppo2.py:137][0m Total time:       4.39 min
[32m[20221208 13:49:44 @agent_ppo2.py:139][0m 364544 total steps have happened
[32m[20221208 13:49:44 @agent_ppo2.py:115][0m #------------------------ Iteration 178 --------------------------#
[32m[20221208 13:49:44 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:44 @agent_ppo2.py:179][0m |           0.0130 |         155.6896 |           0.9207 |
[32m[20221208 13:49:44 @agent_ppo2.py:179][0m |          -0.0083 |         143.6990 |           0.9159 |
[32m[20221208 13:49:44 @agent_ppo2.py:179][0m |          -0.0225 |         138.9494 |           0.9142 |
[32m[20221208 13:49:44 @agent_ppo2.py:179][0m |          -0.0255 |         135.0931 |           0.9147 |
[32m[20221208 13:49:44 @agent_ppo2.py:179][0m |          -0.0329 |         132.3476 |           0.9129 |
[32m[20221208 13:49:44 @agent_ppo2.py:179][0m |          -0.0363 |         131.0170 |           0.9141 |
[32m[20221208 13:49:45 @agent_ppo2.py:179][0m |          -0.0413 |         128.5754 |           0.9135 |
[32m[20221208 13:49:45 @agent_ppo2.py:179][0m |          -0.0404 |         127.8644 |           0.9127 |
[32m[20221208 13:49:45 @agent_ppo2.py:179][0m |          -0.0416 |         125.2533 |           0.9142 |
[32m[20221208 13:49:45 @agent_ppo2.py:179][0m |          -0.0439 |         124.9295 |           0.9125 |
[32m[20221208 13:49:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 722.14
[32m[20221208 13:49:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 796.95
[32m[20221208 13:49:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 796.32
[32m[20221208 13:49:45 @agent_ppo2.py:137][0m Total time:       4.41 min
[32m[20221208 13:49:45 @agent_ppo2.py:139][0m 366592 total steps have happened
[32m[20221208 13:49:45 @agent_ppo2.py:115][0m #------------------------ Iteration 179 --------------------------#
[32m[20221208 13:49:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |           0.0109 |         154.6861 |           0.8821 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0012 |         149.1167 |           0.8748 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0145 |         145.5408 |           0.8858 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0134 |         143.2408 |           0.8855 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0248 |         141.5304 |           0.8821 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0279 |         140.3835 |           0.8787 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0253 |         139.1614 |           0.8794 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0274 |         138.0471 |           0.8783 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0328 |         137.0572 |           0.8774 |
[32m[20221208 13:49:46 @agent_ppo2.py:179][0m |          -0.0336 |         136.1417 |           0.8822 |
[32m[20221208 13:49:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:49:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 725.42
[32m[20221208 13:49:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 756.24
[32m[20221208 13:49:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 806.97
[32m[20221208 13:49:47 @agent_ppo2.py:137][0m Total time:       4.44 min
[32m[20221208 13:49:47 @agent_ppo2.py:139][0m 368640 total steps have happened
[32m[20221208 13:49:47 @agent_ppo2.py:115][0m #------------------------ Iteration 180 --------------------------#
[32m[20221208 13:49:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:47 @agent_ppo2.py:179][0m |           0.0154 |         146.0105 |           0.8843 |
[32m[20221208 13:49:47 @agent_ppo2.py:179][0m |           0.0033 |         137.3614 |           0.8816 |
[32m[20221208 13:49:47 @agent_ppo2.py:179][0m |          -0.0079 |         131.7809 |           0.8851 |
[32m[20221208 13:49:47 @agent_ppo2.py:179][0m |          -0.0264 |         126.7768 |           0.8910 |
[32m[20221208 13:49:47 @agent_ppo2.py:179][0m |          -0.0283 |         123.1461 |           0.8883 |
[32m[20221208 13:49:47 @agent_ppo2.py:179][0m |          -0.0294 |         120.2284 |           0.8893 |
[32m[20221208 13:49:47 @agent_ppo2.py:179][0m |          -0.0346 |         117.4172 |           0.8892 |
[32m[20221208 13:49:48 @agent_ppo2.py:179][0m |          -0.0369 |         116.4478 |           0.8929 |
[32m[20221208 13:49:48 @agent_ppo2.py:179][0m |          -0.0404 |         113.2164 |           0.8962 |
[32m[20221208 13:49:48 @agent_ppo2.py:179][0m |          -0.0402 |         111.9026 |           0.8924 |
[32m[20221208 13:49:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 761.71
[32m[20221208 13:49:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 838.03
[32m[20221208 13:49:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 419.92
[32m[20221208 13:49:48 @agent_ppo2.py:137][0m Total time:       4.46 min
[32m[20221208 13:49:48 @agent_ppo2.py:139][0m 370688 total steps have happened
[32m[20221208 13:49:48 @agent_ppo2.py:115][0m #------------------------ Iteration 181 --------------------------#
[32m[20221208 13:49:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:49:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |           0.0066 |         172.5378 |           0.9245 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0104 |         166.5347 |           0.9216 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0113 |         164.5304 |           0.9241 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0189 |         161.9166 |           0.9283 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0275 |         160.7879 |           0.9267 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0235 |         159.2201 |           0.9245 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0297 |         158.3500 |           0.9270 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0317 |         157.3108 |           0.9331 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0351 |         155.9280 |           0.9349 |
[32m[20221208 13:49:49 @agent_ppo2.py:179][0m |          -0.0360 |         155.2568 |           0.9358 |
[32m[20221208 13:49:49 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 785.92
[32m[20221208 13:49:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 837.95
[32m[20221208 13:49:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 867.28
[32m[20221208 13:49:50 @agent_ppo2.py:137][0m Total time:       4.49 min
[32m[20221208 13:49:50 @agent_ppo2.py:139][0m 372736 total steps have happened
[32m[20221208 13:49:50 @agent_ppo2.py:115][0m #------------------------ Iteration 182 --------------------------#
[32m[20221208 13:49:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |           0.0255 |         159.4040 |           0.9104 |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |           0.0105 |         143.3369 |           0.9061 |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |          -0.0145 |         136.3663 |           0.9200 |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |          -0.0179 |         131.8352 |           0.9205 |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |          -0.0194 |         129.5882 |           0.9167 |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |          -0.0214 |         127.8396 |           0.9275 |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |          -0.0277 |         126.9669 |           0.9242 |
[32m[20221208 13:49:50 @agent_ppo2.py:179][0m |          -0.0333 |         125.0034 |           0.9249 |
[32m[20221208 13:49:51 @agent_ppo2.py:179][0m |          -0.0317 |         123.9887 |           0.9243 |
[32m[20221208 13:49:51 @agent_ppo2.py:179][0m |          -0.0244 |         124.0846 |           0.9202 |
[32m[20221208 13:49:51 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:49:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 754.97
[32m[20221208 13:49:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 788.73
[32m[20221208 13:49:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 732.53
[32m[20221208 13:49:51 @agent_ppo2.py:137][0m Total time:       4.51 min
[32m[20221208 13:49:51 @agent_ppo2.py:139][0m 374784 total steps have happened
[32m[20221208 13:49:51 @agent_ppo2.py:115][0m #------------------------ Iteration 183 --------------------------#
[32m[20221208 13:49:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |           0.0150 |         170.9460 |           0.9191 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0090 |         158.2419 |           0.9197 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0189 |         154.3708 |           0.9225 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0260 |         152.0656 |           0.9266 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0298 |         149.5438 |           0.9268 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0334 |         147.7016 |           0.9220 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0364 |         148.0020 |           0.9226 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0392 |         145.0257 |           0.9283 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0404 |         144.6473 |           0.9252 |
[32m[20221208 13:49:52 @agent_ppo2.py:179][0m |          -0.0413 |         142.2854 |           0.9281 |
[32m[20221208 13:49:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:49:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 663.71
[32m[20221208 13:49:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 716.38
[32m[20221208 13:49:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 809.34
[32m[20221208 13:49:52 @agent_ppo2.py:137][0m Total time:       4.53 min
[32m[20221208 13:49:52 @agent_ppo2.py:139][0m 376832 total steps have happened
[32m[20221208 13:49:52 @agent_ppo2.py:115][0m #------------------------ Iteration 184 --------------------------#
[32m[20221208 13:49:53 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |           0.0100 |         129.2418 |           0.9602 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0108 |         112.5432 |           0.9481 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0245 |         104.8004 |           0.9542 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0341 |          99.7870 |           0.9556 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0397 |          95.6389 |           0.9578 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0418 |          92.0960 |           0.9578 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0431 |          88.0419 |           0.9583 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0466 |          85.1679 |           0.9554 |
[32m[20221208 13:49:53 @agent_ppo2.py:179][0m |          -0.0477 |          81.8871 |           0.9550 |
[32m[20221208 13:49:54 @agent_ppo2.py:179][0m |          -0.0478 |          81.2042 |           0.9568 |
[32m[20221208 13:49:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 666.57
[32m[20221208 13:49:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 712.78
[32m[20221208 13:49:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 824.46
[32m[20221208 13:49:54 @agent_ppo2.py:137][0m Total time:       4.56 min
[32m[20221208 13:49:54 @agent_ppo2.py:139][0m 378880 total steps have happened
[32m[20221208 13:49:54 @agent_ppo2.py:115][0m #------------------------ Iteration 185 --------------------------#
[32m[20221208 13:49:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:54 @agent_ppo2.py:179][0m |           0.0077 |         146.3423 |           0.9662 |
[32m[20221208 13:49:54 @agent_ppo2.py:179][0m |          -0.0140 |         127.0968 |           0.9545 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0222 |         117.5346 |           0.9560 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0279 |         110.1808 |           0.9572 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0289 |         106.1772 |           0.9548 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0305 |         104.6569 |           0.9571 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0359 |         102.0122 |           0.9551 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0371 |         101.3581 |           0.9597 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0380 |          99.5885 |           0.9600 |
[32m[20221208 13:49:55 @agent_ppo2.py:179][0m |          -0.0408 |          98.3634 |           0.9620 |
[32m[20221208 13:49:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 822.07
[32m[20221208 13:49:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 844.59
[32m[20221208 13:49:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 763.72
[32m[20221208 13:49:55 @agent_ppo2.py:137][0m Total time:       4.58 min
[32m[20221208 13:49:55 @agent_ppo2.py:139][0m 380928 total steps have happened
[32m[20221208 13:49:55 @agent_ppo2.py:115][0m #------------------------ Iteration 186 --------------------------#
[32m[20221208 13:49:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |           0.0205 |         189.3492 |           0.9618 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0052 |         174.7495 |           0.9443 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0181 |         170.1935 |           0.9591 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0288 |         166.6754 |           0.9662 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0277 |         165.1822 |           0.9664 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0288 |         163.5808 |           0.9601 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0346 |         162.2238 |           0.9679 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0336 |         162.9026 |           0.9693 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0385 |         161.1293 |           0.9677 |
[32m[20221208 13:49:56 @agent_ppo2.py:179][0m |          -0.0394 |         160.1803 |           0.9694 |
[32m[20221208 13:49:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:49:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 776.83
[32m[20221208 13:49:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 827.54
[32m[20221208 13:49:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 699.44
[32m[20221208 13:49:57 @agent_ppo2.py:137][0m Total time:       4.61 min
[32m[20221208 13:49:57 @agent_ppo2.py:139][0m 382976 total steps have happened
[32m[20221208 13:49:57 @agent_ppo2.py:115][0m #------------------------ Iteration 187 --------------------------#
[32m[20221208 13:49:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:57 @agent_ppo2.py:179][0m |           0.0201 |         168.1197 |           0.9233 |
[32m[20221208 13:49:57 @agent_ppo2.py:179][0m |          -0.0092 |         160.0799 |           0.9120 |
[32m[20221208 13:49:57 @agent_ppo2.py:179][0m |          -0.0252 |         155.4013 |           0.9239 |
[32m[20221208 13:49:58 @agent_ppo2.py:179][0m |          -0.0301 |         153.0448 |           0.9199 |
[32m[20221208 13:49:58 @agent_ppo2.py:179][0m |          -0.0349 |         150.9111 |           0.9241 |
[32m[20221208 13:49:58 @agent_ppo2.py:179][0m |          -0.0390 |         148.5492 |           0.9231 |
[32m[20221208 13:49:58 @agent_ppo2.py:179][0m |          -0.0404 |         146.9607 |           0.9239 |
[32m[20221208 13:49:58 @agent_ppo2.py:179][0m |          -0.0427 |         146.0630 |           0.9243 |
[32m[20221208 13:49:58 @agent_ppo2.py:179][0m |          -0.0445 |         144.1328 |           0.9273 |
[32m[20221208 13:49:58 @agent_ppo2.py:179][0m |          -0.0467 |         143.5186 |           0.9307 |
[32m[20221208 13:49:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:49:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 715.81
[32m[20221208 13:49:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 731.81
[32m[20221208 13:49:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 891.68
[32m[20221208 13:49:58 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 891.68
[32m[20221208 13:49:58 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 891.68
[32m[20221208 13:49:58 @agent_ppo2.py:137][0m Total time:       4.63 min
[32m[20221208 13:49:58 @agent_ppo2.py:139][0m 385024 total steps have happened
[32m[20221208 13:49:58 @agent_ppo2.py:115][0m #------------------------ Iteration 188 --------------------------#
[32m[20221208 13:49:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:49:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |           0.0137 |         177.8607 |           0.9402 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0047 |         168.5418 |           0.9484 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0126 |         162.6526 |           0.9469 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0223 |         159.2578 |           0.9462 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0216 |         157.0987 |           0.9471 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0256 |         155.0351 |           0.9423 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0290 |         154.0427 |           0.9449 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0317 |         153.8178 |           0.9487 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0351 |         151.5754 |           0.9504 |
[32m[20221208 13:49:59 @agent_ppo2.py:179][0m |          -0.0389 |         150.6350 |           0.9496 |
[32m[20221208 13:49:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 800.84
[32m[20221208 13:50:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 880.34
[32m[20221208 13:50:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 623.06
[32m[20221208 13:50:00 @agent_ppo2.py:137][0m Total time:       4.66 min
[32m[20221208 13:50:00 @agent_ppo2.py:139][0m 387072 total steps have happened
[32m[20221208 13:50:00 @agent_ppo2.py:115][0m #------------------------ Iteration 189 --------------------------#
[32m[20221208 13:50:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:50:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:00 @agent_ppo2.py:179][0m |           0.0293 |         179.0144 |           0.9596 |
[32m[20221208 13:50:00 @agent_ppo2.py:179][0m |          -0.0007 |         166.3940 |           0.9503 |
[32m[20221208 13:50:00 @agent_ppo2.py:179][0m |          -0.0140 |         160.3775 |           0.9528 |
[32m[20221208 13:50:01 @agent_ppo2.py:179][0m |          -0.0238 |         155.5089 |           0.9560 |
[32m[20221208 13:50:01 @agent_ppo2.py:179][0m |          -0.0288 |         151.1585 |           0.9587 |
[32m[20221208 13:50:01 @agent_ppo2.py:179][0m |          -0.0339 |         146.7641 |           0.9592 |
[32m[20221208 13:50:01 @agent_ppo2.py:179][0m |          -0.0356 |         143.1984 |           0.9617 |
[32m[20221208 13:50:01 @agent_ppo2.py:179][0m |          -0.0370 |         139.0721 |           0.9624 |
[32m[20221208 13:50:01 @agent_ppo2.py:179][0m |          -0.0401 |         136.5934 |           0.9660 |
[32m[20221208 13:50:01 @agent_ppo2.py:179][0m |          -0.0407 |         132.8420 |           0.9688 |
[32m[20221208 13:50:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 782.91
[32m[20221208 13:50:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 812.04
[32m[20221208 13:50:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 707.21
[32m[20221208 13:50:01 @agent_ppo2.py:137][0m Total time:       4.68 min
[32m[20221208 13:50:01 @agent_ppo2.py:139][0m 389120 total steps have happened
[32m[20221208 13:50:01 @agent_ppo2.py:115][0m #------------------------ Iteration 190 --------------------------#
[32m[20221208 13:50:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |           0.0080 |         204.6840 |           0.9799 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |           0.0135 |         194.8347 |           0.9577 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |           0.0002 |         191.3505 |           0.9602 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |          -0.0092 |         189.0379 |           0.9650 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |          -0.0164 |         186.4940 |           0.9632 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |          -0.0179 |         183.5154 |           0.9647 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |          -0.0187 |         181.4909 |           0.9524 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |          -0.0190 |         179.2652 |           0.9607 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |          -0.0178 |         178.5216 |           0.9570 |
[32m[20221208 13:50:02 @agent_ppo2.py:179][0m |          -0.0249 |         175.6767 |           0.9609 |
[32m[20221208 13:50:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 809.32
[32m[20221208 13:50:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.06
[32m[20221208 13:50:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 555.39
[32m[20221208 13:50:03 @agent_ppo2.py:137][0m Total time:       4.71 min
[32m[20221208 13:50:03 @agent_ppo2.py:139][0m 391168 total steps have happened
[32m[20221208 13:50:03 @agent_ppo2.py:115][0m #------------------------ Iteration 191 --------------------------#
[32m[20221208 13:50:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:03 @agent_ppo2.py:179][0m |           0.0140 |         203.5230 |           0.9651 |
[32m[20221208 13:50:03 @agent_ppo2.py:179][0m |          -0.0030 |         191.3682 |           0.9617 |
[32m[20221208 13:50:03 @agent_ppo2.py:179][0m |          -0.0044 |         186.6612 |           0.9615 |
[32m[20221208 13:50:03 @agent_ppo2.py:179][0m |          -0.0092 |         183.2895 |           0.9548 |
[32m[20221208 13:50:04 @agent_ppo2.py:179][0m |          -0.0146 |         180.9707 |           0.9553 |
[32m[20221208 13:50:04 @agent_ppo2.py:179][0m |          -0.0178 |         179.4377 |           0.9588 |
[32m[20221208 13:50:04 @agent_ppo2.py:179][0m |          -0.0177 |         179.4506 |           0.9544 |
[32m[20221208 13:50:04 @agent_ppo2.py:179][0m |          -0.0237 |         177.8269 |           0.9621 |
[32m[20221208 13:50:04 @agent_ppo2.py:179][0m |          -0.0255 |         176.5262 |           0.9615 |
[32m[20221208 13:50:04 @agent_ppo2.py:179][0m |          -0.0275 |         175.8620 |           0.9579 |
[32m[20221208 13:50:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 778.48
[32m[20221208 13:50:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 927.51
[32m[20221208 13:50:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 691.08
[32m[20221208 13:50:04 @agent_ppo2.py:137][0m Total time:       4.73 min
[32m[20221208 13:50:04 @agent_ppo2.py:139][0m 393216 total steps have happened
[32m[20221208 13:50:04 @agent_ppo2.py:115][0m #------------------------ Iteration 192 --------------------------#
[32m[20221208 13:50:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |           0.0145 |         168.5301 |           0.9264 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0130 |         156.0896 |           0.9218 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0215 |         151.0391 |           0.9217 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0272 |         147.8095 |           0.9234 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0291 |         144.9668 |           0.9275 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0279 |         143.3631 |           0.9215 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0319 |         141.0723 |           0.9266 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0308 |         139.9328 |           0.9246 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0321 |         137.9296 |           0.9270 |
[32m[20221208 13:50:05 @agent_ppo2.py:179][0m |          -0.0373 |         136.4321 |           0.9298 |
[32m[20221208 13:50:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 800.42
[32m[20221208 13:50:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 864.97
[32m[20221208 13:50:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 884.62
[32m[20221208 13:50:06 @agent_ppo2.py:137][0m Total time:       4.76 min
[32m[20221208 13:50:06 @agent_ppo2.py:139][0m 395264 total steps have happened
[32m[20221208 13:50:06 @agent_ppo2.py:115][0m #------------------------ Iteration 193 --------------------------#
[32m[20221208 13:50:06 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:50:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:06 @agent_ppo2.py:179][0m |           0.0091 |         170.8305 |           0.9714 |
[32m[20221208 13:50:06 @agent_ppo2.py:179][0m |          -0.0086 |         155.0733 |           0.9617 |
[32m[20221208 13:50:06 @agent_ppo2.py:179][0m |          -0.0165 |         144.1305 |           0.9728 |
[32m[20221208 13:50:06 @agent_ppo2.py:179][0m |          -0.0285 |         139.0096 |           0.9750 |
[32m[20221208 13:50:06 @agent_ppo2.py:179][0m |          -0.0322 |         136.4652 |           0.9802 |
[32m[20221208 13:50:07 @agent_ppo2.py:179][0m |          -0.0336 |         133.6446 |           0.9832 |
[32m[20221208 13:50:07 @agent_ppo2.py:179][0m |          -0.0377 |         130.0314 |           0.9851 |
[32m[20221208 13:50:07 @agent_ppo2.py:179][0m |          -0.0369 |         127.6641 |           0.9858 |
[32m[20221208 13:50:07 @agent_ppo2.py:179][0m |          -0.0379 |         124.6371 |           0.9873 |
[32m[20221208 13:50:07 @agent_ppo2.py:179][0m |          -0.0414 |         122.6879 |           0.9933 |
[32m[20221208 13:50:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 753.60
[32m[20221208 13:50:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 804.89
[32m[20221208 13:50:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 966.37
[32m[20221208 13:50:07 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 966.37
[32m[20221208 13:50:07 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 966.37
[32m[20221208 13:50:07 @agent_ppo2.py:137][0m Total time:       4.78 min
[32m[20221208 13:50:07 @agent_ppo2.py:139][0m 397312 total steps have happened
[32m[20221208 13:50:07 @agent_ppo2.py:115][0m #------------------------ Iteration 194 --------------------------#
[32m[20221208 13:50:08 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 13:50:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |           0.0216 |         170.3369 |           0.9720 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0053 |         151.0953 |           0.9679 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0213 |         143.7348 |           0.9683 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0317 |         138.7998 |           0.9669 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0380 |         134.5924 |           0.9710 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0386 |         130.1204 |           0.9702 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0451 |         128.5066 |           0.9681 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0475 |         125.7456 |           0.9695 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0492 |         124.9493 |           0.9702 |
[32m[20221208 13:50:08 @agent_ppo2.py:179][0m |          -0.0501 |         122.8663 |           0.9720 |
[32m[20221208 13:50:08 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 637.12
[32m[20221208 13:50:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 848.86
[32m[20221208 13:50:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 857.27
[32m[20221208 13:50:09 @agent_ppo2.py:137][0m Total time:       4.80 min
[32m[20221208 13:50:09 @agent_ppo2.py:139][0m 399360 total steps have happened
[32m[20221208 13:50:09 @agent_ppo2.py:115][0m #------------------------ Iteration 195 --------------------------#
[32m[20221208 13:50:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:09 @agent_ppo2.py:179][0m |           0.0173 |         197.9445 |           0.9323 |
[32m[20221208 13:50:09 @agent_ppo2.py:179][0m |           0.0072 |         187.9394 |           0.9350 |
[32m[20221208 13:50:09 @agent_ppo2.py:179][0m |          -0.0115 |         183.7174 |           0.9303 |
[32m[20221208 13:50:09 @agent_ppo2.py:179][0m |          -0.0203 |         180.4232 |           0.9328 |
[32m[20221208 13:50:09 @agent_ppo2.py:179][0m |          -0.0282 |         179.2955 |           0.9365 |
[32m[20221208 13:50:09 @agent_ppo2.py:179][0m |          -0.0312 |         176.4011 |           0.9371 |
[32m[20221208 13:50:10 @agent_ppo2.py:179][0m |          -0.0335 |         174.7354 |           0.9384 |
[32m[20221208 13:50:10 @agent_ppo2.py:179][0m |          -0.0355 |         173.5085 |           0.9381 |
[32m[20221208 13:50:10 @agent_ppo2.py:179][0m |          -0.0348 |         172.2986 |           0.9350 |
[32m[20221208 13:50:10 @agent_ppo2.py:179][0m |          -0.0372 |         171.9519 |           0.9372 |
[32m[20221208 13:50:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 752.83
[32m[20221208 13:50:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 787.69
[32m[20221208 13:50:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 601.64
[32m[20221208 13:50:10 @agent_ppo2.py:137][0m Total time:       4.83 min
[32m[20221208 13:50:10 @agent_ppo2.py:139][0m 401408 total steps have happened
[32m[20221208 13:50:10 @agent_ppo2.py:115][0m #------------------------ Iteration 196 --------------------------#
[32m[20221208 13:50:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |           0.0106 |         160.5207 |           0.9477 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0117 |         137.4739 |           0.9360 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0249 |         127.1894 |           0.9377 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0299 |         121.2694 |           0.9374 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0319 |         117.0379 |           0.9355 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0375 |         112.3985 |           0.9372 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0403 |         108.8028 |           0.9383 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0416 |         105.7820 |           0.9374 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0427 |         102.3852 |           0.9388 |
[32m[20221208 13:50:11 @agent_ppo2.py:179][0m |          -0.0442 |          99.8154 |           0.9389 |
[32m[20221208 13:50:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 793.73
[32m[20221208 13:50:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 894.35
[32m[20221208 13:50:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 840.52
[32m[20221208 13:50:12 @agent_ppo2.py:137][0m Total time:       4.85 min
[32m[20221208 13:50:12 @agent_ppo2.py:139][0m 403456 total steps have happened
[32m[20221208 13:50:12 @agent_ppo2.py:115][0m #------------------------ Iteration 197 --------------------------#
[32m[20221208 13:50:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |           0.0206 |         205.3939 |           0.9620 |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |          -0.0050 |         195.6765 |           0.9659 |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |          -0.0147 |         192.6978 |           0.9714 |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |          -0.0176 |         190.1956 |           0.9673 |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |          -0.0217 |         188.5071 |           0.9701 |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |          -0.0236 |         186.7317 |           0.9708 |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |          -0.0286 |         185.2371 |           0.9783 |
[32m[20221208 13:50:12 @agent_ppo2.py:179][0m |          -0.0314 |         183.7990 |           0.9789 |
[32m[20221208 13:50:13 @agent_ppo2.py:179][0m |          -0.0346 |         183.4752 |           0.9836 |
[32m[20221208 13:50:13 @agent_ppo2.py:179][0m |          -0.0363 |         183.2654 |           0.9844 |
[32m[20221208 13:50:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 832.30
[32m[20221208 13:50:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 872.74
[32m[20221208 13:50:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.58
[32m[20221208 13:50:13 @agent_ppo2.py:137][0m Total time:       4.88 min
[32m[20221208 13:50:13 @agent_ppo2.py:139][0m 405504 total steps have happened
[32m[20221208 13:50:13 @agent_ppo2.py:115][0m #------------------------ Iteration 198 --------------------------#
[32m[20221208 13:50:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:50:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |           0.0071 |         189.5318 |           0.9794 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0087 |         180.3584 |           0.9802 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0145 |         175.6856 |           0.9807 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0227 |         172.1403 |           0.9812 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0240 |         169.8187 |           0.9794 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0243 |         167.4150 |           0.9839 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0248 |         165.7158 |           0.9855 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0303 |         163.7663 |           0.9920 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0309 |         162.4725 |           0.9928 |
[32m[20221208 13:50:14 @agent_ppo2.py:179][0m |          -0.0325 |         159.6037 |           0.9875 |
[32m[20221208 13:50:14 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 855.68
[32m[20221208 13:50:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 886.93
[32m[20221208 13:50:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 629.57
[32m[20221208 13:50:14 @agent_ppo2.py:137][0m Total time:       4.90 min
[32m[20221208 13:50:14 @agent_ppo2.py:139][0m 407552 total steps have happened
[32m[20221208 13:50:14 @agent_ppo2.py:115][0m #------------------------ Iteration 199 --------------------------#
[32m[20221208 13:50:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |           0.0213 |         231.7040 |           0.9963 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |           0.0020 |         217.2140 |           0.9935 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |           0.0170 |         211.6338 |           0.9919 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |           0.0031 |         209.2706 |           0.9894 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |          -0.0000 |         206.7945 |           0.9983 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |          -0.0080 |         206.2413 |           0.9872 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |          -0.0174 |         205.3704 |           0.9928 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |          -0.0167 |         204.4837 |           0.9919 |
[32m[20221208 13:50:15 @agent_ppo2.py:179][0m |          -0.0151 |         204.0878 |           0.9904 |
[32m[20221208 13:50:16 @agent_ppo2.py:179][0m |          -0.0256 |         203.6514 |           0.9982 |
[32m[20221208 13:50:16 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 712.75
[32m[20221208 13:50:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 858.52
[32m[20221208 13:50:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 905.05
[32m[20221208 13:50:16 @agent_ppo2.py:137][0m Total time:       4.93 min
[32m[20221208 13:50:16 @agent_ppo2.py:139][0m 409600 total steps have happened
[32m[20221208 13:50:16 @agent_ppo2.py:115][0m #------------------------ Iteration 200 --------------------------#
[32m[20221208 13:50:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:16 @agent_ppo2.py:179][0m |           0.0121 |         205.9999 |           0.9522 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |           0.0101 |         202.8486 |           0.9480 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |          -0.0045 |         200.9752 |           0.9450 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |           0.0032 |         200.5044 |           0.9524 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |          -0.0095 |         199.9558 |           0.9571 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |          -0.0095 |         200.8132 |           0.9536 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |          -0.0031 |         199.7003 |           0.9480 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |          -0.0121 |         198.9999 |           0.9570 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |          -0.0146 |         198.9484 |           0.9519 |
[32m[20221208 13:50:17 @agent_ppo2.py:179][0m |          -0.0178 |         198.8826 |           0.9577 |
[32m[20221208 13:50:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 864.17
[32m[20221208 13:50:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 896.13
[32m[20221208 13:50:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 861.17
[32m[20221208 13:50:17 @agent_ppo2.py:137][0m Total time:       4.95 min
[32m[20221208 13:50:17 @agent_ppo2.py:139][0m 411648 total steps have happened
[32m[20221208 13:50:17 @agent_ppo2.py:115][0m #------------------------ Iteration 201 --------------------------#
[32m[20221208 13:50:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:50:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |           0.0002 |         205.3829 |           0.9416 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |           0.0031 |         203.4232 |           0.9338 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0033 |         201.9900 |           0.9355 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0043 |         202.4121 |           0.9380 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0114 |         201.3743 |           0.9324 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0098 |         201.6456 |           0.9388 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0086 |         201.5896 |           0.9313 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0130 |         202.2108 |           0.9329 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0183 |         200.6987 |           0.9386 |
[32m[20221208 13:50:18 @agent_ppo2.py:179][0m |          -0.0137 |         200.5584 |           0.9385 |
[32m[20221208 13:50:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.78
[32m[20221208 13:50:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.37
[32m[20221208 13:50:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 741.10
[32m[20221208 13:50:19 @agent_ppo2.py:137][0m Total time:       4.98 min
[32m[20221208 13:50:19 @agent_ppo2.py:139][0m 413696 total steps have happened
[32m[20221208 13:50:19 @agent_ppo2.py:115][0m #------------------------ Iteration 202 --------------------------#
[32m[20221208 13:50:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:19 @agent_ppo2.py:179][0m |           0.0287 |         166.9468 |           0.9879 |
[32m[20221208 13:50:19 @agent_ppo2.py:179][0m |          -0.0038 |         155.4731 |           0.9724 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0173 |         153.4364 |           0.9774 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0222 |         152.1040 |           0.9811 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0261 |         151.3587 |           0.9892 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0254 |         151.7986 |           0.9888 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0270 |         150.1013 |           0.9882 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0300 |         149.5308 |           0.9951 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0311 |         149.5939 |           0.9965 |
[32m[20221208 13:50:20 @agent_ppo2.py:179][0m |          -0.0348 |         149.0757 |           1.0006 |
[32m[20221208 13:50:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 846.81
[32m[20221208 13:50:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 894.95
[32m[20221208 13:50:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.63
[32m[20221208 13:50:20 @agent_ppo2.py:137][0m Total time:       5.00 min
[32m[20221208 13:50:20 @agent_ppo2.py:139][0m 415744 total steps have happened
[32m[20221208 13:50:20 @agent_ppo2.py:115][0m #------------------------ Iteration 203 --------------------------#
[32m[20221208 13:50:21 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |           0.0089 |         152.2533 |           1.0288 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |           0.0043 |         142.5706 |           1.0084 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0134 |         135.4262 |           1.0269 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0115 |         133.9887 |           1.0321 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0170 |         131.6936 |           1.0318 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0194 |         129.7418 |           1.0270 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0183 |         128.5947 |           1.0315 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0188 |         127.3356 |           1.0249 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0219 |         126.9576 |           1.0291 |
[32m[20221208 13:50:21 @agent_ppo2.py:179][0m |          -0.0266 |         126.3266 |           1.0340 |
[32m[20221208 13:50:21 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 544.81
[32m[20221208 13:50:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 884.96
[32m[20221208 13:50:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 966.03
[32m[20221208 13:50:22 @agent_ppo2.py:137][0m Total time:       5.02 min
[32m[20221208 13:50:22 @agent_ppo2.py:139][0m 417792 total steps have happened
[32m[20221208 13:50:22 @agent_ppo2.py:115][0m #------------------------ Iteration 204 --------------------------#
[32m[20221208 13:50:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:22 @agent_ppo2.py:179][0m |           0.0024 |         208.7476 |           1.0059 |
[32m[20221208 13:50:22 @agent_ppo2.py:179][0m |          -0.0058 |         203.5235 |           1.0022 |
[32m[20221208 13:50:22 @agent_ppo2.py:179][0m |          -0.0149 |         201.9730 |           1.0030 |
[32m[20221208 13:50:22 @agent_ppo2.py:179][0m |          -0.0113 |         200.9707 |           0.9976 |
[32m[20221208 13:50:23 @agent_ppo2.py:179][0m |          -0.0194 |         199.6158 |           0.9982 |
[32m[20221208 13:50:23 @agent_ppo2.py:179][0m |          -0.0199 |         199.9282 |           0.9979 |
[32m[20221208 13:50:23 @agent_ppo2.py:179][0m |          -0.0207 |         199.7211 |           0.9940 |
[32m[20221208 13:50:23 @agent_ppo2.py:179][0m |          -0.0191 |         198.6337 |           0.9955 |
[32m[20221208 13:50:23 @agent_ppo2.py:179][0m |          -0.0213 |         197.6444 |           0.9979 |
[32m[20221208 13:50:23 @agent_ppo2.py:179][0m |          -0.0208 |         197.6746 |           0.9993 |
[32m[20221208 13:50:23 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 13:50:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 873.77
[32m[20221208 13:50:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.86
[32m[20221208 13:50:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 903.54
[32m[20221208 13:50:23 @agent_ppo2.py:137][0m Total time:       5.05 min
[32m[20221208 13:50:23 @agent_ppo2.py:139][0m 419840 total steps have happened
[32m[20221208 13:50:23 @agent_ppo2.py:115][0m #------------------------ Iteration 205 --------------------------#
[32m[20221208 13:50:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |           0.0062 |         212.3205 |           0.9445 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0049 |         202.7621 |           0.9583 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0135 |         199.8223 |           0.9532 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0156 |         198.6335 |           0.9503 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0170 |         197.8864 |           0.9523 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0165 |         196.5957 |           0.9527 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0173 |         195.6467 |           0.9496 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0217 |         194.7503 |           0.9492 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0153 |         193.3641 |           0.9483 |
[32m[20221208 13:50:24 @agent_ppo2.py:179][0m |          -0.0256 |         193.4444 |           0.9506 |
[32m[20221208 13:50:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 793.07
[32m[20221208 13:50:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 857.90
[32m[20221208 13:50:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 769.71
[32m[20221208 13:50:25 @agent_ppo2.py:137][0m Total time:       5.07 min
[32m[20221208 13:50:25 @agent_ppo2.py:139][0m 421888 total steps have happened
[32m[20221208 13:50:25 @agent_ppo2.py:115][0m #------------------------ Iteration 206 --------------------------#
[32m[20221208 13:50:25 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:25 @agent_ppo2.py:179][0m |           0.0143 |         189.3662 |           0.9370 |
[32m[20221208 13:50:25 @agent_ppo2.py:179][0m |          -0.0074 |         175.9174 |           0.9298 |
[32m[20221208 13:50:25 @agent_ppo2.py:179][0m |          -0.0148 |         167.9803 |           0.9324 |
[32m[20221208 13:50:25 @agent_ppo2.py:179][0m |          -0.0219 |         161.3712 |           0.9358 |
[32m[20221208 13:50:25 @agent_ppo2.py:179][0m |          -0.0229 |         155.3736 |           0.9352 |
[32m[20221208 13:50:26 @agent_ppo2.py:179][0m |          -0.0271 |         151.7898 |           0.9341 |
[32m[20221208 13:50:26 @agent_ppo2.py:179][0m |          -0.0291 |         149.5503 |           0.9389 |
[32m[20221208 13:50:26 @agent_ppo2.py:179][0m |          -0.0307 |         146.2834 |           0.9404 |
[32m[20221208 13:50:26 @agent_ppo2.py:179][0m |          -0.0349 |         144.2798 |           0.9432 |
[32m[20221208 13:50:26 @agent_ppo2.py:179][0m |          -0.0364 |         142.4723 |           0.9453 |
[32m[20221208 13:50:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 763.73
[32m[20221208 13:50:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 826.54
[32m[20221208 13:50:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 902.37
[32m[20221208 13:50:26 @agent_ppo2.py:137][0m Total time:       5.10 min
[32m[20221208 13:50:26 @agent_ppo2.py:139][0m 423936 total steps have happened
[32m[20221208 13:50:26 @agent_ppo2.py:115][0m #------------------------ Iteration 207 --------------------------#
[32m[20221208 13:50:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |           0.0153 |         214.7809 |           0.9493 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0061 |         198.3915 |           0.9378 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0218 |         193.5511 |           0.9440 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0338 |         191.3234 |           0.9477 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0338 |         188.5366 |           0.9463 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0387 |         187.2567 |           0.9533 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0370 |         184.4805 |           0.9538 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0428 |         184.6190 |           0.9590 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0428 |         182.3664 |           0.9583 |
[32m[20221208 13:50:27 @agent_ppo2.py:179][0m |          -0.0417 |         181.6528 |           0.9610 |
[32m[20221208 13:50:27 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 741.40
[32m[20221208 13:50:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 821.24
[32m[20221208 13:50:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 694.41
[32m[20221208 13:50:28 @agent_ppo2.py:137][0m Total time:       5.12 min
[32m[20221208 13:50:28 @agent_ppo2.py:139][0m 425984 total steps have happened
[32m[20221208 13:50:28 @agent_ppo2.py:115][0m #------------------------ Iteration 208 --------------------------#
[32m[20221208 13:50:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:28 @agent_ppo2.py:179][0m |           0.0134 |         152.3252 |           0.9476 |
[32m[20221208 13:50:28 @agent_ppo2.py:179][0m |          -0.0009 |         134.5062 |           0.9403 |
[32m[20221208 13:50:28 @agent_ppo2.py:179][0m |          -0.0103 |         128.0504 |           0.9351 |
[32m[20221208 13:50:28 @agent_ppo2.py:179][0m |          -0.0120 |         123.3994 |           0.9323 |
[32m[20221208 13:50:28 @agent_ppo2.py:179][0m |          -0.0220 |         120.6013 |           0.9448 |
[32m[20221208 13:50:28 @agent_ppo2.py:179][0m |          -0.0246 |         117.9655 |           0.9462 |
[32m[20221208 13:50:29 @agent_ppo2.py:179][0m |          -0.0295 |         117.3317 |           0.9493 |
[32m[20221208 13:50:29 @agent_ppo2.py:179][0m |          -0.0326 |         115.0726 |           0.9507 |
[32m[20221208 13:50:29 @agent_ppo2.py:179][0m |          -0.0361 |         114.2837 |           0.9493 |
[32m[20221208 13:50:29 @agent_ppo2.py:179][0m |          -0.0367 |         113.4483 |           0.9542 |
[32m[20221208 13:50:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 483.53
[32m[20221208 13:50:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 822.74
[32m[20221208 13:50:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 679.52
[32m[20221208 13:50:29 @agent_ppo2.py:137][0m Total time:       5.15 min
[32m[20221208 13:50:29 @agent_ppo2.py:139][0m 428032 total steps have happened
[32m[20221208 13:50:29 @agent_ppo2.py:115][0m #------------------------ Iteration 209 --------------------------#
[32m[20221208 13:50:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |           0.0135 |         209.4024 |           0.9489 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0211 |         172.0566 |           0.9447 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0336 |         157.4909 |           0.9447 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0402 |         150.4772 |           0.9483 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0448 |         144.0243 |           0.9525 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0470 |         138.5225 |           0.9547 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0468 |         135.5893 |           0.9543 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0498 |         132.2994 |           0.9556 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0508 |         129.4271 |           0.9595 |
[32m[20221208 13:50:30 @agent_ppo2.py:179][0m |          -0.0521 |         126.6715 |           0.9581 |
[32m[20221208 13:50:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 781.08
[32m[20221208 13:50:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 818.55
[32m[20221208 13:50:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 845.88
[32m[20221208 13:50:31 @agent_ppo2.py:137][0m Total time:       5.17 min
[32m[20221208 13:50:31 @agent_ppo2.py:139][0m 430080 total steps have happened
[32m[20221208 13:50:31 @agent_ppo2.py:115][0m #------------------------ Iteration 210 --------------------------#
[32m[20221208 13:50:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:31 @agent_ppo2.py:179][0m |           0.0027 |         179.8161 |           0.9840 |
[32m[20221208 13:50:31 @agent_ppo2.py:179][0m |          -0.0158 |         161.5959 |           0.9847 |
[32m[20221208 13:50:31 @agent_ppo2.py:179][0m |          -0.0242 |         155.6863 |           0.9878 |
[32m[20221208 13:50:31 @agent_ppo2.py:179][0m |          -0.0286 |         152.7018 |           0.9958 |
[32m[20221208 13:50:31 @agent_ppo2.py:179][0m |          -0.0320 |         150.2695 |           0.9953 |
[32m[20221208 13:50:31 @agent_ppo2.py:179][0m |          -0.0313 |         147.3055 |           0.9980 |
[32m[20221208 13:50:31 @agent_ppo2.py:179][0m |          -0.0332 |         145.8991 |           1.0018 |
[32m[20221208 13:50:32 @agent_ppo2.py:179][0m |          -0.0368 |         144.1739 |           1.0069 |
[32m[20221208 13:50:32 @agent_ppo2.py:179][0m |          -0.0390 |         143.7522 |           1.0042 |
[32m[20221208 13:50:32 @agent_ppo2.py:179][0m |          -0.0387 |         141.8555 |           1.0059 |
[32m[20221208 13:50:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 792.85
[32m[20221208 13:50:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 820.27
[32m[20221208 13:50:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 917.62
[32m[20221208 13:50:32 @agent_ppo2.py:137][0m Total time:       5.19 min
[32m[20221208 13:50:32 @agent_ppo2.py:139][0m 432128 total steps have happened
[32m[20221208 13:50:32 @agent_ppo2.py:115][0m #------------------------ Iteration 211 --------------------------#
[32m[20221208 13:50:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |           0.0325 |         191.1276 |           0.9885 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |           0.0065 |         176.8314 |           0.9529 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0187 |         169.6878 |           0.9709 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0267 |         164.1108 |           0.9826 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0301 |         158.6439 |           0.9823 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0341 |         154.3289 |           0.9875 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0359 |         151.3078 |           0.9885 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0371 |         148.0695 |           0.9905 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0393 |         146.5859 |           0.9913 |
[32m[20221208 13:50:33 @agent_ppo2.py:179][0m |          -0.0414 |         143.1686 |           0.9960 |
[32m[20221208 13:50:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 784.55
[32m[20221208 13:50:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 932.04
[32m[20221208 13:50:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 907.55
[32m[20221208 13:50:34 @agent_ppo2.py:137][0m Total time:       5.22 min
[32m[20221208 13:50:34 @agent_ppo2.py:139][0m 434176 total steps have happened
[32m[20221208 13:50:34 @agent_ppo2.py:115][0m #------------------------ Iteration 212 --------------------------#
[32m[20221208 13:50:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:50:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:34 @agent_ppo2.py:179][0m |           0.0173 |         206.9685 |           0.9919 |
[32m[20221208 13:50:34 @agent_ppo2.py:179][0m |          -0.0014 |         200.4425 |           0.9948 |
[32m[20221208 13:50:34 @agent_ppo2.py:179][0m |          -0.0093 |         193.6339 |           0.9923 |
[32m[20221208 13:50:34 @agent_ppo2.py:179][0m |          -0.0161 |         189.1325 |           0.9969 |
[32m[20221208 13:50:34 @agent_ppo2.py:179][0m |          -0.0141 |         184.7640 |           1.0002 |
[32m[20221208 13:50:34 @agent_ppo2.py:179][0m |          -0.0127 |         181.5360 |           0.9903 |
[32m[20221208 13:50:34 @agent_ppo2.py:179][0m |          -0.0184 |         179.3656 |           1.0037 |
[32m[20221208 13:50:35 @agent_ppo2.py:179][0m |          -0.0191 |         177.7379 |           1.0019 |
[32m[20221208 13:50:35 @agent_ppo2.py:179][0m |          -0.0207 |         175.3109 |           0.9970 |
[32m[20221208 13:50:35 @agent_ppo2.py:179][0m |          -0.0243 |         173.8441 |           1.0009 |
[32m[20221208 13:50:35 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:50:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 908.21
[32m[20221208 13:50:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 923.36
[32m[20221208 13:50:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 627.26
[32m[20221208 13:50:35 @agent_ppo2.py:137][0m Total time:       5.24 min
[32m[20221208 13:50:35 @agent_ppo2.py:139][0m 436224 total steps have happened
[32m[20221208 13:50:35 @agent_ppo2.py:115][0m #------------------------ Iteration 213 --------------------------#
[32m[20221208 13:50:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:50:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |           0.0152 |         190.1625 |           1.0111 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |           0.0011 |         164.8438 |           1.0075 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0119 |         152.5367 |           1.0113 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0168 |         144.8303 |           1.0146 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0179 |         137.6101 |           1.0166 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0235 |         133.5837 |           1.0215 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0266 |         130.5496 |           1.0256 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0274 |         127.5927 |           1.0261 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0297 |         125.1783 |           1.0276 |
[32m[20221208 13:50:36 @agent_ppo2.py:179][0m |          -0.0314 |         122.6372 |           1.0301 |
[32m[20221208 13:50:36 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:50:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 868.12
[32m[20221208 13:50:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 901.53
[32m[20221208 13:50:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 876.98
[32m[20221208 13:50:37 @agent_ppo2.py:137][0m Total time:       5.27 min
[32m[20221208 13:50:37 @agent_ppo2.py:139][0m 438272 total steps have happened
[32m[20221208 13:50:37 @agent_ppo2.py:115][0m #------------------------ Iteration 214 --------------------------#
[32m[20221208 13:50:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:50:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:37 @agent_ppo2.py:179][0m |           0.0196 |         246.3548 |           1.0111 |
[32m[20221208 13:50:37 @agent_ppo2.py:179][0m |           0.0005 |         217.9319 |           1.0089 |
[32m[20221208 13:50:37 @agent_ppo2.py:179][0m |          -0.0174 |         212.9389 |           1.0160 |
[32m[20221208 13:50:37 @agent_ppo2.py:179][0m |          -0.0254 |         209.0420 |           1.0210 |
[32m[20221208 13:50:37 @agent_ppo2.py:179][0m |          -0.0290 |         207.1266 |           1.0149 |
[32m[20221208 13:50:38 @agent_ppo2.py:179][0m |          -0.0338 |         205.3197 |           1.0265 |
[32m[20221208 13:50:38 @agent_ppo2.py:179][0m |          -0.0370 |         202.9389 |           1.0322 |
[32m[20221208 13:50:38 @agent_ppo2.py:179][0m |          -0.0384 |         201.7981 |           1.0274 |
[32m[20221208 13:50:38 @agent_ppo2.py:179][0m |          -0.0449 |         200.1444 |           1.0314 |
[32m[20221208 13:50:38 @agent_ppo2.py:179][0m |          -0.0358 |         198.5230 |           1.0261 |
[32m[20221208 13:50:38 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:50:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 785.17
[32m[20221208 13:50:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 797.35
[32m[20221208 13:50:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 896.77
[32m[20221208 13:50:38 @agent_ppo2.py:137][0m Total time:       5.30 min
[32m[20221208 13:50:38 @agent_ppo2.py:139][0m 440320 total steps have happened
[32m[20221208 13:50:38 @agent_ppo2.py:115][0m #------------------------ Iteration 215 --------------------------#
[32m[20221208 13:50:39 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:50:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |           0.0130 |         210.2175 |           1.0962 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0035 |         204.3687 |           1.0953 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0097 |         200.7027 |           1.0984 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0140 |         198.4059 |           1.1030 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0153 |         196.2749 |           1.0962 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0182 |         194.4650 |           1.0997 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0156 |         192.5638 |           1.0983 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0182 |         190.4008 |           1.0983 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0223 |         189.6502 |           1.1024 |
[32m[20221208 13:50:39 @agent_ppo2.py:179][0m |          -0.0194 |         188.2776 |           1.1045 |
[32m[20221208 13:50:39 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 13:50:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 882.63
[32m[20221208 13:50:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.61
[32m[20221208 13:50:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 925.13
[32m[20221208 13:50:40 @agent_ppo2.py:137][0m Total time:       5.32 min
[32m[20221208 13:50:40 @agent_ppo2.py:139][0m 442368 total steps have happened
[32m[20221208 13:50:40 @agent_ppo2.py:115][0m #------------------------ Iteration 216 --------------------------#
[32m[20221208 13:50:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:50:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:40 @agent_ppo2.py:179][0m |           0.0130 |         155.0181 |           1.0565 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0009 |         149.6742 |           1.0682 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0050 |         148.3017 |           1.0804 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0092 |         148.9118 |           1.0849 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0101 |         146.6056 |           1.0924 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0129 |         146.0333 |           1.0928 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0174 |         146.1794 |           1.0931 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0207 |         143.9865 |           1.1003 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0246 |         143.5065 |           1.1024 |
[32m[20221208 13:50:41 @agent_ppo2.py:179][0m |          -0.0261 |         143.2585 |           1.1041 |
[32m[20221208 13:50:41 @agent_ppo2.py:124][0m Policy update time: 0.96 s
[32m[20221208 13:50:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 596.30
[32m[20221208 13:50:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 918.19
[32m[20221208 13:50:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 596.67
[32m[20221208 13:50:42 @agent_ppo2.py:137][0m Total time:       5.35 min
[32m[20221208 13:50:42 @agent_ppo2.py:139][0m 444416 total steps have happened
[32m[20221208 13:50:42 @agent_ppo2.py:115][0m #------------------------ Iteration 217 --------------------------#
[32m[20221208 13:50:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:42 @agent_ppo2.py:179][0m |           0.0191 |         177.0341 |           1.1203 |
[32m[20221208 13:50:42 @agent_ppo2.py:179][0m |           0.0478 |         159.4364 |           1.0432 |
[32m[20221208 13:50:42 @agent_ppo2.py:179][0m |          -0.0105 |         155.4366 |           1.1045 |
[32m[20221208 13:50:42 @agent_ppo2.py:179][0m |          -0.0262 |         151.8492 |           1.1198 |
[32m[20221208 13:50:42 @agent_ppo2.py:179][0m |          -0.0298 |         149.0447 |           1.1196 |
[32m[20221208 13:50:42 @agent_ppo2.py:179][0m |          -0.0373 |         147.0461 |           1.1269 |
[32m[20221208 13:50:43 @agent_ppo2.py:179][0m |          -0.0359 |         145.1938 |           1.1261 |
[32m[20221208 13:50:43 @agent_ppo2.py:179][0m |          -0.0393 |         143.3573 |           1.1274 |
[32m[20221208 13:50:43 @agent_ppo2.py:179][0m |          -0.0371 |         142.3975 |           1.1301 |
[32m[20221208 13:50:43 @agent_ppo2.py:179][0m |          -0.0415 |         140.6561 |           1.1317 |
[32m[20221208 13:50:43 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:50:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 771.77
[32m[20221208 13:50:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 880.70
[32m[20221208 13:50:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 636.77
[32m[20221208 13:50:43 @agent_ppo2.py:137][0m Total time:       5.38 min
[32m[20221208 13:50:43 @agent_ppo2.py:139][0m 446464 total steps have happened
[32m[20221208 13:50:43 @agent_ppo2.py:115][0m #------------------------ Iteration 218 --------------------------#
[32m[20221208 13:50:44 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |           0.0226 |         179.6997 |           1.1830 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |           0.0063 |         159.4887 |           1.1737 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0191 |         148.1779 |           1.1761 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0273 |         140.4248 |           1.1730 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0298 |         135.8779 |           1.1774 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0364 |         131.0702 |           1.1803 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0392 |         127.6588 |           1.1826 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0407 |         124.1567 |           1.1796 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0418 |         119.9184 |           1.1878 |
[32m[20221208 13:50:44 @agent_ppo2.py:179][0m |          -0.0426 |         115.5504 |           1.1864 |
[32m[20221208 13:50:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 760.27
[32m[20221208 13:50:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 853.20
[32m[20221208 13:50:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.16
[32m[20221208 13:50:45 @agent_ppo2.py:137][0m Total time:       5.40 min
[32m[20221208 13:50:45 @agent_ppo2.py:139][0m 448512 total steps have happened
[32m[20221208 13:50:45 @agent_ppo2.py:115][0m #------------------------ Iteration 219 --------------------------#
[32m[20221208 13:50:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:45 @agent_ppo2.py:179][0m |           0.0080 |         220.6574 |           1.1843 |
[32m[20221208 13:50:45 @agent_ppo2.py:179][0m |          -0.0160 |         198.7212 |           1.1809 |
[32m[20221208 13:50:45 @agent_ppo2.py:179][0m |          -0.0260 |         187.7507 |           1.1699 |
[32m[20221208 13:50:45 @agent_ppo2.py:179][0m |          -0.0374 |         181.7639 |           1.1806 |
[32m[20221208 13:50:45 @agent_ppo2.py:179][0m |          -0.0424 |         176.4593 |           1.1830 |
[32m[20221208 13:50:45 @agent_ppo2.py:179][0m |          -0.0469 |         172.7568 |           1.1844 |
[32m[20221208 13:50:45 @agent_ppo2.py:179][0m |          -0.0460 |         170.8764 |           1.1843 |
[32m[20221208 13:50:46 @agent_ppo2.py:179][0m |          -0.0478 |         167.9372 |           1.1879 |
[32m[20221208 13:50:46 @agent_ppo2.py:179][0m |          -0.0505 |         166.2927 |           1.1938 |
[32m[20221208 13:50:46 @agent_ppo2.py:179][0m |          -0.0526 |         164.2890 |           1.1936 |
[32m[20221208 13:50:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 755.86
[32m[20221208 13:50:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 827.25
[32m[20221208 13:50:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 908.93
[32m[20221208 13:50:46 @agent_ppo2.py:137][0m Total time:       5.43 min
[32m[20221208 13:50:46 @agent_ppo2.py:139][0m 450560 total steps have happened
[32m[20221208 13:50:46 @agent_ppo2.py:115][0m #------------------------ Iteration 220 --------------------------#
[32m[20221208 13:50:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |           0.0113 |         154.6898 |           1.2020 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |           0.0096 |         143.8489 |           1.1887 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0135 |         137.7430 |           1.2037 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0213 |         132.3569 |           1.2049 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0232 |         128.8362 |           1.2095 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0260 |         126.3253 |           1.2124 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0288 |         123.7128 |           1.2216 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0333 |         121.2501 |           1.2258 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0348 |         120.3804 |           1.2278 |
[32m[20221208 13:50:47 @agent_ppo2.py:179][0m |          -0.0360 |         119.0515 |           1.2308 |
[32m[20221208 13:50:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 569.83
[32m[20221208 13:50:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 887.93
[32m[20221208 13:50:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 523.64
[32m[20221208 13:50:48 @agent_ppo2.py:137][0m Total time:       5.45 min
[32m[20221208 13:50:48 @agent_ppo2.py:139][0m 452608 total steps have happened
[32m[20221208 13:50:48 @agent_ppo2.py:115][0m #------------------------ Iteration 221 --------------------------#
[32m[20221208 13:50:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:50:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |           0.0121 |         205.3452 |           1.2405 |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |          -0.0177 |         189.7851 |           1.2447 |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |          -0.0290 |         182.5034 |           1.2474 |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |          -0.0375 |         178.5307 |           1.2526 |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |          -0.0388 |         174.5398 |           1.2568 |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |          -0.0394 |         173.4192 |           1.2586 |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |          -0.0473 |         170.7273 |           1.2614 |
[32m[20221208 13:50:48 @agent_ppo2.py:179][0m |          -0.0478 |         167.9043 |           1.2637 |
[32m[20221208 13:50:49 @agent_ppo2.py:179][0m |          -0.0520 |         165.4607 |           1.2661 |
[32m[20221208 13:50:49 @agent_ppo2.py:179][0m |          -0.0526 |         164.6471 |           1.2697 |
[32m[20221208 13:50:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 545.75
[32m[20221208 13:50:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 871.73
[32m[20221208 13:50:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 889.08
[32m[20221208 13:50:49 @agent_ppo2.py:137][0m Total time:       5.48 min
[32m[20221208 13:50:49 @agent_ppo2.py:139][0m 454656 total steps have happened
[32m[20221208 13:50:49 @agent_ppo2.py:115][0m #------------------------ Iteration 222 --------------------------#
[32m[20221208 13:50:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |           0.0175 |         214.2343 |           1.2815 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |           0.0146 |         208.7620 |           1.2838 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |           0.0004 |         205.9470 |           1.2958 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |          -0.0080 |         203.2020 |           1.2850 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |          -0.0186 |         201.8665 |           1.2958 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |          -0.0173 |         201.2019 |           1.2939 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |          -0.0150 |         200.1377 |           1.2896 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |          -0.0252 |         199.4131 |           1.2979 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |          -0.0261 |         197.2285 |           1.2979 |
[32m[20221208 13:50:50 @agent_ppo2.py:179][0m |          -0.0235 |         197.9448 |           1.3023 |
[32m[20221208 13:50:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 878.92
[32m[20221208 13:50:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.83
[32m[20221208 13:50:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.68
[32m[20221208 13:50:50 @agent_ppo2.py:137][0m Total time:       5.50 min
[32m[20221208 13:50:50 @agent_ppo2.py:139][0m 456704 total steps have happened
[32m[20221208 13:50:50 @agent_ppo2.py:115][0m #------------------------ Iteration 223 --------------------------#
[32m[20221208 13:50:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |           0.0071 |         206.0112 |           1.2448 |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |           0.0028 |         194.0911 |           1.2373 |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |          -0.0159 |         188.0820 |           1.2433 |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |          -0.0164 |         184.9900 |           1.2467 |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |          -0.0191 |         182.4822 |           1.2411 |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |          -0.0245 |         180.9982 |           1.2464 |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |          -0.0295 |         177.6819 |           1.2493 |
[32m[20221208 13:50:51 @agent_ppo2.py:179][0m |          -0.0282 |         175.4177 |           1.2469 |
[32m[20221208 13:50:52 @agent_ppo2.py:179][0m |          -0.0298 |         174.1286 |           1.2482 |
[32m[20221208 13:50:52 @agent_ppo2.py:179][0m |          -0.0343 |         172.5059 |           1.2528 |
[32m[20221208 13:50:52 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:50:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 841.86
[32m[20221208 13:50:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 948.19
[32m[20221208 13:50:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 911.82
[32m[20221208 13:50:52 @agent_ppo2.py:137][0m Total time:       5.53 min
[32m[20221208 13:50:52 @agent_ppo2.py:139][0m 458752 total steps have happened
[32m[20221208 13:50:52 @agent_ppo2.py:115][0m #------------------------ Iteration 224 --------------------------#
[32m[20221208 13:50:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |           0.0122 |         211.7334 |           1.2369 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0023 |         202.4865 |           1.2356 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0057 |         202.3440 |           1.2404 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0155 |         199.5619 |           1.2392 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0139 |         199.1094 |           1.2363 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0157 |         198.2508 |           1.2385 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0225 |         198.3810 |           1.2359 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0165 |         198.8508 |           1.2305 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0187 |         197.4745 |           1.2297 |
[32m[20221208 13:50:53 @agent_ppo2.py:179][0m |          -0.0241 |         197.4097 |           1.2356 |
[32m[20221208 13:50:53 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 853.19
[32m[20221208 13:50:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 894.74
[32m[20221208 13:50:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 921.01
[32m[20221208 13:50:53 @agent_ppo2.py:137][0m Total time:       5.55 min
[32m[20221208 13:50:53 @agent_ppo2.py:139][0m 460800 total steps have happened
[32m[20221208 13:50:53 @agent_ppo2.py:115][0m #------------------------ Iteration 225 --------------------------#
[32m[20221208 13:50:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |           0.0090 |         149.3094 |           1.1601 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0078 |         141.1887 |           1.1644 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0199 |         137.1663 |           1.1747 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0216 |         134.5806 |           1.1813 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0255 |         133.5632 |           1.1885 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0290 |         130.1768 |           1.1944 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0352 |         128.8497 |           1.2034 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0358 |         126.5064 |           1.2047 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0332 |         125.5970 |           1.2056 |
[32m[20221208 13:50:54 @agent_ppo2.py:179][0m |          -0.0380 |         123.4527 |           1.2109 |
[32m[20221208 13:50:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:50:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 573.14
[32m[20221208 13:50:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 860.72
[32m[20221208 13:50:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 926.11
[32m[20221208 13:50:55 @agent_ppo2.py:137][0m Total time:       5.58 min
[32m[20221208 13:50:55 @agent_ppo2.py:139][0m 462848 total steps have happened
[32m[20221208 13:50:55 @agent_ppo2.py:115][0m #------------------------ Iteration 226 --------------------------#
[32m[20221208 13:50:55 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:55 @agent_ppo2.py:179][0m |           0.0167 |         143.7156 |           1.2727 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0072 |         132.9443 |           1.2757 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0168 |         126.8398 |           1.2727 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0191 |         122.7738 |           1.2789 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0292 |         118.7991 |           1.2870 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0312 |         116.5303 |           1.2928 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0344 |         113.9242 |           1.2940 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0316 |         111.7375 |           1.2876 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0340 |         110.2362 |           1.2910 |
[32m[20221208 13:50:56 @agent_ppo2.py:179][0m |          -0.0350 |         108.3179 |           1.3004 |
[32m[20221208 13:50:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:50:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 556.73
[32m[20221208 13:50:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 850.05
[32m[20221208 13:50:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 409.46
[32m[20221208 13:50:56 @agent_ppo2.py:137][0m Total time:       5.60 min
[32m[20221208 13:50:56 @agent_ppo2.py:139][0m 464896 total steps have happened
[32m[20221208 13:50:56 @agent_ppo2.py:115][0m #------------------------ Iteration 227 --------------------------#
[32m[20221208 13:50:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:50:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |           0.0110 |         225.8596 |           1.2381 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |           0.0057 |         204.4371 |           1.2211 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0135 |         199.3176 |           1.2333 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0134 |         197.1597 |           1.2333 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0266 |         194.5580 |           1.2406 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0272 |         191.8979 |           1.2391 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0291 |         191.7807 |           1.2412 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0290 |         188.8881 |           1.2387 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0346 |         188.0488 |           1.2426 |
[32m[20221208 13:50:57 @agent_ppo2.py:179][0m |          -0.0367 |         186.0494 |           1.2449 |
[32m[20221208 13:50:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:50:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 858.17
[32m[20221208 13:50:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 914.18
[32m[20221208 13:50:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 907.29
[32m[20221208 13:50:58 @agent_ppo2.py:137][0m Total time:       5.63 min
[32m[20221208 13:50:58 @agent_ppo2.py:139][0m 466944 total steps have happened
[32m[20221208 13:50:58 @agent_ppo2.py:115][0m #------------------------ Iteration 228 --------------------------#
[32m[20221208 13:50:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:50:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:50:58 @agent_ppo2.py:179][0m |           0.0145 |         210.9397 |           1.2810 |
[32m[20221208 13:50:58 @agent_ppo2.py:179][0m |          -0.0023 |         200.9920 |           1.2415 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0144 |         194.3826 |           1.2721 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0241 |         189.8849 |           1.2743 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0303 |         186.7284 |           1.2855 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0340 |         183.8196 |           1.2853 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0343 |         182.8669 |           1.2857 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0347 |         180.3096 |           1.2843 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0415 |         178.5122 |           1.2939 |
[32m[20221208 13:50:59 @agent_ppo2.py:179][0m |          -0.0424 |         177.6815 |           1.2925 |
[32m[20221208 13:50:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:50:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 800.09
[32m[20221208 13:50:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 906.11
[32m[20221208 13:50:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 644.88
[32m[20221208 13:50:59 @agent_ppo2.py:137][0m Total time:       5.65 min
[32m[20221208 13:50:59 @agent_ppo2.py:139][0m 468992 total steps have happened
[32m[20221208 13:50:59 @agent_ppo2.py:115][0m #------------------------ Iteration 229 --------------------------#
[32m[20221208 13:51:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |           0.0044 |         201.3822 |           1.2675 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0041 |         185.9154 |           1.2537 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0194 |         175.7742 |           1.2593 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0263 |         167.0628 |           1.2619 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0300 |         158.5522 |           1.2680 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0295 |         151.1272 |           1.2749 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0367 |         146.7914 |           1.2780 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0416 |         142.9829 |           1.2817 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0382 |         140.2066 |           1.2824 |
[32m[20221208 13:51:00 @agent_ppo2.py:179][0m |          -0.0425 |         138.5997 |           1.2854 |
[32m[20221208 13:51:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 662.78
[32m[20221208 13:51:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 863.50
[32m[20221208 13:51:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.81
[32m[20221208 13:51:01 @agent_ppo2.py:137][0m Total time:       5.67 min
[32m[20221208 13:51:01 @agent_ppo2.py:139][0m 471040 total steps have happened
[32m[20221208 13:51:01 @agent_ppo2.py:115][0m #------------------------ Iteration 230 --------------------------#
[32m[20221208 13:51:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:01 @agent_ppo2.py:179][0m |           0.0099 |         149.2931 |           1.3447 |
[32m[20221208 13:51:01 @agent_ppo2.py:179][0m |          -0.0125 |         131.0009 |           1.3358 |
[32m[20221208 13:51:01 @agent_ppo2.py:179][0m |          -0.0229 |         125.7264 |           1.3366 |
[32m[20221208 13:51:02 @agent_ppo2.py:179][0m |          -0.0269 |         121.2550 |           1.3364 |
[32m[20221208 13:51:02 @agent_ppo2.py:179][0m |          -0.0334 |         118.7561 |           1.3458 |
[32m[20221208 13:51:02 @agent_ppo2.py:179][0m |          -0.0360 |         116.8382 |           1.3459 |
[32m[20221208 13:51:02 @agent_ppo2.py:179][0m |          -0.0356 |         114.7252 |           1.3459 |
[32m[20221208 13:51:02 @agent_ppo2.py:179][0m |          -0.0389 |         113.3447 |           1.3510 |
[32m[20221208 13:51:02 @agent_ppo2.py:179][0m |          -0.0386 |         110.9534 |           1.3538 |
[32m[20221208 13:51:02 @agent_ppo2.py:179][0m |          -0.0416 |         109.1256 |           1.3535 |
[32m[20221208 13:51:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 610.70
[32m[20221208 13:51:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.14
[32m[20221208 13:51:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 893.12
[32m[20221208 13:51:02 @agent_ppo2.py:137][0m Total time:       5.70 min
[32m[20221208 13:51:02 @agent_ppo2.py:139][0m 473088 total steps have happened
[32m[20221208 13:51:02 @agent_ppo2.py:115][0m #------------------------ Iteration 231 --------------------------#
[32m[20221208 13:51:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |           0.0122 |         193.8234 |           1.3235 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0155 |         176.8615 |           1.3161 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0251 |         169.4377 |           1.3181 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0279 |         161.5971 |           1.3211 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0350 |         158.5797 |           1.3230 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0367 |         155.4509 |           1.3229 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0386 |         151.2142 |           1.3224 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0413 |         149.3915 |           1.3259 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0412 |         148.7216 |           1.3306 |
[32m[20221208 13:51:03 @agent_ppo2.py:179][0m |          -0.0431 |         144.1848 |           1.3332 |
[32m[20221208 13:51:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 812.40
[32m[20221208 13:51:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 851.12
[32m[20221208 13:51:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.56
[32m[20221208 13:51:04 @agent_ppo2.py:137][0m Total time:       5.72 min
[32m[20221208 13:51:04 @agent_ppo2.py:139][0m 475136 total steps have happened
[32m[20221208 13:51:04 @agent_ppo2.py:115][0m #------------------------ Iteration 232 --------------------------#
[32m[20221208 13:51:04 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:04 @agent_ppo2.py:179][0m |           0.0213 |         200.6461 |           1.3676 |
[32m[20221208 13:51:04 @agent_ppo2.py:179][0m |          -0.0125 |         173.6867 |           1.3595 |
[32m[20221208 13:51:04 @agent_ppo2.py:179][0m |          -0.0227 |         162.5155 |           1.3616 |
[32m[20221208 13:51:04 @agent_ppo2.py:179][0m |          -0.0276 |         156.7793 |           1.3629 |
[32m[20221208 13:51:05 @agent_ppo2.py:179][0m |          -0.0310 |         151.2516 |           1.3596 |
[32m[20221208 13:51:05 @agent_ppo2.py:179][0m |          -0.0319 |         147.0165 |           1.3574 |
[32m[20221208 13:51:05 @agent_ppo2.py:179][0m |          -0.0359 |         143.8894 |           1.3602 |
[32m[20221208 13:51:05 @agent_ppo2.py:179][0m |          -0.0363 |         141.8336 |           1.3521 |
[32m[20221208 13:51:05 @agent_ppo2.py:179][0m |          -0.0407 |         138.8755 |           1.3554 |
[32m[20221208 13:51:05 @agent_ppo2.py:179][0m |          -0.0440 |         136.9742 |           1.3629 |
[32m[20221208 13:51:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 825.71
[32m[20221208 13:51:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 912.87
[32m[20221208 13:51:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 355.67
[32m[20221208 13:51:05 @agent_ppo2.py:137][0m Total time:       5.75 min
[32m[20221208 13:51:05 @agent_ppo2.py:139][0m 477184 total steps have happened
[32m[20221208 13:51:05 @agent_ppo2.py:115][0m #------------------------ Iteration 233 --------------------------#
[32m[20221208 13:51:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |           0.0266 |         245.7034 |           1.3040 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0028 |         220.5119 |           1.2974 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0258 |         212.5198 |           1.3071 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0339 |         209.2321 |           1.3091 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0394 |         206.7885 |           1.3071 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0441 |         204.0885 |           1.3086 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0480 |         202.6211 |           1.3111 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0510 |         200.7348 |           1.3133 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0534 |         199.3425 |           1.3155 |
[32m[20221208 13:51:06 @agent_ppo2.py:179][0m |          -0.0548 |         197.9429 |           1.3158 |
[32m[20221208 13:51:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 713.59
[32m[20221208 13:51:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 781.42
[32m[20221208 13:51:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 772.87
[32m[20221208 13:51:07 @agent_ppo2.py:137][0m Total time:       5.77 min
[32m[20221208 13:51:07 @agent_ppo2.py:139][0m 479232 total steps have happened
[32m[20221208 13:51:07 @agent_ppo2.py:115][0m #------------------------ Iteration 234 --------------------------#
[32m[20221208 13:51:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:07 @agent_ppo2.py:179][0m |           0.0151 |         187.6896 |           1.3056 |
[32m[20221208 13:51:07 @agent_ppo2.py:179][0m |          -0.0174 |         161.8166 |           1.3021 |
[32m[20221208 13:51:07 @agent_ppo2.py:179][0m |          -0.0364 |         147.8288 |           1.3028 |
[32m[20221208 13:51:07 @agent_ppo2.py:179][0m |          -0.0453 |         138.2584 |           1.3063 |
[32m[20221208 13:51:07 @agent_ppo2.py:179][0m |          -0.0517 |         132.3842 |           1.3083 |
[32m[20221208 13:51:08 @agent_ppo2.py:179][0m |          -0.0561 |         127.6930 |           1.3099 |
[32m[20221208 13:51:08 @agent_ppo2.py:179][0m |          -0.0612 |         123.0137 |           1.3109 |
[32m[20221208 13:51:08 @agent_ppo2.py:179][0m |          -0.0634 |         120.0160 |           1.3126 |
[32m[20221208 13:51:08 @agent_ppo2.py:179][0m |          -0.0628 |         116.2989 |           1.3148 |
[32m[20221208 13:51:08 @agent_ppo2.py:179][0m |          -0.0641 |         113.0326 |           1.3172 |
[32m[20221208 13:51:08 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:51:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 718.24
[32m[20221208 13:51:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 799.30
[32m[20221208 13:51:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 660.39
[32m[20221208 13:51:08 @agent_ppo2.py:137][0m Total time:       5.80 min
[32m[20221208 13:51:08 @agent_ppo2.py:139][0m 481280 total steps have happened
[32m[20221208 13:51:08 @agent_ppo2.py:115][0m #------------------------ Iteration 235 --------------------------#
[32m[20221208 13:51:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |           0.0172 |         222.1990 |           1.3183 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |           0.0007 |         195.5101 |           1.2964 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0225 |         185.6906 |           1.3069 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0295 |         181.0233 |           1.3052 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0343 |         177.3863 |           1.3068 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0404 |         174.2905 |           1.3124 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0365 |         172.2050 |           1.3030 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0395 |         169.9574 |           1.3128 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0412 |         167.2797 |           1.3145 |
[32m[20221208 13:51:09 @agent_ppo2.py:179][0m |          -0.0487 |         165.6527 |           1.3155 |
[32m[20221208 13:51:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 775.41
[32m[20221208 13:51:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 807.67
[32m[20221208 13:51:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.54
[32m[20221208 13:51:10 @agent_ppo2.py:137][0m Total time:       5.82 min
[32m[20221208 13:51:10 @agent_ppo2.py:139][0m 483328 total steps have happened
[32m[20221208 13:51:10 @agent_ppo2.py:115][0m #------------------------ Iteration 236 --------------------------#
[32m[20221208 13:51:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:10 @agent_ppo2.py:179][0m |           0.0182 |         132.5312 |           1.3182 |
[32m[20221208 13:51:10 @agent_ppo2.py:179][0m |          -0.0016 |         114.4808 |           1.3151 |
[32m[20221208 13:51:10 @agent_ppo2.py:179][0m |          -0.0144 |         107.1402 |           1.3259 |
[32m[20221208 13:51:10 @agent_ppo2.py:179][0m |          -0.0174 |         102.3568 |           1.3295 |
[32m[20221208 13:51:10 @agent_ppo2.py:179][0m |          -0.0250 |          99.7227 |           1.3370 |
[32m[20221208 13:51:10 @agent_ppo2.py:179][0m |          -0.0269 |          99.0443 |           1.3376 |
[32m[20221208 13:51:11 @agent_ppo2.py:179][0m |          -0.0261 |          97.8747 |           1.3478 |
[32m[20221208 13:51:11 @agent_ppo2.py:179][0m |          -0.0263 |          97.2519 |           1.3478 |
[32m[20221208 13:51:11 @agent_ppo2.py:179][0m |          -0.0308 |          96.6278 |           1.3527 |
[32m[20221208 13:51:11 @agent_ppo2.py:179][0m |          -0.0319 |          95.4164 |           1.3525 |
[32m[20221208 13:51:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:51:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 620.41
[32m[20221208 13:51:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 932.23
[32m[20221208 13:51:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 661.43
[32m[20221208 13:51:11 @agent_ppo2.py:137][0m Total time:       5.85 min
[32m[20221208 13:51:11 @agent_ppo2.py:139][0m 485376 total steps have happened
[32m[20221208 13:51:11 @agent_ppo2.py:115][0m #------------------------ Iteration 237 --------------------------#
[32m[20221208 13:51:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |           0.0071 |         241.9860 |           1.3331 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0089 |         220.0404 |           1.3118 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0206 |         208.1124 |           1.3252 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0209 |         202.6021 |           1.3029 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0275 |         197.5588 |           1.3193 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0364 |         194.0149 |           1.3244 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0327 |         189.2530 |           1.3198 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0413 |         186.3476 |           1.3286 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0413 |         184.2533 |           1.3254 |
[32m[20221208 13:51:12 @agent_ppo2.py:179][0m |          -0.0463 |         181.2322 |           1.3285 |
[32m[20221208 13:51:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 673.46
[32m[20221208 13:51:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.52
[32m[20221208 13:51:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 646.16
[32m[20221208 13:51:13 @agent_ppo2.py:137][0m Total time:       5.87 min
[32m[20221208 13:51:13 @agent_ppo2.py:139][0m 487424 total steps have happened
[32m[20221208 13:51:13 @agent_ppo2.py:115][0m #------------------------ Iteration 238 --------------------------#
[32m[20221208 13:51:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:13 @agent_ppo2.py:179][0m |           0.0185 |         212.4184 |           1.3037 |
[32m[20221208 13:51:13 @agent_ppo2.py:179][0m |          -0.0037 |         197.9718 |           1.2965 |
[32m[20221208 13:51:13 @agent_ppo2.py:179][0m |          -0.0116 |         192.6104 |           1.3013 |
[32m[20221208 13:51:13 @agent_ppo2.py:179][0m |          -0.0190 |         186.4912 |           1.2997 |
[32m[20221208 13:51:13 @agent_ppo2.py:179][0m |          -0.0261 |         184.9999 |           1.3126 |
[32m[20221208 13:51:13 @agent_ppo2.py:179][0m |          -0.0320 |         181.4391 |           1.3108 |
[32m[20221208 13:51:13 @agent_ppo2.py:179][0m |          -0.0317 |         179.3178 |           1.3121 |
[32m[20221208 13:51:14 @agent_ppo2.py:179][0m |          -0.0327 |         178.4265 |           1.3091 |
[32m[20221208 13:51:14 @agent_ppo2.py:179][0m |          -0.0365 |         176.9402 |           1.3130 |
[32m[20221208 13:51:14 @agent_ppo2.py:179][0m |          -0.0389 |         177.5231 |           1.3172 |
[32m[20221208 13:51:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:51:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 884.73
[32m[20221208 13:51:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 916.66
[32m[20221208 13:51:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.06
[32m[20221208 13:51:14 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 972.06
[32m[20221208 13:51:14 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 972.06
[32m[20221208 13:51:14 @agent_ppo2.py:137][0m Total time:       5.89 min
[32m[20221208 13:51:14 @agent_ppo2.py:139][0m 489472 total steps have happened
[32m[20221208 13:51:14 @agent_ppo2.py:115][0m #------------------------ Iteration 239 --------------------------#
[32m[20221208 13:51:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |           0.0068 |         219.3128 |           1.3358 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0117 |         195.7704 |           1.3303 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0228 |         184.7176 |           1.3385 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0253 |         180.9820 |           1.3380 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0269 |         177.5953 |           1.3443 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0329 |         175.1160 |           1.3453 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0339 |         172.3386 |           1.3490 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0378 |         170.0033 |           1.3511 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0409 |         169.8118 |           1.3517 |
[32m[20221208 13:51:15 @agent_ppo2.py:179][0m |          -0.0422 |         166.4665 |           1.3519 |
[32m[20221208 13:51:15 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:51:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 819.10
[32m[20221208 13:51:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 921.42
[32m[20221208 13:51:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 970.67
[32m[20221208 13:51:15 @agent_ppo2.py:137][0m Total time:       5.92 min
[32m[20221208 13:51:15 @agent_ppo2.py:139][0m 491520 total steps have happened
[32m[20221208 13:51:15 @agent_ppo2.py:115][0m #------------------------ Iteration 240 --------------------------#
[32m[20221208 13:51:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |           0.0059 |         231.9124 |           1.3898 |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |          -0.0001 |         218.9137 |           1.3911 |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |          -0.0076 |         214.0545 |           1.3869 |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |          -0.0092 |         210.5900 |           1.3838 |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |          -0.0125 |         208.5792 |           1.3818 |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |          -0.0082 |         207.3930 |           1.3805 |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |          -0.0232 |         205.5893 |           1.3841 |
[32m[20221208 13:51:16 @agent_ppo2.py:179][0m |          -0.0133 |         204.8578 |           1.3778 |
[32m[20221208 13:51:17 @agent_ppo2.py:179][0m |          -0.0231 |         203.6674 |           1.3836 |
[32m[20221208 13:51:17 @agent_ppo2.py:179][0m |          -0.0282 |         203.5457 |           1.3851 |
[32m[20221208 13:51:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 727.57
[32m[20221208 13:51:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 909.90
[32m[20221208 13:51:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 979.31
[32m[20221208 13:51:17 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 979.31
[32m[20221208 13:51:17 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 979.31
[32m[20221208 13:51:17 @agent_ppo2.py:137][0m Total time:       5.94 min
[32m[20221208 13:51:17 @agent_ppo2.py:139][0m 493568 total steps have happened
[32m[20221208 13:51:17 @agent_ppo2.py:115][0m #------------------------ Iteration 241 --------------------------#
[32m[20221208 13:51:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:51:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |           0.0129 |         223.8354 |           1.3975 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0104 |         209.1931 |           1.3966 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0161 |         205.5077 |           1.4041 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0274 |         202.6814 |           1.4106 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0289 |         201.5213 |           1.4148 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0313 |         199.5233 |           1.4141 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0350 |         197.7125 |           1.4190 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0367 |         196.8904 |           1.4160 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0311 |         195.7343 |           1.4142 |
[32m[20221208 13:51:18 @agent_ppo2.py:179][0m |          -0.0372 |         195.4910 |           1.4244 |
[32m[20221208 13:51:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 868.11
[32m[20221208 13:51:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 920.00
[32m[20221208 13:51:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.18
[32m[20221208 13:51:18 @agent_ppo2.py:137][0m Total time:       5.97 min
[32m[20221208 13:51:18 @agent_ppo2.py:139][0m 495616 total steps have happened
[32m[20221208 13:51:18 @agent_ppo2.py:115][0m #------------------------ Iteration 242 --------------------------#
[32m[20221208 13:51:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |           0.0145 |         227.4297 |           1.3988 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0091 |         204.5574 |           1.3843 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0149 |         187.1047 |           1.3943 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0202 |         169.7534 |           1.3917 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0310 |         158.3338 |           1.4027 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0306 |         148.6140 |           1.4002 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0287 |         142.6042 |           1.3998 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0291 |         134.9179 |           1.4000 |
[32m[20221208 13:51:19 @agent_ppo2.py:179][0m |          -0.0367 |         129.4742 |           1.3935 |
[32m[20221208 13:51:20 @agent_ppo2.py:179][0m |          -0.0397 |         126.5742 |           1.4065 |
[32m[20221208 13:51:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 746.65
[32m[20221208 13:51:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 903.29
[32m[20221208 13:51:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.35
[32m[20221208 13:51:20 @agent_ppo2.py:137][0m Total time:       5.99 min
[32m[20221208 13:51:20 @agent_ppo2.py:139][0m 497664 total steps have happened
[32m[20221208 13:51:20 @agent_ppo2.py:115][0m #------------------------ Iteration 243 --------------------------#
[32m[20221208 13:51:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:20 @agent_ppo2.py:179][0m |           0.0184 |         260.7058 |           1.4155 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0080 |         231.1555 |           1.4137 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0179 |         224.3162 |           1.4192 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0263 |         219.9650 |           1.4197 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0269 |         217.6507 |           1.4112 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0292 |         216.1770 |           1.4154 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0325 |         212.8269 |           1.4147 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0344 |         214.0644 |           1.4143 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0365 |         212.1151 |           1.4135 |
[32m[20221208 13:51:21 @agent_ppo2.py:179][0m |          -0.0364 |         210.2553 |           1.4204 |
[32m[20221208 13:51:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.42
[32m[20221208 13:51:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 929.20
[32m[20221208 13:51:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 670.41
[32m[20221208 13:51:21 @agent_ppo2.py:137][0m Total time:       6.02 min
[32m[20221208 13:51:21 @agent_ppo2.py:139][0m 499712 total steps have happened
[32m[20221208 13:51:21 @agent_ppo2.py:115][0m #------------------------ Iteration 244 --------------------------#
[32m[20221208 13:51:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |           0.0130 |         236.6229 |           1.3643 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0034 |         226.5166 |           1.3505 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0160 |         223.8010 |           1.3540 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0138 |         221.6888 |           1.3497 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0236 |         221.5739 |           1.3464 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0224 |         219.0461 |           1.3325 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0258 |         218.9308 |           1.3490 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0311 |         218.0280 |           1.3444 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0306 |         218.6361 |           1.3479 |
[32m[20221208 13:51:22 @agent_ppo2.py:179][0m |          -0.0297 |         217.4583 |           1.3506 |
[32m[20221208 13:51:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:51:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 859.62
[32m[20221208 13:51:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.45
[32m[20221208 13:51:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.68
[32m[20221208 13:51:23 @agent_ppo2.py:137][0m Total time:       6.04 min
[32m[20221208 13:51:23 @agent_ppo2.py:139][0m 501760 total steps have happened
[32m[20221208 13:51:23 @agent_ppo2.py:115][0m #------------------------ Iteration 245 --------------------------#
[32m[20221208 13:51:23 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:23 @agent_ppo2.py:179][0m |           0.0058 |         195.9306 |           1.3676 |
[32m[20221208 13:51:23 @agent_ppo2.py:179][0m |          -0.0201 |         183.0586 |           1.3640 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0310 |         176.1924 |           1.3651 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0366 |         172.1915 |           1.3677 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0389 |         167.4025 |           1.3657 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0405 |         164.3251 |           1.3687 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0441 |         161.5533 |           1.3692 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0463 |         158.1208 |           1.3758 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0473 |         154.9369 |           1.3725 |
[32m[20221208 13:51:24 @agent_ppo2.py:179][0m |          -0.0482 |         152.3077 |           1.3748 |
[32m[20221208 13:51:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:51:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 860.43
[32m[20221208 13:51:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 916.77
[32m[20221208 13:51:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.09
[32m[20221208 13:51:24 @agent_ppo2.py:137][0m Total time:       6.07 min
[32m[20221208 13:51:24 @agent_ppo2.py:139][0m 503808 total steps have happened
[32m[20221208 13:51:24 @agent_ppo2.py:115][0m #------------------------ Iteration 246 --------------------------#
[32m[20221208 13:51:25 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |           0.0179 |         230.0182 |           1.4421 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |           0.0039 |         220.3672 |           1.4460 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0177 |         213.1152 |           1.4713 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0248 |         205.4883 |           1.4714 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0234 |         198.6197 |           1.4723 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0297 |         191.8117 |           1.4701 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0359 |         188.6435 |           1.4855 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0346 |         184.9507 |           1.4848 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0350 |         183.5503 |           1.4875 |
[32m[20221208 13:51:25 @agent_ppo2.py:179][0m |          -0.0322 |         180.4462 |           1.4779 |
[32m[20221208 13:51:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:51:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 891.23
[32m[20221208 13:51:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 931.83
[32m[20221208 13:51:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 898.65
[32m[20221208 13:51:26 @agent_ppo2.py:137][0m Total time:       6.09 min
[32m[20221208 13:51:26 @agent_ppo2.py:139][0m 505856 total steps have happened
[32m[20221208 13:51:26 @agent_ppo2.py:115][0m #------------------------ Iteration 247 --------------------------#
[32m[20221208 13:51:26 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:26 @agent_ppo2.py:179][0m |           0.0331 |         227.4301 |           1.4335 |
[32m[20221208 13:51:26 @agent_ppo2.py:179][0m |           0.0069 |         204.8714 |           1.4296 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0158 |         198.8284 |           1.4412 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0242 |         194.0629 |           1.4452 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0315 |         190.8959 |           1.4451 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0351 |         188.2553 |           1.4426 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0380 |         186.6299 |           1.4395 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0388 |         187.7631 |           1.4501 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0389 |         183.7909 |           1.4449 |
[32m[20221208 13:51:27 @agent_ppo2.py:179][0m |          -0.0443 |         183.2945 |           1.4459 |
[32m[20221208 13:51:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 761.16
[32m[20221208 13:51:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 948.77
[32m[20221208 13:51:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 792.46
[32m[20221208 13:51:27 @agent_ppo2.py:137][0m Total time:       6.12 min
[32m[20221208 13:51:27 @agent_ppo2.py:139][0m 507904 total steps have happened
[32m[20221208 13:51:27 @agent_ppo2.py:115][0m #------------------------ Iteration 248 --------------------------#
[32m[20221208 13:51:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |           0.0177 |         217.3123 |           1.4275 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |           0.0113 |         209.4554 |           1.3900 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0147 |         205.0598 |           1.4171 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0250 |         201.9329 |           1.4297 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0246 |         199.1066 |           1.4268 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0293 |         197.0083 |           1.4363 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0285 |         196.1749 |           1.4325 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0308 |         192.1129 |           1.4315 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0299 |         190.1442 |           1.4314 |
[32m[20221208 13:51:28 @agent_ppo2.py:179][0m |          -0.0346 |         188.6052 |           1.4312 |
[32m[20221208 13:51:28 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 893.32
[32m[20221208 13:51:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 926.07
[32m[20221208 13:51:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.47
[32m[20221208 13:51:29 @agent_ppo2.py:137][0m Total time:       6.14 min
[32m[20221208 13:51:29 @agent_ppo2.py:139][0m 509952 total steps have happened
[32m[20221208 13:51:29 @agent_ppo2.py:115][0m #------------------------ Iteration 249 --------------------------#
[32m[20221208 13:51:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:29 @agent_ppo2.py:179][0m |           0.0241 |         243.3492 |           1.4465 |
[32m[20221208 13:51:29 @agent_ppo2.py:179][0m |           0.0045 |         236.7511 |           1.4340 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0011 |         235.0971 |           1.4488 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0067 |         231.9015 |           1.4415 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0116 |         231.0615 |           1.4470 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0175 |         229.4315 |           1.4520 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0223 |         228.7656 |           1.4548 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0211 |         228.0533 |           1.4489 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0235 |         227.9281 |           1.4519 |
[32m[20221208 13:51:30 @agent_ppo2.py:179][0m |          -0.0242 |         227.3988 |           1.4543 |
[32m[20221208 13:51:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.59
[32m[20221208 13:51:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.19
[32m[20221208 13:51:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.14
[32m[20221208 13:51:30 @agent_ppo2.py:137][0m Total time:       6.17 min
[32m[20221208 13:51:30 @agent_ppo2.py:139][0m 512000 total steps have happened
[32m[20221208 13:51:30 @agent_ppo2.py:115][0m #------------------------ Iteration 250 --------------------------#
[32m[20221208 13:51:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |           0.0125 |         229.3791 |           1.4507 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0076 |         212.9566 |           1.4518 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0166 |         198.8631 |           1.4464 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0256 |         191.1861 |           1.4551 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0278 |         185.0858 |           1.4626 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0303 |         182.9084 |           1.4625 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0290 |         177.7159 |           1.4605 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0257 |         175.1613 |           1.4590 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0311 |         173.1522 |           1.4628 |
[32m[20221208 13:51:31 @agent_ppo2.py:179][0m |          -0.0365 |         171.4498 |           1.4675 |
[32m[20221208 13:51:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:51:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 859.33
[32m[20221208 13:51:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 943.84
[32m[20221208 13:51:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.00
[32m[20221208 13:51:32 @agent_ppo2.py:137][0m Total time:       6.19 min
[32m[20221208 13:51:32 @agent_ppo2.py:139][0m 514048 total steps have happened
[32m[20221208 13:51:32 @agent_ppo2.py:115][0m #------------------------ Iteration 251 --------------------------#
[32m[20221208 13:51:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:32 @agent_ppo2.py:179][0m |           0.0122 |         241.3250 |           1.4599 |
[32m[20221208 13:51:32 @agent_ppo2.py:179][0m |          -0.0010 |         231.5549 |           1.4487 |
[32m[20221208 13:51:32 @agent_ppo2.py:179][0m |          -0.0104 |         227.1841 |           1.4416 |
[32m[20221208 13:51:33 @agent_ppo2.py:179][0m |          -0.0149 |         225.1771 |           1.4372 |
[32m[20221208 13:51:33 @agent_ppo2.py:179][0m |          -0.0173 |         224.4355 |           1.4473 |
[32m[20221208 13:51:33 @agent_ppo2.py:179][0m |          -0.0215 |         221.7708 |           1.4440 |
[32m[20221208 13:51:33 @agent_ppo2.py:179][0m |          -0.0224 |         221.5777 |           1.4440 |
[32m[20221208 13:51:33 @agent_ppo2.py:179][0m |          -0.0267 |         219.8895 |           1.4482 |
[32m[20221208 13:51:33 @agent_ppo2.py:179][0m |          -0.0250 |         219.5401 |           1.4490 |
[32m[20221208 13:51:33 @agent_ppo2.py:179][0m |          -0.0268 |         219.1826 |           1.4429 |
[32m[20221208 13:51:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 966.90
[32m[20221208 13:51:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 994.56
[32m[20221208 13:51:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 724.39
[32m[20221208 13:51:33 @agent_ppo2.py:137][0m Total time:       6.22 min
[32m[20221208 13:51:33 @agent_ppo2.py:139][0m 516096 total steps have happened
[32m[20221208 13:51:33 @agent_ppo2.py:115][0m #------------------------ Iteration 252 --------------------------#
[32m[20221208 13:51:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |           0.0135 |         238.3575 |           1.4292 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0020 |         224.0076 |           1.4281 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0100 |         217.5139 |           1.4183 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0172 |         215.2078 |           1.4297 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0247 |         214.6180 |           1.4300 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0267 |         214.1790 |           1.4266 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0280 |         211.1573 |           1.4254 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0291 |         212.1889 |           1.4268 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0341 |         210.4381 |           1.4313 |
[32m[20221208 13:51:34 @agent_ppo2.py:179][0m |          -0.0332 |         209.3904 |           1.4233 |
[32m[20221208 13:51:34 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:51:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 737.09
[32m[20221208 13:51:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.29
[32m[20221208 13:51:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 974.12
[32m[20221208 13:51:35 @agent_ppo2.py:137][0m Total time:       6.24 min
[32m[20221208 13:51:35 @agent_ppo2.py:139][0m 518144 total steps have happened
[32m[20221208 13:51:35 @agent_ppo2.py:115][0m #------------------------ Iteration 253 --------------------------#
[32m[20221208 13:51:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:35 @agent_ppo2.py:179][0m |           0.0077 |         236.9422 |           1.4233 |
[32m[20221208 13:51:35 @agent_ppo2.py:179][0m |          -0.0049 |         232.4066 |           1.4180 |
[32m[20221208 13:51:35 @agent_ppo2.py:179][0m |          -0.0145 |         228.2103 |           1.4261 |
[32m[20221208 13:51:35 @agent_ppo2.py:179][0m |          -0.0182 |         226.9116 |           1.4238 |
[32m[20221208 13:51:36 @agent_ppo2.py:179][0m |          -0.0188 |         224.5765 |           1.4260 |
[32m[20221208 13:51:36 @agent_ppo2.py:179][0m |          -0.0178 |         223.5348 |           1.4253 |
[32m[20221208 13:51:36 @agent_ppo2.py:179][0m |          -0.0271 |         221.7583 |           1.4279 |
[32m[20221208 13:51:36 @agent_ppo2.py:179][0m |          -0.0270 |         221.5900 |           1.4347 |
[32m[20221208 13:51:36 @agent_ppo2.py:179][0m |          -0.0226 |         219.8310 |           1.4270 |
[32m[20221208 13:51:36 @agent_ppo2.py:179][0m |          -0.0271 |         220.0757 |           1.4353 |
[32m[20221208 13:51:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:51:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 919.83
[32m[20221208 13:51:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.10
[32m[20221208 13:51:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 970.89
[32m[20221208 13:51:36 @agent_ppo2.py:137][0m Total time:       6.26 min
[32m[20221208 13:51:36 @agent_ppo2.py:139][0m 520192 total steps have happened
[32m[20221208 13:51:36 @agent_ppo2.py:115][0m #------------------------ Iteration 254 --------------------------#
[32m[20221208 13:51:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |           0.0293 |         246.9488 |           1.4122 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |           0.0021 |         241.1660 |           1.4184 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0023 |         236.0347 |           1.4295 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0004 |         236.3205 |           1.4132 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0080 |         232.8802 |           1.4230 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0044 |         231.7476 |           1.4268 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0172 |         231.9404 |           1.4360 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0200 |         230.2589 |           1.4399 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0165 |         230.2608 |           1.4386 |
[32m[20221208 13:51:37 @agent_ppo2.py:179][0m |          -0.0137 |         227.8754 |           1.4366 |
[32m[20221208 13:51:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 895.35
[32m[20221208 13:51:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 920.98
[32m[20221208 13:51:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.02
[32m[20221208 13:51:38 @agent_ppo2.py:137][0m Total time:       6.29 min
[32m[20221208 13:51:38 @agent_ppo2.py:139][0m 522240 total steps have happened
[32m[20221208 13:51:38 @agent_ppo2.py:115][0m #------------------------ Iteration 255 --------------------------#
[32m[20221208 13:51:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:38 @agent_ppo2.py:179][0m |           0.0174 |         236.0337 |           1.4284 |
[32m[20221208 13:51:38 @agent_ppo2.py:179][0m |           0.0171 |         229.7870 |           1.4203 |
[32m[20221208 13:51:38 @agent_ppo2.py:179][0m |          -0.0056 |         228.5227 |           1.4284 |
[32m[20221208 13:51:38 @agent_ppo2.py:179][0m |           0.0014 |         229.3359 |           1.4275 |
[32m[20221208 13:51:39 @agent_ppo2.py:179][0m |          -0.0153 |         226.5338 |           1.4413 |
[32m[20221208 13:51:39 @agent_ppo2.py:179][0m |          -0.0158 |         227.2427 |           1.4407 |
[32m[20221208 13:51:39 @agent_ppo2.py:179][0m |          -0.0176 |         226.2484 |           1.4444 |
[32m[20221208 13:51:39 @agent_ppo2.py:179][0m |          -0.0208 |         226.7572 |           1.4514 |
[32m[20221208 13:51:39 @agent_ppo2.py:179][0m |          -0.0219 |         226.7118 |           1.4518 |
[32m[20221208 13:51:39 @agent_ppo2.py:179][0m |          -0.0172 |         224.8871 |           1.4461 |
[32m[20221208 13:51:39 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:51:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.93
[32m[20221208 13:51:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.60
[32m[20221208 13:51:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.75
[32m[20221208 13:51:39 @agent_ppo2.py:137][0m Total time:       6.31 min
[32m[20221208 13:51:39 @agent_ppo2.py:139][0m 524288 total steps have happened
[32m[20221208 13:51:39 @agent_ppo2.py:115][0m #------------------------ Iteration 256 --------------------------#
[32m[20221208 13:51:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |           0.0112 |         233.6840 |           1.4469 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0007 |         227.9539 |           1.4275 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0138 |         223.9312 |           1.4530 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0168 |         223.6755 |           1.4509 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0189 |         219.9089 |           1.4526 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0213 |         218.1153 |           1.4514 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0211 |         217.0246 |           1.4544 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0243 |         216.1912 |           1.4505 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0275 |         215.7148 |           1.4523 |
[32m[20221208 13:51:40 @agent_ppo2.py:179][0m |          -0.0268 |         215.7729 |           1.4512 |
[32m[20221208 13:51:40 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:51:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 908.38
[32m[20221208 13:51:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.51
[32m[20221208 13:51:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.55
[32m[20221208 13:51:41 @agent_ppo2.py:137][0m Total time:       6.34 min
[32m[20221208 13:51:41 @agent_ppo2.py:139][0m 526336 total steps have happened
[32m[20221208 13:51:41 @agent_ppo2.py:115][0m #------------------------ Iteration 257 --------------------------#
[32m[20221208 13:51:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:41 @agent_ppo2.py:179][0m |           0.0230 |         198.7586 |           1.3775 |
[32m[20221208 13:51:41 @agent_ppo2.py:179][0m |          -0.0091 |         179.0133 |           1.3521 |
[32m[20221208 13:51:41 @agent_ppo2.py:179][0m |          -0.0151 |         173.3839 |           1.3523 |
[32m[20221208 13:51:41 @agent_ppo2.py:179][0m |          -0.0244 |         171.7599 |           1.3582 |
[32m[20221208 13:51:41 @agent_ppo2.py:179][0m |          -0.0308 |         167.2760 |           1.3636 |
[32m[20221208 13:51:42 @agent_ppo2.py:179][0m |          -0.0335 |         165.2326 |           1.3674 |
[32m[20221208 13:51:42 @agent_ppo2.py:179][0m |          -0.0302 |         163.6163 |           1.3507 |
[32m[20221208 13:51:42 @agent_ppo2.py:179][0m |          -0.0343 |         162.7328 |           1.3575 |
[32m[20221208 13:51:42 @agent_ppo2.py:179][0m |          -0.0327 |         159.6826 |           1.3597 |
[32m[20221208 13:51:42 @agent_ppo2.py:179][0m |          -0.0387 |         158.0366 |           1.3657 |
[32m[20221208 13:51:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 890.03
[32m[20221208 13:51:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 978.93
[32m[20221208 13:51:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 964.12
[32m[20221208 13:51:42 @agent_ppo2.py:137][0m Total time:       6.36 min
[32m[20221208 13:51:42 @agent_ppo2.py:139][0m 528384 total steps have happened
[32m[20221208 13:51:42 @agent_ppo2.py:115][0m #------------------------ Iteration 258 --------------------------#
[32m[20221208 13:51:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |           0.0113 |         239.2980 |           1.3787 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |           0.0082 |         238.3705 |           1.3745 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0078 |         236.9080 |           1.3789 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0094 |         236.0162 |           1.3864 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0130 |         235.4064 |           1.3878 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0146 |         235.2311 |           1.3961 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0230 |         234.3926 |           1.3999 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0184 |         234.6001 |           1.3989 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0185 |         234.1231 |           1.3913 |
[32m[20221208 13:51:43 @agent_ppo2.py:179][0m |          -0.0258 |         234.5514 |           1.4029 |
[32m[20221208 13:51:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.13
[32m[20221208 13:51:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.98
[32m[20221208 13:51:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 968.83
[32m[20221208 13:51:44 @agent_ppo2.py:137][0m Total time:       6.39 min
[32m[20221208 13:51:44 @agent_ppo2.py:139][0m 530432 total steps have happened
[32m[20221208 13:51:44 @agent_ppo2.py:115][0m #------------------------ Iteration 259 --------------------------#
[32m[20221208 13:51:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:44 @agent_ppo2.py:179][0m |           0.0108 |         236.0252 |           1.4149 |
[32m[20221208 13:51:44 @agent_ppo2.py:179][0m |          -0.0041 |         235.2858 |           1.4134 |
[32m[20221208 13:51:44 @agent_ppo2.py:179][0m |          -0.0067 |         232.4730 |           1.4108 |
[32m[20221208 13:51:44 @agent_ppo2.py:179][0m |          -0.0038 |         232.7712 |           1.4085 |
[32m[20221208 13:51:44 @agent_ppo2.py:179][0m |          -0.0086 |         233.5767 |           1.3983 |
[32m[20221208 13:51:44 @agent_ppo2.py:179][0m |          -0.0146 |         231.7454 |           1.3976 |
[32m[20221208 13:51:45 @agent_ppo2.py:179][0m |          -0.0097 |         231.6876 |           1.4011 |
[32m[20221208 13:51:45 @agent_ppo2.py:179][0m |          -0.0116 |         230.4673 |           1.3933 |
[32m[20221208 13:51:45 @agent_ppo2.py:179][0m |          -0.0157 |         229.9178 |           1.3969 |
[32m[20221208 13:51:45 @agent_ppo2.py:179][0m |          -0.0153 |         229.6956 |           1.3880 |
[32m[20221208 13:51:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 977.79
[32m[20221208 13:51:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 990.17
[32m[20221208 13:51:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 688.41
[32m[20221208 13:51:45 @agent_ppo2.py:137][0m Total time:       6.41 min
[32m[20221208 13:51:45 @agent_ppo2.py:139][0m 532480 total steps have happened
[32m[20221208 13:51:45 @agent_ppo2.py:115][0m #------------------------ Iteration 260 --------------------------#
[32m[20221208 13:51:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |           0.0291 |         237.6752 |           1.4063 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0060 |         207.8745 |           1.3986 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0145 |         190.3127 |           1.4099 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0246 |         181.0079 |           1.4150 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0305 |         172.3354 |           1.4180 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0321 |         164.3310 |           1.4192 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0318 |         161.4822 |           1.4185 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0358 |         155.7850 |           1.4302 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0355 |         152.8041 |           1.4240 |
[32m[20221208 13:51:46 @agent_ppo2.py:179][0m |          -0.0354 |         148.6063 |           1.4220 |
[32m[20221208 13:51:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 867.96
[32m[20221208 13:51:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.81
[32m[20221208 13:51:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.17
[32m[20221208 13:51:47 @agent_ppo2.py:137][0m Total time:       6.44 min
[32m[20221208 13:51:47 @agent_ppo2.py:139][0m 534528 total steps have happened
[32m[20221208 13:51:47 @agent_ppo2.py:115][0m #------------------------ Iteration 261 --------------------------#
[32m[20221208 13:51:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:51:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:47 @agent_ppo2.py:179][0m |           0.0203 |         260.4501 |           1.4103 |
[32m[20221208 13:51:47 @agent_ppo2.py:179][0m |           0.0042 |         251.9881 |           1.4051 |
[32m[20221208 13:51:47 @agent_ppo2.py:179][0m |          -0.0101 |         249.2345 |           1.4026 |
[32m[20221208 13:51:47 @agent_ppo2.py:179][0m |          -0.0161 |         249.1486 |           1.4122 |
[32m[20221208 13:51:47 @agent_ppo2.py:179][0m |          -0.0157 |         247.1746 |           1.4020 |
[32m[20221208 13:51:47 @agent_ppo2.py:179][0m |          -0.0209 |         246.2923 |           1.3910 |
[32m[20221208 13:51:48 @agent_ppo2.py:179][0m |          -0.0168 |         245.8707 |           1.3959 |
[32m[20221208 13:51:48 @agent_ppo2.py:179][0m |          -0.0206 |         245.4207 |           1.4006 |
[32m[20221208 13:51:48 @agent_ppo2.py:179][0m |          -0.0228 |         244.7145 |           1.3966 |
[32m[20221208 13:51:48 @agent_ppo2.py:179][0m |          -0.0266 |         245.1856 |           1.3992 |
[32m[20221208 13:51:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.78
[32m[20221208 13:51:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.84
[32m[20221208 13:51:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.76
[32m[20221208 13:51:48 @agent_ppo2.py:137][0m Total time:       6.46 min
[32m[20221208 13:51:48 @agent_ppo2.py:139][0m 536576 total steps have happened
[32m[20221208 13:51:48 @agent_ppo2.py:115][0m #------------------------ Iteration 262 --------------------------#
[32m[20221208 13:51:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |           0.0089 |         244.3740 |           1.3546 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0026 |         240.8129 |           1.3504 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0021 |         239.2818 |           1.3339 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0036 |         239.7161 |           1.3451 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0101 |         236.7544 |           1.3409 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0157 |         236.8969 |           1.3483 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0204 |         235.8686 |           1.3413 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0147 |         235.2546 |           1.3320 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0127 |         236.2325 |           1.3295 |
[32m[20221208 13:51:49 @agent_ppo2.py:179][0m |          -0.0075 |         235.1253 |           1.3201 |
[32m[20221208 13:51:49 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:51:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.37
[32m[20221208 13:51:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 989.98
[32m[20221208 13:51:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.61
[32m[20221208 13:51:50 @agent_ppo2.py:137][0m Total time:       6.49 min
[32m[20221208 13:51:50 @agent_ppo2.py:139][0m 538624 total steps have happened
[32m[20221208 13:51:50 @agent_ppo2.py:115][0m #------------------------ Iteration 263 --------------------------#
[32m[20221208 13:51:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:50 @agent_ppo2.py:179][0m |           0.0098 |         238.9788 |           1.3281 |
[32m[20221208 13:51:50 @agent_ppo2.py:179][0m |           0.0026 |         226.8808 |           1.3151 |
[32m[20221208 13:51:50 @agent_ppo2.py:179][0m |          -0.0081 |         224.3610 |           1.3223 |
[32m[20221208 13:51:50 @agent_ppo2.py:179][0m |          -0.0154 |         223.0057 |           1.3218 |
[32m[20221208 13:51:50 @agent_ppo2.py:179][0m |          -0.0191 |         219.8263 |           1.3251 |
[32m[20221208 13:51:50 @agent_ppo2.py:179][0m |          -0.0212 |         219.1733 |           1.3233 |
[32m[20221208 13:51:50 @agent_ppo2.py:179][0m |          -0.0238 |         218.1680 |           1.3255 |
[32m[20221208 13:51:51 @agent_ppo2.py:179][0m |          -0.0247 |         217.2277 |           1.3244 |
[32m[20221208 13:51:51 @agent_ppo2.py:179][0m |          -0.0266 |         214.6738 |           1.3220 |
[32m[20221208 13:51:51 @agent_ppo2.py:179][0m |          -0.0275 |         213.6620 |           1.3221 |
[32m[20221208 13:51:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 845.48
[32m[20221208 13:51:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.22
[32m[20221208 13:51:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 632.02
[32m[20221208 13:51:51 @agent_ppo2.py:137][0m Total time:       6.51 min
[32m[20221208 13:51:51 @agent_ppo2.py:139][0m 540672 total steps have happened
[32m[20221208 13:51:51 @agent_ppo2.py:115][0m #------------------------ Iteration 264 --------------------------#
[32m[20221208 13:51:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:51:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |           0.0058 |         259.5600 |           1.2916 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0061 |         243.1610 |           1.2825 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0174 |         240.1703 |           1.2781 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0239 |         238.7456 |           1.2734 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0244 |         238.2597 |           1.2679 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0244 |         236.9098 |           1.2610 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0290 |         237.6528 |           1.2687 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0339 |         235.2800 |           1.2681 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0372 |         234.8054 |           1.2655 |
[32m[20221208 13:51:52 @agent_ppo2.py:179][0m |          -0.0369 |         234.6062 |           1.2608 |
[32m[20221208 13:51:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 744.83
[32m[20221208 13:51:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 895.21
[32m[20221208 13:51:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.45
[32m[20221208 13:51:53 @agent_ppo2.py:137][0m Total time:       6.54 min
[32m[20221208 13:51:53 @agent_ppo2.py:139][0m 542720 total steps have happened
[32m[20221208 13:51:53 @agent_ppo2.py:115][0m #------------------------ Iteration 265 --------------------------#
[32m[20221208 13:51:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:53 @agent_ppo2.py:179][0m |           0.0087 |         232.8271 |           1.2171 |
[32m[20221208 13:51:53 @agent_ppo2.py:179][0m |           0.0053 |         225.9500 |           1.1999 |
[32m[20221208 13:51:53 @agent_ppo2.py:179][0m |          -0.0145 |         219.5036 |           1.2201 |
[32m[20221208 13:51:53 @agent_ppo2.py:179][0m |          -0.0198 |         213.3401 |           1.2259 |
[32m[20221208 13:51:53 @agent_ppo2.py:179][0m |          -0.0195 |         209.4751 |           1.2253 |
[32m[20221208 13:51:53 @agent_ppo2.py:179][0m |          -0.0215 |         207.3071 |           1.2330 |
[32m[20221208 13:51:53 @agent_ppo2.py:179][0m |          -0.0271 |         205.4819 |           1.2327 |
[32m[20221208 13:51:54 @agent_ppo2.py:179][0m |          -0.0274 |         201.4466 |           1.2300 |
[32m[20221208 13:51:54 @agent_ppo2.py:179][0m |          -0.0294 |         198.8740 |           1.2396 |
[32m[20221208 13:51:54 @agent_ppo2.py:179][0m |          -0.0305 |         195.5293 |           1.2373 |
[32m[20221208 13:51:54 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:51:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 926.11
[32m[20221208 13:51:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 978.98
[32m[20221208 13:51:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.95
[32m[20221208 13:51:54 @agent_ppo2.py:137][0m Total time:       6.56 min
[32m[20221208 13:51:54 @agent_ppo2.py:139][0m 544768 total steps have happened
[32m[20221208 13:51:54 @agent_ppo2.py:115][0m #------------------------ Iteration 266 --------------------------#
[32m[20221208 13:51:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |           0.0101 |         225.3655 |           1.2593 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0145 |         207.9982 |           1.2473 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0193 |         201.2629 |           1.2519 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0348 |         198.3800 |           1.2573 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0333 |         197.2928 |           1.2646 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0344 |         194.3570 |           1.2571 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0379 |         190.4992 |           1.2656 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0390 |         189.9521 |           1.2569 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0425 |         187.3377 |           1.2609 |
[32m[20221208 13:51:55 @agent_ppo2.py:179][0m |          -0.0447 |         185.7863 |           1.2657 |
[32m[20221208 13:51:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:51:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 851.76
[32m[20221208 13:51:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.29
[32m[20221208 13:51:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 975.90
[32m[20221208 13:51:56 @agent_ppo2.py:137][0m Total time:       6.59 min
[32m[20221208 13:51:56 @agent_ppo2.py:139][0m 546816 total steps have happened
[32m[20221208 13:51:56 @agent_ppo2.py:115][0m #------------------------ Iteration 267 --------------------------#
[32m[20221208 13:51:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |           0.0220 |         170.8511 |           1.2464 |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |          -0.0023 |         145.3653 |           1.2404 |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |          -0.0216 |         132.3264 |           1.2587 |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |          -0.0306 |         122.2089 |           1.2580 |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |          -0.0300 |         115.0847 |           1.2642 |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |          -0.0371 |         110.2210 |           1.2659 |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |          -0.0400 |         104.4428 |           1.2658 |
[32m[20221208 13:51:56 @agent_ppo2.py:179][0m |          -0.0420 |          99.8769 |           1.2728 |
[32m[20221208 13:51:57 @agent_ppo2.py:179][0m |          -0.0436 |          96.4596 |           1.2703 |
[32m[20221208 13:51:57 @agent_ppo2.py:179][0m |          -0.0428 |          94.0515 |           1.2717 |
[32m[20221208 13:51:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:51:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.42
[32m[20221208 13:51:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.78
[32m[20221208 13:51:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.44
[32m[20221208 13:51:57 @agent_ppo2.py:137][0m Total time:       6.61 min
[32m[20221208 13:51:57 @agent_ppo2.py:139][0m 548864 total steps have happened
[32m[20221208 13:51:57 @agent_ppo2.py:115][0m #------------------------ Iteration 268 --------------------------#
[32m[20221208 13:51:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |           0.0129 |         337.7108 |           1.3185 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0119 |         304.0126 |           1.3247 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0232 |         288.2038 |           1.3246 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0326 |         276.8651 |           1.3342 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0369 |         270.7232 |           1.3361 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0405 |         263.0392 |           1.3422 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0390 |         259.0571 |           1.3418 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0451 |         255.3573 |           1.3461 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0435 |         252.5721 |           1.3432 |
[32m[20221208 13:51:58 @agent_ppo2.py:179][0m |          -0.0475 |         250.2047 |           1.3549 |
[32m[20221208 13:51:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:51:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 870.83
[32m[20221208 13:51:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.40
[32m[20221208 13:51:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.86
[32m[20221208 13:51:58 @agent_ppo2.py:137][0m Total time:       6.64 min
[32m[20221208 13:51:58 @agent_ppo2.py:139][0m 550912 total steps have happened
[32m[20221208 13:51:58 @agent_ppo2.py:115][0m #------------------------ Iteration 269 --------------------------#
[32m[20221208 13:51:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:51:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |           0.0026 |         257.5337 |           1.3832 |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |          -0.0081 |         252.2425 |           1.3752 |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |          -0.0082 |         249.3894 |           1.3779 |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |          -0.0171 |         246.9674 |           1.3818 |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |          -0.0176 |         246.0029 |           1.3729 |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |          -0.0213 |         244.8469 |           1.3797 |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |          -0.0164 |         245.2064 |           1.3611 |
[32m[20221208 13:51:59 @agent_ppo2.py:179][0m |          -0.0217 |         245.3428 |           1.3735 |
[32m[20221208 13:52:00 @agent_ppo2.py:179][0m |          -0.0255 |         243.0339 |           1.3742 |
[32m[20221208 13:52:00 @agent_ppo2.py:179][0m |          -0.0256 |         242.5592 |           1.3703 |
[32m[20221208 13:52:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 956.29
[32m[20221208 13:52:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.83
[32m[20221208 13:52:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 968.82
[32m[20221208 13:52:00 @agent_ppo2.py:137][0m Total time:       6.66 min
[32m[20221208 13:52:00 @agent_ppo2.py:139][0m 552960 total steps have happened
[32m[20221208 13:52:00 @agent_ppo2.py:115][0m #------------------------ Iteration 270 --------------------------#
[32m[20221208 13:52:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |           0.0107 |         243.2748 |           1.3126 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0063 |         226.3782 |           1.3094 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0149 |         219.4380 |           1.3084 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0192 |         216.4063 |           1.3031 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0187 |         213.7773 |           1.3000 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0242 |         212.2765 |           1.2990 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0246 |         211.1143 |           1.2971 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0242 |         211.0239 |           1.2937 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0295 |         211.0915 |           1.2985 |
[32m[20221208 13:52:01 @agent_ppo2.py:179][0m |          -0.0305 |         209.8301 |           1.2979 |
[32m[20221208 13:52:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 932.83
[32m[20221208 13:52:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 961.41
[32m[20221208 13:52:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 977.03
[32m[20221208 13:52:01 @agent_ppo2.py:137][0m Total time:       6.68 min
[32m[20221208 13:52:01 @agent_ppo2.py:139][0m 555008 total steps have happened
[32m[20221208 13:52:01 @agent_ppo2.py:115][0m #------------------------ Iteration 271 --------------------------#
[32m[20221208 13:52:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |           0.0082 |         249.8661 |           1.2394 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0001 |         243.4322 |           1.2235 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0113 |         242.7702 |           1.2259 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0184 |         237.8262 |           1.2255 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0207 |         236.2255 |           1.2305 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0246 |         236.8531 |           1.2303 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0244 |         235.6919 |           1.2279 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0256 |         234.4336 |           1.2267 |
[32m[20221208 13:52:02 @agent_ppo2.py:179][0m |          -0.0275 |         233.6989 |           1.2239 |
[32m[20221208 13:52:03 @agent_ppo2.py:179][0m |          -0.0283 |         234.6062 |           1.2324 |
[32m[20221208 13:52:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.00
[32m[20221208 13:52:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.48
[32m[20221208 13:52:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 490.15
[32m[20221208 13:52:03 @agent_ppo2.py:137][0m Total time:       6.71 min
[32m[20221208 13:52:03 @agent_ppo2.py:139][0m 557056 total steps have happened
[32m[20221208 13:52:03 @agent_ppo2.py:115][0m #------------------------ Iteration 272 --------------------------#
[32m[20221208 13:52:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:03 @agent_ppo2.py:179][0m |           0.0245 |         254.2884 |           1.2735 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |           0.0044 |         244.9852 |           1.2738 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0059 |         238.8508 |           1.2782 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0120 |         237.1849 |           1.2802 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0069 |         237.1998 |           1.2794 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0177 |         236.4565 |           1.2851 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0206 |         236.0565 |           1.2869 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0202 |         236.4064 |           1.2804 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0213 |         235.7342 |           1.2786 |
[32m[20221208 13:52:04 @agent_ppo2.py:179][0m |          -0.0242 |         235.1775 |           1.2825 |
[32m[20221208 13:52:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 966.11
[32m[20221208 13:52:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.16
[32m[20221208 13:52:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.46
[32m[20221208 13:52:04 @agent_ppo2.py:137][0m Total time:       6.73 min
[32m[20221208 13:52:04 @agent_ppo2.py:139][0m 559104 total steps have happened
[32m[20221208 13:52:04 @agent_ppo2.py:115][0m #------------------------ Iteration 273 --------------------------#
[32m[20221208 13:52:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |           0.0126 |         256.3747 |           1.2238 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |           0.0061 |         250.4281 |           1.2183 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0049 |         246.2984 |           1.2077 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0210 |         245.0938 |           1.2218 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0269 |         242.7004 |           1.2206 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0279 |         241.5323 |           1.2196 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0327 |         240.7873 |           1.2199 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0338 |         240.3140 |           1.2171 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0324 |         238.2177 |           1.2152 |
[32m[20221208 13:52:05 @agent_ppo2.py:179][0m |          -0.0318 |         237.2920 |           1.2133 |
[32m[20221208 13:52:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 873.60
[32m[20221208 13:52:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.75
[32m[20221208 13:52:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 944.35
[32m[20221208 13:52:06 @agent_ppo2.py:137][0m Total time:       6.76 min
[32m[20221208 13:52:06 @agent_ppo2.py:139][0m 561152 total steps have happened
[32m[20221208 13:52:06 @agent_ppo2.py:115][0m #------------------------ Iteration 274 --------------------------#
[32m[20221208 13:52:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:06 @agent_ppo2.py:179][0m |           0.0175 |         215.0119 |           1.1752 |
[32m[20221208 13:52:06 @agent_ppo2.py:179][0m |          -0.0101 |         190.6065 |           1.1488 |
[32m[20221208 13:52:06 @agent_ppo2.py:179][0m |          -0.0201 |         176.6795 |           1.1627 |
[32m[20221208 13:52:07 @agent_ppo2.py:179][0m |          -0.0224 |         169.7622 |           1.1692 |
[32m[20221208 13:52:07 @agent_ppo2.py:179][0m |          -0.0287 |         165.8623 |           1.1736 |
[32m[20221208 13:52:07 @agent_ppo2.py:179][0m |          -0.0292 |         162.9721 |           1.1704 |
[32m[20221208 13:52:07 @agent_ppo2.py:179][0m |          -0.0317 |         162.5955 |           1.1759 |
[32m[20221208 13:52:07 @agent_ppo2.py:179][0m |          -0.0338 |         159.5172 |           1.1783 |
[32m[20221208 13:52:07 @agent_ppo2.py:179][0m |          -0.0337 |         158.4098 |           1.1827 |
[32m[20221208 13:52:07 @agent_ppo2.py:179][0m |          -0.0312 |         157.1765 |           1.1839 |
[32m[20221208 13:52:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.00
[32m[20221208 13:52:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.92
[32m[20221208 13:52:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.29
[32m[20221208 13:52:07 @agent_ppo2.py:137][0m Total time:       6.78 min
[32m[20221208 13:52:07 @agent_ppo2.py:139][0m 563200 total steps have happened
[32m[20221208 13:52:07 @agent_ppo2.py:115][0m #------------------------ Iteration 275 --------------------------#
[32m[20221208 13:52:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |           0.0057 |         265.6660 |           1.2149 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0065 |         242.4604 |           1.2096 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0228 |         235.1368 |           1.2105 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0316 |         230.0942 |           1.2070 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0306 |         226.9349 |           1.2052 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0354 |         224.7583 |           1.1995 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0374 |         223.8709 |           1.2031 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0321 |         221.9439 |           1.1934 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0391 |         221.3245 |           1.2033 |
[32m[20221208 13:52:08 @agent_ppo2.py:179][0m |          -0.0388 |         219.7659 |           1.2005 |
[32m[20221208 13:52:08 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 819.57
[32m[20221208 13:52:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 889.24
[32m[20221208 13:52:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.64
[32m[20221208 13:52:09 @agent_ppo2.py:137][0m Total time:       6.81 min
[32m[20221208 13:52:09 @agent_ppo2.py:139][0m 565248 total steps have happened
[32m[20221208 13:52:09 @agent_ppo2.py:115][0m #------------------------ Iteration 276 --------------------------#
[32m[20221208 13:52:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:09 @agent_ppo2.py:179][0m |           0.0138 |         265.3608 |           1.1547 |
[32m[20221208 13:52:09 @agent_ppo2.py:179][0m |          -0.0150 |         242.2876 |           1.1438 |
[32m[20221208 13:52:09 @agent_ppo2.py:179][0m |          -0.0249 |         235.7828 |           1.1436 |
[32m[20221208 13:52:10 @agent_ppo2.py:179][0m |          -0.0296 |         231.8361 |           1.1407 |
[32m[20221208 13:52:10 @agent_ppo2.py:179][0m |          -0.0353 |         229.3748 |           1.1431 |
[32m[20221208 13:52:10 @agent_ppo2.py:179][0m |          -0.0347 |         228.0302 |           1.1417 |
[32m[20221208 13:52:10 @agent_ppo2.py:179][0m |          -0.0363 |         227.4274 |           1.1386 |
[32m[20221208 13:52:10 @agent_ppo2.py:179][0m |          -0.0410 |         226.9752 |           1.1447 |
[32m[20221208 13:52:10 @agent_ppo2.py:179][0m |          -0.0421 |         226.8001 |           1.1440 |
[32m[20221208 13:52:10 @agent_ppo2.py:179][0m |          -0.0400 |         222.6136 |           1.1361 |
[32m[20221208 13:52:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 792.97
[32m[20221208 13:52:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 851.63
[32m[20221208 13:52:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.60
[32m[20221208 13:52:10 @agent_ppo2.py:137][0m Total time:       6.83 min
[32m[20221208 13:52:10 @agent_ppo2.py:139][0m 567296 total steps have happened
[32m[20221208 13:52:10 @agent_ppo2.py:115][0m #------------------------ Iteration 277 --------------------------#
[32m[20221208 13:52:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |           0.0226 |         231.3720 |           1.0828 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0108 |         216.7851 |           1.0972 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0217 |         209.9976 |           1.1010 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0275 |         207.2104 |           1.1039 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0290 |         204.3216 |           1.1056 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0313 |         201.8586 |           1.1045 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0328 |         200.2715 |           1.1059 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0353 |         198.9628 |           1.1060 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0386 |         198.0124 |           1.1099 |
[32m[20221208 13:52:11 @agent_ppo2.py:179][0m |          -0.0393 |         196.9713 |           1.1094 |
[32m[20221208 13:52:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 906.49
[32m[20221208 13:52:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 926.17
[32m[20221208 13:52:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 928.72
[32m[20221208 13:52:12 @agent_ppo2.py:137][0m Total time:       6.86 min
[32m[20221208 13:52:12 @agent_ppo2.py:139][0m 569344 total steps have happened
[32m[20221208 13:52:12 @agent_ppo2.py:115][0m #------------------------ Iteration 278 --------------------------#
[32m[20221208 13:52:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:12 @agent_ppo2.py:179][0m |           0.0126 |         218.0894 |           1.1077 |
[32m[20221208 13:52:12 @agent_ppo2.py:179][0m |          -0.0102 |         195.3289 |           1.1009 |
[32m[20221208 13:52:12 @agent_ppo2.py:179][0m |          -0.0239 |         184.9214 |           1.1002 |
[32m[20221208 13:52:12 @agent_ppo2.py:179][0m |          -0.0315 |         178.5739 |           1.1007 |
[32m[20221208 13:52:13 @agent_ppo2.py:179][0m |          -0.0337 |         176.1025 |           1.0944 |
[32m[20221208 13:52:13 @agent_ppo2.py:179][0m |          -0.0397 |         170.7289 |           1.0982 |
[32m[20221208 13:52:13 @agent_ppo2.py:179][0m |          -0.0402 |         166.0097 |           1.0996 |
[32m[20221208 13:52:13 @agent_ppo2.py:179][0m |          -0.0457 |         164.1052 |           1.1009 |
[32m[20221208 13:52:13 @agent_ppo2.py:179][0m |          -0.0467 |         162.7036 |           1.1038 |
[32m[20221208 13:52:13 @agent_ppo2.py:179][0m |          -0.0476 |         159.5064 |           1.1063 |
[32m[20221208 13:52:13 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:52:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 835.89
[32m[20221208 13:52:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 857.20
[32m[20221208 13:52:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 975.24
[32m[20221208 13:52:13 @agent_ppo2.py:137][0m Total time:       6.88 min
[32m[20221208 13:52:13 @agent_ppo2.py:139][0m 571392 total steps have happened
[32m[20221208 13:52:13 @agent_ppo2.py:115][0m #------------------------ Iteration 279 --------------------------#
[32m[20221208 13:52:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |           0.0182 |         257.6562 |           1.1051 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |           0.0067 |         242.8542 |           1.0965 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0191 |         233.9278 |           1.1063 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0319 |         229.7991 |           1.1138 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0363 |         225.1808 |           1.1099 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0372 |         223.7867 |           1.1118 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0422 |         221.4421 |           1.1151 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0405 |         219.6618 |           1.1145 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0433 |         218.0622 |           1.1108 |
[32m[20221208 13:52:14 @agent_ppo2.py:179][0m |          -0.0466 |         217.2711 |           1.1141 |
[32m[20221208 13:52:14 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 835.86
[32m[20221208 13:52:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.68
[32m[20221208 13:52:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 734.59
[32m[20221208 13:52:15 @agent_ppo2.py:137][0m Total time:       6.91 min
[32m[20221208 13:52:15 @agent_ppo2.py:139][0m 573440 total steps have happened
[32m[20221208 13:52:15 @agent_ppo2.py:115][0m #------------------------ Iteration 280 --------------------------#
[32m[20221208 13:52:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:15 @agent_ppo2.py:179][0m |           0.0169 |         239.7712 |           1.0653 |
[32m[20221208 13:52:15 @agent_ppo2.py:179][0m |          -0.0109 |         199.6570 |           1.0559 |
[32m[20221208 13:52:15 @agent_ppo2.py:179][0m |          -0.0229 |         179.7048 |           1.0641 |
[32m[20221208 13:52:15 @agent_ppo2.py:179][0m |          -0.0280 |         165.8253 |           1.0591 |
[32m[20221208 13:52:15 @agent_ppo2.py:179][0m |          -0.0327 |         155.6473 |           1.0661 |
[32m[20221208 13:52:16 @agent_ppo2.py:179][0m |          -0.0371 |         149.3685 |           1.0685 |
[32m[20221208 13:52:16 @agent_ppo2.py:179][0m |          -0.0388 |         142.0013 |           1.0679 |
[32m[20221208 13:52:16 @agent_ppo2.py:179][0m |          -0.0394 |         136.9594 |           1.0701 |
[32m[20221208 13:52:16 @agent_ppo2.py:179][0m |          -0.0434 |         132.1425 |           1.0739 |
[32m[20221208 13:52:16 @agent_ppo2.py:179][0m |          -0.0439 |         128.0529 |           1.0777 |
[32m[20221208 13:52:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 871.17
[32m[20221208 13:52:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.07
[32m[20221208 13:52:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.30
[32m[20221208 13:52:16 @agent_ppo2.py:137][0m Total time:       6.93 min
[32m[20221208 13:52:16 @agent_ppo2.py:139][0m 575488 total steps have happened
[32m[20221208 13:52:16 @agent_ppo2.py:115][0m #------------------------ Iteration 281 --------------------------#
[32m[20221208 13:52:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:52:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |           0.0174 |         284.5541 |           1.1220 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0125 |         253.5711 |           1.1035 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0254 |         242.8661 |           1.1121 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0308 |         236.0176 |           1.1237 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0341 |         231.7848 |           1.1232 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0397 |         229.7759 |           1.1287 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0424 |         226.8637 |           1.1243 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0454 |         224.7063 |           1.1285 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0430 |         222.9037 |           1.1278 |
[32m[20221208 13:52:17 @agent_ppo2.py:179][0m |          -0.0435 |         219.3261 |           1.1293 |
[32m[20221208 13:52:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 888.43
[32m[20221208 13:52:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 948.13
[32m[20221208 13:52:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.84
[32m[20221208 13:52:18 @agent_ppo2.py:137][0m Total time:       6.95 min
[32m[20221208 13:52:18 @agent_ppo2.py:139][0m 577536 total steps have happened
[32m[20221208 13:52:18 @agent_ppo2.py:115][0m #------------------------ Iteration 282 --------------------------#
[32m[20221208 13:52:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:18 @agent_ppo2.py:179][0m |           0.0142 |         259.9587 |           1.0951 |
[32m[20221208 13:52:18 @agent_ppo2.py:179][0m |           0.0091 |         247.1424 |           1.0804 |
[32m[20221208 13:52:18 @agent_ppo2.py:179][0m |          -0.0162 |         240.4427 |           1.0946 |
[32m[20221208 13:52:18 @agent_ppo2.py:179][0m |          -0.0214 |         235.7448 |           1.0911 |
[32m[20221208 13:52:18 @agent_ppo2.py:179][0m |          -0.0322 |         232.0298 |           1.0975 |
[32m[20221208 13:52:19 @agent_ppo2.py:179][0m |          -0.0284 |         229.5539 |           1.0961 |
[32m[20221208 13:52:19 @agent_ppo2.py:179][0m |          -0.0334 |         227.0695 |           1.1041 |
[32m[20221208 13:52:19 @agent_ppo2.py:179][0m |          -0.0360 |         225.6598 |           1.1015 |
[32m[20221208 13:52:19 @agent_ppo2.py:179][0m |          -0.0351 |         225.0459 |           1.1043 |
[32m[20221208 13:52:19 @agent_ppo2.py:179][0m |          -0.0370 |         223.4983 |           1.1022 |
[32m[20221208 13:52:19 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.80
[32m[20221208 13:52:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.42
[32m[20221208 13:52:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 928.12
[32m[20221208 13:52:19 @agent_ppo2.py:137][0m Total time:       6.98 min
[32m[20221208 13:52:19 @agent_ppo2.py:139][0m 579584 total steps have happened
[32m[20221208 13:52:19 @agent_ppo2.py:115][0m #------------------------ Iteration 283 --------------------------#
[32m[20221208 13:52:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |           0.0140 |         298.3070 |           1.0984 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0051 |         281.2549 |           1.0981 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0188 |         275.6819 |           1.1088 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0179 |         271.2502 |           1.1061 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0222 |         269.0951 |           1.1149 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0250 |         267.4299 |           1.1128 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0300 |         266.3440 |           1.1167 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0283 |         266.3774 |           1.1156 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0349 |         264.5722 |           1.1184 |
[32m[20221208 13:52:20 @agent_ppo2.py:179][0m |          -0.0350 |         265.5948 |           1.1237 |
[32m[20221208 13:52:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 897.69
[32m[20221208 13:52:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.80
[32m[20221208 13:52:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 985.24
[32m[20221208 13:52:21 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 985.24
[32m[20221208 13:52:21 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 985.24
[32m[20221208 13:52:21 @agent_ppo2.py:137][0m Total time:       7.01 min
[32m[20221208 13:52:21 @agent_ppo2.py:139][0m 581632 total steps have happened
[32m[20221208 13:52:21 @agent_ppo2.py:115][0m #------------------------ Iteration 284 --------------------------#
[32m[20221208 13:52:21 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:21 @agent_ppo2.py:179][0m |           0.0324 |         217.5229 |           1.1595 |
[32m[20221208 13:52:21 @agent_ppo2.py:179][0m |           0.0141 |         200.2712 |           1.1486 |
[32m[20221208 13:52:21 @agent_ppo2.py:179][0m |          -0.0162 |         194.8010 |           1.1649 |
[32m[20221208 13:52:21 @agent_ppo2.py:179][0m |          -0.0198 |         190.5491 |           1.1705 |
[32m[20221208 13:52:21 @agent_ppo2.py:179][0m |          -0.0286 |         188.6264 |           1.1724 |
[32m[20221208 13:52:22 @agent_ppo2.py:179][0m |          -0.0348 |         185.7417 |           1.1758 |
[32m[20221208 13:52:22 @agent_ppo2.py:179][0m |          -0.0371 |         184.0218 |           1.1768 |
[32m[20221208 13:52:22 @agent_ppo2.py:179][0m |          -0.0383 |         182.0026 |           1.1800 |
[32m[20221208 13:52:22 @agent_ppo2.py:179][0m |          -0.0378 |         178.9870 |           1.1807 |
[32m[20221208 13:52:22 @agent_ppo2.py:179][0m |          -0.0440 |         179.9668 |           1.1825 |
[32m[20221208 13:52:22 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 687.99
[32m[20221208 13:52:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.24
[32m[20221208 13:52:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 931.10
[32m[20221208 13:52:22 @agent_ppo2.py:137][0m Total time:       7.03 min
[32m[20221208 13:52:22 @agent_ppo2.py:139][0m 583680 total steps have happened
[32m[20221208 13:52:22 @agent_ppo2.py:115][0m #------------------------ Iteration 285 --------------------------#
[32m[20221208 13:52:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |           0.0051 |         259.5980 |           1.1682 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0049 |         243.9782 |           1.1597 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0189 |         237.1390 |           1.1632 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0238 |         230.8782 |           1.1668 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0315 |         227.3157 |           1.1691 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0332 |         224.1864 |           1.1733 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0336 |         221.7886 |           1.1694 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0354 |         221.1074 |           1.1722 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0376 |         217.9271 |           1.1729 |
[32m[20221208 13:52:23 @agent_ppo2.py:179][0m |          -0.0402 |         216.2170 |           1.1777 |
[32m[20221208 13:52:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.60
[32m[20221208 13:52:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.09
[32m[20221208 13:52:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 660.07
[32m[20221208 13:52:24 @agent_ppo2.py:137][0m Total time:       7.06 min
[32m[20221208 13:52:24 @agent_ppo2.py:139][0m 585728 total steps have happened
[32m[20221208 13:52:24 @agent_ppo2.py:115][0m #------------------------ Iteration 286 --------------------------#
[32m[20221208 13:52:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:24 @agent_ppo2.py:179][0m |           0.0350 |         216.2323 |           1.1565 |
[32m[20221208 13:52:24 @agent_ppo2.py:179][0m |          -0.0115 |         190.0047 |           1.1658 |
[32m[20221208 13:52:24 @agent_ppo2.py:179][0m |          -0.0243 |         183.9845 |           1.1771 |
[32m[20221208 13:52:24 @agent_ppo2.py:179][0m |          -0.0328 |         180.3170 |           1.1776 |
[32m[20221208 13:52:24 @agent_ppo2.py:179][0m |          -0.0368 |         177.4109 |           1.1833 |
[32m[20221208 13:52:25 @agent_ppo2.py:179][0m |          -0.0424 |         175.3524 |           1.1833 |
[32m[20221208 13:52:25 @agent_ppo2.py:179][0m |          -0.0443 |         173.6857 |           1.1863 |
[32m[20221208 13:52:25 @agent_ppo2.py:179][0m |          -0.0442 |         171.5320 |           1.1849 |
[32m[20221208 13:52:25 @agent_ppo2.py:179][0m |          -0.0495 |         170.2300 |           1.1920 |
[32m[20221208 13:52:25 @agent_ppo2.py:179][0m |          -0.0479 |         170.2311 |           1.1846 |
[32m[20221208 13:52:25 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 617.71
[32m[20221208 13:52:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 883.83
[32m[20221208 13:52:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 894.93
[32m[20221208 13:52:25 @agent_ppo2.py:137][0m Total time:       7.08 min
[32m[20221208 13:52:25 @agent_ppo2.py:139][0m 587776 total steps have happened
[32m[20221208 13:52:25 @agent_ppo2.py:115][0m #------------------------ Iteration 287 --------------------------#
[32m[20221208 13:52:26 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |           0.0226 |         184.9554 |           1.1988 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0067 |         162.9702 |           1.1794 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0239 |         156.6705 |           1.1947 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0282 |         148.8673 |           1.1965 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0375 |         144.4389 |           1.2068 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0447 |         139.5945 |           1.2111 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0418 |         134.1003 |           1.2110 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0453 |         128.3906 |           1.2142 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0488 |         123.8648 |           1.2172 |
[32m[20221208 13:52:26 @agent_ppo2.py:179][0m |          -0.0525 |         120.7826 |           1.2162 |
[32m[20221208 13:52:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 431.75
[32m[20221208 13:52:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.76
[32m[20221208 13:52:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.70
[32m[20221208 13:52:27 @agent_ppo2.py:137][0m Total time:       7.10 min
[32m[20221208 13:52:27 @agent_ppo2.py:139][0m 589824 total steps have happened
[32m[20221208 13:52:27 @agent_ppo2.py:115][0m #------------------------ Iteration 288 --------------------------#
[32m[20221208 13:52:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:27 @agent_ppo2.py:179][0m |           0.0036 |         245.7147 |           1.2799 |
[32m[20221208 13:52:27 @agent_ppo2.py:179][0m |          -0.0165 |         232.1304 |           1.2770 |
[32m[20221208 13:52:27 @agent_ppo2.py:179][0m |          -0.0252 |         225.5259 |           1.2810 |
[32m[20221208 13:52:27 @agent_ppo2.py:179][0m |          -0.0289 |         222.8186 |           1.2759 |
[32m[20221208 13:52:27 @agent_ppo2.py:179][0m |          -0.0317 |         219.8103 |           1.2810 |
[32m[20221208 13:52:27 @agent_ppo2.py:179][0m |          -0.0319 |         217.3228 |           1.2724 |
[32m[20221208 13:52:28 @agent_ppo2.py:179][0m |          -0.0390 |         215.6579 |           1.2821 |
[32m[20221208 13:52:28 @agent_ppo2.py:179][0m |          -0.0379 |         214.2284 |           1.2776 |
[32m[20221208 13:52:28 @agent_ppo2.py:179][0m |          -0.0420 |         211.5120 |           1.2772 |
[32m[20221208 13:52:28 @agent_ppo2.py:179][0m |          -0.0412 |         209.3390 |           1.2840 |
[32m[20221208 13:52:28 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 876.95
[32m[20221208 13:52:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 929.34
[32m[20221208 13:52:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 688.74
[32m[20221208 13:52:28 @agent_ppo2.py:137][0m Total time:       7.13 min
[32m[20221208 13:52:28 @agent_ppo2.py:139][0m 591872 total steps have happened
[32m[20221208 13:52:28 @agent_ppo2.py:115][0m #------------------------ Iteration 289 --------------------------#
[32m[20221208 13:52:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |           0.0057 |         193.9092 |           1.2585 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0081 |         163.8967 |           1.2537 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0179 |         153.5320 |           1.2460 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0270 |         149.3958 |           1.2453 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0311 |         148.0529 |           1.2532 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0331 |         146.5082 |           1.2487 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0337 |         145.6346 |           1.2459 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0386 |         143.6750 |           1.2516 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0408 |         143.4703 |           1.2534 |
[32m[20221208 13:52:29 @agent_ppo2.py:179][0m |          -0.0386 |         142.7920 |           1.2529 |
[32m[20221208 13:52:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:52:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 648.86
[32m[20221208 13:52:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.42
[32m[20221208 13:52:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 966.15
[32m[20221208 13:52:30 @agent_ppo2.py:137][0m Total time:       7.16 min
[32m[20221208 13:52:30 @agent_ppo2.py:139][0m 593920 total steps have happened
[32m[20221208 13:52:30 @agent_ppo2.py:115][0m #------------------------ Iteration 290 --------------------------#
[32m[20221208 13:52:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:30 @agent_ppo2.py:179][0m |           0.0098 |         220.0769 |           1.2674 |
[32m[20221208 13:52:30 @agent_ppo2.py:179][0m |          -0.0146 |         204.6320 |           1.2642 |
[32m[20221208 13:52:30 @agent_ppo2.py:179][0m |          -0.0152 |         199.5905 |           1.2644 |
[32m[20221208 13:52:30 @agent_ppo2.py:179][0m |          -0.0260 |         196.3969 |           1.2698 |
[32m[20221208 13:52:30 @agent_ppo2.py:179][0m |          -0.0346 |         194.3077 |           1.2722 |
[32m[20221208 13:52:31 @agent_ppo2.py:179][0m |          -0.0388 |         191.8093 |           1.2758 |
[32m[20221208 13:52:31 @agent_ppo2.py:179][0m |          -0.0365 |         189.8677 |           1.2771 |
[32m[20221208 13:52:31 @agent_ppo2.py:179][0m |          -0.0415 |         186.3007 |           1.2832 |
[32m[20221208 13:52:31 @agent_ppo2.py:179][0m |          -0.0375 |         184.1480 |           1.2881 |
[32m[20221208 13:52:31 @agent_ppo2.py:179][0m |          -0.0428 |         181.5728 |           1.2822 |
[32m[20221208 13:52:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 785.18
[32m[20221208 13:52:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.39
[32m[20221208 13:52:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.52
[32m[20221208 13:52:31 @agent_ppo2.py:137][0m Total time:       7.18 min
[32m[20221208 13:52:31 @agent_ppo2.py:139][0m 595968 total steps have happened
[32m[20221208 13:52:31 @agent_ppo2.py:115][0m #------------------------ Iteration 291 --------------------------#
[32m[20221208 13:52:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |           0.0029 |         240.4445 |           1.3086 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0068 |         227.9766 |           1.2806 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0093 |         225.2461 |           1.2789 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0207 |         223.1324 |           1.2844 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0227 |         221.7708 |           1.2837 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0226 |         220.8822 |           1.2813 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0282 |         221.7227 |           1.2837 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0312 |         220.4788 |           1.2862 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0319 |         219.3402 |           1.2873 |
[32m[20221208 13:52:32 @agent_ppo2.py:179][0m |          -0.0315 |         218.6560 |           1.2893 |
[32m[20221208 13:52:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 955.97
[32m[20221208 13:52:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.66
[32m[20221208 13:52:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.22
[32m[20221208 13:52:33 @agent_ppo2.py:137][0m Total time:       7.21 min
[32m[20221208 13:52:33 @agent_ppo2.py:139][0m 598016 total steps have happened
[32m[20221208 13:52:33 @agent_ppo2.py:115][0m #------------------------ Iteration 292 --------------------------#
[32m[20221208 13:52:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:33 @agent_ppo2.py:179][0m |           0.0139 |         250.2446 |           1.2530 |
[32m[20221208 13:52:33 @agent_ppo2.py:179][0m |          -0.0077 |         240.8843 |           1.2573 |
[32m[20221208 13:52:33 @agent_ppo2.py:179][0m |          -0.0111 |         236.8097 |           1.2454 |
[32m[20221208 13:52:33 @agent_ppo2.py:179][0m |          -0.0151 |         235.2242 |           1.2507 |
[32m[20221208 13:52:33 @agent_ppo2.py:179][0m |          -0.0218 |         233.9981 |           1.2517 |
[32m[20221208 13:52:34 @agent_ppo2.py:179][0m |          -0.0232 |         234.3105 |           1.2462 |
[32m[20221208 13:52:34 @agent_ppo2.py:179][0m |          -0.0262 |         233.1582 |           1.2534 |
[32m[20221208 13:52:34 @agent_ppo2.py:179][0m |          -0.0247 |         233.2160 |           1.2508 |
[32m[20221208 13:52:34 @agent_ppo2.py:179][0m |          -0.0260 |         233.2437 |           1.2520 |
[32m[20221208 13:52:34 @agent_ppo2.py:179][0m |          -0.0264 |         232.5489 |           1.2533 |
[32m[20221208 13:52:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 967.12
[32m[20221208 13:52:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.35
[32m[20221208 13:52:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 659.84
[32m[20221208 13:52:34 @agent_ppo2.py:137][0m Total time:       7.23 min
[32m[20221208 13:52:34 @agent_ppo2.py:139][0m 600064 total steps have happened
[32m[20221208 13:52:34 @agent_ppo2.py:115][0m #------------------------ Iteration 293 --------------------------#
[32m[20221208 13:52:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |           0.0134 |         244.9820 |           1.2127 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |           0.0004 |         229.7858 |           1.2094 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0096 |         219.0342 |           1.2032 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0154 |         213.1173 |           1.2183 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0236 |         211.0057 |           1.2199 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0264 |         206.9651 |           1.2214 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0258 |         203.9191 |           1.2185 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0299 |         202.4292 |           1.2171 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0355 |         199.7384 |           1.2251 |
[32m[20221208 13:52:35 @agent_ppo2.py:179][0m |          -0.0327 |         197.0009 |           1.2234 |
[32m[20221208 13:52:35 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:52:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 843.91
[32m[20221208 13:52:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.78
[32m[20221208 13:52:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 694.53
[32m[20221208 13:52:36 @agent_ppo2.py:137][0m Total time:       7.26 min
[32m[20221208 13:52:36 @agent_ppo2.py:139][0m 602112 total steps have happened
[32m[20221208 13:52:36 @agent_ppo2.py:115][0m #------------------------ Iteration 294 --------------------------#
[32m[20221208 13:52:36 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:52:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:36 @agent_ppo2.py:179][0m |           0.0089 |         252.3486 |           1.2265 |
[32m[20221208 13:52:36 @agent_ppo2.py:179][0m |          -0.0121 |         230.7452 |           1.2273 |
[32m[20221208 13:52:36 @agent_ppo2.py:179][0m |          -0.0213 |         221.3709 |           1.2255 |
[32m[20221208 13:52:36 @agent_ppo2.py:179][0m |          -0.0276 |         214.8889 |           1.2345 |
[32m[20221208 13:52:36 @agent_ppo2.py:179][0m |          -0.0333 |         210.4300 |           1.2412 |
[32m[20221208 13:52:37 @agent_ppo2.py:179][0m |          -0.0316 |         207.8985 |           1.2347 |
[32m[20221208 13:52:37 @agent_ppo2.py:179][0m |          -0.0378 |         207.4622 |           1.2415 |
[32m[20221208 13:52:37 @agent_ppo2.py:179][0m |          -0.0384 |         204.2608 |           1.2453 |
[32m[20221208 13:52:37 @agent_ppo2.py:179][0m |          -0.0393 |         202.3171 |           1.2421 |
[32m[20221208 13:52:37 @agent_ppo2.py:179][0m |          -0.0423 |         200.0035 |           1.2492 |
[32m[20221208 13:52:37 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:52:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.08
[32m[20221208 13:52:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.77
[32m[20221208 13:52:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.73
[32m[20221208 13:52:37 @agent_ppo2.py:137][0m Total time:       7.28 min
[32m[20221208 13:52:37 @agent_ppo2.py:139][0m 604160 total steps have happened
[32m[20221208 13:52:37 @agent_ppo2.py:115][0m #------------------------ Iteration 295 --------------------------#
[32m[20221208 13:52:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |           0.0278 |         241.6383 |           1.2129 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0097 |         220.1508 |           1.2091 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0258 |         205.0754 |           1.2288 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0329 |         195.7951 |           1.2286 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0344 |         190.5407 |           1.2320 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0398 |         185.1960 |           1.2344 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0418 |         181.4757 |           1.2371 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0428 |         178.7896 |           1.2442 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0452 |         176.5351 |           1.2441 |
[32m[20221208 13:52:38 @agent_ppo2.py:179][0m |          -0.0443 |         175.3903 |           1.2449 |
[32m[20221208 13:52:38 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:52:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.78
[32m[20221208 13:52:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 968.08
[32m[20221208 13:52:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.22
[32m[20221208 13:52:39 @agent_ppo2.py:137][0m Total time:       7.31 min
[32m[20221208 13:52:39 @agent_ppo2.py:139][0m 606208 total steps have happened
[32m[20221208 13:52:39 @agent_ppo2.py:115][0m #------------------------ Iteration 296 --------------------------#
[32m[20221208 13:52:39 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:52:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:39 @agent_ppo2.py:179][0m |           0.0227 |         254.8654 |           1.2306 |
[32m[20221208 13:52:39 @agent_ppo2.py:179][0m |          -0.0100 |         240.3580 |           1.2259 |
[32m[20221208 13:52:39 @agent_ppo2.py:179][0m |          -0.0135 |         233.1903 |           1.2357 |
[32m[20221208 13:52:40 @agent_ppo2.py:179][0m |          -0.0190 |         228.7171 |           1.2406 |
[32m[20221208 13:52:40 @agent_ppo2.py:179][0m |          -0.0248 |         226.6377 |           1.2488 |
[32m[20221208 13:52:40 @agent_ppo2.py:179][0m |          -0.0291 |         223.5644 |           1.2456 |
[32m[20221208 13:52:40 @agent_ppo2.py:179][0m |          -0.0255 |         221.2996 |           1.2461 |
[32m[20221208 13:52:40 @agent_ppo2.py:179][0m |          -0.0290 |         219.7770 |           1.2458 |
[32m[20221208 13:52:40 @agent_ppo2.py:179][0m |          -0.0321 |         218.6247 |           1.2546 |
[32m[20221208 13:52:40 @agent_ppo2.py:179][0m |          -0.0324 |         218.3516 |           1.2588 |
[32m[20221208 13:52:40 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:52:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 927.71
[32m[20221208 13:52:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.42
[32m[20221208 13:52:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.22
[32m[20221208 13:52:40 @agent_ppo2.py:137][0m Total time:       7.33 min
[32m[20221208 13:52:40 @agent_ppo2.py:139][0m 608256 total steps have happened
[32m[20221208 13:52:40 @agent_ppo2.py:115][0m #------------------------ Iteration 297 --------------------------#
[32m[20221208 13:52:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |           0.0273 |         264.5518 |           1.2894 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |           0.0029 |         254.8725 |           1.2828 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0123 |         250.7294 |           1.3001 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0165 |         248.2655 |           1.2929 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0106 |         249.1637 |           1.2971 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0188 |         246.7392 |           1.2970 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0263 |         246.4884 |           1.3119 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0309 |         245.7967 |           1.3195 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0291 |         246.9624 |           1.3179 |
[32m[20221208 13:52:41 @agent_ppo2.py:179][0m |          -0.0290 |         244.1640 |           1.3170 |
[32m[20221208 13:52:41 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:52:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 968.07
[32m[20221208 13:52:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.90
[32m[20221208 13:52:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 980.15
[32m[20221208 13:52:42 @agent_ppo2.py:137][0m Total time:       7.36 min
[32m[20221208 13:52:42 @agent_ppo2.py:139][0m 610304 total steps have happened
[32m[20221208 13:52:42 @agent_ppo2.py:115][0m #------------------------ Iteration 298 --------------------------#
[32m[20221208 13:52:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:42 @agent_ppo2.py:179][0m |           0.0263 |         250.5490 |           1.2795 |
[32m[20221208 13:52:42 @agent_ppo2.py:179][0m |           0.0048 |         235.6299 |           1.2635 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0121 |         228.3253 |           1.2801 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0246 |         220.9259 |           1.2852 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0275 |         217.4644 |           1.2890 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0285 |         215.8034 |           1.2888 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0314 |         213.9853 |           1.2935 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0319 |         213.5392 |           1.2911 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0291 |         212.4470 |           1.2953 |
[32m[20221208 13:52:43 @agent_ppo2.py:179][0m |          -0.0328 |         211.9396 |           1.2936 |
[32m[20221208 13:52:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.81
[32m[20221208 13:52:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 961.41
[32m[20221208 13:52:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 669.31
[32m[20221208 13:52:43 @agent_ppo2.py:137][0m Total time:       7.38 min
[32m[20221208 13:52:43 @agent_ppo2.py:139][0m 612352 total steps have happened
[32m[20221208 13:52:43 @agent_ppo2.py:115][0m #------------------------ Iteration 299 --------------------------#
[32m[20221208 13:52:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |           0.0156 |         261.5642 |           1.3366 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0002 |         253.9383 |           1.3219 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0161 |         251.6510 |           1.3430 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0223 |         249.6951 |           1.3429 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0267 |         248.8071 |           1.3428 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0293 |         247.6432 |           1.3446 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0319 |         249.0340 |           1.3531 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0345 |         246.0567 |           1.3461 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0330 |         245.4078 |           1.3515 |
[32m[20221208 13:52:44 @agent_ppo2.py:179][0m |          -0.0355 |         245.3649 |           1.3513 |
[32m[20221208 13:52:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 831.52
[32m[20221208 13:52:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.16
[32m[20221208 13:52:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.32
[32m[20221208 13:52:45 @agent_ppo2.py:137][0m Total time:       7.41 min
[32m[20221208 13:52:45 @agent_ppo2.py:139][0m 614400 total steps have happened
[32m[20221208 13:52:45 @agent_ppo2.py:115][0m #------------------------ Iteration 300 --------------------------#
[32m[20221208 13:52:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:45 @agent_ppo2.py:179][0m |           0.0140 |         255.9525 |           1.2689 |
[32m[20221208 13:52:45 @agent_ppo2.py:179][0m |          -0.0084 |         248.3921 |           1.2679 |
[32m[20221208 13:52:45 @agent_ppo2.py:179][0m |          -0.0176 |         243.6966 |           1.2869 |
[32m[20221208 13:52:46 @agent_ppo2.py:179][0m |          -0.0236 |         240.3125 |           1.2883 |
[32m[20221208 13:52:46 @agent_ppo2.py:179][0m |          -0.0266 |         236.6522 |           1.2914 |
[32m[20221208 13:52:46 @agent_ppo2.py:179][0m |          -0.0249 |         234.3196 |           1.2808 |
[32m[20221208 13:52:46 @agent_ppo2.py:179][0m |          -0.0294 |         233.0484 |           1.2862 |
[32m[20221208 13:52:46 @agent_ppo2.py:179][0m |          -0.0319 |         232.3633 |           1.2986 |
[32m[20221208 13:52:46 @agent_ppo2.py:179][0m |          -0.0328 |         232.2657 |           1.2982 |
[32m[20221208 13:52:46 @agent_ppo2.py:179][0m |          -0.0337 |         232.7909 |           1.2963 |
[32m[20221208 13:52:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 952.95
[32m[20221208 13:52:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.91
[32m[20221208 13:52:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.02
[32m[20221208 13:52:46 @agent_ppo2.py:137][0m Total time:       7.43 min
[32m[20221208 13:52:46 @agent_ppo2.py:139][0m 616448 total steps have happened
[32m[20221208 13:52:46 @agent_ppo2.py:115][0m #------------------------ Iteration 301 --------------------------#
[32m[20221208 13:52:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:52:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |           0.0155 |         243.7232 |           1.2881 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0102 |         225.2552 |           1.2740 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0260 |         212.9828 |           1.2812 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0284 |         204.7704 |           1.2854 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0341 |         197.2128 |           1.2877 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0366 |         191.1376 |           1.2900 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0383 |         186.5611 |           1.2943 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0365 |         183.7330 |           1.2947 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0375 |         178.4601 |           1.2899 |
[32m[20221208 13:52:47 @agent_ppo2.py:179][0m |          -0.0396 |         174.8524 |           1.2997 |
[32m[20221208 13:52:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 902.21
[32m[20221208 13:52:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.82
[32m[20221208 13:52:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.52
[32m[20221208 13:52:48 @agent_ppo2.py:137][0m Total time:       7.46 min
[32m[20221208 13:52:48 @agent_ppo2.py:139][0m 618496 total steps have happened
[32m[20221208 13:52:48 @agent_ppo2.py:115][0m #------------------------ Iteration 302 --------------------------#
[32m[20221208 13:52:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:48 @agent_ppo2.py:179][0m |           0.0191 |         261.5467 |           1.2456 |
[32m[20221208 13:52:48 @agent_ppo2.py:179][0m |          -0.0103 |         251.8601 |           1.2543 |
[32m[20221208 13:52:48 @agent_ppo2.py:179][0m |          -0.0209 |         246.3843 |           1.2595 |
[32m[20221208 13:52:49 @agent_ppo2.py:179][0m |          -0.0244 |         244.2553 |           1.2661 |
[32m[20221208 13:52:49 @agent_ppo2.py:179][0m |          -0.0292 |         241.8929 |           1.2598 |
[32m[20221208 13:52:49 @agent_ppo2.py:179][0m |          -0.0293 |         241.2857 |           1.2637 |
[32m[20221208 13:52:49 @agent_ppo2.py:179][0m |          -0.0326 |         241.2356 |           1.2682 |
[32m[20221208 13:52:49 @agent_ppo2.py:179][0m |          -0.0327 |         240.2804 |           1.2685 |
[32m[20221208 13:52:49 @agent_ppo2.py:179][0m |          -0.0347 |         238.8354 |           1.2713 |
[32m[20221208 13:52:49 @agent_ppo2.py:179][0m |          -0.0363 |         238.9499 |           1.2712 |
[32m[20221208 13:52:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.03
[32m[20221208 13:52:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.48
[32m[20221208 13:52:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 655.57
[32m[20221208 13:52:49 @agent_ppo2.py:137][0m Total time:       7.48 min
[32m[20221208 13:52:49 @agent_ppo2.py:139][0m 620544 total steps have happened
[32m[20221208 13:52:49 @agent_ppo2.py:115][0m #------------------------ Iteration 303 --------------------------#
[32m[20221208 13:52:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |           0.0140 |         254.6317 |           1.2450 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0085 |         249.8350 |           1.2410 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0148 |         247.8760 |           1.2516 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0248 |         245.6370 |           1.2518 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0281 |         244.9056 |           1.2603 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0297 |         243.9701 |           1.2587 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0312 |         242.9633 |           1.2627 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0351 |         242.0063 |           1.2649 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0366 |         243.7761 |           1.2675 |
[32m[20221208 13:52:50 @agent_ppo2.py:179][0m |          -0.0383 |         241.7284 |           1.2708 |
[32m[20221208 13:52:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 900.73
[32m[20221208 13:52:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 937.66
[32m[20221208 13:52:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.03
[32m[20221208 13:52:51 @agent_ppo2.py:137][0m Total time:       7.51 min
[32m[20221208 13:52:51 @agent_ppo2.py:139][0m 622592 total steps have happened
[32m[20221208 13:52:51 @agent_ppo2.py:115][0m #------------------------ Iteration 304 --------------------------#
[32m[20221208 13:52:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:52:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:51 @agent_ppo2.py:179][0m |           0.0181 |         237.9398 |           1.3413 |
[32m[20221208 13:52:51 @agent_ppo2.py:179][0m |          -0.0014 |         233.6681 |           1.3252 |
[32m[20221208 13:52:51 @agent_ppo2.py:179][0m |          -0.0136 |         231.8049 |           1.3502 |
[32m[20221208 13:52:51 @agent_ppo2.py:179][0m |          -0.0125 |         231.8595 |           1.3567 |
[32m[20221208 13:52:52 @agent_ppo2.py:179][0m |          -0.0164 |         230.6537 |           1.3580 |
[32m[20221208 13:52:52 @agent_ppo2.py:179][0m |          -0.0204 |         229.1536 |           1.3615 |
[32m[20221208 13:52:52 @agent_ppo2.py:179][0m |          -0.0190 |         228.4269 |           1.3609 |
[32m[20221208 13:52:52 @agent_ppo2.py:179][0m |          -0.0279 |         227.7259 |           1.3628 |
[32m[20221208 13:52:52 @agent_ppo2.py:179][0m |          -0.0203 |         227.4928 |           1.3619 |
[32m[20221208 13:52:52 @agent_ppo2.py:179][0m |          -0.0255 |         226.8037 |           1.3607 |
[32m[20221208 13:52:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:52:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 969.23
[32m[20221208 13:52:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.78
[32m[20221208 13:52:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 765.41
[32m[20221208 13:52:52 @agent_ppo2.py:137][0m Total time:       7.53 min
[32m[20221208 13:52:52 @agent_ppo2.py:139][0m 624640 total steps have happened
[32m[20221208 13:52:52 @agent_ppo2.py:115][0m #------------------------ Iteration 305 --------------------------#
[32m[20221208 13:52:53 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |           0.0130 |         255.7474 |           1.3117 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0127 |         241.8059 |           1.3078 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0227 |         235.6522 |           1.3135 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0301 |         232.7614 |           1.3167 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0328 |         230.1872 |           1.3200 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0368 |         227.6961 |           1.3201 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0416 |         226.8149 |           1.3284 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0446 |         224.2568 |           1.3294 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0452 |         223.0251 |           1.3303 |
[32m[20221208 13:52:53 @agent_ppo2.py:179][0m |          -0.0453 |         221.3840 |           1.3339 |
[32m[20221208 13:52:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 708.75
[32m[20221208 13:52:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 904.47
[32m[20221208 13:52:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.63
[32m[20221208 13:52:54 @agent_ppo2.py:137][0m Total time:       7.56 min
[32m[20221208 13:52:54 @agent_ppo2.py:139][0m 626688 total steps have happened
[32m[20221208 13:52:54 @agent_ppo2.py:115][0m #------------------------ Iteration 306 --------------------------#
[32m[20221208 13:52:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:54 @agent_ppo2.py:179][0m |           0.0108 |         247.5516 |           1.3166 |
[32m[20221208 13:52:54 @agent_ppo2.py:179][0m |          -0.0001 |         242.6127 |           1.3060 |
[32m[20221208 13:52:54 @agent_ppo2.py:179][0m |          -0.0185 |         241.9944 |           1.3299 |
[32m[20221208 13:52:54 @agent_ppo2.py:179][0m |          -0.0240 |         241.7646 |           1.3287 |
[32m[20221208 13:52:54 @agent_ppo2.py:179][0m |          -0.0264 |         240.3759 |           1.3360 |
[32m[20221208 13:52:55 @agent_ppo2.py:179][0m |          -0.0312 |         239.2324 |           1.3326 |
[32m[20221208 13:52:55 @agent_ppo2.py:179][0m |          -0.0319 |         240.3171 |           1.3356 |
[32m[20221208 13:52:55 @agent_ppo2.py:179][0m |          -0.0301 |         240.4264 |           1.3356 |
[32m[20221208 13:52:55 @agent_ppo2.py:179][0m |          -0.0366 |         237.8598 |           1.3377 |
[32m[20221208 13:52:55 @agent_ppo2.py:179][0m |          -0.0382 |         237.3336 |           1.3479 |
[32m[20221208 13:52:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 901.55
[32m[20221208 13:52:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.45
[32m[20221208 13:52:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 662.62
[32m[20221208 13:52:55 @agent_ppo2.py:137][0m Total time:       7.58 min
[32m[20221208 13:52:55 @agent_ppo2.py:139][0m 628736 total steps have happened
[32m[20221208 13:52:55 @agent_ppo2.py:115][0m #------------------------ Iteration 307 --------------------------#
[32m[20221208 13:52:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |           0.0084 |         236.4865 |           1.3004 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0104 |         226.7633 |           1.3096 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0194 |         219.3830 |           1.3110 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0268 |         213.2519 |           1.3133 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0252 |         208.3868 |           1.3123 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0305 |         205.5064 |           1.3177 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0332 |         202.4117 |           1.3124 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0351 |         200.5823 |           1.3230 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0361 |         197.5468 |           1.3218 |
[32m[20221208 13:52:56 @agent_ppo2.py:179][0m |          -0.0368 |         197.2865 |           1.3231 |
[32m[20221208 13:52:56 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:52:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 927.72
[32m[20221208 13:52:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 977.92
[32m[20221208 13:52:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 649.05
[32m[20221208 13:52:57 @agent_ppo2.py:137][0m Total time:       7.60 min
[32m[20221208 13:52:57 @agent_ppo2.py:139][0m 630784 total steps have happened
[32m[20221208 13:52:57 @agent_ppo2.py:115][0m #------------------------ Iteration 308 --------------------------#
[32m[20221208 13:52:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:57 @agent_ppo2.py:179][0m |           0.0101 |         249.4387 |           1.3236 |
[32m[20221208 13:52:57 @agent_ppo2.py:179][0m |          -0.0141 |         232.1229 |           1.3061 |
[32m[20221208 13:52:57 @agent_ppo2.py:179][0m |          -0.0270 |         220.0059 |           1.3135 |
[32m[20221208 13:52:57 @agent_ppo2.py:179][0m |          -0.0290 |         209.5747 |           1.3072 |
[32m[20221208 13:52:57 @agent_ppo2.py:179][0m |          -0.0378 |         204.1192 |           1.3175 |
[32m[20221208 13:52:57 @agent_ppo2.py:179][0m |          -0.0400 |         197.3648 |           1.3140 |
[32m[20221208 13:52:58 @agent_ppo2.py:179][0m |          -0.0434 |         190.3954 |           1.3122 |
[32m[20221208 13:52:58 @agent_ppo2.py:179][0m |          -0.0447 |         185.7773 |           1.3141 |
[32m[20221208 13:52:58 @agent_ppo2.py:179][0m |          -0.0441 |         179.7087 |           1.3146 |
[32m[20221208 13:52:58 @agent_ppo2.py:179][0m |          -0.0481 |         175.3918 |           1.3165 |
[32m[20221208 13:52:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:52:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 772.56
[32m[20221208 13:52:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.27
[32m[20221208 13:52:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 666.57
[32m[20221208 13:52:58 @agent_ppo2.py:137][0m Total time:       7.63 min
[32m[20221208 13:52:58 @agent_ppo2.py:139][0m 632832 total steps have happened
[32m[20221208 13:52:58 @agent_ppo2.py:115][0m #------------------------ Iteration 309 --------------------------#
[32m[20221208 13:52:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:52:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |           0.0170 |         290.8875 |           1.3692 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0100 |         257.2432 |           1.3575 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0257 |         248.1788 |           1.3526 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0395 |         243.0024 |           1.3583 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0417 |         239.2022 |           1.3645 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0457 |         237.2687 |           1.3666 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0481 |         235.2280 |           1.3649 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0511 |         235.2553 |           1.3658 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0542 |         233.3500 |           1.3622 |
[32m[20221208 13:52:59 @agent_ppo2.py:179][0m |          -0.0565 |         231.4510 |           1.3682 |
[32m[20221208 13:52:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 840.56
[32m[20221208 13:53:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 880.18
[32m[20221208 13:53:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.55
[32m[20221208 13:53:00 @agent_ppo2.py:137][0m Total time:       7.65 min
[32m[20221208 13:53:00 @agent_ppo2.py:139][0m 634880 total steps have happened
[32m[20221208 13:53:00 @agent_ppo2.py:115][0m #------------------------ Iteration 310 --------------------------#
[32m[20221208 13:53:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:00 @agent_ppo2.py:179][0m |           0.0181 |         245.6838 |           1.3148 |
[32m[20221208 13:53:00 @agent_ppo2.py:179][0m |          -0.0084 |         235.3767 |           1.3049 |
[32m[20221208 13:53:00 @agent_ppo2.py:179][0m |          -0.0187 |         230.5093 |           1.3113 |
[32m[20221208 13:53:00 @agent_ppo2.py:179][0m |          -0.0260 |         228.7620 |           1.3210 |
[32m[20221208 13:53:00 @agent_ppo2.py:179][0m |          -0.0303 |         226.8340 |           1.3299 |
[32m[20221208 13:53:00 @agent_ppo2.py:179][0m |          -0.0343 |         226.0431 |           1.3333 |
[32m[20221208 13:53:01 @agent_ppo2.py:179][0m |          -0.0369 |         225.0108 |           1.3329 |
[32m[20221208 13:53:01 @agent_ppo2.py:179][0m |          -0.0370 |         225.2093 |           1.3387 |
[32m[20221208 13:53:01 @agent_ppo2.py:179][0m |          -0.0381 |         223.8100 |           1.3368 |
[32m[20221208 13:53:01 @agent_ppo2.py:179][0m |          -0.0387 |         223.2380 |           1.3408 |
[32m[20221208 13:53:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.77
[32m[20221208 13:53:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.99
[32m[20221208 13:53:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 976.08
[32m[20221208 13:53:01 @agent_ppo2.py:137][0m Total time:       7.68 min
[32m[20221208 13:53:01 @agent_ppo2.py:139][0m 636928 total steps have happened
[32m[20221208 13:53:01 @agent_ppo2.py:115][0m #------------------------ Iteration 311 --------------------------#
[32m[20221208 13:53:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |           0.0233 |         245.5431 |           1.3602 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |           0.0016 |         240.0240 |           1.3454 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0159 |         235.6791 |           1.3589 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0234 |         232.4069 |           1.3708 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0263 |         231.5382 |           1.3650 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0305 |         228.7947 |           1.3670 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0332 |         226.4491 |           1.3711 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0382 |         225.5373 |           1.3756 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0394 |         224.9727 |           1.3725 |
[32m[20221208 13:53:02 @agent_ppo2.py:179][0m |          -0.0355 |         224.5265 |           1.3721 |
[32m[20221208 13:53:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 855.49
[32m[20221208 13:53:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 904.44
[32m[20221208 13:53:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.06
[32m[20221208 13:53:03 @agent_ppo2.py:137][0m Total time:       7.70 min
[32m[20221208 13:53:03 @agent_ppo2.py:139][0m 638976 total steps have happened
[32m[20221208 13:53:03 @agent_ppo2.py:115][0m #------------------------ Iteration 312 --------------------------#
[32m[20221208 13:53:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:03 @agent_ppo2.py:179][0m |           0.0147 |         232.3928 |           1.2981 |
[32m[20221208 13:53:03 @agent_ppo2.py:179][0m |          -0.0147 |         219.0465 |           1.2888 |
[32m[20221208 13:53:03 @agent_ppo2.py:179][0m |          -0.0228 |         210.0964 |           1.2878 |
[32m[20221208 13:53:03 @agent_ppo2.py:179][0m |          -0.0305 |         205.3270 |           1.2916 |
[32m[20221208 13:53:03 @agent_ppo2.py:179][0m |          -0.0319 |         201.1212 |           1.2906 |
[32m[20221208 13:53:03 @agent_ppo2.py:179][0m |          -0.0336 |         199.9265 |           1.2933 |
[32m[20221208 13:53:03 @agent_ppo2.py:179][0m |          -0.0379 |         194.8403 |           1.2942 |
[32m[20221208 13:53:04 @agent_ppo2.py:179][0m |          -0.0392 |         190.8426 |           1.2977 |
[32m[20221208 13:53:04 @agent_ppo2.py:179][0m |          -0.0404 |         189.2228 |           1.2999 |
[32m[20221208 13:53:04 @agent_ppo2.py:179][0m |          -0.0421 |         186.0083 |           1.2980 |
[32m[20221208 13:53:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 908.03
[32m[20221208 13:53:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 927.63
[32m[20221208 13:53:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.84
[32m[20221208 13:53:04 @agent_ppo2.py:137][0m Total time:       7.73 min
[32m[20221208 13:53:04 @agent_ppo2.py:139][0m 641024 total steps have happened
[32m[20221208 13:53:04 @agent_ppo2.py:115][0m #------------------------ Iteration 313 --------------------------#
[32m[20221208 13:53:04 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |           0.0087 |         229.4776 |           1.2970 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0146 |         211.7768 |           1.2756 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0269 |         203.8616 |           1.2824 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0319 |         195.2538 |           1.2871 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0354 |         189.3270 |           1.2913 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0375 |         186.1314 |           1.2959 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0400 |         182.8567 |           1.2941 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0445 |         180.5070 |           1.3025 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0435 |         177.7333 |           1.3012 |
[32m[20221208 13:53:05 @agent_ppo2.py:179][0m |          -0.0463 |         176.2955 |           1.3068 |
[32m[20221208 13:53:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 827.13
[32m[20221208 13:53:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 901.58
[32m[20221208 13:53:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.46
[32m[20221208 13:53:05 @agent_ppo2.py:137][0m Total time:       7.75 min
[32m[20221208 13:53:05 @agent_ppo2.py:139][0m 643072 total steps have happened
[32m[20221208 13:53:05 @agent_ppo2.py:115][0m #------------------------ Iteration 314 --------------------------#
[32m[20221208 13:53:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |           0.0101 |         244.1556 |           1.3430 |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |          -0.0040 |         232.7294 |           1.3213 |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |          -0.0238 |         230.6544 |           1.3396 |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |          -0.0288 |         226.6160 |           1.3408 |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |          -0.0362 |         224.5555 |           1.3408 |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |          -0.0400 |         223.2494 |           1.3465 |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |          -0.0424 |         223.2338 |           1.3471 |
[32m[20221208 13:53:06 @agent_ppo2.py:179][0m |          -0.0451 |         223.0644 |           1.3501 |
[32m[20221208 13:53:07 @agent_ppo2.py:179][0m |          -0.0469 |         220.8925 |           1.3503 |
[32m[20221208 13:53:07 @agent_ppo2.py:179][0m |          -0.0511 |         220.1634 |           1.3512 |
[32m[20221208 13:53:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 807.68
[32m[20221208 13:53:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.27
[32m[20221208 13:53:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.00
[32m[20221208 13:53:07 @agent_ppo2.py:137][0m Total time:       7.78 min
[32m[20221208 13:53:07 @agent_ppo2.py:139][0m 645120 total steps have happened
[32m[20221208 13:53:07 @agent_ppo2.py:115][0m #------------------------ Iteration 315 --------------------------#
[32m[20221208 13:53:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |           0.0254 |         200.3157 |           1.3589 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |           0.0224 |         186.1068 |           1.3102 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0157 |         183.0431 |           1.3467 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0216 |         181.1715 |           1.3542 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0247 |         180.7520 |           1.3481 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0334 |         178.4966 |           1.3595 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0349 |         177.5174 |           1.3649 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0370 |         176.7244 |           1.3639 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0371 |         176.2718 |           1.3593 |
[32m[20221208 13:53:08 @agent_ppo2.py:179][0m |          -0.0449 |         175.3025 |           1.3723 |
[32m[20221208 13:53:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 673.58
[32m[20221208 13:53:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.36
[32m[20221208 13:53:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.15
[32m[20221208 13:53:08 @agent_ppo2.py:137][0m Total time:       7.80 min
[32m[20221208 13:53:08 @agent_ppo2.py:139][0m 647168 total steps have happened
[32m[20221208 13:53:08 @agent_ppo2.py:115][0m #------------------------ Iteration 316 --------------------------#
[32m[20221208 13:53:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |           0.0275 |         185.2347 |           1.3621 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0102 |         151.4687 |           1.3620 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0257 |         136.2794 |           1.3651 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0268 |         129.7619 |           1.3644 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0352 |         125.5398 |           1.3691 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0375 |         122.1075 |           1.3586 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0417 |         120.3051 |           1.3675 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0404 |         118.7776 |           1.3713 |
[32m[20221208 13:53:09 @agent_ppo2.py:179][0m |          -0.0427 |         116.8940 |           1.3672 |
[32m[20221208 13:53:10 @agent_ppo2.py:179][0m |          -0.0455 |         115.6872 |           1.3735 |
[32m[20221208 13:53:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 658.53
[32m[20221208 13:53:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.33
[32m[20221208 13:53:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.47
[32m[20221208 13:53:10 @agent_ppo2.py:137][0m Total time:       7.83 min
[32m[20221208 13:53:10 @agent_ppo2.py:139][0m 649216 total steps have happened
[32m[20221208 13:53:10 @agent_ppo2.py:115][0m #------------------------ Iteration 317 --------------------------#
[32m[20221208 13:53:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:10 @agent_ppo2.py:179][0m |           0.0128 |         236.4546 |           1.3382 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0051 |         221.2618 |           1.3202 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0228 |         213.3231 |           1.3491 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0304 |         208.7694 |           1.3565 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0345 |         203.4710 |           1.3610 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0369 |         201.6132 |           1.3644 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0397 |         196.7827 |           1.3714 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0413 |         194.2037 |           1.3726 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0439 |         190.8219 |           1.3768 |
[32m[20221208 13:53:11 @agent_ppo2.py:179][0m |          -0.0464 |         187.6676 |           1.3823 |
[32m[20221208 13:53:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.55
[32m[20221208 13:53:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.48
[32m[20221208 13:53:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.41
[32m[20221208 13:53:11 @agent_ppo2.py:137][0m Total time:       7.85 min
[32m[20221208 13:53:11 @agent_ppo2.py:139][0m 651264 total steps have happened
[32m[20221208 13:53:11 @agent_ppo2.py:115][0m #------------------------ Iteration 318 --------------------------#
[32m[20221208 13:53:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |           0.0172 |         193.2325 |           1.3980 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0072 |         179.7139 |           1.4008 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0286 |         173.6444 |           1.3991 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0259 |         169.4477 |           1.4018 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0346 |         168.0259 |           1.4094 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0392 |         164.4178 |           1.4108 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0403 |         164.4434 |           1.4148 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0428 |         162.1354 |           1.4166 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0438 |         161.3914 |           1.4170 |
[32m[20221208 13:53:12 @agent_ppo2.py:179][0m |          -0.0455 |         160.4868 |           1.4254 |
[32m[20221208 13:53:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 686.57
[32m[20221208 13:53:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.81
[32m[20221208 13:53:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 921.93
[32m[20221208 13:53:13 @agent_ppo2.py:137][0m Total time:       7.88 min
[32m[20221208 13:53:13 @agent_ppo2.py:139][0m 653312 total steps have happened
[32m[20221208 13:53:13 @agent_ppo2.py:115][0m #------------------------ Iteration 319 --------------------------#
[32m[20221208 13:53:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:13 @agent_ppo2.py:179][0m |           0.0173 |         237.0022 |           1.5037 |
[32m[20221208 13:53:13 @agent_ppo2.py:179][0m |          -0.0003 |         227.3032 |           1.4707 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0169 |         227.1311 |           1.4862 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0241 |         224.5270 |           1.4872 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0281 |         223.2139 |           1.4911 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0297 |         223.5780 |           1.4948 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0343 |         220.8364 |           1.4961 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0333 |         220.3109 |           1.4954 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0343 |         219.1557 |           1.4977 |
[32m[20221208 13:53:14 @agent_ppo2.py:179][0m |          -0.0352 |         218.3242 |           1.5006 |
[32m[20221208 13:53:14 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.16
[32m[20221208 13:53:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.34
[32m[20221208 13:53:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.23
[32m[20221208 13:53:14 @agent_ppo2.py:137][0m Total time:       7.90 min
[32m[20221208 13:53:14 @agent_ppo2.py:139][0m 655360 total steps have happened
[32m[20221208 13:53:14 @agent_ppo2.py:115][0m #------------------------ Iteration 320 --------------------------#
[32m[20221208 13:53:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |           0.0219 |         228.9273 |           1.4027 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0089 |         220.1300 |           1.4109 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0130 |         215.9144 |           1.4106 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0212 |         213.8192 |           1.4088 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0240 |         211.3025 |           1.4136 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0287 |         208.8798 |           1.4237 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0308 |         207.6786 |           1.4253 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0340 |         206.7819 |           1.4271 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0344 |         207.3496 |           1.4332 |
[32m[20221208 13:53:15 @agent_ppo2.py:179][0m |          -0.0369 |         205.0727 |           1.4326 |
[32m[20221208 13:53:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.37
[32m[20221208 13:53:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.56
[32m[20221208 13:53:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 917.74
[32m[20221208 13:53:16 @agent_ppo2.py:137][0m Total time:       7.92 min
[32m[20221208 13:53:16 @agent_ppo2.py:139][0m 657408 total steps have happened
[32m[20221208 13:53:16 @agent_ppo2.py:115][0m #------------------------ Iteration 321 --------------------------#
[32m[20221208 13:53:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:53:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:16 @agent_ppo2.py:179][0m |           0.0177 |         229.5533 |           1.4058 |
[32m[20221208 13:53:16 @agent_ppo2.py:179][0m |          -0.0061 |         203.8808 |           1.4248 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0222 |         196.5046 |           1.4220 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0305 |         193.1452 |           1.4326 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0370 |         188.3383 |           1.4343 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0374 |         186.7938 |           1.4381 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0398 |         185.2116 |           1.4371 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0446 |         184.2258 |           1.4421 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0452 |         184.3833 |           1.4493 |
[32m[20221208 13:53:17 @agent_ppo2.py:179][0m |          -0.0464 |         181.4736 |           1.4513 |
[32m[20221208 13:53:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 871.93
[32m[20221208 13:53:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 901.15
[32m[20221208 13:53:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.41
[32m[20221208 13:53:17 @agent_ppo2.py:137][0m Total time:       7.95 min
[32m[20221208 13:53:17 @agent_ppo2.py:139][0m 659456 total steps have happened
[32m[20221208 13:53:17 @agent_ppo2.py:115][0m #------------------------ Iteration 322 --------------------------#
[32m[20221208 13:53:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |           0.0103 |         232.2119 |           1.4462 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0093 |         207.1771 |           1.4304 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0273 |         191.1031 |           1.4303 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0338 |         181.3960 |           1.4319 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0387 |         174.6779 |           1.4354 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0441 |         168.3378 |           1.4391 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0447 |         164.8833 |           1.4427 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0489 |         160.2660 |           1.4499 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0528 |         157.1780 |           1.4536 |
[32m[20221208 13:53:18 @agent_ppo2.py:179][0m |          -0.0535 |         154.4339 |           1.4568 |
[32m[20221208 13:53:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 665.41
[32m[20221208 13:53:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.06
[32m[20221208 13:53:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 908.04
[32m[20221208 13:53:19 @agent_ppo2.py:137][0m Total time:       7.97 min
[32m[20221208 13:53:19 @agent_ppo2.py:139][0m 661504 total steps have happened
[32m[20221208 13:53:19 @agent_ppo2.py:115][0m #------------------------ Iteration 323 --------------------------#
[32m[20221208 13:53:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:19 @agent_ppo2.py:179][0m |           0.0259 |         226.4005 |           1.5012 |
[32m[20221208 13:53:19 @agent_ppo2.py:179][0m |          -0.0086 |         201.1778 |           1.4914 |
[32m[20221208 13:53:19 @agent_ppo2.py:179][0m |          -0.0252 |         192.7627 |           1.5033 |
[32m[20221208 13:53:20 @agent_ppo2.py:179][0m |          -0.0324 |         185.6308 |           1.5030 |
[32m[20221208 13:53:20 @agent_ppo2.py:179][0m |          -0.0388 |         180.4183 |           1.5105 |
[32m[20221208 13:53:20 @agent_ppo2.py:179][0m |          -0.0407 |         177.5593 |           1.5140 |
[32m[20221208 13:53:20 @agent_ppo2.py:179][0m |          -0.0446 |         173.4234 |           1.5189 |
[32m[20221208 13:53:20 @agent_ppo2.py:179][0m |          -0.0462 |         172.9167 |           1.5150 |
[32m[20221208 13:53:20 @agent_ppo2.py:179][0m |          -0.0461 |         170.8637 |           1.5191 |
[32m[20221208 13:53:20 @agent_ppo2.py:179][0m |          -0.0489 |         169.0977 |           1.5229 |
[32m[20221208 13:53:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 733.63
[32m[20221208 13:53:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 923.17
[32m[20221208 13:53:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.18
[32m[20221208 13:53:20 @agent_ppo2.py:137][0m Total time:       8.00 min
[32m[20221208 13:53:20 @agent_ppo2.py:139][0m 663552 total steps have happened
[32m[20221208 13:53:20 @agent_ppo2.py:115][0m #------------------------ Iteration 324 --------------------------#
[32m[20221208 13:53:21 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |           0.0113 |         206.7294 |           1.4720 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0243 |         180.8610 |           1.4689 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0340 |         167.0028 |           1.4834 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0438 |         159.6874 |           1.4841 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0501 |         154.2372 |           1.4932 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0523 |         148.8405 |           1.4957 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0543 |         145.6230 |           1.4993 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0590 |         143.6832 |           1.4978 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0604 |         143.2973 |           1.5019 |
[32m[20221208 13:53:21 @agent_ppo2.py:179][0m |          -0.0631 |         139.1348 |           1.4986 |
[32m[20221208 13:53:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 630.27
[32m[20221208 13:53:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 903.12
[32m[20221208 13:53:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 690.08
[32m[20221208 13:53:22 @agent_ppo2.py:137][0m Total time:       8.02 min
[32m[20221208 13:53:22 @agent_ppo2.py:139][0m 665600 total steps have happened
[32m[20221208 13:53:22 @agent_ppo2.py:115][0m #------------------------ Iteration 325 --------------------------#
[32m[20221208 13:53:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:22 @agent_ppo2.py:179][0m |           0.0174 |         239.2115 |           1.5565 |
[32m[20221208 13:53:22 @agent_ppo2.py:179][0m |          -0.0107 |         204.0252 |           1.5512 |
[32m[20221208 13:53:22 @agent_ppo2.py:179][0m |          -0.0266 |         192.0593 |           1.5533 |
[32m[20221208 13:53:22 @agent_ppo2.py:179][0m |          -0.0402 |         186.2682 |           1.5630 |
[32m[20221208 13:53:23 @agent_ppo2.py:179][0m |          -0.0447 |         182.0301 |           1.5621 |
[32m[20221208 13:53:23 @agent_ppo2.py:179][0m |          -0.0490 |         178.6680 |           1.5750 |
[32m[20221208 13:53:23 @agent_ppo2.py:179][0m |          -0.0489 |         175.0193 |           1.5702 |
[32m[20221208 13:53:23 @agent_ppo2.py:179][0m |          -0.0525 |         173.4922 |           1.5776 |
[32m[20221208 13:53:23 @agent_ppo2.py:179][0m |          -0.0576 |         169.0484 |           1.5809 |
[32m[20221208 13:53:23 @agent_ppo2.py:179][0m |          -0.0579 |         167.4345 |           1.5845 |
[32m[20221208 13:53:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 739.57
[32m[20221208 13:53:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 889.17
[32m[20221208 13:53:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 834.34
[32m[20221208 13:53:23 @agent_ppo2.py:137][0m Total time:       8.05 min
[32m[20221208 13:53:23 @agent_ppo2.py:139][0m 667648 total steps have happened
[32m[20221208 13:53:23 @agent_ppo2.py:115][0m #------------------------ Iteration 326 --------------------------#
[32m[20221208 13:53:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |           0.0201 |         186.4471 |           1.6320 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0119 |         172.9256 |           1.6078 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0229 |         168.2394 |           1.6182 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0361 |         165.6967 |           1.6271 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0380 |         164.1969 |           1.6238 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0423 |         162.6569 |           1.6240 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0425 |         161.9470 |           1.6299 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0425 |         160.8207 |           1.6332 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0477 |         161.4644 |           1.6385 |
[32m[20221208 13:53:24 @agent_ppo2.py:179][0m |          -0.0498 |         159.3533 |           1.6356 |
[32m[20221208 13:53:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 654.10
[32m[20221208 13:53:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.77
[32m[20221208 13:53:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 657.65
[32m[20221208 13:53:25 @agent_ppo2.py:137][0m Total time:       8.07 min
[32m[20221208 13:53:25 @agent_ppo2.py:139][0m 669696 total steps have happened
[32m[20221208 13:53:25 @agent_ppo2.py:115][0m #------------------------ Iteration 327 --------------------------#
[32m[20221208 13:53:25 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:25 @agent_ppo2.py:179][0m |           0.0188 |         224.8004 |           1.5107 |
[32m[20221208 13:53:25 @agent_ppo2.py:179][0m |          -0.0093 |         206.7077 |           1.5013 |
[32m[20221208 13:53:25 @agent_ppo2.py:179][0m |          -0.0252 |         203.5027 |           1.5157 |
[32m[20221208 13:53:25 @agent_ppo2.py:179][0m |          -0.0336 |         198.6833 |           1.5230 |
[32m[20221208 13:53:25 @agent_ppo2.py:179][0m |          -0.0384 |         195.1422 |           1.5257 |
[32m[20221208 13:53:26 @agent_ppo2.py:179][0m |          -0.0415 |         192.8020 |           1.5277 |
[32m[20221208 13:53:26 @agent_ppo2.py:179][0m |          -0.0426 |         190.2722 |           1.5310 |
[32m[20221208 13:53:26 @agent_ppo2.py:179][0m |          -0.0469 |         188.3632 |           1.5319 |
[32m[20221208 13:53:26 @agent_ppo2.py:179][0m |          -0.0487 |         186.8806 |           1.5364 |
[32m[20221208 13:53:26 @agent_ppo2.py:179][0m |          -0.0507 |         185.6824 |           1.5368 |
[32m[20221208 13:53:26 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 856.59
[32m[20221208 13:53:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 884.16
[32m[20221208 13:53:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.22
[32m[20221208 13:53:26 @agent_ppo2.py:137][0m Total time:       8.10 min
[32m[20221208 13:53:26 @agent_ppo2.py:139][0m 671744 total steps have happened
[32m[20221208 13:53:26 @agent_ppo2.py:115][0m #------------------------ Iteration 328 --------------------------#
[32m[20221208 13:53:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |           0.0090 |         238.5508 |           1.6413 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0156 |         215.3808 |           1.6345 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0326 |         199.4986 |           1.6452 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0335 |         191.2864 |           1.6424 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0380 |         186.2584 |           1.6395 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0419 |         181.8278 |           1.6484 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0435 |         178.8147 |           1.6500 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0435 |         176.7303 |           1.6517 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0476 |         176.2981 |           1.6590 |
[32m[20221208 13:53:27 @agent_ppo2.py:179][0m |          -0.0491 |         172.5622 |           1.6640 |
[32m[20221208 13:53:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 670.85
[32m[20221208 13:53:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.93
[32m[20221208 13:53:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 649.88
[32m[20221208 13:53:28 @agent_ppo2.py:137][0m Total time:       8.12 min
[32m[20221208 13:53:28 @agent_ppo2.py:139][0m 673792 total steps have happened
[32m[20221208 13:53:28 @agent_ppo2.py:115][0m #------------------------ Iteration 329 --------------------------#
[32m[20221208 13:53:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:28 @agent_ppo2.py:179][0m |           0.0286 |         228.7407 |           1.5622 |
[32m[20221208 13:53:28 @agent_ppo2.py:179][0m |          -0.0127 |         212.4217 |           1.5497 |
[32m[20221208 13:53:28 @agent_ppo2.py:179][0m |          -0.0246 |         204.0743 |           1.5659 |
[32m[20221208 13:53:28 @agent_ppo2.py:179][0m |          -0.0331 |         198.7175 |           1.5633 |
[32m[20221208 13:53:28 @agent_ppo2.py:179][0m |          -0.0404 |         195.3261 |           1.5599 |
[32m[20221208 13:53:29 @agent_ppo2.py:179][0m |          -0.0441 |         192.1054 |           1.5712 |
[32m[20221208 13:53:29 @agent_ppo2.py:179][0m |          -0.0477 |         188.9097 |           1.5730 |
[32m[20221208 13:53:29 @agent_ppo2.py:179][0m |          -0.0480 |         185.5619 |           1.5723 |
[32m[20221208 13:53:29 @agent_ppo2.py:179][0m |          -0.0500 |         183.1459 |           1.5755 |
[32m[20221208 13:53:29 @agent_ppo2.py:179][0m |          -0.0496 |         183.8444 |           1.5797 |
[32m[20221208 13:53:29 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:53:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 885.91
[32m[20221208 13:53:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.92
[32m[20221208 13:53:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.90
[32m[20221208 13:53:29 @agent_ppo2.py:137][0m Total time:       8.15 min
[32m[20221208 13:53:29 @agent_ppo2.py:139][0m 675840 total steps have happened
[32m[20221208 13:53:29 @agent_ppo2.py:115][0m #------------------------ Iteration 330 --------------------------#
[32m[20221208 13:53:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |           0.0197 |         228.9415 |           1.5864 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |           0.0062 |         209.7006 |           1.5258 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0180 |         204.6310 |           1.5699 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0287 |         202.2422 |           1.5937 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0312 |         201.6042 |           1.5901 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0354 |         199.1993 |           1.6013 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0383 |         201.1350 |           1.6059 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0381 |         197.6951 |           1.6076 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0407 |         196.5721 |           1.6114 |
[32m[20221208 13:53:30 @agent_ppo2.py:179][0m |          -0.0409 |         196.0190 |           1.6147 |
[32m[20221208 13:53:30 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 957.74
[32m[20221208 13:53:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.51
[32m[20221208 13:53:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.95
[32m[20221208 13:53:31 @agent_ppo2.py:137][0m Total time:       8.17 min
[32m[20221208 13:53:31 @agent_ppo2.py:139][0m 677888 total steps have happened
[32m[20221208 13:53:31 @agent_ppo2.py:115][0m #------------------------ Iteration 331 --------------------------#
[32m[20221208 13:53:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:31 @agent_ppo2.py:179][0m |           0.0104 |         222.6484 |           1.6339 |
[32m[20221208 13:53:31 @agent_ppo2.py:179][0m |          -0.0156 |         205.6399 |           1.6204 |
[32m[20221208 13:53:31 @agent_ppo2.py:179][0m |          -0.0223 |         199.2204 |           1.6364 |
[32m[20221208 13:53:31 @agent_ppo2.py:179][0m |          -0.0326 |         192.7148 |           1.6319 |
[32m[20221208 13:53:31 @agent_ppo2.py:179][0m |          -0.0361 |         188.7471 |           1.6312 |
[32m[20221208 13:53:31 @agent_ppo2.py:179][0m |          -0.0400 |         185.1268 |           1.6411 |
[32m[20221208 13:53:32 @agent_ppo2.py:179][0m |          -0.0388 |         183.3428 |           1.6423 |
[32m[20221208 13:53:32 @agent_ppo2.py:179][0m |          -0.0401 |         180.8999 |           1.6492 |
[32m[20221208 13:53:32 @agent_ppo2.py:179][0m |          -0.0455 |         177.8078 |           1.6493 |
[32m[20221208 13:53:32 @agent_ppo2.py:179][0m |          -0.0462 |         177.5026 |           1.6484 |
[32m[20221208 13:53:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.00
[32m[20221208 13:53:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.54
[32m[20221208 13:53:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 777.48
[32m[20221208 13:53:32 @agent_ppo2.py:137][0m Total time:       8.20 min
[32m[20221208 13:53:32 @agent_ppo2.py:139][0m 679936 total steps have happened
[32m[20221208 13:53:32 @agent_ppo2.py:115][0m #------------------------ Iteration 332 --------------------------#
[32m[20221208 13:53:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |           0.0116 |         217.7810 |           1.5853 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |           0.0046 |         208.1264 |           1.5668 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0156 |         204.8012 |           1.5904 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0243 |         201.9317 |           1.5948 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0245 |         200.5006 |           1.5975 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0307 |         199.3497 |           1.5987 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0341 |         199.5670 |           1.6136 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0380 |         198.2449 |           1.6139 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0360 |         197.2416 |           1.6175 |
[32m[20221208 13:53:33 @agent_ppo2.py:179][0m |          -0.0348 |         196.9649 |           1.6203 |
[32m[20221208 13:53:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 941.28
[32m[20221208 13:53:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.69
[32m[20221208 13:53:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 669.00
[32m[20221208 13:53:34 @agent_ppo2.py:137][0m Total time:       8.22 min
[32m[20221208 13:53:34 @agent_ppo2.py:139][0m 681984 total steps have happened
[32m[20221208 13:53:34 @agent_ppo2.py:115][0m #------------------------ Iteration 333 --------------------------#
[32m[20221208 13:53:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:34 @agent_ppo2.py:179][0m |           0.0225 |         228.6532 |           1.7193 |
[32m[20221208 13:53:34 @agent_ppo2.py:179][0m |          -0.0023 |         219.7694 |           1.6935 |
[32m[20221208 13:53:34 @agent_ppo2.py:179][0m |          -0.0113 |         213.6265 |           1.7037 |
[32m[20221208 13:53:34 @agent_ppo2.py:179][0m |          -0.0228 |         203.9536 |           1.7290 |
[32m[20221208 13:53:34 @agent_ppo2.py:179][0m |          -0.0293 |         195.5105 |           1.7348 |
[32m[20221208 13:53:34 @agent_ppo2.py:179][0m |          -0.0329 |         191.8917 |           1.7403 |
[32m[20221208 13:53:34 @agent_ppo2.py:179][0m |          -0.0353 |         187.0763 |           1.7431 |
[32m[20221208 13:53:35 @agent_ppo2.py:179][0m |          -0.0338 |         184.7858 |           1.7420 |
[32m[20221208 13:53:35 @agent_ppo2.py:179][0m |          -0.0382 |         182.4194 |           1.7514 |
[32m[20221208 13:53:35 @agent_ppo2.py:179][0m |          -0.0397 |         180.1222 |           1.7539 |
[32m[20221208 13:53:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 910.60
[32m[20221208 13:53:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.94
[32m[20221208 13:53:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.51
[32m[20221208 13:53:35 @agent_ppo2.py:137][0m Total time:       8.25 min
[32m[20221208 13:53:35 @agent_ppo2.py:139][0m 684032 total steps have happened
[32m[20221208 13:53:35 @agent_ppo2.py:115][0m #------------------------ Iteration 334 --------------------------#
[32m[20221208 13:53:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |           0.0126 |         243.8399 |           1.6901 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |           0.0002 |         234.0134 |           1.6725 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0166 |         229.0366 |           1.6828 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0199 |         227.9618 |           1.6844 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0237 |         226.9239 |           1.6849 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0272 |         226.6948 |           1.6933 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0254 |         226.6907 |           1.6912 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0292 |         224.9465 |           1.6956 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0298 |         224.5780 |           1.6965 |
[32m[20221208 13:53:36 @agent_ppo2.py:179][0m |          -0.0315 |         225.5848 |           1.7064 |
[32m[20221208 13:53:36 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 967.76
[32m[20221208 13:53:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.36
[32m[20221208 13:53:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 944.60
[32m[20221208 13:53:37 @agent_ppo2.py:137][0m Total time:       8.27 min
[32m[20221208 13:53:37 @agent_ppo2.py:139][0m 686080 total steps have happened
[32m[20221208 13:53:37 @agent_ppo2.py:115][0m #------------------------ Iteration 335 --------------------------#
[32m[20221208 13:53:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |           0.0189 |         236.2173 |           1.7062 |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |           0.0016 |         223.9070 |           1.6659 |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |          -0.0177 |         219.1355 |           1.7138 |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |          -0.0268 |         217.0062 |           1.7315 |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |          -0.0281 |         214.9225 |           1.7231 |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |          -0.0334 |         214.5893 |           1.7335 |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |          -0.0372 |         212.3328 |           1.7407 |
[32m[20221208 13:53:37 @agent_ppo2.py:179][0m |          -0.0353 |         212.7259 |           1.7418 |
[32m[20221208 13:53:38 @agent_ppo2.py:179][0m |          -0.0380 |         210.8831 |           1.7503 |
[32m[20221208 13:53:38 @agent_ppo2.py:179][0m |          -0.0355 |         210.8444 |           1.7488 |
[32m[20221208 13:53:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.17
[32m[20221208 13:53:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.68
[32m[20221208 13:53:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.66
[32m[20221208 13:53:38 @agent_ppo2.py:137][0m Total time:       8.29 min
[32m[20221208 13:53:38 @agent_ppo2.py:139][0m 688128 total steps have happened
[32m[20221208 13:53:38 @agent_ppo2.py:115][0m #------------------------ Iteration 336 --------------------------#
[32m[20221208 13:53:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |           0.0173 |         242.2063 |           1.7273 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0091 |         232.8204 |           1.7187 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0122 |         227.8695 |           1.7217 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0208 |         225.6887 |           1.7349 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0284 |         224.1067 |           1.7400 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0328 |         222.0938 |           1.7449 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0346 |         220.8305 |           1.7483 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0330 |         221.4587 |           1.7494 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0368 |         219.5431 |           1.7557 |
[32m[20221208 13:53:39 @agent_ppo2.py:179][0m |          -0.0326 |         219.6950 |           1.7593 |
[32m[20221208 13:53:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 900.04
[32m[20221208 13:53:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 940.78
[32m[20221208 13:53:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 744.38
[32m[20221208 13:53:39 @agent_ppo2.py:137][0m Total time:       8.32 min
[32m[20221208 13:53:39 @agent_ppo2.py:139][0m 690176 total steps have happened
[32m[20221208 13:53:39 @agent_ppo2.py:115][0m #------------------------ Iteration 337 --------------------------#
[32m[20221208 13:53:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |           0.0132 |         260.6141 |           1.7826 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0146 |         246.8700 |           1.8007 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0211 |         242.9366 |           1.7977 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0269 |         239.6442 |           1.8040 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0346 |         235.7388 |           1.8110 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0312 |         233.3949 |           1.7973 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0353 |         232.3505 |           1.8080 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0386 |         232.9443 |           1.8178 |
[32m[20221208 13:53:40 @agent_ppo2.py:179][0m |          -0.0420 |         230.8241 |           1.8207 |
[32m[20221208 13:53:41 @agent_ppo2.py:179][0m |          -0.0391 |         231.5907 |           1.8219 |
[32m[20221208 13:53:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.85
[32m[20221208 13:53:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.56
[32m[20221208 13:53:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 896.53
[32m[20221208 13:53:41 @agent_ppo2.py:137][0m Total time:       8.34 min
[32m[20221208 13:53:41 @agent_ppo2.py:139][0m 692224 total steps have happened
[32m[20221208 13:53:41 @agent_ppo2.py:115][0m #------------------------ Iteration 338 --------------------------#
[32m[20221208 13:53:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:41 @agent_ppo2.py:179][0m |           0.0148 |         245.0006 |           1.7780 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |           0.0011 |         240.8316 |           1.7639 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0128 |         239.1472 |           1.7787 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0170 |         236.7728 |           1.7680 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0230 |         234.6466 |           1.7717 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0271 |         234.9971 |           1.7831 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0280 |         234.2537 |           1.7873 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0233 |         232.8252 |           1.7843 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0310 |         232.8491 |           1.7955 |
[32m[20221208 13:53:42 @agent_ppo2.py:179][0m |          -0.0325 |         232.8995 |           1.7977 |
[32m[20221208 13:53:42 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 915.34
[32m[20221208 13:53:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.59
[32m[20221208 13:53:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.19
[32m[20221208 13:53:42 @agent_ppo2.py:137][0m Total time:       8.37 min
[32m[20221208 13:53:42 @agent_ppo2.py:139][0m 694272 total steps have happened
[32m[20221208 13:53:42 @agent_ppo2.py:115][0m #------------------------ Iteration 339 --------------------------#
[32m[20221208 13:53:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |           0.0132 |         243.2256 |           1.8098 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0081 |         234.9763 |           1.7960 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0152 |         231.2008 |           1.8031 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0223 |         229.9761 |           1.8022 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0238 |         228.1439 |           1.7990 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0303 |         227.6835 |           1.8012 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0329 |         226.9996 |           1.8065 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0331 |         225.7736 |           1.8027 |
[32m[20221208 13:53:43 @agent_ppo2.py:179][0m |          -0.0316 |         226.7764 |           1.8062 |
[32m[20221208 13:53:44 @agent_ppo2.py:179][0m |          -0.0381 |         225.4802 |           1.8165 |
[32m[20221208 13:53:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 893.48
[32m[20221208 13:53:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.93
[32m[20221208 13:53:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.05
[32m[20221208 13:53:44 @agent_ppo2.py:137][0m Total time:       8.39 min
[32m[20221208 13:53:44 @agent_ppo2.py:139][0m 696320 total steps have happened
[32m[20221208 13:53:44 @agent_ppo2.py:115][0m #------------------------ Iteration 340 --------------------------#
[32m[20221208 13:53:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:44 @agent_ppo2.py:179][0m |           0.0158 |         246.0789 |           1.7917 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0141 |         241.3123 |           1.7889 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0172 |         239.1127 |           1.7938 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0209 |         238.2920 |           1.7979 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0233 |         236.6772 |           1.7903 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0279 |         236.6096 |           1.8039 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0291 |         236.1231 |           1.8065 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0312 |         234.6238 |           1.8068 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0338 |         234.1634 |           1.8098 |
[32m[20221208 13:53:45 @agent_ppo2.py:179][0m |          -0.0325 |         235.1545 |           1.8111 |
[32m[20221208 13:53:45 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:53:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.08
[32m[20221208 13:53:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.12
[32m[20221208 13:53:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.90
[32m[20221208 13:53:45 @agent_ppo2.py:137][0m Total time:       8.42 min
[32m[20221208 13:53:45 @agent_ppo2.py:139][0m 698368 total steps have happened
[32m[20221208 13:53:45 @agent_ppo2.py:115][0m #------------------------ Iteration 341 --------------------------#
[32m[20221208 13:53:46 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:53:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |           0.0248 |         242.3011 |           1.8341 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |           0.0024 |         235.7471 |           1.8022 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0144 |         232.9519 |           1.8250 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0244 |         231.7068 |           1.8292 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0280 |         232.0616 |           1.8395 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0234 |         230.2596 |           1.8332 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0310 |         229.1519 |           1.8385 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0307 |         228.6658 |           1.8334 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0354 |         228.8396 |           1.8488 |
[32m[20221208 13:53:46 @agent_ppo2.py:179][0m |          -0.0358 |         228.3721 |           1.8544 |
[32m[20221208 13:53:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 884.21
[32m[20221208 13:53:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.27
[32m[20221208 13:53:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 976.36
[32m[20221208 13:53:47 @agent_ppo2.py:137][0m Total time:       8.44 min
[32m[20221208 13:53:47 @agent_ppo2.py:139][0m 700416 total steps have happened
[32m[20221208 13:53:47 @agent_ppo2.py:115][0m #------------------------ Iteration 342 --------------------------#
[32m[20221208 13:53:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:47 @agent_ppo2.py:179][0m |           0.0242 |         244.3434 |           1.6986 |
[32m[20221208 13:53:47 @agent_ppo2.py:179][0m |          -0.0083 |         234.8723 |           1.6926 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0094 |         229.8903 |           1.6821 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0240 |         228.2190 |           1.6952 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0210 |         227.0662 |           1.6754 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0302 |         227.5167 |           1.6821 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0332 |         227.1658 |           1.6907 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0353 |         225.2344 |           1.6852 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0333 |         224.8849 |           1.6835 |
[32m[20221208 13:53:48 @agent_ppo2.py:179][0m |          -0.0369 |         224.3880 |           1.6930 |
[32m[20221208 13:53:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 854.32
[32m[20221208 13:53:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.39
[32m[20221208 13:53:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.64
[32m[20221208 13:53:48 @agent_ppo2.py:137][0m Total time:       8.47 min
[32m[20221208 13:53:48 @agent_ppo2.py:139][0m 702464 total steps have happened
[32m[20221208 13:53:48 @agent_ppo2.py:115][0m #------------------------ Iteration 343 --------------------------#
[32m[20221208 13:53:49 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |           0.0104 |         236.5424 |           1.6528 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0072 |         232.4807 |           1.6358 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0187 |         230.2403 |           1.6412 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0244 |         228.2960 |           1.6399 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0277 |         227.1046 |           1.6367 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0304 |         225.1454 |           1.6364 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0312 |         224.5472 |           1.6453 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0267 |         222.7646 |           1.6343 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0347 |         222.6997 |           1.6411 |
[32m[20221208 13:53:49 @agent_ppo2.py:179][0m |          -0.0374 |         222.3385 |           1.6519 |
[32m[20221208 13:53:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.96
[32m[20221208 13:53:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.25
[32m[20221208 13:53:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.55
[32m[20221208 13:53:50 @agent_ppo2.py:137][0m Total time:       8.49 min
[32m[20221208 13:53:50 @agent_ppo2.py:139][0m 704512 total steps have happened
[32m[20221208 13:53:50 @agent_ppo2.py:115][0m #------------------------ Iteration 344 --------------------------#
[32m[20221208 13:53:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:50 @agent_ppo2.py:179][0m |           0.0040 |         231.7526 |           1.6476 |
[32m[20221208 13:53:50 @agent_ppo2.py:179][0m |           0.0064 |         223.0434 |           1.6090 |
[32m[20221208 13:53:50 @agent_ppo2.py:179][0m |           0.0080 |         220.5191 |           1.6007 |
[32m[20221208 13:53:51 @agent_ppo2.py:179][0m |          -0.0153 |         220.2431 |           1.6281 |
[32m[20221208 13:53:51 @agent_ppo2.py:179][0m |          -0.0238 |         219.0823 |           1.6379 |
[32m[20221208 13:53:51 @agent_ppo2.py:179][0m |          -0.0236 |         217.8562 |           1.6377 |
[32m[20221208 13:53:51 @agent_ppo2.py:179][0m |          -0.0297 |         217.2127 |           1.6470 |
[32m[20221208 13:53:51 @agent_ppo2.py:179][0m |          -0.0313 |         216.7833 |           1.6472 |
[32m[20221208 13:53:51 @agent_ppo2.py:179][0m |          -0.0331 |         216.4197 |           1.6468 |
[32m[20221208 13:53:51 @agent_ppo2.py:179][0m |          -0.0320 |         215.9795 |           1.6538 |
[32m[20221208 13:53:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:53:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.47
[32m[20221208 13:53:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.43
[32m[20221208 13:53:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 642.16
[32m[20221208 13:53:51 @agent_ppo2.py:137][0m Total time:       8.52 min
[32m[20221208 13:53:51 @agent_ppo2.py:139][0m 706560 total steps have happened
[32m[20221208 13:53:51 @agent_ppo2.py:115][0m #------------------------ Iteration 345 --------------------------#
[32m[20221208 13:53:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:53:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |           0.0221 |         215.3529 |           1.5330 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |           0.0177 |         206.2493 |           1.5033 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0113 |         201.4336 |           1.4901 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0252 |         198.5017 |           1.5112 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0328 |         195.3022 |           1.5145 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0375 |         191.1188 |           1.5156 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0362 |         191.1011 |           1.5115 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0420 |         188.8973 |           1.5201 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0452 |         187.8418 |           1.5204 |
[32m[20221208 13:53:52 @agent_ppo2.py:179][0m |          -0.0461 |         186.0385 |           1.5234 |
[32m[20221208 13:53:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 799.84
[32m[20221208 13:53:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.37
[32m[20221208 13:53:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.09
[32m[20221208 13:53:53 @agent_ppo2.py:137][0m Total time:       8.54 min
[32m[20221208 13:53:53 @agent_ppo2.py:139][0m 708608 total steps have happened
[32m[20221208 13:53:53 @agent_ppo2.py:115][0m #------------------------ Iteration 346 --------------------------#
[32m[20221208 13:53:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:53 @agent_ppo2.py:179][0m |           0.0092 |         236.6827 |           1.5696 |
[32m[20221208 13:53:53 @agent_ppo2.py:179][0m |           0.0062 |         232.8219 |           1.5499 |
[32m[20221208 13:53:53 @agent_ppo2.py:179][0m |          -0.0126 |         231.4959 |           1.5885 |
[32m[20221208 13:53:53 @agent_ppo2.py:179][0m |          -0.0193 |         230.5003 |           1.6022 |
[32m[20221208 13:53:54 @agent_ppo2.py:179][0m |          -0.0195 |         229.6407 |           1.5998 |
[32m[20221208 13:53:54 @agent_ppo2.py:179][0m |          -0.0226 |         229.2907 |           1.6098 |
[32m[20221208 13:53:54 @agent_ppo2.py:179][0m |          -0.0281 |         230.8077 |           1.6206 |
[32m[20221208 13:53:54 @agent_ppo2.py:179][0m |          -0.0305 |         228.8658 |           1.6223 |
[32m[20221208 13:53:54 @agent_ppo2.py:179][0m |          -0.0331 |         228.8771 |           1.6286 |
[32m[20221208 13:53:54 @agent_ppo2.py:179][0m |          -0.0316 |         229.2583 |           1.6350 |
[32m[20221208 13:53:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.83
[32m[20221208 13:53:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.91
[32m[20221208 13:53:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 610.24
[32m[20221208 13:53:54 @agent_ppo2.py:137][0m Total time:       8.56 min
[32m[20221208 13:53:54 @agent_ppo2.py:139][0m 710656 total steps have happened
[32m[20221208 13:53:54 @agent_ppo2.py:115][0m #------------------------ Iteration 347 --------------------------#
[32m[20221208 13:53:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |           0.0249 |         239.0164 |           1.6882 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |           0.0098 |         237.1855 |           1.6679 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |           0.0022 |         234.4859 |           1.6610 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |          -0.0178 |         232.8980 |           1.6970 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |          -0.0245 |         233.7925 |           1.7063 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |          -0.0256 |         232.9716 |           1.7054 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |          -0.0288 |         232.2443 |           1.7131 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |          -0.0297 |         231.5292 |           1.7157 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |          -0.0302 |         232.3508 |           1.7189 |
[32m[20221208 13:53:55 @agent_ppo2.py:179][0m |          -0.0301 |         232.7323 |           1.7227 |
[32m[20221208 13:53:55 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.00
[32m[20221208 13:53:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.79
[32m[20221208 13:53:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 643.58
[32m[20221208 13:53:56 @agent_ppo2.py:137][0m Total time:       8.59 min
[32m[20221208 13:53:56 @agent_ppo2.py:139][0m 712704 total steps have happened
[32m[20221208 13:53:56 @agent_ppo2.py:115][0m #------------------------ Iteration 348 --------------------------#
[32m[20221208 13:53:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:53:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:56 @agent_ppo2.py:179][0m |           0.0195 |         198.8228 |           1.7216 |
[32m[20221208 13:53:56 @agent_ppo2.py:179][0m |          -0.0129 |         178.8433 |           1.7224 |
[32m[20221208 13:53:56 @agent_ppo2.py:179][0m |          -0.0270 |         171.0709 |           1.7240 |
[32m[20221208 13:53:56 @agent_ppo2.py:179][0m |          -0.0317 |         167.5814 |           1.7290 |
[32m[20221208 13:53:56 @agent_ppo2.py:179][0m |          -0.0272 |         166.3446 |           1.7235 |
[32m[20221208 13:53:57 @agent_ppo2.py:179][0m |          -0.0346 |         163.5056 |           1.7273 |
[32m[20221208 13:53:57 @agent_ppo2.py:179][0m |          -0.0395 |         162.2967 |           1.7400 |
[32m[20221208 13:53:57 @agent_ppo2.py:179][0m |          -0.0420 |         161.9519 |           1.7458 |
[32m[20221208 13:53:57 @agent_ppo2.py:179][0m |          -0.0420 |         159.9530 |           1.7555 |
[32m[20221208 13:53:57 @agent_ppo2.py:179][0m |          -0.0411 |         158.4365 |           1.7594 |
[32m[20221208 13:53:57 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:53:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 662.45
[32m[20221208 13:53:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.14
[32m[20221208 13:53:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 660.90
[32m[20221208 13:53:57 @agent_ppo2.py:137][0m Total time:       8.61 min
[32m[20221208 13:53:57 @agent_ppo2.py:139][0m 714752 total steps have happened
[32m[20221208 13:53:57 @agent_ppo2.py:115][0m #------------------------ Iteration 349 --------------------------#
[32m[20221208 13:53:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |           0.0159 |         246.4098 |           1.7485 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0010 |         236.8065 |           1.7363 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0161 |         234.8232 |           1.7598 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0195 |         231.7902 |           1.7726 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0233 |         230.4592 |           1.7717 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0273 |         229.4211 |           1.7781 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0303 |         228.7893 |           1.7839 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0309 |         229.1059 |           1.7842 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0343 |         227.3647 |           1.7926 |
[32m[20221208 13:53:58 @agent_ppo2.py:179][0m |          -0.0303 |         227.0274 |           1.7879 |
[32m[20221208 13:53:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:53:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.96
[32m[20221208 13:53:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.39
[32m[20221208 13:53:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.15
[32m[20221208 13:53:59 @agent_ppo2.py:137][0m Total time:       8.64 min
[32m[20221208 13:53:59 @agent_ppo2.py:139][0m 716800 total steps have happened
[32m[20221208 13:53:59 @agent_ppo2.py:115][0m #------------------------ Iteration 350 --------------------------#
[32m[20221208 13:53:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:53:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:53:59 @agent_ppo2.py:179][0m |           0.0246 |         218.8446 |           1.7157 |
[32m[20221208 13:53:59 @agent_ppo2.py:179][0m |           0.0071 |         193.1497 |           1.7143 |
[32m[20221208 13:53:59 @agent_ppo2.py:179][0m |          -0.0046 |         188.1174 |           1.7166 |
[32m[20221208 13:53:59 @agent_ppo2.py:179][0m |          -0.0219 |         185.4266 |           1.7242 |
[32m[20221208 13:53:59 @agent_ppo2.py:179][0m |          -0.0318 |         183.5448 |           1.7450 |
[32m[20221208 13:54:00 @agent_ppo2.py:179][0m |          -0.0366 |         183.9480 |           1.7491 |
[32m[20221208 13:54:00 @agent_ppo2.py:179][0m |          -0.0380 |         180.9068 |           1.7479 |
[32m[20221208 13:54:00 @agent_ppo2.py:179][0m |          -0.0424 |         180.6736 |           1.7525 |
[32m[20221208 13:54:00 @agent_ppo2.py:179][0m |          -0.0445 |         180.1756 |           1.7562 |
[32m[20221208 13:54:00 @agent_ppo2.py:179][0m |          -0.0473 |         179.5935 |           1.7555 |
[32m[20221208 13:54:00 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 634.85
[32m[20221208 13:54:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.88
[32m[20221208 13:54:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 683.48
[32m[20221208 13:54:00 @agent_ppo2.py:137][0m Total time:       8.66 min
[32m[20221208 13:54:00 @agent_ppo2.py:139][0m 718848 total steps have happened
[32m[20221208 13:54:00 @agent_ppo2.py:115][0m #------------------------ Iteration 351 --------------------------#
[32m[20221208 13:54:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |           0.0121 |         247.7240 |           1.8381 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |           0.0005 |         232.3768 |           1.8229 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0258 |         226.3879 |           1.8418 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0357 |         223.5593 |           1.8410 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0389 |         221.6271 |           1.8436 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0434 |         219.9209 |           1.8530 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0438 |         219.1711 |           1.8561 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0466 |         217.7225 |           1.8557 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0486 |         216.6539 |           1.8623 |
[32m[20221208 13:54:01 @agent_ppo2.py:179][0m |          -0.0478 |         216.3257 |           1.8612 |
[32m[20221208 13:54:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 854.50
[32m[20221208 13:54:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 915.34
[32m[20221208 13:54:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 656.84
[32m[20221208 13:54:02 @agent_ppo2.py:137][0m Total time:       8.69 min
[32m[20221208 13:54:02 @agent_ppo2.py:139][0m 720896 total steps have happened
[32m[20221208 13:54:02 @agent_ppo2.py:115][0m #------------------------ Iteration 352 --------------------------#
[32m[20221208 13:54:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:02 @agent_ppo2.py:179][0m |           0.0041 |         243.8897 |           1.7514 |
[32m[20221208 13:54:02 @agent_ppo2.py:179][0m |          -0.0175 |         215.0006 |           1.7354 |
[32m[20221208 13:54:02 @agent_ppo2.py:179][0m |          -0.0247 |         199.4998 |           1.7365 |
[32m[20221208 13:54:02 @agent_ppo2.py:179][0m |          -0.0316 |         190.7980 |           1.7399 |
[32m[20221208 13:54:02 @agent_ppo2.py:179][0m |          -0.0364 |         184.8260 |           1.7462 |
[32m[20221208 13:54:02 @agent_ppo2.py:179][0m |          -0.0407 |         179.9266 |           1.7405 |
[32m[20221208 13:54:03 @agent_ppo2.py:179][0m |          -0.0428 |         177.3363 |           1.7524 |
[32m[20221208 13:54:03 @agent_ppo2.py:179][0m |          -0.0466 |         175.2925 |           1.7535 |
[32m[20221208 13:54:03 @agent_ppo2.py:179][0m |          -0.0485 |         173.3302 |           1.7556 |
[32m[20221208 13:54:03 @agent_ppo2.py:179][0m |          -0.0485 |         172.0037 |           1.7545 |
[32m[20221208 13:54:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 614.17
[32m[20221208 13:54:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.38
[32m[20221208 13:54:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.85
[32m[20221208 13:54:03 @agent_ppo2.py:137][0m Total time:       8.71 min
[32m[20221208 13:54:03 @agent_ppo2.py:139][0m 722944 total steps have happened
[32m[20221208 13:54:03 @agent_ppo2.py:115][0m #------------------------ Iteration 353 --------------------------#
[32m[20221208 13:54:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |           0.0219 |         199.7263 |           1.7787 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0087 |         172.2664 |           1.7716 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0239 |         164.2898 |           1.7870 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0311 |         158.4971 |           1.7834 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0370 |         154.4315 |           1.7928 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0375 |         152.0063 |           1.7854 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0390 |         148.1601 |           1.7927 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0441 |         145.2535 |           1.7996 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0473 |         143.3445 |           1.7994 |
[32m[20221208 13:54:04 @agent_ppo2.py:179][0m |          -0.0477 |         142.5415 |           1.7985 |
[32m[20221208 13:54:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 634.52
[32m[20221208 13:54:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 902.21
[32m[20221208 13:54:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.63
[32m[20221208 13:54:05 @agent_ppo2.py:137][0m Total time:       8.74 min
[32m[20221208 13:54:05 @agent_ppo2.py:139][0m 724992 total steps have happened
[32m[20221208 13:54:05 @agent_ppo2.py:115][0m #------------------------ Iteration 354 --------------------------#
[32m[20221208 13:54:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:05 @agent_ppo2.py:179][0m |           0.0347 |         227.1638 |           1.8229 |
[32m[20221208 13:54:05 @agent_ppo2.py:179][0m |           0.0045 |         201.2593 |           1.8265 |
[32m[20221208 13:54:05 @agent_ppo2.py:179][0m |          -0.0311 |         193.1386 |           1.8451 |
[32m[20221208 13:54:05 @agent_ppo2.py:179][0m |          -0.0394 |         188.8085 |           1.8521 |
[32m[20221208 13:54:05 @agent_ppo2.py:179][0m |          -0.0459 |         186.3288 |           1.8538 |
[32m[20221208 13:54:05 @agent_ppo2.py:179][0m |          -0.0502 |         185.0348 |           1.8582 |
[32m[20221208 13:54:06 @agent_ppo2.py:179][0m |          -0.0502 |         182.7392 |           1.8523 |
[32m[20221208 13:54:06 @agent_ppo2.py:179][0m |          -0.0568 |         181.2461 |           1.8613 |
[32m[20221208 13:54:06 @agent_ppo2.py:179][0m |          -0.0584 |         181.7843 |           1.8635 |
[32m[20221208 13:54:06 @agent_ppo2.py:179][0m |          -0.0579 |         179.5108 |           1.8651 |
[32m[20221208 13:54:06 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:54:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 684.51
[32m[20221208 13:54:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 901.61
[32m[20221208 13:54:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 667.08
[32m[20221208 13:54:06 @agent_ppo2.py:137][0m Total time:       8.76 min
[32m[20221208 13:54:06 @agent_ppo2.py:139][0m 727040 total steps have happened
[32m[20221208 13:54:06 @agent_ppo2.py:115][0m #------------------------ Iteration 355 --------------------------#
[32m[20221208 13:54:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |           0.0219 |         188.5803 |           1.9037 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0031 |         173.8556 |           1.8809 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0221 |         168.2140 |           1.9023 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0308 |         164.6232 |           1.9038 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0297 |         163.0253 |           1.8953 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0392 |         162.2448 |           1.9101 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0386 |         160.1384 |           1.9033 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0400 |         158.0115 |           1.9073 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0390 |         156.5383 |           1.9071 |
[32m[20221208 13:54:07 @agent_ppo2.py:179][0m |          -0.0439 |         154.1207 |           1.9135 |
[32m[20221208 13:54:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:54:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 688.51
[32m[20221208 13:54:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.94
[32m[20221208 13:54:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 398.90
[32m[20221208 13:54:08 @agent_ppo2.py:137][0m Total time:       8.79 min
[32m[20221208 13:54:08 @agent_ppo2.py:139][0m 729088 total steps have happened
[32m[20221208 13:54:08 @agent_ppo2.py:115][0m #------------------------ Iteration 356 --------------------------#
[32m[20221208 13:54:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:08 @agent_ppo2.py:179][0m |           0.0163 |         244.6600 |           1.8522 |
[32m[20221208 13:54:08 @agent_ppo2.py:179][0m |           0.0070 |         238.6785 |           1.8035 |
[32m[20221208 13:54:08 @agent_ppo2.py:179][0m |          -0.0201 |         235.1804 |           1.8607 |
[32m[20221208 13:54:08 @agent_ppo2.py:179][0m |          -0.0260 |         233.3505 |           1.8657 |
[32m[20221208 13:54:08 @agent_ppo2.py:179][0m |          -0.0305 |         230.7430 |           1.8766 |
[32m[20221208 13:54:08 @agent_ppo2.py:179][0m |          -0.0321 |         229.9111 |           1.8770 |
[32m[20221208 13:54:08 @agent_ppo2.py:179][0m |          -0.0328 |         228.4374 |           1.8827 |
[32m[20221208 13:54:09 @agent_ppo2.py:179][0m |          -0.0369 |         227.1193 |           1.8918 |
[32m[20221208 13:54:09 @agent_ppo2.py:179][0m |          -0.0370 |         226.4333 |           1.8944 |
[32m[20221208 13:54:09 @agent_ppo2.py:179][0m |          -0.0373 |         225.8386 |           1.9032 |
[32m[20221208 13:54:09 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:54:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 975.28
[32m[20221208 13:54:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 997.81
[32m[20221208 13:54:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.64
[32m[20221208 13:54:09 @agent_ppo2.py:137][0m Total time:       8.81 min
[32m[20221208 13:54:09 @agent_ppo2.py:139][0m 731136 total steps have happened
[32m[20221208 13:54:09 @agent_ppo2.py:115][0m #------------------------ Iteration 357 --------------------------#
[32m[20221208 13:54:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |           0.0142 |         210.0466 |           1.8817 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0145 |         188.8503 |           1.8688 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0248 |         183.3166 |           1.8725 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0266 |         178.7964 |           1.8665 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0365 |         175.7935 |           1.8789 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0378 |         174.3680 |           1.8879 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0433 |         172.9991 |           1.8881 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0471 |         170.0833 |           1.8978 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0474 |         169.3523 |           1.8945 |
[32m[20221208 13:54:10 @agent_ppo2.py:179][0m |          -0.0486 |         168.1924 |           1.8995 |
[32m[20221208 13:54:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 701.45
[32m[20221208 13:54:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.03
[32m[20221208 13:54:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 399.50
[32m[20221208 13:54:11 @agent_ppo2.py:137][0m Total time:       8.84 min
[32m[20221208 13:54:11 @agent_ppo2.py:139][0m 733184 total steps have happened
[32m[20221208 13:54:11 @agent_ppo2.py:115][0m #------------------------ Iteration 358 --------------------------#
[32m[20221208 13:54:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |           0.0087 |         251.2967 |           1.9299 |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |          -0.0108 |         240.2127 |           1.9081 |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |          -0.0226 |         236.3761 |           1.9350 |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |          -0.0247 |         235.2148 |           1.9433 |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |          -0.0310 |         232.9440 |           1.9452 |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |          -0.0322 |         232.8625 |           1.9443 |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |          -0.0351 |         230.1766 |           1.9505 |
[32m[20221208 13:54:11 @agent_ppo2.py:179][0m |          -0.0372 |         230.0466 |           1.9587 |
[32m[20221208 13:54:12 @agent_ppo2.py:179][0m |          -0.0368 |         228.7620 |           1.9591 |
[32m[20221208 13:54:12 @agent_ppo2.py:179][0m |          -0.0389 |         229.0771 |           1.9587 |
[32m[20221208 13:54:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 960.16
[32m[20221208 13:54:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.25
[32m[20221208 13:54:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 976.55
[32m[20221208 13:54:12 @agent_ppo2.py:137][0m Total time:       8.86 min
[32m[20221208 13:54:12 @agent_ppo2.py:139][0m 735232 total steps have happened
[32m[20221208 13:54:12 @agent_ppo2.py:115][0m #------------------------ Iteration 359 --------------------------#
[32m[20221208 13:54:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |           0.0202 |         238.4788 |           1.8888 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |           0.0133 |         227.1847 |           1.8541 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0170 |         217.7183 |           1.9071 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0257 |         209.8995 |           1.9066 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0232 |         204.5084 |           1.8983 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0252 |         201.2462 |           1.9060 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0341 |         196.6190 |           1.9071 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0362 |         195.6527 |           1.9252 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0369 |         193.6197 |           1.9261 |
[32m[20221208 13:54:13 @agent_ppo2.py:179][0m |          -0.0425 |         190.4067 |           1.9364 |
[32m[20221208 13:54:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.04
[32m[20221208 13:54:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.63
[32m[20221208 13:54:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.87
[32m[20221208 13:54:13 @agent_ppo2.py:137][0m Total time:       8.89 min
[32m[20221208 13:54:13 @agent_ppo2.py:139][0m 737280 total steps have happened
[32m[20221208 13:54:13 @agent_ppo2.py:115][0m #------------------------ Iteration 360 --------------------------#
[32m[20221208 13:54:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |           0.0129 |         241.6780 |           1.8866 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0070 |         229.7762 |           1.8832 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0245 |         225.1737 |           1.8792 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0320 |         221.5529 |           1.8869 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0348 |         220.7182 |           1.9040 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0400 |         217.3880 |           1.9017 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0427 |         216.4422 |           1.9044 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0429 |         214.3122 |           1.9079 |
[32m[20221208 13:54:14 @agent_ppo2.py:179][0m |          -0.0446 |         213.3986 |           1.9053 |
[32m[20221208 13:54:15 @agent_ppo2.py:179][0m |          -0.0458 |         212.9543 |           1.9052 |
[32m[20221208 13:54:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 929.57
[32m[20221208 13:54:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.11
[32m[20221208 13:54:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.93
[32m[20221208 13:54:15 @agent_ppo2.py:137][0m Total time:       8.91 min
[32m[20221208 13:54:15 @agent_ppo2.py:139][0m 739328 total steps have happened
[32m[20221208 13:54:15 @agent_ppo2.py:115][0m #------------------------ Iteration 361 --------------------------#
[32m[20221208 13:54:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:54:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |           0.0265 |          48.0328 |           1.8102 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |           0.0052 |          40.6089 |           1.8034 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |           0.0324 |          40.0520 |           1.7377 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |           0.0211 |          39.4421 |           1.7609 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |           0.0473 |          39.1408 |           1.6922 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |           0.0194 |          38.9111 |           1.7086 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |           0.0006 |          38.7772 |           1.7880 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |          -0.0011 |          38.7077 |           1.7744 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |          -0.0132 |          38.6182 |           1.8059 |
[32m[20221208 13:54:16 @agent_ppo2.py:179][0m |          -0.0123 |          38.5595 |           1.8012 |
[32m[20221208 13:54:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:54:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 108.92
[32m[20221208 13:54:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 112.47
[32m[20221208 13:54:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 674.16
[32m[20221208 13:54:16 @agent_ppo2.py:137][0m Total time:       8.93 min
[32m[20221208 13:54:16 @agent_ppo2.py:139][0m 741376 total steps have happened
[32m[20221208 13:54:16 @agent_ppo2.py:115][0m #------------------------ Iteration 362 --------------------------#
[32m[20221208 13:54:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |           0.0113 |         255.2500 |           1.7824 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0106 |         246.0155 |           1.7661 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0210 |         241.9232 |           1.7944 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0272 |         240.7571 |           1.7976 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0264 |         242.2539 |           1.7930 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0294 |         239.2552 |           1.8014 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0364 |         239.8654 |           1.8126 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0338 |         238.5730 |           1.7940 |
[32m[20221208 13:54:17 @agent_ppo2.py:179][0m |          -0.0386 |         238.6313 |           1.8113 |
[32m[20221208 13:54:18 @agent_ppo2.py:179][0m |          -0.0383 |         237.6257 |           1.8116 |
[32m[20221208 13:54:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 920.49
[32m[20221208 13:54:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.18
[32m[20221208 13:54:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.74
[32m[20221208 13:54:18 @agent_ppo2.py:137][0m Total time:       8.96 min
[32m[20221208 13:54:18 @agent_ppo2.py:139][0m 743424 total steps have happened
[32m[20221208 13:54:18 @agent_ppo2.py:115][0m #------------------------ Iteration 363 --------------------------#
[32m[20221208 13:54:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:18 @agent_ppo2.py:179][0m |           0.0174 |         125.0246 |           1.8175 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0113 |         111.0726 |           1.7996 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0252 |         109.0144 |           1.8147 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0318 |         107.7319 |           1.8093 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0339 |         106.6735 |           1.8098 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0400 |         105.0324 |           1.8118 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0428 |         103.4072 |           1.8179 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0378 |         101.2316 |           1.8245 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0449 |         100.6060 |           1.8199 |
[32m[20221208 13:54:19 @agent_ppo2.py:179][0m |          -0.0494 |         100.4129 |           1.8251 |
[32m[20221208 13:54:19 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 403.24
[32m[20221208 13:54:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 900.75
[32m[20221208 13:54:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 378.55
[32m[20221208 13:54:19 @agent_ppo2.py:137][0m Total time:       8.98 min
[32m[20221208 13:54:19 @agent_ppo2.py:139][0m 745472 total steps have happened
[32m[20221208 13:54:19 @agent_ppo2.py:115][0m #------------------------ Iteration 364 --------------------------#
[32m[20221208 13:54:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |           0.0195 |         254.1746 |           1.7653 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0011 |         239.6086 |           1.7700 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0197 |         237.0845 |           1.7684 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0236 |         234.8348 |           1.7770 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0303 |         233.6499 |           1.7878 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0335 |         233.9392 |           1.7879 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0364 |         232.5313 |           1.7985 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0364 |         231.9428 |           1.7911 |
[32m[20221208 13:54:20 @agent_ppo2.py:179][0m |          -0.0366 |         232.4145 |           1.8020 |
[32m[20221208 13:54:21 @agent_ppo2.py:179][0m |          -0.0377 |         231.2452 |           1.8094 |
[32m[20221208 13:54:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.40
[32m[20221208 13:54:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.43
[32m[20221208 13:54:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.58
[32m[20221208 13:54:21 @agent_ppo2.py:137][0m Total time:       9.01 min
[32m[20221208 13:54:21 @agent_ppo2.py:139][0m 747520 total steps have happened
[32m[20221208 13:54:21 @agent_ppo2.py:115][0m #------------------------ Iteration 365 --------------------------#
[32m[20221208 13:54:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:21 @agent_ppo2.py:179][0m |           0.0172 |         183.9584 |           1.8057 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0104 |         175.1823 |           1.7886 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0224 |         172.4865 |           1.7910 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0256 |         171.9200 |           1.8060 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0316 |         169.7247 |           1.8149 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0290 |         169.4528 |           1.8165 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0296 |         169.2900 |           1.8067 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0371 |         167.7145 |           1.8197 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0380 |         167.3384 |           1.8255 |
[32m[20221208 13:54:22 @agent_ppo2.py:179][0m |          -0.0410 |         167.7829 |           1.8240 |
[32m[20221208 13:54:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 699.35
[32m[20221208 13:54:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.62
[32m[20221208 13:54:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.52
[32m[20221208 13:54:22 @agent_ppo2.py:137][0m Total time:       9.03 min
[32m[20221208 13:54:22 @agent_ppo2.py:139][0m 749568 total steps have happened
[32m[20221208 13:54:22 @agent_ppo2.py:115][0m #------------------------ Iteration 366 --------------------------#
[32m[20221208 13:54:23 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |           0.0114 |         256.6979 |           1.8460 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0089 |         246.4847 |           1.8437 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0254 |         241.3484 |           1.8512 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0295 |         239.6690 |           1.8632 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0288 |         236.9580 |           1.8656 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0381 |         236.5576 |           1.8687 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0401 |         234.9491 |           1.8669 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0423 |         234.2478 |           1.8752 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0418 |         233.0831 |           1.8801 |
[32m[20221208 13:54:23 @agent_ppo2.py:179][0m |          -0.0452 |         233.3594 |           1.8793 |
[32m[20221208 13:54:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:54:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 864.44
[32m[20221208 13:54:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.73
[32m[20221208 13:54:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.63
[32m[20221208 13:54:24 @agent_ppo2.py:137][0m Total time:       9.06 min
[32m[20221208 13:54:24 @agent_ppo2.py:139][0m 751616 total steps have happened
[32m[20221208 13:54:24 @agent_ppo2.py:115][0m #------------------------ Iteration 367 --------------------------#
[32m[20221208 13:54:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:24 @agent_ppo2.py:179][0m |           0.0186 |         210.5924 |           1.9139 |
[32m[20221208 13:54:24 @agent_ppo2.py:179][0m |          -0.0157 |         187.8980 |           1.8835 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0264 |         177.3283 |           1.8955 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0349 |         171.8887 |           1.9039 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0365 |         168.6686 |           1.9057 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0438 |         165.5586 |           1.9112 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0463 |         163.9621 |           1.9110 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0499 |         161.8578 |           1.9105 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0521 |         160.0560 |           1.9134 |
[32m[20221208 13:54:25 @agent_ppo2.py:179][0m |          -0.0538 |         158.6549 |           1.9169 |
[32m[20221208 13:54:25 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 643.30
[32m[20221208 13:54:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.92
[32m[20221208 13:54:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 654.75
[32m[20221208 13:54:25 @agent_ppo2.py:137][0m Total time:       9.08 min
[32m[20221208 13:54:25 @agent_ppo2.py:139][0m 753664 total steps have happened
[32m[20221208 13:54:25 @agent_ppo2.py:115][0m #------------------------ Iteration 368 --------------------------#
[32m[20221208 13:54:26 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |           0.0113 |         262.2898 |           1.8852 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0160 |         244.7002 |           1.8902 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0287 |         241.2565 |           1.8968 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0349 |         239.0919 |           1.9099 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0346 |         236.7175 |           1.9098 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0408 |         235.4891 |           1.9234 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0434 |         234.9023 |           1.9181 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0427 |         233.4822 |           1.9216 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0465 |         233.0420 |           1.9357 |
[32m[20221208 13:54:26 @agent_ppo2.py:179][0m |          -0.0475 |         232.8185 |           1.9392 |
[32m[20221208 13:54:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 926.41
[32m[20221208 13:54:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.18
[32m[20221208 13:54:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 963.26
[32m[20221208 13:54:27 @agent_ppo2.py:137][0m Total time:       9.11 min
[32m[20221208 13:54:27 @agent_ppo2.py:139][0m 755712 total steps have happened
[32m[20221208 13:54:27 @agent_ppo2.py:115][0m #------------------------ Iteration 369 --------------------------#
[32m[20221208 13:54:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:27 @agent_ppo2.py:179][0m |           0.0223 |         248.3484 |           1.8543 |
[32m[20221208 13:54:27 @agent_ppo2.py:179][0m |          -0.0072 |         243.3362 |           1.8577 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0137 |         241.4930 |           1.8596 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0198 |         239.6217 |           1.8542 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0265 |         240.0220 |           1.8653 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0294 |         238.5992 |           1.8686 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0301 |         237.5670 |           1.8678 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0348 |         236.9820 |           1.8771 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0358 |         237.5695 |           1.8821 |
[32m[20221208 13:54:28 @agent_ppo2.py:179][0m |          -0.0334 |         238.4170 |           1.8794 |
[32m[20221208 13:54:28 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.62
[32m[20221208 13:54:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 978.12
[32m[20221208 13:54:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.86
[32m[20221208 13:54:28 @agent_ppo2.py:137][0m Total time:       9.13 min
[32m[20221208 13:54:28 @agent_ppo2.py:139][0m 757760 total steps have happened
[32m[20221208 13:54:28 @agent_ppo2.py:115][0m #------------------------ Iteration 370 --------------------------#
[32m[20221208 13:54:29 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |           0.0360 |         267.4622 |           1.8521 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |           0.0108 |         247.9868 |           1.8352 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0175 |         237.9895 |           1.8677 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0371 |         230.6003 |           1.8863 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0334 |         226.9056 |           1.8986 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0362 |         222.1871 |           1.8986 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0470 |         219.3967 |           1.9147 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0454 |         217.6233 |           1.9235 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0496 |         215.0995 |           1.9264 |
[32m[20221208 13:54:29 @agent_ppo2.py:179][0m |          -0.0515 |         213.8001 |           1.9313 |
[32m[20221208 13:54:29 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:54:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 535.26
[32m[20221208 13:54:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.54
[32m[20221208 13:54:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.12
[32m[20221208 13:54:30 @agent_ppo2.py:137][0m Total time:       9.16 min
[32m[20221208 13:54:30 @agent_ppo2.py:139][0m 759808 total steps have happened
[32m[20221208 13:54:30 @agent_ppo2.py:115][0m #------------------------ Iteration 371 --------------------------#
[32m[20221208 13:54:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:30 @agent_ppo2.py:179][0m |           0.0152 |         265.6134 |           1.9303 |
[32m[20221208 13:54:30 @agent_ppo2.py:179][0m |           0.0064 |         252.2030 |           1.9099 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0196 |         247.8271 |           1.9175 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0320 |         244.7846 |           1.9182 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0353 |         242.7996 |           1.9166 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0419 |         241.4108 |           1.9192 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0445 |         240.2675 |           1.9224 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0453 |         240.0528 |           1.9283 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0485 |         238.2659 |           1.9238 |
[32m[20221208 13:54:31 @agent_ppo2.py:179][0m |          -0.0515 |         238.0736 |           1.9294 |
[32m[20221208 13:54:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 736.12
[32m[20221208 13:54:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 876.87
[32m[20221208 13:54:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.82
[32m[20221208 13:54:31 @agent_ppo2.py:137][0m Total time:       9.18 min
[32m[20221208 13:54:31 @agent_ppo2.py:139][0m 761856 total steps have happened
[32m[20221208 13:54:31 @agent_ppo2.py:115][0m #------------------------ Iteration 372 --------------------------#
[32m[20221208 13:54:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |           0.0109 |         235.7553 |           1.8790 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0046 |         229.8526 |           1.8689 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0165 |         225.1062 |           1.8763 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0265 |         222.6918 |           1.8864 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0278 |         221.3256 |           1.8866 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0313 |         220.6187 |           1.8956 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0327 |         220.9941 |           1.9057 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0347 |         219.5752 |           1.8967 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0374 |         219.5449 |           1.9077 |
[32m[20221208 13:54:32 @agent_ppo2.py:179][0m |          -0.0384 |         218.9920 |           1.9057 |
[32m[20221208 13:54:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:54:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.99
[32m[20221208 13:54:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.59
[32m[20221208 13:54:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 973.57
[32m[20221208 13:54:33 @agent_ppo2.py:137][0m Total time:       9.21 min
[32m[20221208 13:54:33 @agent_ppo2.py:139][0m 763904 total steps have happened
[32m[20221208 13:54:33 @agent_ppo2.py:115][0m #------------------------ Iteration 373 --------------------------#
[32m[20221208 13:54:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:33 @agent_ppo2.py:179][0m |           0.0121 |         248.5951 |           1.8845 |
[32m[20221208 13:54:33 @agent_ppo2.py:179][0m |          -0.0194 |         230.4269 |           1.8826 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0317 |         220.3023 |           1.8845 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0419 |         213.9872 |           1.8796 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0446 |         210.0864 |           1.8864 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0453 |         207.4391 |           1.8883 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0493 |         203.1280 |           1.8946 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0532 |         199.8957 |           1.8988 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0528 |         198.0679 |           1.9046 |
[32m[20221208 13:54:34 @agent_ppo2.py:179][0m |          -0.0554 |         194.9264 |           1.9080 |
[32m[20221208 13:54:34 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:54:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 790.27
[32m[20221208 13:54:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 938.94
[32m[20221208 13:54:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.29
[32m[20221208 13:54:34 @agent_ppo2.py:137][0m Total time:       9.23 min
[32m[20221208 13:54:34 @agent_ppo2.py:139][0m 765952 total steps have happened
[32m[20221208 13:54:34 @agent_ppo2.py:115][0m #------------------------ Iteration 374 --------------------------#
[32m[20221208 13:54:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |           0.0196 |         233.6867 |           1.9542 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0036 |         229.3854 |           1.8909 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0194 |         227.2031 |           1.9519 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0259 |         225.6796 |           1.9592 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0299 |         225.0261 |           1.9794 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0299 |         224.3413 |           1.9846 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0334 |         222.9457 |           1.9892 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0345 |         222.1696 |           1.9974 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0361 |         222.0883 |           1.9979 |
[32m[20221208 13:54:35 @agent_ppo2.py:179][0m |          -0.0395 |         221.4566 |           2.0091 |
[32m[20221208 13:54:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:54:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.67
[32m[20221208 13:54:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.75
[32m[20221208 13:54:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.81
[32m[20221208 13:54:36 @agent_ppo2.py:137][0m Total time:       9.26 min
[32m[20221208 13:54:36 @agent_ppo2.py:139][0m 768000 total steps have happened
[32m[20221208 13:54:36 @agent_ppo2.py:115][0m #------------------------ Iteration 375 --------------------------#
[32m[20221208 13:54:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:36 @agent_ppo2.py:179][0m |           0.0124 |         237.2955 |           1.9835 |
[32m[20221208 13:54:36 @agent_ppo2.py:179][0m |          -0.0164 |         230.3033 |           1.9824 |
[32m[20221208 13:54:36 @agent_ppo2.py:179][0m |          -0.0276 |         228.3455 |           1.9946 |
[32m[20221208 13:54:37 @agent_ppo2.py:179][0m |          -0.0323 |         227.3002 |           2.0091 |
[32m[20221208 13:54:37 @agent_ppo2.py:179][0m |          -0.0354 |         224.8432 |           2.0186 |
[32m[20221208 13:54:37 @agent_ppo2.py:179][0m |          -0.0396 |         223.2604 |           2.0166 |
[32m[20221208 13:54:37 @agent_ppo2.py:179][0m |          -0.0407 |         223.3840 |           2.0224 |
[32m[20221208 13:54:37 @agent_ppo2.py:179][0m |          -0.0448 |         222.4366 |           2.0369 |
[32m[20221208 13:54:37 @agent_ppo2.py:179][0m |          -0.0450 |         221.5301 |           2.0345 |
[32m[20221208 13:54:37 @agent_ppo2.py:179][0m |          -0.0483 |         220.3873 |           2.0440 |
[32m[20221208 13:54:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 877.79
[32m[20221208 13:54:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.79
[32m[20221208 13:54:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 680.36
[32m[20221208 13:54:37 @agent_ppo2.py:137][0m Total time:       9.28 min
[32m[20221208 13:54:37 @agent_ppo2.py:139][0m 770048 total steps have happened
[32m[20221208 13:54:37 @agent_ppo2.py:115][0m #------------------------ Iteration 376 --------------------------#
[32m[20221208 13:54:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |           0.0151 |         248.8703 |           2.0481 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0128 |         238.9051 |           2.0504 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0264 |         235.0939 |           2.0571 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0338 |         233.8580 |           2.0761 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0380 |         231.1806 |           2.0792 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0398 |         228.8151 |           2.0796 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0427 |         225.6718 |           2.0898 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0417 |         224.4356 |           2.0907 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0436 |         223.2931 |           2.0987 |
[32m[20221208 13:54:38 @agent_ppo2.py:179][0m |          -0.0456 |         222.1633 |           2.1013 |
[32m[20221208 13:54:38 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:54:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 878.70
[32m[20221208 13:54:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.14
[32m[20221208 13:54:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 652.70
[32m[20221208 13:54:39 @agent_ppo2.py:137][0m Total time:       9.31 min
[32m[20221208 13:54:39 @agent_ppo2.py:139][0m 772096 total steps have happened
[32m[20221208 13:54:39 @agent_ppo2.py:115][0m #------------------------ Iteration 377 --------------------------#
[32m[20221208 13:54:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:39 @agent_ppo2.py:179][0m |           0.0247 |         224.0285 |           2.0767 |
[32m[20221208 13:54:39 @agent_ppo2.py:179][0m |          -0.0095 |         193.0124 |           2.0764 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0235 |         179.5347 |           2.0963 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0285 |         170.5375 |           2.1058 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0330 |         162.9596 |           2.1167 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0355 |         157.9870 |           2.1305 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0413 |         153.6632 |           2.1283 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0423 |         152.7308 |           2.1399 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0409 |         149.8949 |           2.1579 |
[32m[20221208 13:54:40 @agent_ppo2.py:179][0m |          -0.0439 |         147.3032 |           2.1513 |
[32m[20221208 13:54:40 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:54:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 741.10
[32m[20221208 13:54:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.88
[32m[20221208 13:54:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.90
[32m[20221208 13:54:40 @agent_ppo2.py:137][0m Total time:       9.33 min
[32m[20221208 13:54:40 @agent_ppo2.py:139][0m 774144 total steps have happened
[32m[20221208 13:54:40 @agent_ppo2.py:115][0m #------------------------ Iteration 378 --------------------------#
[32m[20221208 13:54:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:54:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |           0.0202 |         240.1341 |           2.1165 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0085 |         218.9658 |           2.1192 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0293 |         214.0557 |           2.1296 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0377 |         210.9423 |           2.1319 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0386 |         209.9114 |           2.1379 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0405 |         206.0144 |           2.1354 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0453 |         205.9940 |           2.1439 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0496 |         203.1533 |           2.1530 |
[32m[20221208 13:54:41 @agent_ppo2.py:179][0m |          -0.0520 |         203.1635 |           2.1568 |
[32m[20221208 13:54:42 @agent_ppo2.py:179][0m |          -0.0528 |         202.2392 |           2.1597 |
[32m[20221208 13:54:42 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:54:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 763.82
[32m[20221208 13:54:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.99
[32m[20221208 13:54:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.41
[32m[20221208 13:54:42 @agent_ppo2.py:137][0m Total time:       9.36 min
[32m[20221208 13:54:42 @agent_ppo2.py:139][0m 776192 total steps have happened
[32m[20221208 13:54:42 @agent_ppo2.py:115][0m #------------------------ Iteration 379 --------------------------#
[32m[20221208 13:54:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:42 @agent_ppo2.py:179][0m |           0.0319 |         246.9893 |           2.2210 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |           0.0114 |         233.0450 |           2.1779 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0217 |         226.1343 |           2.2265 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0238 |         223.2961 |           2.2366 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0311 |         220.4144 |           2.2409 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0375 |         220.7051 |           2.2477 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0400 |         218.0113 |           2.2556 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0413 |         216.4017 |           2.2655 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0453 |         216.8069 |           2.2643 |
[32m[20221208 13:54:43 @agent_ppo2.py:179][0m |          -0.0458 |         215.5141 |           2.2686 |
[32m[20221208 13:54:43 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:54:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.31
[32m[20221208 13:54:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.94
[32m[20221208 13:54:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 895.41
[32m[20221208 13:54:43 @agent_ppo2.py:137][0m Total time:       9.38 min
[32m[20221208 13:54:43 @agent_ppo2.py:139][0m 778240 total steps have happened
[32m[20221208 13:54:43 @agent_ppo2.py:115][0m #------------------------ Iteration 380 --------------------------#
[32m[20221208 13:54:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |           0.0229 |         251.9195 |           2.2230 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |           0.0019 |         244.5139 |           2.1828 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |          -0.0214 |         242.7688 |           2.2722 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |          -0.0289 |         241.0971 |           2.2796 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |          -0.0333 |         240.1340 |           2.2974 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |          -0.0312 |         239.6536 |           2.2909 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |          -0.0326 |         239.1919 |           2.3051 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |          -0.0347 |         238.5105 |           2.3026 |
[32m[20221208 13:54:44 @agent_ppo2.py:179][0m |          -0.0317 |         238.6469 |           2.3014 |
[32m[20221208 13:54:45 @agent_ppo2.py:179][0m |          -0.0379 |         237.5755 |           2.3108 |
[32m[20221208 13:54:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 968.76
[32m[20221208 13:54:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.94
[32m[20221208 13:54:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.48
[32m[20221208 13:54:45 @agent_ppo2.py:137][0m Total time:       9.41 min
[32m[20221208 13:54:45 @agent_ppo2.py:139][0m 780288 total steps have happened
[32m[20221208 13:54:45 @agent_ppo2.py:115][0m #------------------------ Iteration 381 --------------------------#
[32m[20221208 13:54:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:54:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |           0.0199 |         240.6657 |           2.2540 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |           0.0115 |         234.0519 |           2.2082 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0133 |         230.7794 |           2.2578 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0214 |         229.0616 |           2.2822 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0259 |         228.6199 |           2.2873 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0272 |         226.6586 |           2.3040 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0304 |         225.5700 |           2.3037 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0327 |         224.6672 |           2.3095 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0343 |         223.4177 |           2.3167 |
[32m[20221208 13:54:46 @agent_ppo2.py:179][0m |          -0.0336 |         222.7622 |           2.3164 |
[32m[20221208 13:54:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.34
[32m[20221208 13:54:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.08
[32m[20221208 13:54:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.68
[32m[20221208 13:54:46 @agent_ppo2.py:137][0m Total time:       9.43 min
[32m[20221208 13:54:46 @agent_ppo2.py:139][0m 782336 total steps have happened
[32m[20221208 13:54:46 @agent_ppo2.py:115][0m #------------------------ Iteration 382 --------------------------#
[32m[20221208 13:54:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |           0.0109 |         256.7460 |           2.2289 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0101 |         247.0012 |           2.2174 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0246 |         242.9959 |           2.2524 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0307 |         240.9772 |           2.2546 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0331 |         239.1171 |           2.2588 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0375 |         236.9768 |           2.2685 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0382 |         237.3476 |           2.2816 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0409 |         235.9438 |           2.2762 |
[32m[20221208 13:54:47 @agent_ppo2.py:179][0m |          -0.0422 |         235.7478 |           2.2933 |
[32m[20221208 13:54:48 @agent_ppo2.py:179][0m |          -0.0449 |         237.2986 |           2.2942 |
[32m[20221208 13:54:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.68
[32m[20221208 13:54:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.28
[32m[20221208 13:54:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.69
[32m[20221208 13:54:48 @agent_ppo2.py:137][0m Total time:       9.46 min
[32m[20221208 13:54:48 @agent_ppo2.py:139][0m 784384 total steps have happened
[32m[20221208 13:54:48 @agent_ppo2.py:115][0m #------------------------ Iteration 383 --------------------------#
[32m[20221208 13:54:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:48 @agent_ppo2.py:179][0m |           0.0265 |         250.6704 |           2.3068 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0113 |         241.7589 |           2.3152 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0215 |         237.8738 |           2.3098 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0295 |         235.5971 |           2.3235 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0358 |         233.6123 |           2.3307 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0352 |         232.1463 |           2.3271 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0373 |         230.6840 |           2.3280 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0391 |         229.7994 |           2.3270 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0400 |         230.1324 |           2.3235 |
[32m[20221208 13:54:49 @agent_ppo2.py:179][0m |          -0.0421 |         228.6315 |           2.3308 |
[32m[20221208 13:54:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 851.22
[32m[20221208 13:54:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.78
[32m[20221208 13:54:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 503.06
[32m[20221208 13:54:49 @agent_ppo2.py:137][0m Total time:       9.48 min
[32m[20221208 13:54:49 @agent_ppo2.py:139][0m 786432 total steps have happened
[32m[20221208 13:54:49 @agent_ppo2.py:115][0m #------------------------ Iteration 384 --------------------------#
[32m[20221208 13:54:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |           0.0285 |         225.8610 |           2.3380 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |           0.0055 |         202.5345 |           2.3186 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0190 |         194.5652 |           2.3450 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0242 |         186.9477 |           2.3488 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0322 |         184.0314 |           2.3627 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0318 |         179.0732 |           2.3638 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0374 |         174.7973 |           2.3654 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0395 |         168.4844 |           2.3759 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0408 |         165.4822 |           2.3815 |
[32m[20221208 13:54:50 @agent_ppo2.py:179][0m |          -0.0438 |         161.8358 |           2.3827 |
[32m[20221208 13:54:50 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:54:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 871.34
[32m[20221208 13:54:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.80
[32m[20221208 13:54:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 974.80
[32m[20221208 13:54:51 @agent_ppo2.py:137][0m Total time:       9.51 min
[32m[20221208 13:54:51 @agent_ppo2.py:139][0m 788480 total steps have happened
[32m[20221208 13:54:51 @agent_ppo2.py:115][0m #------------------------ Iteration 385 --------------------------#
[32m[20221208 13:54:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:51 @agent_ppo2.py:179][0m |           0.0194 |         266.0148 |           2.2930 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0081 |         257.1227 |           2.2659 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0242 |         253.4302 |           2.2982 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0332 |         251.8749 |           2.3144 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0351 |         251.3120 |           2.3207 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0406 |         248.3587 |           2.3410 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0420 |         247.9841 |           2.3490 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0453 |         247.4914 |           2.3487 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0486 |         246.1415 |           2.3664 |
[32m[20221208 13:54:52 @agent_ppo2.py:179][0m |          -0.0485 |         244.7245 |           2.3707 |
[32m[20221208 13:54:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 930.49
[32m[20221208 13:54:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 961.21
[32m[20221208 13:54:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 975.73
[32m[20221208 13:54:52 @agent_ppo2.py:137][0m Total time:       9.53 min
[32m[20221208 13:54:52 @agent_ppo2.py:139][0m 790528 total steps have happened
[32m[20221208 13:54:52 @agent_ppo2.py:115][0m #------------------------ Iteration 386 --------------------------#
[32m[20221208 13:54:53 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |           0.0240 |         220.9582 |           2.4075 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |           0.0155 |         202.3455 |           2.3060 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0121 |         193.5737 |           2.4189 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0239 |         184.6045 |           2.4705 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0281 |         177.1041 |           2.4668 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0319 |         171.9666 |           2.4506 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0373 |         168.2514 |           2.4628 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0415 |         164.8209 |           2.4905 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0408 |         163.0555 |           2.4959 |
[32m[20221208 13:54:53 @agent_ppo2.py:179][0m |          -0.0482 |         162.9323 |           2.5082 |
[32m[20221208 13:54:53 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:54:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 549.86
[32m[20221208 13:54:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.34
[32m[20221208 13:54:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.50
[32m[20221208 13:54:54 @agent_ppo2.py:137][0m Total time:       9.56 min
[32m[20221208 13:54:54 @agent_ppo2.py:139][0m 792576 total steps have happened
[32m[20221208 13:54:54 @agent_ppo2.py:115][0m #------------------------ Iteration 387 --------------------------#
[32m[20221208 13:54:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:54 @agent_ppo2.py:179][0m |           0.0167 |         255.4427 |           2.4024 |
[32m[20221208 13:54:54 @agent_ppo2.py:179][0m |           0.0203 |         250.9837 |           2.3476 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0127 |         247.7110 |           2.3967 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0233 |         246.6995 |           2.4209 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0238 |         245.2749 |           2.4263 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0229 |         244.3432 |           2.4009 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0312 |         243.8370 |           2.4282 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0345 |         243.7153 |           2.4437 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0340 |         243.3661 |           2.4501 |
[32m[20221208 13:54:55 @agent_ppo2.py:179][0m |          -0.0360 |         243.2474 |           2.4462 |
[32m[20221208 13:54:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.01
[32m[20221208 13:54:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.40
[32m[20221208 13:54:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.69
[32m[20221208 13:54:55 @agent_ppo2.py:137][0m Total time:       9.58 min
[32m[20221208 13:54:55 @agent_ppo2.py:139][0m 794624 total steps have happened
[32m[20221208 13:54:55 @agent_ppo2.py:115][0m #------------------------ Iteration 388 --------------------------#
[32m[20221208 13:54:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:54:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |           0.0269 |         253.7121 |           2.4049 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0079 |         247.8706 |           2.3993 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0227 |         244.6274 |           2.4116 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0264 |         243.9772 |           2.4158 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0322 |         242.6686 |           2.4284 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0357 |         241.9303 |           2.4433 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0385 |         240.9163 |           2.4536 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0391 |         240.4712 |           2.4586 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0393 |         240.8140 |           2.4646 |
[32m[20221208 13:54:56 @agent_ppo2.py:179][0m |          -0.0404 |         239.7058 |           2.4693 |
[32m[20221208 13:54:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:54:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 932.23
[32m[20221208 13:54:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.73
[32m[20221208 13:54:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 688.12
[32m[20221208 13:54:57 @agent_ppo2.py:137][0m Total time:       9.61 min
[32m[20221208 13:54:57 @agent_ppo2.py:139][0m 796672 total steps have happened
[32m[20221208 13:54:57 @agent_ppo2.py:115][0m #------------------------ Iteration 389 --------------------------#
[32m[20221208 13:54:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:57 @agent_ppo2.py:179][0m |           0.0193 |         252.1049 |           2.4532 |
[32m[20221208 13:54:57 @agent_ppo2.py:179][0m |          -0.0044 |         246.1952 |           2.4559 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0243 |         242.2298 |           2.4776 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0289 |         241.0985 |           2.4727 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0347 |         239.3832 |           2.4699 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0352 |         238.0220 |           2.4696 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0400 |         236.6178 |           2.4736 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0406 |         237.0619 |           2.4833 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0428 |         235.6651 |           2.4890 |
[32m[20221208 13:54:58 @agent_ppo2.py:179][0m |          -0.0445 |         236.0262 |           2.4985 |
[32m[20221208 13:54:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:54:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 861.83
[32m[20221208 13:54:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 877.11
[32m[20221208 13:54:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.75
[32m[20221208 13:54:58 @agent_ppo2.py:137][0m Total time:       9.63 min
[32m[20221208 13:54:58 @agent_ppo2.py:139][0m 798720 total steps have happened
[32m[20221208 13:54:58 @agent_ppo2.py:115][0m #------------------------ Iteration 390 --------------------------#
[32m[20221208 13:54:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:54:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |           0.0159 |         245.4030 |           2.5065 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0031 |         234.1142 |           2.4363 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0198 |         228.1630 |           2.4947 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0273 |         222.8493 |           2.5093 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0297 |         218.9742 |           2.5239 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0331 |         215.1488 |           2.5164 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0351 |         213.0040 |           2.5407 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0345 |         209.6790 |           2.5359 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0368 |         208.3756 |           2.5516 |
[32m[20221208 13:54:59 @agent_ppo2.py:179][0m |          -0.0384 |         206.3848 |           2.5578 |
[32m[20221208 13:54:59 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:55:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.29
[32m[20221208 13:55:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.96
[32m[20221208 13:55:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.82
[32m[20221208 13:55:00 @agent_ppo2.py:137][0m Total time:       9.66 min
[32m[20221208 13:55:00 @agent_ppo2.py:139][0m 800768 total steps have happened
[32m[20221208 13:55:00 @agent_ppo2.py:115][0m #------------------------ Iteration 391 --------------------------#
[32m[20221208 13:55:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:00 @agent_ppo2.py:179][0m |           0.0182 |         242.2378 |           2.4605 |
[32m[20221208 13:55:00 @agent_ppo2.py:179][0m |          -0.0117 |         218.7497 |           2.4238 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0302 |         211.9657 |           2.4560 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0377 |         207.9799 |           2.4731 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0408 |         206.4562 |           2.4680 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0427 |         204.7647 |           2.4662 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0480 |         202.8258 |           2.4809 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0495 |         201.6849 |           2.4908 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0514 |         201.4434 |           2.4980 |
[32m[20221208 13:55:01 @agent_ppo2.py:179][0m |          -0.0533 |         200.5903 |           2.5020 |
[32m[20221208 13:55:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:55:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 875.53
[32m[20221208 13:55:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 921.20
[32m[20221208 13:55:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 927.04
[32m[20221208 13:55:01 @agent_ppo2.py:137][0m Total time:       9.68 min
[32m[20221208 13:55:01 @agent_ppo2.py:139][0m 802816 total steps have happened
[32m[20221208 13:55:01 @agent_ppo2.py:115][0m #------------------------ Iteration 392 --------------------------#
[32m[20221208 13:55:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |           0.0165 |         252.6887 |           2.4993 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0065 |         241.1016 |           2.4732 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0210 |         236.2781 |           2.4923 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0289 |         232.3524 |           2.5179 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0364 |         230.9111 |           2.5391 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0386 |         229.6738 |           2.5408 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0425 |         227.8757 |           2.5519 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0402 |         227.6309 |           2.5493 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0432 |         226.2856 |           2.5653 |
[32m[20221208 13:55:02 @agent_ppo2.py:179][0m |          -0.0470 |         224.4538 |           2.5828 |
[32m[20221208 13:55:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 932.64
[32m[20221208 13:55:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 968.32
[32m[20221208 13:55:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.65
[32m[20221208 13:55:03 @agent_ppo2.py:137][0m Total time:       9.71 min
[32m[20221208 13:55:03 @agent_ppo2.py:139][0m 804864 total steps have happened
[32m[20221208 13:55:03 @agent_ppo2.py:115][0m #------------------------ Iteration 393 --------------------------#
[32m[20221208 13:55:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:03 @agent_ppo2.py:179][0m |           0.0191 |         251.0925 |           2.4580 |
[32m[20221208 13:55:03 @agent_ppo2.py:179][0m |          -0.0031 |         244.8755 |           2.3984 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0185 |         243.0827 |           2.4641 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0135 |         240.8994 |           2.4406 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0267 |         240.4004 |           2.4637 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0293 |         239.8293 |           2.4530 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0319 |         239.0843 |           2.4682 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0314 |         240.3421 |           2.4624 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0355 |         238.8678 |           2.4768 |
[32m[20221208 13:55:04 @agent_ppo2.py:179][0m |          -0.0377 |         239.1781 |           2.4867 |
[32m[20221208 13:55:04 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:55:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.99
[32m[20221208 13:55:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.68
[32m[20221208 13:55:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 682.63
[32m[20221208 13:55:04 @agent_ppo2.py:137][0m Total time:       9.73 min
[32m[20221208 13:55:04 @agent_ppo2.py:139][0m 806912 total steps have happened
[32m[20221208 13:55:04 @agent_ppo2.py:115][0m #------------------------ Iteration 394 --------------------------#
[32m[20221208 13:55:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |           0.0220 |         236.3698 |           2.4103 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0038 |         225.2320 |           2.3837 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0262 |         218.7729 |           2.4367 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0333 |         214.6498 |           2.4602 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0344 |         212.7623 |           2.4588 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0420 |         209.8373 |           2.4661 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0438 |         208.6429 |           2.4789 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0465 |         205.7152 |           2.4848 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0477 |         202.7913 |           2.4881 |
[32m[20221208 13:55:05 @agent_ppo2.py:179][0m |          -0.0492 |         200.6435 |           2.5064 |
[32m[20221208 13:55:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:55:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 899.42
[32m[20221208 13:55:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.60
[32m[20221208 13:55:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.47
[32m[20221208 13:55:06 @agent_ppo2.py:137][0m Total time:       9.76 min
[32m[20221208 13:55:06 @agent_ppo2.py:139][0m 808960 total steps have happened
[32m[20221208 13:55:06 @agent_ppo2.py:115][0m #------------------------ Iteration 395 --------------------------#
[32m[20221208 13:55:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:06 @agent_ppo2.py:179][0m |           0.0198 |         249.0920 |           2.5408 |
[32m[20221208 13:55:06 @agent_ppo2.py:179][0m |          -0.0078 |         241.9660 |           2.4994 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0213 |         238.8017 |           2.5781 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0271 |         237.2508 |           2.5905 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0293 |         236.6878 |           2.5967 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0348 |         236.3479 |           2.6194 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0357 |         234.6170 |           2.6260 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0350 |         234.8549 |           2.6350 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0373 |         234.0590 |           2.6415 |
[32m[20221208 13:55:07 @agent_ppo2.py:179][0m |          -0.0363 |         233.6836 |           2.6398 |
[32m[20221208 13:55:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:55:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 967.82
[32m[20221208 13:55:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.65
[32m[20221208 13:55:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 635.21
[32m[20221208 13:55:07 @agent_ppo2.py:137][0m Total time:       9.78 min
[32m[20221208 13:55:07 @agent_ppo2.py:139][0m 811008 total steps have happened
[32m[20221208 13:55:07 @agent_ppo2.py:115][0m #------------------------ Iteration 396 --------------------------#
[32m[20221208 13:55:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |           0.0209 |         254.2286 |           2.4586 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |           0.0199 |         247.0392 |           2.3978 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0180 |         245.0796 |           2.4629 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0302 |         243.2976 |           2.4938 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0325 |         242.2926 |           2.5019 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0371 |         240.7830 |           2.5013 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0376 |         240.4890 |           2.5154 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0439 |         239.0292 |           2.5244 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0444 |         238.8142 |           2.5277 |
[32m[20221208 13:55:08 @agent_ppo2.py:179][0m |          -0.0427 |         238.5221 |           2.5359 |
[32m[20221208 13:55:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.06
[32m[20221208 13:55:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 917.61
[32m[20221208 13:55:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.16
[32m[20221208 13:55:09 @agent_ppo2.py:137][0m Total time:       9.81 min
[32m[20221208 13:55:09 @agent_ppo2.py:139][0m 813056 total steps have happened
[32m[20221208 13:55:09 @agent_ppo2.py:115][0m #------------------------ Iteration 397 --------------------------#
[32m[20221208 13:55:09 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:55:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:09 @agent_ppo2.py:179][0m |           0.0239 |         271.0762 |           2.5593 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0058 |         261.8131 |           2.5405 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0211 |         257.6669 |           2.5714 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0359 |         254.0062 |           2.6046 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0369 |         249.0812 |           2.6179 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0410 |         242.8684 |           2.6340 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0441 |         238.7214 |           2.6310 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0437 |         237.7742 |           2.6455 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0475 |         237.2966 |           2.6551 |
[32m[20221208 13:55:10 @agent_ppo2.py:179][0m |          -0.0482 |         236.2353 |           2.6630 |
[32m[20221208 13:55:10 @agent_ppo2.py:124][0m Policy update time: 0.83 s
[32m[20221208 13:55:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.35
[32m[20221208 13:55:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.98
[32m[20221208 13:55:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.38
[32m[20221208 13:55:11 @agent_ppo2.py:137][0m Total time:       9.84 min
[32m[20221208 13:55:11 @agent_ppo2.py:139][0m 815104 total steps have happened
[32m[20221208 13:55:11 @agent_ppo2.py:115][0m #------------------------ Iteration 398 --------------------------#
[32m[20221208 13:55:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:55:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:11 @agent_ppo2.py:179][0m |           0.0124 |         237.9662 |           2.6247 |
[32m[20221208 13:55:11 @agent_ppo2.py:179][0m |          -0.0077 |         228.6002 |           2.6128 |
[32m[20221208 13:55:11 @agent_ppo2.py:179][0m |          -0.0211 |         224.1737 |           2.6249 |
[32m[20221208 13:55:11 @agent_ppo2.py:179][0m |          -0.0263 |         218.5633 |           2.6380 |
[32m[20221208 13:55:11 @agent_ppo2.py:179][0m |          -0.0280 |         217.0605 |           2.6390 |
[32m[20221208 13:55:11 @agent_ppo2.py:179][0m |          -0.0308 |         214.9460 |           2.6327 |
[32m[20221208 13:55:11 @agent_ppo2.py:179][0m |          -0.0357 |         212.4918 |           2.6334 |
[32m[20221208 13:55:12 @agent_ppo2.py:179][0m |          -0.0398 |         211.2865 |           2.6618 |
[32m[20221208 13:55:12 @agent_ppo2.py:179][0m |          -0.0414 |         209.6179 |           2.6728 |
[32m[20221208 13:55:12 @agent_ppo2.py:179][0m |          -0.0410 |         208.6053 |           2.6655 |
[32m[20221208 13:55:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:55:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.80
[32m[20221208 13:55:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.37
[32m[20221208 13:55:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.18
[32m[20221208 13:55:12 @agent_ppo2.py:137][0m Total time:       9.86 min
[32m[20221208 13:55:12 @agent_ppo2.py:139][0m 817152 total steps have happened
[32m[20221208 13:55:12 @agent_ppo2.py:115][0m #------------------------ Iteration 399 --------------------------#
[32m[20221208 13:55:13 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 13:55:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |           0.0347 |         253.1997 |           2.5622 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |           0.0193 |         245.0404 |           2.4753 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0165 |         242.1601 |           2.6024 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0247 |         236.4875 |           2.6413 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0294 |         234.7807 |           2.6488 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0333 |         233.3285 |           2.6620 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0321 |         232.7060 |           2.6702 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0350 |         230.6114 |           2.6733 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0389 |         229.5658 |           2.6846 |
[32m[20221208 13:55:13 @agent_ppo2.py:179][0m |          -0.0409 |         228.2729 |           2.6962 |
[32m[20221208 13:55:13 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:55:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 955.06
[32m[20221208 13:55:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.53
[32m[20221208 13:55:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 637.55
[32m[20221208 13:55:14 @agent_ppo2.py:137][0m Total time:       9.89 min
[32m[20221208 13:55:14 @agent_ppo2.py:139][0m 819200 total steps have happened
[32m[20221208 13:55:14 @agent_ppo2.py:115][0m #------------------------ Iteration 400 --------------------------#
[32m[20221208 13:55:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:14 @agent_ppo2.py:179][0m |           0.0430 |         213.0084 |           2.4941 |
[32m[20221208 13:55:14 @agent_ppo2.py:179][0m |           0.0291 |         186.6228 |           2.5483 |
[32m[20221208 13:55:14 @agent_ppo2.py:179][0m |          -0.0125 |         171.6265 |           2.6650 |
[32m[20221208 13:55:14 @agent_ppo2.py:179][0m |          -0.0220 |         160.7216 |           2.6839 |
[32m[20221208 13:55:15 @agent_ppo2.py:179][0m |          -0.0249 |         155.5585 |           2.7017 |
[32m[20221208 13:55:15 @agent_ppo2.py:179][0m |          -0.0267 |         151.7634 |           2.7212 |
[32m[20221208 13:55:15 @agent_ppo2.py:179][0m |          -0.0321 |         149.5282 |           2.7383 |
[32m[20221208 13:55:15 @agent_ppo2.py:179][0m |          -0.0338 |         147.7671 |           2.7447 |
[32m[20221208 13:55:15 @agent_ppo2.py:179][0m |          -0.0316 |         145.3958 |           2.7246 |
[32m[20221208 13:55:15 @agent_ppo2.py:179][0m |          -0.0341 |         144.1643 |           2.7206 |
[32m[20221208 13:55:15 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 13:55:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 633.53
[32m[20221208 13:55:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 928.47
[32m[20221208 13:55:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.15
[32m[20221208 13:55:15 @agent_ppo2.py:137][0m Total time:       9.92 min
[32m[20221208 13:55:15 @agent_ppo2.py:139][0m 821248 total steps have happened
[32m[20221208 13:55:15 @agent_ppo2.py:115][0m #------------------------ Iteration 401 --------------------------#
[32m[20221208 13:55:16 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 13:55:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |           0.0629 |         269.1220 |           2.6251 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |           0.0203 |         256.2525 |           2.5371 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0042 |         243.8059 |           2.6720 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0212 |         236.7190 |           2.7747 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0289 |         230.7602 |           2.7999 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0347 |         225.0617 |           2.8112 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0376 |         220.6512 |           2.8336 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0391 |         217.7110 |           2.8425 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0404 |         214.8613 |           2.8517 |
[32m[20221208 13:55:16 @agent_ppo2.py:179][0m |          -0.0427 |         211.2666 |           2.8584 |
[32m[20221208 13:55:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:55:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.82
[32m[20221208 13:55:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.85
[32m[20221208 13:55:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 375.76
[32m[20221208 13:55:17 @agent_ppo2.py:137][0m Total time:       9.94 min
[32m[20221208 13:55:17 @agent_ppo2.py:139][0m 823296 total steps have happened
[32m[20221208 13:55:17 @agent_ppo2.py:115][0m #------------------------ Iteration 402 --------------------------#
[32m[20221208 13:55:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:17 @agent_ppo2.py:179][0m |           0.0156 |         280.1809 |           2.8664 |
[32m[20221208 13:55:17 @agent_ppo2.py:179][0m |          -0.0146 |         241.3865 |           2.8409 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0232 |         229.0171 |           2.8521 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0293 |         221.5987 |           2.8724 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0380 |         218.5286 |           2.8837 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0356 |         216.0624 |           2.8817 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0450 |         215.0168 |           2.8947 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0489 |         213.4631 |           2.9005 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0491 |         211.7769 |           2.9109 |
[32m[20221208 13:55:18 @agent_ppo2.py:179][0m |          -0.0517 |         211.4835 |           2.9106 |
[32m[20221208 13:55:18 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:55:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 595.73
[32m[20221208 13:55:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 885.88
[32m[20221208 13:55:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.45
[32m[20221208 13:55:18 @agent_ppo2.py:137][0m Total time:       9.97 min
[32m[20221208 13:55:18 @agent_ppo2.py:139][0m 825344 total steps have happened
[32m[20221208 13:55:18 @agent_ppo2.py:115][0m #------------------------ Iteration 403 --------------------------#
[32m[20221208 13:55:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |           0.0193 |         266.2463 |           2.8802 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |           0.0042 |         253.2762 |           2.8318 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |          -0.0222 |         247.4197 |           2.8968 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |          -0.0311 |         244.4762 |           2.9039 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |          -0.0325 |         242.0018 |           2.8994 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |          -0.0398 |         240.3516 |           2.9142 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |          -0.0416 |         238.9693 |           2.9082 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |          -0.0411 |         237.8591 |           2.9151 |
[32m[20221208 13:55:19 @agent_ppo2.py:179][0m |          -0.0443 |         237.8652 |           2.9237 |
[32m[20221208 13:55:20 @agent_ppo2.py:179][0m |          -0.0439 |         236.5156 |           2.9290 |
[32m[20221208 13:55:20 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:55:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 931.75
[32m[20221208 13:55:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 943.85
[32m[20221208 13:55:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.08
[32m[20221208 13:55:20 @agent_ppo2.py:137][0m Total time:       9.99 min
[32m[20221208 13:55:20 @agent_ppo2.py:139][0m 827392 total steps have happened
[32m[20221208 13:55:20 @agent_ppo2.py:115][0m #------------------------ Iteration 404 --------------------------#
[32m[20221208 13:55:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:20 @agent_ppo2.py:179][0m |           0.0157 |         220.5570 |           2.8783 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0044 |         206.3481 |           2.8620 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0142 |         203.5670 |           2.8661 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0170 |         201.9682 |           2.8664 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0244 |         201.1850 |           2.8797 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0293 |         200.3567 |           2.8975 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0308 |         199.6737 |           2.9086 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0373 |         199.1269 |           2.9262 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0339 |         198.7809 |           2.9139 |
[32m[20221208 13:55:21 @agent_ppo2.py:179][0m |          -0.0389 |         198.1943 |           2.9344 |
[32m[20221208 13:55:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 628.05
[32m[20221208 13:55:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.16
[32m[20221208 13:55:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 653.55
[32m[20221208 13:55:21 @agent_ppo2.py:137][0m Total time:      10.02 min
[32m[20221208 13:55:21 @agent_ppo2.py:139][0m 829440 total steps have happened
[32m[20221208 13:55:21 @agent_ppo2.py:115][0m #------------------------ Iteration 405 --------------------------#
[32m[20221208 13:55:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |           0.0204 |         247.8341 |           2.8340 |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |           0.0004 |         241.5380 |           2.7794 |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |          -0.0180 |         235.8186 |           2.8303 |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |          -0.0260 |         231.5680 |           2.8438 |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |          -0.0312 |         230.3886 |           2.8479 |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |          -0.0330 |         230.4361 |           2.8547 |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |          -0.0332 |         229.3459 |           2.8627 |
[32m[20221208 13:55:22 @agent_ppo2.py:179][0m |          -0.0334 |         228.6328 |           2.8564 |
[32m[20221208 13:55:23 @agent_ppo2.py:179][0m |          -0.0378 |         229.1077 |           2.8767 |
[32m[20221208 13:55:23 @agent_ppo2.py:179][0m |          -0.0358 |         227.4789 |           2.8774 |
[32m[20221208 13:55:23 @agent_ppo2.py:124][0m Policy update time: 0.81 s
[32m[20221208 13:55:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 972.18
[32m[20221208 13:55:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.77
[32m[20221208 13:55:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 969.90
[32m[20221208 13:55:23 @agent_ppo2.py:137][0m Total time:      10.05 min
[32m[20221208 13:55:23 @agent_ppo2.py:139][0m 831488 total steps have happened
[32m[20221208 13:55:23 @agent_ppo2.py:115][0m #------------------------ Iteration 406 --------------------------#
[32m[20221208 13:55:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |           0.0158 |         251.4240 |           2.7900 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0031 |         237.8574 |           2.8102 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0181 |         235.1664 |           2.8350 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0238 |         232.9018 |           2.8387 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0287 |         231.9221 |           2.8548 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0290 |         231.1225 |           2.8435 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0338 |         231.6691 |           2.8603 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0310 |         230.1317 |           2.8577 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0355 |         230.5770 |           2.8612 |
[32m[20221208 13:55:24 @agent_ppo2.py:179][0m |          -0.0382 |         230.0236 |           2.8712 |
[32m[20221208 13:55:24 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:55:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 969.52
[32m[20221208 13:55:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.62
[32m[20221208 13:55:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.51
[32m[20221208 13:55:25 @agent_ppo2.py:137][0m Total time:      10.07 min
[32m[20221208 13:55:25 @agent_ppo2.py:139][0m 833536 total steps have happened
[32m[20221208 13:55:25 @agent_ppo2.py:115][0m #------------------------ Iteration 407 --------------------------#
[32m[20221208 13:55:25 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:25 @agent_ppo2.py:179][0m |           0.0176 |         254.3920 |           2.7761 |
[32m[20221208 13:55:25 @agent_ppo2.py:179][0m |          -0.0084 |         248.5236 |           2.7527 |
[32m[20221208 13:55:25 @agent_ppo2.py:179][0m |          -0.0225 |         246.2592 |           2.7618 |
[32m[20221208 13:55:25 @agent_ppo2.py:179][0m |          -0.0267 |         245.3101 |           2.7519 |
[32m[20221208 13:55:25 @agent_ppo2.py:179][0m |          -0.0352 |         244.5629 |           2.7952 |
[32m[20221208 13:55:26 @agent_ppo2.py:179][0m |          -0.0376 |         243.3645 |           2.8049 |
[32m[20221208 13:55:26 @agent_ppo2.py:179][0m |          -0.0392 |         242.7550 |           2.8119 |
[32m[20221208 13:55:26 @agent_ppo2.py:179][0m |          -0.0384 |         243.5000 |           2.8132 |
[32m[20221208 13:55:26 @agent_ppo2.py:179][0m |          -0.0421 |         241.5904 |           2.8167 |
[32m[20221208 13:55:26 @agent_ppo2.py:179][0m |          -0.0380 |         241.5330 |           2.8158 |
[32m[20221208 13:55:26 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:55:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.87
[32m[20221208 13:55:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.91
[32m[20221208 13:55:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.09
[32m[20221208 13:55:26 @agent_ppo2.py:137][0m Total time:      10.10 min
[32m[20221208 13:55:26 @agent_ppo2.py:139][0m 835584 total steps have happened
[32m[20221208 13:55:26 @agent_ppo2.py:115][0m #------------------------ Iteration 408 --------------------------#
[32m[20221208 13:55:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |           0.0289 |         258.5302 |           2.7031 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |           0.0034 |         242.4621 |           2.6811 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0249 |         236.9611 |           2.7092 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0343 |         235.0575 |           2.7127 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0412 |         231.4451 |           2.7239 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0418 |         229.5353 |           2.7160 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0465 |         228.0102 |           2.7236 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0493 |         225.8172 |           2.7255 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0530 |         224.3144 |           2.7367 |
[32m[20221208 13:55:27 @agent_ppo2.py:179][0m |          -0.0516 |         222.8251 |           2.7379 |
[32m[20221208 13:55:27 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:55:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 744.06
[32m[20221208 13:55:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 850.08
[32m[20221208 13:55:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.07
[32m[20221208 13:55:28 @agent_ppo2.py:137][0m Total time:      10.12 min
[32m[20221208 13:55:28 @agent_ppo2.py:139][0m 837632 total steps have happened
[32m[20221208 13:55:28 @agent_ppo2.py:115][0m #------------------------ Iteration 409 --------------------------#
[32m[20221208 13:55:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:28 @agent_ppo2.py:179][0m |           0.0164 |         252.9719 |           2.7267 |
[32m[20221208 13:55:28 @agent_ppo2.py:179][0m |           0.0026 |         241.6814 |           2.6651 |
[32m[20221208 13:55:28 @agent_ppo2.py:179][0m |          -0.0135 |         233.9840 |           2.6921 |
[32m[20221208 13:55:28 @agent_ppo2.py:179][0m |          -0.0278 |         232.2612 |           2.7162 |
[32m[20221208 13:55:29 @agent_ppo2.py:179][0m |          -0.0330 |         229.5224 |           2.7225 |
[32m[20221208 13:55:29 @agent_ppo2.py:179][0m |          -0.0395 |         227.5608 |           2.7439 |
[32m[20221208 13:55:29 @agent_ppo2.py:179][0m |          -0.0399 |         226.5982 |           2.7459 |
[32m[20221208 13:55:29 @agent_ppo2.py:179][0m |          -0.0445 |         224.4919 |           2.7630 |
[32m[20221208 13:55:29 @agent_ppo2.py:179][0m |          -0.0438 |         224.3497 |           2.7672 |
[32m[20221208 13:55:29 @agent_ppo2.py:179][0m |          -0.0448 |         222.5168 |           2.7749 |
[32m[20221208 13:55:29 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:55:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.22
[32m[20221208 13:55:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 946.26
[32m[20221208 13:55:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 870.85
[32m[20221208 13:55:29 @agent_ppo2.py:137][0m Total time:      10.15 min
[32m[20221208 13:55:29 @agent_ppo2.py:139][0m 839680 total steps have happened
[32m[20221208 13:55:29 @agent_ppo2.py:115][0m #------------------------ Iteration 410 --------------------------#
[32m[20221208 13:55:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |           0.0260 |         204.2980 |           2.2499 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0218 |         185.7220 |           1.9154 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0320 |         176.8660 |           1.9235 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0450 |         170.0742 |           1.9462 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0556 |         167.2881 |           1.9659 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0559 |         163.3201 |           1.9846 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0621 |         159.4463 |           2.0045 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0648 |         157.8648 |           2.0097 |
[32m[20221208 13:55:30 @agent_ppo2.py:179][0m |          -0.0672 |         155.9273 |           2.0198 |
[32m[20221208 13:55:31 @agent_ppo2.py:179][0m |          -0.0682 |         154.2991 |           2.0290 |
[32m[20221208 13:55:31 @agent_ppo2.py:124][0m Policy update time: 0.90 s
[32m[20221208 13:55:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 661.56
[32m[20221208 13:55:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.55
[32m[20221208 13:55:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 971.91
[32m[20221208 13:55:31 @agent_ppo2.py:137][0m Total time:      10.18 min
[32m[20221208 13:55:31 @agent_ppo2.py:139][0m 841728 total steps have happened
[32m[20221208 13:55:31 @agent_ppo2.py:115][0m #------------------------ Iteration 411 --------------------------#
[32m[20221208 13:55:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |           0.0259 |         249.9779 |           2.8482 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0095 |         237.4905 |           2.8354 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0231 |         234.3574 |           2.8905 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0320 |         230.6943 |           2.9028 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0383 |         229.6764 |           2.9259 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0388 |         225.8231 |           2.9321 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0395 |         223.7787 |           2.9419 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0409 |         222.1528 |           2.9463 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0441 |         218.4260 |           2.9463 |
[32m[20221208 13:55:32 @agent_ppo2.py:179][0m |          -0.0448 |         218.0672 |           2.9610 |
[32m[20221208 13:55:32 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 13:55:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.20
[32m[20221208 13:55:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.83
[32m[20221208 13:55:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 852.03
[32m[20221208 13:55:33 @agent_ppo2.py:137][0m Total time:      10.20 min
[32m[20221208 13:55:33 @agent_ppo2.py:139][0m 843776 total steps have happened
[32m[20221208 13:55:33 @agent_ppo2.py:115][0m #------------------------ Iteration 412 --------------------------#
[32m[20221208 13:55:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:33 @agent_ppo2.py:179][0m |           0.0337 |         258.0945 |           2.9340 |
[32m[20221208 13:55:33 @agent_ppo2.py:179][0m |          -0.0120 |         249.6449 |           2.9472 |
[32m[20221208 13:55:33 @agent_ppo2.py:179][0m |          -0.0186 |         247.3732 |           2.9858 |
[32m[20221208 13:55:33 @agent_ppo2.py:179][0m |          -0.0291 |         244.6311 |           2.9918 |
[32m[20221208 13:55:33 @agent_ppo2.py:179][0m |          -0.0343 |         242.6230 |           3.0096 |
[32m[20221208 13:55:34 @agent_ppo2.py:179][0m |          -0.0383 |         241.6808 |           3.0164 |
[32m[20221208 13:55:34 @agent_ppo2.py:179][0m |          -0.0410 |         241.2559 |           3.0271 |
[32m[20221208 13:55:34 @agent_ppo2.py:179][0m |          -0.0443 |         240.0365 |           3.0393 |
[32m[20221208 13:55:34 @agent_ppo2.py:179][0m |          -0.0429 |         238.8381 |           3.0458 |
[32m[20221208 13:55:34 @agent_ppo2.py:179][0m |          -0.0452 |         238.7170 |           3.0395 |
[32m[20221208 13:55:34 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:55:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 871.46
[32m[20221208 13:55:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.71
[32m[20221208 13:55:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.81
[32m[20221208 13:55:34 @agent_ppo2.py:137][0m Total time:      10.23 min
[32m[20221208 13:55:34 @agent_ppo2.py:139][0m 845824 total steps have happened
[32m[20221208 13:55:34 @agent_ppo2.py:115][0m #------------------------ Iteration 413 --------------------------#
[32m[20221208 13:55:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |           0.0171 |         215.3529 |           3.0293 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0050 |         204.9323 |           3.0219 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0217 |         200.7023 |           3.0514 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0266 |         198.8996 |           3.0750 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0348 |         196.9711 |           3.0996 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0252 |         195.1704 |           3.0765 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0401 |         194.3991 |           3.1160 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0412 |         193.2651 |           3.1127 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0409 |         193.1648 |           3.1292 |
[32m[20221208 13:55:35 @agent_ppo2.py:179][0m |          -0.0345 |         192.8718 |           3.1381 |
[32m[20221208 13:55:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 654.60
[32m[20221208 13:55:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 961.31
[32m[20221208 13:55:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 917.13
[32m[20221208 13:55:36 @agent_ppo2.py:137][0m Total time:      10.26 min
[32m[20221208 13:55:36 @agent_ppo2.py:139][0m 847872 total steps have happened
[32m[20221208 13:55:36 @agent_ppo2.py:115][0m #------------------------ Iteration 414 --------------------------#
[32m[20221208 13:55:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:36 @agent_ppo2.py:179][0m |           0.0321 |         261.6931 |           3.1931 |
[32m[20221208 13:55:36 @agent_ppo2.py:179][0m |          -0.0109 |         251.2430 |           3.1522 |
[32m[20221208 13:55:36 @agent_ppo2.py:179][0m |          -0.0211 |         247.6218 |           3.2062 |
[32m[20221208 13:55:36 @agent_ppo2.py:179][0m |          -0.0340 |         244.9037 |           3.2161 |
[32m[20221208 13:55:36 @agent_ppo2.py:179][0m |          -0.0359 |         243.4410 |           3.2173 |
[32m[20221208 13:55:37 @agent_ppo2.py:179][0m |          -0.0342 |         242.8095 |           3.1972 |
[32m[20221208 13:55:37 @agent_ppo2.py:179][0m |          -0.0415 |         241.4834 |           3.2380 |
[32m[20221208 13:55:37 @agent_ppo2.py:179][0m |          -0.0428 |         240.7226 |           3.2560 |
[32m[20221208 13:55:37 @agent_ppo2.py:179][0m |          -0.0438 |         240.6694 |           3.2461 |
[32m[20221208 13:55:37 @agent_ppo2.py:179][0m |          -0.0450 |         239.5394 |           3.2716 |
[32m[20221208 13:55:37 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:55:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.90
[32m[20221208 13:55:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.27
[32m[20221208 13:55:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 653.90
[32m[20221208 13:55:37 @agent_ppo2.py:137][0m Total time:      10.28 min
[32m[20221208 13:55:37 @agent_ppo2.py:139][0m 849920 total steps have happened
[32m[20221208 13:55:37 @agent_ppo2.py:115][0m #------------------------ Iteration 415 --------------------------#
[32m[20221208 13:55:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |           0.0265 |         247.8631 |           3.1386 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |           0.0356 |         244.3553 |           3.0388 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |           0.0039 |         243.6262 |           3.0388 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |          -0.0138 |         245.6987 |           3.1538 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |          -0.0260 |         242.1347 |           3.1954 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |          -0.0280 |         242.0074 |           3.1947 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |          -0.0327 |         241.2150 |           3.2062 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |          -0.0340 |         240.2982 |           3.2106 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |          -0.0355 |         239.1082 |           3.2108 |
[32m[20221208 13:55:38 @agent_ppo2.py:179][0m |          -0.0371 |         239.6025 |           3.2304 |
[32m[20221208 13:55:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 959.79
[32m[20221208 13:55:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.63
[32m[20221208 13:55:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 884.56
[32m[20221208 13:55:39 @agent_ppo2.py:137][0m Total time:      10.31 min
[32m[20221208 13:55:39 @agent_ppo2.py:139][0m 851968 total steps have happened
[32m[20221208 13:55:39 @agent_ppo2.py:115][0m #------------------------ Iteration 416 --------------------------#
[32m[20221208 13:55:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:39 @agent_ppo2.py:179][0m |           0.0244 |         232.0635 |           3.0889 |
[32m[20221208 13:55:39 @agent_ppo2.py:179][0m |           0.0085 |         217.9485 |           3.1169 |
[32m[20221208 13:55:39 @agent_ppo2.py:179][0m |          -0.0171 |         212.3454 |           3.1632 |
[32m[20221208 13:55:39 @agent_ppo2.py:179][0m |          -0.0262 |         211.9406 |           3.1789 |
[32m[20221208 13:55:40 @agent_ppo2.py:179][0m |          -0.0300 |         209.3351 |           3.1818 |
[32m[20221208 13:55:40 @agent_ppo2.py:179][0m |          -0.0335 |         207.6962 |           3.1924 |
[32m[20221208 13:55:40 @agent_ppo2.py:179][0m |          -0.0369 |         206.2413 |           3.1964 |
[32m[20221208 13:55:40 @agent_ppo2.py:179][0m |          -0.0452 |         204.4985 |           3.2065 |
[32m[20221208 13:55:40 @agent_ppo2.py:179][0m |          -0.0453 |         203.9612 |           3.2162 |
[32m[20221208 13:55:40 @agent_ppo2.py:179][0m |          -0.0445 |         203.2952 |           3.2126 |
[32m[20221208 13:55:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 756.53
[32m[20221208 13:55:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 938.14
[32m[20221208 13:55:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.23
[32m[20221208 13:55:40 @agent_ppo2.py:137][0m Total time:      10.33 min
[32m[20221208 13:55:40 @agent_ppo2.py:139][0m 854016 total steps have happened
[32m[20221208 13:55:40 @agent_ppo2.py:115][0m #------------------------ Iteration 417 --------------------------#
[32m[20221208 13:55:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |           0.0153 |         225.3920 |           3.2554 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0043 |         207.0627 |           3.1840 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0204 |         200.8642 |           3.1938 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0282 |         198.0536 |           3.1992 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0336 |         196.0552 |           3.2133 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0363 |         195.4331 |           3.2199 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0396 |         194.4096 |           3.2170 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0417 |         195.0857 |           3.2405 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0439 |         193.4530 |           3.2494 |
[32m[20221208 13:55:41 @agent_ppo2.py:179][0m |          -0.0449 |         192.4546 |           3.2577 |
[32m[20221208 13:55:41 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:55:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.18
[32m[20221208 13:55:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 929.01
[32m[20221208 13:55:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 981.70
[32m[20221208 13:55:42 @agent_ppo2.py:137][0m Total time:      10.36 min
[32m[20221208 13:55:42 @agent_ppo2.py:139][0m 856064 total steps have happened
[32m[20221208 13:55:42 @agent_ppo2.py:115][0m #------------------------ Iteration 418 --------------------------#
[32m[20221208 13:55:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:42 @agent_ppo2.py:179][0m |           0.0173 |         250.1605 |           3.0969 |
[32m[20221208 13:55:42 @agent_ppo2.py:179][0m |          -0.0102 |         245.5191 |           3.1119 |
[32m[20221208 13:55:42 @agent_ppo2.py:179][0m |          -0.0125 |         241.2639 |           3.1008 |
[32m[20221208 13:55:42 @agent_ppo2.py:179][0m |          -0.0180 |         238.6837 |           3.1091 |
[32m[20221208 13:55:43 @agent_ppo2.py:179][0m |          -0.0268 |         237.4070 |           3.1577 |
[32m[20221208 13:55:43 @agent_ppo2.py:179][0m |          -0.0325 |         236.4447 |           3.1554 |
[32m[20221208 13:55:43 @agent_ppo2.py:179][0m |          -0.0353 |         235.8595 |           3.1769 |
[32m[20221208 13:55:43 @agent_ppo2.py:179][0m |          -0.0375 |         235.1903 |           3.1840 |
[32m[20221208 13:55:43 @agent_ppo2.py:179][0m |          -0.0367 |         236.7879 |           3.1868 |
[32m[20221208 13:55:43 @agent_ppo2.py:179][0m |          -0.0377 |         234.5356 |           3.1913 |
[32m[20221208 13:55:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:55:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.74
[32m[20221208 13:55:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.64
[32m[20221208 13:55:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.25
[32m[20221208 13:55:43 @agent_ppo2.py:137][0m Total time:      10.38 min
[32m[20221208 13:55:43 @agent_ppo2.py:139][0m 858112 total steps have happened
[32m[20221208 13:55:43 @agent_ppo2.py:115][0m #------------------------ Iteration 419 --------------------------#
[32m[20221208 13:55:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |           0.0225 |         248.1528 |           3.1841 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |           0.0035 |         244.8102 |           3.1411 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0130 |         243.5624 |           3.1847 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0101 |         243.4155 |           3.1603 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0192 |         242.3183 |           3.2083 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0294 |         241.6544 |           3.2237 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0287 |         241.1974 |           3.2202 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0271 |         240.7546 |           3.2320 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0334 |         241.6847 |           3.2544 |
[32m[20221208 13:55:44 @agent_ppo2.py:179][0m |          -0.0346 |         240.1878 |           3.2666 |
[32m[20221208 13:55:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 954.57
[32m[20221208 13:55:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.36
[32m[20221208 13:55:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 120.98
[32m[20221208 13:55:45 @agent_ppo2.py:137][0m Total time:      10.41 min
[32m[20221208 13:55:45 @agent_ppo2.py:139][0m 860160 total steps have happened
[32m[20221208 13:55:45 @agent_ppo2.py:115][0m #------------------------ Iteration 420 --------------------------#
[32m[20221208 13:55:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:45 @agent_ppo2.py:179][0m |           0.0185 |         242.7429 |           3.3647 |
[32m[20221208 13:55:45 @agent_ppo2.py:179][0m |          -0.0086 |         235.1827 |           3.3543 |
[32m[20221208 13:55:45 @agent_ppo2.py:179][0m |          -0.0225 |         232.8593 |           3.3694 |
[32m[20221208 13:55:45 @agent_ppo2.py:179][0m |          -0.0260 |         232.1001 |           3.3841 |
[32m[20221208 13:55:45 @agent_ppo2.py:179][0m |          -0.0274 |         230.3243 |           3.3664 |
[32m[20221208 13:55:46 @agent_ppo2.py:179][0m |          -0.0353 |         230.2519 |           3.3739 |
[32m[20221208 13:55:46 @agent_ppo2.py:179][0m |          -0.0401 |         229.2959 |           3.3853 |
[32m[20221208 13:55:46 @agent_ppo2.py:179][0m |          -0.0423 |         227.1479 |           3.3885 |
[32m[20221208 13:55:46 @agent_ppo2.py:179][0m |          -0.0428 |         225.7217 |           3.3890 |
[32m[20221208 13:55:46 @agent_ppo2.py:179][0m |          -0.0450 |         225.7133 |           3.4024 |
[32m[20221208 13:55:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 817.78
[32m[20221208 13:55:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.50
[32m[20221208 13:55:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.51
[32m[20221208 13:55:46 @agent_ppo2.py:137][0m Total time:      10.43 min
[32m[20221208 13:55:46 @agent_ppo2.py:139][0m 862208 total steps have happened
[32m[20221208 13:55:46 @agent_ppo2.py:115][0m #------------------------ Iteration 421 --------------------------#
[32m[20221208 13:55:47 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:55:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |           0.0136 |         252.1068 |           3.1787 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |           0.0002 |         233.2614 |           3.1654 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0218 |         231.5376 |           3.1717 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0281 |         229.7099 |           3.1847 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0297 |         230.0304 |           3.1743 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0366 |         228.6223 |           3.1912 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0381 |         228.0296 |           3.1806 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0418 |         228.0318 |           3.2111 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0408 |         228.3469 |           3.1932 |
[32m[20221208 13:55:47 @agent_ppo2.py:179][0m |          -0.0441 |         227.7557 |           3.2057 |
[32m[20221208 13:55:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 707.13
[32m[20221208 13:55:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.20
[32m[20221208 13:55:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.34
[32m[20221208 13:55:48 @agent_ppo2.py:137][0m Total time:      10.46 min
[32m[20221208 13:55:48 @agent_ppo2.py:139][0m 864256 total steps have happened
[32m[20221208 13:55:48 @agent_ppo2.py:115][0m #------------------------ Iteration 422 --------------------------#
[32m[20221208 13:55:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:48 @agent_ppo2.py:179][0m |           0.0214 |         238.3034 |           3.2045 |
[32m[20221208 13:55:48 @agent_ppo2.py:179][0m |          -0.0050 |         219.6607 |           3.1936 |
[32m[20221208 13:55:48 @agent_ppo2.py:179][0m |          -0.0192 |         211.5297 |           3.2254 |
[32m[20221208 13:55:48 @agent_ppo2.py:179][0m |          -0.0236 |         205.5600 |           3.2332 |
[32m[20221208 13:55:49 @agent_ppo2.py:179][0m |          -0.0234 |         198.2343 |           3.2429 |
[32m[20221208 13:55:49 @agent_ppo2.py:179][0m |          -0.0306 |         192.7539 |           3.2472 |
[32m[20221208 13:55:49 @agent_ppo2.py:179][0m |          -0.0316 |         191.2773 |           3.2688 |
[32m[20221208 13:55:49 @agent_ppo2.py:179][0m |          -0.0319 |         190.5972 |           3.2698 |
[32m[20221208 13:55:49 @agent_ppo2.py:179][0m |          -0.0337 |         188.8859 |           3.2662 |
[32m[20221208 13:55:49 @agent_ppo2.py:179][0m |          -0.0373 |         188.1551 |           3.2737 |
[32m[20221208 13:55:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 940.48
[32m[20221208 13:55:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.17
[32m[20221208 13:55:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 751.36
[32m[20221208 13:55:49 @agent_ppo2.py:137][0m Total time:      10.48 min
[32m[20221208 13:55:49 @agent_ppo2.py:139][0m 866304 total steps have happened
[32m[20221208 13:55:49 @agent_ppo2.py:115][0m #------------------------ Iteration 423 --------------------------#
[32m[20221208 13:55:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |           0.0279 |         249.4013 |           3.1846 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0012 |         242.1165 |           3.1110 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0148 |         238.3331 |           3.1908 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0258 |         236.0532 |           3.2162 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0290 |         235.6122 |           3.2302 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0317 |         234.7696 |           3.2360 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0329 |         233.4843 |           3.2448 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0338 |         233.1075 |           3.2458 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0342 |         233.9152 |           3.2556 |
[32m[20221208 13:55:50 @agent_ppo2.py:179][0m |          -0.0365 |         231.6737 |           3.2613 |
[32m[20221208 13:55:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:55:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 962.39
[32m[20221208 13:55:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.24
[32m[20221208 13:55:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 661.54
[32m[20221208 13:55:51 @agent_ppo2.py:137][0m Total time:      10.51 min
[32m[20221208 13:55:51 @agent_ppo2.py:139][0m 868352 total steps have happened
[32m[20221208 13:55:51 @agent_ppo2.py:115][0m #------------------------ Iteration 424 --------------------------#
[32m[20221208 13:55:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:51 @agent_ppo2.py:179][0m |           0.0250 |         245.2402 |           3.1938 |
[32m[20221208 13:55:51 @agent_ppo2.py:179][0m |           0.0081 |         238.9285 |           3.1290 |
[32m[20221208 13:55:51 @agent_ppo2.py:179][0m |          -0.0076 |         237.3557 |           3.1811 |
[32m[20221208 13:55:51 @agent_ppo2.py:179][0m |          -0.0238 |         235.2891 |           3.2082 |
[32m[20221208 13:55:51 @agent_ppo2.py:179][0m |          -0.0291 |         234.3515 |           3.2064 |
[32m[20221208 13:55:52 @agent_ppo2.py:179][0m |          -0.0294 |         233.0180 |           3.2244 |
[32m[20221208 13:55:52 @agent_ppo2.py:179][0m |          -0.0336 |         232.2763 |           3.2224 |
[32m[20221208 13:55:52 @agent_ppo2.py:179][0m |          -0.0394 |         230.8214 |           3.2377 |
[32m[20221208 13:55:52 @agent_ppo2.py:179][0m |          -0.0420 |         230.5308 |           3.2475 |
[32m[20221208 13:55:52 @agent_ppo2.py:179][0m |          -0.0437 |         229.4742 |           3.2629 |
[32m[20221208 13:55:52 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:55:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 903.09
[32m[20221208 13:55:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.74
[32m[20221208 13:55:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.51
[32m[20221208 13:55:52 @agent_ppo2.py:137][0m Total time:      10.53 min
[32m[20221208 13:55:52 @agent_ppo2.py:139][0m 870400 total steps have happened
[32m[20221208 13:55:52 @agent_ppo2.py:115][0m #------------------------ Iteration 425 --------------------------#
[32m[20221208 13:55:53 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |           0.0193 |         260.4734 |           3.2134 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |           0.0004 |         250.4053 |           3.1771 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0239 |         247.1142 |           3.1862 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0350 |         245.2792 |           3.2016 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0348 |         244.1378 |           3.1934 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0419 |         244.0106 |           3.2151 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0416 |         242.9087 |           3.2144 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0448 |         242.1742 |           3.2100 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0488 |         242.6273 |           3.2181 |
[32m[20221208 13:55:53 @agent_ppo2.py:179][0m |          -0.0498 |         242.9988 |           3.2237 |
[32m[20221208 13:55:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 773.61
[32m[20221208 13:55:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 821.70
[32m[20221208 13:55:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.98
[32m[20221208 13:55:54 @agent_ppo2.py:137][0m Total time:      10.56 min
[32m[20221208 13:55:54 @agent_ppo2.py:139][0m 872448 total steps have happened
[32m[20221208 13:55:54 @agent_ppo2.py:115][0m #------------------------ Iteration 426 --------------------------#
[32m[20221208 13:55:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:55:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:54 @agent_ppo2.py:179][0m |           0.0213 |         246.1360 |           3.2666 |
[32m[20221208 13:55:54 @agent_ppo2.py:179][0m |          -0.0082 |         241.8881 |           3.2384 |
[32m[20221208 13:55:54 @agent_ppo2.py:179][0m |          -0.0244 |         238.7706 |           3.2971 |
[32m[20221208 13:55:54 @agent_ppo2.py:179][0m |          -0.0318 |         237.1946 |           3.3328 |
[32m[20221208 13:55:54 @agent_ppo2.py:179][0m |          -0.0302 |         235.6081 |           3.3188 |
[32m[20221208 13:55:55 @agent_ppo2.py:179][0m |          -0.0349 |         235.9787 |           3.3420 |
[32m[20221208 13:55:55 @agent_ppo2.py:179][0m |          -0.0398 |         234.3865 |           3.3500 |
[32m[20221208 13:55:55 @agent_ppo2.py:179][0m |          -0.0417 |         233.4966 |           3.3551 |
[32m[20221208 13:55:55 @agent_ppo2.py:179][0m |          -0.0406 |         234.0018 |           3.3640 |
[32m[20221208 13:55:55 @agent_ppo2.py:179][0m |          -0.0429 |         233.3452 |           3.3691 |
[32m[20221208 13:55:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 930.38
[32m[20221208 13:55:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 997.00
[32m[20221208 13:55:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.85
[32m[20221208 13:55:55 @agent_ppo2.py:137][0m Total time:      10.58 min
[32m[20221208 13:55:55 @agent_ppo2.py:139][0m 874496 total steps have happened
[32m[20221208 13:55:55 @agent_ppo2.py:115][0m #------------------------ Iteration 427 --------------------------#
[32m[20221208 13:55:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |           0.0553 |         152.8863 |           3.1813 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |           0.0393 |         133.8384 |           2.8083 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |           0.0344 |         127.1468 |           2.7278 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |          -0.0188 |         123.3767 |           2.5535 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |          -0.0307 |         120.6162 |           2.4683 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |          -0.0447 |         119.6343 |           2.4203 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |          -0.0494 |         118.3069 |           2.3584 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |          -0.0572 |         117.7663 |           2.3448 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |          -0.0614 |         116.7706 |           2.3276 |
[32m[20221208 13:55:56 @agent_ppo2.py:179][0m |          -0.0636 |         117.4369 |           2.3337 |
[32m[20221208 13:55:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:55:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 628.74
[32m[20221208 13:55:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 898.87
[32m[20221208 13:55:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 478.79
[32m[20221208 13:55:57 @agent_ppo2.py:137][0m Total time:      10.61 min
[32m[20221208 13:55:57 @agent_ppo2.py:139][0m 876544 total steps have happened
[32m[20221208 13:55:57 @agent_ppo2.py:115][0m #------------------------ Iteration 428 --------------------------#
[32m[20221208 13:55:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:57 @agent_ppo2.py:179][0m |           0.0270 |         208.8220 |           3.4812 |
[32m[20221208 13:55:57 @agent_ppo2.py:179][0m |           0.0290 |         202.3723 |           3.3975 |
[32m[20221208 13:55:57 @agent_ppo2.py:179][0m |          -0.0021 |         201.3476 |           3.4801 |
[32m[20221208 13:55:57 @agent_ppo2.py:179][0m |          -0.0108 |         202.8763 |           3.5142 |
[32m[20221208 13:55:57 @agent_ppo2.py:179][0m |          -0.0193 |         200.7074 |           3.5238 |
[32m[20221208 13:55:58 @agent_ppo2.py:179][0m |          -0.0062 |         201.5714 |           3.4553 |
[32m[20221208 13:55:58 @agent_ppo2.py:179][0m |          -0.0216 |         199.8403 |           3.5450 |
[32m[20221208 13:55:58 @agent_ppo2.py:179][0m |          -0.0244 |         199.7005 |           3.5743 |
[32m[20221208 13:55:58 @agent_ppo2.py:179][0m |          -0.0301 |         199.3886 |           3.5959 |
[32m[20221208 13:55:58 @agent_ppo2.py:179][0m |          -0.0289 |         199.2673 |           3.6085 |
[32m[20221208 13:55:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:55:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 626.09
[32m[20221208 13:55:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 921.42
[32m[20221208 13:55:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.03
[32m[20221208 13:55:58 @agent_ppo2.py:137][0m Total time:      10.63 min
[32m[20221208 13:55:58 @agent_ppo2.py:139][0m 878592 total steps have happened
[32m[20221208 13:55:58 @agent_ppo2.py:115][0m #------------------------ Iteration 429 --------------------------#
[32m[20221208 13:55:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:55:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |           0.0508 |         240.2673 |           3.3315 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |           0.0495 |         230.4044 |           3.0620 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |           0.0030 |         226.3623 |           3.2424 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |          -0.0132 |         224.5564 |           3.3629 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |          -0.0148 |         223.6676 |           3.3725 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |          -0.0241 |         221.7916 |           3.4304 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |          -0.0284 |         220.8825 |           3.4441 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |          -0.0341 |         220.2204 |           3.4763 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |          -0.0317 |         219.4444 |           3.4674 |
[32m[20221208 13:55:59 @agent_ppo2.py:179][0m |          -0.0293 |         218.1799 |           3.4757 |
[32m[20221208 13:55:59 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:56:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.01
[32m[20221208 13:56:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.69
[32m[20221208 13:56:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.74
[32m[20221208 13:56:00 @agent_ppo2.py:137][0m Total time:      10.66 min
[32m[20221208 13:56:00 @agent_ppo2.py:139][0m 880640 total steps have happened
[32m[20221208 13:56:00 @agent_ppo2.py:115][0m #------------------------ Iteration 430 --------------------------#
[32m[20221208 13:56:00 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:56:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:00 @agent_ppo2.py:179][0m |           0.0222 |         262.3134 |           3.6025 |
[32m[20221208 13:56:00 @agent_ppo2.py:179][0m |           0.0039 |         257.1601 |           3.4582 |
[32m[20221208 13:56:00 @agent_ppo2.py:179][0m |          -0.0096 |         253.5553 |           3.5456 |
[32m[20221208 13:56:00 @agent_ppo2.py:179][0m |          -0.0197 |         251.3860 |           3.5809 |
[32m[20221208 13:56:01 @agent_ppo2.py:179][0m |          -0.0243 |         251.6851 |           3.5979 |
[32m[20221208 13:56:01 @agent_ppo2.py:179][0m |          -0.0248 |         250.7267 |           3.5739 |
[32m[20221208 13:56:01 @agent_ppo2.py:179][0m |          -0.0331 |         248.2910 |           3.5993 |
[32m[20221208 13:56:01 @agent_ppo2.py:179][0m |          -0.0307 |         248.7021 |           3.5977 |
[32m[20221208 13:56:01 @agent_ppo2.py:179][0m |          -0.0316 |         247.8004 |           3.5771 |
[32m[20221208 13:56:01 @agent_ppo2.py:179][0m |          -0.0337 |         246.8635 |           3.6009 |
[32m[20221208 13:56:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:56:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 930.06
[32m[20221208 13:56:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.92
[32m[20221208 13:56:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.48
[32m[20221208 13:56:01 @agent_ppo2.py:137][0m Total time:      10.68 min
[32m[20221208 13:56:01 @agent_ppo2.py:139][0m 882688 total steps have happened
[32m[20221208 13:56:01 @agent_ppo2.py:115][0m #------------------------ Iteration 431 --------------------------#
[32m[20221208 13:56:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |           0.0158 |         199.0133 |           3.3402 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0058 |         190.3489 |           3.2747 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0169 |         188.1765 |           3.2548 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0220 |         186.5886 |           3.2962 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0285 |         184.1742 |           3.2785 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0241 |         182.8696 |           3.2847 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0327 |         181.6183 |           3.3013 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0356 |         180.4041 |           3.3321 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0383 |         178.1687 |           3.3603 |
[32m[20221208 13:56:02 @agent_ppo2.py:179][0m |          -0.0315 |         177.1458 |           3.2561 |
[32m[20221208 13:56:02 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:56:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 720.98
[32m[20221208 13:56:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.09
[32m[20221208 13:56:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 621.62
[32m[20221208 13:56:03 @agent_ppo2.py:137][0m Total time:      10.71 min
[32m[20221208 13:56:03 @agent_ppo2.py:139][0m 884736 total steps have happened
[32m[20221208 13:56:03 @agent_ppo2.py:115][0m #------------------------ Iteration 432 --------------------------#
[32m[20221208 13:56:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:56:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:03 @agent_ppo2.py:179][0m |           0.0095 |         207.5430 |           3.3635 |
[32m[20221208 13:56:03 @agent_ppo2.py:179][0m |          -0.0094 |         195.4116 |           3.2913 |
[32m[20221208 13:56:03 @agent_ppo2.py:179][0m |          -0.0306 |         191.9932 |           3.3576 |
[32m[20221208 13:56:03 @agent_ppo2.py:179][0m |          -0.0376 |         190.3986 |           3.3339 |
[32m[20221208 13:56:04 @agent_ppo2.py:179][0m |          -0.0386 |         189.5601 |           3.3208 |
[32m[20221208 13:56:04 @agent_ppo2.py:179][0m |          -0.0408 |         188.9645 |           3.3024 |
[32m[20221208 13:56:04 @agent_ppo2.py:179][0m |          -0.0449 |         188.6273 |           3.3230 |
[32m[20221208 13:56:04 @agent_ppo2.py:179][0m |          -0.0505 |         187.0417 |           3.3249 |
[32m[20221208 13:56:04 @agent_ppo2.py:179][0m |          -0.0529 |         186.5636 |           3.3406 |
[32m[20221208 13:56:04 @agent_ppo2.py:179][0m |          -0.0555 |         186.1711 |           3.3422 |
[32m[20221208 13:56:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 787.92
[32m[20221208 13:56:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 913.94
[32m[20221208 13:56:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 773.70
[32m[20221208 13:56:04 @agent_ppo2.py:137][0m Total time:      10.73 min
[32m[20221208 13:56:04 @agent_ppo2.py:139][0m 886784 total steps have happened
[32m[20221208 13:56:04 @agent_ppo2.py:115][0m #------------------------ Iteration 433 --------------------------#
[32m[20221208 13:56:05 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |           0.0198 |         251.4980 |           3.2231 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |           0.0027 |         245.8217 |           3.1261 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0194 |         244.7805 |           3.2583 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0215 |         244.4561 |           3.2873 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0257 |         243.1491 |           3.2856 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0287 |         242.5609 |           3.3077 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0304 |         241.2259 |           3.3088 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0312 |         241.0761 |           3.3083 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0349 |         241.3721 |           3.3177 |
[32m[20221208 13:56:05 @agent_ppo2.py:179][0m |          -0.0346 |         240.9394 |           3.3196 |
[32m[20221208 13:56:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.97
[32m[20221208 13:56:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.28
[32m[20221208 13:56:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.86
[32m[20221208 13:56:06 @agent_ppo2.py:137][0m Total time:      10.76 min
[32m[20221208 13:56:06 @agent_ppo2.py:139][0m 888832 total steps have happened
[32m[20221208 13:56:06 @agent_ppo2.py:115][0m #------------------------ Iteration 434 --------------------------#
[32m[20221208 13:56:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:06 @agent_ppo2.py:179][0m |           0.0436 |         248.8365 |           3.3450 |
[32m[20221208 13:56:06 @agent_ppo2.py:179][0m |          -0.0023 |         240.7197 |           3.3012 |
[32m[20221208 13:56:06 @agent_ppo2.py:179][0m |          -0.0170 |         240.0186 |           3.3634 |
[32m[20221208 13:56:06 @agent_ppo2.py:179][0m |          -0.0227 |         240.1947 |           3.3718 |
[32m[20221208 13:56:06 @agent_ppo2.py:179][0m |          -0.0291 |         237.6419 |           3.4185 |
[32m[20221208 13:56:07 @agent_ppo2.py:179][0m |          -0.0320 |         237.2832 |           3.4210 |
[32m[20221208 13:56:07 @agent_ppo2.py:179][0m |          -0.0317 |         236.5746 |           3.4301 |
[32m[20221208 13:56:07 @agent_ppo2.py:179][0m |          -0.0333 |         236.4955 |           3.4409 |
[32m[20221208 13:56:07 @agent_ppo2.py:179][0m |          -0.0360 |         235.3646 |           3.4585 |
[32m[20221208 13:56:07 @agent_ppo2.py:179][0m |          -0.0371 |         234.7277 |           3.4597 |
[32m[20221208 13:56:07 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.52
[32m[20221208 13:56:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.64
[32m[20221208 13:56:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.78
[32m[20221208 13:56:07 @agent_ppo2.py:137][0m Total time:      10.78 min
[32m[20221208 13:56:07 @agent_ppo2.py:139][0m 890880 total steps have happened
[32m[20221208 13:56:07 @agent_ppo2.py:115][0m #------------------------ Iteration 435 --------------------------#
[32m[20221208 13:56:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |           0.0277 |         197.9321 |           3.3658 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0042 |         184.2348 |           3.3345 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0230 |         179.0467 |           3.3882 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0344 |         175.9866 |           3.4065 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0369 |         173.9518 |           3.4225 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0370 |         171.9442 |           3.4252 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0426 |         171.1145 |           3.4360 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0445 |         168.8962 |           3.4422 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0462 |         166.3717 |           3.4604 |
[32m[20221208 13:56:08 @agent_ppo2.py:179][0m |          -0.0475 |         164.6455 |           3.4619 |
[32m[20221208 13:56:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 729.35
[32m[20221208 13:56:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.08
[32m[20221208 13:56:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 670.02
[32m[20221208 13:56:09 @agent_ppo2.py:137][0m Total time:      10.81 min
[32m[20221208 13:56:09 @agent_ppo2.py:139][0m 892928 total steps have happened
[32m[20221208 13:56:09 @agent_ppo2.py:115][0m #------------------------ Iteration 436 --------------------------#
[32m[20221208 13:56:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:09 @agent_ppo2.py:179][0m |           0.0532 |         146.7915 |           3.0239 |
[32m[20221208 13:56:09 @agent_ppo2.py:179][0m |           0.0261 |         135.6823 |           2.6611 |
[32m[20221208 13:56:09 @agent_ppo2.py:179][0m |          -0.0035 |         131.0689 |           2.8443 |
[32m[20221208 13:56:09 @agent_ppo2.py:179][0m |          -0.0150 |         127.8140 |           2.9391 |
[32m[20221208 13:56:09 @agent_ppo2.py:179][0m |          -0.0173 |         125.6136 |           2.9682 |
[32m[20221208 13:56:10 @agent_ppo2.py:179][0m |          -0.0245 |         124.5945 |           3.0479 |
[32m[20221208 13:56:10 @agent_ppo2.py:179][0m |          -0.0230 |         124.5293 |           3.1845 |
[32m[20221208 13:56:10 @agent_ppo2.py:179][0m |          -0.0310 |         123.8851 |           3.2138 |
[32m[20221208 13:56:10 @agent_ppo2.py:179][0m |          -0.0314 |         123.1798 |           3.2379 |
[32m[20221208 13:56:10 @agent_ppo2.py:179][0m |          -0.0366 |         122.0457 |           3.2949 |
[32m[20221208 13:56:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:56:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 563.96
[32m[20221208 13:56:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.37
[32m[20221208 13:56:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.77
[32m[20221208 13:56:10 @agent_ppo2.py:137][0m Total time:      10.83 min
[32m[20221208 13:56:10 @agent_ppo2.py:139][0m 894976 total steps have happened
[32m[20221208 13:56:10 @agent_ppo2.py:115][0m #------------------------ Iteration 437 --------------------------#
[32m[20221208 13:56:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |           0.0173 |         253.4974 |           3.4991 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0056 |         245.2684 |           3.4895 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0213 |         242.7265 |           3.5295 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0205 |         240.8991 |           3.5087 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0270 |         240.3578 |           3.5242 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0298 |         239.2015 |           3.5525 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0327 |         238.7017 |           3.5526 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0320 |         240.3587 |           3.5574 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0336 |         238.9373 |           3.5728 |
[32m[20221208 13:56:11 @agent_ppo2.py:179][0m |          -0.0349 |         239.2693 |           3.5653 |
[32m[20221208 13:56:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 957.55
[32m[20221208 13:56:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.60
[32m[20221208 13:56:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 898.22
[32m[20221208 13:56:12 @agent_ppo2.py:137][0m Total time:      10.86 min
[32m[20221208 13:56:12 @agent_ppo2.py:139][0m 897024 total steps have happened
[32m[20221208 13:56:12 @agent_ppo2.py:115][0m #------------------------ Iteration 438 --------------------------#
[32m[20221208 13:56:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:12 @agent_ppo2.py:179][0m |           0.0300 |         144.1054 |           3.5014 |
[32m[20221208 13:56:12 @agent_ppo2.py:179][0m |           0.0027 |         135.5415 |           3.4826 |
[32m[20221208 13:56:12 @agent_ppo2.py:179][0m |          -0.0149 |         133.4064 |           3.3768 |
[32m[20221208 13:56:12 @agent_ppo2.py:179][0m |          -0.0231 |         130.9959 |           3.2676 |
[32m[20221208 13:56:12 @agent_ppo2.py:179][0m |          -0.0322 |         130.5943 |           3.2416 |
[32m[20221208 13:56:13 @agent_ppo2.py:179][0m |          -0.0386 |         128.6809 |           3.2607 |
[32m[20221208 13:56:13 @agent_ppo2.py:179][0m |          -0.0419 |         127.8347 |           3.1770 |
[32m[20221208 13:56:13 @agent_ppo2.py:179][0m |          -0.0449 |         126.9875 |           3.1990 |
[32m[20221208 13:56:13 @agent_ppo2.py:179][0m |          -0.0453 |         127.9984 |           3.2136 |
[32m[20221208 13:56:13 @agent_ppo2.py:179][0m |          -0.0470 |         126.7451 |           3.2071 |
[32m[20221208 13:56:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 760.16
[32m[20221208 13:56:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.57
[32m[20221208 13:56:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 485.08
[32m[20221208 13:56:13 @agent_ppo2.py:137][0m Total time:      10.88 min
[32m[20221208 13:56:13 @agent_ppo2.py:139][0m 899072 total steps have happened
[32m[20221208 13:56:13 @agent_ppo2.py:115][0m #------------------------ Iteration 439 --------------------------#
[32m[20221208 13:56:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |           0.0196 |         250.3601 |           3.5043 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |           0.0190 |         236.6752 |           3.4113 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |           0.0348 |         234.1488 |           3.2294 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |          -0.0003 |         232.7030 |           3.3819 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |          -0.0173 |         232.3341 |           3.4888 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |          -0.0185 |         231.4894 |           3.4887 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |          -0.0201 |         230.4759 |           3.4767 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |          -0.0187 |         230.1530 |           3.4818 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |          -0.0277 |         230.7210 |           3.4942 |
[32m[20221208 13:56:14 @agent_ppo2.py:179][0m |          -0.0311 |         229.5393 |           3.4875 |
[32m[20221208 13:56:14 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:56:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 941.92
[32m[20221208 13:56:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.10
[32m[20221208 13:56:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 963.95
[32m[20221208 13:56:15 @agent_ppo2.py:137][0m Total time:      10.90 min
[32m[20221208 13:56:15 @agent_ppo2.py:139][0m 901120 total steps have happened
[32m[20221208 13:56:15 @agent_ppo2.py:115][0m #------------------------ Iteration 440 --------------------------#
[32m[20221208 13:56:15 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:15 @agent_ppo2.py:179][0m |           0.0223 |         255.7547 |           3.3548 |
[32m[20221208 13:56:15 @agent_ppo2.py:179][0m |           0.0028 |         244.9744 |           3.2669 |
[32m[20221208 13:56:15 @agent_ppo2.py:179][0m |          -0.0161 |         239.9190 |           3.3200 |
[32m[20221208 13:56:15 @agent_ppo2.py:179][0m |          -0.0232 |         238.5126 |           3.3402 |
[32m[20221208 13:56:15 @agent_ppo2.py:179][0m |          -0.0275 |         237.7741 |           3.3673 |
[32m[20221208 13:56:15 @agent_ppo2.py:179][0m |          -0.0290 |         235.6599 |           3.3682 |
[32m[20221208 13:56:16 @agent_ppo2.py:179][0m |          -0.0336 |         234.8214 |           3.3755 |
[32m[20221208 13:56:16 @agent_ppo2.py:179][0m |          -0.0340 |         233.5823 |           3.3904 |
[32m[20221208 13:56:16 @agent_ppo2.py:179][0m |          -0.0368 |         233.5374 |           3.3885 |
[32m[20221208 13:56:16 @agent_ppo2.py:179][0m |          -0.0391 |         231.1050 |           3.4033 |
[32m[20221208 13:56:16 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.76
[32m[20221208 13:56:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.76
[32m[20221208 13:56:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 896.85
[32m[20221208 13:56:16 @agent_ppo2.py:137][0m Total time:      10.93 min
[32m[20221208 13:56:16 @agent_ppo2.py:139][0m 903168 total steps have happened
[32m[20221208 13:56:16 @agent_ppo2.py:115][0m #------------------------ Iteration 441 --------------------------#
[32m[20221208 13:56:17 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:56:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |           0.0145 |         245.7438 |           3.3312 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0072 |         240.2009 |           3.3060 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0111 |         239.4709 |           3.3119 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0185 |         238.0067 |           3.2885 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0277 |         237.7523 |           3.3334 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0270 |         235.8510 |           3.3250 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0275 |         235.1280 |           3.3286 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0312 |         235.1913 |           3.3517 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0316 |         234.6598 |           3.3715 |
[32m[20221208 13:56:17 @agent_ppo2.py:179][0m |          -0.0371 |         235.6881 |           3.3776 |
[32m[20221208 13:56:17 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:56:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.06
[32m[20221208 13:56:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.87
[32m[20221208 13:56:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 630.04
[32m[20221208 13:56:18 @agent_ppo2.py:137][0m Total time:      10.95 min
[32m[20221208 13:56:18 @agent_ppo2.py:139][0m 905216 total steps have happened
[32m[20221208 13:56:18 @agent_ppo2.py:115][0m #------------------------ Iteration 442 --------------------------#
[32m[20221208 13:56:18 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:18 @agent_ppo2.py:179][0m |           0.0281 |         249.1304 |           3.4166 |
[32m[20221208 13:56:18 @agent_ppo2.py:179][0m |           0.1082 |         241.7608 |           3.0662 |
[32m[20221208 13:56:18 @agent_ppo2.py:179][0m |           0.0140 |         241.0293 |           3.2873 |
[32m[20221208 13:56:18 @agent_ppo2.py:179][0m |          -0.0161 |         239.1038 |           3.4351 |
[32m[20221208 13:56:18 @agent_ppo2.py:179][0m |          -0.0254 |         236.2384 |           3.4650 |
[32m[20221208 13:56:18 @agent_ppo2.py:179][0m |          -0.0276 |         235.7383 |           3.4791 |
[32m[20221208 13:56:19 @agent_ppo2.py:179][0m |          -0.0237 |         234.6539 |           3.4731 |
[32m[20221208 13:56:19 @agent_ppo2.py:179][0m |          -0.0279 |         234.6794 |           3.4736 |
[32m[20221208 13:56:19 @agent_ppo2.py:179][0m |          -0.0263 |         233.7704 |           3.4670 |
[32m[20221208 13:56:19 @agent_ppo2.py:179][0m |          -0.0232 |         234.2296 |           3.4719 |
[32m[20221208 13:56:19 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:56:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.91
[32m[20221208 13:56:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.64
[32m[20221208 13:56:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 895.28
[32m[20221208 13:56:19 @agent_ppo2.py:137][0m Total time:      10.98 min
[32m[20221208 13:56:19 @agent_ppo2.py:139][0m 907264 total steps have happened
[32m[20221208 13:56:19 @agent_ppo2.py:115][0m #------------------------ Iteration 443 --------------------------#
[32m[20221208 13:56:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |           0.0263 |         216.3381 |           3.5206 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0081 |         207.5844 |           3.5193 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0132 |         205.3307 |           3.4980 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0186 |         204.3617 |           3.5249 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0194 |         203.3118 |           3.5457 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0212 |         202.5207 |           3.5546 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0195 |         203.1753 |           3.5486 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0256 |         201.9902 |           3.5645 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0287 |         201.5539 |           3.5763 |
[32m[20221208 13:56:20 @agent_ppo2.py:179][0m |          -0.0302 |         202.0114 |           3.5738 |
[32m[20221208 13:56:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:56:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 652.70
[32m[20221208 13:56:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.59
[32m[20221208 13:56:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 744.34
[32m[20221208 13:56:21 @agent_ppo2.py:137][0m Total time:      11.00 min
[32m[20221208 13:56:21 @agent_ppo2.py:139][0m 909312 total steps have happened
[32m[20221208 13:56:21 @agent_ppo2.py:115][0m #------------------------ Iteration 444 --------------------------#
[32m[20221208 13:56:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:21 @agent_ppo2.py:179][0m |           0.0248 |         213.7776 |           3.4318 |
[32m[20221208 13:56:21 @agent_ppo2.py:179][0m |          -0.0035 |         203.0691 |           3.4021 |
[32m[20221208 13:56:21 @agent_ppo2.py:179][0m |          -0.0226 |         200.0628 |           3.4403 |
[32m[20221208 13:56:21 @agent_ppo2.py:179][0m |          -0.0272 |         198.4898 |           3.4519 |
[32m[20221208 13:56:21 @agent_ppo2.py:179][0m |          -0.0357 |         197.4759 |           3.4606 |
[32m[20221208 13:56:21 @agent_ppo2.py:179][0m |          -0.0385 |         198.8359 |           3.4746 |
[32m[20221208 13:56:21 @agent_ppo2.py:179][0m |          -0.0428 |         196.1032 |           3.4968 |
[32m[20221208 13:56:22 @agent_ppo2.py:179][0m |          -0.0414 |         195.7112 |           3.4959 |
[32m[20221208 13:56:22 @agent_ppo2.py:179][0m |          -0.0441 |         195.3889 |           3.4800 |
[32m[20221208 13:56:22 @agent_ppo2.py:179][0m |          -0.0449 |         194.6667 |           3.5064 |
[32m[20221208 13:56:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 728.89
[32m[20221208 13:56:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 931.52
[32m[20221208 13:56:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 966.77
[32m[20221208 13:56:22 @agent_ppo2.py:137][0m Total time:      11.03 min
[32m[20221208 13:56:22 @agent_ppo2.py:139][0m 911360 total steps have happened
[32m[20221208 13:56:22 @agent_ppo2.py:115][0m #------------------------ Iteration 445 --------------------------#
[32m[20221208 13:56:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |           0.0145 |         206.3124 |           3.5552 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0174 |         197.7276 |           3.5402 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0164 |         194.8873 |           3.5162 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0256 |         194.5663 |           3.5793 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0358 |         191.8117 |           3.5940 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0409 |         191.3443 |           3.6128 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0407 |         189.8641 |           3.6004 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0396 |         189.7737 |           3.5992 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0459 |         188.0715 |           3.6023 |
[32m[20221208 13:56:23 @agent_ppo2.py:179][0m |          -0.0483 |         187.2954 |           3.6213 |
[32m[20221208 13:56:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 745.54
[32m[20221208 13:56:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 925.31
[32m[20221208 13:56:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.49
[32m[20221208 13:56:24 @agent_ppo2.py:137][0m Total time:      11.05 min
[32m[20221208 13:56:24 @agent_ppo2.py:139][0m 913408 total steps have happened
[32m[20221208 13:56:24 @agent_ppo2.py:115][0m #------------------------ Iteration 446 --------------------------#
[32m[20221208 13:56:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:24 @agent_ppo2.py:179][0m |           0.0125 |         254.9790 |           3.5258 |
[32m[20221208 13:56:24 @agent_ppo2.py:179][0m |          -0.0097 |         250.4892 |           3.4771 |
[32m[20221208 13:56:24 @agent_ppo2.py:179][0m |          -0.0252 |         247.3936 |           3.5446 |
[32m[20221208 13:56:24 @agent_ppo2.py:179][0m |          -0.0269 |         247.1213 |           3.5374 |
[32m[20221208 13:56:24 @agent_ppo2.py:179][0m |          -0.0250 |         246.4892 |           3.5414 |
[32m[20221208 13:56:24 @agent_ppo2.py:179][0m |          -0.0328 |         247.1457 |           3.5637 |
[32m[20221208 13:56:24 @agent_ppo2.py:179][0m |          -0.0353 |         245.3956 |           3.5702 |
[32m[20221208 13:56:25 @agent_ppo2.py:179][0m |          -0.0369 |         244.1908 |           3.5828 |
[32m[20221208 13:56:25 @agent_ppo2.py:179][0m |          -0.0386 |         243.9510 |           3.5836 |
[32m[20221208 13:56:25 @agent_ppo2.py:179][0m |          -0.0378 |         244.1384 |           3.5947 |
[32m[20221208 13:56:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 926.22
[32m[20221208 13:56:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.96
[32m[20221208 13:56:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 875.30
[32m[20221208 13:56:25 @agent_ppo2.py:137][0m Total time:      11.08 min
[32m[20221208 13:56:25 @agent_ppo2.py:139][0m 915456 total steps have happened
[32m[20221208 13:56:25 @agent_ppo2.py:115][0m #------------------------ Iteration 447 --------------------------#
[32m[20221208 13:56:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |           0.0204 |         204.9230 |           3.5970 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |           0.0045 |         194.2964 |           3.5701 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0101 |         188.8344 |           3.5920 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0176 |         187.1250 |           3.5972 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0292 |         186.7091 |           3.6319 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0339 |         186.0354 |           3.6261 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0370 |         184.6177 |           3.6321 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0374 |         185.1315 |           3.6360 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0387 |         183.7230 |           3.6450 |
[32m[20221208 13:56:26 @agent_ppo2.py:179][0m |          -0.0435 |         182.9676 |           3.6698 |
[32m[20221208 13:56:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.87
[32m[20221208 13:56:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.39
[32m[20221208 13:56:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 786.10
[32m[20221208 13:56:27 @agent_ppo2.py:137][0m Total time:      11.10 min
[32m[20221208 13:56:27 @agent_ppo2.py:139][0m 917504 total steps have happened
[32m[20221208 13:56:27 @agent_ppo2.py:115][0m #------------------------ Iteration 448 --------------------------#
[32m[20221208 13:56:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |           0.0227 |         203.3287 |           3.5460 |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |           0.0111 |         195.6479 |           3.4571 |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |          -0.0199 |         192.7576 |           3.5172 |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |          -0.0249 |         191.6078 |           3.5358 |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |          -0.0331 |         190.8442 |           3.5688 |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |          -0.0322 |         188.3201 |           3.5752 |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |          -0.0442 |         187.3608 |           3.5951 |
[32m[20221208 13:56:27 @agent_ppo2.py:179][0m |          -0.0446 |         187.4651 |           3.5973 |
[32m[20221208 13:56:28 @agent_ppo2.py:179][0m |          -0.0435 |         186.3575 |           3.6057 |
[32m[20221208 13:56:28 @agent_ppo2.py:179][0m |          -0.0434 |         185.6976 |           3.6087 |
[32m[20221208 13:56:28 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:56:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 730.11
[32m[20221208 13:56:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.27
[32m[20221208 13:56:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 784.18
[32m[20221208 13:56:28 @agent_ppo2.py:137][0m Total time:      11.13 min
[32m[20221208 13:56:28 @agent_ppo2.py:139][0m 919552 total steps have happened
[32m[20221208 13:56:28 @agent_ppo2.py:115][0m #------------------------ Iteration 449 --------------------------#
[32m[20221208 13:56:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |           0.0100 |         250.3014 |           3.6738 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0075 |         246.9282 |           3.6528 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0109 |         245.2493 |           3.6341 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0160 |         243.4560 |           3.6802 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0191 |         243.2859 |           3.7143 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0200 |         241.2551 |           3.7359 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0197 |         240.9526 |           3.7415 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0256 |         240.9494 |           3.7492 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0260 |         241.2524 |           3.7681 |
[32m[20221208 13:56:29 @agent_ppo2.py:179][0m |          -0.0264 |         240.4804 |           3.7578 |
[32m[20221208 13:56:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.44
[32m[20221208 13:56:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.17
[32m[20221208 13:56:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 662.49
[32m[20221208 13:56:30 @agent_ppo2.py:137][0m Total time:      11.15 min
[32m[20221208 13:56:30 @agent_ppo2.py:139][0m 921600 total steps have happened
[32m[20221208 13:56:30 @agent_ppo2.py:115][0m #------------------------ Iteration 450 --------------------------#
[32m[20221208 13:56:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |           0.0209 |         156.4745 |           3.6048 |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |          -0.0015 |         146.6068 |           3.6067 |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |          -0.0173 |         143.7514 |           3.6372 |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |          -0.0184 |         142.4171 |           3.6648 |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |          -0.0289 |         141.5506 |           3.6667 |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |          -0.0240 |         140.4102 |           3.6607 |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |          -0.0293 |         139.9300 |           3.7040 |
[32m[20221208 13:56:30 @agent_ppo2.py:179][0m |          -0.0329 |         138.7931 |           3.7337 |
[32m[20221208 13:56:31 @agent_ppo2.py:179][0m |          -0.0411 |         138.5702 |           3.7326 |
[32m[20221208 13:56:31 @agent_ppo2.py:179][0m |          -0.0379 |         138.6346 |           3.7450 |
[32m[20221208 13:56:31 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:56:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 439.21
[32m[20221208 13:56:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 866.95
[32m[20221208 13:56:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.66
[32m[20221208 13:56:31 @agent_ppo2.py:137][0m Total time:      11.18 min
[32m[20221208 13:56:31 @agent_ppo2.py:139][0m 923648 total steps have happened
[32m[20221208 13:56:31 @agent_ppo2.py:115][0m #------------------------ Iteration 451 --------------------------#
[32m[20221208 13:56:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |           0.0545 |         161.9687 |           3.3154 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0144 |         137.5524 |           2.4898 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0410 |         130.9456 |           2.5021 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0470 |         127.4284 |           2.5223 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0544 |         125.5510 |           2.5341 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0591 |         123.7143 |           2.5490 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0612 |         122.6849 |           2.5439 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0603 |         121.6590 |           2.5602 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0615 |         120.9133 |           2.5669 |
[32m[20221208 13:56:32 @agent_ppo2.py:179][0m |          -0.0639 |         120.4326 |           2.5755 |
[32m[20221208 13:56:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 13:56:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 305.08
[32m[20221208 13:56:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 477.15
[32m[20221208 13:56:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 670.86
[32m[20221208 13:56:33 @agent_ppo2.py:137][0m Total time:      11.20 min
[32m[20221208 13:56:33 @agent_ppo2.py:139][0m 925696 total steps have happened
[32m[20221208 13:56:33 @agent_ppo2.py:115][0m #------------------------ Iteration 452 --------------------------#
[32m[20221208 13:56:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:33 @agent_ppo2.py:179][0m |           0.0243 |         165.5658 |           3.7842 |
[32m[20221208 13:56:33 @agent_ppo2.py:179][0m |          -0.0052 |         139.8817 |           3.7221 |
[32m[20221208 13:56:33 @agent_ppo2.py:179][0m |          -0.0269 |         127.6444 |           3.7735 |
[32m[20221208 13:56:33 @agent_ppo2.py:179][0m |          -0.0365 |         119.7590 |           3.7852 |
[32m[20221208 13:56:33 @agent_ppo2.py:179][0m |          -0.0430 |         114.4348 |           3.7948 |
[32m[20221208 13:56:33 @agent_ppo2.py:179][0m |          -0.0430 |         112.0647 |           3.8005 |
[32m[20221208 13:56:33 @agent_ppo2.py:179][0m |          -0.0464 |         109.5266 |           3.8167 |
[32m[20221208 13:56:34 @agent_ppo2.py:179][0m |          -0.0490 |         107.4821 |           3.8271 |
[32m[20221208 13:56:34 @agent_ppo2.py:179][0m |          -0.0502 |         105.9581 |           3.8226 |
[32m[20221208 13:56:34 @agent_ppo2.py:179][0m |          -0.0497 |         104.9707 |           3.8196 |
[32m[20221208 13:56:34 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:56:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 734.41
[32m[20221208 13:56:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.27
[32m[20221208 13:56:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.00
[32m[20221208 13:56:34 @agent_ppo2.py:137][0m Total time:      11.23 min
[32m[20221208 13:56:34 @agent_ppo2.py:139][0m 927744 total steps have happened
[32m[20221208 13:56:34 @agent_ppo2.py:115][0m #------------------------ Iteration 453 --------------------------#
[32m[20221208 13:56:34 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |           0.0429 |         249.7786 |           3.8010 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |           0.0171 |         235.4452 |           3.7654 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0245 |         229.0020 |           3.8817 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0346 |         225.2500 |           3.9183 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0386 |         223.0713 |           3.9340 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0423 |         221.8560 |           3.9426 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0434 |         220.4424 |           3.9553 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0459 |         218.9626 |           3.9761 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0468 |         218.0788 |           3.9767 |
[32m[20221208 13:56:35 @agent_ppo2.py:179][0m |          -0.0485 |         217.1767 |           3.9784 |
[32m[20221208 13:56:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:56:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 805.66
[32m[20221208 13:56:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.84
[32m[20221208 13:56:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 669.44
[32m[20221208 13:56:36 @agent_ppo2.py:137][0m Total time:      11.25 min
[32m[20221208 13:56:36 @agent_ppo2.py:139][0m 929792 total steps have happened
[32m[20221208 13:56:36 @agent_ppo2.py:115][0m #------------------------ Iteration 454 --------------------------#
[32m[20221208 13:56:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:36 @agent_ppo2.py:179][0m |           0.0238 |         158.6203 |           3.9072 |
[32m[20221208 13:56:36 @agent_ppo2.py:179][0m |          -0.0003 |         142.0866 |           3.9678 |
[32m[20221208 13:56:36 @agent_ppo2.py:179][0m |          -0.0133 |         138.6109 |           3.9867 |
[32m[20221208 13:56:36 @agent_ppo2.py:179][0m |          -0.0234 |         136.0760 |           4.0351 |
[32m[20221208 13:56:36 @agent_ppo2.py:179][0m |          -0.0236 |         133.6502 |           4.0094 |
[32m[20221208 13:56:36 @agent_ppo2.py:179][0m |          -0.0331 |         133.9068 |           4.0490 |
[32m[20221208 13:56:36 @agent_ppo2.py:179][0m |          -0.0351 |         131.5640 |           4.0622 |
[32m[20221208 13:56:37 @agent_ppo2.py:179][0m |          -0.0353 |         131.0125 |           4.0854 |
[32m[20221208 13:56:37 @agent_ppo2.py:179][0m |          -0.0384 |         130.1708 |           4.0831 |
[32m[20221208 13:56:37 @agent_ppo2.py:179][0m |          -0.0425 |         129.3797 |           4.1031 |
[32m[20221208 13:56:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 465.22
[32m[20221208 13:56:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.10
[32m[20221208 13:56:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 642.37
[32m[20221208 13:56:37 @agent_ppo2.py:137][0m Total time:      11.28 min
[32m[20221208 13:56:37 @agent_ppo2.py:139][0m 931840 total steps have happened
[32m[20221208 13:56:37 @agent_ppo2.py:115][0m #------------------------ Iteration 455 --------------------------#
[32m[20221208 13:56:37 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |           0.0091 |         153.1767 |           4.0896 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0213 |         139.7398 |           4.0700 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0360 |         135.1756 |           4.0875 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0462 |         131.1447 |           4.1286 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0496 |         129.6464 |           4.1304 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0543 |         127.8475 |           4.1399 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0556 |         126.2568 |           4.1573 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0580 |         125.8335 |           4.1515 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0532 |         124.3649 |           4.1679 |
[32m[20221208 13:56:38 @agent_ppo2.py:179][0m |          -0.0578 |         123.9969 |           4.1726 |
[32m[20221208 13:56:38 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:56:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 577.22
[32m[20221208 13:56:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 907.06
[32m[20221208 13:56:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 782.36
[32m[20221208 13:56:39 @agent_ppo2.py:137][0m Total time:      11.30 min
[32m[20221208 13:56:39 @agent_ppo2.py:139][0m 933888 total steps have happened
[32m[20221208 13:56:39 @agent_ppo2.py:115][0m #------------------------ Iteration 456 --------------------------#
[32m[20221208 13:56:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:39 @agent_ppo2.py:179][0m |           0.0142 |         254.4325 |           4.3037 |
[32m[20221208 13:56:39 @agent_ppo2.py:179][0m |          -0.0003 |         242.8229 |           4.2257 |
[32m[20221208 13:56:39 @agent_ppo2.py:179][0m |          -0.0205 |         238.7896 |           4.3049 |
[32m[20221208 13:56:39 @agent_ppo2.py:179][0m |          -0.0291 |         235.5087 |           4.3071 |
[32m[20221208 13:56:39 @agent_ppo2.py:179][0m |          -0.0348 |         231.7207 |           4.3407 |
[32m[20221208 13:56:39 @agent_ppo2.py:179][0m |          -0.0268 |         229.6865 |           4.2969 |
[32m[20221208 13:56:39 @agent_ppo2.py:179][0m |          -0.0381 |         227.6264 |           4.3329 |
[32m[20221208 13:56:40 @agent_ppo2.py:179][0m |          -0.0440 |         224.3763 |           4.3778 |
[32m[20221208 13:56:40 @agent_ppo2.py:179][0m |          -0.0423 |         223.1953 |           4.3796 |
[32m[20221208 13:56:40 @agent_ppo2.py:179][0m |          -0.0406 |         222.5877 |           4.3480 |
[32m[20221208 13:56:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:56:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 657.31
[32m[20221208 13:56:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 926.51
[32m[20221208 13:56:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.30
[32m[20221208 13:56:40 @agent_ppo2.py:137][0m Total time:      11.33 min
[32m[20221208 13:56:40 @agent_ppo2.py:139][0m 935936 total steps have happened
[32m[20221208 13:56:40 @agent_ppo2.py:115][0m #------------------------ Iteration 457 --------------------------#
[32m[20221208 13:56:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:56:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |           0.0201 |         260.5611 |           4.1524 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0053 |         238.3650 |           4.0769 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0207 |         227.4538 |           4.1142 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0336 |         224.5738 |           4.1382 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0336 |         221.8460 |           4.1515 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0374 |         220.5191 |           4.1461 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0395 |         220.0304 |           4.1455 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0432 |         217.2208 |           4.1866 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0466 |         216.7071 |           4.1599 |
[32m[20221208 13:56:41 @agent_ppo2.py:179][0m |          -0.0490 |         215.8806 |           4.1915 |
[32m[20221208 13:56:41 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:56:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.80
[32m[20221208 13:56:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.35
[32m[20221208 13:56:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 916.31
[32m[20221208 13:56:42 @agent_ppo2.py:137][0m Total time:      11.35 min
[32m[20221208 13:56:42 @agent_ppo2.py:139][0m 937984 total steps have happened
[32m[20221208 13:56:42 @agent_ppo2.py:115][0m #------------------------ Iteration 458 --------------------------#
[32m[20221208 13:56:42 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:56:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:42 @agent_ppo2.py:179][0m |           0.0194 |         237.7500 |           4.1474 |
[32m[20221208 13:56:42 @agent_ppo2.py:179][0m |           0.0140 |         221.1512 |           4.0106 |
[32m[20221208 13:56:42 @agent_ppo2.py:179][0m |          -0.0064 |         212.1843 |           4.1116 |
[32m[20221208 13:56:42 @agent_ppo2.py:179][0m |          -0.0183 |         207.8095 |           4.1753 |
[32m[20221208 13:56:42 @agent_ppo2.py:179][0m |          -0.0226 |         204.4792 |           4.1769 |
[32m[20221208 13:56:43 @agent_ppo2.py:179][0m |          -0.0282 |         202.1925 |           4.2218 |
[32m[20221208 13:56:43 @agent_ppo2.py:179][0m |          -0.0276 |         198.6339 |           4.2087 |
[32m[20221208 13:56:43 @agent_ppo2.py:179][0m |          -0.0315 |         197.0451 |           4.2246 |
[32m[20221208 13:56:43 @agent_ppo2.py:179][0m |          -0.0296 |         194.8812 |           4.2100 |
[32m[20221208 13:56:43 @agent_ppo2.py:179][0m |          -0.0354 |         194.1796 |           4.2476 |
[32m[20221208 13:56:43 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:56:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 960.33
[32m[20221208 13:56:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.74
[32m[20221208 13:56:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 690.72
[32m[20221208 13:56:43 @agent_ppo2.py:137][0m Total time:      11.38 min
[32m[20221208 13:56:43 @agent_ppo2.py:139][0m 940032 total steps have happened
[32m[20221208 13:56:43 @agent_ppo2.py:115][0m #------------------------ Iteration 459 --------------------------#
[32m[20221208 13:56:44 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:56:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |           0.0198 |         250.2729 |           4.1769 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0117 |         238.3386 |           4.1617 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0257 |         233.0836 |           4.1887 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0297 |         233.1581 |           4.2025 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0340 |         230.6536 |           4.2009 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0376 |         231.4872 |           4.2223 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0380 |         229.0962 |           4.2062 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0415 |         228.7441 |           4.2329 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0432 |         226.4841 |           4.2422 |
[32m[20221208 13:56:44 @agent_ppo2.py:179][0m |          -0.0422 |         228.2005 |           4.2496 |
[32m[20221208 13:56:44 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:56:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.94
[32m[20221208 13:56:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.18
[32m[20221208 13:56:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.91
[32m[20221208 13:56:45 @agent_ppo2.py:137][0m Total time:      11.41 min
[32m[20221208 13:56:45 @agent_ppo2.py:139][0m 942080 total steps have happened
[32m[20221208 13:56:45 @agent_ppo2.py:115][0m #------------------------ Iteration 460 --------------------------#
[32m[20221208 13:56:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:56:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:45 @agent_ppo2.py:179][0m |           0.0200 |         238.6254 |           4.2013 |
[32m[20221208 13:56:45 @agent_ppo2.py:179][0m |           0.0114 |         231.8962 |           4.0830 |
[32m[20221208 13:56:45 @agent_ppo2.py:179][0m |          -0.0130 |         229.8724 |           4.2090 |
[32m[20221208 13:56:46 @agent_ppo2.py:179][0m |          -0.0246 |         230.0275 |           4.2111 |
[32m[20221208 13:56:46 @agent_ppo2.py:179][0m |          -0.0259 |         229.5445 |           4.2193 |
[32m[20221208 13:56:46 @agent_ppo2.py:179][0m |          -0.0285 |         227.9041 |           4.2450 |
[32m[20221208 13:56:46 @agent_ppo2.py:179][0m |          -0.0277 |         228.8234 |           4.2414 |
[32m[20221208 13:56:46 @agent_ppo2.py:179][0m |          -0.0338 |         227.5729 |           4.2656 |
[32m[20221208 13:56:46 @agent_ppo2.py:179][0m |          -0.0334 |         226.8194 |           4.2579 |
[32m[20221208 13:56:46 @agent_ppo2.py:179][0m |          -0.0346 |         228.7392 |           4.2648 |
[32m[20221208 13:56:46 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:56:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.19
[32m[20221208 13:56:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.24
[32m[20221208 13:56:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 872.98
[32m[20221208 13:56:46 @agent_ppo2.py:137][0m Total time:      11.43 min
[32m[20221208 13:56:46 @agent_ppo2.py:139][0m 944128 total steps have happened
[32m[20221208 13:56:46 @agent_ppo2.py:115][0m #------------------------ Iteration 461 --------------------------#
[32m[20221208 13:56:47 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 13:56:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |           0.0373 |         248.6781 |           4.0502 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |           0.0005 |         241.1983 |           4.0564 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0258 |         238.1243 |           4.1361 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0288 |         236.1429 |           4.1522 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0313 |         235.6744 |           4.1750 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0343 |         234.7531 |           4.1803 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0375 |         234.9407 |           4.1901 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0379 |         234.3598 |           4.1996 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0395 |         232.2705 |           4.2112 |
[32m[20221208 13:56:47 @agent_ppo2.py:179][0m |          -0.0353 |         232.1778 |           4.2154 |
[32m[20221208 13:56:47 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:56:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 902.36
[32m[20221208 13:56:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.44
[32m[20221208 13:56:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.73
[32m[20221208 13:56:48 @agent_ppo2.py:137][0m Total time:      11.46 min
[32m[20221208 13:56:48 @agent_ppo2.py:139][0m 946176 total steps have happened
[32m[20221208 13:56:48 @agent_ppo2.py:115][0m #------------------------ Iteration 462 --------------------------#
[32m[20221208 13:56:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:48 @agent_ppo2.py:179][0m |           0.0476 |         237.5859 |           4.0649 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |           0.0169 |         228.8137 |           4.0700 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0175 |         225.7227 |           4.1681 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0240 |         224.2903 |           4.2006 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0249 |         223.6474 |           4.1981 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0308 |         222.9039 |           4.2181 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0326 |         221.6145 |           4.2241 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0329 |         220.8859 |           4.2386 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0348 |         220.5970 |           4.2491 |
[32m[20221208 13:56:49 @agent_ppo2.py:179][0m |          -0.0384 |         219.7429 |           4.2735 |
[32m[20221208 13:56:49 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:56:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.13
[32m[20221208 13:56:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.53
[32m[20221208 13:56:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 658.62
[32m[20221208 13:56:49 @agent_ppo2.py:137][0m Total time:      11.48 min
[32m[20221208 13:56:49 @agent_ppo2.py:139][0m 948224 total steps have happened
[32m[20221208 13:56:49 @agent_ppo2.py:115][0m #------------------------ Iteration 463 --------------------------#
[32m[20221208 13:56:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |           0.0207 |         253.1357 |           4.1966 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0042 |         244.7018 |           4.1493 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0170 |         239.7878 |           4.2118 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0204 |         238.7769 |           4.2470 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0265 |         237.6017 |           4.2400 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0321 |         237.9621 |           4.2435 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0348 |         237.0837 |           4.2667 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0354 |         236.0391 |           4.2592 |
[32m[20221208 13:56:50 @agent_ppo2.py:179][0m |          -0.0370 |         235.1648 |           4.2795 |
[32m[20221208 13:56:51 @agent_ppo2.py:179][0m |          -0.0410 |         234.9779 |           4.2849 |
[32m[20221208 13:56:51 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:56:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 896.35
[32m[20221208 13:56:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.43
[32m[20221208 13:56:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.51
[32m[20221208 13:56:51 @agent_ppo2.py:137][0m Total time:      11.51 min
[32m[20221208 13:56:51 @agent_ppo2.py:139][0m 950272 total steps have happened
[32m[20221208 13:56:51 @agent_ppo2.py:115][0m #------------------------ Iteration 464 --------------------------#
[32m[20221208 13:56:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |           0.0184 |         253.5149 |           4.4005 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |           0.0005 |         246.4937 |           4.3266 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0172 |         243.6090 |           4.3668 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0265 |         242.0345 |           4.3984 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0343 |         240.0793 |           4.4332 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0300 |         239.7562 |           4.4181 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0373 |         238.6038 |           4.4313 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0386 |         239.2250 |           4.4316 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0413 |         238.4710 |           4.4494 |
[32m[20221208 13:56:52 @agent_ppo2.py:179][0m |          -0.0449 |         237.4380 |           4.4550 |
[32m[20221208 13:56:52 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:56:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 852.30
[32m[20221208 13:56:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 896.26
[32m[20221208 13:56:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.56
[32m[20221208 13:56:53 @agent_ppo2.py:137][0m Total time:      11.54 min
[32m[20221208 13:56:53 @agent_ppo2.py:139][0m 952320 total steps have happened
[32m[20221208 13:56:53 @agent_ppo2.py:115][0m #------------------------ Iteration 465 --------------------------#
[32m[20221208 13:56:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:56:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:53 @agent_ppo2.py:179][0m |           0.0207 |         235.0499 |           4.1284 |
[32m[20221208 13:56:53 @agent_ppo2.py:179][0m |           0.0028 |         228.7337 |           4.1403 |
[32m[20221208 13:56:53 @agent_ppo2.py:179][0m |          -0.0089 |         227.3693 |           4.1865 |
[32m[20221208 13:56:53 @agent_ppo2.py:179][0m |          -0.0199 |         226.9431 |           4.2146 |
[32m[20221208 13:56:53 @agent_ppo2.py:179][0m |          -0.0086 |         227.4409 |           4.2060 |
[32m[20221208 13:56:53 @agent_ppo2.py:179][0m |          -0.0161 |         225.9979 |           4.2204 |
[32m[20221208 13:56:53 @agent_ppo2.py:179][0m |          -0.0233 |         225.8293 |           4.2350 |
[32m[20221208 13:56:54 @agent_ppo2.py:179][0m |          -0.0249 |         225.6697 |           4.2559 |
[32m[20221208 13:56:54 @agent_ppo2.py:179][0m |          -0.0227 |         225.1958 |           4.2379 |
[32m[20221208 13:56:54 @agent_ppo2.py:179][0m |          -0.0270 |         224.6693 |           4.2688 |
[32m[20221208 13:56:54 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:56:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 968.79
[32m[20221208 13:56:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.22
[32m[20221208 13:56:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 940.36
[32m[20221208 13:56:54 @agent_ppo2.py:137][0m Total time:      11.56 min
[32m[20221208 13:56:54 @agent_ppo2.py:139][0m 954368 total steps have happened
[32m[20221208 13:56:54 @agent_ppo2.py:115][0m #------------------------ Iteration 466 --------------------------#
[32m[20221208 13:56:55 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |           0.0187 |         235.2868 |           4.1077 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |           0.0163 |         233.0764 |           3.9732 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0010 |         231.3516 |           4.0622 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0223 |         230.3799 |           4.1348 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0290 |         230.3499 |           4.1906 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0288 |         229.5926 |           4.1919 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0311 |         229.4318 |           4.1907 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0303 |         228.5992 |           4.2020 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0316 |         228.2078 |           4.2167 |
[32m[20221208 13:56:55 @agent_ppo2.py:179][0m |          -0.0346 |         227.2320 |           4.2472 |
[32m[20221208 13:56:55 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:56:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.05
[32m[20221208 13:56:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.60
[32m[20221208 13:56:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 973.47
[32m[20221208 13:56:56 @agent_ppo2.py:137][0m Total time:      11.59 min
[32m[20221208 13:56:56 @agent_ppo2.py:139][0m 956416 total steps have happened
[32m[20221208 13:56:56 @agent_ppo2.py:115][0m #------------------------ Iteration 467 --------------------------#
[32m[20221208 13:56:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:56:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:56 @agent_ppo2.py:179][0m |           0.0215 |         250.7196 |           4.2828 |
[32m[20221208 13:56:56 @agent_ppo2.py:179][0m |           0.0007 |         247.7549 |           4.2529 |
[32m[20221208 13:56:56 @agent_ppo2.py:179][0m |          -0.0189 |         245.3948 |           4.3023 |
[32m[20221208 13:56:56 @agent_ppo2.py:179][0m |          -0.0244 |         244.2695 |           4.3499 |
[32m[20221208 13:56:56 @agent_ppo2.py:179][0m |          -0.0285 |         244.5785 |           4.3499 |
[32m[20221208 13:56:57 @agent_ppo2.py:179][0m |          -0.0265 |         243.6380 |           4.3415 |
[32m[20221208 13:56:57 @agent_ppo2.py:179][0m |          -0.0326 |         242.0330 |           4.3569 |
[32m[20221208 13:56:57 @agent_ppo2.py:179][0m |          -0.0349 |         241.5601 |           4.3737 |
[32m[20221208 13:56:57 @agent_ppo2.py:179][0m |          -0.0319 |         241.2660 |           4.3767 |
[32m[20221208 13:56:57 @agent_ppo2.py:179][0m |          -0.0345 |         241.7077 |           4.3640 |
[32m[20221208 13:56:57 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:56:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.48
[32m[20221208 13:56:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.93
[32m[20221208 13:56:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.49
[32m[20221208 13:56:57 @agent_ppo2.py:137][0m Total time:      11.61 min
[32m[20221208 13:56:57 @agent_ppo2.py:139][0m 958464 total steps have happened
[32m[20221208 13:56:57 @agent_ppo2.py:115][0m #------------------------ Iteration 468 --------------------------#
[32m[20221208 13:56:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |           0.0223 |         230.0283 |           4.1356 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |           0.0006 |         218.6757 |           4.0597 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0231 |         212.4112 |           4.1344 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0359 |         209.9014 |           4.1739 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0373 |         207.4003 |           4.1620 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0428 |         206.6686 |           4.1894 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0449 |         204.5451 |           4.2159 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0481 |         203.7113 |           4.2193 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0523 |         203.0843 |           4.2474 |
[32m[20221208 13:56:58 @agent_ppo2.py:179][0m |          -0.0523 |         202.4293 |           4.2517 |
[32m[20221208 13:56:58 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:56:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 817.31
[32m[20221208 13:56:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.92
[32m[20221208 13:56:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 681.03
[32m[20221208 13:56:59 @agent_ppo2.py:137][0m Total time:      11.64 min
[32m[20221208 13:56:59 @agent_ppo2.py:139][0m 960512 total steps have happened
[32m[20221208 13:56:59 @agent_ppo2.py:115][0m #------------------------ Iteration 469 --------------------------#
[32m[20221208 13:56:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:56:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:56:59 @agent_ppo2.py:179][0m |           0.0108 |         245.2508 |           4.2591 |
[32m[20221208 13:56:59 @agent_ppo2.py:179][0m |           0.0039 |         239.8076 |           4.1664 |
[32m[20221208 13:56:59 @agent_ppo2.py:179][0m |          -0.0055 |         238.7632 |           4.1642 |
[32m[20221208 13:57:00 @agent_ppo2.py:179][0m |          -0.0205 |         237.8010 |           4.2398 |
[32m[20221208 13:57:00 @agent_ppo2.py:179][0m |          -0.0221 |         238.5445 |           4.2430 |
[32m[20221208 13:57:00 @agent_ppo2.py:179][0m |          -0.0273 |         238.1602 |           4.2571 |
[32m[20221208 13:57:00 @agent_ppo2.py:179][0m |          -0.0274 |         237.6521 |           4.2671 |
[32m[20221208 13:57:00 @agent_ppo2.py:179][0m |          -0.0348 |         236.9604 |           4.2868 |
[32m[20221208 13:57:00 @agent_ppo2.py:179][0m |          -0.0335 |         236.8944 |           4.2905 |
[32m[20221208 13:57:00 @agent_ppo2.py:179][0m |          -0.0321 |         236.3100 |           4.2837 |
[32m[20221208 13:57:00 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 13:57:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.77
[32m[20221208 13:57:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.55
[32m[20221208 13:57:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.88
[32m[20221208 13:57:00 @agent_ppo2.py:137][0m Total time:      11.67 min
[32m[20221208 13:57:00 @agent_ppo2.py:139][0m 962560 total steps have happened
[32m[20221208 13:57:00 @agent_ppo2.py:115][0m #------------------------ Iteration 470 --------------------------#
[32m[20221208 13:57:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |           0.0412 |         256.3008 |           3.9751 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0087 |         247.4001 |           4.0075 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0214 |         242.9203 |           4.0237 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0296 |         238.4952 |           4.0656 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0355 |         237.5752 |           4.0664 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0388 |         235.6386 |           4.0827 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0420 |         233.6206 |           4.0840 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0433 |         232.5766 |           4.0983 |
[32m[20221208 13:57:01 @agent_ppo2.py:179][0m |          -0.0446 |         232.1250 |           4.1000 |
[32m[20221208 13:57:02 @agent_ppo2.py:179][0m |          -0.0456 |         230.9981 |           4.1087 |
[32m[20221208 13:57:02 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 891.75
[32m[20221208 13:57:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.68
[32m[20221208 13:57:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.26
[32m[20221208 13:57:02 @agent_ppo2.py:137][0m Total time:      11.69 min
[32m[20221208 13:57:02 @agent_ppo2.py:139][0m 964608 total steps have happened
[32m[20221208 13:57:02 @agent_ppo2.py:115][0m #------------------------ Iteration 471 --------------------------#
[32m[20221208 13:57:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |           0.0243 |         251.0826 |           4.1521 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |           0.0180 |         245.4115 |           3.9848 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0110 |         240.5666 |           4.0925 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0236 |         238.3626 |           4.1541 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0276 |         235.7874 |           4.1916 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0288 |         233.2343 |           4.1893 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0341 |         233.3916 |           4.1980 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0338 |         231.9819 |           4.2208 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0372 |         229.8982 |           4.2457 |
[32m[20221208 13:57:03 @agent_ppo2.py:179][0m |          -0.0392 |         227.6839 |           4.2468 |
[32m[20221208 13:57:03 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:57:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.07
[32m[20221208 13:57:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.16
[32m[20221208 13:57:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 899.93
[32m[20221208 13:57:04 @agent_ppo2.py:137][0m Total time:      11.72 min
[32m[20221208 13:57:04 @agent_ppo2.py:139][0m 966656 total steps have happened
[32m[20221208 13:57:04 @agent_ppo2.py:115][0m #------------------------ Iteration 472 --------------------------#
[32m[20221208 13:57:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:04 @agent_ppo2.py:179][0m |           0.0103 |         231.2555 |           4.0067 |
[32m[20221208 13:57:04 @agent_ppo2.py:179][0m |          -0.0107 |         206.7310 |           3.9684 |
[32m[20221208 13:57:04 @agent_ppo2.py:179][0m |          -0.0234 |         195.0162 |           3.9511 |
[32m[20221208 13:57:04 @agent_ppo2.py:179][0m |          -0.0299 |         188.6002 |           4.0084 |
[32m[20221208 13:57:04 @agent_ppo2.py:179][0m |          -0.0364 |         185.6103 |           4.0300 |
[32m[20221208 13:57:04 @agent_ppo2.py:179][0m |          -0.0390 |         182.3261 |           4.0391 |
[32m[20221208 13:57:04 @agent_ppo2.py:179][0m |          -0.0426 |         180.9103 |           4.0484 |
[32m[20221208 13:57:05 @agent_ppo2.py:179][0m |          -0.0419 |         179.0278 |           4.0545 |
[32m[20221208 13:57:05 @agent_ppo2.py:179][0m |          -0.0442 |         177.8637 |           4.0617 |
[32m[20221208 13:57:05 @agent_ppo2.py:179][0m |          -0.0453 |         175.8645 |           4.0737 |
[32m[20221208 13:57:05 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:57:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.28
[32m[20221208 13:57:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.39
[32m[20221208 13:57:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.26
[32m[20221208 13:57:05 @agent_ppo2.py:137][0m Total time:      11.75 min
[32m[20221208 13:57:05 @agent_ppo2.py:139][0m 968704 total steps have happened
[32m[20221208 13:57:05 @agent_ppo2.py:115][0m #------------------------ Iteration 473 --------------------------#
[32m[20221208 13:57:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |           0.0222 |         255.9398 |           4.1962 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0245 |         209.9654 |           4.1727 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0341 |         194.6225 |           4.1837 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0462 |         187.1785 |           4.2003 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0504 |         181.8284 |           4.2331 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0549 |         178.4895 |           4.2384 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0564 |         176.9525 |           4.2638 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0588 |         174.2294 |           4.2623 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0606 |         171.4554 |           4.2739 |
[32m[20221208 13:57:06 @agent_ppo2.py:179][0m |          -0.0615 |         170.9072 |           4.2890 |
[32m[20221208 13:57:06 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:57:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 834.02
[32m[20221208 13:57:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.32
[32m[20221208 13:57:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.22
[32m[20221208 13:57:07 @agent_ppo2.py:137][0m Total time:      11.77 min
[32m[20221208 13:57:07 @agent_ppo2.py:139][0m 970752 total steps have happened
[32m[20221208 13:57:07 @agent_ppo2.py:115][0m #------------------------ Iteration 474 --------------------------#
[32m[20221208 13:57:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:07 @agent_ppo2.py:179][0m |           0.0568 |         274.3401 |           4.1651 |
[32m[20221208 13:57:07 @agent_ppo2.py:179][0m |           0.0093 |         253.5955 |           4.1938 |
[32m[20221208 13:57:07 @agent_ppo2.py:179][0m |          -0.0217 |         249.1875 |           4.2764 |
[32m[20221208 13:57:07 @agent_ppo2.py:179][0m |          -0.0259 |         246.3849 |           4.2750 |
[32m[20221208 13:57:07 @agent_ppo2.py:179][0m |          -0.0356 |         243.7831 |           4.3304 |
[32m[20221208 13:57:08 @agent_ppo2.py:179][0m |          -0.0364 |         241.6514 |           4.3328 |
[32m[20221208 13:57:08 @agent_ppo2.py:179][0m |          -0.0430 |         240.8852 |           4.3585 |
[32m[20221208 13:57:08 @agent_ppo2.py:179][0m |          -0.0399 |         240.8146 |           4.3563 |
[32m[20221208 13:57:08 @agent_ppo2.py:179][0m |          -0.0441 |         239.5448 |           4.3779 |
[32m[20221208 13:57:08 @agent_ppo2.py:179][0m |          -0.0448 |         238.7217 |           4.3738 |
[32m[20221208 13:57:08 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 954.77
[32m[20221208 13:57:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.38
[32m[20221208 13:57:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.42
[32m[20221208 13:57:08 @agent_ppo2.py:137][0m Total time:      11.80 min
[32m[20221208 13:57:08 @agent_ppo2.py:139][0m 972800 total steps have happened
[32m[20221208 13:57:08 @agent_ppo2.py:115][0m #------------------------ Iteration 475 --------------------------#
[32m[20221208 13:57:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |           0.0294 |         255.2868 |           4.2874 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |           0.0115 |         242.9338 |           4.1711 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0195 |         237.4661 |           4.3056 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0322 |         233.1588 |           4.3613 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0386 |         231.2938 |           4.3674 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0419 |         228.8147 |           4.3841 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0443 |         228.7395 |           4.4177 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0490 |         226.0793 |           4.4339 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0506 |         225.9344 |           4.4579 |
[32m[20221208 13:57:09 @agent_ppo2.py:179][0m |          -0.0521 |         224.4958 |           4.4579 |
[32m[20221208 13:57:09 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:57:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 915.24
[32m[20221208 13:57:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.03
[32m[20221208 13:57:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 915.39
[32m[20221208 13:57:10 @agent_ppo2.py:137][0m Total time:      11.82 min
[32m[20221208 13:57:10 @agent_ppo2.py:139][0m 974848 total steps have happened
[32m[20221208 13:57:10 @agent_ppo2.py:115][0m #------------------------ Iteration 476 --------------------------#
[32m[20221208 13:57:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:10 @agent_ppo2.py:179][0m |           0.0205 |         260.0004 |           4.3943 |
[32m[20221208 13:57:10 @agent_ppo2.py:179][0m |           0.0081 |         242.3664 |           4.2734 |
[32m[20221208 13:57:10 @agent_ppo2.py:179][0m |          -0.0217 |         237.7099 |           4.3856 |
[32m[20221208 13:57:11 @agent_ppo2.py:179][0m |          -0.0181 |         235.2623 |           4.3665 |
[32m[20221208 13:57:11 @agent_ppo2.py:179][0m |          -0.0290 |         233.6222 |           4.4074 |
[32m[20221208 13:57:11 @agent_ppo2.py:179][0m |          -0.0324 |         230.1351 |           4.4232 |
[32m[20221208 13:57:11 @agent_ppo2.py:179][0m |          -0.0374 |         228.2221 |           4.4538 |
[32m[20221208 13:57:11 @agent_ppo2.py:179][0m |          -0.0327 |         227.5957 |           4.4348 |
[32m[20221208 13:57:11 @agent_ppo2.py:179][0m |          -0.0348 |         226.2059 |           4.4373 |
[32m[20221208 13:57:11 @agent_ppo2.py:179][0m |          -0.0391 |         226.1739 |           4.4830 |
[32m[20221208 13:57:11 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.23
[32m[20221208 13:57:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.75
[32m[20221208 13:57:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.83
[32m[20221208 13:57:11 @agent_ppo2.py:137][0m Total time:      11.85 min
[32m[20221208 13:57:11 @agent_ppo2.py:139][0m 976896 total steps have happened
[32m[20221208 13:57:11 @agent_ppo2.py:115][0m #------------------------ Iteration 477 --------------------------#
[32m[20221208 13:57:12 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |           0.0363 |         226.3771 |           4.2465 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |           0.0322 |         212.9666 |           4.0443 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |          -0.0155 |         208.0209 |           4.2163 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |          -0.0332 |         206.2245 |           4.2592 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |          -0.0336 |         203.8366 |           4.2467 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |          -0.0407 |         202.1332 |           4.2735 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |          -0.0418 |         201.3933 |           4.2816 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |          -0.0445 |         200.3202 |           4.3120 |
[32m[20221208 13:57:12 @agent_ppo2.py:179][0m |          -0.0484 |         199.1046 |           4.3194 |
[32m[20221208 13:57:13 @agent_ppo2.py:179][0m |          -0.0483 |         198.2452 |           4.3145 |
[32m[20221208 13:57:13 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:57:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 790.40
[32m[20221208 13:57:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.52
[32m[20221208 13:57:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.49
[32m[20221208 13:57:13 @agent_ppo2.py:137][0m Total time:      11.88 min
[32m[20221208 13:57:13 @agent_ppo2.py:139][0m 978944 total steps have happened
[32m[20221208 13:57:13 @agent_ppo2.py:115][0m #------------------------ Iteration 478 --------------------------#
[32m[20221208 13:57:13 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:13 @agent_ppo2.py:179][0m |           0.0305 |         209.7549 |           4.3356 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |           0.0047 |         197.6124 |           4.3350 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0209 |         191.8984 |           4.3927 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0287 |         188.6230 |           4.4076 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0296 |         186.5798 |           4.4351 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0376 |         184.6186 |           4.4586 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0413 |         183.0425 |           4.4794 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0399 |         182.5505 |           4.4844 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0452 |         181.7749 |           4.5145 |
[32m[20221208 13:57:14 @agent_ppo2.py:179][0m |          -0.0425 |         181.3398 |           4.5042 |
[32m[20221208 13:57:14 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:57:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 647.08
[32m[20221208 13:57:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.27
[32m[20221208 13:57:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 697.51
[32m[20221208 13:57:14 @agent_ppo2.py:137][0m Total time:      11.90 min
[32m[20221208 13:57:14 @agent_ppo2.py:139][0m 980992 total steps have happened
[32m[20221208 13:57:14 @agent_ppo2.py:115][0m #------------------------ Iteration 479 --------------------------#
[32m[20221208 13:57:15 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |           0.0264 |         235.7936 |           4.2989 |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |          -0.0032 |         226.7285 |           4.2700 |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |          -0.0171 |         224.0376 |           4.3312 |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |          -0.0212 |         222.4324 |           4.3663 |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |          -0.0227 |         222.0438 |           4.3769 |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |          -0.0316 |         220.8385 |           4.4000 |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |          -0.0318 |         220.8973 |           4.4227 |
[32m[20221208 13:57:15 @agent_ppo2.py:179][0m |          -0.0331 |         220.1682 |           4.4300 |
[32m[20221208 13:57:16 @agent_ppo2.py:179][0m |          -0.0346 |         218.9518 |           4.4424 |
[32m[20221208 13:57:16 @agent_ppo2.py:179][0m |          -0.0371 |         218.4888 |           4.4663 |
[32m[20221208 13:57:16 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:57:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.18
[32m[20221208 13:57:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.98
[32m[20221208 13:57:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 626.91
[32m[20221208 13:57:16 @agent_ppo2.py:137][0m Total time:      11.93 min
[32m[20221208 13:57:16 @agent_ppo2.py:139][0m 983040 total steps have happened
[32m[20221208 13:57:16 @agent_ppo2.py:115][0m #------------------------ Iteration 480 --------------------------#
[32m[20221208 13:57:16 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |           0.0378 |         238.8061 |           4.2267 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |           0.0113 |         232.7652 |           4.1582 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0158 |         230.5641 |           4.3151 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0251 |         229.4551 |           4.3389 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0300 |         230.4668 |           4.3854 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0346 |         228.2119 |           4.4024 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0341 |         229.4710 |           4.4135 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0335 |         229.2348 |           4.4149 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0358 |         227.3135 |           4.4273 |
[32m[20221208 13:57:17 @agent_ppo2.py:179][0m |          -0.0306 |         226.6818 |           4.4293 |
[32m[20221208 13:57:17 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.76
[32m[20221208 13:57:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.03
[32m[20221208 13:57:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 670.45
[32m[20221208 13:57:18 @agent_ppo2.py:137][0m Total time:      11.95 min
[32m[20221208 13:57:18 @agent_ppo2.py:139][0m 985088 total steps have happened
[32m[20221208 13:57:18 @agent_ppo2.py:115][0m #------------------------ Iteration 481 --------------------------#
[32m[20221208 13:57:18 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:57:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:18 @agent_ppo2.py:179][0m |           0.0282 |         263.9386 |           4.3491 |
[32m[20221208 13:57:18 @agent_ppo2.py:179][0m |           0.0010 |         239.7022 |           4.3345 |
[32m[20221208 13:57:18 @agent_ppo2.py:179][0m |          -0.0276 |         231.7541 |           4.3604 |
[32m[20221208 13:57:18 @agent_ppo2.py:179][0m |          -0.0412 |         222.4664 |           4.3838 |
[32m[20221208 13:57:18 @agent_ppo2.py:179][0m |          -0.0493 |         217.9858 |           4.3915 |
[32m[20221208 13:57:18 @agent_ppo2.py:179][0m |          -0.0521 |         214.6541 |           4.3822 |
[32m[20221208 13:57:19 @agent_ppo2.py:179][0m |          -0.0571 |         210.9778 |           4.4084 |
[32m[20221208 13:57:19 @agent_ppo2.py:179][0m |          -0.0573 |         209.0494 |           4.4162 |
[32m[20221208 13:57:19 @agent_ppo2.py:179][0m |          -0.0611 |         207.1674 |           4.4228 |
[32m[20221208 13:57:19 @agent_ppo2.py:179][0m |          -0.0666 |         204.9962 |           4.4536 |
[32m[20221208 13:57:19 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:57:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 667.65
[32m[20221208 13:57:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 826.05
[32m[20221208 13:57:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.36
[32m[20221208 13:57:19 @agent_ppo2.py:137][0m Total time:      11.98 min
[32m[20221208 13:57:19 @agent_ppo2.py:139][0m 987136 total steps have happened
[32m[20221208 13:57:19 @agent_ppo2.py:115][0m #------------------------ Iteration 482 --------------------------#
[32m[20221208 13:57:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |           0.0369 |         188.1851 |           4.5307 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |           0.0185 |         168.8657 |           4.4511 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0071 |         165.6409 |           4.5600 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0237 |         164.4261 |           4.5996 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0277 |         162.8242 |           4.5984 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0251 |         160.9850 |           4.6156 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0315 |         159.0912 |           4.6543 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0324 |         158.1919 |           4.6626 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0366 |         157.7628 |           4.6668 |
[32m[20221208 13:57:20 @agent_ppo2.py:179][0m |          -0.0430 |         156.8582 |           4.6801 |
[32m[20221208 13:57:20 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:57:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 636.29
[32m[20221208 13:57:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 931.59
[32m[20221208 13:57:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 781.85
[32m[20221208 13:57:21 @agent_ppo2.py:137][0m Total time:      12.00 min
[32m[20221208 13:57:21 @agent_ppo2.py:139][0m 989184 total steps have happened
[32m[20221208 13:57:21 @agent_ppo2.py:115][0m #------------------------ Iteration 483 --------------------------#
[32m[20221208 13:57:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:21 @agent_ppo2.py:179][0m |           0.0139 |         222.8763 |           4.5098 |
[32m[20221208 13:57:21 @agent_ppo2.py:179][0m |          -0.0078 |         195.6601 |           4.4668 |
[32m[20221208 13:57:21 @agent_ppo2.py:179][0m |          -0.0288 |         185.4111 |           4.4777 |
[32m[20221208 13:57:21 @agent_ppo2.py:179][0m |          -0.0364 |         178.7667 |           4.4893 |
[32m[20221208 13:57:21 @agent_ppo2.py:179][0m |          -0.0405 |         174.9501 |           4.4796 |
[32m[20221208 13:57:22 @agent_ppo2.py:179][0m |          -0.0448 |         173.3367 |           4.4857 |
[32m[20221208 13:57:22 @agent_ppo2.py:179][0m |          -0.0457 |         170.7972 |           4.4920 |
[32m[20221208 13:57:22 @agent_ppo2.py:179][0m |          -0.0475 |         169.8836 |           4.4928 |
[32m[20221208 13:57:22 @agent_ppo2.py:179][0m |          -0.0492 |         167.1876 |           4.4962 |
[32m[20221208 13:57:22 @agent_ppo2.py:179][0m |          -0.0529 |         164.1328 |           4.5303 |
[32m[20221208 13:57:22 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:57:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 845.40
[32m[20221208 13:57:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.16
[32m[20221208 13:57:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 800.14
[32m[20221208 13:57:22 @agent_ppo2.py:137][0m Total time:      12.03 min
[32m[20221208 13:57:22 @agent_ppo2.py:139][0m 991232 total steps have happened
[32m[20221208 13:57:22 @agent_ppo2.py:115][0m #------------------------ Iteration 484 --------------------------#
[32m[20221208 13:57:23 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |           0.0380 |         221.7372 |           4.2567 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |           0.0004 |         202.9865 |           4.2897 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0239 |         195.5795 |           4.3643 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0376 |         190.2970 |           4.3900 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0486 |         187.5104 |           4.4208 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0512 |         185.7768 |           4.4227 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0583 |         185.3168 |           4.4489 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0595 |         182.7542 |           4.4544 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0629 |         182.5290 |           4.4701 |
[32m[20221208 13:57:23 @agent_ppo2.py:179][0m |          -0.0626 |         180.8793 |           4.4647 |
[32m[20221208 13:57:23 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:57:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 737.56
[32m[20221208 13:57:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 851.29
[32m[20221208 13:57:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.90
[32m[20221208 13:57:24 @agent_ppo2.py:137][0m Total time:      12.06 min
[32m[20221208 13:57:24 @agent_ppo2.py:139][0m 993280 total steps have happened
[32m[20221208 13:57:24 @agent_ppo2.py:115][0m #------------------------ Iteration 485 --------------------------#
[32m[20221208 13:57:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:24 @agent_ppo2.py:179][0m |           0.0279 |         217.4002 |           4.6361 |
[32m[20221208 13:57:24 @agent_ppo2.py:179][0m |           0.0125 |         207.7322 |           4.5483 |
[32m[20221208 13:57:24 @agent_ppo2.py:179][0m |          -0.0231 |         201.7370 |           4.6558 |
[32m[20221208 13:57:25 @agent_ppo2.py:179][0m |          -0.0284 |         198.2225 |           4.6686 |
[32m[20221208 13:57:25 @agent_ppo2.py:179][0m |          -0.0399 |         195.8554 |           4.6922 |
[32m[20221208 13:57:25 @agent_ppo2.py:179][0m |          -0.0428 |         193.6467 |           4.6957 |
[32m[20221208 13:57:25 @agent_ppo2.py:179][0m |          -0.0468 |         192.5921 |           4.7097 |
[32m[20221208 13:57:25 @agent_ppo2.py:179][0m |          -0.0513 |         190.9211 |           4.7321 |
[32m[20221208 13:57:25 @agent_ppo2.py:179][0m |          -0.0526 |         189.9018 |           4.7353 |
[32m[20221208 13:57:25 @agent_ppo2.py:179][0m |          -0.0551 |         188.1106 |           4.7472 |
[32m[20221208 13:57:25 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:57:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 855.25
[32m[20221208 13:57:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 905.83
[32m[20221208 13:57:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 964.69
[32m[20221208 13:57:25 @agent_ppo2.py:137][0m Total time:      12.08 min
[32m[20221208 13:57:25 @agent_ppo2.py:139][0m 995328 total steps have happened
[32m[20221208 13:57:25 @agent_ppo2.py:115][0m #------------------------ Iteration 486 --------------------------#
[32m[20221208 13:57:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |           0.0188 |         228.6179 |           4.5242 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0144 |         218.1570 |           4.5049 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0248 |         216.0252 |           4.5076 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0315 |         211.7231 |           4.5267 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0383 |         208.9020 |           4.5300 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0434 |         207.7731 |           4.5538 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0448 |         205.9626 |           4.5598 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0428 |         204.7388 |           4.5707 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0463 |         203.6707 |           4.5680 |
[32m[20221208 13:57:26 @agent_ppo2.py:179][0m |          -0.0501 |         202.6006 |           4.5957 |
[32m[20221208 13:57:26 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.55
[32m[20221208 13:57:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 996.43
[32m[20221208 13:57:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 970.33
[32m[20221208 13:57:27 @agent_ppo2.py:137][0m Total time:      12.11 min
[32m[20221208 13:57:27 @agent_ppo2.py:139][0m 997376 total steps have happened
[32m[20221208 13:57:27 @agent_ppo2.py:115][0m #------------------------ Iteration 487 --------------------------#
[32m[20221208 13:57:27 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:57:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:27 @agent_ppo2.py:179][0m |           0.0225 |         198.7986 |           4.5026 |
[32m[20221208 13:57:27 @agent_ppo2.py:179][0m |          -0.0071 |         192.7866 |           4.4692 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0222 |         191.8089 |           4.5188 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0262 |         190.5362 |           4.5288 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0307 |         189.0124 |           4.5576 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0367 |         188.9827 |           4.5861 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0382 |         187.0514 |           4.5982 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0404 |         186.8039 |           4.6128 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0413 |         186.8259 |           4.6154 |
[32m[20221208 13:57:28 @agent_ppo2.py:179][0m |          -0.0423 |         185.8994 |           4.6259 |
[32m[20221208 13:57:28 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 13:57:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 948.75
[32m[20221208 13:57:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.38
[32m[20221208 13:57:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.74
[32m[20221208 13:57:28 @agent_ppo2.py:137][0m Total time:      12.14 min
[32m[20221208 13:57:28 @agent_ppo2.py:139][0m 999424 total steps have happened
[32m[20221208 13:57:28 @agent_ppo2.py:115][0m #------------------------ Iteration 488 --------------------------#
[32m[20221208 13:57:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |           0.0335 |         247.0412 |           4.5413 |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |           0.0014 |         236.8478 |           4.4448 |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |          -0.0146 |         231.7977 |           4.6186 |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |          -0.0287 |         230.9774 |           4.6124 |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |          -0.0314 |         230.8632 |           4.6581 |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |          -0.0332 |         227.1452 |           4.6429 |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |          -0.0352 |         227.0110 |           4.6646 |
[32m[20221208 13:57:29 @agent_ppo2.py:179][0m |          -0.0384 |         226.8772 |           4.6501 |
[32m[20221208 13:57:30 @agent_ppo2.py:179][0m |          -0.0407 |         225.3118 |           4.6645 |
[32m[20221208 13:57:30 @agent_ppo2.py:179][0m |          -0.0377 |         224.0736 |           4.6792 |
[32m[20221208 13:57:30 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.07
[32m[20221208 13:57:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.27
[32m[20221208 13:57:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.91
[32m[20221208 13:57:30 @agent_ppo2.py:137][0m Total time:      12.16 min
[32m[20221208 13:57:30 @agent_ppo2.py:139][0m 1001472 total steps have happened
[32m[20221208 13:57:30 @agent_ppo2.py:115][0m #------------------------ Iteration 489 --------------------------#
[32m[20221208 13:57:30 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:57:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |           0.0395 |         191.4829 |           4.3506 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |           0.0078 |         179.2435 |           4.2363 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0145 |         175.9591 |           4.3087 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0293 |         175.1702 |           4.4796 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0412 |         172.8217 |           4.5244 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0442 |         172.4798 |           4.5570 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0482 |         171.4392 |           4.6125 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0477 |         170.4146 |           4.6228 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0553 |         170.1660 |           4.6572 |
[32m[20221208 13:57:31 @agent_ppo2.py:179][0m |          -0.0544 |         168.8640 |           4.6591 |
[32m[20221208 13:57:31 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:57:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 596.09
[32m[20221208 13:57:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.67
[32m[20221208 13:57:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.07
[32m[20221208 13:57:32 @agent_ppo2.py:137][0m Total time:      12.19 min
[32m[20221208 13:57:32 @agent_ppo2.py:139][0m 1003520 total steps have happened
[32m[20221208 13:57:32 @agent_ppo2.py:115][0m #------------------------ Iteration 490 --------------------------#
[32m[20221208 13:57:32 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:32 @agent_ppo2.py:179][0m |           0.0188 |         180.3360 |           4.7427 |
[32m[20221208 13:57:32 @agent_ppo2.py:179][0m |          -0.0083 |         169.6318 |           4.7141 |
[32m[20221208 13:57:32 @agent_ppo2.py:179][0m |          -0.0138 |         167.0251 |           4.7168 |
[32m[20221208 13:57:32 @agent_ppo2.py:179][0m |          -0.0249 |         166.1099 |           4.7527 |
[32m[20221208 13:57:32 @agent_ppo2.py:179][0m |          -0.0307 |         164.0330 |           4.7598 |
[32m[20221208 13:57:32 @agent_ppo2.py:179][0m |          -0.0348 |         162.7930 |           4.7658 |
[32m[20221208 13:57:33 @agent_ppo2.py:179][0m |          -0.0365 |         162.2500 |           4.7893 |
[32m[20221208 13:57:33 @agent_ppo2.py:179][0m |          -0.0371 |         161.4838 |           4.7780 |
[32m[20221208 13:57:33 @agent_ppo2.py:179][0m |          -0.0393 |         161.2318 |           4.7784 |
[32m[20221208 13:57:33 @agent_ppo2.py:179][0m |          -0.0407 |         160.3570 |           4.7923 |
[32m[20221208 13:57:33 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:57:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 860.78
[32m[20221208 13:57:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.57
[32m[20221208 13:57:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 365.23
[32m[20221208 13:57:33 @agent_ppo2.py:137][0m Total time:      12.21 min
[32m[20221208 13:57:33 @agent_ppo2.py:139][0m 1005568 total steps have happened
[32m[20221208 13:57:33 @agent_ppo2.py:115][0m #------------------------ Iteration 491 --------------------------#
[32m[20221208 13:57:34 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:57:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |           0.0208 |         218.5511 |           4.8973 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0030 |         211.8718 |           4.8592 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0138 |         209.0223 |           4.8687 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0218 |         206.8409 |           4.9127 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0277 |         205.4961 |           4.9450 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0215 |         204.8492 |           4.8924 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0274 |         204.1916 |           4.8939 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0293 |         204.0338 |           4.9370 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0311 |         203.0934 |           4.9469 |
[32m[20221208 13:57:34 @agent_ppo2.py:179][0m |          -0.0317 |         203.5396 |           4.9391 |
[32m[20221208 13:57:34 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.22
[32m[20221208 13:57:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 938.26
[32m[20221208 13:57:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.43
[32m[20221208 13:57:35 @agent_ppo2.py:137][0m Total time:      12.24 min
[32m[20221208 13:57:35 @agent_ppo2.py:139][0m 1007616 total steps have happened
[32m[20221208 13:57:35 @agent_ppo2.py:115][0m #------------------------ Iteration 492 --------------------------#
[32m[20221208 13:57:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:57:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:35 @agent_ppo2.py:179][0m |           0.0395 |         215.0930 |           4.6567 |
[32m[20221208 13:57:35 @agent_ppo2.py:179][0m |          -0.0043 |         208.1093 |           4.6585 |
[32m[20221208 13:57:35 @agent_ppo2.py:179][0m |          -0.0204 |         203.4080 |           4.6660 |
[32m[20221208 13:57:35 @agent_ppo2.py:179][0m |          -0.0266 |         201.1155 |           4.7011 |
[32m[20221208 13:57:36 @agent_ppo2.py:179][0m |          -0.0348 |         198.5658 |           4.7098 |
[32m[20221208 13:57:36 @agent_ppo2.py:179][0m |          -0.0423 |         197.1312 |           4.7281 |
[32m[20221208 13:57:36 @agent_ppo2.py:179][0m |          -0.0404 |         196.5133 |           4.7188 |
[32m[20221208 13:57:36 @agent_ppo2.py:179][0m |          -0.0421 |         195.2790 |           4.7286 |
[32m[20221208 13:57:36 @agent_ppo2.py:179][0m |          -0.0468 |         194.2555 |           4.7536 |
[32m[20221208 13:57:36 @agent_ppo2.py:179][0m |          -0.0472 |         192.9110 |           4.7503 |
[32m[20221208 13:57:36 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:57:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 864.85
[32m[20221208 13:57:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.12
[32m[20221208 13:57:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.40
[32m[20221208 13:57:36 @agent_ppo2.py:137][0m Total time:      12.27 min
[32m[20221208 13:57:36 @agent_ppo2.py:139][0m 1009664 total steps have happened
[32m[20221208 13:57:36 @agent_ppo2.py:115][0m #------------------------ Iteration 493 --------------------------#
[32m[20221208 13:57:37 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |           0.0157 |         253.2540 |           4.7882 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0091 |         247.2651 |           4.7777 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0246 |         245.3485 |           4.8261 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0272 |         242.3994 |           4.8072 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0360 |         241.4491 |           4.8596 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0376 |         240.3808 |           4.8524 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0362 |         239.2052 |           4.8604 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0425 |         239.8390 |           4.8875 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0443 |         238.4503 |           4.9066 |
[32m[20221208 13:57:37 @agent_ppo2.py:179][0m |          -0.0442 |         237.5801 |           4.9056 |
[32m[20221208 13:57:37 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:57:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 848.17
[32m[20221208 13:57:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 961.85
[32m[20221208 13:57:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.96
[32m[20221208 13:57:38 @agent_ppo2.py:137][0m Total time:      12.29 min
[32m[20221208 13:57:38 @agent_ppo2.py:139][0m 1011712 total steps have happened
[32m[20221208 13:57:38 @agent_ppo2.py:115][0m #------------------------ Iteration 494 --------------------------#
[32m[20221208 13:57:38 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:38 @agent_ppo2.py:179][0m |           0.0267 |         245.5771 |           4.7334 |
[32m[20221208 13:57:38 @agent_ppo2.py:179][0m |           0.0008 |         233.5395 |           4.6992 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0267 |         228.2023 |           4.7646 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0344 |         224.5396 |           4.7478 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0426 |         222.5000 |           4.7818 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0454 |         221.1193 |           4.7797 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0451 |         220.8254 |           4.7915 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0476 |         219.2281 |           4.8048 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0518 |         218.7992 |           4.8155 |
[32m[20221208 13:57:39 @agent_ppo2.py:179][0m |          -0.0538 |         217.8189 |           4.8222 |
[32m[20221208 13:57:39 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:57:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 812.61
[32m[20221208 13:57:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.30
[32m[20221208 13:57:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.20
[32m[20221208 13:57:39 @agent_ppo2.py:137][0m Total time:      12.32 min
[32m[20221208 13:57:39 @agent_ppo2.py:139][0m 1013760 total steps have happened
[32m[20221208 13:57:39 @agent_ppo2.py:115][0m #------------------------ Iteration 495 --------------------------#
[32m[20221208 13:57:40 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:57:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |           0.0297 |         267.9005 |           4.7353 |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |           0.0016 |         255.5959 |           4.6683 |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |          -0.0207 |         251.2341 |           4.7816 |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |          -0.0288 |         248.7085 |           4.8233 |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |          -0.0315 |         246.0761 |           4.8080 |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |          -0.0357 |         245.4635 |           4.8331 |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |          -0.0386 |         243.7948 |           4.8598 |
[32m[20221208 13:57:40 @agent_ppo2.py:179][0m |          -0.0369 |         243.3294 |           4.8681 |
[32m[20221208 13:57:41 @agent_ppo2.py:179][0m |          -0.0382 |         241.5489 |           4.8479 |
[32m[20221208 13:57:41 @agent_ppo2.py:179][0m |          -0.0413 |         241.8453 |           4.8665 |
[32m[20221208 13:57:41 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:57:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.79
[32m[20221208 13:57:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.58
[32m[20221208 13:57:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.34
[32m[20221208 13:57:41 @agent_ppo2.py:137][0m Total time:      12.34 min
[32m[20221208 13:57:41 @agent_ppo2.py:139][0m 1015808 total steps have happened
[32m[20221208 13:57:41 @agent_ppo2.py:115][0m #------------------------ Iteration 496 --------------------------#
[32m[20221208 13:57:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |           0.0351 |         257.2283 |           4.7684 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |           0.0002 |         251.5602 |           4.7322 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0217 |         250.4194 |           4.8511 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0307 |         249.4056 |           4.8507 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0376 |         247.9749 |           4.8622 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0415 |         247.6385 |           4.8859 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0426 |         246.5755 |           4.9045 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0433 |         246.4012 |           4.9239 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0432 |         245.6386 |           4.9362 |
[32m[20221208 13:57:42 @agent_ppo2.py:179][0m |          -0.0466 |         245.3652 |           4.9322 |
[32m[20221208 13:57:42 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:57:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 876.54
[32m[20221208 13:57:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 946.48
[32m[20221208 13:57:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.83
[32m[20221208 13:57:43 @agent_ppo2.py:137][0m Total time:      12.37 min
[32m[20221208 13:57:43 @agent_ppo2.py:139][0m 1017856 total steps have happened
[32m[20221208 13:57:43 @agent_ppo2.py:115][0m #------------------------ Iteration 497 --------------------------#
[32m[20221208 13:57:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:43 @agent_ppo2.py:179][0m |           0.0291 |         254.0142 |           4.9368 |
[32m[20221208 13:57:43 @agent_ppo2.py:179][0m |           0.0075 |         246.6579 |           4.8576 |
[32m[20221208 13:57:43 @agent_ppo2.py:179][0m |          -0.0140 |         247.4126 |           4.9574 |
[32m[20221208 13:57:43 @agent_ppo2.py:179][0m |          -0.0192 |         244.3178 |           4.9411 |
[32m[20221208 13:57:43 @agent_ppo2.py:179][0m |          -0.0320 |         243.8409 |           5.0179 |
[32m[20221208 13:57:43 @agent_ppo2.py:179][0m |          -0.0341 |         243.4202 |           5.0402 |
[32m[20221208 13:57:44 @agent_ppo2.py:179][0m |          -0.0394 |         243.4426 |           5.0615 |
[32m[20221208 13:57:44 @agent_ppo2.py:179][0m |          -0.0371 |         242.5689 |           5.0466 |
[32m[20221208 13:57:44 @agent_ppo2.py:179][0m |          -0.0396 |         242.9174 |           5.0656 |
[32m[20221208 13:57:44 @agent_ppo2.py:179][0m |          -0.0416 |         243.7600 |           5.0792 |
[32m[20221208 13:57:44 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:57:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 857.68
[32m[20221208 13:57:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.55
[32m[20221208 13:57:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.12
[32m[20221208 13:57:44 @agent_ppo2.py:137][0m Total time:      12.40 min
[32m[20221208 13:57:44 @agent_ppo2.py:139][0m 1019904 total steps have happened
[32m[20221208 13:57:44 @agent_ppo2.py:115][0m #------------------------ Iteration 498 --------------------------#
[32m[20221208 13:57:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |           0.0200 |         248.8202 |           4.9662 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |           0.0111 |         246.0363 |           4.9013 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0151 |         243.0083 |           4.9308 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0230 |         241.9396 |           5.0117 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0263 |         241.3260 |           5.0217 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0281 |         240.8890 |           5.0296 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0278 |         242.8145 |           5.0315 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0299 |         241.9692 |           5.0271 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0367 |         241.5460 |           5.0578 |
[32m[20221208 13:57:45 @agent_ppo2.py:179][0m |          -0.0387 |         240.2420 |           5.0890 |
[32m[20221208 13:57:45 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:57:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.57
[32m[20221208 13:57:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 978.50
[32m[20221208 13:57:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.13
[32m[20221208 13:57:46 @agent_ppo2.py:137][0m Total time:      12.42 min
[32m[20221208 13:57:46 @agent_ppo2.py:139][0m 1021952 total steps have happened
[32m[20221208 13:57:46 @agent_ppo2.py:115][0m #------------------------ Iteration 499 --------------------------#
[32m[20221208 13:57:46 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:46 @agent_ppo2.py:179][0m |           0.0377 |         210.4650 |           4.7837 |
[32m[20221208 13:57:46 @agent_ppo2.py:179][0m |           0.0030 |         192.4463 |           4.7618 |
[32m[20221208 13:57:46 @agent_ppo2.py:179][0m |          -0.0217 |         186.3833 |           4.7861 |
[32m[20221208 13:57:46 @agent_ppo2.py:179][0m |          -0.0271 |         181.5011 |           4.8311 |
[32m[20221208 13:57:47 @agent_ppo2.py:179][0m |          -0.0368 |         178.6623 |           4.8648 |
[32m[20221208 13:57:47 @agent_ppo2.py:179][0m |          -0.0423 |         176.0495 |           4.9016 |
[32m[20221208 13:57:47 @agent_ppo2.py:179][0m |          -0.0464 |         173.3196 |           4.8947 |
[32m[20221208 13:57:47 @agent_ppo2.py:179][0m |          -0.0494 |         172.7428 |           4.9336 |
[32m[20221208 13:57:47 @agent_ppo2.py:179][0m |          -0.0489 |         171.0811 |           4.9285 |
[32m[20221208 13:57:47 @agent_ppo2.py:179][0m |          -0.0489 |         169.9854 |           4.9251 |
[32m[20221208 13:57:47 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:57:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 704.11
[32m[20221208 13:57:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 932.92
[32m[20221208 13:57:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.53
[32m[20221208 13:57:47 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 985.24
[32m[20221208 13:57:47 @agent_ppo2.py:137][0m Total time:      12.45 min
[32m[20221208 13:57:47 @agent_ppo2.py:139][0m 1024000 total steps have happened
[32m[20221208 13:57:47 @agent_ppo2.py:115][0m #------------------------ Iteration 500 --------------------------#
[32m[20221208 13:57:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |           0.0140 |         255.3346 |           4.9905 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0069 |         246.3314 |           5.0159 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0166 |         242.5465 |           5.0282 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0200 |         240.6846 |           5.0146 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0289 |         239.5508 |           5.0398 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0313 |         239.5833 |           5.0703 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0327 |         237.9143 |           5.0945 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0353 |         237.4396 |           5.1153 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0329 |         236.6781 |           5.1089 |
[32m[20221208 13:57:48 @agent_ppo2.py:179][0m |          -0.0399 |         235.6782 |           5.1202 |
[32m[20221208 13:57:48 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:57:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 911.57
[32m[20221208 13:57:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.87
[32m[20221208 13:57:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 914.85
[32m[20221208 13:57:49 @agent_ppo2.py:137][0m Total time:      12.48 min
[32m[20221208 13:57:49 @agent_ppo2.py:139][0m 1026048 total steps have happened
[32m[20221208 13:57:49 @agent_ppo2.py:115][0m #------------------------ Iteration 501 --------------------------#
[32m[20221208 13:57:49 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 13:57:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:49 @agent_ppo2.py:179][0m |           0.0478 |         254.4883 |           4.8826 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |           0.0218 |         250.1082 |           4.6634 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0105 |         248.1449 |           4.9242 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0173 |         247.0300 |           5.0328 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0228 |         246.8888 |           5.0294 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0250 |         247.6691 |           5.0634 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0279 |         246.3436 |           5.0589 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0165 |         245.9900 |           5.0145 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0326 |         246.0550 |           5.0791 |
[32m[20221208 13:57:50 @agent_ppo2.py:179][0m |          -0.0356 |         245.7351 |           5.1156 |
[32m[20221208 13:57:50 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:57:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 977.76
[32m[20221208 13:57:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 991.74
[32m[20221208 13:57:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 978.75
[32m[20221208 13:57:50 @agent_ppo2.py:137][0m Total time:      12.50 min
[32m[20221208 13:57:50 @agent_ppo2.py:139][0m 1028096 total steps have happened
[32m[20221208 13:57:50 @agent_ppo2.py:115][0m #------------------------ Iteration 502 --------------------------#
[32m[20221208 13:57:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |           0.0125 |         231.9141 |           5.0543 |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |          -0.0016 |         221.5848 |           4.9459 |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |          -0.0207 |         218.3072 |           4.9850 |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |          -0.0350 |         216.7253 |           5.0726 |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |          -0.0391 |         215.9807 |           5.0758 |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |          -0.0430 |         214.0563 |           5.1119 |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |          -0.0468 |         212.8454 |           5.1284 |
[32m[20221208 13:57:51 @agent_ppo2.py:179][0m |          -0.0471 |         212.2712 |           5.1353 |
[32m[20221208 13:57:52 @agent_ppo2.py:179][0m |          -0.0482 |         211.9368 |           5.1405 |
[32m[20221208 13:57:52 @agent_ppo2.py:179][0m |          -0.0506 |         211.2640 |           5.1349 |
[32m[20221208 13:57:52 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:57:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 777.38
[32m[20221208 13:57:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.12
[32m[20221208 13:57:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 922.86
[32m[20221208 13:57:52 @agent_ppo2.py:137][0m Total time:      12.53 min
[32m[20221208 13:57:52 @agent_ppo2.py:139][0m 1030144 total steps have happened
[32m[20221208 13:57:52 @agent_ppo2.py:115][0m #------------------------ Iteration 503 --------------------------#
[32m[20221208 13:57:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |           0.0301 |         245.9251 |           5.0940 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |           0.0167 |         238.9665 |           4.8931 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0047 |         235.5815 |           4.9690 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0220 |         233.1409 |           5.0852 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0248 |         231.1768 |           5.1130 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0327 |         228.9259 |           5.1295 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0308 |         228.0866 |           5.1470 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0294 |         226.2824 |           5.1279 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0287 |         225.1852 |           5.1454 |
[32m[20221208 13:57:53 @agent_ppo2.py:179][0m |          -0.0335 |         222.6613 |           5.1750 |
[32m[20221208 13:57:53 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:57:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 931.69
[32m[20221208 13:57:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.76
[32m[20221208 13:57:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.16
[32m[20221208 13:57:54 @agent_ppo2.py:137][0m Total time:      12.55 min
[32m[20221208 13:57:54 @agent_ppo2.py:139][0m 1032192 total steps have happened
[32m[20221208 13:57:54 @agent_ppo2.py:115][0m #------------------------ Iteration 504 --------------------------#
[32m[20221208 13:57:54 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:54 @agent_ppo2.py:179][0m |           0.0180 |         249.4301 |           5.0845 |
[32m[20221208 13:57:54 @agent_ppo2.py:179][0m |           0.0042 |         244.0791 |           4.9640 |
[32m[20221208 13:57:54 @agent_ppo2.py:179][0m |          -0.0193 |         241.7647 |           5.0286 |
[32m[20221208 13:57:54 @agent_ppo2.py:179][0m |          -0.0238 |         237.5666 |           5.0660 |
[32m[20221208 13:57:54 @agent_ppo2.py:179][0m |          -0.0293 |         235.7985 |           5.0848 |
[32m[20221208 13:57:54 @agent_ppo2.py:179][0m |          -0.0297 |         234.1262 |           5.1063 |
[32m[20221208 13:57:55 @agent_ppo2.py:179][0m |          -0.0351 |         232.8856 |           5.0970 |
[32m[20221208 13:57:55 @agent_ppo2.py:179][0m |          -0.0390 |         232.1533 |           5.1153 |
[32m[20221208 13:57:55 @agent_ppo2.py:179][0m |          -0.0339 |         231.4467 |           5.1102 |
[32m[20221208 13:57:55 @agent_ppo2.py:179][0m |          -0.0411 |         230.8074 |           5.1161 |
[32m[20221208 13:57:55 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:57:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 865.72
[32m[20221208 13:57:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.90
[32m[20221208 13:57:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 841.96
[32m[20221208 13:57:55 @agent_ppo2.py:137][0m Total time:      12.58 min
[32m[20221208 13:57:55 @agent_ppo2.py:139][0m 1034240 total steps have happened
[32m[20221208 13:57:55 @agent_ppo2.py:115][0m #------------------------ Iteration 505 --------------------------#
[32m[20221208 13:57:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |           0.0424 |         208.0395 |           5.0077 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |           0.0134 |         185.4288 |           4.8545 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |           0.0001 |         178.1331 |           4.8863 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |          -0.0177 |         174.8329 |           4.9951 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |          -0.0287 |         172.8034 |           5.0793 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |          -0.0320 |         170.6225 |           5.0759 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |          -0.0359 |         168.0980 |           5.0940 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |          -0.0402 |         167.0136 |           5.1102 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |          -0.0414 |         166.0496 |           5.1190 |
[32m[20221208 13:57:56 @agent_ppo2.py:179][0m |          -0.0450 |         164.1956 |           5.1389 |
[32m[20221208 13:57:56 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:57:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 914.51
[32m[20221208 13:57:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.33
[32m[20221208 13:57:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 904.39
[32m[20221208 13:57:57 @agent_ppo2.py:137][0m Total time:      12.61 min
[32m[20221208 13:57:57 @agent_ppo2.py:139][0m 1036288 total steps have happened
[32m[20221208 13:57:57 @agent_ppo2.py:115][0m #------------------------ Iteration 506 --------------------------#
[32m[20221208 13:57:57 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:57:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:57 @agent_ppo2.py:179][0m |           0.0193 |         234.1879 |           5.1685 |
[32m[20221208 13:57:57 @agent_ppo2.py:179][0m |          -0.0096 |         205.9346 |           5.1024 |
[32m[20221208 13:57:57 @agent_ppo2.py:179][0m |          -0.0273 |         197.9690 |           5.1627 |
[32m[20221208 13:57:57 @agent_ppo2.py:179][0m |          -0.0372 |         192.8489 |           5.1962 |
[32m[20221208 13:57:58 @agent_ppo2.py:179][0m |          -0.0403 |         189.0419 |           5.1752 |
[32m[20221208 13:57:58 @agent_ppo2.py:179][0m |          -0.0396 |         186.6122 |           5.1810 |
[32m[20221208 13:57:58 @agent_ppo2.py:179][0m |          -0.0439 |         185.4079 |           5.2031 |
[32m[20221208 13:57:58 @agent_ppo2.py:179][0m |          -0.0424 |         182.2323 |           5.1991 |
[32m[20221208 13:57:58 @agent_ppo2.py:179][0m |          -0.0467 |         180.5224 |           5.2178 |
[32m[20221208 13:57:58 @agent_ppo2.py:179][0m |          -0.0497 |         179.1675 |           5.2345 |
[32m[20221208 13:57:58 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:57:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 857.29
[32m[20221208 13:57:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.64
[32m[20221208 13:57:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 975.79
[32m[20221208 13:57:58 @agent_ppo2.py:137][0m Total time:      12.63 min
[32m[20221208 13:57:58 @agent_ppo2.py:139][0m 1038336 total steps have happened
[32m[20221208 13:57:58 @agent_ppo2.py:115][0m #------------------------ Iteration 507 --------------------------#
[32m[20221208 13:57:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:57:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |           0.0296 |         265.4072 |           5.0394 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0021 |         235.2240 |           4.9872 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0171 |         229.5547 |           5.0971 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0258 |         226.5377 |           5.1344 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0316 |         224.0376 |           5.1618 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0335 |         223.3661 |           5.1289 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0424 |         221.7246 |           5.2165 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0443 |         222.7733 |           5.2416 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0432 |         219.7643 |           5.2133 |
[32m[20221208 13:57:59 @agent_ppo2.py:179][0m |          -0.0420 |         219.7595 |           5.2334 |
[32m[20221208 13:57:59 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:58:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.07
[32m[20221208 13:58:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.84
[32m[20221208 13:58:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 926.46
[32m[20221208 13:58:00 @agent_ppo2.py:137][0m Total time:      12.66 min
[32m[20221208 13:58:00 @agent_ppo2.py:139][0m 1040384 total steps have happened
[32m[20221208 13:58:00 @agent_ppo2.py:115][0m #------------------------ Iteration 508 --------------------------#
[32m[20221208 13:58:00 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:58:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:00 @agent_ppo2.py:179][0m |           0.0273 |         261.1061 |           5.0267 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |           0.0074 |         252.5462 |           4.9887 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0091 |         250.5556 |           5.1288 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0229 |         247.6261 |           5.1839 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0252 |         247.0122 |           5.1864 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0278 |         245.8209 |           5.1777 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0345 |         246.1758 |           5.2095 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0334 |         243.9416 |           5.1968 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0352 |         243.4722 |           5.2122 |
[32m[20221208 13:58:01 @agent_ppo2.py:179][0m |          -0.0395 |         242.7359 |           5.2388 |
[32m[20221208 13:58:01 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:58:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.67
[32m[20221208 13:58:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.75
[32m[20221208 13:58:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.13
[32m[20221208 13:58:01 @agent_ppo2.py:137][0m Total time:      12.68 min
[32m[20221208 13:58:01 @agent_ppo2.py:139][0m 1042432 total steps have happened
[32m[20221208 13:58:01 @agent_ppo2.py:115][0m #------------------------ Iteration 509 --------------------------#
[32m[20221208 13:58:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |           0.0339 |         247.9031 |           5.1424 |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |           0.0026 |         239.0371 |           5.0369 |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |          -0.0206 |         233.7789 |           5.1560 |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |          -0.0303 |         231.7294 |           5.1904 |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |          -0.0324 |         230.0125 |           5.1650 |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |          -0.0378 |         228.1811 |           5.2087 |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |          -0.0390 |         227.6698 |           5.2125 |
[32m[20221208 13:58:02 @agent_ppo2.py:179][0m |          -0.0398 |         228.2513 |           5.2119 |
[32m[20221208 13:58:03 @agent_ppo2.py:179][0m |          -0.0442 |         223.8221 |           5.2272 |
[32m[20221208 13:58:03 @agent_ppo2.py:179][0m |          -0.0441 |         222.6090 |           5.2275 |
[32m[20221208 13:58:03 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:58:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 893.28
[32m[20221208 13:58:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.37
[32m[20221208 13:58:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 987.40
[32m[20221208 13:58:03 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 987.40
[32m[20221208 13:58:03 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 987.40
[32m[20221208 13:58:03 @agent_ppo2.py:137][0m Total time:      12.71 min
[32m[20221208 13:58:03 @agent_ppo2.py:139][0m 1044480 total steps have happened
[32m[20221208 13:58:03 @agent_ppo2.py:115][0m #------------------------ Iteration 510 --------------------------#
[32m[20221208 13:58:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |           0.0148 |         257.7199 |           4.8858 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |           0.0036 |         243.7832 |           4.8480 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |           0.0026 |         239.0631 |           4.7978 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |          -0.0222 |         237.9998 |           4.9323 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |          -0.0332 |         236.2806 |           4.9224 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |          -0.0361 |         235.7651 |           4.9212 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |          -0.0354 |         233.8851 |           4.9311 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |          -0.0413 |         234.2739 |           4.9267 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |          -0.0397 |         234.4918 |           4.9298 |
[32m[20221208 13:58:04 @agent_ppo2.py:179][0m |          -0.0417 |         232.3359 |           4.9473 |
[32m[20221208 13:58:04 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:58:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 782.80
[32m[20221208 13:58:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 864.90
[32m[20221208 13:58:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 928.90
[32m[20221208 13:58:05 @agent_ppo2.py:137][0m Total time:      12.74 min
[32m[20221208 13:58:05 @agent_ppo2.py:139][0m 1046528 total steps have happened
[32m[20221208 13:58:05 @agent_ppo2.py:115][0m #------------------------ Iteration 511 --------------------------#
[32m[20221208 13:58:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:05 @agent_ppo2.py:179][0m |           0.0161 |         248.2269 |           5.0701 |
[32m[20221208 13:58:05 @agent_ppo2.py:179][0m |           0.0087 |         237.2429 |           5.0044 |
[32m[20221208 13:58:05 @agent_ppo2.py:179][0m |          -0.0199 |         232.5251 |           5.1223 |
[32m[20221208 13:58:05 @agent_ppo2.py:179][0m |          -0.0254 |         230.6263 |           5.1420 |
[32m[20221208 13:58:05 @agent_ppo2.py:179][0m |          -0.0316 |         229.0580 |           5.1557 |
[32m[20221208 13:58:05 @agent_ppo2.py:179][0m |          -0.0336 |         228.6473 |           5.1653 |
[32m[20221208 13:58:06 @agent_ppo2.py:179][0m |          -0.0332 |         227.9115 |           5.1598 |
[32m[20221208 13:58:06 @agent_ppo2.py:179][0m |          -0.0361 |         227.5448 |           5.1795 |
[32m[20221208 13:58:06 @agent_ppo2.py:179][0m |          -0.0417 |         227.8410 |           5.1952 |
[32m[20221208 13:58:06 @agent_ppo2.py:179][0m |          -0.0418 |         226.3601 |           5.2047 |
[32m[20221208 13:58:06 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 13:58:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 919.44
[32m[20221208 13:58:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.40
[32m[20221208 13:58:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.90
[32m[20221208 13:58:06 @agent_ppo2.py:137][0m Total time:      12.76 min
[32m[20221208 13:58:06 @agent_ppo2.py:139][0m 1048576 total steps have happened
[32m[20221208 13:58:06 @agent_ppo2.py:115][0m #------------------------ Iteration 512 --------------------------#
[32m[20221208 13:58:07 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |           0.0202 |         227.0748 |           5.0898 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0010 |         212.9244 |           4.9547 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0262 |         205.6075 |           5.0223 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0311 |         200.6953 |           5.0631 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0370 |         197.7679 |           5.0683 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0394 |         194.5378 |           5.0624 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0450 |         192.3295 |           5.0858 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0448 |         189.7772 |           5.0773 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0441 |         187.8879 |           5.0896 |
[32m[20221208 13:58:07 @agent_ppo2.py:179][0m |          -0.0446 |         186.4839 |           5.0965 |
[32m[20221208 13:58:07 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:58:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 832.74
[32m[20221208 13:58:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.47
[32m[20221208 13:58:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.42
[32m[20221208 13:58:08 @agent_ppo2.py:137][0m Total time:      12.79 min
[32m[20221208 13:58:08 @agent_ppo2.py:139][0m 1050624 total steps have happened
[32m[20221208 13:58:08 @agent_ppo2.py:115][0m #------------------------ Iteration 513 --------------------------#
[32m[20221208 13:58:08 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:08 @agent_ppo2.py:179][0m |           0.0115 |         247.5476 |           5.1110 |
[32m[20221208 13:58:08 @agent_ppo2.py:179][0m |          -0.0055 |         232.0768 |           5.0327 |
[32m[20221208 13:58:08 @agent_ppo2.py:179][0m |          -0.0232 |         228.3556 |           5.0492 |
[32m[20221208 13:58:09 @agent_ppo2.py:179][0m |          -0.0338 |         227.5242 |           5.1108 |
[32m[20221208 13:58:09 @agent_ppo2.py:179][0m |          -0.0395 |         225.3408 |           5.1306 |
[32m[20221208 13:58:09 @agent_ppo2.py:179][0m |          -0.0383 |         224.0523 |           5.1446 |
[32m[20221208 13:58:09 @agent_ppo2.py:179][0m |          -0.0428 |         222.9608 |           5.1413 |
[32m[20221208 13:58:09 @agent_ppo2.py:179][0m |          -0.0463 |         222.2136 |           5.1310 |
[32m[20221208 13:58:09 @agent_ppo2.py:179][0m |          -0.0476 |         222.1554 |           5.1568 |
[32m[20221208 13:58:09 @agent_ppo2.py:179][0m |          -0.0491 |         221.5140 |           5.1515 |
[32m[20221208 13:58:09 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 13:58:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 861.59
[32m[20221208 13:58:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.65
[32m[20221208 13:58:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.30
[32m[20221208 13:58:09 @agent_ppo2.py:137][0m Total time:      12.82 min
[32m[20221208 13:58:09 @agent_ppo2.py:139][0m 1052672 total steps have happened
[32m[20221208 13:58:09 @agent_ppo2.py:115][0m #------------------------ Iteration 514 --------------------------#
[32m[20221208 13:58:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |           0.0264 |         241.7452 |           4.7558 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0058 |         225.0104 |           4.7377 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0186 |         220.5408 |           4.7668 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0286 |         218.0677 |           4.7875 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0346 |         216.7388 |           4.8011 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0344 |         215.5993 |           4.8217 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0379 |         214.3550 |           4.8353 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0373 |         213.0193 |           4.8358 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0407 |         211.6898 |           4.8514 |
[32m[20221208 13:58:10 @agent_ppo2.py:179][0m |          -0.0419 |         210.8112 |           4.8680 |
[32m[20221208 13:58:10 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:58:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 900.57
[32m[20221208 13:58:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.02
[32m[20221208 13:58:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 884.23
[32m[20221208 13:58:11 @agent_ppo2.py:137][0m Total time:      12.84 min
[32m[20221208 13:58:11 @agent_ppo2.py:139][0m 1054720 total steps have happened
[32m[20221208 13:58:11 @agent_ppo2.py:115][0m #------------------------ Iteration 515 --------------------------#
[32m[20221208 13:58:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:58:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:11 @agent_ppo2.py:179][0m |           0.0308 |         211.2346 |           4.9362 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |           0.0053 |         199.6316 |           4.5725 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0273 |         195.0046 |           4.7097 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0368 |         192.1135 |           4.7722 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0433 |         189.8339 |           4.8055 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0474 |         186.6000 |           4.8321 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0479 |         186.3123 |           4.8493 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0537 |         185.0001 |           4.8655 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0541 |         183.8533 |           4.8858 |
[32m[20221208 13:58:12 @agent_ppo2.py:179][0m |          -0.0566 |         183.7788 |           4.8971 |
[32m[20221208 13:58:12 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 13:58:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 835.42
[32m[20221208 13:58:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.23
[32m[20221208 13:58:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 415.34
[32m[20221208 13:58:12 @agent_ppo2.py:137][0m Total time:      12.87 min
[32m[20221208 13:58:12 @agent_ppo2.py:139][0m 1056768 total steps have happened
[32m[20221208 13:58:12 @agent_ppo2.py:115][0m #------------------------ Iteration 516 --------------------------#
[32m[20221208 13:58:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:13 @agent_ppo2.py:179][0m |           0.0211 |         249.4311 |           5.1712 |
[32m[20221208 13:58:13 @agent_ppo2.py:179][0m |          -0.0015 |         242.5955 |           5.0808 |
[32m[20221208 13:58:13 @agent_ppo2.py:179][0m |          -0.0016 |         237.4599 |           5.0789 |
[32m[20221208 13:58:13 @agent_ppo2.py:179][0m |          -0.0256 |         235.3999 |           5.1597 |
[32m[20221208 13:58:13 @agent_ppo2.py:179][0m |          -0.0322 |         233.9859 |           5.2340 |
[32m[20221208 13:58:13 @agent_ppo2.py:179][0m |          -0.0348 |         233.0911 |           5.2510 |
[32m[20221208 13:58:13 @agent_ppo2.py:179][0m |          -0.0372 |         232.2210 |           5.2734 |
[32m[20221208 13:58:14 @agent_ppo2.py:179][0m |          -0.0372 |         233.2334 |           5.3050 |
[32m[20221208 13:58:14 @agent_ppo2.py:179][0m |          -0.0382 |         229.9623 |           5.2981 |
[32m[20221208 13:58:14 @agent_ppo2.py:179][0m |          -0.0430 |         229.0344 |           5.3294 |
[32m[20221208 13:58:14 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:58:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.78
[32m[20221208 13:58:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 977.78
[32m[20221208 13:58:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.30
[32m[20221208 13:58:14 @agent_ppo2.py:137][0m Total time:      12.90 min
[32m[20221208 13:58:14 @agent_ppo2.py:139][0m 1058816 total steps have happened
[32m[20221208 13:58:14 @agent_ppo2.py:115][0m #------------------------ Iteration 517 --------------------------#
[32m[20221208 13:58:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:58:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |           0.0226 |         249.3946 |           5.2030 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0030 |         236.4197 |           5.1299 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0276 |         230.4657 |           5.2102 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0334 |         230.3825 |           5.2008 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0378 |         227.7540 |           5.2181 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0468 |         226.3161 |           5.2274 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0470 |         225.7198 |           5.2148 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0425 |         225.6652 |           5.2221 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0485 |         224.1230 |           5.2276 |
[32m[20221208 13:58:15 @agent_ppo2.py:179][0m |          -0.0536 |         223.8218 |           5.2267 |
[32m[20221208 13:58:15 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:58:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 742.80
[32m[20221208 13:58:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.79
[32m[20221208 13:58:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 905.78
[32m[20221208 13:58:16 @agent_ppo2.py:137][0m Total time:      12.92 min
[32m[20221208 13:58:16 @agent_ppo2.py:139][0m 1060864 total steps have happened
[32m[20221208 13:58:16 @agent_ppo2.py:115][0m #------------------------ Iteration 518 --------------------------#
[32m[20221208 13:58:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:16 @agent_ppo2.py:179][0m |           0.0096 |         245.6398 |           5.1222 |
[32m[20221208 13:58:16 @agent_ppo2.py:179][0m |          -0.0081 |         237.6841 |           5.0526 |
[32m[20221208 13:58:16 @agent_ppo2.py:179][0m |          -0.0179 |         234.3700 |           5.0684 |
[32m[20221208 13:58:16 @agent_ppo2.py:179][0m |          -0.0325 |         230.0916 |           5.1370 |
[32m[20221208 13:58:16 @agent_ppo2.py:179][0m |          -0.0337 |         227.5194 |           5.1457 |
[32m[20221208 13:58:17 @agent_ppo2.py:179][0m |          -0.0318 |         226.8967 |           5.1416 |
[32m[20221208 13:58:17 @agent_ppo2.py:179][0m |          -0.0343 |         226.8485 |           5.1407 |
[32m[20221208 13:58:17 @agent_ppo2.py:179][0m |          -0.0387 |         224.4101 |           5.1638 |
[32m[20221208 13:58:17 @agent_ppo2.py:179][0m |          -0.0412 |         224.0639 |           5.1733 |
[32m[20221208 13:58:17 @agent_ppo2.py:179][0m |          -0.0419 |         223.7138 |           5.1785 |
[32m[20221208 13:58:17 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:58:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 929.30
[32m[20221208 13:58:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.51
[32m[20221208 13:58:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 889.54
[32m[20221208 13:58:17 @agent_ppo2.py:137][0m Total time:      12.95 min
[32m[20221208 13:58:17 @agent_ppo2.py:139][0m 1062912 total steps have happened
[32m[20221208 13:58:17 @agent_ppo2.py:115][0m #------------------------ Iteration 519 --------------------------#
[32m[20221208 13:58:18 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |           0.0233 |         198.5048 |           5.1519 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |           0.0085 |         190.0333 |           5.0590 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0068 |         187.1314 |           5.1208 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0055 |         185.7251 |           4.9795 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0066 |         185.2364 |           4.6721 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0486 |         185.1993 |           4.0962 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0577 |         184.7931 |           4.1435 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0623 |         183.4878 |           4.1938 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0668 |         183.9856 |           4.2045 |
[32m[20221208 13:58:18 @agent_ppo2.py:179][0m |          -0.0684 |         183.6575 |           4.2275 |
[32m[20221208 13:58:18 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:58:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 703.71
[32m[20221208 13:58:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.38
[32m[20221208 13:58:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.84
[32m[20221208 13:58:19 @agent_ppo2.py:137][0m Total time:      12.97 min
[32m[20221208 13:58:19 @agent_ppo2.py:139][0m 1064960 total steps have happened
[32m[20221208 13:58:19 @agent_ppo2.py:115][0m #------------------------ Iteration 520 --------------------------#
[32m[20221208 13:58:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:19 @agent_ppo2.py:179][0m |           0.0214 |         239.9395 |           5.1510 |
[32m[20221208 13:58:19 @agent_ppo2.py:179][0m |          -0.0075 |         234.4854 |           5.1494 |
[32m[20221208 13:58:19 @agent_ppo2.py:179][0m |          -0.0171 |         234.1126 |           5.1736 |
[32m[20221208 13:58:20 @agent_ppo2.py:179][0m |          -0.0259 |         231.2018 |           5.2493 |
[32m[20221208 13:58:20 @agent_ppo2.py:179][0m |          -0.0328 |         229.7349 |           5.2582 |
[32m[20221208 13:58:20 @agent_ppo2.py:179][0m |          -0.0360 |         229.6700 |           5.2791 |
[32m[20221208 13:58:20 @agent_ppo2.py:179][0m |          -0.0397 |         228.9456 |           5.3228 |
[32m[20221208 13:58:20 @agent_ppo2.py:179][0m |          -0.0395 |         228.2774 |           5.3264 |
[32m[20221208 13:58:20 @agent_ppo2.py:179][0m |          -0.0385 |         228.9752 |           5.3231 |
[32m[20221208 13:58:20 @agent_ppo2.py:179][0m |          -0.0305 |         227.9439 |           5.2682 |
[32m[20221208 13:58:20 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:58:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.81
[32m[20221208 13:58:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.80
[32m[20221208 13:58:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.30
[32m[20221208 13:58:20 @agent_ppo2.py:137][0m Total time:      13.00 min
[32m[20221208 13:58:20 @agent_ppo2.py:139][0m 1067008 total steps have happened
[32m[20221208 13:58:20 @agent_ppo2.py:115][0m #------------------------ Iteration 521 --------------------------#
[32m[20221208 13:58:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |           0.0223 |         248.8750 |           5.2460 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0063 |         235.7153 |           5.1537 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0231 |         227.8062 |           5.2289 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0365 |         223.1078 |           5.2720 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0426 |         220.3429 |           5.2803 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0467 |         215.9019 |           5.3091 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0416 |         212.9203 |           5.2913 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0480 |         211.2671 |           5.3248 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0507 |         208.5667 |           5.3286 |
[32m[20221208 13:58:21 @agent_ppo2.py:179][0m |          -0.0533 |         206.7679 |           5.3499 |
[32m[20221208 13:58:21 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 829.63
[32m[20221208 13:58:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.13
[32m[20221208 13:58:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.46
[32m[20221208 13:58:22 @agent_ppo2.py:137][0m Total time:      13.03 min
[32m[20221208 13:58:22 @agent_ppo2.py:139][0m 1069056 total steps have happened
[32m[20221208 13:58:22 @agent_ppo2.py:115][0m #------------------------ Iteration 522 --------------------------#
[32m[20221208 13:58:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:22 @agent_ppo2.py:179][0m |           0.0224 |         245.4873 |           5.3355 |
[32m[20221208 13:58:22 @agent_ppo2.py:179][0m |          -0.0037 |         240.4337 |           5.2768 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0195 |         238.1866 |           5.3678 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0283 |         236.6611 |           5.4048 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0291 |         234.3861 |           5.4198 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0352 |         233.0520 |           5.4553 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0395 |         232.3620 |           5.4806 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0384 |         233.1122 |           5.5107 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0387 |         231.3754 |           5.5123 |
[32m[20221208 13:58:23 @agent_ppo2.py:179][0m |          -0.0392 |         230.1692 |           5.4909 |
[32m[20221208 13:58:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:58:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.97
[32m[20221208 13:58:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.94
[32m[20221208 13:58:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 971.71
[32m[20221208 13:58:23 @agent_ppo2.py:137][0m Total time:      13.05 min
[32m[20221208 13:58:23 @agent_ppo2.py:139][0m 1071104 total steps have happened
[32m[20221208 13:58:23 @agent_ppo2.py:115][0m #------------------------ Iteration 523 --------------------------#
[32m[20221208 13:58:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |           0.0317 |         237.9293 |           5.2913 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |           0.0162 |         230.2441 |           5.1335 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0106 |         224.9391 |           5.3099 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0249 |         222.4251 |           5.3393 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0325 |         220.5169 |           5.4095 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0319 |         219.9456 |           5.4059 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0346 |         217.4749 |           5.4040 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0374 |         215.7596 |           5.4256 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0400 |         214.4547 |           5.4562 |
[32m[20221208 13:58:24 @agent_ppo2.py:179][0m |          -0.0373 |         213.5554 |           5.4484 |
[32m[20221208 13:58:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:58:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 892.38
[32m[20221208 13:58:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.83
[32m[20221208 13:58:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.00
[32m[20221208 13:58:25 @agent_ppo2.py:137][0m Total time:      13.08 min
[32m[20221208 13:58:25 @agent_ppo2.py:139][0m 1073152 total steps have happened
[32m[20221208 13:58:25 @agent_ppo2.py:115][0m #------------------------ Iteration 524 --------------------------#
[32m[20221208 13:58:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:25 @agent_ppo2.py:179][0m |           0.0212 |         239.4838 |           5.6164 |
[32m[20221208 13:58:25 @agent_ppo2.py:179][0m |           0.0046 |         231.4696 |           5.4610 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0188 |         229.9135 |           5.5966 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0212 |         229.2388 |           5.5843 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0269 |         229.6455 |           5.6463 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0356 |         228.2368 |           5.6875 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0359 |         227.4526 |           5.7029 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0379 |         227.1405 |           5.7165 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0403 |         226.4134 |           5.7463 |
[32m[20221208 13:58:26 @agent_ppo2.py:179][0m |          -0.0418 |         225.8812 |           5.7499 |
[32m[20221208 13:58:26 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.06
[32m[20221208 13:58:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.42
[32m[20221208 13:58:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 969.70
[32m[20221208 13:58:26 @agent_ppo2.py:137][0m Total time:      13.10 min
[32m[20221208 13:58:26 @agent_ppo2.py:139][0m 1075200 total steps have happened
[32m[20221208 13:58:26 @agent_ppo2.py:115][0m #------------------------ Iteration 525 --------------------------#
[32m[20221208 13:58:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |           0.0427 |         243.6157 |           5.3674 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |           0.0085 |         237.2625 |           5.2566 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0168 |         234.9250 |           5.4923 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0223 |         232.9708 |           5.5094 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0284 |         231.9880 |           5.5575 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0319 |         231.7367 |           5.5654 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0342 |         229.7263 |           5.6032 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0359 |         228.8618 |           5.6276 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0361 |         228.5710 |           5.6330 |
[32m[20221208 13:58:27 @agent_ppo2.py:179][0m |          -0.0360 |         229.3755 |           5.6508 |
[32m[20221208 13:58:27 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 975.08
[32m[20221208 13:58:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.01
[32m[20221208 13:58:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.41
[32m[20221208 13:58:28 @agent_ppo2.py:137][0m Total time:      13.13 min
[32m[20221208 13:58:28 @agent_ppo2.py:139][0m 1077248 total steps have happened
[32m[20221208 13:58:28 @agent_ppo2.py:115][0m #------------------------ Iteration 526 --------------------------#
[32m[20221208 13:58:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:28 @agent_ppo2.py:179][0m |           0.0353 |         240.4472 |           5.3423 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |           0.0204 |         232.2197 |           5.2778 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0066 |         226.4615 |           5.2649 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0286 |         223.5101 |           5.4289 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0362 |         220.6002 |           5.4536 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0380 |         216.7168 |           5.4694 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0428 |         215.6164 |           5.4885 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0452 |         213.2954 |           5.5027 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0488 |         212.7314 |           5.5265 |
[32m[20221208 13:58:29 @agent_ppo2.py:179][0m |          -0.0495 |         211.8464 |           5.5329 |
[32m[20221208 13:58:29 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 869.39
[32m[20221208 13:58:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.67
[32m[20221208 13:58:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.63
[32m[20221208 13:58:29 @agent_ppo2.py:137][0m Total time:      13.15 min
[32m[20221208 13:58:29 @agent_ppo2.py:139][0m 1079296 total steps have happened
[32m[20221208 13:58:29 @agent_ppo2.py:115][0m #------------------------ Iteration 527 --------------------------#
[32m[20221208 13:58:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |           0.0255 |         241.0105 |           5.4118 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |           0.0097 |         231.2759 |           5.3254 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0105 |         226.8773 |           5.4515 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0251 |         225.9431 |           5.4574 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0301 |         225.2928 |           5.5048 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0342 |         224.1371 |           5.5337 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0344 |         222.9899 |           5.5252 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0368 |         222.3274 |           5.5503 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0397 |         221.9597 |           5.5444 |
[32m[20221208 13:58:30 @agent_ppo2.py:179][0m |          -0.0365 |         222.9624 |           5.5592 |
[32m[20221208 13:58:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:58:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 908.42
[32m[20221208 13:58:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.71
[32m[20221208 13:58:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.08
[32m[20221208 13:58:31 @agent_ppo2.py:137][0m Total time:      13.18 min
[32m[20221208 13:58:31 @agent_ppo2.py:139][0m 1081344 total steps have happened
[32m[20221208 13:58:31 @agent_ppo2.py:115][0m #------------------------ Iteration 528 --------------------------#
[32m[20221208 13:58:31 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:31 @agent_ppo2.py:179][0m |           0.0182 |         235.8729 |           5.3523 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0068 |         223.0004 |           5.3241 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0240 |         217.2679 |           5.3659 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0268 |         213.0557 |           5.3831 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0303 |         211.1766 |           5.3630 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0367 |         209.5804 |           5.4135 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0377 |         208.9024 |           5.4260 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0424 |         207.4375 |           5.4523 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0419 |         206.3434 |           5.4725 |
[32m[20221208 13:58:32 @agent_ppo2.py:179][0m |          -0.0408 |         207.1242 |           5.4427 |
[32m[20221208 13:58:32 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:58:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 885.54
[32m[20221208 13:58:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.80
[32m[20221208 13:58:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 897.91
[32m[20221208 13:58:32 @agent_ppo2.py:137][0m Total time:      13.20 min
[32m[20221208 13:58:32 @agent_ppo2.py:139][0m 1083392 total steps have happened
[32m[20221208 13:58:32 @agent_ppo2.py:115][0m #------------------------ Iteration 529 --------------------------#
[32m[20221208 13:58:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |           0.0275 |         233.2955 |           5.4655 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |           0.0098 |         220.9511 |           5.3949 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |          -0.0049 |         218.9142 |           5.4586 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |          -0.0209 |         216.9417 |           5.5055 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |          -0.0271 |         216.7134 |           5.5616 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |          -0.0347 |         214.8164 |           5.5673 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |          -0.0360 |         213.6833 |           5.5912 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |          -0.0395 |         213.3119 |           5.6144 |
[32m[20221208 13:58:33 @agent_ppo2.py:179][0m |          -0.0426 |         212.6948 |           5.6499 |
[32m[20221208 13:58:34 @agent_ppo2.py:179][0m |          -0.0446 |         212.2461 |           5.6714 |
[32m[20221208 13:58:34 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 932.32
[32m[20221208 13:58:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.45
[32m[20221208 13:58:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 873.59
[32m[20221208 13:58:34 @agent_ppo2.py:137][0m Total time:      13.23 min
[32m[20221208 13:58:34 @agent_ppo2.py:139][0m 1085440 total steps have happened
[32m[20221208 13:58:34 @agent_ppo2.py:115][0m #------------------------ Iteration 530 --------------------------#
[32m[20221208 13:58:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:34 @agent_ppo2.py:179][0m |           0.0293 |         228.2134 |           5.5862 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |           0.0119 |         223.6391 |           5.4529 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0154 |         221.9122 |           5.5971 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0281 |         220.7579 |           5.6586 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0365 |         220.0778 |           5.7114 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0341 |         219.4060 |           5.7255 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0363 |         218.6838 |           5.7381 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0425 |         218.6527 |           5.7558 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0440 |         218.0620 |           5.7664 |
[32m[20221208 13:58:35 @agent_ppo2.py:179][0m |          -0.0429 |         218.6582 |           5.7549 |
[32m[20221208 13:58:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.61
[32m[20221208 13:58:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.75
[32m[20221208 13:58:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.47
[32m[20221208 13:58:35 @agent_ppo2.py:137][0m Total time:      13.25 min
[32m[20221208 13:58:35 @agent_ppo2.py:139][0m 1087488 total steps have happened
[32m[20221208 13:58:35 @agent_ppo2.py:115][0m #------------------------ Iteration 531 --------------------------#
[32m[20221208 13:58:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |           0.0255 |         242.1891 |           5.4735 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0022 |         231.9097 |           5.4107 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0218 |         228.6138 |           5.5135 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0297 |         227.7327 |           5.5551 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0289 |         225.4208 |           5.5219 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0378 |         224.5780 |           5.5907 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0381 |         223.9275 |           5.5960 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0367 |         223.3003 |           5.5960 |
[32m[20221208 13:58:36 @agent_ppo2.py:179][0m |          -0.0405 |         222.9119 |           5.6237 |
[32m[20221208 13:58:37 @agent_ppo2.py:179][0m |          -0.0382 |         222.9013 |           5.6320 |
[32m[20221208 13:58:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 948.26
[32m[20221208 13:58:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 997.37
[32m[20221208 13:58:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.08
[32m[20221208 13:58:37 @agent_ppo2.py:137][0m Total time:      13.28 min
[32m[20221208 13:58:37 @agent_ppo2.py:139][0m 1089536 total steps have happened
[32m[20221208 13:58:37 @agent_ppo2.py:115][0m #------------------------ Iteration 532 --------------------------#
[32m[20221208 13:58:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:37 @agent_ppo2.py:179][0m |           0.0853 |         237.4379 |           5.2037 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |           0.0349 |         230.2422 |           4.8789 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0000 |         227.2915 |           5.1608 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0194 |         226.9944 |           5.2973 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0255 |         224.3933 |           5.3565 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0297 |         223.8914 |           5.4095 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0342 |         224.4511 |           5.4391 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0366 |         223.4419 |           5.4702 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0355 |         222.8820 |           5.4823 |
[32m[20221208 13:58:38 @agent_ppo2.py:179][0m |          -0.0389 |         221.9986 |           5.4921 |
[32m[20221208 13:58:38 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:58:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.57
[32m[20221208 13:58:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.57
[32m[20221208 13:58:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.29
[32m[20221208 13:58:38 @agent_ppo2.py:137][0m Total time:      13.30 min
[32m[20221208 13:58:38 @agent_ppo2.py:139][0m 1091584 total steps have happened
[32m[20221208 13:58:38 @agent_ppo2.py:115][0m #------------------------ Iteration 533 --------------------------#
[32m[20221208 13:58:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |           0.0241 |         240.0015 |           5.4250 |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |          -0.0005 |         217.0511 |           5.3714 |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |          -0.0312 |         208.0785 |           5.4309 |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |          -0.0417 |         198.9130 |           5.4863 |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |          -0.0463 |         195.7654 |           5.4912 |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |          -0.0501 |         193.2673 |           5.5060 |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |          -0.0559 |         190.1753 |           5.5312 |
[32m[20221208 13:58:39 @agent_ppo2.py:179][0m |          -0.0574 |         186.1678 |           5.5491 |
[32m[20221208 13:58:40 @agent_ppo2.py:179][0m |          -0.0603 |         184.8684 |           5.5659 |
[32m[20221208 13:58:40 @agent_ppo2.py:179][0m |          -0.0621 |         182.7650 |           5.5645 |
[32m[20221208 13:58:40 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:58:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 728.05
[32m[20221208 13:58:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.00
[32m[20221208 13:58:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.46
[32m[20221208 13:58:40 @agent_ppo2.py:137][0m Total time:      13.33 min
[32m[20221208 13:58:40 @agent_ppo2.py:139][0m 1093632 total steps have happened
[32m[20221208 13:58:40 @agent_ppo2.py:115][0m #------------------------ Iteration 534 --------------------------#
[32m[20221208 13:58:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |           0.0355 |         241.5694 |           5.7243 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |           0.0034 |         228.8419 |           5.6563 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0238 |         221.9050 |           5.8201 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0332 |         217.9605 |           5.8361 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0362 |         214.1060 |           5.8381 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0407 |         211.2249 |           5.8471 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0446 |         208.9008 |           5.8793 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0456 |         207.5601 |           5.8763 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0492 |         206.1010 |           5.8917 |
[32m[20221208 13:58:41 @agent_ppo2.py:179][0m |          -0.0530 |         205.5098 |           5.9098 |
[32m[20221208 13:58:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:58:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 864.88
[32m[20221208 13:58:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.93
[32m[20221208 13:58:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 801.51
[32m[20221208 13:58:41 @agent_ppo2.py:137][0m Total time:      13.35 min
[32m[20221208 13:58:41 @agent_ppo2.py:139][0m 1095680 total steps have happened
[32m[20221208 13:58:41 @agent_ppo2.py:115][0m #------------------------ Iteration 535 --------------------------#
[32m[20221208 13:58:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |           0.0275 |         219.5513 |           5.7122 |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |          -0.0002 |         200.5402 |           5.6529 |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |          -0.0208 |         193.0417 |           5.7521 |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |          -0.0329 |         188.5318 |           5.8117 |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |          -0.0389 |         183.6990 |           5.8555 |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |          -0.0424 |         180.8004 |           5.8938 |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |          -0.0454 |         177.4856 |           5.9006 |
[32m[20221208 13:58:42 @agent_ppo2.py:179][0m |          -0.0445 |         176.2370 |           5.9060 |
[32m[20221208 13:58:43 @agent_ppo2.py:179][0m |          -0.0489 |         172.9898 |           5.9485 |
[32m[20221208 13:58:43 @agent_ppo2.py:179][0m |          -0.0498 |         171.3971 |           5.9589 |
[32m[20221208 13:58:43 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 872.43
[32m[20221208 13:58:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 931.26
[32m[20221208 13:58:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 860.43
[32m[20221208 13:58:43 @agent_ppo2.py:137][0m Total time:      13.38 min
[32m[20221208 13:58:43 @agent_ppo2.py:139][0m 1097728 total steps have happened
[32m[20221208 13:58:43 @agent_ppo2.py:115][0m #------------------------ Iteration 536 --------------------------#
[32m[20221208 13:58:43 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |           0.0274 |         240.8456 |           6.0925 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |           0.0037 |         227.9609 |           5.9053 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0177 |         225.6913 |           6.0794 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0295 |         221.7715 |           6.1501 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0321 |         222.3953 |           6.1809 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0381 |         219.7744 |           6.2150 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0403 |         218.2414 |           6.2329 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0401 |         218.1324 |           6.2418 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0416 |         216.5646 |           6.2690 |
[32m[20221208 13:58:44 @agent_ppo2.py:179][0m |          -0.0429 |         216.1211 |           6.2560 |
[32m[20221208 13:58:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:58:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 957.48
[32m[20221208 13:58:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.96
[32m[20221208 13:58:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 973.51
[32m[20221208 13:58:45 @agent_ppo2.py:137][0m Total time:      13.40 min
[32m[20221208 13:58:45 @agent_ppo2.py:139][0m 1099776 total steps have happened
[32m[20221208 13:58:45 @agent_ppo2.py:115][0m #------------------------ Iteration 537 --------------------------#
[32m[20221208 13:58:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |           0.0399 |         248.3699 |           5.6924 |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |           0.0105 |         238.4339 |           5.5597 |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |          -0.0189 |         235.0311 |           5.7526 |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |          -0.0300 |         232.1497 |           5.8164 |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |          -0.0360 |         229.5203 |           5.8459 |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |          -0.0416 |         228.3726 |           5.8671 |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |          -0.0385 |         226.6024 |           5.8540 |
[32m[20221208 13:58:45 @agent_ppo2.py:179][0m |          -0.0413 |         225.8405 |           5.8715 |
[32m[20221208 13:58:46 @agent_ppo2.py:179][0m |          -0.0459 |         225.3249 |           5.8800 |
[32m[20221208 13:58:46 @agent_ppo2.py:179][0m |          -0.0464 |         224.8287 |           5.8981 |
[32m[20221208 13:58:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.78
[32m[20221208 13:58:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.93
[32m[20221208 13:58:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.30
[32m[20221208 13:58:46 @agent_ppo2.py:137][0m Total time:      13.43 min
[32m[20221208 13:58:46 @agent_ppo2.py:139][0m 1101824 total steps have happened
[32m[20221208 13:58:46 @agent_ppo2.py:115][0m #------------------------ Iteration 538 --------------------------#
[32m[20221208 13:58:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |           0.0413 |         249.4996 |           5.8110 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |           0.0038 |         235.4299 |           5.7064 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0263 |         231.0692 |           5.7949 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0385 |         229.3185 |           5.8208 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0433 |         226.4023 |           5.8209 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0516 |         225.0749 |           5.8609 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0527 |         225.1045 |           5.8588 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0566 |         222.8915 |           5.8679 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0557 |         222.8699 |           5.8674 |
[32m[20221208 13:58:47 @agent_ppo2.py:179][0m |          -0.0521 |         221.9698 |           5.8482 |
[32m[20221208 13:58:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 720.10
[32m[20221208 13:58:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 780.50
[32m[20221208 13:58:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.34
[32m[20221208 13:58:48 @agent_ppo2.py:137][0m Total time:      13.45 min
[32m[20221208 13:58:48 @agent_ppo2.py:139][0m 1103872 total steps have happened
[32m[20221208 13:58:48 @agent_ppo2.py:115][0m #------------------------ Iteration 539 --------------------------#
[32m[20221208 13:58:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |           0.0264 |         216.5606 |           6.0219 |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |           0.0111 |         192.3672 |           5.6144 |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |          -0.0178 |         183.2280 |           5.8472 |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |          -0.0267 |         177.2087 |           5.9451 |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |          -0.0295 |         173.8913 |           5.9761 |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |          -0.0341 |         169.3473 |           6.0013 |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |          -0.0347 |         165.9199 |           6.0421 |
[32m[20221208 13:58:48 @agent_ppo2.py:179][0m |          -0.0363 |         163.5479 |           6.0364 |
[32m[20221208 13:58:49 @agent_ppo2.py:179][0m |          -0.0387 |         161.5470 |           6.0710 |
[32m[20221208 13:58:49 @agent_ppo2.py:179][0m |          -0.0382 |         159.6966 |           6.0730 |
[32m[20221208 13:58:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 973.01
[32m[20221208 13:58:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.16
[32m[20221208 13:58:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 920.25
[32m[20221208 13:58:49 @agent_ppo2.py:137][0m Total time:      13.48 min
[32m[20221208 13:58:49 @agent_ppo2.py:139][0m 1105920 total steps have happened
[32m[20221208 13:58:49 @agent_ppo2.py:115][0m #------------------------ Iteration 540 --------------------------#
[32m[20221208 13:58:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:58:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |           0.0289 |         230.0850 |           5.8945 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |           0.0626 |         214.7641 |           5.1929 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |           0.0060 |         208.7740 |           5.7034 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |          -0.0154 |         207.0557 |           5.8248 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |          -0.0288 |         205.0202 |           5.9210 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |          -0.0357 |         203.8042 |           5.9432 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |          -0.0391 |         203.3265 |           5.9679 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |          -0.0372 |         202.8862 |           5.9697 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |          -0.0408 |         202.4407 |           5.9969 |
[32m[20221208 13:58:50 @agent_ppo2.py:179][0m |          -0.0436 |         201.6100 |           6.0082 |
[32m[20221208 13:58:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:58:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.42
[32m[20221208 13:58:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.07
[32m[20221208 13:58:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 878.83
[32m[20221208 13:58:51 @agent_ppo2.py:137][0m Total time:      13.50 min
[32m[20221208 13:58:51 @agent_ppo2.py:139][0m 1107968 total steps have happened
[32m[20221208 13:58:51 @agent_ppo2.py:115][0m #------------------------ Iteration 541 --------------------------#
[32m[20221208 13:58:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:51 @agent_ppo2.py:179][0m |           0.0274 |         257.4090 |           5.9359 |
[32m[20221208 13:58:51 @agent_ppo2.py:179][0m |          -0.0009 |         248.3274 |           5.7608 |
[32m[20221208 13:58:51 @agent_ppo2.py:179][0m |          -0.0222 |         246.0325 |           5.8320 |
[32m[20221208 13:58:51 @agent_ppo2.py:179][0m |          -0.0330 |         242.3998 |           5.8729 |
[32m[20221208 13:58:51 @agent_ppo2.py:179][0m |          -0.0399 |         242.1600 |           5.8915 |
[32m[20221208 13:58:51 @agent_ppo2.py:179][0m |          -0.0366 |         239.4819 |           5.8780 |
[32m[20221208 13:58:51 @agent_ppo2.py:179][0m |          -0.0457 |         238.5116 |           5.9059 |
[32m[20221208 13:58:52 @agent_ppo2.py:179][0m |          -0.0480 |         237.7730 |           5.9315 |
[32m[20221208 13:58:52 @agent_ppo2.py:179][0m |          -0.0514 |         237.9927 |           5.9224 |
[32m[20221208 13:58:52 @agent_ppo2.py:179][0m |          -0.0514 |         236.6356 |           5.9332 |
[32m[20221208 13:58:52 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:58:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 843.78
[32m[20221208 13:58:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.71
[32m[20221208 13:58:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.87
[32m[20221208 13:58:52 @agent_ppo2.py:137][0m Total time:      13.53 min
[32m[20221208 13:58:52 @agent_ppo2.py:139][0m 1110016 total steps have happened
[32m[20221208 13:58:52 @agent_ppo2.py:115][0m #------------------------ Iteration 542 --------------------------#
[32m[20221208 13:58:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |           0.0336 |         242.7030 |           5.7524 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |           0.0255 |         235.3279 |           5.3863 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |           0.0027 |         232.8565 |           5.6788 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |          -0.0169 |         230.9654 |           5.7592 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |          -0.0252 |         230.5217 |           5.7901 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |          -0.0262 |         229.4421 |           5.8097 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |          -0.0271 |         229.2147 |           5.8426 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |          -0.0319 |         228.4178 |           5.8412 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |          -0.0344 |         227.6933 |           5.8646 |
[32m[20221208 13:58:53 @agent_ppo2.py:179][0m |          -0.0372 |         227.1878 |           5.8878 |
[32m[20221208 13:58:53 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:58:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 955.82
[32m[20221208 13:58:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.63
[32m[20221208 13:58:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.12
[32m[20221208 13:58:54 @agent_ppo2.py:137][0m Total time:      13.56 min
[32m[20221208 13:58:54 @agent_ppo2.py:139][0m 1112064 total steps have happened
[32m[20221208 13:58:54 @agent_ppo2.py:115][0m #------------------------ Iteration 543 --------------------------#
[32m[20221208 13:58:54 @agent_ppo2.py:121][0m Sampling time: 0.51 s by 1 slaves
[32m[20221208 13:58:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:54 @agent_ppo2.py:179][0m |           0.0194 |         248.7435 |           5.8238 |
[32m[20221208 13:58:54 @agent_ppo2.py:179][0m |           0.0040 |         241.3808 |           5.7169 |
[32m[20221208 13:58:54 @agent_ppo2.py:179][0m |          -0.0176 |         237.3924 |           5.7710 |
[32m[20221208 13:58:55 @agent_ppo2.py:179][0m |          -0.0286 |         236.3556 |           5.8057 |
[32m[20221208 13:58:55 @agent_ppo2.py:179][0m |          -0.0346 |         235.8732 |           5.8220 |
[32m[20221208 13:58:55 @agent_ppo2.py:179][0m |          -0.0368 |         232.9172 |           5.8332 |
[32m[20221208 13:58:55 @agent_ppo2.py:179][0m |          -0.0382 |         234.2369 |           5.8278 |
[32m[20221208 13:58:55 @agent_ppo2.py:179][0m |          -0.0431 |         230.8224 |           5.8471 |
[32m[20221208 13:58:55 @agent_ppo2.py:179][0m |          -0.0444 |         231.0523 |           5.8655 |
[32m[20221208 13:58:55 @agent_ppo2.py:179][0m |          -0.0466 |         229.0934 |           5.8745 |
[32m[20221208 13:58:55 @agent_ppo2.py:124][0m Policy update time: 0.94 s
[32m[20221208 13:58:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 821.55
[32m[20221208 13:58:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.68
[32m[20221208 13:58:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.40
[32m[20221208 13:58:56 @agent_ppo2.py:137][0m Total time:      13.59 min
[32m[20221208 13:58:56 @agent_ppo2.py:139][0m 1114112 total steps have happened
[32m[20221208 13:58:56 @agent_ppo2.py:115][0m #------------------------ Iteration 544 --------------------------#
[32m[20221208 13:58:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:58:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:56 @agent_ppo2.py:179][0m |           0.0131 |         231.5234 |           5.8289 |
[32m[20221208 13:58:56 @agent_ppo2.py:179][0m |          -0.0054 |         222.2567 |           5.6809 |
[32m[20221208 13:58:56 @agent_ppo2.py:179][0m |          -0.0197 |         219.1664 |           5.7986 |
[32m[20221208 13:58:56 @agent_ppo2.py:179][0m |          -0.0241 |         217.7424 |           5.8560 |
[32m[20221208 13:58:56 @agent_ppo2.py:179][0m |          -0.0246 |         215.0905 |           5.8388 |
[32m[20221208 13:58:56 @agent_ppo2.py:179][0m |          -0.0304 |         214.2618 |           5.8824 |
[32m[20221208 13:58:56 @agent_ppo2.py:179][0m |          -0.0326 |         213.5083 |           5.9041 |
[32m[20221208 13:58:57 @agent_ppo2.py:179][0m |          -0.0330 |         212.5321 |           5.9169 |
[32m[20221208 13:58:57 @agent_ppo2.py:179][0m |          -0.0378 |         211.2122 |           5.9232 |
[32m[20221208 13:58:57 @agent_ppo2.py:179][0m |          -0.0369 |         211.2843 |           5.9463 |
[32m[20221208 13:58:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:58:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 952.70
[32m[20221208 13:58:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.56
[32m[20221208 13:58:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 882.34
[32m[20221208 13:58:57 @agent_ppo2.py:137][0m Total time:      13.61 min
[32m[20221208 13:58:57 @agent_ppo2.py:139][0m 1116160 total steps have happened
[32m[20221208 13:58:57 @agent_ppo2.py:115][0m #------------------------ Iteration 545 --------------------------#
[32m[20221208 13:58:58 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:58:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |           0.0434 |         253.7214 |           5.5923 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |           0.0311 |         248.4125 |           5.3702 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0043 |         244.4407 |           5.6067 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0136 |         243.1983 |           5.6498 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0196 |         241.3480 |           5.6660 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0245 |         240.2467 |           5.7004 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0310 |         241.2112 |           5.7357 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0319 |         239.7725 |           5.7374 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0305 |         239.0678 |           5.7417 |
[32m[20221208 13:58:58 @agent_ppo2.py:179][0m |          -0.0327 |         238.8803 |           5.7415 |
[32m[20221208 13:58:58 @agent_ppo2.py:124][0m Policy update time: 0.81 s
[32m[20221208 13:58:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 916.81
[32m[20221208 13:58:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 932.22
[32m[20221208 13:58:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.15
[32m[20221208 13:58:59 @agent_ppo2.py:137][0m Total time:      13.64 min
[32m[20221208 13:58:59 @agent_ppo2.py:139][0m 1118208 total steps have happened
[32m[20221208 13:58:59 @agent_ppo2.py:115][0m #------------------------ Iteration 546 --------------------------#
[32m[20221208 13:58:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:58:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:58:59 @agent_ppo2.py:179][0m |           0.0225 |         248.3496 |           5.6931 |
[32m[20221208 13:58:59 @agent_ppo2.py:179][0m |          -0.0100 |         239.4329 |           5.5842 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0251 |         235.0212 |           5.5994 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0349 |         232.2704 |           5.6162 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0339 |         230.2428 |           5.6124 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0412 |         229.4787 |           5.6293 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0436 |         228.1670 |           5.6440 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0431 |         227.1536 |           5.6501 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0452 |         227.8246 |           5.6681 |
[32m[20221208 13:59:00 @agent_ppo2.py:179][0m |          -0.0416 |         226.6292 |           5.6758 |
[32m[20221208 13:59:00 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 13:59:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 845.14
[32m[20221208 13:59:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 920.35
[32m[20221208 13:59:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.56
[32m[20221208 13:59:00 @agent_ppo2.py:137][0m Total time:      13.67 min
[32m[20221208 13:59:00 @agent_ppo2.py:139][0m 1120256 total steps have happened
[32m[20221208 13:59:00 @agent_ppo2.py:115][0m #------------------------ Iteration 547 --------------------------#
[32m[20221208 13:59:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |           0.0224 |         242.9090 |           5.3966 |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |          -0.0112 |         237.5459 |           5.4018 |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |          -0.0220 |         235.9100 |           5.4307 |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |          -0.0300 |         234.2962 |           5.4913 |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |          -0.0366 |         232.8965 |           5.4767 |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |          -0.0367 |         232.1930 |           5.5011 |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |          -0.0433 |         231.6192 |           5.5143 |
[32m[20221208 13:59:01 @agent_ppo2.py:179][0m |          -0.0430 |         231.5236 |           5.5490 |
[32m[20221208 13:59:02 @agent_ppo2.py:179][0m |          -0.0453 |         230.9991 |           5.5430 |
[32m[20221208 13:59:02 @agent_ppo2.py:179][0m |          -0.0463 |         231.8008 |           5.5518 |
[32m[20221208 13:59:02 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 13:59:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 859.39
[32m[20221208 13:59:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.94
[32m[20221208 13:59:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.38
[32m[20221208 13:59:02 @agent_ppo2.py:137][0m Total time:      13.69 min
[32m[20221208 13:59:02 @agent_ppo2.py:139][0m 1122304 total steps have happened
[32m[20221208 13:59:02 @agent_ppo2.py:115][0m #------------------------ Iteration 548 --------------------------#
[32m[20221208 13:59:02 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |           0.0287 |         241.0760 |           5.5173 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0021 |         236.5929 |           5.4983 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0071 |         234.7208 |           5.4992 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0247 |         233.6365 |           5.5831 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0238 |         233.9277 |           5.5833 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0257 |         232.9154 |           5.5951 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0275 |         231.8331 |           5.5846 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0254 |         231.0751 |           5.5771 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0319 |         232.2174 |           5.6066 |
[32m[20221208 13:59:03 @agent_ppo2.py:179][0m |          -0.0332 |         230.8048 |           5.6364 |
[32m[20221208 13:59:03 @agent_ppo2.py:124][0m Policy update time: 0.83 s
[32m[20221208 13:59:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.16
[32m[20221208 13:59:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.88
[32m[20221208 13:59:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 975.75
[32m[20221208 13:59:04 @agent_ppo2.py:137][0m Total time:      13.72 min
[32m[20221208 13:59:04 @agent_ppo2.py:139][0m 1124352 total steps have happened
[32m[20221208 13:59:04 @agent_ppo2.py:115][0m #------------------------ Iteration 549 --------------------------#
[32m[20221208 13:59:04 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 13:59:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:04 @agent_ppo2.py:179][0m |           0.0461 |         239.1482 |           5.2939 |
[32m[20221208 13:59:04 @agent_ppo2.py:179][0m |           0.0264 |         234.5157 |           4.9753 |
[32m[20221208 13:59:04 @agent_ppo2.py:179][0m |          -0.0053 |         233.8463 |           5.3629 |
[32m[20221208 13:59:05 @agent_ppo2.py:179][0m |          -0.0195 |         233.4431 |           5.4837 |
[32m[20221208 13:59:05 @agent_ppo2.py:179][0m |          -0.0239 |         233.4168 |           5.4884 |
[32m[20221208 13:59:05 @agent_ppo2.py:179][0m |          -0.0269 |         232.1643 |           5.4940 |
[32m[20221208 13:59:05 @agent_ppo2.py:179][0m |          -0.0222 |         233.2214 |           5.5291 |
[32m[20221208 13:59:05 @agent_ppo2.py:179][0m |          -0.0222 |         232.1724 |           5.4695 |
[32m[20221208 13:59:05 @agent_ppo2.py:179][0m |          -0.0330 |         231.7547 |           5.5312 |
[32m[20221208 13:59:05 @agent_ppo2.py:179][0m |          -0.0323 |         231.2701 |           5.5277 |
[32m[20221208 13:59:05 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 13:59:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 960.64
[32m[20221208 13:59:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.85
[32m[20221208 13:59:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 969.52
[32m[20221208 13:59:05 @agent_ppo2.py:137][0m Total time:      13.75 min
[32m[20221208 13:59:05 @agent_ppo2.py:139][0m 1126400 total steps have happened
[32m[20221208 13:59:05 @agent_ppo2.py:115][0m #------------------------ Iteration 550 --------------------------#
[32m[20221208 13:59:06 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |           0.0212 |         244.4226 |           5.3969 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0109 |         241.7553 |           5.4108 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0248 |         239.2369 |           5.4617 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0265 |         239.9765 |           5.4386 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0352 |         237.5379 |           5.4772 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0382 |         238.0617 |           5.5193 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0412 |         236.4829 |           5.5015 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0420 |         235.7876 |           5.5034 |
[32m[20221208 13:59:06 @agent_ppo2.py:179][0m |          -0.0366 |         235.5406 |           5.4967 |
[32m[20221208 13:59:07 @agent_ppo2.py:179][0m |          -0.0376 |         236.0375 |           5.4926 |
[32m[20221208 13:59:07 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:59:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 859.25
[32m[20221208 13:59:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.14
[32m[20221208 13:59:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.54
[32m[20221208 13:59:07 @agent_ppo2.py:137][0m Total time:      13.78 min
[32m[20221208 13:59:07 @agent_ppo2.py:139][0m 1128448 total steps have happened
[32m[20221208 13:59:07 @agent_ppo2.py:115][0m #------------------------ Iteration 551 --------------------------#
[32m[20221208 13:59:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |           0.0267 |         237.0472 |           5.3904 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |           0.0095 |         232.4980 |           5.3832 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0053 |         231.1942 |           5.3586 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0195 |         230.3522 |           5.4297 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0279 |         230.0267 |           5.4936 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0315 |         229.2432 |           5.4917 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0290 |         228.6150 |           5.4767 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0327 |         228.2901 |           5.4999 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0326 |         228.1773 |           5.5131 |
[32m[20221208 13:59:08 @agent_ppo2.py:179][0m |          -0.0378 |         227.3038 |           5.5344 |
[32m[20221208 13:59:08 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:59:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 858.92
[32m[20221208 13:59:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 912.92
[32m[20221208 13:59:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 896.57
[32m[20221208 13:59:09 @agent_ppo2.py:137][0m Total time:      13.80 min
[32m[20221208 13:59:09 @agent_ppo2.py:139][0m 1130496 total steps have happened
[32m[20221208 13:59:09 @agent_ppo2.py:115][0m #------------------------ Iteration 552 --------------------------#
[32m[20221208 13:59:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |           0.0196 |         243.5205 |           5.5350 |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |           0.0131 |         237.4526 |           5.3014 |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |          -0.0143 |         235.3517 |           5.5059 |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |          -0.0280 |         233.2008 |           5.5577 |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |          -0.0269 |         231.3662 |           5.5689 |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |          -0.0275 |         232.4024 |           5.5643 |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |          -0.0379 |         230.7415 |           5.6018 |
[32m[20221208 13:59:09 @agent_ppo2.py:179][0m |          -0.0415 |         229.2246 |           5.6130 |
[32m[20221208 13:59:10 @agent_ppo2.py:179][0m |          -0.0440 |         228.6041 |           5.6354 |
[32m[20221208 13:59:10 @agent_ppo2.py:179][0m |          -0.0435 |         228.1722 |           5.6431 |
[32m[20221208 13:59:10 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:59:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 891.78
[32m[20221208 13:59:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 921.34
[32m[20221208 13:59:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 659.78
[32m[20221208 13:59:10 @agent_ppo2.py:137][0m Total time:      13.83 min
[32m[20221208 13:59:10 @agent_ppo2.py:139][0m 1132544 total steps have happened
[32m[20221208 13:59:10 @agent_ppo2.py:115][0m #------------------------ Iteration 553 --------------------------#
[32m[20221208 13:59:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |           0.0288 |         238.6330 |           5.2729 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0031 |         232.0514 |           5.2452 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0179 |         232.8284 |           5.3774 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0245 |         230.2830 |           5.4190 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0283 |         227.6496 |           5.4140 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0302 |         227.4574 |           5.4408 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0332 |         227.2824 |           5.4414 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0341 |         226.1975 |           5.4582 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0337 |         226.6187 |           5.4581 |
[32m[20221208 13:59:11 @agent_ppo2.py:179][0m |          -0.0359 |         226.9614 |           5.4726 |
[32m[20221208 13:59:11 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:59:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.53
[32m[20221208 13:59:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.91
[32m[20221208 13:59:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.50
[32m[20221208 13:59:12 @agent_ppo2.py:137][0m Total time:      13.85 min
[32m[20221208 13:59:12 @agent_ppo2.py:139][0m 1134592 total steps have happened
[32m[20221208 13:59:12 @agent_ppo2.py:115][0m #------------------------ Iteration 554 --------------------------#
[32m[20221208 13:59:12 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:12 @agent_ppo2.py:179][0m |           0.0333 |         237.1808 |           5.3209 |
[32m[20221208 13:59:12 @agent_ppo2.py:179][0m |           0.0649 |         234.9433 |           4.9171 |
[32m[20221208 13:59:12 @agent_ppo2.py:179][0m |          -0.0116 |         230.9145 |           5.3416 |
[32m[20221208 13:59:12 @agent_ppo2.py:179][0m |          -0.0089 |         229.6512 |           5.3963 |
[32m[20221208 13:59:12 @agent_ppo2.py:179][0m |          -0.0166 |         229.4384 |           5.4172 |
[32m[20221208 13:59:12 @agent_ppo2.py:179][0m |          -0.0283 |         228.1462 |           5.4608 |
[32m[20221208 13:59:12 @agent_ppo2.py:179][0m |          -0.0246 |         228.4760 |           5.4495 |
[32m[20221208 13:59:13 @agent_ppo2.py:179][0m |          -0.0261 |         227.4555 |           5.4334 |
[32m[20221208 13:59:13 @agent_ppo2.py:179][0m |          -0.0332 |         227.3266 |           5.5065 |
[32m[20221208 13:59:13 @agent_ppo2.py:179][0m |          -0.0347 |         227.3606 |           5.5263 |
[32m[20221208 13:59:13 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:59:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 979.87
[32m[20221208 13:59:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.95
[32m[20221208 13:59:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.95
[32m[20221208 13:59:13 @agent_ppo2.py:137][0m Total time:      13.88 min
[32m[20221208 13:59:13 @agent_ppo2.py:139][0m 1136640 total steps have happened
[32m[20221208 13:59:13 @agent_ppo2.py:115][0m #------------------------ Iteration 555 --------------------------#
[32m[20221208 13:59:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |           0.0235 |         248.4269 |           5.2166 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0043 |         239.7349 |           5.1432 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0217 |         236.8185 |           5.1665 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0303 |         236.6651 |           5.2191 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0359 |         234.9123 |           5.2215 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0364 |         234.0753 |           5.2431 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0416 |         233.7654 |           5.2627 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0417 |         234.1898 |           5.2558 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0428 |         232.8948 |           5.2710 |
[32m[20221208 13:59:14 @agent_ppo2.py:179][0m |          -0.0386 |         232.3574 |           5.2559 |
[32m[20221208 13:59:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:59:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 884.11
[32m[20221208 13:59:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.12
[32m[20221208 13:59:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 861.38
[32m[20221208 13:59:15 @agent_ppo2.py:137][0m Total time:      13.90 min
[32m[20221208 13:59:15 @agent_ppo2.py:139][0m 1138688 total steps have happened
[32m[20221208 13:59:15 @agent_ppo2.py:115][0m #------------------------ Iteration 556 --------------------------#
[32m[20221208 13:59:15 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:59:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:15 @agent_ppo2.py:179][0m |           0.0239 |         245.6302 |           5.2296 |
[32m[20221208 13:59:15 @agent_ppo2.py:179][0m |           0.0077 |         237.0175 |           5.0949 |
[32m[20221208 13:59:15 @agent_ppo2.py:179][0m |          -0.0168 |         233.0877 |           5.2323 |
[32m[20221208 13:59:15 @agent_ppo2.py:179][0m |          -0.0217 |         230.2719 |           5.2868 |
[32m[20221208 13:59:15 @agent_ppo2.py:179][0m |          -0.0294 |         229.0762 |           5.3176 |
[32m[20221208 13:59:16 @agent_ppo2.py:179][0m |          -0.0328 |         228.3431 |           5.3329 |
[32m[20221208 13:59:16 @agent_ppo2.py:179][0m |          -0.0292 |         228.0881 |           5.3113 |
[32m[20221208 13:59:16 @agent_ppo2.py:179][0m |          -0.0338 |         227.6650 |           5.3593 |
[32m[20221208 13:59:16 @agent_ppo2.py:179][0m |          -0.0322 |         228.7176 |           5.3426 |
[32m[20221208 13:59:16 @agent_ppo2.py:179][0m |          -0.0358 |         227.6454 |           5.3952 |
[32m[20221208 13:59:16 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:59:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 971.23
[32m[20221208 13:59:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.79
[32m[20221208 13:59:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 895.85
[32m[20221208 13:59:16 @agent_ppo2.py:137][0m Total time:      13.93 min
[32m[20221208 13:59:16 @agent_ppo2.py:139][0m 1140736 total steps have happened
[32m[20221208 13:59:16 @agent_ppo2.py:115][0m #------------------------ Iteration 557 --------------------------#
[32m[20221208 13:59:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |           0.0277 |         231.3387 |           5.2711 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |           0.0055 |         223.5377 |           5.1631 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0200 |         219.6722 |           5.3033 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0219 |         216.8446 |           5.3353 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0312 |         212.7087 |           5.3537 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0365 |         210.1777 |           5.3816 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0364 |         209.1088 |           5.3880 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0392 |         206.4456 |           5.4187 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0398 |         204.7269 |           5.4349 |
[32m[20221208 13:59:17 @agent_ppo2.py:179][0m |          -0.0409 |         203.2689 |           5.4387 |
[32m[20221208 13:59:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:59:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.98
[32m[20221208 13:59:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.68
[32m[20221208 13:59:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 885.82
[32m[20221208 13:59:18 @agent_ppo2.py:137][0m Total time:      13.96 min
[32m[20221208 13:59:18 @agent_ppo2.py:139][0m 1142784 total steps have happened
[32m[20221208 13:59:18 @agent_ppo2.py:115][0m #------------------------ Iteration 558 --------------------------#
[32m[20221208 13:59:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:18 @agent_ppo2.py:179][0m |           0.0341 |         237.0268 |           5.3020 |
[32m[20221208 13:59:18 @agent_ppo2.py:179][0m |          -0.0009 |         224.5014 |           5.1902 |
[32m[20221208 13:59:18 @agent_ppo2.py:179][0m |          -0.0253 |         217.1284 |           5.3491 |
[32m[20221208 13:59:18 @agent_ppo2.py:179][0m |          -0.0321 |         212.8741 |           5.3571 |
[32m[20221208 13:59:19 @agent_ppo2.py:179][0m |          -0.0358 |         210.7558 |           5.3887 |
[32m[20221208 13:59:19 @agent_ppo2.py:179][0m |          -0.0399 |         206.8125 |           5.4022 |
[32m[20221208 13:59:19 @agent_ppo2.py:179][0m |          -0.0415 |         205.6246 |           5.4240 |
[32m[20221208 13:59:19 @agent_ppo2.py:179][0m |          -0.0440 |         202.7082 |           5.4404 |
[32m[20221208 13:59:19 @agent_ppo2.py:179][0m |          -0.0464 |         202.2328 |           5.4604 |
[32m[20221208 13:59:19 @agent_ppo2.py:179][0m |          -0.0476 |         201.5912 |           5.4671 |
[32m[20221208 13:59:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:59:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 882.21
[32m[20221208 13:59:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.84
[32m[20221208 13:59:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.74
[32m[20221208 13:59:19 @agent_ppo2.py:137][0m Total time:      13.98 min
[32m[20221208 13:59:19 @agent_ppo2.py:139][0m 1144832 total steps have happened
[32m[20221208 13:59:19 @agent_ppo2.py:115][0m #------------------------ Iteration 559 --------------------------#
[32m[20221208 13:59:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |           0.0223 |         266.9818 |           5.4740 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0108 |         259.2480 |           5.4817 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0190 |         256.3197 |           5.4803 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0277 |         254.2818 |           5.5423 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0329 |         252.9108 |           5.5547 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0348 |         252.7447 |           5.5857 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0399 |         252.1105 |           5.6073 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0401 |         250.3078 |           5.6250 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0413 |         250.2274 |           5.6272 |
[32m[20221208 13:59:20 @agent_ppo2.py:179][0m |          -0.0415 |         249.0775 |           5.6443 |
[32m[20221208 13:59:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:59:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.14
[32m[20221208 13:59:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.44
[32m[20221208 13:59:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 974.84
[32m[20221208 13:59:21 @agent_ppo2.py:137][0m Total time:      14.01 min
[32m[20221208 13:59:21 @agent_ppo2.py:139][0m 1146880 total steps have happened
[32m[20221208 13:59:21 @agent_ppo2.py:115][0m #------------------------ Iteration 560 --------------------------#
[32m[20221208 13:59:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:21 @agent_ppo2.py:179][0m |           0.0239 |         272.7271 |           5.5631 |
[32m[20221208 13:59:21 @agent_ppo2.py:179][0m |           0.0028 |         261.6450 |           5.5243 |
[32m[20221208 13:59:21 @agent_ppo2.py:179][0m |          -0.0110 |         257.6861 |           5.5733 |
[32m[20221208 13:59:21 @agent_ppo2.py:179][0m |          -0.0240 |         257.9585 |           5.6041 |
[32m[20221208 13:59:22 @agent_ppo2.py:179][0m |          -0.0364 |         255.5332 |           5.6764 |
[32m[20221208 13:59:22 @agent_ppo2.py:179][0m |          -0.0399 |         254.4224 |           5.6905 |
[32m[20221208 13:59:22 @agent_ppo2.py:179][0m |          -0.0406 |         254.3997 |           5.7196 |
[32m[20221208 13:59:22 @agent_ppo2.py:179][0m |          -0.0428 |         253.6838 |           5.7354 |
[32m[20221208 13:59:22 @agent_ppo2.py:179][0m |          -0.0415 |         253.5055 |           5.7582 |
[32m[20221208 13:59:22 @agent_ppo2.py:179][0m |          -0.0444 |         252.9583 |           5.7681 |
[32m[20221208 13:59:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:59:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 940.61
[32m[20221208 13:59:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.90
[32m[20221208 13:59:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 687.94
[32m[20221208 13:59:22 @agent_ppo2.py:137][0m Total time:      14.03 min
[32m[20221208 13:59:22 @agent_ppo2.py:139][0m 1148928 total steps have happened
[32m[20221208 13:59:22 @agent_ppo2.py:115][0m #------------------------ Iteration 561 --------------------------#
[32m[20221208 13:59:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |           0.0527 |         258.5064 |           5.2605 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |           0.0361 |         252.3362 |           5.0180 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |           0.0051 |         249.2056 |           5.3449 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |          -0.0164 |         248.0187 |           5.5224 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |          -0.0242 |         247.2529 |           5.5631 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |          -0.0276 |         247.0343 |           5.6097 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |          -0.0326 |         245.7829 |           5.6260 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |          -0.0380 |         246.7088 |           5.6898 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |          -0.0393 |         243.6932 |           5.7076 |
[32m[20221208 13:59:23 @agent_ppo2.py:179][0m |          -0.0404 |         244.4035 |           5.7199 |
[32m[20221208 13:59:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:59:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 952.08
[32m[20221208 13:59:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.85
[32m[20221208 13:59:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 922.41
[32m[20221208 13:59:24 @agent_ppo2.py:137][0m Total time:      14.06 min
[32m[20221208 13:59:24 @agent_ppo2.py:139][0m 1150976 total steps have happened
[32m[20221208 13:59:24 @agent_ppo2.py:115][0m #------------------------ Iteration 562 --------------------------#
[32m[20221208 13:59:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:24 @agent_ppo2.py:179][0m |           0.0166 |         257.6018 |           5.6644 |
[32m[20221208 13:59:24 @agent_ppo2.py:179][0m |          -0.0020 |         249.8532 |           5.5204 |
[32m[20221208 13:59:24 @agent_ppo2.py:179][0m |          -0.0203 |         248.1276 |           5.6098 |
[32m[20221208 13:59:24 @agent_ppo2.py:179][0m |          -0.0321 |         244.4794 |           5.6366 |
[32m[20221208 13:59:25 @agent_ppo2.py:179][0m |          -0.0380 |         243.2108 |           5.6701 |
[32m[20221208 13:59:25 @agent_ppo2.py:179][0m |          -0.0396 |         242.3944 |           5.6810 |
[32m[20221208 13:59:25 @agent_ppo2.py:179][0m |          -0.0427 |         241.9049 |           5.7243 |
[32m[20221208 13:59:25 @agent_ppo2.py:179][0m |          -0.0429 |         241.3646 |           5.7238 |
[32m[20221208 13:59:25 @agent_ppo2.py:179][0m |          -0.0444 |         240.5015 |           5.7364 |
[32m[20221208 13:59:25 @agent_ppo2.py:179][0m |          -0.0462 |         240.3104 |           5.7421 |
[32m[20221208 13:59:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:59:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 916.97
[32m[20221208 13:59:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.15
[32m[20221208 13:59:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.28
[32m[20221208 13:59:25 @agent_ppo2.py:137][0m Total time:      14.08 min
[32m[20221208 13:59:25 @agent_ppo2.py:139][0m 1153024 total steps have happened
[32m[20221208 13:59:25 @agent_ppo2.py:115][0m #------------------------ Iteration 563 --------------------------#
[32m[20221208 13:59:26 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |           0.0317 |         250.9087 |           5.3146 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |           0.0011 |         244.6510 |           5.2922 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0118 |         240.9358 |           5.3686 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0208 |         236.0422 |           5.3708 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0304 |         235.1439 |           5.4715 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0319 |         234.0224 |           5.5033 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0345 |         234.2203 |           5.5126 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0360 |         231.5285 |           5.5204 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0373 |         232.1090 |           5.5359 |
[32m[20221208 13:59:26 @agent_ppo2.py:179][0m |          -0.0420 |         230.2992 |           5.5844 |
[32m[20221208 13:59:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:59:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 952.40
[32m[20221208 13:59:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.35
[32m[20221208 13:59:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 976.26
[32m[20221208 13:59:27 @agent_ppo2.py:137][0m Total time:      14.11 min
[32m[20221208 13:59:27 @agent_ppo2.py:139][0m 1155072 total steps have happened
[32m[20221208 13:59:27 @agent_ppo2.py:115][0m #------------------------ Iteration 564 --------------------------#
[32m[20221208 13:59:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:27 @agent_ppo2.py:179][0m |           0.0228 |         253.2997 |           5.6033 |
[32m[20221208 13:59:27 @agent_ppo2.py:179][0m |           0.0167 |         245.6172 |           5.4967 |
[32m[20221208 13:59:27 @agent_ppo2.py:179][0m |          -0.0125 |         242.0545 |           5.5727 |
[32m[20221208 13:59:27 @agent_ppo2.py:179][0m |          -0.0221 |         240.1969 |           5.6895 |
[32m[20221208 13:59:27 @agent_ppo2.py:179][0m |          -0.0288 |         238.9668 |           5.6873 |
[32m[20221208 13:59:28 @agent_ppo2.py:179][0m |          -0.0336 |         237.8851 |           5.7348 |
[32m[20221208 13:59:28 @agent_ppo2.py:179][0m |          -0.0391 |         238.2143 |           5.7707 |
[32m[20221208 13:59:28 @agent_ppo2.py:179][0m |          -0.0402 |         237.1152 |           5.7824 |
[32m[20221208 13:59:28 @agent_ppo2.py:179][0m |          -0.0397 |         236.3296 |           5.7762 |
[32m[20221208 13:59:28 @agent_ppo2.py:179][0m |          -0.0404 |         237.2969 |           5.8253 |
[32m[20221208 13:59:28 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 13:59:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.74
[32m[20221208 13:59:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.17
[32m[20221208 13:59:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.14
[32m[20221208 13:59:28 @agent_ppo2.py:137][0m Total time:      14.13 min
[32m[20221208 13:59:28 @agent_ppo2.py:139][0m 1157120 total steps have happened
[32m[20221208 13:59:28 @agent_ppo2.py:115][0m #------------------------ Iteration 565 --------------------------#
[32m[20221208 13:59:29 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |           0.0330 |         269.9789 |           5.6253 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |           0.0134 |         259.9504 |           5.4688 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0099 |         252.5333 |           5.5584 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0262 |         251.7127 |           5.7397 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0307 |         252.1301 |           5.7989 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0302 |         250.9713 |           5.7804 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0369 |         248.2984 |           5.8188 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0394 |         246.8348 |           5.8477 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0428 |         246.2445 |           5.8549 |
[32m[20221208 13:59:29 @agent_ppo2.py:179][0m |          -0.0436 |         245.4833 |           5.8874 |
[32m[20221208 13:59:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:59:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 900.89
[32m[20221208 13:59:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.11
[32m[20221208 13:59:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 917.79
[32m[20221208 13:59:30 @agent_ppo2.py:137][0m Total time:      14.15 min
[32m[20221208 13:59:30 @agent_ppo2.py:139][0m 1159168 total steps have happened
[32m[20221208 13:59:30 @agent_ppo2.py:115][0m #------------------------ Iteration 566 --------------------------#
[32m[20221208 13:59:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:30 @agent_ppo2.py:179][0m |           0.0177 |         250.2023 |           5.8212 |
[32m[20221208 13:59:30 @agent_ppo2.py:179][0m |          -0.0136 |         240.4316 |           5.8211 |
[32m[20221208 13:59:30 @agent_ppo2.py:179][0m |          -0.0256 |         236.4948 |           5.9194 |
[32m[20221208 13:59:30 @agent_ppo2.py:179][0m |          -0.0283 |         233.7355 |           5.9230 |
[32m[20221208 13:59:30 @agent_ppo2.py:179][0m |          -0.0331 |         232.9498 |           5.9813 |
[32m[20221208 13:59:30 @agent_ppo2.py:179][0m |          -0.0371 |         231.2134 |           5.9948 |
[32m[20221208 13:59:31 @agent_ppo2.py:179][0m |          -0.0371 |         229.1894 |           6.0118 |
[32m[20221208 13:59:31 @agent_ppo2.py:179][0m |          -0.0432 |         228.4012 |           6.0437 |
[32m[20221208 13:59:31 @agent_ppo2.py:179][0m |          -0.0429 |         228.1251 |           6.0724 |
[32m[20221208 13:59:31 @agent_ppo2.py:179][0m |          -0.0449 |         226.1242 |           6.0847 |
[32m[20221208 13:59:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 13:59:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 919.90
[32m[20221208 13:59:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 944.95
[32m[20221208 13:59:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 662.42
[32m[20221208 13:59:31 @agent_ppo2.py:137][0m Total time:      14.18 min
[32m[20221208 13:59:31 @agent_ppo2.py:139][0m 1161216 total steps have happened
[32m[20221208 13:59:31 @agent_ppo2.py:115][0m #------------------------ Iteration 567 --------------------------#
[32m[20221208 13:59:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |           0.0359 |         240.9653 |           5.7559 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0068 |         218.0022 |           5.6256 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0256 |         207.9661 |           5.8383 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0360 |         200.4516 |           5.9138 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0382 |         194.9361 |           5.9317 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0408 |         192.2623 |           5.9747 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0438 |         186.9815 |           6.0198 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0471 |         184.8204 |           6.0473 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0486 |         180.9161 |           6.0873 |
[32m[20221208 13:59:32 @agent_ppo2.py:179][0m |          -0.0502 |         178.9534 |           6.1168 |
[32m[20221208 13:59:32 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:59:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 895.75
[32m[20221208 13:59:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.35
[32m[20221208 13:59:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.96
[32m[20221208 13:59:33 @agent_ppo2.py:137][0m Total time:      14.21 min
[32m[20221208 13:59:33 @agent_ppo2.py:139][0m 1163264 total steps have happened
[32m[20221208 13:59:33 @agent_ppo2.py:115][0m #------------------------ Iteration 568 --------------------------#
[32m[20221208 13:59:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:33 @agent_ppo2.py:179][0m |           0.0407 |         269.6726 |           5.8222 |
[32m[20221208 13:59:33 @agent_ppo2.py:179][0m |           0.0177 |         261.2174 |           5.7748 |
[32m[20221208 13:59:33 @agent_ppo2.py:179][0m |          -0.0071 |         255.3846 |           5.7701 |
[32m[20221208 13:59:33 @agent_ppo2.py:179][0m |          -0.0284 |         250.1185 |           5.9420 |
[32m[20221208 13:59:33 @agent_ppo2.py:179][0m |          -0.0391 |         248.5785 |           5.9936 |
[32m[20221208 13:59:34 @agent_ppo2.py:179][0m |          -0.0427 |         247.3653 |           6.0260 |
[32m[20221208 13:59:34 @agent_ppo2.py:179][0m |          -0.0461 |         244.7789 |           6.0492 |
[32m[20221208 13:59:34 @agent_ppo2.py:179][0m |          -0.0486 |         244.2010 |           6.0834 |
[32m[20221208 13:59:34 @agent_ppo2.py:179][0m |          -0.0490 |         243.7509 |           6.0853 |
[32m[20221208 13:59:34 @agent_ppo2.py:179][0m |          -0.0510 |         242.7016 |           6.1032 |
[32m[20221208 13:59:34 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:59:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 911.98
[32m[20221208 13:59:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.91
[32m[20221208 13:59:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 610.05
[32m[20221208 13:59:34 @agent_ppo2.py:137][0m Total time:      14.23 min
[32m[20221208 13:59:34 @agent_ppo2.py:139][0m 1165312 total steps have happened
[32m[20221208 13:59:34 @agent_ppo2.py:115][0m #------------------------ Iteration 569 --------------------------#
[32m[20221208 13:59:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |           0.0302 |         239.1900 |           6.0335 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |           0.0171 |         219.0916 |           5.8037 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0211 |         208.1967 |           5.9412 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0323 |         199.7471 |           6.0070 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0379 |         195.1067 |           6.0606 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0434 |         189.4393 |           6.0776 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0482 |         185.9288 |           6.1307 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0499 |         183.7959 |           6.1600 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0499 |         180.8568 |           6.1548 |
[32m[20221208 13:59:35 @agent_ppo2.py:179][0m |          -0.0519 |         177.2179 |           6.1915 |
[32m[20221208 13:59:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 13:59:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.96
[32m[20221208 13:59:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 932.76
[32m[20221208 13:59:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.61
[32m[20221208 13:59:36 @agent_ppo2.py:137][0m Total time:      14.26 min
[32m[20221208 13:59:36 @agent_ppo2.py:139][0m 1167360 total steps have happened
[32m[20221208 13:59:36 @agent_ppo2.py:115][0m #------------------------ Iteration 570 --------------------------#
[32m[20221208 13:59:36 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 13:59:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:36 @agent_ppo2.py:179][0m |           0.0298 |         265.7828 |           6.0787 |
[32m[20221208 13:59:36 @agent_ppo2.py:179][0m |          -0.0069 |         242.3902 |           6.0718 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0276 |         231.7244 |           6.1900 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0339 |         227.3115 |           6.2220 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0413 |         224.0225 |           6.2683 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0438 |         220.4376 |           6.2842 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0452 |         218.9479 |           6.3151 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0505 |         216.1362 |           6.3156 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0518 |         213.8654 |           6.3527 |
[32m[20221208 13:59:37 @agent_ppo2.py:179][0m |          -0.0533 |         210.7253 |           6.3782 |
[32m[20221208 13:59:37 @agent_ppo2.py:124][0m Policy update time: 0.77 s
[32m[20221208 13:59:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.87
[32m[20221208 13:59:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.36
[32m[20221208 13:59:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 907.69
[32m[20221208 13:59:37 @agent_ppo2.py:137][0m Total time:      14.28 min
[32m[20221208 13:59:37 @agent_ppo2.py:139][0m 1169408 total steps have happened
[32m[20221208 13:59:37 @agent_ppo2.py:115][0m #------------------------ Iteration 571 --------------------------#
[32m[20221208 13:59:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |           0.0267 |         266.6909 |           6.4259 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0008 |         257.4559 |           6.3398 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0246 |         254.8285 |           6.4055 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0376 |         251.3787 |           6.4773 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0403 |         249.2641 |           6.5075 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0439 |         247.5015 |           6.5219 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0476 |         246.5897 |           6.5543 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0514 |         245.4982 |           6.5916 |
[32m[20221208 13:59:38 @agent_ppo2.py:179][0m |          -0.0476 |         246.5458 |           6.5775 |
[32m[20221208 13:59:39 @agent_ppo2.py:179][0m |          -0.0497 |         245.5161 |           6.5866 |
[32m[20221208 13:59:39 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:59:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 874.93
[32m[20221208 13:59:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.91
[32m[20221208 13:59:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.87
[32m[20221208 13:59:39 @agent_ppo2.py:137][0m Total time:      14.31 min
[32m[20221208 13:59:39 @agent_ppo2.py:139][0m 1171456 total steps have happened
[32m[20221208 13:59:39 @agent_ppo2.py:115][0m #------------------------ Iteration 572 --------------------------#
[32m[20221208 13:59:39 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |           0.0292 |         255.7939 |           6.2451 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |           0.0094 |         240.8282 |           5.9810 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0171 |         234.7066 |           6.2917 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0279 |         232.4591 |           6.3378 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0306 |         229.4527 |           6.4002 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0413 |         228.7430 |           6.4756 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0431 |         227.6473 |           6.5118 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0444 |         226.2500 |           6.5266 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0460 |         225.3391 |           6.5504 |
[32m[20221208 13:59:40 @agent_ppo2.py:179][0m |          -0.0467 |         224.3729 |           6.5668 |
[32m[20221208 13:59:40 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 13:59:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.65
[32m[20221208 13:59:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 900.82
[32m[20221208 13:59:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.75
[32m[20221208 13:59:41 @agent_ppo2.py:137][0m Total time:      14.34 min
[32m[20221208 13:59:41 @agent_ppo2.py:139][0m 1173504 total steps have happened
[32m[20221208 13:59:41 @agent_ppo2.py:115][0m #------------------------ Iteration 573 --------------------------#
[32m[20221208 13:59:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:41 @agent_ppo2.py:179][0m |           0.0472 |         236.2509 |           6.3084 |
[32m[20221208 13:59:41 @agent_ppo2.py:179][0m |           0.0199 |         219.9547 |           5.9870 |
[32m[20221208 13:59:41 @agent_ppo2.py:179][0m |          -0.0099 |         215.3287 |           6.3334 |
[32m[20221208 13:59:41 @agent_ppo2.py:179][0m |          -0.0272 |         212.0049 |           6.4422 |
[32m[20221208 13:59:41 @agent_ppo2.py:179][0m |          -0.0337 |         210.3704 |           6.4907 |
[32m[20221208 13:59:42 @agent_ppo2.py:179][0m |          -0.0391 |         209.2568 |           6.5373 |
[32m[20221208 13:59:42 @agent_ppo2.py:179][0m |          -0.0415 |         206.5962 |           6.5451 |
[32m[20221208 13:59:42 @agent_ppo2.py:179][0m |          -0.0445 |         205.4854 |           6.5511 |
[32m[20221208 13:59:42 @agent_ppo2.py:179][0m |          -0.0453 |         203.7757 |           6.5754 |
[32m[20221208 13:59:42 @agent_ppo2.py:179][0m |          -0.0474 |         202.0670 |           6.5949 |
[32m[20221208 13:59:42 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:59:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.28
[32m[20221208 13:59:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.80
[32m[20221208 13:59:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.26
[32m[20221208 13:59:42 @agent_ppo2.py:137][0m Total time:      14.36 min
[32m[20221208 13:59:42 @agent_ppo2.py:139][0m 1175552 total steps have happened
[32m[20221208 13:59:42 @agent_ppo2.py:115][0m #------------------------ Iteration 574 --------------------------#
[32m[20221208 13:59:43 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |           0.0367 |         257.7261 |           6.3835 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |           0.0020 |         239.7959 |           6.3535 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0221 |         235.3589 |           6.5106 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0334 |         231.6954 |           6.5681 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0375 |         231.3127 |           6.5699 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0408 |         229.3406 |           6.5888 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0428 |         227.5711 |           6.6111 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0452 |         226.8090 |           6.6271 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0477 |         226.1712 |           6.6507 |
[32m[20221208 13:59:43 @agent_ppo2.py:179][0m |          -0.0494 |         225.5331 |           6.6772 |
[32m[20221208 13:59:43 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 13:59:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 891.44
[32m[20221208 13:59:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.98
[32m[20221208 13:59:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.01
[32m[20221208 13:59:44 @agent_ppo2.py:137][0m Total time:      14.39 min
[32m[20221208 13:59:44 @agent_ppo2.py:139][0m 1177600 total steps have happened
[32m[20221208 13:59:44 @agent_ppo2.py:115][0m #------------------------ Iteration 575 --------------------------#
[32m[20221208 13:59:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 13:59:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:44 @agent_ppo2.py:179][0m |           0.0415 |         238.8717 |           6.2542 |
[32m[20221208 13:59:44 @agent_ppo2.py:179][0m |           0.0277 |         227.6319 |           5.8404 |
[32m[20221208 13:59:44 @agent_ppo2.py:179][0m |          -0.0030 |         222.0950 |           6.2237 |
[32m[20221208 13:59:45 @agent_ppo2.py:179][0m |          -0.0157 |         221.4971 |           6.3401 |
[32m[20221208 13:59:45 @agent_ppo2.py:179][0m |          -0.0221 |         219.5552 |           6.4294 |
[32m[20221208 13:59:45 @agent_ppo2.py:179][0m |          -0.0325 |         216.6600 |           6.4873 |
[32m[20221208 13:59:45 @agent_ppo2.py:179][0m |          -0.0372 |         216.3696 |           6.5360 |
[32m[20221208 13:59:45 @agent_ppo2.py:179][0m |          -0.0392 |         214.4306 |           6.5586 |
[32m[20221208 13:59:45 @agent_ppo2.py:179][0m |          -0.0406 |         215.1641 |           6.5921 |
[32m[20221208 13:59:45 @agent_ppo2.py:179][0m |          -0.0426 |         212.8591 |           6.6294 |
[32m[20221208 13:59:45 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:59:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.13
[32m[20221208 13:59:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.97
[32m[20221208 13:59:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.18
[32m[20221208 13:59:45 @agent_ppo2.py:137][0m Total time:      14.42 min
[32m[20221208 13:59:45 @agent_ppo2.py:139][0m 1179648 total steps have happened
[32m[20221208 13:59:45 @agent_ppo2.py:115][0m #------------------------ Iteration 576 --------------------------#
[32m[20221208 13:59:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |           0.0279 |         251.0323 |           6.5129 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0036 |         240.1722 |           6.4453 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0070 |         233.4233 |           6.3781 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0240 |         229.0828 |           6.5667 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0358 |         226.5040 |           6.6502 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0389 |         224.0431 |           6.6767 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0429 |         223.6609 |           6.7110 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0413 |         221.3539 |           6.7121 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0488 |         219.5512 |           6.7784 |
[32m[20221208 13:59:46 @agent_ppo2.py:179][0m |          -0.0494 |         218.1479 |           6.7780 |
[32m[20221208 13:59:46 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:59:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.44
[32m[20221208 13:59:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.44
[32m[20221208 13:59:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 927.23
[32m[20221208 13:59:47 @agent_ppo2.py:137][0m Total time:      14.44 min
[32m[20221208 13:59:47 @agent_ppo2.py:139][0m 1181696 total steps have happened
[32m[20221208 13:59:47 @agent_ppo2.py:115][0m #------------------------ Iteration 577 --------------------------#
[32m[20221208 13:59:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:47 @agent_ppo2.py:179][0m |           0.0432 |         235.0336 |           6.4093 |
[32m[20221208 13:59:47 @agent_ppo2.py:179][0m |           0.0135 |         213.2439 |           6.0369 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0168 |         204.0394 |           6.2604 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0304 |         200.6880 |           6.4083 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0402 |         199.1774 |           6.4766 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0440 |         198.7328 |           6.5107 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0452 |         197.2517 |           6.5430 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0478 |         196.7832 |           6.5285 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0517 |         195.1338 |           6.5805 |
[32m[20221208 13:59:48 @agent_ppo2.py:179][0m |          -0.0546 |         193.3509 |           6.6050 |
[32m[20221208 13:59:48 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 13:59:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 792.59
[32m[20221208 13:59:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 911.55
[32m[20221208 13:59:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.97
[32m[20221208 13:59:48 @agent_ppo2.py:137][0m Total time:      14.47 min
[32m[20221208 13:59:48 @agent_ppo2.py:139][0m 1183744 total steps have happened
[32m[20221208 13:59:48 @agent_ppo2.py:115][0m #------------------------ Iteration 578 --------------------------#
[32m[20221208 13:59:49 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |           0.0167 |         227.4637 |           6.5335 |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |          -0.0061 |         204.6797 |           6.4530 |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |          -0.0264 |         195.4364 |           6.5528 |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |          -0.0367 |         189.8297 |           6.5961 |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |          -0.0418 |         186.8392 |           6.6081 |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |          -0.0454 |         183.3572 |           6.6303 |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |          -0.0474 |         182.0351 |           6.6461 |
[32m[20221208 13:59:49 @agent_ppo2.py:179][0m |          -0.0489 |         179.4318 |           6.6609 |
[32m[20221208 13:59:50 @agent_ppo2.py:179][0m |          -0.0511 |         179.0113 |           6.6985 |
[32m[20221208 13:59:50 @agent_ppo2.py:179][0m |          -0.0523 |         177.7580 |           6.7283 |
[32m[20221208 13:59:50 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 13:59:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 905.84
[32m[20221208 13:59:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.85
[32m[20221208 13:59:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 905.60
[32m[20221208 13:59:50 @agent_ppo2.py:137][0m Total time:      14.49 min
[32m[20221208 13:59:50 @agent_ppo2.py:139][0m 1185792 total steps have happened
[32m[20221208 13:59:50 @agent_ppo2.py:115][0m #------------------------ Iteration 579 --------------------------#
[32m[20221208 13:59:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |           0.0172 |         235.2987 |           6.4757 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |           0.0109 |         225.2876 |           6.2740 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0133 |         219.7430 |           6.4599 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0276 |         215.4859 |           6.5540 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0353 |         212.0396 |           6.6393 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0373 |         209.8770 |           6.6785 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0390 |         207.8397 |           6.6844 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0401 |         206.5605 |           6.7022 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0434 |         205.9211 |           6.7370 |
[32m[20221208 13:59:51 @agent_ppo2.py:179][0m |          -0.0442 |         205.1770 |           6.7498 |
[32m[20221208 13:59:51 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 13:59:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.68
[32m[20221208 13:59:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.40
[32m[20221208 13:59:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 928.20
[32m[20221208 13:59:52 @agent_ppo2.py:137][0m Total time:      14.52 min
[32m[20221208 13:59:52 @agent_ppo2.py:139][0m 1187840 total steps have happened
[32m[20221208 13:59:52 @agent_ppo2.py:115][0m #------------------------ Iteration 580 --------------------------#
[32m[20221208 13:59:52 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 13:59:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:52 @agent_ppo2.py:179][0m |           0.0169 |         253.3649 |           6.5230 |
[32m[20221208 13:59:52 @agent_ppo2.py:179][0m |           0.0140 |         247.2749 |           6.3434 |
[32m[20221208 13:59:52 @agent_ppo2.py:179][0m |          -0.0198 |         243.0933 |           6.5422 |
[32m[20221208 13:59:52 @agent_ppo2.py:179][0m |          -0.0284 |         240.1187 |           6.6153 |
[32m[20221208 13:59:52 @agent_ppo2.py:179][0m |          -0.0344 |         237.4530 |           6.6048 |
[32m[20221208 13:59:52 @agent_ppo2.py:179][0m |          -0.0396 |         235.8509 |           6.6407 |
[32m[20221208 13:59:53 @agent_ppo2.py:179][0m |          -0.0438 |         234.6927 |           6.6928 |
[32m[20221208 13:59:53 @agent_ppo2.py:179][0m |          -0.0427 |         232.8782 |           6.7223 |
[32m[20221208 13:59:53 @agent_ppo2.py:179][0m |          -0.0453 |         232.0216 |           6.7037 |
[32m[20221208 13:59:53 @agent_ppo2.py:179][0m |          -0.0449 |         232.5798 |           6.7234 |
[32m[20221208 13:59:53 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 13:59:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 900.42
[32m[20221208 13:59:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.40
[32m[20221208 13:59:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 862.02
[32m[20221208 13:59:53 @agent_ppo2.py:137][0m Total time:      14.55 min
[32m[20221208 13:59:53 @agent_ppo2.py:139][0m 1189888 total steps have happened
[32m[20221208 13:59:53 @agent_ppo2.py:115][0m #------------------------ Iteration 581 --------------------------#
[32m[20221208 13:59:54 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 13:59:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |           0.0394 |         237.0110 |           6.6173 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |           0.0365 |         211.1781 |           6.3213 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |           0.0142 |         199.0839 |           6.3659 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |          -0.0159 |         188.4942 |           6.8145 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |          -0.0284 |         184.6082 |           6.8647 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |          -0.0358 |         181.0867 |           6.9618 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |          -0.0397 |         179.8103 |           6.9862 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |          -0.0435 |         176.9525 |           7.0143 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |          -0.0472 |         175.1187 |           7.0590 |
[32m[20221208 13:59:54 @agent_ppo2.py:179][0m |          -0.0475 |         173.7153 |           7.0794 |
[32m[20221208 13:59:54 @agent_ppo2.py:124][0m Policy update time: 0.81 s
[32m[20221208 13:59:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 896.05
[32m[20221208 13:59:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 943.58
[32m[20221208 13:59:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 902.64
[32m[20221208 13:59:55 @agent_ppo2.py:137][0m Total time:      14.58 min
[32m[20221208 13:59:55 @agent_ppo2.py:139][0m 1191936 total steps have happened
[32m[20221208 13:59:55 @agent_ppo2.py:115][0m #------------------------ Iteration 582 --------------------------#
[32m[20221208 13:59:56 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 13:59:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |           0.0305 |         290.0291 |           6.8919 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |           0.0017 |         259.2931 |           6.7326 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0180 |         249.6945 |           6.8290 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0348 |         244.0806 |           6.9422 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0404 |         240.0279 |           6.9667 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0470 |         236.4328 |           7.0144 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0498 |         234.2635 |           7.0839 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0458 |         231.7689 |           7.0763 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0508 |         229.7777 |           7.0895 |
[32m[20221208 13:59:56 @agent_ppo2.py:179][0m |          -0.0510 |         228.4794 |           7.1254 |
[32m[20221208 13:59:56 @agent_ppo2.py:124][0m Policy update time: 0.88 s
[32m[20221208 13:59:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 864.68
[32m[20221208 13:59:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.21
[32m[20221208 13:59:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.67
[32m[20221208 13:59:57 @agent_ppo2.py:137][0m Total time:      14.61 min
[32m[20221208 13:59:57 @agent_ppo2.py:139][0m 1193984 total steps have happened
[32m[20221208 13:59:57 @agent_ppo2.py:115][0m #------------------------ Iteration 583 --------------------------#
[32m[20221208 13:59:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 13:59:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:57 @agent_ppo2.py:179][0m |           0.0554 |         299.1997 |           6.5304 |
[32m[20221208 13:59:57 @agent_ppo2.py:179][0m |           0.0185 |         272.3105 |           6.4162 |
[32m[20221208 13:59:57 @agent_ppo2.py:179][0m |          -0.0157 |         259.8415 |           6.5897 |
[32m[20221208 13:59:58 @agent_ppo2.py:179][0m |          -0.0274 |         253.1874 |           6.7001 |
[32m[20221208 13:59:58 @agent_ppo2.py:179][0m |          -0.0378 |         249.1261 |           6.7819 |
[32m[20221208 13:59:58 @agent_ppo2.py:179][0m |          -0.0468 |         244.4097 |           6.8436 |
[32m[20221208 13:59:58 @agent_ppo2.py:179][0m |          -0.0510 |         240.2771 |           6.8680 |
[32m[20221208 13:59:58 @agent_ppo2.py:179][0m |          -0.0546 |         237.6607 |           6.9017 |
[32m[20221208 13:59:58 @agent_ppo2.py:179][0m |          -0.0574 |         235.2165 |           6.9325 |
[32m[20221208 13:59:58 @agent_ppo2.py:179][0m |          -0.0604 |         233.0638 |           6.9526 |
[32m[20221208 13:59:58 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 13:59:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 824.38
[32m[20221208 13:59:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 909.86
[32m[20221208 13:59:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 915.47
[32m[20221208 13:59:58 @agent_ppo2.py:137][0m Total time:      14.63 min
[32m[20221208 13:59:58 @agent_ppo2.py:139][0m 1196032 total steps have happened
[32m[20221208 13:59:58 @agent_ppo2.py:115][0m #------------------------ Iteration 584 --------------------------#
[32m[20221208 13:59:59 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 13:59:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |           0.0321 |         245.7681 |           6.9217 |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |           0.0175 |         227.6294 |           6.6527 |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |           0.0020 |         218.8285 |           6.6722 |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |          -0.0291 |         212.2478 |           6.8759 |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |          -0.0357 |         208.6181 |           7.0132 |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |          -0.0414 |         205.9973 |           7.0525 |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |          -0.0447 |         204.0323 |           7.1018 |
[32m[20221208 13:59:59 @agent_ppo2.py:179][0m |          -0.0459 |         202.8317 |           7.1325 |
[32m[20221208 14:00:00 @agent_ppo2.py:179][0m |          -0.0509 |         199.3334 |           7.1464 |
[32m[20221208 14:00:00 @agent_ppo2.py:179][0m |          -0.0521 |         197.7839 |           7.1624 |
[32m[20221208 14:00:00 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:00:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.31
[32m[20221208 14:00:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.87
[32m[20221208 14:00:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.42
[32m[20221208 14:00:00 @agent_ppo2.py:137][0m Total time:      14.66 min
[32m[20221208 14:00:00 @agent_ppo2.py:139][0m 1198080 total steps have happened
[32m[20221208 14:00:00 @agent_ppo2.py:115][0m #------------------------ Iteration 585 --------------------------#
[32m[20221208 14:00:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |           0.0239 |         240.9300 |           7.3814 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0100 |         205.2695 |           7.3132 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0339 |         183.7302 |           7.4026 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0430 |         175.3435 |           7.4465 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0509 |         168.1312 |           7.4980 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0542 |         163.6430 |           7.5103 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0566 |         159.6275 |           7.5284 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0554 |         155.8847 |           7.5295 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0584 |         153.6824 |           7.5312 |
[32m[20221208 14:00:01 @agent_ppo2.py:179][0m |          -0.0633 |         150.2700 |           7.5862 |
[32m[20221208 14:00:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:00:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 811.37
[32m[20221208 14:00:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 883.38
[32m[20221208 14:00:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.77
[32m[20221208 14:00:01 @agent_ppo2.py:137][0m Total time:      14.69 min
[32m[20221208 14:00:01 @agent_ppo2.py:139][0m 1200128 total steps have happened
[32m[20221208 14:00:01 @agent_ppo2.py:115][0m #------------------------ Iteration 586 --------------------------#
[32m[20221208 14:00:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |           0.0284 |         269.2302 |           7.6158 |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |           0.0106 |         246.6811 |           7.4266 |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |          -0.0217 |         240.2087 |           7.5225 |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |          -0.0416 |         236.5665 |           7.6439 |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |          -0.0484 |         233.6603 |           7.6947 |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |          -0.0555 |         231.3224 |           7.7189 |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |          -0.0534 |         228.9499 |           7.7242 |
[32m[20221208 14:00:02 @agent_ppo2.py:179][0m |          -0.0560 |         227.9597 |           7.7378 |
[32m[20221208 14:00:03 @agent_ppo2.py:179][0m |          -0.0622 |         227.3382 |           7.7851 |
[32m[20221208 14:00:03 @agent_ppo2.py:179][0m |          -0.0619 |         225.9209 |           7.7916 |
[32m[20221208 14:00:03 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:00:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 816.88
[32m[20221208 14:00:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.28
[32m[20221208 14:00:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.72
[32m[20221208 14:00:03 @agent_ppo2.py:137][0m Total time:      14.71 min
[32m[20221208 14:00:03 @agent_ppo2.py:139][0m 1202176 total steps have happened
[32m[20221208 14:00:03 @agent_ppo2.py:115][0m #------------------------ Iteration 587 --------------------------#
[32m[20221208 14:00:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:00:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |           0.0272 |         251.6912 |           7.6493 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0120 |         240.4369 |           7.6457 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0272 |         235.1832 |           7.7143 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0408 |         232.5839 |           7.7892 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0466 |         229.8522 |           7.8486 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0469 |         228.6089 |           7.8620 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0439 |         227.5588 |           7.8164 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0494 |         226.5427 |           7.8697 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0531 |         225.3774 |           7.8852 |
[32m[20221208 14:00:04 @agent_ppo2.py:179][0m |          -0.0547 |         223.5176 |           7.9118 |
[32m[20221208 14:00:04 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:00:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 828.19
[32m[20221208 14:00:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.51
[32m[20221208 14:00:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.29
[32m[20221208 14:00:05 @agent_ppo2.py:137][0m Total time:      14.74 min
[32m[20221208 14:00:05 @agent_ppo2.py:139][0m 1204224 total steps have happened
[32m[20221208 14:00:05 @agent_ppo2.py:115][0m #------------------------ Iteration 588 --------------------------#
[32m[20221208 14:00:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:05 @agent_ppo2.py:179][0m |           0.0246 |         216.8749 |           7.6923 |
[32m[20221208 14:00:05 @agent_ppo2.py:179][0m |          -0.0025 |         192.5374 |           7.4266 |
[32m[20221208 14:00:05 @agent_ppo2.py:179][0m |          -0.0259 |         177.0275 |           7.6358 |
[32m[20221208 14:00:05 @agent_ppo2.py:179][0m |          -0.0369 |         166.1330 |           7.7281 |
[32m[20221208 14:00:05 @agent_ppo2.py:179][0m |          -0.0441 |         160.7521 |           7.7670 |
[32m[20221208 14:00:05 @agent_ppo2.py:179][0m |          -0.0509 |         156.3554 |           7.8515 |
[32m[20221208 14:00:05 @agent_ppo2.py:179][0m |          -0.0545 |         151.5104 |           7.8785 |
[32m[20221208 14:00:06 @agent_ppo2.py:179][0m |          -0.0539 |         148.4110 |           7.9051 |
[32m[20221208 14:00:06 @agent_ppo2.py:179][0m |          -0.0499 |         146.7689 |           7.9016 |
[32m[20221208 14:00:06 @agent_ppo2.py:179][0m |          -0.0562 |         142.4691 |           7.9333 |
[32m[20221208 14:00:06 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:00:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 888.71
[32m[20221208 14:00:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 916.60
[32m[20221208 14:00:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.62
[32m[20221208 14:00:06 @agent_ppo2.py:137][0m Total time:      14.76 min
[32m[20221208 14:00:06 @agent_ppo2.py:139][0m 1206272 total steps have happened
[32m[20221208 14:00:06 @agent_ppo2.py:115][0m #------------------------ Iteration 589 --------------------------#
[32m[20221208 14:00:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:00:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |           0.0305 |         248.0900 |           7.9753 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0077 |         225.6894 |           7.8850 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0339 |         218.2380 |           8.1315 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0411 |         212.8941 |           8.1603 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0483 |         207.6808 |           8.1962 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0521 |         204.1018 |           8.2236 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0559 |         202.1466 |           8.2692 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0586 |         199.1502 |           8.2724 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0606 |         196.4497 |           8.2797 |
[32m[20221208 14:00:07 @agent_ppo2.py:179][0m |          -0.0622 |         195.8764 |           8.3137 |
[32m[20221208 14:00:07 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:00:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.03
[32m[20221208 14:00:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.26
[32m[20221208 14:00:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.97
[32m[20221208 14:00:08 @agent_ppo2.py:137][0m Total time:      14.79 min
[32m[20221208 14:00:08 @agent_ppo2.py:139][0m 1208320 total steps have happened
[32m[20221208 14:00:08 @agent_ppo2.py:115][0m #------------------------ Iteration 590 --------------------------#
[32m[20221208 14:00:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:00:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:08 @agent_ppo2.py:179][0m |           0.0316 |         233.4715 |           7.9963 |
[32m[20221208 14:00:08 @agent_ppo2.py:179][0m |           0.0018 |         220.4876 |           7.7248 |
[32m[20221208 14:00:08 @agent_ppo2.py:179][0m |          -0.0159 |         214.7775 |           7.9593 |
[32m[20221208 14:00:08 @agent_ppo2.py:179][0m |          -0.0320 |         211.2312 |           8.0711 |
[32m[20221208 14:00:08 @agent_ppo2.py:179][0m |          -0.0370 |         207.0512 |           8.1326 |
[32m[20221208 14:00:09 @agent_ppo2.py:179][0m |          -0.0404 |         204.2199 |           8.1728 |
[32m[20221208 14:00:09 @agent_ppo2.py:179][0m |          -0.0419 |         201.1099 |           8.1887 |
[32m[20221208 14:00:09 @agent_ppo2.py:179][0m |          -0.0412 |         199.0362 |           8.1993 |
[32m[20221208 14:00:09 @agent_ppo2.py:179][0m |          -0.0457 |         197.0129 |           8.2491 |
[32m[20221208 14:00:09 @agent_ppo2.py:179][0m |          -0.0494 |         195.2692 |           8.2755 |
[32m[20221208 14:00:09 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:00:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.00
[32m[20221208 14:00:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.35
[32m[20221208 14:00:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 933.61
[32m[20221208 14:00:09 @agent_ppo2.py:137][0m Total time:      14.81 min
[32m[20221208 14:00:09 @agent_ppo2.py:139][0m 1210368 total steps have happened
[32m[20221208 14:00:09 @agent_ppo2.py:115][0m #------------------------ Iteration 591 --------------------------#
[32m[20221208 14:00:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:00:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |           0.0504 |         251.0777 |           7.9926 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |           0.0170 |         239.8537 |           7.7384 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0179 |         234.2187 |           8.1398 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0267 |         231.8811 |           8.2959 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0350 |         230.6438 |           8.3310 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0410 |         230.1067 |           8.3454 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0440 |         230.4442 |           8.3943 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0418 |         228.0752 |           8.3808 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0459 |         227.3570 |           8.4218 |
[32m[20221208 14:00:10 @agent_ppo2.py:179][0m |          -0.0474 |         227.6298 |           8.4287 |
[32m[20221208 14:00:10 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 14:00:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.21
[32m[20221208 14:00:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.13
[32m[20221208 14:00:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.56
[32m[20221208 14:00:11 @agent_ppo2.py:137][0m Total time:      14.84 min
[32m[20221208 14:00:11 @agent_ppo2.py:139][0m 1212416 total steps have happened
[32m[20221208 14:00:11 @agent_ppo2.py:115][0m #------------------------ Iteration 592 --------------------------#
[32m[20221208 14:00:11 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:00:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:11 @agent_ppo2.py:179][0m |           0.0362 |         240.7080 |           8.1398 |
[32m[20221208 14:00:11 @agent_ppo2.py:179][0m |           0.0238 |         234.4441 |           7.6471 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0130 |         232.2792 |           8.3274 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0265 |         233.3361 |           8.4661 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0327 |         231.5351 |           8.5604 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0372 |         229.5310 |           8.6075 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0425 |         228.8476 |           8.6325 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0458 |         229.4794 |           8.6597 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0486 |         228.2805 |           8.6840 |
[32m[20221208 14:00:12 @agent_ppo2.py:179][0m |          -0.0495 |         227.7575 |           8.6990 |
[32m[20221208 14:00:12 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:00:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 910.88
[32m[20221208 14:00:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.17
[32m[20221208 14:00:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.58
[32m[20221208 14:00:12 @agent_ppo2.py:137][0m Total time:      14.87 min
[32m[20221208 14:00:12 @agent_ppo2.py:139][0m 1214464 total steps have happened
[32m[20221208 14:00:12 @agent_ppo2.py:115][0m #------------------------ Iteration 593 --------------------------#
[32m[20221208 14:00:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:00:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |           0.0328 |         242.0020 |           8.3318 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |           0.0018 |         236.5470 |           8.3904 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |          -0.0091 |         234.0958 |           8.3876 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |          -0.0202 |         232.9413 |           8.4953 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |          -0.0326 |         232.5703 |           8.5993 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |          -0.0376 |         231.9610 |           8.6621 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |          -0.0409 |         228.5547 |           8.6916 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |          -0.0419 |         228.1641 |           8.7319 |
[32m[20221208 14:00:13 @agent_ppo2.py:179][0m |          -0.0454 |         226.9440 |           8.7724 |
[32m[20221208 14:00:14 @agent_ppo2.py:179][0m |          -0.0441 |         226.0518 |           8.7627 |
[32m[20221208 14:00:14 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:00:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 905.51
[32m[20221208 14:00:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.60
[32m[20221208 14:00:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 914.40
[32m[20221208 14:00:14 @agent_ppo2.py:137][0m Total time:      14.89 min
[32m[20221208 14:00:14 @agent_ppo2.py:139][0m 1216512 total steps have happened
[32m[20221208 14:00:14 @agent_ppo2.py:115][0m #------------------------ Iteration 594 --------------------------#
[32m[20221208 14:00:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:14 @agent_ppo2.py:179][0m |           0.0282 |         242.1493 |           8.5639 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0031 |         231.5789 |           8.4490 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0217 |         227.7525 |           8.6089 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0332 |         225.7437 |           8.7106 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0393 |         224.0092 |           8.7657 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0421 |         222.9050 |           8.8239 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0447 |         222.2088 |           8.8374 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0454 |         220.9530 |           8.8874 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0443 |         220.1439 |           8.8738 |
[32m[20221208 14:00:15 @agent_ppo2.py:179][0m |          -0.0465 |         218.4606 |           8.8930 |
[32m[20221208 14:00:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:00:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 891.36
[32m[20221208 14:00:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.05
[32m[20221208 14:00:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 904.60
[32m[20221208 14:00:15 @agent_ppo2.py:137][0m Total time:      14.92 min
[32m[20221208 14:00:15 @agent_ppo2.py:139][0m 1218560 total steps have happened
[32m[20221208 14:00:15 @agent_ppo2.py:115][0m #------------------------ Iteration 595 --------------------------#
[32m[20221208 14:00:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:00:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |           0.0395 |         234.3631 |           8.2261 |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |           0.0781 |         225.9274 |           7.0103 |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |           0.0169 |         222.2697 |           7.7049 |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |          -0.0136 |         221.0929 |           8.2723 |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |          -0.0268 |         218.9058 |           8.4724 |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |          -0.0311 |         217.5126 |           8.5341 |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |          -0.0372 |         216.3035 |           8.6044 |
[32m[20221208 14:00:16 @agent_ppo2.py:179][0m |          -0.0417 |         214.9832 |           8.6620 |
[32m[20221208 14:00:17 @agent_ppo2.py:179][0m |          -0.0420 |         213.7933 |           8.7131 |
[32m[20221208 14:00:17 @agent_ppo2.py:179][0m |          -0.0437 |         213.3953 |           8.7213 |
[32m[20221208 14:00:17 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:00:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 912.98
[32m[20221208 14:00:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.13
[32m[20221208 14:00:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 915.09
[32m[20221208 14:00:17 @agent_ppo2.py:137][0m Total time:      14.94 min
[32m[20221208 14:00:17 @agent_ppo2.py:139][0m 1220608 total steps have happened
[32m[20221208 14:00:17 @agent_ppo2.py:115][0m #------------------------ Iteration 596 --------------------------#
[32m[20221208 14:00:17 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:00:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |           0.0286 |         239.7400 |           8.4008 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |           0.0172 |         223.7599 |           8.2530 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0175 |         218.1504 |           8.5802 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0250 |         213.8466 |           8.6572 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0305 |         209.8189 |           8.7076 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0360 |         207.7182 |           8.7544 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0385 |         206.0175 |           8.7778 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0414 |         205.8388 |           8.8103 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0439 |         204.0572 |           8.8462 |
[32m[20221208 14:00:18 @agent_ppo2.py:179][0m |          -0.0440 |         203.9850 |           8.8677 |
[32m[20221208 14:00:18 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:00:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.84
[32m[20221208 14:00:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.84
[32m[20221208 14:00:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.50
[32m[20221208 14:00:19 @agent_ppo2.py:137][0m Total time:      14.97 min
[32m[20221208 14:00:19 @agent_ppo2.py:139][0m 1222656 total steps have happened
[32m[20221208 14:00:19 @agent_ppo2.py:115][0m #------------------------ Iteration 597 --------------------------#
[32m[20221208 14:00:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:00:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:19 @agent_ppo2.py:179][0m |           0.0477 |         235.7905 |           8.2651 |
[32m[20221208 14:00:19 @agent_ppo2.py:179][0m |           0.0796 |         224.2647 |           7.5902 |
[32m[20221208 14:00:19 @agent_ppo2.py:179][0m |           0.0025 |         219.6058 |           7.9292 |
[32m[20221208 14:00:19 @agent_ppo2.py:179][0m |          -0.0194 |         217.5194 |           8.2090 |
[32m[20221208 14:00:19 @agent_ppo2.py:179][0m |          -0.0283 |         216.7535 |           8.3523 |
[32m[20221208 14:00:19 @agent_ppo2.py:179][0m |          -0.0352 |         215.6546 |           8.4390 |
[32m[20221208 14:00:20 @agent_ppo2.py:179][0m |          -0.0396 |         214.8815 |           8.4871 |
[32m[20221208 14:00:20 @agent_ppo2.py:179][0m |          -0.0439 |         213.8858 |           8.5247 |
[32m[20221208 14:00:20 @agent_ppo2.py:179][0m |          -0.0441 |         211.0147 |           8.5044 |
[32m[20221208 14:00:20 @agent_ppo2.py:179][0m |          -0.0470 |         210.1535 |           8.5629 |
[32m[20221208 14:00:20 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:00:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 901.92
[32m[20221208 14:00:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.06
[32m[20221208 14:00:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.68
[32m[20221208 14:00:20 @agent_ppo2.py:137][0m Total time:      15.00 min
[32m[20221208 14:00:20 @agent_ppo2.py:139][0m 1224704 total steps have happened
[32m[20221208 14:00:20 @agent_ppo2.py:115][0m #------------------------ Iteration 598 --------------------------#
[32m[20221208 14:00:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:00:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |           0.0543 |         232.9483 |           8.2544 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |           0.0422 |         224.5080 |           7.8221 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0042 |         220.5101 |           8.2211 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0218 |         219.4265 |           8.3103 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0331 |         215.9045 |           8.4351 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0374 |         214.5898 |           8.4774 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0427 |         214.2635 |           8.5293 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0440 |         214.4061 |           8.5474 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0462 |         212.3750 |           8.5734 |
[32m[20221208 14:00:21 @agent_ppo2.py:179][0m |          -0.0478 |         211.9170 |           8.5838 |
[32m[20221208 14:00:21 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:00:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 843.83
[32m[20221208 14:00:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 872.87
[32m[20221208 14:00:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.28
[32m[20221208 14:00:22 @agent_ppo2.py:137][0m Total time:      15.02 min
[32m[20221208 14:00:22 @agent_ppo2.py:139][0m 1226752 total steps have happened
[32m[20221208 14:00:22 @agent_ppo2.py:115][0m #------------------------ Iteration 599 --------------------------#
[32m[20221208 14:00:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:22 @agent_ppo2.py:179][0m |           0.0548 |         227.9309 |           8.5559 |
[32m[20221208 14:00:22 @agent_ppo2.py:179][0m |           0.0310 |         220.6121 |           7.9138 |
[32m[20221208 14:00:22 @agent_ppo2.py:179][0m |           0.0001 |         217.5339 |           8.3082 |
[32m[20221208 14:00:22 @agent_ppo2.py:179][0m |          -0.0201 |         214.8302 |           8.5817 |
[32m[20221208 14:00:22 @agent_ppo2.py:179][0m |          -0.0286 |         212.5121 |           8.6955 |
[32m[20221208 14:00:23 @agent_ppo2.py:179][0m |          -0.0331 |         210.4188 |           8.7927 |
[32m[20221208 14:00:23 @agent_ppo2.py:179][0m |          -0.0372 |         208.4389 |           8.8150 |
[32m[20221208 14:00:23 @agent_ppo2.py:179][0m |          -0.0411 |         207.3816 |           8.8761 |
[32m[20221208 14:00:23 @agent_ppo2.py:179][0m |          -0.0354 |         206.6686 |           8.8928 |
[32m[20221208 14:00:23 @agent_ppo2.py:179][0m |          -0.0355 |         204.1676 |           8.8314 |
[32m[20221208 14:00:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:00:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 914.64
[32m[20221208 14:00:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.95
[32m[20221208 14:00:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.68
[32m[20221208 14:00:23 @agent_ppo2.py:137][0m Total time:      15.05 min
[32m[20221208 14:00:23 @agent_ppo2.py:139][0m 1228800 total steps have happened
[32m[20221208 14:00:23 @agent_ppo2.py:115][0m #------------------------ Iteration 600 --------------------------#
[32m[20221208 14:00:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |           0.0294 |         242.8243 |           8.6770 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |           0.0015 |         235.7247 |           8.6610 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0179 |         233.6006 |           8.8498 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0247 |         232.9403 |           8.9261 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0331 |         229.9728 |           9.0178 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0346 |         230.0037 |           9.0221 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0376 |         229.3554 |           9.0553 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0372 |         231.4564 |           9.0745 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0383 |         228.8013 |           9.0915 |
[32m[20221208 14:00:24 @agent_ppo2.py:179][0m |          -0.0409 |         227.6024 |           9.1103 |
[32m[20221208 14:00:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:00:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 967.86
[32m[20221208 14:00:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.63
[32m[20221208 14:00:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.07
[32m[20221208 14:00:25 @agent_ppo2.py:137][0m Total time:      15.07 min
[32m[20221208 14:00:25 @agent_ppo2.py:139][0m 1230848 total steps have happened
[32m[20221208 14:00:25 @agent_ppo2.py:115][0m #------------------------ Iteration 601 --------------------------#
[32m[20221208 14:00:25 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:00:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:25 @agent_ppo2.py:179][0m |           0.0416 |         238.1968 |           8.8355 |
[32m[20221208 14:00:25 @agent_ppo2.py:179][0m |           0.0529 |         232.4087 |           7.8938 |
[32m[20221208 14:00:25 @agent_ppo2.py:179][0m |          -0.0032 |         230.5756 |           8.6069 |
[32m[20221208 14:00:25 @agent_ppo2.py:179][0m |          -0.0140 |         228.5681 |           8.7701 |
[32m[20221208 14:00:26 @agent_ppo2.py:179][0m |          -0.0299 |         226.5538 |           8.8565 |
[32m[20221208 14:00:26 @agent_ppo2.py:179][0m |          -0.0369 |         226.8769 |           8.9305 |
[32m[20221208 14:00:26 @agent_ppo2.py:179][0m |          -0.0413 |         225.1429 |           8.9441 |
[32m[20221208 14:00:26 @agent_ppo2.py:179][0m |          -0.0450 |         224.3684 |           8.9945 |
[32m[20221208 14:00:26 @agent_ppo2.py:179][0m |          -0.0470 |         225.4136 |           9.0295 |
[32m[20221208 14:00:26 @agent_ppo2.py:179][0m |          -0.0464 |         224.5491 |           9.0077 |
[32m[20221208 14:00:26 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:00:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 856.48
[32m[20221208 14:00:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 890.44
[32m[20221208 14:00:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.17
[32m[20221208 14:00:26 @agent_ppo2.py:137][0m Total time:      15.10 min
[32m[20221208 14:00:26 @agent_ppo2.py:139][0m 1232896 total steps have happened
[32m[20221208 14:00:26 @agent_ppo2.py:115][0m #------------------------ Iteration 602 --------------------------#
[32m[20221208 14:00:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |           0.0362 |         229.1077 |           8.9570 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |           0.0180 |         211.2587 |           8.6115 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0177 |         202.7072 |           8.9935 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0291 |         197.5091 |           9.1286 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0372 |         192.2483 |           9.2143 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0404 |         190.3404 |           9.2781 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0423 |         188.3967 |           9.3109 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0425 |         187.3084 |           9.3353 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0451 |         185.6100 |           9.3675 |
[32m[20221208 14:00:27 @agent_ppo2.py:179][0m |          -0.0455 |         184.7253 |           9.3792 |
[32m[20221208 14:00:27 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:00:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.82
[32m[20221208 14:00:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.90
[32m[20221208 14:00:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.28
[32m[20221208 14:00:28 @agent_ppo2.py:137][0m Total time:      15.12 min
[32m[20221208 14:00:28 @agent_ppo2.py:139][0m 1234944 total steps have happened
[32m[20221208 14:00:28 @agent_ppo2.py:115][0m #------------------------ Iteration 603 --------------------------#
[32m[20221208 14:00:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:28 @agent_ppo2.py:179][0m |           0.0259 |         255.8891 |           9.2510 |
[32m[20221208 14:00:28 @agent_ppo2.py:179][0m |           0.0139 |         242.7516 |           8.8700 |
[32m[20221208 14:00:28 @agent_ppo2.py:179][0m |          -0.0089 |         238.7701 |           9.1675 |
[32m[20221208 14:00:29 @agent_ppo2.py:179][0m |          -0.0273 |         237.3889 |           9.3673 |
[32m[20221208 14:00:29 @agent_ppo2.py:179][0m |          -0.0349 |         236.2734 |           9.4091 |
[32m[20221208 14:00:29 @agent_ppo2.py:179][0m |          -0.0412 |         235.3214 |           9.4915 |
[32m[20221208 14:00:29 @agent_ppo2.py:179][0m |          -0.0435 |         234.5329 |           9.5409 |
[32m[20221208 14:00:29 @agent_ppo2.py:179][0m |          -0.0456 |         233.8548 |           9.5699 |
[32m[20221208 14:00:29 @agent_ppo2.py:179][0m |          -0.0465 |         233.3876 |           9.5879 |
[32m[20221208 14:00:29 @agent_ppo2.py:179][0m |          -0.0468 |         233.4712 |           9.6341 |
[32m[20221208 14:00:29 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 14:00:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 964.74
[32m[20221208 14:00:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.99
[32m[20221208 14:00:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 767.06
[32m[20221208 14:00:29 @agent_ppo2.py:137][0m Total time:      15.15 min
[32m[20221208 14:00:29 @agent_ppo2.py:139][0m 1236992 total steps have happened
[32m[20221208 14:00:29 @agent_ppo2.py:115][0m #------------------------ Iteration 604 --------------------------#
[32m[20221208 14:00:30 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:00:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |           0.0450 |         207.4738 |           9.3230 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |           0.0946 |         195.3379 |           8.5403 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |           0.0590 |         191.2995 |           8.1599 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |          -0.0119 |         190.3026 |           6.9734 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |          -0.0386 |         187.3663 |           6.5739 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |          -0.0522 |         186.3871 |           6.6691 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |          -0.0599 |         185.4012 |           6.7544 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |          -0.0649 |         184.7550 |           6.7994 |
[32m[20221208 14:00:30 @agent_ppo2.py:179][0m |          -0.0687 |         183.2370 |           6.8332 |
[32m[20221208 14:00:31 @agent_ppo2.py:179][0m |          -0.0707 |         182.5624 |           6.8570 |
[32m[20221208 14:00:31 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:00:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 629.50
[32m[20221208 14:00:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.27
[32m[20221208 14:00:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 670.20
[32m[20221208 14:00:31 @agent_ppo2.py:137][0m Total time:      15.18 min
[32m[20221208 14:00:31 @agent_ppo2.py:139][0m 1239040 total steps have happened
[32m[20221208 14:00:31 @agent_ppo2.py:115][0m #------------------------ Iteration 605 --------------------------#
[32m[20221208 14:00:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:31 @agent_ppo2.py:179][0m |           0.0222 |         239.4748 |           9.3918 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |           0.0039 |         232.7100 |           9.1036 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0202 |         227.3485 |           9.4178 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0292 |         225.7237 |           9.4793 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0349 |         223.9115 |           9.5809 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0376 |         221.8716 |           9.5712 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0419 |         221.0830 |           9.6382 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0398 |         220.9986 |           9.6520 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0416 |         219.3473 |           9.6594 |
[32m[20221208 14:00:32 @agent_ppo2.py:179][0m |          -0.0451 |         218.1230 |           9.6908 |
[32m[20221208 14:00:32 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:00:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.85
[32m[20221208 14:00:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.68
[32m[20221208 14:00:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.03
[32m[20221208 14:00:32 @agent_ppo2.py:137][0m Total time:      15.20 min
[32m[20221208 14:00:32 @agent_ppo2.py:139][0m 1241088 total steps have happened
[32m[20221208 14:00:32 @agent_ppo2.py:115][0m #------------------------ Iteration 606 --------------------------#
[32m[20221208 14:00:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |           0.0349 |         238.2579 |           9.2886 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |           0.0142 |         233.5637 |           9.1229 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |          -0.0196 |         232.3210 |           9.4314 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |          -0.0260 |         230.2754 |           9.5294 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |          -0.0326 |         228.5228 |           9.5949 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |          -0.0367 |         228.8605 |           9.6754 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |          -0.0382 |         227.6308 |           9.6872 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |          -0.0428 |         226.4207 |           9.7470 |
[32m[20221208 14:00:33 @agent_ppo2.py:179][0m |          -0.0432 |         225.9797 |           9.7663 |
[32m[20221208 14:00:34 @agent_ppo2.py:179][0m |          -0.0439 |         226.2221 |           9.7869 |
[32m[20221208 14:00:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:00:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 940.38
[32m[20221208 14:00:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.25
[32m[20221208 14:00:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.36
[32m[20221208 14:00:34 @agent_ppo2.py:137][0m Total time:      15.23 min
[32m[20221208 14:00:34 @agent_ppo2.py:139][0m 1243136 total steps have happened
[32m[20221208 14:00:34 @agent_ppo2.py:115][0m #------------------------ Iteration 607 --------------------------#
[32m[20221208 14:00:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:34 @agent_ppo2.py:179][0m |           0.0324 |         241.9020 |           9.5714 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |           0.0100 |         233.8887 |           9.5105 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0011 |         230.5018 |           9.6179 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0257 |         229.5740 |           9.8682 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0338 |         229.3667 |           9.9416 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0398 |         226.2654 |          10.0085 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0428 |         225.7763 |          10.0519 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0462 |         226.4150 |          10.0796 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0458 |         225.9858 |          10.1153 |
[32m[20221208 14:00:35 @agent_ppo2.py:179][0m |          -0.0489 |         225.6056 |          10.1352 |
[32m[20221208 14:00:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 889.61
[32m[20221208 14:00:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.76
[32m[20221208 14:00:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.02
[32m[20221208 14:00:35 @agent_ppo2.py:137][0m Total time:      15.25 min
[32m[20221208 14:00:35 @agent_ppo2.py:139][0m 1245184 total steps have happened
[32m[20221208 14:00:35 @agent_ppo2.py:115][0m #------------------------ Iteration 608 --------------------------#
[32m[20221208 14:00:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |           0.0219 |         239.2676 |           9.8643 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |           0.0185 |         229.8106 |           9.4016 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |          -0.0097 |         223.3042 |           9.7598 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |          -0.0247 |         218.7116 |          10.0055 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |          -0.0272 |         217.3775 |          10.0584 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |          -0.0309 |         214.4546 |          10.1425 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |          -0.0349 |         213.5436 |          10.1527 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |          -0.0363 |         215.3423 |          10.1979 |
[32m[20221208 14:00:36 @agent_ppo2.py:179][0m |          -0.0371 |         212.2431 |          10.2030 |
[32m[20221208 14:00:37 @agent_ppo2.py:179][0m |          -0.0371 |         210.9257 |          10.2136 |
[32m[20221208 14:00:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.62
[32m[20221208 14:00:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.77
[32m[20221208 14:00:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.84
[32m[20221208 14:00:37 @agent_ppo2.py:137][0m Total time:      15.28 min
[32m[20221208 14:00:37 @agent_ppo2.py:139][0m 1247232 total steps have happened
[32m[20221208 14:00:37 @agent_ppo2.py:115][0m #------------------------ Iteration 609 --------------------------#
[32m[20221208 14:00:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:37 @agent_ppo2.py:179][0m |           0.0427 |         247.8119 |           9.7018 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |           0.0080 |         242.0069 |           9.5895 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0106 |         242.2834 |           9.8018 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0302 |         239.8772 |          10.0004 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0338 |         238.2164 |          10.0252 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0383 |         237.9530 |          10.0826 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0412 |         236.8488 |          10.0773 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0447 |         237.6738 |          10.0988 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0465 |         235.7712 |          10.1627 |
[32m[20221208 14:00:38 @agent_ppo2.py:179][0m |          -0.0493 |         236.1281 |          10.1503 |
[32m[20221208 14:00:38 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 886.19
[32m[20221208 14:00:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.78
[32m[20221208 14:00:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 916.29
[32m[20221208 14:00:38 @agent_ppo2.py:137][0m Total time:      15.30 min
[32m[20221208 14:00:38 @agent_ppo2.py:139][0m 1249280 total steps have happened
[32m[20221208 14:00:38 @agent_ppo2.py:115][0m #------------------------ Iteration 610 --------------------------#
[32m[20221208 14:00:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |           0.0345 |         241.0138 |          10.0336 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |           0.0194 |         232.1319 |           9.3187 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |          -0.0196 |         229.7530 |           9.2924 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |          -0.0324 |         226.3965 |           9.5267 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |          -0.0396 |         225.3337 |           9.5947 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |          -0.0455 |         224.8822 |           9.6816 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |          -0.0482 |         225.1906 |           9.6985 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |          -0.0487 |         224.5105 |           9.7428 |
[32m[20221208 14:00:39 @agent_ppo2.py:179][0m |          -0.0496 |         223.4745 |           9.7470 |
[32m[20221208 14:00:40 @agent_ppo2.py:179][0m |          -0.0492 |         223.4570 |           9.7346 |
[32m[20221208 14:00:40 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:00:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 806.39
[32m[20221208 14:00:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 904.13
[32m[20221208 14:00:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 963.65
[32m[20221208 14:00:40 @agent_ppo2.py:137][0m Total time:      15.33 min
[32m[20221208 14:00:40 @agent_ppo2.py:139][0m 1251328 total steps have happened
[32m[20221208 14:00:40 @agent_ppo2.py:115][0m #------------------------ Iteration 611 --------------------------#
[32m[20221208 14:00:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:40 @agent_ppo2.py:179][0m |           0.0450 |         234.5734 |           9.7654 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |           0.0076 |         225.9440 |           9.7672 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0189 |         223.6463 |          10.0151 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0259 |         222.0883 |          10.0571 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0289 |         221.3633 |          10.0681 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0369 |         220.3618 |          10.1715 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0426 |         219.7807 |          10.2217 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0430 |         219.3011 |          10.2474 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0443 |         218.3735 |          10.2766 |
[32m[20221208 14:00:41 @agent_ppo2.py:179][0m |          -0.0469 |         217.7818 |          10.3059 |
[32m[20221208 14:00:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:00:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.74
[32m[20221208 14:00:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.02
[32m[20221208 14:00:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.18
[32m[20221208 14:00:41 @agent_ppo2.py:137][0m Total time:      15.35 min
[32m[20221208 14:00:41 @agent_ppo2.py:139][0m 1253376 total steps have happened
[32m[20221208 14:00:41 @agent_ppo2.py:115][0m #------------------------ Iteration 612 --------------------------#
[32m[20221208 14:00:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |           0.0281 |         233.6844 |          10.3302 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |           0.0104 |         228.6987 |           9.9005 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0173 |         226.7512 |          10.2628 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0259 |         226.2138 |          10.3818 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0149 |         227.6537 |          10.2387 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0304 |         225.4534 |          10.4497 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0382 |         224.8778 |          10.5242 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0390 |         225.5979 |          10.5829 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0414 |         224.3038 |          10.6239 |
[32m[20221208 14:00:42 @agent_ppo2.py:179][0m |          -0.0408 |         226.1948 |          10.6317 |
[32m[20221208 14:00:42 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:00:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 931.85
[32m[20221208 14:00:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 943.97
[32m[20221208 14:00:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.66
[32m[20221208 14:00:43 @agent_ppo2.py:137][0m Total time:      15.38 min
[32m[20221208 14:00:43 @agent_ppo2.py:139][0m 1255424 total steps have happened
[32m[20221208 14:00:43 @agent_ppo2.py:115][0m #------------------------ Iteration 613 --------------------------#
[32m[20221208 14:00:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:43 @agent_ppo2.py:179][0m |           0.0283 |         232.5564 |           9.9888 |
[32m[20221208 14:00:43 @agent_ppo2.py:179][0m |           0.0289 |         224.3327 |           9.7597 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |           0.0499 |         221.7240 |           9.0040 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |          -0.0106 |         220.7023 |           8.8904 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |          -0.0274 |         219.1101 |           9.0806 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |          -0.0374 |         217.4921 |           9.2028 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |          -0.0396 |         216.1224 |           9.2262 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |          -0.0431 |         213.0729 |           9.2719 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |          -0.0442 |         212.0633 |           9.3163 |
[32m[20221208 14:00:44 @agent_ppo2.py:179][0m |          -0.0451 |         211.9818 |           9.3590 |
[32m[20221208 14:00:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 797.34
[32m[20221208 14:00:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 921.64
[32m[20221208 14:00:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 911.68
[32m[20221208 14:00:44 @agent_ppo2.py:137][0m Total time:      15.40 min
[32m[20221208 14:00:44 @agent_ppo2.py:139][0m 1257472 total steps have happened
[32m[20221208 14:00:44 @agent_ppo2.py:115][0m #------------------------ Iteration 614 --------------------------#
[32m[20221208 14:00:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |           0.0299 |         236.7969 |           9.7251 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |           0.0067 |         224.7385 |           9.6357 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0174 |         220.9714 |           9.9165 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0297 |         217.1490 |          10.0555 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0299 |         214.4256 |          10.1208 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0356 |         212.8873 |          10.1718 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0390 |         212.6016 |          10.2054 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0423 |         210.3061 |          10.2264 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0419 |         208.7393 |          10.2644 |
[32m[20221208 14:00:45 @agent_ppo2.py:179][0m |          -0.0391 |         209.7874 |          10.2552 |
[32m[20221208 14:00:45 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:00:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 966.50
[32m[20221208 14:00:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.32
[32m[20221208 14:00:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 915.07
[32m[20221208 14:00:46 @agent_ppo2.py:137][0m Total time:      15.43 min
[32m[20221208 14:00:46 @agent_ppo2.py:139][0m 1259520 total steps have happened
[32m[20221208 14:00:46 @agent_ppo2.py:115][0m #------------------------ Iteration 615 --------------------------#
[32m[20221208 14:00:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:46 @agent_ppo2.py:179][0m |           0.0319 |         241.7311 |           9.7929 |
[32m[20221208 14:00:46 @agent_ppo2.py:179][0m |           0.0197 |         237.9425 |           9.2748 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0095 |         234.5609 |           9.9247 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0149 |         233.1093 |          10.0956 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0214 |         231.4494 |          10.1477 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0266 |         230.1458 |          10.2443 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0287 |         230.4383 |          10.2755 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0331 |         229.0749 |          10.3626 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0364 |         227.1564 |          10.4232 |
[32m[20221208 14:00:47 @agent_ppo2.py:179][0m |          -0.0367 |         225.8256 |          10.4459 |
[32m[20221208 14:00:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 957.63
[32m[20221208 14:00:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.79
[32m[20221208 14:00:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 979.82
[32m[20221208 14:00:47 @agent_ppo2.py:137][0m Total time:      15.45 min
[32m[20221208 14:00:47 @agent_ppo2.py:139][0m 1261568 total steps have happened
[32m[20221208 14:00:47 @agent_ppo2.py:115][0m #------------------------ Iteration 616 --------------------------#
[32m[20221208 14:00:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |           0.0477 |         237.1928 |           9.6383 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |           0.0506 |         232.2526 |           8.9018 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |           0.0084 |         228.4125 |           9.2357 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |          -0.0164 |         224.4772 |           9.8376 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |          -0.0236 |         223.4385 |          10.0462 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |          -0.0292 |         220.9891 |          10.1089 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |          -0.0312 |         220.0289 |          10.1477 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |          -0.0359 |         219.3499 |          10.2195 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |          -0.0372 |         219.4092 |          10.2492 |
[32m[20221208 14:00:48 @agent_ppo2.py:179][0m |          -0.0377 |         217.7271 |          10.2612 |
[32m[20221208 14:00:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 975.94
[32m[20221208 14:00:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.10
[32m[20221208 14:00:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.34
[32m[20221208 14:00:49 @agent_ppo2.py:137][0m Total time:      15.47 min
[32m[20221208 14:00:49 @agent_ppo2.py:139][0m 1263616 total steps have happened
[32m[20221208 14:00:49 @agent_ppo2.py:115][0m #------------------------ Iteration 617 --------------------------#
[32m[20221208 14:00:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:49 @agent_ppo2.py:179][0m |           0.0315 |         239.5995 |          10.1406 |
[32m[20221208 14:00:49 @agent_ppo2.py:179][0m |           0.0184 |         234.2360 |           9.8135 |
[32m[20221208 14:00:49 @agent_ppo2.py:179][0m |          -0.0104 |         231.5353 |          10.1736 |
[32m[20221208 14:00:50 @agent_ppo2.py:179][0m |          -0.0235 |         227.6688 |          10.3036 |
[32m[20221208 14:00:50 @agent_ppo2.py:179][0m |          -0.0312 |         224.9133 |          10.4120 |
[32m[20221208 14:00:50 @agent_ppo2.py:179][0m |          -0.0329 |         222.0042 |          10.4284 |
[32m[20221208 14:00:50 @agent_ppo2.py:179][0m |          -0.0378 |         219.8291 |          10.4866 |
[32m[20221208 14:00:50 @agent_ppo2.py:179][0m |          -0.0365 |         216.7340 |          10.4737 |
[32m[20221208 14:00:50 @agent_ppo2.py:179][0m |          -0.0391 |         214.2684 |          10.5175 |
[32m[20221208 14:00:50 @agent_ppo2.py:179][0m |          -0.0426 |         211.3438 |          10.5282 |
[32m[20221208 14:00:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 948.89
[32m[20221208 14:00:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.75
[32m[20221208 14:00:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.27
[32m[20221208 14:00:50 @agent_ppo2.py:137][0m Total time:      15.50 min
[32m[20221208 14:00:50 @agent_ppo2.py:139][0m 1265664 total steps have happened
[32m[20221208 14:00:50 @agent_ppo2.py:115][0m #------------------------ Iteration 618 --------------------------#
[32m[20221208 14:00:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |           0.0373 |         231.7157 |          10.0308 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |           0.0101 |         209.3253 |           9.8298 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0133 |         192.2890 |          10.2737 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0203 |         181.2365 |          10.3247 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0273 |         175.9219 |          10.4132 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0323 |         173.6384 |          10.5225 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0376 |         172.4789 |          10.5619 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0405 |         170.2235 |          10.5781 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0427 |         168.9429 |          10.6199 |
[32m[20221208 14:00:51 @agent_ppo2.py:179][0m |          -0.0423 |         167.6654 |          10.6400 |
[32m[20221208 14:00:51 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 969.00
[32m[20221208 14:00:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.80
[32m[20221208 14:00:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 931.88
[32m[20221208 14:00:52 @agent_ppo2.py:137][0m Total time:      15.52 min
[32m[20221208 14:00:52 @agent_ppo2.py:139][0m 1267712 total steps have happened
[32m[20221208 14:00:52 @agent_ppo2.py:115][0m #------------------------ Iteration 619 --------------------------#
[32m[20221208 14:00:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:52 @agent_ppo2.py:179][0m |           0.0496 |         228.9833 |           9.7317 |
[32m[20221208 14:00:52 @agent_ppo2.py:179][0m |           0.0306 |         208.8456 |           9.5828 |
[32m[20221208 14:00:52 @agent_ppo2.py:179][0m |          -0.0084 |         200.8573 |          10.1578 |
[32m[20221208 14:00:53 @agent_ppo2.py:179][0m |          -0.0239 |         195.5265 |          10.4563 |
[32m[20221208 14:00:53 @agent_ppo2.py:179][0m |          -0.0335 |         192.7446 |          10.6169 |
[32m[20221208 14:00:53 @agent_ppo2.py:179][0m |          -0.0396 |         187.8296 |          10.6897 |
[32m[20221208 14:00:53 @agent_ppo2.py:179][0m |          -0.0436 |         185.6527 |          10.7305 |
[32m[20221208 14:00:53 @agent_ppo2.py:179][0m |          -0.0446 |         182.7295 |          10.7580 |
[32m[20221208 14:00:53 @agent_ppo2.py:179][0m |          -0.0476 |         181.5448 |          10.7674 |
[32m[20221208 14:00:53 @agent_ppo2.py:179][0m |          -0.0473 |         177.6436 |          10.8093 |
[32m[20221208 14:00:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:00:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.37
[32m[20221208 14:00:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.66
[32m[20221208 14:00:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.47
[32m[20221208 14:00:53 @agent_ppo2.py:137][0m Total time:      15.55 min
[32m[20221208 14:00:53 @agent_ppo2.py:139][0m 1269760 total steps have happened
[32m[20221208 14:00:53 @agent_ppo2.py:115][0m #------------------------ Iteration 620 --------------------------#
[32m[20221208 14:00:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |           0.0268 |         240.6506 |          10.5675 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |           0.0138 |         229.6174 |          10.2430 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0034 |         221.3324 |          10.4870 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0206 |         216.5099 |          10.6234 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0329 |         211.7262 |          10.7664 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0381 |         209.0757 |          10.8432 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0416 |         205.5558 |          10.9145 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0435 |         204.5909 |          10.9265 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0462 |         201.7789 |          10.9868 |
[32m[20221208 14:00:54 @agent_ppo2.py:179][0m |          -0.0453 |         200.3151 |          10.9756 |
[32m[20221208 14:00:54 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:00:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.06
[32m[20221208 14:00:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.46
[32m[20221208 14:00:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.41
[32m[20221208 14:00:55 @agent_ppo2.py:137][0m Total time:      15.57 min
[32m[20221208 14:00:55 @agent_ppo2.py:139][0m 1271808 total steps have happened
[32m[20221208 14:00:55 @agent_ppo2.py:115][0m #------------------------ Iteration 621 --------------------------#
[32m[20221208 14:00:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:00:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:55 @agent_ppo2.py:179][0m |           0.0614 |         250.7492 |          10.5154 |
[32m[20221208 14:00:55 @agent_ppo2.py:179][0m |           0.0379 |         246.6232 |           9.7912 |
[32m[20221208 14:00:55 @agent_ppo2.py:179][0m |           0.0006 |         243.6073 |          10.2313 |
[32m[20221208 14:00:56 @agent_ppo2.py:179][0m |          -0.0177 |         242.0715 |          10.5166 |
[32m[20221208 14:00:56 @agent_ppo2.py:179][0m |          -0.0256 |         241.5937 |          10.6390 |
[32m[20221208 14:00:56 @agent_ppo2.py:179][0m |          -0.0336 |         239.5423 |          10.7379 |
[32m[20221208 14:00:56 @agent_ppo2.py:179][0m |          -0.0377 |         239.1737 |          10.7864 |
[32m[20221208 14:00:56 @agent_ppo2.py:179][0m |          -0.0373 |         238.2636 |          10.8254 |
[32m[20221208 14:00:56 @agent_ppo2.py:179][0m |          -0.0407 |         237.5971 |          10.8627 |
[32m[20221208 14:00:56 @agent_ppo2.py:179][0m |          -0.0456 |         237.0196 |          10.9244 |
[32m[20221208 14:00:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:00:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 896.98
[32m[20221208 14:00:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.89
[32m[20221208 14:00:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 899.12
[32m[20221208 14:00:56 @agent_ppo2.py:137][0m Total time:      15.60 min
[32m[20221208 14:00:56 @agent_ppo2.py:139][0m 1273856 total steps have happened
[32m[20221208 14:00:56 @agent_ppo2.py:115][0m #------------------------ Iteration 622 --------------------------#
[32m[20221208 14:00:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |           0.0324 |         255.0779 |          10.6448 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |           0.0398 |         237.9026 |          10.0428 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |           0.0034 |         230.4016 |          10.2068 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |          -0.0191 |         223.4209 |          10.6306 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |          -0.0290 |         217.3738 |          10.7761 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |          -0.0319 |         208.6610 |          10.8253 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |          -0.0351 |         206.6322 |          10.8955 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |          -0.0351 |         200.9016 |          10.8922 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |          -0.0399 |         198.5495 |          10.9586 |
[32m[20221208 14:00:57 @agent_ppo2.py:179][0m |          -0.0424 |         197.7597 |          11.0259 |
[32m[20221208 14:00:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:00:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 948.94
[32m[20221208 14:00:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.89
[32m[20221208 14:00:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.04
[32m[20221208 14:00:58 @agent_ppo2.py:137][0m Total time:      15.62 min
[32m[20221208 14:00:58 @agent_ppo2.py:139][0m 1275904 total steps have happened
[32m[20221208 14:00:58 @agent_ppo2.py:115][0m #------------------------ Iteration 623 --------------------------#
[32m[20221208 14:00:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:00:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:00:58 @agent_ppo2.py:179][0m |           0.0331 |         262.6836 |          10.8161 |
[32m[20221208 14:00:58 @agent_ppo2.py:179][0m |           0.0091 |         252.0817 |          10.6814 |
[32m[20221208 14:00:58 @agent_ppo2.py:179][0m |          -0.0118 |         249.0534 |          10.9145 |
[32m[20221208 14:00:58 @agent_ppo2.py:179][0m |          -0.0294 |         248.5083 |          11.2063 |
[32m[20221208 14:00:59 @agent_ppo2.py:179][0m |          -0.0355 |         246.3326 |          11.2395 |
[32m[20221208 14:00:59 @agent_ppo2.py:179][0m |          -0.0410 |         245.5574 |          11.3186 |
[32m[20221208 14:00:59 @agent_ppo2.py:179][0m |          -0.0408 |         244.3501 |          11.3191 |
[32m[20221208 14:00:59 @agent_ppo2.py:179][0m |          -0.0415 |         244.5271 |          11.3320 |
[32m[20221208 14:00:59 @agent_ppo2.py:179][0m |          -0.0465 |         242.8228 |          11.4130 |
[32m[20221208 14:00:59 @agent_ppo2.py:179][0m |          -0.0470 |         241.9214 |          11.3978 |
[32m[20221208 14:00:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:00:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 927.20
[32m[20221208 14:00:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 948.82
[32m[20221208 14:00:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.21
[32m[20221208 14:00:59 @agent_ppo2.py:137][0m Total time:      15.65 min
[32m[20221208 14:00:59 @agent_ppo2.py:139][0m 1277952 total steps have happened
[32m[20221208 14:00:59 @agent_ppo2.py:115][0m #------------------------ Iteration 624 --------------------------#
[32m[20221208 14:01:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |           0.0403 |         253.5983 |          10.8568 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |           0.0360 |         245.8191 |          10.0514 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |           0.0047 |         242.0913 |          10.3966 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |          -0.0162 |         239.9704 |          10.6861 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |          -0.0257 |         239.1670 |          10.8631 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |          -0.0343 |         238.6745 |          10.9717 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |          -0.0384 |         238.0139 |          11.0180 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |          -0.0385 |         238.2930 |          11.0543 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |          -0.0398 |         237.7945 |          11.0814 |
[32m[20221208 14:01:00 @agent_ppo2.py:179][0m |          -0.0404 |         239.2011 |          11.0642 |
[32m[20221208 14:01:00 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:01:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.34
[32m[20221208 14:01:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 961.19
[32m[20221208 14:01:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 969.05
[32m[20221208 14:01:01 @agent_ppo2.py:137][0m Total time:      15.67 min
[32m[20221208 14:01:01 @agent_ppo2.py:139][0m 1280000 total steps have happened
[32m[20221208 14:01:01 @agent_ppo2.py:115][0m #------------------------ Iteration 625 --------------------------#
[32m[20221208 14:01:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:01 @agent_ppo2.py:179][0m |           0.0788 |         247.6235 |          10.6373 |
[32m[20221208 14:01:01 @agent_ppo2.py:179][0m |           0.0239 |         240.2948 |           9.9706 |
[32m[20221208 14:01:01 @agent_ppo2.py:179][0m |          -0.0099 |         235.4412 |          10.7369 |
[32m[20221208 14:01:01 @agent_ppo2.py:179][0m |          -0.0222 |         233.6976 |          10.8906 |
[32m[20221208 14:01:02 @agent_ppo2.py:179][0m |          -0.0286 |         230.6396 |          10.9606 |
[32m[20221208 14:01:02 @agent_ppo2.py:179][0m |          -0.0363 |         230.1357 |          11.0537 |
[32m[20221208 14:01:02 @agent_ppo2.py:179][0m |          -0.0348 |         227.9102 |          11.0563 |
[32m[20221208 14:01:02 @agent_ppo2.py:179][0m |          -0.0377 |         226.4878 |          11.1183 |
[32m[20221208 14:01:02 @agent_ppo2.py:179][0m |          -0.0408 |         225.3804 |          11.1028 |
[32m[20221208 14:01:02 @agent_ppo2.py:179][0m |          -0.0420 |         226.1425 |          11.1612 |
[32m[20221208 14:01:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 962.07
[32m[20221208 14:01:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.20
[32m[20221208 14:01:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.72
[32m[20221208 14:01:02 @agent_ppo2.py:137][0m Total time:      15.70 min
[32m[20221208 14:01:02 @agent_ppo2.py:139][0m 1282048 total steps have happened
[32m[20221208 14:01:02 @agent_ppo2.py:115][0m #------------------------ Iteration 626 --------------------------#
[32m[20221208 14:01:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:01:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |           0.0418 |         246.9116 |          10.7088 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |           0.0156 |         231.8698 |          10.0289 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0135 |         225.0805 |          10.9121 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0266 |         215.6485 |          11.0999 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0332 |         213.4185 |          11.2259 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0368 |         211.1129 |          11.2821 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0380 |         211.3938 |          11.3152 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0400 |         208.4109 |          11.3450 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0395 |         207.6598 |          11.3158 |
[32m[20221208 14:01:03 @agent_ppo2.py:179][0m |          -0.0414 |         207.9189 |          11.3870 |
[32m[20221208 14:01:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 941.78
[32m[20221208 14:01:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.61
[32m[20221208 14:01:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 917.28
[32m[20221208 14:01:04 @agent_ppo2.py:137][0m Total time:      15.72 min
[32m[20221208 14:01:04 @agent_ppo2.py:139][0m 1284096 total steps have happened
[32m[20221208 14:01:04 @agent_ppo2.py:115][0m #------------------------ Iteration 627 --------------------------#
[32m[20221208 14:01:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:01:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:04 @agent_ppo2.py:179][0m |           0.0266 |         247.7226 |          10.9969 |
[32m[20221208 14:01:04 @agent_ppo2.py:179][0m |           0.0147 |         237.4023 |          10.4561 |
[32m[20221208 14:01:04 @agent_ppo2.py:179][0m |          -0.0088 |         234.6642 |          10.9378 |
[32m[20221208 14:01:04 @agent_ppo2.py:179][0m |          -0.0258 |         232.1876 |          11.1262 |
[32m[20221208 14:01:04 @agent_ppo2.py:179][0m |          -0.0358 |         229.1785 |          11.2592 |
[32m[20221208 14:01:05 @agent_ppo2.py:179][0m |          -0.0389 |         228.0833 |          11.2754 |
[32m[20221208 14:01:05 @agent_ppo2.py:179][0m |          -0.0430 |         226.5028 |          11.3555 |
[32m[20221208 14:01:05 @agent_ppo2.py:179][0m |          -0.0452 |         226.8053 |          11.3695 |
[32m[20221208 14:01:05 @agent_ppo2.py:179][0m |          -0.0452 |         223.9256 |          11.3985 |
[32m[20221208 14:01:05 @agent_ppo2.py:179][0m |          -0.0452 |         222.4500 |          11.3839 |
[32m[20221208 14:01:05 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 911.80
[32m[20221208 14:01:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.23
[32m[20221208 14:01:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.64
[32m[20221208 14:01:05 @agent_ppo2.py:137][0m Total time:      15.75 min
[32m[20221208 14:01:05 @agent_ppo2.py:139][0m 1286144 total steps have happened
[32m[20221208 14:01:05 @agent_ppo2.py:115][0m #------------------------ Iteration 628 --------------------------#
[32m[20221208 14:01:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |           0.0303 |         240.3895 |          10.9367 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |           0.0520 |         234.9245 |           9.6456 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |           0.0094 |         233.0569 |          10.4788 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |          -0.0175 |         230.8723 |          10.9844 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |          -0.0254 |         229.5504 |          11.2183 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |          -0.0286 |         227.8772 |          11.2830 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |          -0.0346 |         226.9453 |          11.3532 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |          -0.0350 |         225.7587 |          11.3691 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |          -0.0386 |         224.9283 |          11.4128 |
[32m[20221208 14:01:06 @agent_ppo2.py:179][0m |          -0.0393 |         224.4171 |          11.4589 |
[32m[20221208 14:01:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 961.91
[32m[20221208 14:01:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.02
[32m[20221208 14:01:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.31
[32m[20221208 14:01:07 @agent_ppo2.py:137][0m Total time:      15.77 min
[32m[20221208 14:01:07 @agent_ppo2.py:139][0m 1288192 total steps have happened
[32m[20221208 14:01:07 @agent_ppo2.py:115][0m #------------------------ Iteration 629 --------------------------#
[32m[20221208 14:01:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:07 @agent_ppo2.py:179][0m |           0.0473 |         242.8830 |          10.9425 |
[32m[20221208 14:01:07 @agent_ppo2.py:179][0m |           0.0288 |         220.5633 |          10.3564 |
[32m[20221208 14:01:07 @agent_ppo2.py:179][0m |          -0.0099 |         211.0495 |          10.7437 |
[32m[20221208 14:01:07 @agent_ppo2.py:179][0m |          -0.0285 |         205.9876 |          11.0028 |
[32m[20221208 14:01:07 @agent_ppo2.py:179][0m |          -0.0368 |         203.3819 |          11.1223 |
[32m[20221208 14:01:08 @agent_ppo2.py:179][0m |          -0.0424 |         199.7179 |          11.2135 |
[32m[20221208 14:01:08 @agent_ppo2.py:179][0m |          -0.0420 |         196.4822 |          11.2591 |
[32m[20221208 14:01:08 @agent_ppo2.py:179][0m |          -0.0456 |         194.0483 |          11.2534 |
[32m[20221208 14:01:08 @agent_ppo2.py:179][0m |          -0.0493 |         193.7982 |          11.2957 |
[32m[20221208 14:01:08 @agent_ppo2.py:179][0m |          -0.0505 |         191.2169 |          11.3182 |
[32m[20221208 14:01:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 854.04
[32m[20221208 14:01:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 912.60
[32m[20221208 14:01:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.20
[32m[20221208 14:01:08 @agent_ppo2.py:137][0m Total time:      15.80 min
[32m[20221208 14:01:08 @agent_ppo2.py:139][0m 1290240 total steps have happened
[32m[20221208 14:01:08 @agent_ppo2.py:115][0m #------------------------ Iteration 630 --------------------------#
[32m[20221208 14:01:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |           0.0365 |         226.1609 |          10.8387 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0018 |         217.8112 |          10.8762 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0236 |         209.1946 |          11.1777 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0327 |         201.1733 |          11.2810 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0351 |         199.4110 |          11.3052 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0400 |         198.1050 |          11.3769 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0430 |         194.0214 |          11.4032 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0427 |         194.1364 |          11.4354 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0457 |         190.3918 |          11.4483 |
[32m[20221208 14:01:09 @agent_ppo2.py:179][0m |          -0.0478 |         188.7858 |          11.4699 |
[32m[20221208 14:01:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.01
[32m[20221208 14:01:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.42
[32m[20221208 14:01:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.90
[32m[20221208 14:01:10 @agent_ppo2.py:137][0m Total time:      15.82 min
[32m[20221208 14:01:10 @agent_ppo2.py:139][0m 1292288 total steps have happened
[32m[20221208 14:01:10 @agent_ppo2.py:115][0m #------------------------ Iteration 631 --------------------------#
[32m[20221208 14:01:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:10 @agent_ppo2.py:179][0m |           0.0313 |         248.5042 |          10.9921 |
[32m[20221208 14:01:10 @agent_ppo2.py:179][0m |           0.0123 |         241.0635 |          10.6909 |
[32m[20221208 14:01:10 @agent_ppo2.py:179][0m |          -0.0062 |         238.2570 |          11.0968 |
[32m[20221208 14:01:10 @agent_ppo2.py:179][0m |          -0.0232 |         237.1373 |          11.4065 |
[32m[20221208 14:01:10 @agent_ppo2.py:179][0m |          -0.0269 |         235.8453 |          11.4944 |
[32m[20221208 14:01:10 @agent_ppo2.py:179][0m |          -0.0307 |         234.0460 |          11.5236 |
[32m[20221208 14:01:11 @agent_ppo2.py:179][0m |          -0.0303 |         234.2922 |          11.6022 |
[32m[20221208 14:01:11 @agent_ppo2.py:179][0m |          -0.0348 |         233.9685 |          11.6469 |
[32m[20221208 14:01:11 @agent_ppo2.py:179][0m |          -0.0370 |         232.8693 |          11.6821 |
[32m[20221208 14:01:11 @agent_ppo2.py:179][0m |          -0.0382 |         231.8236 |          11.6728 |
[32m[20221208 14:01:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 966.15
[32m[20221208 14:01:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.15
[32m[20221208 14:01:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.36
[32m[20221208 14:01:11 @agent_ppo2.py:137][0m Total time:      15.85 min
[32m[20221208 14:01:11 @agent_ppo2.py:139][0m 1294336 total steps have happened
[32m[20221208 14:01:11 @agent_ppo2.py:115][0m #------------------------ Iteration 632 --------------------------#
[32m[20221208 14:01:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |           0.0324 |         246.0704 |          11.3640 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |           0.0228 |         238.4943 |          11.0268 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0101 |         235.9404 |          11.1993 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0235 |         230.9211 |          11.4577 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0318 |         225.0118 |          11.6266 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0355 |         219.9560 |          11.6615 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0396 |         215.7837 |          11.7430 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0405 |         213.4609 |          11.7364 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0417 |         211.9056 |          11.7710 |
[32m[20221208 14:01:12 @agent_ppo2.py:179][0m |          -0.0438 |         209.0412 |          11.8009 |
[32m[20221208 14:01:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 926.84
[32m[20221208 14:01:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.50
[32m[20221208 14:01:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.18
[32m[20221208 14:01:13 @agent_ppo2.py:137][0m Total time:      15.87 min
[32m[20221208 14:01:13 @agent_ppo2.py:139][0m 1296384 total steps have happened
[32m[20221208 14:01:13 @agent_ppo2.py:115][0m #------------------------ Iteration 633 --------------------------#
[32m[20221208 14:01:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:13 @agent_ppo2.py:179][0m |           0.0402 |         256.9240 |          11.4535 |
[32m[20221208 14:01:13 @agent_ppo2.py:179][0m |           0.0473 |         247.1844 |          10.7054 |
[32m[20221208 14:01:13 @agent_ppo2.py:179][0m |          -0.0012 |         244.2285 |          11.2002 |
[32m[20221208 14:01:13 @agent_ppo2.py:179][0m |          -0.0156 |         241.6566 |          11.4961 |
[32m[20221208 14:01:13 @agent_ppo2.py:179][0m |          -0.0277 |         241.4111 |          11.6433 |
[32m[20221208 14:01:13 @agent_ppo2.py:179][0m |          -0.0342 |         240.3917 |          11.7492 |
[32m[20221208 14:01:14 @agent_ppo2.py:179][0m |          -0.0371 |         240.2598 |          11.7915 |
[32m[20221208 14:01:14 @agent_ppo2.py:179][0m |          -0.0381 |         242.3219 |          11.7952 |
[32m[20221208 14:01:14 @agent_ppo2.py:179][0m |          -0.0405 |         240.0328 |          11.8349 |
[32m[20221208 14:01:14 @agent_ppo2.py:179][0m |          -0.0437 |         239.5451 |          11.8511 |
[32m[20221208 14:01:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.11
[32m[20221208 14:01:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.19
[32m[20221208 14:01:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 960.19
[32m[20221208 14:01:14 @agent_ppo2.py:137][0m Total time:      15.90 min
[32m[20221208 14:01:14 @agent_ppo2.py:139][0m 1298432 total steps have happened
[32m[20221208 14:01:14 @agent_ppo2.py:115][0m #------------------------ Iteration 634 --------------------------#
[32m[20221208 14:01:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |           0.0532 |         245.4364 |          11.4253 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |           0.0623 |         241.4013 |          10.4222 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |           0.0055 |         240.4439 |          11.0278 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |          -0.0120 |         239.2706 |          11.6602 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |          -0.0209 |         237.6887 |          11.9198 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |          -0.0244 |         237.0567 |          12.0347 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |          -0.0250 |         235.9196 |          12.0173 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |          -0.0296 |         234.5733 |          12.1185 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |          -0.0332 |         234.4171 |          12.1675 |
[32m[20221208 14:01:15 @agent_ppo2.py:179][0m |          -0.0330 |         233.8518 |          12.1598 |
[32m[20221208 14:01:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:01:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.27
[32m[20221208 14:01:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.18
[32m[20221208 14:01:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.63
[32m[20221208 14:01:16 @agent_ppo2.py:137][0m Total time:      15.92 min
[32m[20221208 14:01:16 @agent_ppo2.py:139][0m 1300480 total steps have happened
[32m[20221208 14:01:16 @agent_ppo2.py:115][0m #------------------------ Iteration 635 --------------------------#
[32m[20221208 14:01:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:16 @agent_ppo2.py:179][0m |           0.0380 |         220.6987 |          11.4776 |
[32m[20221208 14:01:16 @agent_ppo2.py:179][0m |           0.0196 |         193.7008 |          11.0213 |
[32m[20221208 14:01:16 @agent_ppo2.py:179][0m |          -0.0041 |         184.8570 |          11.4881 |
[32m[20221208 14:01:16 @agent_ppo2.py:179][0m |          -0.0196 |         181.1865 |          11.6266 |
[32m[20221208 14:01:16 @agent_ppo2.py:179][0m |          -0.0290 |         179.5672 |          11.8213 |
[32m[20221208 14:01:16 @agent_ppo2.py:179][0m |          -0.0361 |         179.4819 |          11.9434 |
[32m[20221208 14:01:17 @agent_ppo2.py:179][0m |          -0.0367 |         178.0922 |          11.9188 |
[32m[20221208 14:01:17 @agent_ppo2.py:179][0m |          -0.0395 |         177.4871 |          12.0020 |
[32m[20221208 14:01:17 @agent_ppo2.py:179][0m |          -0.0396 |         177.3852 |          12.0252 |
[32m[20221208 14:01:17 @agent_ppo2.py:179][0m |          -0.0411 |         177.1157 |          12.0288 |
[32m[20221208 14:01:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.21
[32m[20221208 14:01:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.93
[32m[20221208 14:01:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 964.37
[32m[20221208 14:01:17 @agent_ppo2.py:137][0m Total time:      15.95 min
[32m[20221208 14:01:17 @agent_ppo2.py:139][0m 1302528 total steps have happened
[32m[20221208 14:01:17 @agent_ppo2.py:115][0m #------------------------ Iteration 636 --------------------------#
[32m[20221208 14:01:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |           0.0417 |         266.1031 |          11.6481 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |           0.0290 |         252.7787 |          11.2713 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0063 |         249.3751 |          11.7659 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0220 |         248.5752 |          12.0474 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0315 |         247.0954 |          12.1106 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0340 |         246.4827 |          12.1889 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0357 |         247.3923 |          12.2553 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0407 |         245.8628 |          12.2936 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0415 |         245.8835 |          12.3064 |
[32m[20221208 14:01:18 @agent_ppo2.py:179][0m |          -0.0438 |         246.1092 |          12.2988 |
[32m[20221208 14:01:18 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:01:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.63
[32m[20221208 14:01:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.54
[32m[20221208 14:01:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.55
[32m[20221208 14:01:19 @agent_ppo2.py:137][0m Total time:      15.97 min
[32m[20221208 14:01:19 @agent_ppo2.py:139][0m 1304576 total steps have happened
[32m[20221208 14:01:19 @agent_ppo2.py:115][0m #------------------------ Iteration 637 --------------------------#
[32m[20221208 14:01:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:19 @agent_ppo2.py:179][0m |           0.0271 |         260.4926 |          11.8241 |
[32m[20221208 14:01:19 @agent_ppo2.py:179][0m |           0.0028 |         251.5017 |          11.3549 |
[32m[20221208 14:01:19 @agent_ppo2.py:179][0m |          -0.0181 |         249.3985 |          11.7658 |
[32m[20221208 14:01:19 @agent_ppo2.py:179][0m |          -0.0301 |         247.9633 |          11.9073 |
[32m[20221208 14:01:19 @agent_ppo2.py:179][0m |          -0.0348 |         247.9841 |          11.9666 |
[32m[20221208 14:01:19 @agent_ppo2.py:179][0m |          -0.0395 |         246.2434 |          12.0034 |
[32m[20221208 14:01:20 @agent_ppo2.py:179][0m |          -0.0387 |         244.3451 |          12.0033 |
[32m[20221208 14:01:20 @agent_ppo2.py:179][0m |          -0.0411 |         244.6711 |          12.0249 |
[32m[20221208 14:01:20 @agent_ppo2.py:179][0m |          -0.0430 |         242.8961 |          12.0559 |
[32m[20221208 14:01:20 @agent_ppo2.py:179][0m |          -0.0427 |         242.1295 |          12.0294 |
[32m[20221208 14:01:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.11
[32m[20221208 14:01:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.53
[32m[20221208 14:01:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.65
[32m[20221208 14:01:20 @agent_ppo2.py:137][0m Total time:      16.00 min
[32m[20221208 14:01:20 @agent_ppo2.py:139][0m 1306624 total steps have happened
[32m[20221208 14:01:20 @agent_ppo2.py:115][0m #------------------------ Iteration 638 --------------------------#
[32m[20221208 14:01:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:01:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |           0.0770 |         251.2062 |          11.6194 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |           0.0844 |         248.3693 |           9.7711 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |           0.0304 |         242.1802 |          10.3459 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |           0.0013 |         239.6688 |          11.6942 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |          -0.0153 |         237.8151 |          12.0840 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |          -0.0197 |         235.5445 |          12.2998 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |          -0.0274 |         234.9379 |          12.4043 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |          -0.0315 |         233.6822 |          12.4859 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |          -0.0329 |         232.9370 |          12.5028 |
[32m[20221208 14:01:21 @agent_ppo2.py:179][0m |          -0.0351 |         232.9301 |          12.5495 |
[32m[20221208 14:01:21 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:01:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 961.38
[32m[20221208 14:01:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.80
[32m[20221208 14:01:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 981.14
[32m[20221208 14:01:22 @agent_ppo2.py:137][0m Total time:      16.02 min
[32m[20221208 14:01:22 @agent_ppo2.py:139][0m 1308672 total steps have happened
[32m[20221208 14:01:22 @agent_ppo2.py:115][0m #------------------------ Iteration 639 --------------------------#
[32m[20221208 14:01:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:22 @agent_ppo2.py:179][0m |           0.0196 |         244.2154 |          11.9034 |
[32m[20221208 14:01:22 @agent_ppo2.py:179][0m |           0.0076 |         224.3484 |          11.4765 |
[32m[20221208 14:01:22 @agent_ppo2.py:179][0m |          -0.0156 |         213.9437 |          11.7108 |
[32m[20221208 14:01:22 @agent_ppo2.py:179][0m |          -0.0204 |         207.3324 |          11.8400 |
[32m[20221208 14:01:22 @agent_ppo2.py:179][0m |          -0.0275 |         200.2964 |          11.9538 |
[32m[20221208 14:01:22 @agent_ppo2.py:179][0m |          -0.0298 |         197.6012 |          12.0271 |
[32m[20221208 14:01:23 @agent_ppo2.py:179][0m |          -0.0296 |         194.1704 |          12.0440 |
[32m[20221208 14:01:23 @agent_ppo2.py:179][0m |          -0.0351 |         191.7218 |          12.1102 |
[32m[20221208 14:01:23 @agent_ppo2.py:179][0m |          -0.0397 |         188.4077 |          12.1995 |
[32m[20221208 14:01:23 @agent_ppo2.py:179][0m |          -0.0399 |         185.6646 |          12.1982 |
[32m[20221208 14:01:23 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:01:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 967.53
[32m[20221208 14:01:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.99
[32m[20221208 14:01:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.92
[32m[20221208 14:01:23 @agent_ppo2.py:137][0m Total time:      16.05 min
[32m[20221208 14:01:23 @agent_ppo2.py:139][0m 1310720 total steps have happened
[32m[20221208 14:01:23 @agent_ppo2.py:115][0m #------------------------ Iteration 640 --------------------------#
[32m[20221208 14:01:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:01:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |           0.0578 |         229.2731 |           9.3398 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0088 |         219.0877 |           7.7578 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0299 |         218.0855 |           8.0007 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0460 |         215.1608 |           8.1243 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0602 |         215.0226 |           8.2633 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0629 |         212.7004 |           8.2864 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0610 |         212.7553 |           8.2702 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0649 |         211.7991 |           8.2960 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0724 |         213.4285 |           8.3647 |
[32m[20221208 14:01:24 @agent_ppo2.py:179][0m |          -0.0739 |         211.4325 |           8.3785 |
[32m[20221208 14:01:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 650.63
[32m[20221208 14:01:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.40
[32m[20221208 14:01:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.76
[32m[20221208 14:01:25 @agent_ppo2.py:137][0m Total time:      16.07 min
[32m[20221208 14:01:25 @agent_ppo2.py:139][0m 1312768 total steps have happened
[32m[20221208 14:01:25 @agent_ppo2.py:115][0m #------------------------ Iteration 641 --------------------------#
[32m[20221208 14:01:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:01:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:25 @agent_ppo2.py:179][0m |           0.0583 |         294.3639 |          11.5924 |
[32m[20221208 14:01:25 @agent_ppo2.py:179][0m |           0.0111 |         264.0562 |          11.3996 |
[32m[20221208 14:01:25 @agent_ppo2.py:179][0m |          -0.0190 |         256.8066 |          12.0563 |
[32m[20221208 14:01:25 @agent_ppo2.py:179][0m |          -0.0306 |         253.5306 |          12.2044 |
[32m[20221208 14:01:25 @agent_ppo2.py:179][0m |          -0.0361 |         248.3676 |          12.2811 |
[32m[20221208 14:01:25 @agent_ppo2.py:179][0m |          -0.0419 |         245.8013 |          12.3717 |
[32m[20221208 14:01:26 @agent_ppo2.py:179][0m |          -0.0453 |         243.6643 |          12.4147 |
[32m[20221208 14:01:26 @agent_ppo2.py:179][0m |          -0.0474 |         241.3124 |          12.4198 |
[32m[20221208 14:01:26 @agent_ppo2.py:179][0m |          -0.0476 |         240.4748 |          12.4096 |
[32m[20221208 14:01:26 @agent_ppo2.py:179][0m |          -0.0478 |         237.6634 |          12.4117 |
[32m[20221208 14:01:26 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:01:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 972.35
[32m[20221208 14:01:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 995.15
[32m[20221208 14:01:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.44
[32m[20221208 14:01:26 @agent_ppo2.py:137][0m Total time:      16.10 min
[32m[20221208 14:01:26 @agent_ppo2.py:139][0m 1314816 total steps have happened
[32m[20221208 14:01:26 @agent_ppo2.py:115][0m #------------------------ Iteration 642 --------------------------#
[32m[20221208 14:01:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |           0.0250 |         261.4402 |          12.2023 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |           0.0353 |         250.1929 |          11.5202 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0047 |         247.1528 |          11.8534 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0272 |         244.5448 |          12.0807 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0347 |         245.5770 |          12.1806 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0410 |         243.2385 |          12.2326 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0418 |         242.8857 |          12.2952 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0458 |         242.3285 |          12.3085 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0470 |         241.0830 |          12.3182 |
[32m[20221208 14:01:27 @agent_ppo2.py:179][0m |          -0.0493 |         241.6021 |          12.3246 |
[32m[20221208 14:01:27 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:01:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 888.62
[32m[20221208 14:01:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.64
[32m[20221208 14:01:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.00
[32m[20221208 14:01:28 @agent_ppo2.py:137][0m Total time:      16.12 min
[32m[20221208 14:01:28 @agent_ppo2.py:139][0m 1316864 total steps have happened
[32m[20221208 14:01:28 @agent_ppo2.py:115][0m #------------------------ Iteration 643 --------------------------#
[32m[20221208 14:01:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:28 @agent_ppo2.py:179][0m |           0.0356 |         242.4766 |          11.7902 |
[32m[20221208 14:01:28 @agent_ppo2.py:179][0m |           0.0254 |         235.4160 |          11.2790 |
[32m[20221208 14:01:28 @agent_ppo2.py:179][0m |           0.0113 |         234.4613 |          11.5754 |
[32m[20221208 14:01:28 @agent_ppo2.py:179][0m |          -0.0125 |         232.3091 |          11.9558 |
[32m[20221208 14:01:28 @agent_ppo2.py:179][0m |          -0.0248 |         230.8933 |          12.1123 |
[32m[20221208 14:01:29 @agent_ppo2.py:179][0m |          -0.0326 |         229.9969 |          12.2310 |
[32m[20221208 14:01:29 @agent_ppo2.py:179][0m |          -0.0344 |         229.8553 |          12.2602 |
[32m[20221208 14:01:29 @agent_ppo2.py:179][0m |          -0.0375 |         229.4172 |          12.2736 |
[32m[20221208 14:01:29 @agent_ppo2.py:179][0m |          -0.0392 |         228.2668 |          12.3125 |
[32m[20221208 14:01:29 @agent_ppo2.py:179][0m |          -0.0415 |         229.2360 |          12.3490 |
[32m[20221208 14:01:29 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:01:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 951.39
[32m[20221208 14:01:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.20
[32m[20221208 14:01:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 641.59
[32m[20221208 14:01:29 @agent_ppo2.py:137][0m Total time:      16.15 min
[32m[20221208 14:01:29 @agent_ppo2.py:139][0m 1318912 total steps have happened
[32m[20221208 14:01:29 @agent_ppo2.py:115][0m #------------------------ Iteration 644 --------------------------#
[32m[20221208 14:01:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:01:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |           0.0447 |         246.8121 |          11.9046 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |           0.0204 |         244.8894 |          11.8972 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0099 |         244.0784 |          12.3873 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0206 |         241.0905 |          12.4583 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0261 |         240.4404 |          12.5388 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0302 |         239.6753 |          12.5815 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0322 |         239.6570 |          12.5999 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0339 |         239.9056 |          12.6024 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0393 |         239.4615 |          12.7016 |
[32m[20221208 14:01:30 @agent_ppo2.py:179][0m |          -0.0388 |         239.3359 |          12.7033 |
[32m[20221208 14:01:30 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:01:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.24
[32m[20221208 14:01:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.14
[32m[20221208 14:01:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 968.76
[32m[20221208 14:01:31 @agent_ppo2.py:137][0m Total time:      16.17 min
[32m[20221208 14:01:31 @agent_ppo2.py:139][0m 1320960 total steps have happened
[32m[20221208 14:01:31 @agent_ppo2.py:115][0m #------------------------ Iteration 645 --------------------------#
[32m[20221208 14:01:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:31 @agent_ppo2.py:179][0m |           0.0645 |         237.1798 |          11.9734 |
[32m[20221208 14:01:31 @agent_ppo2.py:179][0m |           0.0400 |         221.5603 |          11.5089 |
[32m[20221208 14:01:31 @agent_ppo2.py:179][0m |          -0.0044 |         213.8850 |          12.2638 |
[32m[20221208 14:01:31 @agent_ppo2.py:179][0m |          -0.0186 |         213.1816 |          12.3876 |
[32m[20221208 14:01:31 @agent_ppo2.py:179][0m |          -0.0284 |         210.9299 |          12.5191 |
[32m[20221208 14:01:32 @agent_ppo2.py:179][0m |          -0.0300 |         209.6247 |          12.5588 |
[32m[20221208 14:01:32 @agent_ppo2.py:179][0m |          -0.0304 |         207.3725 |          12.5206 |
[32m[20221208 14:01:32 @agent_ppo2.py:179][0m |          -0.0386 |         205.7005 |          12.6200 |
[32m[20221208 14:01:32 @agent_ppo2.py:179][0m |          -0.0410 |         205.7982 |          12.6479 |
[32m[20221208 14:01:32 @agent_ppo2.py:179][0m |          -0.0436 |         203.3824 |          12.6997 |
[32m[20221208 14:01:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 897.65
[32m[20221208 14:01:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.48
[32m[20221208 14:01:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.53
[32m[20221208 14:01:32 @agent_ppo2.py:137][0m Total time:      16.20 min
[32m[20221208 14:01:32 @agent_ppo2.py:139][0m 1323008 total steps have happened
[32m[20221208 14:01:32 @agent_ppo2.py:115][0m #------------------------ Iteration 646 --------------------------#
[32m[20221208 14:01:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |           0.0320 |         266.0658 |          12.0077 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0039 |         237.9469 |          11.9752 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0246 |         230.8401 |          12.0906 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0410 |         224.8024 |          12.1795 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0472 |         219.6934 |          12.2079 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0525 |         215.4291 |          12.2025 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0560 |         212.3413 |          12.2657 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0599 |         207.8584 |          12.2596 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0619 |         205.2261 |          12.2810 |
[32m[20221208 14:01:33 @agent_ppo2.py:179][0m |          -0.0622 |         204.5497 |          12.2789 |
[32m[20221208 14:01:33 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:01:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 779.35
[32m[20221208 14:01:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.80
[32m[20221208 14:01:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 973.62
[32m[20221208 14:01:34 @agent_ppo2.py:137][0m Total time:      16.22 min
[32m[20221208 14:01:34 @agent_ppo2.py:139][0m 1325056 total steps have happened
[32m[20221208 14:01:34 @agent_ppo2.py:115][0m #------------------------ Iteration 647 --------------------------#
[32m[20221208 14:01:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:34 @agent_ppo2.py:179][0m |           0.0439 |         252.4955 |          12.1988 |
[32m[20221208 14:01:34 @agent_ppo2.py:179][0m |           0.0101 |         249.4339 |          12.0190 |
[32m[20221208 14:01:34 @agent_ppo2.py:179][0m |          -0.0118 |         245.5537 |          12.3091 |
[32m[20221208 14:01:34 @agent_ppo2.py:179][0m |          -0.0246 |         244.5389 |          12.4552 |
[32m[20221208 14:01:34 @agent_ppo2.py:179][0m |          -0.0328 |         241.6252 |          12.5726 |
[32m[20221208 14:01:35 @agent_ppo2.py:179][0m |          -0.0385 |         241.2273 |          12.6362 |
[32m[20221208 14:01:35 @agent_ppo2.py:179][0m |          -0.0423 |         240.7406 |          12.6649 |
[32m[20221208 14:01:35 @agent_ppo2.py:179][0m |          -0.0447 |         240.0374 |          12.6810 |
[32m[20221208 14:01:35 @agent_ppo2.py:179][0m |          -0.0433 |         239.2889 |          12.6360 |
[32m[20221208 14:01:35 @agent_ppo2.py:179][0m |          -0.0452 |         240.1346 |          12.6451 |
[32m[20221208 14:01:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.46
[32m[20221208 14:01:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.86
[32m[20221208 14:01:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 893.41
[32m[20221208 14:01:35 @agent_ppo2.py:137][0m Total time:      16.25 min
[32m[20221208 14:01:35 @agent_ppo2.py:139][0m 1327104 total steps have happened
[32m[20221208 14:01:35 @agent_ppo2.py:115][0m #------------------------ Iteration 648 --------------------------#
[32m[20221208 14:01:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |           0.0279 |         243.4344 |          12.2393 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |           0.0401 |         240.1733 |          11.8827 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |           0.0272 |         236.9755 |          11.1824 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |          -0.0003 |         235.6245 |          11.7907 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |          -0.0192 |         237.1987 |          12.2956 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |          -0.0290 |         235.0273 |          12.5069 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |          -0.0335 |         233.2320 |          12.4823 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |          -0.0382 |         232.3656 |          12.5630 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |          -0.0400 |         231.5456 |          12.6329 |
[32m[20221208 14:01:36 @agent_ppo2.py:179][0m |          -0.0418 |         230.7943 |          12.6127 |
[32m[20221208 14:01:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 926.18
[32m[20221208 14:01:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.56
[32m[20221208 14:01:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 917.77
[32m[20221208 14:01:37 @agent_ppo2.py:137][0m Total time:      16.27 min
[32m[20221208 14:01:37 @agent_ppo2.py:139][0m 1329152 total steps have happened
[32m[20221208 14:01:37 @agent_ppo2.py:115][0m #------------------------ Iteration 649 --------------------------#
[32m[20221208 14:01:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:37 @agent_ppo2.py:179][0m |           0.0352 |         247.8312 |          12.3900 |
[32m[20221208 14:01:37 @agent_ppo2.py:179][0m |           0.0319 |         240.6792 |          11.4622 |
[32m[20221208 14:01:37 @agent_ppo2.py:179][0m |          -0.0002 |         237.4223 |          12.2053 |
[32m[20221208 14:01:37 @agent_ppo2.py:179][0m |          -0.0153 |         236.6094 |          12.3564 |
[32m[20221208 14:01:37 @agent_ppo2.py:179][0m |          -0.0251 |         231.6093 |          12.5653 |
[32m[20221208 14:01:38 @agent_ppo2.py:179][0m |          -0.0300 |         230.3308 |          12.6340 |
[32m[20221208 14:01:38 @agent_ppo2.py:179][0m |          -0.0352 |         228.2018 |          12.6761 |
[32m[20221208 14:01:38 @agent_ppo2.py:179][0m |          -0.0376 |         227.6656 |          12.7092 |
[32m[20221208 14:01:38 @agent_ppo2.py:179][0m |          -0.0393 |         227.3392 |          12.7371 |
[32m[20221208 14:01:38 @agent_ppo2.py:179][0m |          -0.0425 |         226.8510 |          12.7659 |
[32m[20221208 14:01:38 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:01:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.24
[32m[20221208 14:01:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.03
[32m[20221208 14:01:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.09
[32m[20221208 14:01:38 @agent_ppo2.py:137][0m Total time:      16.30 min
[32m[20221208 14:01:38 @agent_ppo2.py:139][0m 1331200 total steps have happened
[32m[20221208 14:01:38 @agent_ppo2.py:115][0m #------------------------ Iteration 650 --------------------------#
[32m[20221208 14:01:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |           0.0503 |         204.3302 |          10.5368 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0284 |         193.9662 |           8.7943 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0465 |         191.2715 |           9.0211 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0581 |         190.4664 |           9.0391 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0662 |         186.5227 |           9.1029 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0688 |         183.6522 |           9.1007 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0716 |         181.5617 |           9.1300 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0745 |         181.5481 |           9.1383 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0768 |         179.1910 |           9.1585 |
[32m[20221208 14:01:39 @agent_ppo2.py:179][0m |          -0.0785 |         178.4711 |           9.1532 |
[32m[20221208 14:01:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 667.09
[32m[20221208 14:01:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.07
[32m[20221208 14:01:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.77
[32m[20221208 14:01:40 @agent_ppo2.py:137][0m Total time:      16.32 min
[32m[20221208 14:01:40 @agent_ppo2.py:139][0m 1333248 total steps have happened
[32m[20221208 14:01:40 @agent_ppo2.py:115][0m #------------------------ Iteration 651 --------------------------#
[32m[20221208 14:01:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:40 @agent_ppo2.py:179][0m |           0.0859 |         239.9605 |          11.9164 |
[32m[20221208 14:01:40 @agent_ppo2.py:179][0m |           0.1074 |         233.9236 |           8.1861 |
[32m[20221208 14:01:40 @agent_ppo2.py:179][0m |           0.0467 |         229.7899 |           9.2560 |
[32m[20221208 14:01:40 @agent_ppo2.py:179][0m |           0.0139 |         228.0813 |          11.3649 |
[32m[20221208 14:01:40 @agent_ppo2.py:179][0m |          -0.0054 |         226.6553 |          12.1209 |
[32m[20221208 14:01:40 @agent_ppo2.py:179][0m |          -0.0149 |         225.5749 |          12.3473 |
[32m[20221208 14:01:41 @agent_ppo2.py:179][0m |          -0.0269 |         223.6951 |          12.6425 |
[32m[20221208 14:01:41 @agent_ppo2.py:179][0m |          -0.0344 |         223.0052 |          12.7174 |
[32m[20221208 14:01:41 @agent_ppo2.py:179][0m |          -0.0363 |         222.2459 |          12.7570 |
[32m[20221208 14:01:41 @agent_ppo2.py:179][0m |          -0.0414 |         221.8882 |          12.8069 |
[32m[20221208 14:01:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.63
[32m[20221208 14:01:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.72
[32m[20221208 14:01:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.25
[32m[20221208 14:01:41 @agent_ppo2.py:137][0m Total time:      16.35 min
[32m[20221208 14:01:41 @agent_ppo2.py:139][0m 1335296 total steps have happened
[32m[20221208 14:01:41 @agent_ppo2.py:115][0m #------------------------ Iteration 652 --------------------------#
[32m[20221208 14:01:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |           0.0339 |         242.5084 |          12.5740 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |           0.0159 |         236.9474 |          12.1124 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0080 |         234.5570 |          12.5041 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0231 |         233.2385 |          12.8159 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0276 |         234.1572 |          12.8587 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0302 |         231.9967 |          12.8847 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0302 |         231.9375 |          12.9402 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0380 |         231.5840 |          12.9816 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0385 |         230.5157 |          12.9920 |
[32m[20221208 14:01:42 @agent_ppo2.py:179][0m |          -0.0384 |         230.3760 |          12.9975 |
[32m[20221208 14:01:42 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:01:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 970.43
[32m[20221208 14:01:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.38
[32m[20221208 14:01:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.38
[32m[20221208 14:01:43 @agent_ppo2.py:137][0m Total time:      16.37 min
[32m[20221208 14:01:43 @agent_ppo2.py:139][0m 1337344 total steps have happened
[32m[20221208 14:01:43 @agent_ppo2.py:115][0m #------------------------ Iteration 653 --------------------------#
[32m[20221208 14:01:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:43 @agent_ppo2.py:179][0m |           0.0519 |         238.2396 |          12.7353 |
[32m[20221208 14:01:43 @agent_ppo2.py:179][0m |           0.0609 |         231.6976 |          11.0167 |
[32m[20221208 14:01:43 @agent_ppo2.py:179][0m |           0.0158 |         229.4804 |          12.0805 |
[32m[20221208 14:01:43 @agent_ppo2.py:179][0m |          -0.0135 |         226.2806 |          12.6987 |
[32m[20221208 14:01:43 @agent_ppo2.py:179][0m |          -0.0234 |         226.3386 |          12.8575 |
[32m[20221208 14:01:44 @agent_ppo2.py:179][0m |          -0.0303 |         224.8685 |          13.0000 |
[32m[20221208 14:01:44 @agent_ppo2.py:179][0m |          -0.0324 |         223.1566 |          13.0407 |
[32m[20221208 14:01:44 @agent_ppo2.py:179][0m |          -0.0349 |         223.9180 |          13.0716 |
[32m[20221208 14:01:44 @agent_ppo2.py:179][0m |          -0.0389 |         224.7600 |          13.1087 |
[32m[20221208 14:01:44 @agent_ppo2.py:179][0m |          -0.0403 |         222.9579 |          13.1065 |
[32m[20221208 14:01:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 948.88
[32m[20221208 14:01:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.04
[32m[20221208 14:01:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.67
[32m[20221208 14:01:44 @agent_ppo2.py:137][0m Total time:      16.40 min
[32m[20221208 14:01:44 @agent_ppo2.py:139][0m 1339392 total steps have happened
[32m[20221208 14:01:44 @agent_ppo2.py:115][0m #------------------------ Iteration 654 --------------------------#
[32m[20221208 14:01:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |           0.0288 |         247.7639 |          12.3366 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |           0.0246 |         239.3235 |          11.6954 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0049 |         236.7909 |          12.3039 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0179 |         234.3071 |          12.4355 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0300 |         232.5582 |          12.5586 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0363 |         233.7674 |          12.6335 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0379 |         230.9652 |          12.6459 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0394 |         230.1000 |          12.6350 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0417 |         229.4526 |          12.6952 |
[32m[20221208 14:01:45 @agent_ppo2.py:179][0m |          -0.0387 |         228.5856 |          12.5985 |
[32m[20221208 14:01:45 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 922.34
[32m[20221208 14:01:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 940.69
[32m[20221208 14:01:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 672.65
[32m[20221208 14:01:46 @agent_ppo2.py:137][0m Total time:      16.42 min
[32m[20221208 14:01:46 @agent_ppo2.py:139][0m 1341440 total steps have happened
[32m[20221208 14:01:46 @agent_ppo2.py:115][0m #------------------------ Iteration 655 --------------------------#
[32m[20221208 14:01:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:46 @agent_ppo2.py:179][0m |           0.0531 |         242.1946 |          12.1073 |
[32m[20221208 14:01:46 @agent_ppo2.py:179][0m |           0.0344 |         233.7113 |          11.8567 |
[32m[20221208 14:01:46 @agent_ppo2.py:179][0m |          -0.0008 |         231.6811 |          12.3629 |
[32m[20221208 14:01:46 @agent_ppo2.py:179][0m |          -0.0230 |         230.5374 |          12.7035 |
[32m[20221208 14:01:46 @agent_ppo2.py:179][0m |          -0.0285 |         228.7329 |          12.7679 |
[32m[20221208 14:01:46 @agent_ppo2.py:179][0m |          -0.0326 |         227.4667 |          12.8955 |
[32m[20221208 14:01:47 @agent_ppo2.py:179][0m |          -0.0332 |         226.6307 |          12.8526 |
[32m[20221208 14:01:47 @agent_ppo2.py:179][0m |          -0.0355 |         226.7994 |          12.8857 |
[32m[20221208 14:01:47 @agent_ppo2.py:179][0m |          -0.0383 |         223.9852 |          12.8825 |
[32m[20221208 14:01:47 @agent_ppo2.py:179][0m |          -0.0423 |         223.7542 |          12.9124 |
[32m[20221208 14:01:47 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:01:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 954.17
[32m[20221208 14:01:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.07
[32m[20221208 14:01:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 927.94
[32m[20221208 14:01:47 @agent_ppo2.py:137][0m Total time:      16.45 min
[32m[20221208 14:01:47 @agent_ppo2.py:139][0m 1343488 total steps have happened
[32m[20221208 14:01:47 @agent_ppo2.py:115][0m #------------------------ Iteration 656 --------------------------#
[32m[20221208 14:01:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:01:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |           0.0537 |         234.3161 |          12.2416 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |           0.0399 |         228.0394 |          11.7551 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |           0.0036 |         226.2760 |          12.6941 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |          -0.0153 |         226.7646 |          13.0027 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |          -0.0225 |         223.4371 |          13.1294 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |          -0.0283 |         221.4853 |          13.2143 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |          -0.0327 |         222.1627 |          13.2221 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |          -0.0355 |         219.8353 |          13.3017 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |          -0.0329 |         219.7929 |          13.2640 |
[32m[20221208 14:01:48 @agent_ppo2.py:179][0m |          -0.0334 |         221.0204 |          13.3042 |
[32m[20221208 14:01:48 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.72
[32m[20221208 14:01:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.09
[32m[20221208 14:01:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.50
[32m[20221208 14:01:49 @agent_ppo2.py:137][0m Total time:      16.47 min
[32m[20221208 14:01:49 @agent_ppo2.py:139][0m 1345536 total steps have happened
[32m[20221208 14:01:49 @agent_ppo2.py:115][0m #------------------------ Iteration 657 --------------------------#
[32m[20221208 14:01:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:49 @agent_ppo2.py:179][0m |           0.0458 |         247.1036 |          12.3133 |
[32m[20221208 14:01:49 @agent_ppo2.py:179][0m |           0.0282 |         244.0825 |          11.7544 |
[32m[20221208 14:01:49 @agent_ppo2.py:179][0m |          -0.0075 |         242.9917 |          12.7293 |
[32m[20221208 14:01:49 @agent_ppo2.py:179][0m |          -0.0182 |         242.6958 |          12.9463 |
[32m[20221208 14:01:49 @agent_ppo2.py:179][0m |          -0.0243 |         240.1627 |          12.8869 |
[32m[20221208 14:01:49 @agent_ppo2.py:179][0m |          -0.0291 |         240.0539 |          12.9509 |
[32m[20221208 14:01:50 @agent_ppo2.py:179][0m |          -0.0342 |         239.2804 |          13.0467 |
[32m[20221208 14:01:50 @agent_ppo2.py:179][0m |          -0.0333 |         239.0110 |          13.0337 |
[32m[20221208 14:01:50 @agent_ppo2.py:179][0m |          -0.0393 |         238.6249 |          13.0754 |
[32m[20221208 14:01:50 @agent_ppo2.py:179][0m |          -0.0396 |         239.5116 |          13.0592 |
[32m[20221208 14:01:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 974.51
[32m[20221208 14:01:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.34
[32m[20221208 14:01:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.10
[32m[20221208 14:01:50 @agent_ppo2.py:137][0m Total time:      16.50 min
[32m[20221208 14:01:50 @agent_ppo2.py:139][0m 1347584 total steps have happened
[32m[20221208 14:01:50 @agent_ppo2.py:115][0m #------------------------ Iteration 658 --------------------------#
[32m[20221208 14:01:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |           0.0285 |         246.1926 |          12.4527 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |           0.0216 |         239.8917 |          12.0228 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0131 |         236.6440 |          12.4389 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0278 |         236.3601 |          12.5713 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0376 |         233.9337 |          12.6323 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0431 |         234.0086 |          12.6843 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0432 |         233.2660 |          12.7184 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0440 |         232.6682 |          12.6784 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0482 |         233.6372 |          12.7317 |
[32m[20221208 14:01:51 @agent_ppo2.py:179][0m |          -0.0508 |         231.5653 |          12.6873 |
[32m[20221208 14:01:51 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:01:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 892.16
[32m[20221208 14:01:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.42
[32m[20221208 14:01:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 789.34
[32m[20221208 14:01:52 @agent_ppo2.py:137][0m Total time:      16.52 min
[32m[20221208 14:01:52 @agent_ppo2.py:139][0m 1349632 total steps have happened
[32m[20221208 14:01:52 @agent_ppo2.py:115][0m #------------------------ Iteration 659 --------------------------#
[32m[20221208 14:01:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:01:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:52 @agent_ppo2.py:179][0m |           0.0471 |         244.5324 |          12.5309 |
[32m[20221208 14:01:52 @agent_ppo2.py:179][0m |           0.0177 |         222.0819 |          12.2597 |
[32m[20221208 14:01:52 @agent_ppo2.py:179][0m |          -0.0143 |         209.3629 |          12.6599 |
[32m[20221208 14:01:52 @agent_ppo2.py:179][0m |          -0.0297 |         201.2416 |          12.7972 |
[32m[20221208 14:01:53 @agent_ppo2.py:179][0m |          -0.0359 |         197.4549 |          12.7872 |
[32m[20221208 14:01:53 @agent_ppo2.py:179][0m |          -0.0413 |         195.9186 |          12.8042 |
[32m[20221208 14:01:53 @agent_ppo2.py:179][0m |          -0.0422 |         194.3229 |          12.7976 |
[32m[20221208 14:01:53 @agent_ppo2.py:179][0m |          -0.0462 |         189.9922 |          12.7516 |
[32m[20221208 14:01:53 @agent_ppo2.py:179][0m |          -0.0482 |         186.0366 |          12.7998 |
[32m[20221208 14:01:53 @agent_ppo2.py:179][0m |          -0.0512 |         184.3769 |          12.7967 |
[32m[20221208 14:01:53 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:01:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 871.67
[32m[20221208 14:01:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.01
[32m[20221208 14:01:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 879.02
[32m[20221208 14:01:53 @agent_ppo2.py:137][0m Total time:      16.55 min
[32m[20221208 14:01:53 @agent_ppo2.py:139][0m 1351680 total steps have happened
[32m[20221208 14:01:53 @agent_ppo2.py:115][0m #------------------------ Iteration 660 --------------------------#
[32m[20221208 14:01:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |           0.0329 |         258.3988 |          12.8307 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |           0.0382 |         244.4433 |          11.7837 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |           0.0008 |         237.9358 |          12.4283 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |          -0.0236 |         234.9775 |          12.7404 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |          -0.0340 |         230.9778 |          12.8642 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |          -0.0429 |         227.9903 |          12.9516 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |          -0.0446 |         226.6357 |          12.9496 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |          -0.0506 |         225.6286 |          12.9985 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |          -0.0478 |         224.8583 |          12.9952 |
[32m[20221208 14:01:54 @agent_ppo2.py:179][0m |          -0.0468 |         223.3363 |          12.8923 |
[32m[20221208 14:01:54 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 861.77
[32m[20221208 14:01:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.07
[32m[20221208 14:01:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 971.45
[32m[20221208 14:01:55 @agent_ppo2.py:137][0m Total time:      16.57 min
[32m[20221208 14:01:55 @agent_ppo2.py:139][0m 1353728 total steps have happened
[32m[20221208 14:01:55 @agent_ppo2.py:115][0m #------------------------ Iteration 661 --------------------------#
[32m[20221208 14:01:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:01:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:55 @agent_ppo2.py:179][0m |           0.0337 |         248.2432 |          12.7631 |
[32m[20221208 14:01:55 @agent_ppo2.py:179][0m |           0.0162 |         238.4118 |          12.5380 |
[32m[20221208 14:01:55 @agent_ppo2.py:179][0m |          -0.0124 |         234.2490 |          12.9147 |
[32m[20221208 14:01:55 @agent_ppo2.py:179][0m |          -0.0202 |         233.3029 |          13.0505 |
[32m[20221208 14:01:56 @agent_ppo2.py:179][0m |          -0.0272 |         230.4340 |          13.0063 |
[32m[20221208 14:01:56 @agent_ppo2.py:179][0m |          -0.0341 |         229.9610 |          13.0021 |
[32m[20221208 14:01:56 @agent_ppo2.py:179][0m |          -0.0394 |         228.6943 |          13.0783 |
[32m[20221208 14:01:56 @agent_ppo2.py:179][0m |          -0.0436 |         228.8261 |          12.9956 |
[32m[20221208 14:01:56 @agent_ppo2.py:179][0m |          -0.0463 |         228.6645 |          13.0939 |
[32m[20221208 14:01:56 @agent_ppo2.py:179][0m |          -0.0466 |         228.8157 |          13.0536 |
[32m[20221208 14:01:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 965.32
[32m[20221208 14:01:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.37
[32m[20221208 14:01:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 660.95
[32m[20221208 14:01:56 @agent_ppo2.py:137][0m Total time:      16.60 min
[32m[20221208 14:01:56 @agent_ppo2.py:139][0m 1355776 total steps have happened
[32m[20221208 14:01:56 @agent_ppo2.py:115][0m #------------------------ Iteration 662 --------------------------#
[32m[20221208 14:01:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |           0.0698 |         206.1210 |          12.3704 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |           0.0296 |         178.1476 |           9.2493 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0271 |         170.8251 |           8.7441 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0406 |         166.5874 |           8.9269 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0557 |         162.6146 |           8.9966 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0593 |         158.6958 |           9.0656 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0629 |         156.4459 |           9.0510 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0643 |         155.0048 |           9.0601 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0717 |         153.1132 |           9.0934 |
[32m[20221208 14:01:57 @agent_ppo2.py:179][0m |          -0.0731 |         152.2208 |           9.0915 |
[32m[20221208 14:01:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:01:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 628.91
[32m[20221208 14:01:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 912.95
[32m[20221208 14:01:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 336.05
[32m[20221208 14:01:58 @agent_ppo2.py:137][0m Total time:      16.62 min
[32m[20221208 14:01:58 @agent_ppo2.py:139][0m 1357824 total steps have happened
[32m[20221208 14:01:58 @agent_ppo2.py:115][0m #------------------------ Iteration 663 --------------------------#
[32m[20221208 14:01:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:01:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:01:58 @agent_ppo2.py:179][0m |           0.0364 |         249.2999 |          12.7635 |
[32m[20221208 14:01:58 @agent_ppo2.py:179][0m |           0.0314 |         240.8043 |          12.3487 |
[32m[20221208 14:01:58 @agent_ppo2.py:179][0m |          -0.0028 |         238.3513 |          12.5417 |
[32m[20221208 14:01:58 @agent_ppo2.py:179][0m |          -0.0189 |         236.1442 |          12.8667 |
[32m[20221208 14:01:59 @agent_ppo2.py:179][0m |          -0.0301 |         236.9466 |          13.0018 |
[32m[20221208 14:01:59 @agent_ppo2.py:179][0m |          -0.0331 |         234.6628 |          13.0258 |
[32m[20221208 14:01:59 @agent_ppo2.py:179][0m |          -0.0368 |         233.9361 |          13.0712 |
[32m[20221208 14:01:59 @agent_ppo2.py:179][0m |          -0.0401 |         233.9702 |          13.0624 |
[32m[20221208 14:01:59 @agent_ppo2.py:179][0m |          -0.0403 |         233.5394 |          13.0603 |
[32m[20221208 14:01:59 @agent_ppo2.py:179][0m |          -0.0400 |         232.5363 |          13.0145 |
[32m[20221208 14:01:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:01:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.87
[32m[20221208 14:01:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.73
[32m[20221208 14:01:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 662.62
[32m[20221208 14:01:59 @agent_ppo2.py:137][0m Total time:      16.65 min
[32m[20221208 14:01:59 @agent_ppo2.py:139][0m 1359872 total steps have happened
[32m[20221208 14:01:59 @agent_ppo2.py:115][0m #------------------------ Iteration 664 --------------------------#
[32m[20221208 14:02:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |           0.0646 |         208.4843 |          10.5009 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0105 |         194.2994 |           8.5089 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0376 |         191.3522 |           8.6901 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0498 |         188.6185 |           8.7698 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0598 |         187.5441 |           8.7961 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0615 |         185.8415 |           8.7678 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0635 |         185.8059 |           8.7493 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0690 |         184.7180 |           8.7341 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0730 |         186.0685 |           8.7736 |
[32m[20221208 14:02:00 @agent_ppo2.py:179][0m |          -0.0742 |         183.5630 |           8.7397 |
[32m[20221208 14:02:00 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 588.62
[32m[20221208 14:02:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 938.28
[32m[20221208 14:02:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 662.55
[32m[20221208 14:02:01 @agent_ppo2.py:137][0m Total time:      16.67 min
[32m[20221208 14:02:01 @agent_ppo2.py:139][0m 1361920 total steps have happened
[32m[20221208 14:02:01 @agent_ppo2.py:115][0m #------------------------ Iteration 665 --------------------------#
[32m[20221208 14:02:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:01 @agent_ppo2.py:179][0m |           0.0303 |         244.7383 |          12.5587 |
[32m[20221208 14:02:01 @agent_ppo2.py:179][0m |           0.0390 |         236.3496 |          11.7360 |
[32m[20221208 14:02:01 @agent_ppo2.py:179][0m |          -0.0042 |         233.0069 |          12.1796 |
[32m[20221208 14:02:01 @agent_ppo2.py:179][0m |          -0.0221 |         230.5821 |          12.4885 |
[32m[20221208 14:02:01 @agent_ppo2.py:179][0m |          -0.0282 |         229.9442 |          12.6162 |
[32m[20221208 14:02:02 @agent_ppo2.py:179][0m |          -0.0335 |         228.8339 |          12.6270 |
[32m[20221208 14:02:02 @agent_ppo2.py:179][0m |          -0.0367 |         227.7446 |          12.6437 |
[32m[20221208 14:02:02 @agent_ppo2.py:179][0m |          -0.0388 |         226.8771 |          12.6495 |
[32m[20221208 14:02:02 @agent_ppo2.py:179][0m |          -0.0406 |         222.7471 |          12.6518 |
[32m[20221208 14:02:02 @agent_ppo2.py:179][0m |          -0.0436 |         222.7336 |          12.6461 |
[32m[20221208 14:02:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.36
[32m[20221208 14:02:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.70
[32m[20221208 14:02:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 68.36
[32m[20221208 14:02:02 @agent_ppo2.py:137][0m Total time:      16.70 min
[32m[20221208 14:02:02 @agent_ppo2.py:139][0m 1363968 total steps have happened
[32m[20221208 14:02:02 @agent_ppo2.py:115][0m #------------------------ Iteration 666 --------------------------#
[32m[20221208 14:02:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |           0.0452 |         157.1302 |          12.7547 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |           0.0530 |         144.0740 |           8.4110 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0303 |         142.0204 |           5.1629 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0458 |         141.0530 |           5.2051 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0609 |         140.3305 |           5.2326 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0675 |         140.8225 |           5.2226 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0727 |         139.6861 |           5.2219 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0713 |         139.9771 |           5.1528 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0769 |         138.7004 |           5.1259 |
[32m[20221208 14:02:03 @agent_ppo2.py:179][0m |          -0.0804 |         138.4118 |           5.1353 |
[32m[20221208 14:02:03 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 365.12
[32m[20221208 14:02:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.11
[32m[20221208 14:02:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 670.01
[32m[20221208 14:02:04 @agent_ppo2.py:137][0m Total time:      16.72 min
[32m[20221208 14:02:04 @agent_ppo2.py:139][0m 1366016 total steps have happened
[32m[20221208 14:02:04 @agent_ppo2.py:115][0m #------------------------ Iteration 667 --------------------------#
[32m[20221208 14:02:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:04 @agent_ppo2.py:179][0m |           0.0805 |          95.0676 |          12.0517 |
[32m[20221208 14:02:04 @agent_ppo2.py:179][0m |           0.1829 |          77.9721 |           3.3765 |
[32m[20221208 14:02:04 @agent_ppo2.py:179][0m |           0.1326 |          77.2098 |           2.4289 |
[32m[20221208 14:02:04 @agent_ppo2.py:179][0m |           0.1309 |          76.9272 |           5.5957 |
[32m[20221208 14:02:04 @agent_ppo2.py:179][0m |           0.1547 |          76.6618 |           3.3008 |
[32m[20221208 14:02:04 @agent_ppo2.py:179][0m |           0.1406 |          76.1035 |           0.8953 |
[32m[20221208 14:02:05 @agent_ppo2.py:179][0m |           0.1365 |          76.9639 |           1.1409 |
[32m[20221208 14:02:05 @agent_ppo2.py:179][0m |           0.1374 |          76.4196 |           1.5726 |
[32m[20221208 14:02:05 @agent_ppo2.py:179][0m |           0.1326 |          76.2835 |           1.2003 |
[32m[20221208 14:02:05 @agent_ppo2.py:179][0m |           0.1286 |          75.8128 |           1.6191 |
[32m[20221208 14:02:05 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:02:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 69.13
[32m[20221208 14:02:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 71.12
[32m[20221208 14:02:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.01
[32m[20221208 14:02:05 @agent_ppo2.py:137][0m Total time:      16.75 min
[32m[20221208 14:02:05 @agent_ppo2.py:139][0m 1368064 total steps have happened
[32m[20221208 14:02:05 @agent_ppo2.py:115][0m #------------------------ Iteration 668 --------------------------#
[32m[20221208 14:02:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |           0.0457 |         246.0135 |          12.2304 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |           0.0970 |         239.9542 |          11.0708 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |           0.0181 |         238.6887 |          11.5051 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |          -0.0063 |         236.1721 |          12.1781 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |          -0.0176 |         235.1523 |          12.3191 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |          -0.0242 |         234.4462 |          12.4146 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |          -0.0279 |         234.1240 |          12.5766 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |          -0.0338 |         234.3022 |          12.5740 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |          -0.0380 |         233.2556 |          12.6581 |
[32m[20221208 14:02:06 @agent_ppo2.py:179][0m |          -0.0403 |         232.8522 |          12.6399 |
[32m[20221208 14:02:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:02:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 954.07
[32m[20221208 14:02:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.66
[32m[20221208 14:02:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 84.27
[32m[20221208 14:02:07 @agent_ppo2.py:137][0m Total time:      16.77 min
[32m[20221208 14:02:07 @agent_ppo2.py:139][0m 1370112 total steps have happened
[32m[20221208 14:02:07 @agent_ppo2.py:115][0m #------------------------ Iteration 669 --------------------------#
[32m[20221208 14:02:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:07 @agent_ppo2.py:179][0m |           0.0233 |         244.5422 |          12.4983 |
[32m[20221208 14:02:07 @agent_ppo2.py:179][0m |           0.0047 |         238.1789 |          12.4425 |
[32m[20221208 14:02:07 @agent_ppo2.py:179][0m |          -0.0191 |         234.7500 |          12.5678 |
[32m[20221208 14:02:07 @agent_ppo2.py:179][0m |          -0.0269 |         234.3272 |          12.6934 |
[32m[20221208 14:02:07 @agent_ppo2.py:179][0m |          -0.0335 |         232.7336 |          12.8106 |
[32m[20221208 14:02:07 @agent_ppo2.py:179][0m |          -0.0370 |         230.9787 |          12.7790 |
[32m[20221208 14:02:07 @agent_ppo2.py:179][0m |          -0.0419 |         229.7371 |          12.8202 |
[32m[20221208 14:02:08 @agent_ppo2.py:179][0m |          -0.0456 |         229.5652 |          12.8362 |
[32m[20221208 14:02:08 @agent_ppo2.py:179][0m |          -0.0405 |         229.3154 |          12.7373 |
[32m[20221208 14:02:08 @agent_ppo2.py:179][0m |          -0.0469 |         229.0474 |          12.7748 |
[32m[20221208 14:02:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:02:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.66
[32m[20221208 14:02:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 989.03
[32m[20221208 14:02:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 974.63
[32m[20221208 14:02:08 @agent_ppo2.py:137][0m Total time:      16.80 min
[32m[20221208 14:02:08 @agent_ppo2.py:139][0m 1372160 total steps have happened
[32m[20221208 14:02:08 @agent_ppo2.py:115][0m #------------------------ Iteration 670 --------------------------#
[32m[20221208 14:02:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |           0.0488 |         212.6451 |          12.1404 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |           0.0469 |         193.1905 |          11.0686 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |           0.0107 |         189.6603 |          11.7416 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |          -0.0147 |         184.5907 |          12.1478 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |          -0.0250 |         181.2103 |          12.2407 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |          -0.0284 |         179.5553 |          12.3026 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |          -0.0344 |         178.1985 |          12.4016 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |          -0.0375 |         178.0188 |          12.3780 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |          -0.0372 |         176.1224 |          12.3149 |
[32m[20221208 14:02:09 @agent_ppo2.py:179][0m |          -0.0372 |         175.8836 |          12.2975 |
[32m[20221208 14:02:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 955.11
[32m[20221208 14:02:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.59
[32m[20221208 14:02:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 351.46
[32m[20221208 14:02:10 @agent_ppo2.py:137][0m Total time:      16.82 min
[32m[20221208 14:02:10 @agent_ppo2.py:139][0m 1374208 total steps have happened
[32m[20221208 14:02:10 @agent_ppo2.py:115][0m #------------------------ Iteration 671 --------------------------#
[32m[20221208 14:02:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:10 @agent_ppo2.py:179][0m |           0.0512 |         143.6298 |          12.5931 |
[32m[20221208 14:02:10 @agent_ppo2.py:179][0m |           0.0317 |         132.0440 |           6.1477 |
[32m[20221208 14:02:10 @agent_ppo2.py:179][0m |          -0.0177 |         129.9494 |           4.8038 |
[32m[20221208 14:02:10 @agent_ppo2.py:179][0m |          -0.0354 |         129.0306 |           5.0161 |
[32m[20221208 14:02:10 @agent_ppo2.py:179][0m |          -0.0434 |         127.9722 |           4.9973 |
[32m[20221208 14:02:10 @agent_ppo2.py:179][0m |          -0.0565 |         127.2414 |           5.0030 |
[32m[20221208 14:02:10 @agent_ppo2.py:179][0m |          -0.0622 |         126.6704 |           5.0216 |
[32m[20221208 14:02:11 @agent_ppo2.py:179][0m |          -0.0662 |         126.2401 |           5.0522 |
[32m[20221208 14:02:11 @agent_ppo2.py:179][0m |          -0.0701 |         125.8692 |           5.0143 |
[32m[20221208 14:02:11 @agent_ppo2.py:179][0m |          -0.0723 |         125.3741 |           4.9448 |
[32m[20221208 14:02:11 @agent_ppo2.py:124][0m Policy update time: 0.63 s
[32m[20221208 14:02:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 363.36
[32m[20221208 14:02:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 923.02
[32m[20221208 14:02:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.26
[32m[20221208 14:02:11 @agent_ppo2.py:137][0m Total time:      16.84 min
[32m[20221208 14:02:11 @agent_ppo2.py:139][0m 1376256 total steps have happened
[32m[20221208 14:02:11 @agent_ppo2.py:115][0m #------------------------ Iteration 672 --------------------------#
[32m[20221208 14:02:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |           0.0324 |         255.9370 |          12.3765 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |           0.0516 |         247.8558 |          11.8626 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |           0.0077 |         244.5891 |          12.1471 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |          -0.0195 |         242.5990 |          12.4376 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |          -0.0261 |         242.5794 |          12.5002 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |          -0.0357 |         240.3001 |          12.6015 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |          -0.0414 |         240.1585 |          12.6430 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |          -0.0442 |         239.2087 |          12.5867 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |          -0.0394 |         237.9033 |          12.5019 |
[32m[20221208 14:02:12 @agent_ppo2.py:179][0m |          -0.0473 |         237.7710 |          12.5162 |
[32m[20221208 14:02:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.55
[32m[20221208 14:02:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.53
[32m[20221208 14:02:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 902.13
[32m[20221208 14:02:13 @agent_ppo2.py:137][0m Total time:      16.87 min
[32m[20221208 14:02:13 @agent_ppo2.py:139][0m 1378304 total steps have happened
[32m[20221208 14:02:13 @agent_ppo2.py:115][0m #------------------------ Iteration 673 --------------------------#
[32m[20221208 14:02:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |           0.0479 |         244.5066 |          11.6740 |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |           0.0213 |         234.0983 |          11.9891 |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |           0.0025 |         230.1469 |          12.2315 |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |          -0.0071 |         228.5936 |          12.0652 |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |          -0.0267 |         227.1553 |          12.3657 |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |          -0.0333 |         227.1158 |          12.4283 |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |          -0.0389 |         225.7281 |          12.5537 |
[32m[20221208 14:02:13 @agent_ppo2.py:179][0m |          -0.0403 |         225.4528 |          12.5060 |
[32m[20221208 14:02:14 @agent_ppo2.py:179][0m |          -0.0432 |         225.8840 |          12.4947 |
[32m[20221208 14:02:14 @agent_ppo2.py:179][0m |          -0.0459 |         226.5478 |          12.5276 |
[32m[20221208 14:02:14 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.67
[32m[20221208 14:02:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.16
[32m[20221208 14:02:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 656.95
[32m[20221208 14:02:14 @agent_ppo2.py:137][0m Total time:      16.89 min
[32m[20221208 14:02:14 @agent_ppo2.py:139][0m 1380352 total steps have happened
[32m[20221208 14:02:14 @agent_ppo2.py:115][0m #------------------------ Iteration 674 --------------------------#
[32m[20221208 14:02:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |           0.0431 |         200.0468 |          12.2534 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |           0.0106 |         185.6528 |          12.2841 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |          -0.0084 |         182.4051 |          12.4131 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |           0.0588 |         181.3597 |          12.1502 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |           0.0179 |         180.4472 |          11.7030 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |           0.3143 |         179.9241 |          11.5263 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |           0.0237 |         178.4591 |          10.2209 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |          -0.0305 |         177.6405 |           8.9880 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |          -0.0429 |         177.8795 |           9.1895 |
[32m[20221208 14:02:15 @agent_ppo2.py:179][0m |          -0.0522 |         178.3687 |           9.2449 |
[32m[20221208 14:02:15 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 666.85
[32m[20221208 14:02:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.15
[32m[20221208 14:02:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.48
[32m[20221208 14:02:15 @agent_ppo2.py:137][0m Total time:      16.92 min
[32m[20221208 14:02:15 @agent_ppo2.py:139][0m 1382400 total steps have happened
[32m[20221208 14:02:15 @agent_ppo2.py:115][0m #------------------------ Iteration 675 --------------------------#
[32m[20221208 14:02:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |           0.0266 |         245.7942 |          12.1215 |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |           0.0164 |         236.2593 |          12.1865 |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |          -0.0200 |         231.5650 |          12.1356 |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |          -0.0317 |         232.2126 |          12.1947 |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |          -0.0393 |         228.5572 |          12.2733 |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |          -0.0408 |         227.8275 |          12.2650 |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |          -0.0384 |         227.3875 |          12.1824 |
[32m[20221208 14:02:16 @agent_ppo2.py:179][0m |          -0.0429 |         226.0915 |          12.1474 |
[32m[20221208 14:02:17 @agent_ppo2.py:179][0m |          -0.0452 |         225.8507 |          12.1062 |
[32m[20221208 14:02:17 @agent_ppo2.py:179][0m |          -0.0510 |         224.7092 |          12.1642 |
[32m[20221208 14:02:17 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 903.33
[32m[20221208 14:02:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.95
[32m[20221208 14:02:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.73
[32m[20221208 14:02:17 @agent_ppo2.py:137][0m Total time:      16.94 min
[32m[20221208 14:02:17 @agent_ppo2.py:139][0m 1384448 total steps have happened
[32m[20221208 14:02:17 @agent_ppo2.py:115][0m #------------------------ Iteration 676 --------------------------#
[32m[20221208 14:02:17 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |           0.0405 |         244.1409 |          12.2716 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |           0.0075 |         238.0356 |          12.1546 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0195 |         234.0333 |          12.4440 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0352 |         233.3598 |          12.6442 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0446 |         230.2598 |          12.6219 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0431 |         229.7885 |          12.6567 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0480 |         228.2685 |          12.6677 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0498 |         228.7796 |          12.5377 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0543 |         226.0017 |          12.5612 |
[32m[20221208 14:02:18 @agent_ppo2.py:179][0m |          -0.0539 |         225.5970 |          12.5411 |
[32m[20221208 14:02:18 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:02:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 882.60
[32m[20221208 14:02:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 902.89
[32m[20221208 14:02:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 897.18
[32m[20221208 14:02:19 @agent_ppo2.py:137][0m Total time:      16.97 min
[32m[20221208 14:02:19 @agent_ppo2.py:139][0m 1386496 total steps have happened
[32m[20221208 14:02:19 @agent_ppo2.py:115][0m #------------------------ Iteration 677 --------------------------#
[32m[20221208 14:02:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:19 @agent_ppo2.py:179][0m |           0.0412 |         241.5256 |          12.2641 |
[32m[20221208 14:02:19 @agent_ppo2.py:179][0m |           0.0049 |         230.7510 |          12.2381 |
[32m[20221208 14:02:19 @agent_ppo2.py:179][0m |          -0.0167 |         227.4791 |          12.2712 |
[32m[20221208 14:02:19 @agent_ppo2.py:179][0m |          -0.0262 |         224.8029 |          12.2764 |
[32m[20221208 14:02:19 @agent_ppo2.py:179][0m |          -0.0367 |         223.4804 |          12.2827 |
[32m[20221208 14:02:19 @agent_ppo2.py:179][0m |          -0.0387 |         222.1652 |          12.2448 |
[32m[20221208 14:02:19 @agent_ppo2.py:179][0m |          -0.0429 |         221.0258 |          12.1310 |
[32m[20221208 14:02:20 @agent_ppo2.py:179][0m |          -0.0446 |         220.0927 |          12.1548 |
[32m[20221208 14:02:20 @agent_ppo2.py:179][0m |          -0.0468 |         217.9699 |          12.1724 |
[32m[20221208 14:02:20 @agent_ppo2.py:179][0m |          -0.0475 |         217.2243 |          11.9969 |
[32m[20221208 14:02:20 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:02:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 763.81
[32m[20221208 14:02:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.18
[32m[20221208 14:02:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.33
[32m[20221208 14:02:20 @agent_ppo2.py:137][0m Total time:      17.00 min
[32m[20221208 14:02:20 @agent_ppo2.py:139][0m 1388544 total steps have happened
[32m[20221208 14:02:20 @agent_ppo2.py:115][0m #------------------------ Iteration 678 --------------------------#
[32m[20221208 14:02:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:02:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |           0.0563 |         252.4741 |          12.1376 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |           0.0398 |         226.7565 |          11.6118 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0140 |         218.0303 |          12.2510 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0331 |         214.7786 |          12.2967 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0428 |         213.5856 |          12.2770 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0481 |         211.4549 |          12.2507 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0510 |         211.2515 |          12.1278 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0564 |         210.3484 |          12.0807 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0588 |         208.5712 |          11.9931 |
[32m[20221208 14:02:21 @agent_ppo2.py:179][0m |          -0.0599 |         207.3908 |          11.8270 |
[32m[20221208 14:02:21 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:02:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 872.10
[32m[20221208 14:02:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.83
[32m[20221208 14:02:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.90
[32m[20221208 14:02:22 @agent_ppo2.py:137][0m Total time:      17.02 min
[32m[20221208 14:02:22 @agent_ppo2.py:139][0m 1390592 total steps have happened
[32m[20221208 14:02:22 @agent_ppo2.py:115][0m #------------------------ Iteration 679 --------------------------#
[32m[20221208 14:02:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:22 @agent_ppo2.py:179][0m |           0.0476 |         191.6586 |          11.9422 |
[32m[20221208 14:02:22 @agent_ppo2.py:179][0m |           0.0466 |         180.2613 |          11.2406 |
[32m[20221208 14:02:22 @agent_ppo2.py:179][0m |           0.0374 |         175.7083 |           8.9954 |
[32m[20221208 14:02:22 @agent_ppo2.py:179][0m |          -0.0320 |         174.7549 |           8.5001 |
[32m[20221208 14:02:22 @agent_ppo2.py:179][0m |          -0.0541 |         171.2319 |           8.5803 |
[32m[20221208 14:02:23 @agent_ppo2.py:179][0m |          -0.0587 |         170.1536 |           8.5690 |
[32m[20221208 14:02:23 @agent_ppo2.py:179][0m |          -0.0649 |         168.9221 |           8.4785 |
[32m[20221208 14:02:23 @agent_ppo2.py:179][0m |          -0.0668 |         168.2455 |           8.3716 |
[32m[20221208 14:02:23 @agent_ppo2.py:179][0m |          -0.0690 |         167.5754 |           8.3276 |
[32m[20221208 14:02:23 @agent_ppo2.py:179][0m |          -0.0710 |         165.9663 |           8.3128 |
[32m[20221208 14:02:23 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:02:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 649.87
[32m[20221208 14:02:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 937.21
[32m[20221208 14:02:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 605.12
[32m[20221208 14:02:23 @agent_ppo2.py:137][0m Total time:      17.05 min
[32m[20221208 14:02:23 @agent_ppo2.py:139][0m 1392640 total steps have happened
[32m[20221208 14:02:23 @agent_ppo2.py:115][0m #------------------------ Iteration 680 --------------------------#
[32m[20221208 14:02:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |           0.0569 |         230.8865 |          11.3818 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |           0.0960 |         225.5110 |          10.9813 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |           0.0497 |         223.5530 |          10.9546 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |          -0.0016 |         222.7192 |          11.5537 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |          -0.0175 |         222.3301 |          11.7171 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |          -0.0226 |         222.6263 |          11.8413 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |          -0.0296 |         221.9210 |          11.8334 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |          -0.0357 |         221.0760 |          11.9414 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |          -0.0342 |         221.3394 |          11.7581 |
[32m[20221208 14:02:24 @agent_ppo2.py:179][0m |          -0.0411 |         220.8369 |          11.8674 |
[32m[20221208 14:02:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.46
[32m[20221208 14:02:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.23
[32m[20221208 14:02:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 978.98
[32m[20221208 14:02:25 @agent_ppo2.py:137][0m Total time:      17.07 min
[32m[20221208 14:02:25 @agent_ppo2.py:139][0m 1394688 total steps have happened
[32m[20221208 14:02:25 @agent_ppo2.py:115][0m #------------------------ Iteration 681 --------------------------#
[32m[20221208 14:02:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:02:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:25 @agent_ppo2.py:179][0m |           0.0455 |         226.2814 |          12.1481 |
[32m[20221208 14:02:25 @agent_ppo2.py:179][0m |           0.0069 |         219.4180 |          12.3036 |
[32m[20221208 14:02:25 @agent_ppo2.py:179][0m |          -0.0183 |         217.2181 |          12.3776 |
[32m[20221208 14:02:25 @agent_ppo2.py:179][0m |          -0.0326 |         215.7374 |          12.5221 |
[32m[20221208 14:02:26 @agent_ppo2.py:179][0m |          -0.0365 |         215.9393 |          12.4536 |
[32m[20221208 14:02:26 @agent_ppo2.py:179][0m |          -0.0400 |         214.2889 |          12.4005 |
[32m[20221208 14:02:26 @agent_ppo2.py:179][0m |          -0.0431 |         214.3392 |          12.3603 |
[32m[20221208 14:02:26 @agent_ppo2.py:179][0m |          -0.0402 |         213.4510 |          12.2358 |
[32m[20221208 14:02:26 @agent_ppo2.py:179][0m |          -0.0484 |         214.6904 |          12.2312 |
[32m[20221208 14:02:26 @agent_ppo2.py:179][0m |          -0.0499 |         212.9904 |          12.1731 |
[32m[20221208 14:02:26 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:02:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.98
[32m[20221208 14:02:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 926.37
[32m[20221208 14:02:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 830.71
[32m[20221208 14:02:26 @agent_ppo2.py:137][0m Total time:      17.10 min
[32m[20221208 14:02:26 @agent_ppo2.py:139][0m 1396736 total steps have happened
[32m[20221208 14:02:26 @agent_ppo2.py:115][0m #------------------------ Iteration 682 --------------------------#
[32m[20221208 14:02:27 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:02:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:27 @agent_ppo2.py:179][0m |           0.0485 |         251.5980 |          11.5526 |
[32m[20221208 14:02:27 @agent_ppo2.py:179][0m |           0.0260 |         240.1475 |          11.6308 |
[32m[20221208 14:02:27 @agent_ppo2.py:179][0m |           0.0014 |         234.8655 |          11.8327 |
[32m[20221208 14:02:27 @agent_ppo2.py:179][0m |          -0.0218 |         230.3891 |          11.6049 |
[32m[20221208 14:02:27 @agent_ppo2.py:179][0m |          -0.0345 |         227.0125 |          11.7455 |
[32m[20221208 14:02:27 @agent_ppo2.py:179][0m |          -0.0368 |         225.3850 |          11.7488 |
[32m[20221208 14:02:27 @agent_ppo2.py:179][0m |          -0.0419 |         224.4032 |          11.7342 |
[32m[20221208 14:02:28 @agent_ppo2.py:179][0m |          -0.0410 |         223.1839 |          11.7142 |
[32m[20221208 14:02:28 @agent_ppo2.py:179][0m |          -0.0414 |         222.5225 |          11.6168 |
[32m[20221208 14:02:28 @agent_ppo2.py:179][0m |          -0.0503 |         220.1835 |          11.5696 |
[32m[20221208 14:02:28 @agent_ppo2.py:124][0m Policy update time: 0.91 s
[32m[20221208 14:02:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.72
[32m[20221208 14:02:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.56
[32m[20221208 14:02:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 751.80
[32m[20221208 14:02:28 @agent_ppo2.py:137][0m Total time:      17.13 min
[32m[20221208 14:02:28 @agent_ppo2.py:139][0m 1398784 total steps have happened
[32m[20221208 14:02:28 @agent_ppo2.py:115][0m #------------------------ Iteration 683 --------------------------#
[32m[20221208 14:02:29 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:02:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |           0.0422 |         229.8266 |          11.2622 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |           0.0068 |         219.2718 |          11.4541 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0139 |         213.0354 |          11.5499 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0258 |         205.6166 |          11.6028 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0314 |         201.5138 |          11.5460 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0375 |         198.8501 |          11.5016 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0440 |         195.5533 |          11.4724 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0431 |         193.9104 |          11.3145 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0390 |         192.4171 |          11.3899 |
[32m[20221208 14:02:29 @agent_ppo2.py:179][0m |          -0.0430 |         191.4770 |          11.1825 |
[32m[20221208 14:02:29 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:02:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.51
[32m[20221208 14:02:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.01
[32m[20221208 14:02:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 907.46
[32m[20221208 14:02:30 @agent_ppo2.py:137][0m Total time:      17.15 min
[32m[20221208 14:02:30 @agent_ppo2.py:139][0m 1400832 total steps have happened
[32m[20221208 14:02:30 @agent_ppo2.py:115][0m #------------------------ Iteration 684 --------------------------#
[32m[20221208 14:02:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:30 @agent_ppo2.py:179][0m |           0.0348 |         252.3614 |          11.8688 |
[32m[20221208 14:02:30 @agent_ppo2.py:179][0m |           0.0401 |         224.7908 |          11.3592 |
[32m[20221208 14:02:30 @agent_ppo2.py:179][0m |          -0.0037 |         218.4351 |          11.8629 |
[32m[20221208 14:02:30 @agent_ppo2.py:179][0m |          -0.0229 |         212.3664 |          11.9398 |
[32m[20221208 14:02:30 @agent_ppo2.py:179][0m |          -0.0297 |         207.0668 |          11.9379 |
[32m[20221208 14:02:31 @agent_ppo2.py:179][0m |          -0.0362 |         203.7397 |          11.8645 |
[32m[20221208 14:02:31 @agent_ppo2.py:179][0m |          -0.0400 |         199.0540 |          11.7668 |
[32m[20221208 14:02:31 @agent_ppo2.py:179][0m |          -0.0371 |         196.4212 |          11.6991 |
[32m[20221208 14:02:31 @agent_ppo2.py:179][0m |          -0.0407 |         194.6955 |          11.6428 |
[32m[20221208 14:02:31 @agent_ppo2.py:179][0m |          -0.0458 |         190.8596 |          11.6648 |
[32m[20221208 14:02:31 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 14:02:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 915.32
[32m[20221208 14:02:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.47
[32m[20221208 14:02:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.51
[32m[20221208 14:02:31 @agent_ppo2.py:137][0m Total time:      17.18 min
[32m[20221208 14:02:31 @agent_ppo2.py:139][0m 1402880 total steps have happened
[32m[20221208 14:02:31 @agent_ppo2.py:115][0m #------------------------ Iteration 685 --------------------------#
[32m[20221208 14:02:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |           0.1367 |         236.6207 |          10.1960 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |           0.0272 |         217.0702 |           7.3264 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0107 |         210.9943 |           7.5183 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0357 |         208.1001 |           7.7414 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0505 |         206.7246 |           7.7068 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0620 |         204.0844 |           7.7195 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0654 |         202.2279 |           7.7074 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0684 |         202.1072 |           7.6724 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0729 |         200.4929 |           7.5880 |
[32m[20221208 14:02:32 @agent_ppo2.py:179][0m |          -0.0739 |         199.8427 |           7.5678 |
[32m[20221208 14:02:32 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 618.77
[32m[20221208 14:02:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 890.22
[32m[20221208 14:02:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.27
[32m[20221208 14:02:33 @agent_ppo2.py:137][0m Total time:      17.21 min
[32m[20221208 14:02:33 @agent_ppo2.py:139][0m 1404928 total steps have happened
[32m[20221208 14:02:33 @agent_ppo2.py:115][0m #------------------------ Iteration 686 --------------------------#
[32m[20221208 14:02:33 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:02:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:33 @agent_ppo2.py:179][0m |           0.0263 |         232.2656 |          11.1463 |
[32m[20221208 14:02:33 @agent_ppo2.py:179][0m |           0.0050 |         209.0730 |          11.3124 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0064 |         195.8000 |          11.2156 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0298 |         188.2205 |          11.1150 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0389 |         184.0019 |          11.0442 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0461 |         178.9120 |          11.0287 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0523 |         176.0843 |          10.9125 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0510 |         173.2875 |          10.7865 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0531 |         171.5683 |          10.6024 |
[32m[20221208 14:02:34 @agent_ppo2.py:179][0m |          -0.0574 |         168.7426 |          10.4922 |
[32m[20221208 14:02:34 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:02:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 804.64
[32m[20221208 14:02:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.71
[32m[20221208 14:02:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 868.82
[32m[20221208 14:02:34 @agent_ppo2.py:137][0m Total time:      17.23 min
[32m[20221208 14:02:34 @agent_ppo2.py:139][0m 1406976 total steps have happened
[32m[20221208 14:02:34 @agent_ppo2.py:115][0m #------------------------ Iteration 687 --------------------------#
[32m[20221208 14:02:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |           0.0439 |         260.7051 |          10.4265 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |           0.0276 |         244.6949 |          10.4723 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0051 |         239.4569 |          10.6741 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0297 |         238.2140 |          10.3340 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0408 |         235.2299 |          10.4243 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0480 |         232.6278 |          10.2815 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0518 |         231.1152 |          10.1203 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0472 |         230.8135 |           9.9830 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0445 |         228.9472 |           9.8177 |
[32m[20221208 14:02:35 @agent_ppo2.py:179][0m |          -0.0550 |         228.3715 |           9.7861 |
[32m[20221208 14:02:35 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:02:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 910.00
[32m[20221208 14:02:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.30
[32m[20221208 14:02:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 868.23
[32m[20221208 14:02:36 @agent_ppo2.py:137][0m Total time:      17.26 min
[32m[20221208 14:02:36 @agent_ppo2.py:139][0m 1409024 total steps have happened
[32m[20221208 14:02:36 @agent_ppo2.py:115][0m #------------------------ Iteration 688 --------------------------#
[32m[20221208 14:02:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:36 @agent_ppo2.py:179][0m |           0.0442 |         251.1858 |           9.7880 |
[32m[20221208 14:02:36 @agent_ppo2.py:179][0m |           0.1195 |         240.9965 |           9.3173 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |           0.0485 |         235.0838 |           9.2289 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |           0.0114 |         232.8643 |           9.7373 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |          -0.0165 |         229.8739 |           9.9597 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |          -0.0235 |         228.0569 |           9.9505 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |          -0.0341 |         226.1141 |           9.9409 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |          -0.0379 |         225.6061 |           9.8043 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |          -0.0437 |         224.9348 |           9.6899 |
[32m[20221208 14:02:37 @agent_ppo2.py:179][0m |          -0.0483 |         223.8298 |           9.6774 |
[32m[20221208 14:02:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 858.69
[32m[20221208 14:02:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.23
[32m[20221208 14:02:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.54
[32m[20221208 14:02:37 @agent_ppo2.py:137][0m Total time:      17.28 min
[32m[20221208 14:02:37 @agent_ppo2.py:139][0m 1411072 total steps have happened
[32m[20221208 14:02:37 @agent_ppo2.py:115][0m #------------------------ Iteration 689 --------------------------#
[32m[20221208 14:02:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |           0.0580 |         229.4685 |           8.7786 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0141 |         215.5389 |           8.2817 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0383 |         206.3312 |           8.1336 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0485 |         200.6005 |           7.7773 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0592 |         196.5455 |           7.6767 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0644 |         192.8665 |           7.3784 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0684 |         190.7790 |           7.2757 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0693 |         188.7514 |           7.2461 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0742 |         186.9616 |           7.1010 |
[32m[20221208 14:02:38 @agent_ppo2.py:179][0m |          -0.0726 |         185.6680 |           6.9939 |
[32m[20221208 14:02:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:02:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 667.12
[32m[20221208 14:02:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 920.51
[32m[20221208 14:02:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 903.81
[32m[20221208 14:02:39 @agent_ppo2.py:137][0m Total time:      17.31 min
[32m[20221208 14:02:39 @agent_ppo2.py:139][0m 1413120 total steps have happened
[32m[20221208 14:02:39 @agent_ppo2.py:115][0m #------------------------ Iteration 690 --------------------------#
[32m[20221208 14:02:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:39 @agent_ppo2.py:179][0m |           0.0439 |         233.9371 |           8.8713 |
[32m[20221208 14:02:39 @agent_ppo2.py:179][0m |           0.0107 |         220.6757 |           8.7837 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0015 |         217.7372 |           8.7398 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0309 |         215.4960 |           8.5662 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0377 |         214.4030 |           8.4718 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0398 |         212.6576 |           8.4376 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0450 |         212.5274 |           8.3937 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0464 |         211.9026 |           8.4609 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0451 |         210.0987 |           8.4200 |
[32m[20221208 14:02:40 @agent_ppo2.py:179][0m |          -0.0448 |         209.4549 |           8.3840 |
[32m[20221208 14:02:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 885.62
[32m[20221208 14:02:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 944.77
[32m[20221208 14:02:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 912.17
[32m[20221208 14:02:40 @agent_ppo2.py:137][0m Total time:      17.33 min
[32m[20221208 14:02:40 @agent_ppo2.py:139][0m 1415168 total steps have happened
[32m[20221208 14:02:40 @agent_ppo2.py:115][0m #------------------------ Iteration 691 --------------------------#
[32m[20221208 14:02:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |           0.0739 |         243.5691 |           9.4112 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |           0.0730 |         235.0678 |           9.7900 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |           0.0110 |         231.8649 |          10.2113 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |          -0.0165 |         229.1946 |          10.0345 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |          -0.0309 |         228.5238 |           9.8365 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |          -0.0362 |         226.8381 |           9.7394 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |          -0.0434 |         224.0327 |           9.5893 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |          -0.0446 |         223.7980 |           9.4628 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |          -0.0437 |         222.4276 |           9.3946 |
[32m[20221208 14:02:41 @agent_ppo2.py:179][0m |          -0.0431 |         221.9954 |           9.1997 |
[32m[20221208 14:02:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.04
[32m[20221208 14:02:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.67
[32m[20221208 14:02:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.72
[32m[20221208 14:02:42 @agent_ppo2.py:137][0m Total time:      17.36 min
[32m[20221208 14:02:42 @agent_ppo2.py:139][0m 1417216 total steps have happened
[32m[20221208 14:02:42 @agent_ppo2.py:115][0m #------------------------ Iteration 692 --------------------------#
[32m[20221208 14:02:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:42 @agent_ppo2.py:179][0m |           0.0387 |         236.7987 |           9.3799 |
[32m[20221208 14:02:42 @agent_ppo2.py:179][0m |           0.0150 |         231.1392 |           9.0883 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0217 |         228.7649 |           9.6311 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0343 |         226.3375 |           9.6640 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0399 |         225.6077 |           9.5049 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0382 |         224.7306 |           9.4972 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0436 |         223.9436 |           9.4630 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0492 |         223.9579 |           9.3562 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0526 |         222.3230 |           9.2331 |
[32m[20221208 14:02:43 @agent_ppo2.py:179][0m |          -0.0542 |         222.4210 |           9.2447 |
[32m[20221208 14:02:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:02:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 842.32
[32m[20221208 14:02:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 900.67
[32m[20221208 14:02:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.06
[32m[20221208 14:02:43 @agent_ppo2.py:137][0m Total time:      17.38 min
[32m[20221208 14:02:43 @agent_ppo2.py:139][0m 1419264 total steps have happened
[32m[20221208 14:02:43 @agent_ppo2.py:115][0m #------------------------ Iteration 693 --------------------------#
[32m[20221208 14:02:44 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |           0.0356 |         232.8252 |           8.6365 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |           0.0210 |         212.6824 |           8.6262 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0011 |         204.9133 |           8.9302 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0188 |         199.4765 |           8.7293 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0279 |         197.5612 |           8.6587 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0310 |         196.0494 |           8.7046 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0373 |         195.3058 |           8.5834 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0382 |         194.0156 |           8.5027 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0411 |         192.8643 |           8.2344 |
[32m[20221208 14:02:44 @agent_ppo2.py:179][0m |          -0.0431 |         191.7461 |           8.3438 |
[32m[20221208 14:02:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.17
[32m[20221208 14:02:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.01
[32m[20221208 14:02:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 897.74
[32m[20221208 14:02:45 @agent_ppo2.py:137][0m Total time:      17.41 min
[32m[20221208 14:02:45 @agent_ppo2.py:139][0m 1421312 total steps have happened
[32m[20221208 14:02:45 @agent_ppo2.py:115][0m #------------------------ Iteration 694 --------------------------#
[32m[20221208 14:02:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:45 @agent_ppo2.py:179][0m |           0.0440 |         237.3285 |           9.5292 |
[32m[20221208 14:02:45 @agent_ppo2.py:179][0m |           0.0062 |         231.3823 |           9.4942 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0133 |         230.0277 |           9.8934 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0274 |         226.7124 |           9.8691 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0330 |         225.5102 |           9.9698 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0289 |         224.8673 |           9.6929 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0331 |         224.0705 |           9.6634 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0377 |         224.6197 |           9.7029 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0403 |         223.2141 |           9.6427 |
[32m[20221208 14:02:46 @agent_ppo2.py:179][0m |          -0.0408 |         223.6786 |           9.6427 |
[32m[20221208 14:02:46 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.84
[32m[20221208 14:02:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.03
[32m[20221208 14:02:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 926.34
[32m[20221208 14:02:46 @agent_ppo2.py:137][0m Total time:      17.43 min
[32m[20221208 14:02:46 @agent_ppo2.py:139][0m 1423360 total steps have happened
[32m[20221208 14:02:46 @agent_ppo2.py:115][0m #------------------------ Iteration 695 --------------------------#
[32m[20221208 14:02:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |           0.0990 |         232.3587 |           9.0765 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |           0.0653 |         225.1147 |           9.4756 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |           0.0048 |         222.4682 |           9.1279 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |          -0.0153 |         220.4042 |           9.2532 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |          -0.0296 |         220.2876 |           9.1023 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |          -0.0330 |         218.1174 |           9.2349 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |          -0.0342 |         218.5407 |           9.0528 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |          -0.0433 |         216.2973 |           8.9026 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |          -0.0443 |         215.9526 |           8.7661 |
[32m[20221208 14:02:47 @agent_ppo2.py:179][0m |          -0.0500 |         215.4238 |           8.7352 |
[32m[20221208 14:02:47 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 794.32
[32m[20221208 14:02:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 892.04
[32m[20221208 14:02:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 812.03
[32m[20221208 14:02:48 @agent_ppo2.py:137][0m Total time:      17.46 min
[32m[20221208 14:02:48 @agent_ppo2.py:139][0m 1425408 total steps have happened
[32m[20221208 14:02:48 @agent_ppo2.py:115][0m #------------------------ Iteration 696 --------------------------#
[32m[20221208 14:02:48 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:48 @agent_ppo2.py:179][0m |           0.0590 |         244.3057 |           9.2328 |
[32m[20221208 14:02:48 @agent_ppo2.py:179][0m |           0.0156 |         236.7572 |           9.0326 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0138 |         231.8245 |           8.5912 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0121 |         230.5040 |           8.6661 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0267 |         228.3609 |           8.5643 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0277 |         227.1547 |           8.4110 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0231 |         226.7688 |           8.0583 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0331 |         226.9637 |           8.1798 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0395 |         224.8554 |           7.8972 |
[32m[20221208 14:02:49 @agent_ppo2.py:179][0m |          -0.0412 |         226.2683 |           7.8941 |
[32m[20221208 14:02:49 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.46
[32m[20221208 14:02:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.13
[32m[20221208 14:02:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 665.97
[32m[20221208 14:02:49 @agent_ppo2.py:137][0m Total time:      17.48 min
[32m[20221208 14:02:49 @agent_ppo2.py:139][0m 1427456 total steps have happened
[32m[20221208 14:02:49 @agent_ppo2.py:115][0m #------------------------ Iteration 697 --------------------------#
[32m[20221208 14:02:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |           0.0472 |         242.5255 |           9.5116 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |           0.0438 |         235.4996 |           9.1134 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |           0.0946 |         233.9242 |           9.1337 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |           0.0240 |         232.7213 |           9.4855 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |          -0.0059 |         232.0561 |           9.8973 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |          -0.0201 |         230.6330 |           9.6059 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |          -0.0243 |         229.6303 |           9.6011 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |          -0.0299 |         230.5378 |           9.5325 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |          -0.0361 |         229.0376 |           9.4307 |
[32m[20221208 14:02:50 @agent_ppo2.py:179][0m |          -0.0380 |         230.1731 |           9.3244 |
[32m[20221208 14:02:50 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.51
[32m[20221208 14:02:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.62
[32m[20221208 14:02:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.53
[32m[20221208 14:02:51 @agent_ppo2.py:137][0m Total time:      17.51 min
[32m[20221208 14:02:51 @agent_ppo2.py:139][0m 1429504 total steps have happened
[32m[20221208 14:02:51 @agent_ppo2.py:115][0m #------------------------ Iteration 698 --------------------------#
[32m[20221208 14:02:51 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:51 @agent_ppo2.py:179][0m |           0.0665 |         240.6332 |           9.0147 |
[32m[20221208 14:02:51 @agent_ppo2.py:179][0m |           0.0703 |         233.7340 |           9.0384 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |           0.0805 |         229.9878 |           9.3767 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |           0.0498 |         227.9240 |           9.6510 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |           0.0010 |         227.5982 |           9.4701 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |          -0.0155 |         226.6278 |           9.5419 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |          -0.0260 |         226.9407 |           9.2335 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |          -0.0282 |         225.5727 |           9.1738 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |          -0.0344 |         226.2218 |           8.9891 |
[32m[20221208 14:02:52 @agent_ppo2.py:179][0m |          -0.0388 |         225.2996 |           8.8574 |
[32m[20221208 14:02:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 892.23
[32m[20221208 14:02:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.21
[32m[20221208 14:02:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 933.65
[32m[20221208 14:02:52 @agent_ppo2.py:137][0m Total time:      17.53 min
[32m[20221208 14:02:52 @agent_ppo2.py:139][0m 1431552 total steps have happened
[32m[20221208 14:02:52 @agent_ppo2.py:115][0m #------------------------ Iteration 699 --------------------------#
[32m[20221208 14:02:53 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:02:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |           0.0366 |         235.3188 |           9.4224 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |           0.0545 |         233.1902 |           9.0640 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |           0.0066 |         229.9040 |           9.6031 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |          -0.0124 |         229.8176 |           9.3985 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |          -0.0251 |         227.1997 |           9.1666 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |          -0.0275 |         226.3051 |           9.0361 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |          -0.0349 |         226.8504 |           9.0078 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |          -0.0343 |         225.8254 |           9.0531 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |          -0.0402 |         224.8988 |           9.0051 |
[32m[20221208 14:02:53 @agent_ppo2.py:179][0m |          -0.0366 |         225.4741 |           8.8946 |
[32m[20221208 14:02:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 905.96
[32m[20221208 14:02:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.68
[32m[20221208 14:02:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 933.51
[32m[20221208 14:02:54 @agent_ppo2.py:137][0m Total time:      17.56 min
[32m[20221208 14:02:54 @agent_ppo2.py:139][0m 1433600 total steps have happened
[32m[20221208 14:02:54 @agent_ppo2.py:115][0m #------------------------ Iteration 700 --------------------------#
[32m[20221208 14:02:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:54 @agent_ppo2.py:179][0m |           0.0468 |         240.2674 |           9.3861 |
[32m[20221208 14:02:54 @agent_ppo2.py:179][0m |           0.0164 |         229.2212 |           9.2431 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0201 |         226.8039 |           9.3413 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0295 |         224.9308 |           9.2658 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0362 |         224.4906 |           9.2287 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0442 |         223.9269 |           9.1228 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0479 |         222.3926 |           9.0560 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0488 |         223.6253 |           9.0635 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0483 |         222.6597 |           8.9375 |
[32m[20221208 14:02:55 @agent_ppo2.py:179][0m |          -0.0547 |         221.1359 |           8.7272 |
[32m[20221208 14:02:55 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:02:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 835.67
[32m[20221208 14:02:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.98
[32m[20221208 14:02:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 969.10
[32m[20221208 14:02:55 @agent_ppo2.py:137][0m Total time:      17.58 min
[32m[20221208 14:02:55 @agent_ppo2.py:139][0m 1435648 total steps have happened
[32m[20221208 14:02:55 @agent_ppo2.py:115][0m #------------------------ Iteration 701 --------------------------#
[32m[20221208 14:02:56 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:02:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |           0.0396 |         235.5681 |           8.9245 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |           0.0148 |         228.8273 |           9.1908 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0095 |         226.7128 |           9.4206 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0174 |         225.4890 |           9.1668 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0270 |         223.3708 |           9.1611 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0332 |         222.8035 |           9.1342 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0354 |         222.5984 |           8.9716 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0344 |         221.3596 |           9.0054 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0400 |         221.0763 |           8.9234 |
[32m[20221208 14:02:56 @agent_ppo2.py:179][0m |          -0.0434 |         220.2389 |           8.7707 |
[32m[20221208 14:02:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:02:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 968.16
[32m[20221208 14:02:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 996.64
[32m[20221208 14:02:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.42
[32m[20221208 14:02:57 @agent_ppo2.py:137][0m Total time:      17.61 min
[32m[20221208 14:02:57 @agent_ppo2.py:139][0m 1437696 total steps have happened
[32m[20221208 14:02:57 @agent_ppo2.py:115][0m #------------------------ Iteration 702 --------------------------#
[32m[20221208 14:02:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:57 @agent_ppo2.py:179][0m |           0.0417 |         244.8341 |           9.5005 |
[32m[20221208 14:02:57 @agent_ppo2.py:179][0m |           0.0194 |         231.2064 |           9.3497 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0185 |         226.4569 |           9.7271 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0353 |         223.7806 |           9.6342 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0384 |         222.0859 |           9.5978 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0429 |         220.4619 |           9.5963 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0513 |         219.1941 |           9.6259 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0521 |         217.3129 |           9.3686 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0561 |         216.3988 |           9.4411 |
[32m[20221208 14:02:58 @agent_ppo2.py:179][0m |          -0.0578 |         214.9025 |           9.3314 |
[32m[20221208 14:02:58 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:02:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 813.38
[32m[20221208 14:02:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 883.99
[32m[20221208 14:02:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.85
[32m[20221208 14:02:58 @agent_ppo2.py:137][0m Total time:      17.63 min
[32m[20221208 14:02:58 @agent_ppo2.py:139][0m 1439744 total steps have happened
[32m[20221208 14:02:58 @agent_ppo2.py:115][0m #------------------------ Iteration 703 --------------------------#
[32m[20221208 14:02:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:02:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |           0.0722 |         241.0867 |           9.2316 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |           0.0423 |         227.4962 |           9.1935 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |           0.0051 |         221.7928 |           9.3153 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |          -0.0203 |         217.3785 |           9.0363 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |          -0.0318 |         215.3307 |           9.0163 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |          -0.0397 |         211.3364 |           8.7673 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |          -0.0455 |         206.1800 |           8.5849 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |          -0.0460 |         201.7138 |           8.5659 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |          -0.0494 |         199.2610 |           8.2221 |
[32m[20221208 14:02:59 @agent_ppo2.py:179][0m |          -0.0533 |         198.1349 |           8.2269 |
[32m[20221208 14:02:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 851.79
[32m[20221208 14:03:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.26
[32m[20221208 14:03:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.39
[32m[20221208 14:03:00 @agent_ppo2.py:137][0m Total time:      17.66 min
[32m[20221208 14:03:00 @agent_ppo2.py:139][0m 1441792 total steps have happened
[32m[20221208 14:03:00 @agent_ppo2.py:115][0m #------------------------ Iteration 704 --------------------------#
[32m[20221208 14:03:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:00 @agent_ppo2.py:179][0m |           0.0398 |         243.7429 |           8.1880 |
[32m[20221208 14:03:00 @agent_ppo2.py:179][0m |           0.0214 |         233.5969 |           8.7858 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0186 |         229.3615 |           8.7533 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0384 |         227.3308 |           8.1859 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0433 |         226.6611 |           7.8231 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0435 |         226.5803 |           7.7313 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0502 |         225.3495 |           7.4733 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0566 |         224.5415 |           7.4139 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0556 |         223.8341 |           7.1674 |
[32m[20221208 14:03:01 @agent_ppo2.py:179][0m |          -0.0587 |         224.0712 |           7.3347 |
[32m[20221208 14:03:01 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:03:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 798.26
[32m[20221208 14:03:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 847.49
[32m[20221208 14:03:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.92
[32m[20221208 14:03:01 @agent_ppo2.py:137][0m Total time:      17.68 min
[32m[20221208 14:03:01 @agent_ppo2.py:139][0m 1443840 total steps have happened
[32m[20221208 14:03:01 @agent_ppo2.py:115][0m #------------------------ Iteration 705 --------------------------#
[32m[20221208 14:03:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |           0.0583 |         241.2264 |           8.1249 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |           0.0522 |         230.5133 |           8.8980 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |           0.0148 |         226.3108 |           8.8832 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |          -0.0152 |         224.8352 |           8.9250 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |          -0.0315 |         223.0752 |           8.8713 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |          -0.0357 |         222.5213 |           8.7976 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |          -0.0428 |         221.8950 |           8.6276 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |          -0.0461 |         222.1018 |           8.4692 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |          -0.0480 |         222.0450 |           8.4010 |
[32m[20221208 14:03:02 @agent_ppo2.py:179][0m |          -0.0411 |         221.3161 |           8.0777 |
[32m[20221208 14:03:02 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:03:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 862.94
[32m[20221208 14:03:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.58
[32m[20221208 14:03:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 842.38
[32m[20221208 14:03:03 @agent_ppo2.py:137][0m Total time:      17.71 min
[32m[20221208 14:03:03 @agent_ppo2.py:139][0m 1445888 total steps have happened
[32m[20221208 14:03:03 @agent_ppo2.py:115][0m #------------------------ Iteration 706 --------------------------#
[32m[20221208 14:03:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:03 @agent_ppo2.py:179][0m |           0.0715 |         247.5751 |           7.7696 |
[32m[20221208 14:03:03 @agent_ppo2.py:179][0m |           0.0445 |         224.7938 |           7.9912 |
[32m[20221208 14:03:03 @agent_ppo2.py:179][0m |          -0.0053 |         211.3084 |           7.8197 |
[32m[20221208 14:03:04 @agent_ppo2.py:179][0m |          -0.0249 |         201.5170 |           7.2956 |
[32m[20221208 14:03:04 @agent_ppo2.py:179][0m |          -0.0331 |         195.6713 |           7.2633 |
[32m[20221208 14:03:04 @agent_ppo2.py:179][0m |          -0.0455 |         190.6768 |           6.8367 |
[32m[20221208 14:03:04 @agent_ppo2.py:179][0m |          -0.0491 |         186.1820 |           6.6205 |
[32m[20221208 14:03:04 @agent_ppo2.py:179][0m |          -0.0535 |         182.8842 |           6.6099 |
[32m[20221208 14:03:04 @agent_ppo2.py:179][0m |          -0.0555 |         178.3767 |           6.3728 |
[32m[20221208 14:03:04 @agent_ppo2.py:179][0m |          -0.0581 |         175.1725 |           6.1289 |
[32m[20221208 14:03:04 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:03:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 819.14
[32m[20221208 14:03:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.13
[32m[20221208 14:03:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 922.06
[32m[20221208 14:03:04 @agent_ppo2.py:137][0m Total time:      17.73 min
[32m[20221208 14:03:04 @agent_ppo2.py:139][0m 1447936 total steps have happened
[32m[20221208 14:03:04 @agent_ppo2.py:115][0m #------------------------ Iteration 707 --------------------------#
[32m[20221208 14:03:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |           0.0725 |         233.4568 |           7.1331 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |           0.0648 |         225.1716 |           7.3897 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |           0.0038 |         221.4497 |           7.3879 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |          -0.0169 |         218.6149 |           7.3391 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |          -0.0317 |         218.2105 |           7.2301 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |          -0.0347 |         217.1154 |           7.0088 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |          -0.0400 |         216.3919 |           6.8754 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |          -0.0443 |         216.4585 |           6.8243 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |          -0.0494 |         214.4282 |           6.5516 |
[32m[20221208 14:03:05 @agent_ppo2.py:179][0m |          -0.0479 |         214.1093 |           6.4177 |
[32m[20221208 14:03:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 893.58
[32m[20221208 14:03:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.67
[32m[20221208 14:03:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 874.99
[32m[20221208 14:03:06 @agent_ppo2.py:137][0m Total time:      17.76 min
[32m[20221208 14:03:06 @agent_ppo2.py:139][0m 1449984 total steps have happened
[32m[20221208 14:03:06 @agent_ppo2.py:115][0m #------------------------ Iteration 708 --------------------------#
[32m[20221208 14:03:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:06 @agent_ppo2.py:179][0m |           0.0796 |         252.8735 |           7.4526 |
[32m[20221208 14:03:06 @agent_ppo2.py:179][0m |           0.0456 |         241.2864 |           7.7216 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0008 |         236.2655 |           7.2425 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0259 |         232.7159 |           6.7495 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0381 |         230.1728 |           6.8052 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0458 |         228.3347 |           6.3507 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0512 |         227.8950 |           6.3203 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0556 |         226.5307 |           6.1774 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0587 |         224.9543 |           5.9284 |
[32m[20221208 14:03:07 @agent_ppo2.py:179][0m |          -0.0586 |         223.1920 |           5.8537 |
[32m[20221208 14:03:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 746.50
[32m[20221208 14:03:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 871.28
[32m[20221208 14:03:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 897.51
[32m[20221208 14:03:07 @agent_ppo2.py:137][0m Total time:      17.78 min
[32m[20221208 14:03:07 @agent_ppo2.py:139][0m 1452032 total steps have happened
[32m[20221208 14:03:07 @agent_ppo2.py:115][0m #------------------------ Iteration 709 --------------------------#
[32m[20221208 14:03:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |           0.9079 |         214.6320 |           6.4581 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |           0.0005 |         200.7501 |           4.9489 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0281 |         197.5107 |           4.7493 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0516 |         195.4260 |           4.2959 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0596 |         194.8751 |           4.1278 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0656 |         193.1074 |           3.8922 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0719 |         191.8013 |           3.8373 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0748 |         191.4765 |           3.6226 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0727 |         191.0763 |           3.4281 |
[32m[20221208 14:03:08 @agent_ppo2.py:179][0m |          -0.0770 |         188.7712 |           3.2220 |
[32m[20221208 14:03:08 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 631.90
[32m[20221208 14:03:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 968.12
[32m[20221208 14:03:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.59
[32m[20221208 14:03:09 @agent_ppo2.py:137][0m Total time:      17.81 min
[32m[20221208 14:03:09 @agent_ppo2.py:139][0m 1454080 total steps have happened
[32m[20221208 14:03:09 @agent_ppo2.py:115][0m #------------------------ Iteration 710 --------------------------#
[32m[20221208 14:03:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:09 @agent_ppo2.py:179][0m |           0.0610 |         246.0010 |           5.7647 |
[32m[20221208 14:03:09 @agent_ppo2.py:179][0m |           0.0288 |         237.2565 |           6.1940 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0097 |         234.6712 |           5.6512 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0265 |         229.8472 |           5.4687 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0309 |         227.2460 |           5.3618 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0391 |         225.3820 |           5.2191 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0447 |         223.7818 |           4.8767 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0490 |         223.6573 |           4.7046 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0456 |         222.9477 |           4.8988 |
[32m[20221208 14:03:10 @agent_ppo2.py:179][0m |          -0.0509 |         220.4848 |           4.3717 |
[32m[20221208 14:03:10 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 885.82
[32m[20221208 14:03:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.32
[32m[20221208 14:03:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 887.23
[32m[20221208 14:03:10 @agent_ppo2.py:137][0m Total time:      17.83 min
[32m[20221208 14:03:10 @agent_ppo2.py:139][0m 1456128 total steps have happened
[32m[20221208 14:03:10 @agent_ppo2.py:115][0m #------------------------ Iteration 711 --------------------------#
[32m[20221208 14:03:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |           0.0632 |         228.1580 |           6.4124 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |           0.0721 |         218.7169 |           7.0837 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |           0.0210 |         214.1116 |           6.8272 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |          -0.0085 |         210.3264 |           6.2467 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |          -0.0238 |         208.6101 |           5.6188 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |          -0.0321 |         206.9347 |           5.6691 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |          -0.0265 |         207.0711 |           5.9147 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |          -0.0337 |         205.7104 |           5.7431 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |          -0.0386 |         205.9456 |           5.6267 |
[32m[20221208 14:03:11 @agent_ppo2.py:179][0m |          -0.0394 |         206.0254 |           5.3247 |
[32m[20221208 14:03:11 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 963.55
[32m[20221208 14:03:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.56
[32m[20221208 14:03:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 897.34
[32m[20221208 14:03:12 @agent_ppo2.py:137][0m Total time:      17.86 min
[32m[20221208 14:03:12 @agent_ppo2.py:139][0m 1458176 total steps have happened
[32m[20221208 14:03:12 @agent_ppo2.py:115][0m #------------------------ Iteration 712 --------------------------#
[32m[20221208 14:03:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:12 @agent_ppo2.py:179][0m |           0.0446 |         251.2088 |           6.4219 |
[32m[20221208 14:03:12 @agent_ppo2.py:179][0m |           0.0491 |         241.1147 |           6.9509 |
[32m[20221208 14:03:12 @agent_ppo2.py:179][0m |           0.0203 |         237.3556 |           7.0686 |
[32m[20221208 14:03:13 @agent_ppo2.py:179][0m |          -0.0120 |         235.6222 |           6.4168 |
[32m[20221208 14:03:13 @agent_ppo2.py:179][0m |          -0.0294 |         234.6593 |           6.3500 |
[32m[20221208 14:03:13 @agent_ppo2.py:179][0m |          -0.0369 |         233.8501 |           6.0749 |
[32m[20221208 14:03:13 @agent_ppo2.py:179][0m |          -0.0436 |         231.9036 |           6.1708 |
[32m[20221208 14:03:13 @agent_ppo2.py:179][0m |          -0.0459 |         231.2269 |           5.8688 |
[32m[20221208 14:03:13 @agent_ppo2.py:179][0m |          -0.0495 |         231.2108 |           5.8375 |
[32m[20221208 14:03:13 @agent_ppo2.py:179][0m |          -0.0490 |         229.8181 |           5.5663 |
[32m[20221208 14:03:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.58
[32m[20221208 14:03:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.86
[32m[20221208 14:03:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 928.43
[32m[20221208 14:03:13 @agent_ppo2.py:137][0m Total time:      17.88 min
[32m[20221208 14:03:13 @agent_ppo2.py:139][0m 1460224 total steps have happened
[32m[20221208 14:03:13 @agent_ppo2.py:115][0m #------------------------ Iteration 713 --------------------------#
[32m[20221208 14:03:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |           0.0599 |         216.4130 |           5.7157 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |           0.0399 |         197.6883 |           6.4525 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |           0.0061 |         184.2790 |           6.1720 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |          -0.0042 |         180.2830 |           6.0766 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |          -0.0193 |         178.0463 |           5.8809 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |          -0.0271 |         175.7673 |           5.6912 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |          -0.0348 |         173.1622 |           5.5185 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |          -0.0376 |         172.4443 |           5.5974 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |          -0.0405 |         170.6819 |           5.2767 |
[32m[20221208 14:03:14 @agent_ppo2.py:179][0m |          -0.0333 |         171.3489 |           5.3726 |
[32m[20221208 14:03:14 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:03:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 965.29
[32m[20221208 14:03:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.02
[32m[20221208 14:03:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 933.31
[32m[20221208 14:03:15 @agent_ppo2.py:137][0m Total time:      17.91 min
[32m[20221208 14:03:15 @agent_ppo2.py:139][0m 1462272 total steps have happened
[32m[20221208 14:03:15 @agent_ppo2.py:115][0m #------------------------ Iteration 714 --------------------------#
[32m[20221208 14:03:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:15 @agent_ppo2.py:179][0m |           0.0651 |         240.6775 |           6.2188 |
[32m[20221208 14:03:15 @agent_ppo2.py:179][0m |           0.0362 |         232.3481 |           7.2212 |
[32m[20221208 14:03:15 @agent_ppo2.py:179][0m |          -0.0093 |         228.1068 |           6.8943 |
[32m[20221208 14:03:16 @agent_ppo2.py:179][0m |          -0.0319 |         224.4031 |           6.6666 |
[32m[20221208 14:03:16 @agent_ppo2.py:179][0m |          -0.0413 |         221.6368 |           6.1294 |
[32m[20221208 14:03:16 @agent_ppo2.py:179][0m |          -0.0454 |         218.0906 |           5.9733 |
[32m[20221208 14:03:16 @agent_ppo2.py:179][0m |          -0.0500 |         217.0778 |           5.6295 |
[32m[20221208 14:03:16 @agent_ppo2.py:179][0m |          -0.0517 |         216.9180 |           5.3580 |
[32m[20221208 14:03:16 @agent_ppo2.py:179][0m |          -0.0527 |         215.1984 |           5.2621 |
[32m[20221208 14:03:16 @agent_ppo2.py:179][0m |          -0.0554 |         214.8739 |           5.0433 |
[32m[20221208 14:03:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 860.65
[32m[20221208 14:03:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.77
[32m[20221208 14:03:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.47
[32m[20221208 14:03:16 @agent_ppo2.py:137][0m Total time:      17.93 min
[32m[20221208 14:03:16 @agent_ppo2.py:139][0m 1464320 total steps have happened
[32m[20221208 14:03:16 @agent_ppo2.py:115][0m #------------------------ Iteration 715 --------------------------#
[32m[20221208 14:03:17 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |           0.0826 |         241.3737 |           5.0201 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |           0.0606 |         236.1036 |           5.9504 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |           0.0020 |         231.9585 |           5.5749 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |          -0.0011 |         230.4828 |           5.7399 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |          -0.0175 |         229.8702 |           5.4581 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |          -0.0249 |         228.6524 |           4.8215 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |          -0.0333 |         228.6026 |           5.1453 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |          -0.0433 |         227.6772 |           4.6251 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |          -0.0473 |         225.9818 |           4.7094 |
[32m[20221208 14:03:17 @agent_ppo2.py:179][0m |          -0.0484 |         225.9262 |           4.6136 |
[32m[20221208 14:03:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 837.79
[32m[20221208 14:03:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 886.29
[32m[20221208 14:03:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.68
[32m[20221208 14:03:18 @agent_ppo2.py:137][0m Total time:      17.96 min
[32m[20221208 14:03:18 @agent_ppo2.py:139][0m 1466368 total steps have happened
[32m[20221208 14:03:18 @agent_ppo2.py:115][0m #------------------------ Iteration 716 --------------------------#
[32m[20221208 14:03:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:18 @agent_ppo2.py:179][0m |           0.0359 |         245.4296 |           5.6827 |
[32m[20221208 14:03:18 @agent_ppo2.py:179][0m |           0.0256 |         236.7539 |           5.8336 |
[32m[20221208 14:03:18 @agent_ppo2.py:179][0m |          -0.0153 |         235.3281 |           5.5369 |
[32m[20221208 14:03:18 @agent_ppo2.py:179][0m |          -0.0239 |         233.0278 |           5.2789 |
[32m[20221208 14:03:19 @agent_ppo2.py:179][0m |          -0.0301 |         230.3509 |           5.2447 |
[32m[20221208 14:03:19 @agent_ppo2.py:179][0m |          -0.0356 |         229.2290 |           5.1228 |
[32m[20221208 14:03:19 @agent_ppo2.py:179][0m |          -0.0417 |         230.0193 |           4.7569 |
[32m[20221208 14:03:19 @agent_ppo2.py:179][0m |          -0.0467 |         228.2708 |           4.8326 |
[32m[20221208 14:03:19 @agent_ppo2.py:179][0m |          -0.0429 |         227.2348 |           4.6993 |
[32m[20221208 14:03:19 @agent_ppo2.py:179][0m |          -0.0448 |         227.6044 |           4.6875 |
[32m[20221208 14:03:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.60
[32m[20221208 14:03:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.78
[32m[20221208 14:03:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 844.09
[32m[20221208 14:03:19 @agent_ppo2.py:137][0m Total time:      17.98 min
[32m[20221208 14:03:19 @agent_ppo2.py:139][0m 1468416 total steps have happened
[32m[20221208 14:03:19 @agent_ppo2.py:115][0m #------------------------ Iteration 717 --------------------------#
[32m[20221208 14:03:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:03:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |           0.0390 |         242.2207 |           5.4012 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |           0.0173 |         236.2847 |           5.8453 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0163 |         232.5477 |           5.4335 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0270 |         230.0007 |           5.3232 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0352 |         228.7538 |           5.2670 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0416 |         228.6721 |           5.0719 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0314 |         228.8473 |           5.0845 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0373 |         227.5817 |           5.0586 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0430 |         227.5077 |           4.7550 |
[32m[20221208 14:03:20 @agent_ppo2.py:179][0m |          -0.0461 |         226.3135 |           4.8886 |
[32m[20221208 14:03:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.83
[32m[20221208 14:03:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 931.38
[32m[20221208 14:03:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.02
[32m[20221208 14:03:21 @agent_ppo2.py:137][0m Total time:      18.01 min
[32m[20221208 14:03:21 @agent_ppo2.py:139][0m 1470464 total steps have happened
[32m[20221208 14:03:21 @agent_ppo2.py:115][0m #------------------------ Iteration 718 --------------------------#
[32m[20221208 14:03:21 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:21 @agent_ppo2.py:179][0m |           0.0457 |         233.5734 |           4.8911 |
[32m[20221208 14:03:21 @agent_ppo2.py:179][0m |           0.0053 |         223.1633 |           5.7259 |
[32m[20221208 14:03:21 @agent_ppo2.py:179][0m |          -0.0189 |         219.9394 |           5.3924 |
[32m[20221208 14:03:21 @agent_ppo2.py:179][0m |          -0.0267 |         218.6508 |           5.4157 |
[32m[20221208 14:03:22 @agent_ppo2.py:179][0m |          -0.0303 |         217.8795 |           5.2740 |
[32m[20221208 14:03:22 @agent_ppo2.py:179][0m |          -0.0396 |         216.9656 |           4.8735 |
[32m[20221208 14:03:22 @agent_ppo2.py:179][0m |          -0.0366 |         215.5368 |           4.7673 |
[32m[20221208 14:03:22 @agent_ppo2.py:179][0m |          -0.0451 |         214.8633 |           4.7369 |
[32m[20221208 14:03:22 @agent_ppo2.py:179][0m |          -0.0434 |         213.0653 |           4.4843 |
[32m[20221208 14:03:22 @agent_ppo2.py:179][0m |          -0.0442 |         212.6100 |           4.1722 |
[32m[20221208 14:03:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.50
[32m[20221208 14:03:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 918.96
[32m[20221208 14:03:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 915.90
[32m[20221208 14:03:22 @agent_ppo2.py:137][0m Total time:      18.03 min
[32m[20221208 14:03:22 @agent_ppo2.py:139][0m 1472512 total steps have happened
[32m[20221208 14:03:22 @agent_ppo2.py:115][0m #------------------------ Iteration 719 --------------------------#
[32m[20221208 14:03:23 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |           0.0612 |         242.5822 |           5.0428 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |           0.0397 |         216.9877 |           6.7237 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |           0.0007 |         199.3048 |           5.2907 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |          -0.0168 |         187.9903 |           4.3113 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |          -0.0286 |         180.7812 |           3.5680 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |          -0.0325 |         176.6554 |           3.2831 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |          -0.0364 |         171.9657 |           3.1324 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |          -0.0411 |         167.5642 |           2.7483 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |          -0.0425 |         165.3759 |           2.7738 |
[32m[20221208 14:03:23 @agent_ppo2.py:179][0m |          -0.0452 |         162.2266 |           2.4774 |
[32m[20221208 14:03:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.73
[32m[20221208 14:03:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.60
[32m[20221208 14:03:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.08
[32m[20221208 14:03:24 @agent_ppo2.py:137][0m Total time:      18.06 min
[32m[20221208 14:03:24 @agent_ppo2.py:139][0m 1474560 total steps have happened
[32m[20221208 14:03:24 @agent_ppo2.py:115][0m #------------------------ Iteration 720 --------------------------#
[32m[20221208 14:03:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:24 @agent_ppo2.py:179][0m |           0.0811 |         258.5365 |           4.1686 |
[32m[20221208 14:03:24 @agent_ppo2.py:179][0m |          -0.0093 |         222.5714 |           3.4338 |
[32m[20221208 14:03:24 @agent_ppo2.py:179][0m |          -0.0402 |         210.6303 |           3.0488 |
[32m[20221208 14:03:24 @agent_ppo2.py:179][0m |          -0.0560 |         205.1408 |           2.4079 |
[32m[20221208 14:03:24 @agent_ppo2.py:179][0m |          -0.0658 |         200.0141 |           2.1609 |
[32m[20221208 14:03:25 @agent_ppo2.py:179][0m |          -0.0733 |         196.8107 |           1.7899 |
[32m[20221208 14:03:25 @agent_ppo2.py:179][0m |          -0.0771 |         193.9496 |           1.5072 |
[32m[20221208 14:03:25 @agent_ppo2.py:179][0m |          -0.0784 |         192.5321 |           1.3355 |
[32m[20221208 14:03:25 @agent_ppo2.py:179][0m |          -0.0817 |         190.3923 |           1.1909 |
[32m[20221208 14:03:25 @agent_ppo2.py:179][0m |          -0.0845 |         189.0641 |           0.8932 |
[32m[20221208 14:03:25 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 592.22
[32m[20221208 14:03:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 814.77
[32m[20221208 14:03:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 922.66
[32m[20221208 14:03:25 @agent_ppo2.py:137][0m Total time:      18.08 min
[32m[20221208 14:03:25 @agent_ppo2.py:139][0m 1476608 total steps have happened
[32m[20221208 14:03:25 @agent_ppo2.py:115][0m #------------------------ Iteration 721 --------------------------#
[32m[20221208 14:03:26 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:03:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |           0.0486 |         252.7445 |           3.0049 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |           0.0218 |         244.6954 |           2.4656 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |           0.0151 |         239.5072 |           3.0417 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |          -0.0218 |         237.0136 |           1.9354 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |          -0.0329 |         234.0395 |           1.2659 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |          -0.0420 |         233.4682 |           0.7917 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |          -0.0475 |         233.1728 |           0.4199 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |          -0.0500 |         233.0546 |           0.0423 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |          -0.0508 |         232.2048 |          -0.1642 |
[32m[20221208 14:03:26 @agent_ppo2.py:179][0m |          -0.0536 |         231.0242 |          -0.2417 |
[32m[20221208 14:03:26 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.19
[32m[20221208 14:03:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.37
[32m[20221208 14:03:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.25
[32m[20221208 14:03:27 @agent_ppo2.py:137][0m Total time:      18.11 min
[32m[20221208 14:03:27 @agent_ppo2.py:139][0m 1478656 total steps have happened
[32m[20221208 14:03:27 @agent_ppo2.py:115][0m #------------------------ Iteration 722 --------------------------#
[32m[20221208 14:03:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:27 @agent_ppo2.py:179][0m |           0.0515 |         255.3319 |           2.7475 |
[32m[20221208 14:03:27 @agent_ppo2.py:179][0m |           0.0152 |         244.2518 |           2.6544 |
[32m[20221208 14:03:27 @agent_ppo2.py:179][0m |           0.0075 |         239.3150 |           2.3690 |
[32m[20221208 14:03:27 @agent_ppo2.py:179][0m |          -0.0218 |         235.2995 |           2.4330 |
[32m[20221208 14:03:27 @agent_ppo2.py:179][0m |          -0.0315 |         231.9638 |           1.1536 |
[32m[20221208 14:03:28 @agent_ppo2.py:179][0m |          -0.0440 |         230.2846 |           1.2790 |
[32m[20221208 14:03:28 @agent_ppo2.py:179][0m |          -0.0473 |         228.1259 |           0.7220 |
[32m[20221208 14:03:28 @agent_ppo2.py:179][0m |          -0.0482 |         225.3944 |           0.6602 |
[32m[20221208 14:03:28 @agent_ppo2.py:179][0m |          -0.0502 |         222.3405 |           0.1984 |
[32m[20221208 14:03:28 @agent_ppo2.py:179][0m |          -0.0565 |         220.2633 |          -0.2792 |
[32m[20221208 14:03:28 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 863.17
[32m[20221208 14:03:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 930.64
[32m[20221208 14:03:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 890.14
[32m[20221208 14:03:28 @agent_ppo2.py:137][0m Total time:      18.13 min
[32m[20221208 14:03:28 @agent_ppo2.py:139][0m 1480704 total steps have happened
[32m[20221208 14:03:28 @agent_ppo2.py:115][0m #------------------------ Iteration 723 --------------------------#
[32m[20221208 14:03:29 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |           0.0678 |         240.3023 |           1.4213 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |           0.0719 |         228.9198 |           2.8621 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |           0.0178 |         223.8801 |           1.3869 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |          -0.0034 |         220.9601 |           0.9262 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |          -0.0191 |         219.3877 |           0.6008 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |          -0.0252 |         218.6058 |           0.2774 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |          -0.0340 |         217.2492 |           0.1033 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |          -0.0394 |         216.3223 |          -0.1868 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |          -0.0450 |         215.9824 |          -0.1593 |
[32m[20221208 14:03:29 @agent_ppo2.py:179][0m |          -0.0453 |         215.6673 |          -0.4889 |
[32m[20221208 14:03:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 912.32
[32m[20221208 14:03:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 997.93
[32m[20221208 14:03:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.57
[32m[20221208 14:03:30 @agent_ppo2.py:137][0m Total time:      18.15 min
[32m[20221208 14:03:30 @agent_ppo2.py:139][0m 1482752 total steps have happened
[32m[20221208 14:03:30 @agent_ppo2.py:115][0m #------------------------ Iteration 724 --------------------------#
[32m[20221208 14:03:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:30 @agent_ppo2.py:179][0m |           0.0542 |         242.4092 |          -0.9651 |
[32m[20221208 14:03:30 @agent_ppo2.py:179][0m |           0.0339 |         233.3772 |           1.0540 |
[32m[20221208 14:03:30 @agent_ppo2.py:179][0m |          -0.0017 |         229.7412 |          -0.3510 |
[32m[20221208 14:03:30 @agent_ppo2.py:179][0m |          -0.0157 |         229.2493 |          -0.2442 |
[32m[20221208 14:03:30 @agent_ppo2.py:179][0m |          -0.0325 |         226.2059 |          -0.9870 |
[32m[20221208 14:03:30 @agent_ppo2.py:179][0m |          -0.0429 |         224.1015 |          -1.6486 |
[32m[20221208 14:03:31 @agent_ppo2.py:179][0m |          -0.0465 |         222.5040 |          -1.8442 |
[32m[20221208 14:03:31 @agent_ppo2.py:179][0m |          -0.0498 |         220.9504 |          -1.9646 |
[32m[20221208 14:03:31 @agent_ppo2.py:179][0m |          -0.0537 |         220.6198 |          -2.4446 |
[32m[20221208 14:03:31 @agent_ppo2.py:179][0m |          -0.0560 |         218.8048 |          -2.4166 |
[32m[20221208 14:03:31 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:03:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 882.34
[32m[20221208 14:03:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.90
[32m[20221208 14:03:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.59
[32m[20221208 14:03:31 @agent_ppo2.py:137][0m Total time:      18.18 min
[32m[20221208 14:03:31 @agent_ppo2.py:139][0m 1484800 total steps have happened
[32m[20221208 14:03:31 @agent_ppo2.py:115][0m #------------------------ Iteration 725 --------------------------#
[32m[20221208 14:03:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |           0.0432 |         253.3354 |           0.8729 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |           0.0642 |         233.9226 |           2.4346 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |           0.0084 |         230.5394 |           1.3452 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |          -0.0070 |         226.8718 |           0.4016 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |          -0.0229 |         225.4195 |           0.1307 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |          -0.0285 |         222.2732 |          -0.1435 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |          -0.0353 |         221.9970 |          -0.4520 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |          -0.0417 |         220.8388 |          -0.9353 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |          -0.0428 |         219.6269 |          -1.1464 |
[32m[20221208 14:03:32 @agent_ppo2.py:179][0m |          -0.0445 |         218.7016 |          -1.3568 |
[32m[20221208 14:03:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.67
[32m[20221208 14:03:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.21
[32m[20221208 14:03:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 853.17
[32m[20221208 14:03:33 @agent_ppo2.py:137][0m Total time:      18.20 min
[32m[20221208 14:03:33 @agent_ppo2.py:139][0m 1486848 total steps have happened
[32m[20221208 14:03:33 @agent_ppo2.py:115][0m #------------------------ Iteration 726 --------------------------#
[32m[20221208 14:03:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:33 @agent_ppo2.py:179][0m |           0.0681 |         233.2999 |          -0.4392 |
[32m[20221208 14:03:33 @agent_ppo2.py:179][0m |           0.0610 |         228.0411 |           2.1084 |
[32m[20221208 14:03:33 @agent_ppo2.py:179][0m |           0.0226 |         226.3782 |           2.5277 |
[32m[20221208 14:03:33 @agent_ppo2.py:179][0m |          -0.0095 |         222.6498 |           0.9388 |
[32m[20221208 14:03:33 @agent_ppo2.py:179][0m |          -0.0191 |         221.6462 |           0.8486 |
[32m[20221208 14:03:33 @agent_ppo2.py:179][0m |          -0.0315 |         220.5519 |          -0.4798 |
[32m[20221208 14:03:34 @agent_ppo2.py:179][0m |          -0.0343 |         220.7771 |          -0.9361 |
[32m[20221208 14:03:34 @agent_ppo2.py:179][0m |          -0.0397 |         219.1281 |          -1.6694 |
[32m[20221208 14:03:34 @agent_ppo2.py:179][0m |          -0.0428 |         220.1580 |          -1.8722 |
[32m[20221208 14:03:34 @agent_ppo2.py:179][0m |          -0.0425 |         219.1821 |          -2.1201 |
[32m[20221208 14:03:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.32
[32m[20221208 14:03:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.38
[32m[20221208 14:03:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.72
[32m[20221208 14:03:34 @agent_ppo2.py:137][0m Total time:      18.23 min
[32m[20221208 14:03:34 @agent_ppo2.py:139][0m 1488896 total steps have happened
[32m[20221208 14:03:34 @agent_ppo2.py:115][0m #------------------------ Iteration 727 --------------------------#
[32m[20221208 14:03:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:03:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |           0.0246 |         229.8465 |          -2.0102 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |           0.0209 |         222.8391 |          -1.1182 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |           0.0048 |         217.3962 |          -0.3393 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |          -0.0160 |         214.7543 |          -1.6523 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |          -0.0276 |         214.4379 |          -2.2186 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |          -0.0323 |         212.6943 |          -2.2423 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |          -0.0373 |         213.1386 |          -2.9691 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |          -0.0365 |         213.8977 |          -3.3087 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |          -0.0323 |         210.5591 |          -2.4742 |
[32m[20221208 14:03:35 @agent_ppo2.py:179][0m |          -0.0428 |         209.0431 |          -3.4100 |
[32m[20221208 14:03:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.11
[32m[20221208 14:03:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.41
[32m[20221208 14:03:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 822.78
[32m[20221208 14:03:36 @agent_ppo2.py:137][0m Total time:      18.25 min
[32m[20221208 14:03:36 @agent_ppo2.py:139][0m 1490944 total steps have happened
[32m[20221208 14:03:36 @agent_ppo2.py:115][0m #------------------------ Iteration 728 --------------------------#
[32m[20221208 14:03:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:03:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:36 @agent_ppo2.py:179][0m |           0.0718 |         239.3027 |          -0.2330 |
[32m[20221208 14:03:36 @agent_ppo2.py:179][0m |           0.0478 |         229.9922 |           1.3890 |
[32m[20221208 14:03:36 @agent_ppo2.py:179][0m |           0.0025 |         222.9538 |           0.4578 |
[32m[20221208 14:03:36 @agent_ppo2.py:179][0m |          -0.0235 |         220.2984 |          -0.1012 |
[32m[20221208 14:03:36 @agent_ppo2.py:179][0m |          -0.0281 |         218.1535 |          -0.4588 |
[32m[20221208 14:03:36 @agent_ppo2.py:179][0m |          -0.0346 |         216.4068 |          -0.6709 |
[32m[20221208 14:03:36 @agent_ppo2.py:179][0m |          -0.0404 |         216.5501 |          -0.8441 |
[32m[20221208 14:03:37 @agent_ppo2.py:179][0m |          -0.0445 |         213.3323 |          -1.2366 |
[32m[20221208 14:03:37 @agent_ppo2.py:179][0m |          -0.0456 |         212.6959 |          -1.2695 |
[32m[20221208 14:03:37 @agent_ppo2.py:179][0m |          -0.0469 |         212.1945 |          -1.7655 |
[32m[20221208 14:03:37 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.15
[32m[20221208 14:03:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.89
[32m[20221208 14:03:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.08
[32m[20221208 14:03:37 @agent_ppo2.py:137][0m Total time:      18.28 min
[32m[20221208 14:03:37 @agent_ppo2.py:139][0m 1492992 total steps have happened
[32m[20221208 14:03:37 @agent_ppo2.py:115][0m #------------------------ Iteration 729 --------------------------#
[32m[20221208 14:03:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |           0.0795 |         228.7393 |           2.7324 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |           0.0438 |         215.9360 |           2.1449 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |           0.0054 |         211.2633 |           1.3751 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |          -0.0040 |         206.9721 |           0.6772 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |          -0.0192 |         205.1557 |           0.1195 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |          -0.0252 |         201.8713 |          -0.2903 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |          -0.0342 |         199.9031 |          -0.7521 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |          -0.0373 |         199.3067 |          -1.2745 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |          -0.0407 |         195.9313 |          -1.7086 |
[32m[20221208 14:03:38 @agent_ppo2.py:179][0m |          -0.0416 |         194.1466 |          -1.6973 |
[32m[20221208 14:03:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.19
[32m[20221208 14:03:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.98
[32m[20221208 14:03:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 916.06
[32m[20221208 14:03:39 @agent_ppo2.py:137][0m Total time:      18.30 min
[32m[20221208 14:03:39 @agent_ppo2.py:139][0m 1495040 total steps have happened
[32m[20221208 14:03:39 @agent_ppo2.py:115][0m #------------------------ Iteration 730 --------------------------#
[32m[20221208 14:03:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |           0.0501 |         238.0592 |           0.3580 |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |           0.0663 |         226.5400 |           1.2719 |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |           0.0093 |         218.6197 |           0.3667 |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |          -0.0017 |         216.1361 |           0.2350 |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |          -0.0163 |         213.2757 |          -0.4543 |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |          -0.0258 |         208.8717 |          -0.6601 |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |          -0.0315 |         207.5379 |          -1.0259 |
[32m[20221208 14:03:39 @agent_ppo2.py:179][0m |          -0.0373 |         204.4709 |          -1.5325 |
[32m[20221208 14:03:40 @agent_ppo2.py:179][0m |          -0.0409 |         203.9005 |          -2.0988 |
[32m[20221208 14:03:40 @agent_ppo2.py:179][0m |          -0.0441 |         201.3835 |          -2.3933 |
[32m[20221208 14:03:40 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:03:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.83
[32m[20221208 14:03:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 961.99
[32m[20221208 14:03:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.25
[32m[20221208 14:03:40 @agent_ppo2.py:137][0m Total time:      18.33 min
[32m[20221208 14:03:40 @agent_ppo2.py:139][0m 1497088 total steps have happened
[32m[20221208 14:03:40 @agent_ppo2.py:115][0m #------------------------ Iteration 731 --------------------------#
[32m[20221208 14:03:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |           0.0576 |         234.0060 |          -0.6352 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |           0.0555 |         224.4000 |           0.6804 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |           0.0134 |         220.7180 |          -2.2524 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |          -0.0026 |         218.6593 |          -2.1619 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |          -0.0219 |         216.3578 |          -2.8867 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |          -0.0335 |         215.4566 |          -3.1310 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |          -0.0364 |         214.9985 |          -3.1618 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |          -0.0383 |         215.0204 |          -3.4325 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |          -0.0424 |         213.3090 |          -3.5685 |
[32m[20221208 14:03:41 @agent_ppo2.py:179][0m |          -0.0457 |         213.7123 |          -3.9052 |
[32m[20221208 14:03:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 897.32
[32m[20221208 14:03:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.37
[32m[20221208 14:03:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 903.04
[32m[20221208 14:03:41 @agent_ppo2.py:137][0m Total time:      18.35 min
[32m[20221208 14:03:41 @agent_ppo2.py:139][0m 1499136 total steps have happened
[32m[20221208 14:03:41 @agent_ppo2.py:115][0m #------------------------ Iteration 732 --------------------------#
[32m[20221208 14:03:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |           0.0316 |         219.8563 |          -2.6906 |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |           0.0149 |         188.4039 |          -1.8508 |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |          -0.0128 |         175.4146 |          -2.0604 |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |          -0.0226 |         169.8474 |          -3.0397 |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |          -0.0346 |         165.9145 |          -2.8668 |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |          -0.0407 |         161.9605 |          -3.2505 |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |          -0.0429 |         158.4110 |          -3.7855 |
[32m[20221208 14:03:42 @agent_ppo2.py:179][0m |          -0.0474 |         156.4263 |          -4.3410 |
[32m[20221208 14:03:43 @agent_ppo2.py:179][0m |          -0.0483 |         154.9550 |          -4.5366 |
[32m[20221208 14:03:43 @agent_ppo2.py:179][0m |          -0.0478 |         153.6948 |          -4.9089 |
[32m[20221208 14:03:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 929.32
[32m[20221208 14:03:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.74
[32m[20221208 14:03:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.91
[32m[20221208 14:03:43 @agent_ppo2.py:137][0m Total time:      18.38 min
[32m[20221208 14:03:43 @agent_ppo2.py:139][0m 1501184 total steps have happened
[32m[20221208 14:03:43 @agent_ppo2.py:115][0m #------------------------ Iteration 733 --------------------------#
[32m[20221208 14:03:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |           0.0528 |         232.4004 |          -0.9257 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |           0.0751 |         225.5371 |           0.1034 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |           0.0483 |         221.9748 |          -0.4295 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |          -0.0065 |         219.5280 |          -0.5488 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |          -0.0119 |         217.5744 |          -1.6346 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |          -0.0196 |         217.0005 |          -1.7209 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |          -0.0295 |         216.0858 |          -2.4642 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |          -0.0356 |         214.6724 |          -3.1608 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |          -0.0387 |         213.3441 |          -3.1388 |
[32m[20221208 14:03:44 @agent_ppo2.py:179][0m |          -0.0412 |         212.6538 |          -3.5319 |
[32m[20221208 14:03:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.15
[32m[20221208 14:03:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.05
[32m[20221208 14:03:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 889.79
[32m[20221208 14:03:44 @agent_ppo2.py:137][0m Total time:      18.40 min
[32m[20221208 14:03:44 @agent_ppo2.py:139][0m 1503232 total steps have happened
[32m[20221208 14:03:44 @agent_ppo2.py:115][0m #------------------------ Iteration 734 --------------------------#
[32m[20221208 14:03:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |           0.0705 |         242.5257 |          -3.8229 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |           0.0181 |         237.3431 |          -2.6140 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |          -0.0097 |         231.7604 |          -3.5517 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |          -0.0160 |         228.8432 |          -3.5470 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |          -0.0267 |         227.0476 |          -3.9484 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |          -0.0346 |         225.1589 |          -4.5107 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |          -0.0396 |         223.5958 |          -4.9926 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |          -0.0435 |         220.6159 |          -5.2757 |
[32m[20221208 14:03:45 @agent_ppo2.py:179][0m |          -0.0441 |         218.9679 |          -5.8796 |
[32m[20221208 14:03:46 @agent_ppo2.py:179][0m |          -0.0458 |         217.3114 |          -5.9894 |
[32m[20221208 14:03:46 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:03:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 951.13
[32m[20221208 14:03:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.91
[32m[20221208 14:03:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 927.64
[32m[20221208 14:03:46 @agent_ppo2.py:137][0m Total time:      18.43 min
[32m[20221208 14:03:46 @agent_ppo2.py:139][0m 1505280 total steps have happened
[32m[20221208 14:03:46 @agent_ppo2.py:115][0m #------------------------ Iteration 735 --------------------------#
[32m[20221208 14:03:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:46 @agent_ppo2.py:179][0m |           0.1228 |         238.2562 |          -2.9745 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |           0.0819 |         222.8548 |           0.9468 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |           0.0336 |         215.2440 |           0.4718 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |          -0.0072 |         209.3342 |          -2.3296 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |          -0.0256 |         205.3980 |          -3.0387 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |          -0.0358 |         202.4720 |          -3.9627 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |          -0.0434 |         200.7491 |          -4.8007 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |          -0.0480 |         198.5162 |          -5.2239 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |          -0.0504 |         197.8135 |          -5.3114 |
[32m[20221208 14:03:47 @agent_ppo2.py:179][0m |          -0.0529 |         196.3127 |          -6.0303 |
[32m[20221208 14:03:47 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:03:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 876.04
[32m[20221208 14:03:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.37
[32m[20221208 14:03:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.57
[32m[20221208 14:03:47 @agent_ppo2.py:137][0m Total time:      18.45 min
[32m[20221208 14:03:47 @agent_ppo2.py:139][0m 1507328 total steps have happened
[32m[20221208 14:03:47 @agent_ppo2.py:115][0m #------------------------ Iteration 736 --------------------------#
[32m[20221208 14:03:48 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:03:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |           0.1060 |         245.0538 |          -1.3195 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |           0.0971 |         239.9170 |           1.9789 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |           0.0346 |         235.7547 |          -0.0127 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |           0.0005 |         233.3067 |          -3.3095 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |          -0.0177 |         232.3983 |          -4.2049 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |          -0.0200 |         228.9239 |          -3.8114 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |          -0.0334 |         227.6081 |          -4.6267 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |          -0.0369 |         225.9551 |          -5.1629 |
[32m[20221208 14:03:48 @agent_ppo2.py:179][0m |          -0.0406 |         224.3877 |          -5.1188 |
[32m[20221208 14:03:49 @agent_ppo2.py:179][0m |          -0.0458 |         223.6045 |          -5.5375 |
[32m[20221208 14:03:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.10
[32m[20221208 14:03:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 938.01
[32m[20221208 14:03:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 933.92
[32m[20221208 14:03:49 @agent_ppo2.py:137][0m Total time:      18.48 min
[32m[20221208 14:03:49 @agent_ppo2.py:139][0m 1509376 total steps have happened
[32m[20221208 14:03:49 @agent_ppo2.py:115][0m #------------------------ Iteration 737 --------------------------#
[32m[20221208 14:03:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:49 @agent_ppo2.py:179][0m |           0.0526 |         255.0760 |          -4.3055 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |           0.0628 |         243.3398 |          -1.2335 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |           0.0066 |         240.6523 |          -2.8785 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |          -0.0133 |         239.2507 |          -3.6715 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |          -0.0268 |         237.6375 |          -4.0610 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |          -0.0315 |         237.0573 |          -4.8963 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |          -0.0322 |         236.5155 |          -4.3657 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |          -0.0312 |         236.1048 |          -4.8753 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |          -0.0421 |         237.4627 |          -5.6002 |
[32m[20221208 14:03:50 @agent_ppo2.py:179][0m |          -0.0426 |         234.8670 |          -5.6308 |
[32m[20221208 14:03:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:03:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.71
[32m[20221208 14:03:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.94
[32m[20221208 14:03:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.85
[32m[20221208 14:03:50 @agent_ppo2.py:137][0m Total time:      18.50 min
[32m[20221208 14:03:50 @agent_ppo2.py:139][0m 1511424 total steps have happened
[32m[20221208 14:03:50 @agent_ppo2.py:115][0m #------------------------ Iteration 738 --------------------------#
[32m[20221208 14:03:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |           0.0551 |         250.1143 |          -3.1669 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |           0.0066 |         237.5165 |          -2.6422 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0032 |         227.8303 |          -3.2732 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0145 |         219.8679 |          -4.3651 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0216 |         214.2363 |          -4.5466 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0280 |         211.5458 |          -4.8039 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0349 |         208.1759 |          -5.4233 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0381 |         206.6186 |          -5.7998 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0406 |         202.3676 |          -6.3693 |
[32m[20221208 14:03:51 @agent_ppo2.py:179][0m |          -0.0425 |         201.4112 |          -6.6474 |
[32m[20221208 14:03:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 960.95
[32m[20221208 14:03:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 992.69
[32m[20221208 14:03:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.35
[32m[20221208 14:03:52 @agent_ppo2.py:137][0m Total time:      18.53 min
[32m[20221208 14:03:52 @agent_ppo2.py:139][0m 1513472 total steps have happened
[32m[20221208 14:03:52 @agent_ppo2.py:115][0m #------------------------ Iteration 739 --------------------------#
[32m[20221208 14:03:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:52 @agent_ppo2.py:179][0m |           0.0639 |         246.2900 |          -5.3691 |
[32m[20221208 14:03:52 @agent_ppo2.py:179][0m |           0.0105 |         235.6106 |          -3.7391 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0216 |         232.4528 |          -5.0083 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0351 |         230.8362 |          -5.3855 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0405 |         227.5598 |          -5.8131 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0462 |         227.6823 |          -6.0019 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0493 |         226.9007 |          -6.3980 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0501 |         225.9666 |          -6.8989 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0559 |         225.0842 |          -7.0943 |
[32m[20221208 14:03:53 @agent_ppo2.py:179][0m |          -0.0599 |         223.3456 |          -7.6520 |
[32m[20221208 14:03:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:03:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 862.07
[32m[20221208 14:03:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.86
[32m[20221208 14:03:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 925.01
[32m[20221208 14:03:53 @agent_ppo2.py:137][0m Total time:      18.55 min
[32m[20221208 14:03:53 @agent_ppo2.py:139][0m 1515520 total steps have happened
[32m[20221208 14:03:53 @agent_ppo2.py:115][0m #------------------------ Iteration 740 --------------------------#
[32m[20221208 14:03:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |           0.0539 |         242.9205 |          -4.0889 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |           0.0495 |         237.0535 |          -1.6279 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |           0.0048 |         234.1651 |          -5.0576 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |          -0.0049 |         233.9940 |          -5.4463 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |          -0.0147 |         232.9334 |          -6.1140 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |          -0.0311 |         232.9341 |          -6.6770 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |          -0.0374 |         231.1907 |          -7.2968 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |          -0.0424 |         231.2035 |          -7.5609 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |          -0.0436 |         232.4117 |          -7.7809 |
[32m[20221208 14:03:54 @agent_ppo2.py:179][0m |          -0.0342 |         230.0204 |          -7.6832 |
[32m[20221208 14:03:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 931.16
[32m[20221208 14:03:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.21
[32m[20221208 14:03:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 896.48
[32m[20221208 14:03:55 @agent_ppo2.py:137][0m Total time:      18.57 min
[32m[20221208 14:03:55 @agent_ppo2.py:139][0m 1517568 total steps have happened
[32m[20221208 14:03:55 @agent_ppo2.py:115][0m #------------------------ Iteration 741 --------------------------#
[32m[20221208 14:03:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:03:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:55 @agent_ppo2.py:179][0m |           0.0564 |         214.6089 |          -5.4161 |
[32m[20221208 14:03:55 @agent_ppo2.py:179][0m |           0.0252 |         207.2508 |          -3.2520 |
[32m[20221208 14:03:55 @agent_ppo2.py:179][0m |          -0.0310 |         205.4171 |          -4.3166 |
[32m[20221208 14:03:56 @agent_ppo2.py:179][0m |          -0.0506 |         204.8618 |          -5.4435 |
[32m[20221208 14:03:56 @agent_ppo2.py:179][0m |          -0.0625 |         203.5108 |          -6.4345 |
[32m[20221208 14:03:56 @agent_ppo2.py:179][0m |          -0.0651 |         203.2110 |          -7.4145 |
[32m[20221208 14:03:56 @agent_ppo2.py:179][0m |          -0.0730 |         202.1893 |          -8.1323 |
[32m[20221208 14:03:56 @agent_ppo2.py:179][0m |          -0.0694 |         201.7673 |          -8.1870 |
[32m[20221208 14:03:56 @agent_ppo2.py:179][0m |          -0.0754 |         201.1617 |          -8.4375 |
[32m[20221208 14:03:56 @agent_ppo2.py:179][0m |          -0.0741 |         201.3527 |          -9.3541 |
[32m[20221208 14:03:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 652.35
[32m[20221208 14:03:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 937.52
[32m[20221208 14:03:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 910.51
[32m[20221208 14:03:56 @agent_ppo2.py:137][0m Total time:      18.60 min
[32m[20221208 14:03:56 @agent_ppo2.py:139][0m 1519616 total steps have happened
[32m[20221208 14:03:56 @agent_ppo2.py:115][0m #------------------------ Iteration 742 --------------------------#
[32m[20221208 14:03:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |           0.0406 |         246.8041 |          -8.1180 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |           0.0166 |         237.8101 |          -7.4121 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |           0.0032 |         235.9557 |          -7.1550 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |          -0.0076 |         234.0218 |          -8.1872 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |          -0.0249 |         233.1264 |          -9.9859 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |          -0.0352 |         232.8573 |         -10.6671 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |          -0.0435 |         231.6682 |         -11.6972 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |          -0.0442 |         233.1377 |         -11.6178 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |          -0.0479 |         230.6422 |         -12.4992 |
[32m[20221208 14:03:57 @agent_ppo2.py:179][0m |          -0.0509 |         231.2393 |         -13.1480 |
[32m[20221208 14:03:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:03:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.60
[32m[20221208 14:03:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.04
[32m[20221208 14:03:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.54
[32m[20221208 14:03:58 @agent_ppo2.py:137][0m Total time:      18.62 min
[32m[20221208 14:03:58 @agent_ppo2.py:139][0m 1521664 total steps have happened
[32m[20221208 14:03:58 @agent_ppo2.py:115][0m #------------------------ Iteration 743 --------------------------#
[32m[20221208 14:03:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:03:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:03:58 @agent_ppo2.py:179][0m |           0.1150 |         233.0485 |          -9.3262 |
[32m[20221208 14:03:58 @agent_ppo2.py:179][0m |           0.0838 |         226.5439 |          -3.1007 |
[32m[20221208 14:03:58 @agent_ppo2.py:179][0m |           0.0298 |         222.8353 |          -7.1264 |
[32m[20221208 14:03:59 @agent_ppo2.py:179][0m |          -0.0022 |         220.7328 |         -11.5484 |
[32m[20221208 14:03:59 @agent_ppo2.py:179][0m |          -0.0169 |         218.4488 |         -12.4962 |
[32m[20221208 14:03:59 @agent_ppo2.py:179][0m |          -0.0242 |         218.1996 |         -13.3075 |
[32m[20221208 14:03:59 @agent_ppo2.py:179][0m |          -0.0277 |         216.3968 |         -13.2052 |
[32m[20221208 14:03:59 @agent_ppo2.py:179][0m |          -0.0351 |         215.7905 |         -13.5040 |
[32m[20221208 14:03:59 @agent_ppo2.py:179][0m |          -0.0352 |         215.0975 |         -14.1858 |
[32m[20221208 14:03:59 @agent_ppo2.py:179][0m |          -0.0362 |         214.8304 |         -13.8922 |
[32m[20221208 14:03:59 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:03:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 903.18
[32m[20221208 14:03:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.49
[32m[20221208 14:03:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 980.32
[32m[20221208 14:03:59 @agent_ppo2.py:137][0m Total time:      18.65 min
[32m[20221208 14:03:59 @agent_ppo2.py:139][0m 1523712 total steps have happened
[32m[20221208 14:03:59 @agent_ppo2.py:115][0m #------------------------ Iteration 744 --------------------------#
[32m[20221208 14:04:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |           0.0726 |         242.2870 |          -9.2699 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |           0.0198 |         230.6921 |          -8.1842 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |           0.0014 |         224.0210 |          -9.9154 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |          -0.0183 |         217.0723 |         -10.4194 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |          -0.0295 |         211.2604 |         -11.6562 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |          -0.0383 |         207.3817 |         -12.2169 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |          -0.0423 |         204.8292 |         -12.6393 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |          -0.0433 |         204.4933 |         -13.2588 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |          -0.0440 |         202.4448 |         -13.4652 |
[32m[20221208 14:04:00 @agent_ppo2.py:179][0m |          -0.0455 |         199.9560 |         -13.5076 |
[32m[20221208 14:04:00 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 888.20
[32m[20221208 14:04:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.65
[32m[20221208 14:04:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.77
[32m[20221208 14:04:01 @agent_ppo2.py:137][0m Total time:      18.67 min
[32m[20221208 14:04:01 @agent_ppo2.py:139][0m 1525760 total steps have happened
[32m[20221208 14:04:01 @agent_ppo2.py:115][0m #------------------------ Iteration 745 --------------------------#
[32m[20221208 14:04:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:01 @agent_ppo2.py:179][0m |           0.0687 |         211.4330 |          -8.9345 |
[32m[20221208 14:04:01 @agent_ppo2.py:179][0m |           0.0937 |         176.6023 |          -4.4676 |
[32m[20221208 14:04:01 @agent_ppo2.py:179][0m |           0.0322 |         164.0699 |          -5.3860 |
[32m[20221208 14:04:01 @agent_ppo2.py:179][0m |           0.0005 |         156.8956 |          -6.8600 |
[32m[20221208 14:04:02 @agent_ppo2.py:179][0m |          -0.0159 |         151.6043 |          -7.7683 |
[32m[20221208 14:04:02 @agent_ppo2.py:179][0m |          -0.0288 |         147.2089 |          -8.1687 |
[32m[20221208 14:04:02 @agent_ppo2.py:179][0m |          -0.0378 |         144.2553 |          -8.7854 |
[32m[20221208 14:04:02 @agent_ppo2.py:179][0m |          -0.0411 |         141.5490 |          -8.8708 |
[32m[20221208 14:04:02 @agent_ppo2.py:179][0m |          -0.0477 |         138.8763 |          -9.5153 |
[32m[20221208 14:04:02 @agent_ppo2.py:179][0m |          -0.0520 |         135.5160 |          -9.9267 |
[32m[20221208 14:04:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 761.26
[32m[20221208 14:04:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 897.32
[32m[20221208 14:04:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 759.66
[32m[20221208 14:04:02 @agent_ppo2.py:137][0m Total time:      18.70 min
[32m[20221208 14:04:02 @agent_ppo2.py:139][0m 1527808 total steps have happened
[32m[20221208 14:04:02 @agent_ppo2.py:115][0m #------------------------ Iteration 746 --------------------------#
[32m[20221208 14:04:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |           0.0475 |         268.5663 |         -13.9195 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |           0.0191 |         246.8005 |         -12.9409 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0139 |         241.5397 |         -14.2596 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0256 |         239.8727 |         -13.3483 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0376 |         237.6199 |         -14.1246 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0420 |         235.9320 |         -14.6612 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0442 |         235.1857 |         -14.3421 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0487 |         233.6768 |         -14.8043 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0516 |         232.0189 |         -15.6708 |
[32m[20221208 14:04:03 @agent_ppo2.py:179][0m |          -0.0530 |         230.9397 |         -15.7730 |
[32m[20221208 14:04:03 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:04:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 880.89
[32m[20221208 14:04:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 932.94
[32m[20221208 14:04:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 665.29
[32m[20221208 14:04:04 @agent_ppo2.py:137][0m Total time:      18.72 min
[32m[20221208 14:04:04 @agent_ppo2.py:139][0m 1529856 total steps have happened
[32m[20221208 14:04:04 @agent_ppo2.py:115][0m #------------------------ Iteration 747 --------------------------#
[32m[20221208 14:04:04 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:04 @agent_ppo2.py:179][0m |           0.0487 |         214.7357 |         -13.6969 |
[32m[20221208 14:04:04 @agent_ppo2.py:179][0m |           0.0320 |         204.1874 |         -11.6266 |
[32m[20221208 14:04:04 @agent_ppo2.py:179][0m |           0.0081 |         197.7845 |         -11.8667 |
[32m[20221208 14:04:04 @agent_ppo2.py:179][0m |          -0.0137 |         194.3179 |         -14.1855 |
[32m[20221208 14:04:05 @agent_ppo2.py:179][0m |          -0.0271 |         189.9085 |         -14.8446 |
[32m[20221208 14:04:05 @agent_ppo2.py:179][0m |          -0.0377 |         188.1237 |         -16.1854 |
[32m[20221208 14:04:05 @agent_ppo2.py:179][0m |          -0.0428 |         187.0548 |         -16.5803 |
[32m[20221208 14:04:05 @agent_ppo2.py:179][0m |          -0.0476 |         184.7180 |         -17.4484 |
[32m[20221208 14:04:05 @agent_ppo2.py:179][0m |          -0.0439 |         184.2355 |         -17.5579 |
[32m[20221208 14:04:05 @agent_ppo2.py:179][0m |          -0.0489 |         181.8951 |         -17.8779 |
[32m[20221208 14:04:05 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:04:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 912.08
[32m[20221208 14:04:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.36
[32m[20221208 14:04:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 925.17
[32m[20221208 14:04:05 @agent_ppo2.py:137][0m Total time:      18.75 min
[32m[20221208 14:04:05 @agent_ppo2.py:139][0m 1531904 total steps have happened
[32m[20221208 14:04:05 @agent_ppo2.py:115][0m #------------------------ Iteration 748 --------------------------#
[32m[20221208 14:04:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |           0.0765 |         240.7655 |         -14.9429 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |           0.0451 |         230.9215 |         -11.6895 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |           0.0063 |         228.0140 |         -13.8909 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |          -0.0001 |         224.9124 |         -12.3480 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |          -0.0190 |         223.5829 |         -14.3411 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |          -0.0288 |         221.9694 |         -15.0218 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |          -0.0349 |         220.8309 |         -16.7624 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |          -0.0322 |         219.9027 |         -16.3502 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |          -0.0408 |         217.5506 |         -16.7931 |
[32m[20221208 14:04:06 @agent_ppo2.py:179][0m |          -0.0404 |         217.1642 |         -17.1064 |
[32m[20221208 14:04:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 930.28
[32m[20221208 14:04:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.64
[32m[20221208 14:04:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 905.40
[32m[20221208 14:04:07 @agent_ppo2.py:137][0m Total time:      18.77 min
[32m[20221208 14:04:07 @agent_ppo2.py:139][0m 1533952 total steps have happened
[32m[20221208 14:04:07 @agent_ppo2.py:115][0m #------------------------ Iteration 749 --------------------------#
[32m[20221208 14:04:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:07 @agent_ppo2.py:179][0m |           0.0632 |         239.4836 |          -9.9244 |
[32m[20221208 14:04:07 @agent_ppo2.py:179][0m |           0.0805 |         230.8926 |          -7.4086 |
[32m[20221208 14:04:07 @agent_ppo2.py:179][0m |           0.0144 |         226.6153 |          -9.4585 |
[32m[20221208 14:04:07 @agent_ppo2.py:179][0m |          -0.0064 |         222.6229 |         -12.6273 |
[32m[20221208 14:04:07 @agent_ppo2.py:179][0m |          -0.0110 |         220.7376 |         -12.1047 |
[32m[20221208 14:04:08 @agent_ppo2.py:179][0m |          -0.0256 |         218.0948 |         -13.7320 |
[32m[20221208 14:04:08 @agent_ppo2.py:179][0m |          -0.0319 |         217.1411 |         -14.0035 |
[32m[20221208 14:04:08 @agent_ppo2.py:179][0m |          -0.0398 |         215.8626 |         -15.2167 |
[32m[20221208 14:04:08 @agent_ppo2.py:179][0m |          -0.0420 |         214.1481 |         -15.5607 |
[32m[20221208 14:04:08 @agent_ppo2.py:179][0m |          -0.0427 |         212.9886 |         -16.0087 |
[32m[20221208 14:04:08 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:04:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 927.65
[32m[20221208 14:04:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.40
[32m[20221208 14:04:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 881.30
[32m[20221208 14:04:08 @agent_ppo2.py:137][0m Total time:      18.80 min
[32m[20221208 14:04:08 @agent_ppo2.py:139][0m 1536000 total steps have happened
[32m[20221208 14:04:08 @agent_ppo2.py:115][0m #------------------------ Iteration 750 --------------------------#
[32m[20221208 14:04:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:04:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |           0.0515 |         227.9822 |         -14.4128 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |           0.0813 |         210.9763 |          -4.5177 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |           0.0502 |         202.1414 |          -5.9190 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |           0.0210 |         196.3343 |          -9.9581 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |          -0.0006 |         193.8733 |         -11.7715 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |          -0.0165 |         191.2661 |         -13.6659 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |          -0.0257 |         190.5021 |         -14.4197 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |          -0.0308 |         188.1396 |         -15.6553 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |          -0.0375 |         187.5231 |         -16.1573 |
[32m[20221208 14:04:09 @agent_ppo2.py:179][0m |          -0.0383 |         185.5014 |         -16.8752 |
[32m[20221208 14:04:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 952.48
[32m[20221208 14:04:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.57
[32m[20221208 14:04:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.85
[32m[20221208 14:04:10 @agent_ppo2.py:137][0m Total time:      18.82 min
[32m[20221208 14:04:10 @agent_ppo2.py:139][0m 1538048 total steps have happened
[32m[20221208 14:04:10 @agent_ppo2.py:115][0m #------------------------ Iteration 751 --------------------------#
[32m[20221208 14:04:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:10 @agent_ppo2.py:179][0m |           0.1319 |         234.8421 |          -7.8945 |
[32m[20221208 14:04:10 @agent_ppo2.py:179][0m |           0.0963 |         216.0407 |          -2.7034 |
[32m[20221208 14:04:10 @agent_ppo2.py:179][0m |           0.0582 |         209.2157 |          -2.9697 |
[32m[20221208 14:04:10 @agent_ppo2.py:179][0m |           0.0184 |         205.6678 |          -8.0785 |
[32m[20221208 14:04:10 @agent_ppo2.py:179][0m |           0.0002 |         203.4292 |         -11.2422 |
[32m[20221208 14:04:11 @agent_ppo2.py:179][0m |          -0.0104 |         201.3494 |         -11.7692 |
[32m[20221208 14:04:11 @agent_ppo2.py:179][0m |          -0.0205 |         198.6131 |         -12.5273 |
[32m[20221208 14:04:11 @agent_ppo2.py:179][0m |          -0.0298 |         198.5578 |         -13.2558 |
[32m[20221208 14:04:11 @agent_ppo2.py:179][0m |          -0.0349 |         195.5305 |         -13.8015 |
[32m[20221208 14:04:11 @agent_ppo2.py:179][0m |          -0.0374 |         193.5089 |         -14.7162 |
[32m[20221208 14:04:11 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 891.22
[32m[20221208 14:04:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.19
[32m[20221208 14:04:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 968.71
[32m[20221208 14:04:11 @agent_ppo2.py:137][0m Total time:      18.85 min
[32m[20221208 14:04:11 @agent_ppo2.py:139][0m 1540096 total steps have happened
[32m[20221208 14:04:11 @agent_ppo2.py:115][0m #------------------------ Iteration 752 --------------------------#
[32m[20221208 14:04:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |           0.0985 |         256.5952 |          -9.2868 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |           0.0596 |         250.2970 |          -6.0766 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |           0.0210 |         248.2200 |          -9.4184 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |          -0.0059 |         241.6249 |         -10.7064 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |          -0.0233 |         238.9472 |         -12.2397 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |          -0.0322 |         239.1700 |         -13.2011 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |          -0.0385 |         236.0521 |         -13.7314 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |          -0.0416 |         234.7420 |         -14.3456 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |          -0.0436 |         233.2198 |         -14.6650 |
[32m[20221208 14:04:12 @agent_ppo2.py:179][0m |          -0.0461 |         232.5632 |         -14.6732 |
[32m[20221208 14:04:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 899.00
[32m[20221208 14:04:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.82
[32m[20221208 14:04:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 896.41
[32m[20221208 14:04:13 @agent_ppo2.py:137][0m Total time:      18.87 min
[32m[20221208 14:04:13 @agent_ppo2.py:139][0m 1542144 total steps have happened
[32m[20221208 14:04:13 @agent_ppo2.py:115][0m #------------------------ Iteration 753 --------------------------#
[32m[20221208 14:04:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:13 @agent_ppo2.py:179][0m |           0.0483 |         241.3569 |         -14.4950 |
[32m[20221208 14:04:13 @agent_ppo2.py:179][0m |           0.0193 |         233.1721 |         -11.9600 |
[32m[20221208 14:04:13 @agent_ppo2.py:179][0m |           0.0043 |         227.2517 |         -12.8673 |
[32m[20221208 14:04:13 @agent_ppo2.py:179][0m |          -0.0192 |         224.2304 |         -14.9402 |
[32m[20221208 14:04:13 @agent_ppo2.py:179][0m |          -0.0268 |         220.6873 |         -16.6367 |
[32m[20221208 14:04:13 @agent_ppo2.py:179][0m |          -0.0307 |         218.8687 |         -17.3535 |
[32m[20221208 14:04:14 @agent_ppo2.py:179][0m |          -0.0352 |         216.9442 |         -17.6108 |
[32m[20221208 14:04:14 @agent_ppo2.py:179][0m |          -0.0408 |         215.7639 |         -18.7534 |
[32m[20221208 14:04:14 @agent_ppo2.py:179][0m |          -0.0426 |         214.5515 |         -19.7223 |
[32m[20221208 14:04:14 @agent_ppo2.py:179][0m |          -0.0449 |         214.1282 |         -20.3843 |
[32m[20221208 14:04:14 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.79
[32m[20221208 14:04:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 944.88
[32m[20221208 14:04:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.36
[32m[20221208 14:04:14 @agent_ppo2.py:137][0m Total time:      18.90 min
[32m[20221208 14:04:14 @agent_ppo2.py:139][0m 1544192 total steps have happened
[32m[20221208 14:04:14 @agent_ppo2.py:115][0m #------------------------ Iteration 754 --------------------------#
[32m[20221208 14:04:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |           0.0882 |         246.9999 |         -15.0275 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |           0.0598 |         229.3161 |         -12.9232 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |           0.0141 |         223.1602 |         -16.4799 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |          -0.0083 |         219.4203 |         -18.2952 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |          -0.0318 |         218.0055 |         -19.5989 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |          -0.0371 |         217.7303 |         -20.2391 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |          -0.0437 |         216.1897 |         -20.7907 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |          -0.0453 |         216.2328 |         -21.2768 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |          -0.0513 |         214.6290 |         -21.6552 |
[32m[20221208 14:04:15 @agent_ppo2.py:179][0m |          -0.0532 |         215.1434 |         -21.8483 |
[32m[20221208 14:04:15 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 700.19
[32m[20221208 14:04:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.07
[32m[20221208 14:04:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.01
[32m[20221208 14:04:16 @agent_ppo2.py:137][0m Total time:      18.92 min
[32m[20221208 14:04:16 @agent_ppo2.py:139][0m 1546240 total steps have happened
[32m[20221208 14:04:16 @agent_ppo2.py:115][0m #------------------------ Iteration 755 --------------------------#
[32m[20221208 14:04:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:16 @agent_ppo2.py:179][0m |           0.0621 |         232.8631 |         -17.3894 |
[32m[20221208 14:04:16 @agent_ppo2.py:179][0m |           0.0586 |         224.3623 |         -13.7015 |
[32m[20221208 14:04:16 @agent_ppo2.py:179][0m |           0.0333 |         221.3037 |         -12.6566 |
[32m[20221208 14:04:16 @agent_ppo2.py:179][0m |          -0.0102 |         218.6702 |         -15.5937 |
[32m[20221208 14:04:16 @agent_ppo2.py:179][0m |          -0.0273 |         217.3736 |         -17.9107 |
[32m[20221208 14:04:16 @agent_ppo2.py:179][0m |          -0.0369 |         215.3191 |         -18.5344 |
[32m[20221208 14:04:16 @agent_ppo2.py:179][0m |          -0.0417 |         212.6305 |         -19.1676 |
[32m[20221208 14:04:17 @agent_ppo2.py:179][0m |          -0.0454 |         209.4055 |         -19.6628 |
[32m[20221208 14:04:17 @agent_ppo2.py:179][0m |          -0.0473 |         207.6661 |         -20.4217 |
[32m[20221208 14:04:17 @agent_ppo2.py:179][0m |          -0.0479 |         208.3197 |         -20.8476 |
[32m[20221208 14:04:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 910.16
[32m[20221208 14:04:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.61
[32m[20221208 14:04:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 973.33
[32m[20221208 14:04:17 @agent_ppo2.py:137][0m Total time:      18.95 min
[32m[20221208 14:04:17 @agent_ppo2.py:139][0m 1548288 total steps have happened
[32m[20221208 14:04:17 @agent_ppo2.py:115][0m #------------------------ Iteration 756 --------------------------#
[32m[20221208 14:04:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |           0.0592 |         213.5932 |         -17.0572 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |           0.0665 |         205.7773 |         -13.3885 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |           0.0115 |         202.2437 |         -16.1599 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |          -0.0011 |         200.6679 |         -16.7498 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |          -0.0163 |         198.7635 |         -18.5390 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |          -0.0296 |         197.2218 |         -20.1786 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |          -0.0332 |         196.2508 |         -20.4479 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |          -0.0365 |         195.3966 |         -20.8400 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |          -0.0367 |         194.1762 |         -20.7857 |
[32m[20221208 14:04:18 @agent_ppo2.py:179][0m |          -0.0386 |         194.3309 |         -21.5111 |
[32m[20221208 14:04:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.89
[32m[20221208 14:04:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.95
[32m[20221208 14:04:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 902.68
[32m[20221208 14:04:19 @agent_ppo2.py:137][0m Total time:      18.97 min
[32m[20221208 14:04:19 @agent_ppo2.py:139][0m 1550336 total steps have happened
[32m[20221208 14:04:19 @agent_ppo2.py:115][0m #------------------------ Iteration 757 --------------------------#
[32m[20221208 14:04:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:19 @agent_ppo2.py:179][0m |           0.2195 |         209.3687 |         -15.0700 |
[32m[20221208 14:04:19 @agent_ppo2.py:179][0m |           0.0066 |         191.0486 |         -11.6303 |
[32m[20221208 14:04:19 @agent_ppo2.py:179][0m |          -0.0295 |         185.9954 |         -14.8834 |
[32m[20221208 14:04:19 @agent_ppo2.py:179][0m |          -0.0465 |         184.5502 |         -16.1642 |
[32m[20221208 14:04:19 @agent_ppo2.py:179][0m |          -0.0541 |         183.1952 |         -16.8359 |
[32m[20221208 14:04:19 @agent_ppo2.py:179][0m |          -0.0587 |         180.8825 |         -17.3933 |
[32m[20221208 14:04:19 @agent_ppo2.py:179][0m |          -0.0636 |         179.9101 |         -18.4564 |
[32m[20221208 14:04:20 @agent_ppo2.py:179][0m |          -0.0706 |         179.7020 |         -18.7946 |
[32m[20221208 14:04:20 @agent_ppo2.py:179][0m |          -0.0751 |         180.6538 |         -19.8976 |
[32m[20221208 14:04:20 @agent_ppo2.py:179][0m |          -0.0742 |         179.2225 |         -20.0453 |
[32m[20221208 14:04:20 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:04:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 681.56
[32m[20221208 14:04:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.92
[32m[20221208 14:04:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 784.08
[32m[20221208 14:04:20 @agent_ppo2.py:137][0m Total time:      18.99 min
[32m[20221208 14:04:20 @agent_ppo2.py:139][0m 1552384 total steps have happened
[32m[20221208 14:04:20 @agent_ppo2.py:115][0m #------------------------ Iteration 758 --------------------------#
[32m[20221208 14:04:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:04:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |           0.0770 |         253.1602 |         -18.2318 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |           0.0575 |         245.1616 |         -14.5797 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |           0.0125 |         239.3437 |         -18.2465 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |          -0.0138 |         236.2145 |         -19.9387 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |          -0.0241 |         234.2488 |         -20.4217 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |          -0.0347 |         230.7868 |         -21.0618 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |          -0.0409 |         228.2592 |         -22.4664 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |          -0.0447 |         227.5629 |         -22.7742 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |          -0.0458 |         227.0643 |         -23.4202 |
[32m[20221208 14:04:21 @agent_ppo2.py:179][0m |          -0.0475 |         225.4935 |         -23.8681 |
[32m[20221208 14:04:21 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:04:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.38
[32m[20221208 14:04:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.92
[32m[20221208 14:04:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 876.43
[32m[20221208 14:04:22 @agent_ppo2.py:137][0m Total time:      19.02 min
[32m[20221208 14:04:22 @agent_ppo2.py:139][0m 1554432 total steps have happened
[32m[20221208 14:04:22 @agent_ppo2.py:115][0m #------------------------ Iteration 759 --------------------------#
[32m[20221208 14:04:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:04:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:22 @agent_ppo2.py:179][0m |           0.0481 |         235.9660 |         -17.9665 |
[32m[20221208 14:04:22 @agent_ppo2.py:179][0m |           0.0787 |         226.7869 |         -12.4776 |
[32m[20221208 14:04:22 @agent_ppo2.py:179][0m |           0.0283 |         223.2882 |         -15.1350 |
[32m[20221208 14:04:22 @agent_ppo2.py:179][0m |          -0.0070 |         221.5807 |         -18.6475 |
[32m[20221208 14:04:22 @agent_ppo2.py:179][0m |          -0.0129 |         220.2728 |         -19.4398 |
[32m[20221208 14:04:22 @agent_ppo2.py:179][0m |          -0.0249 |         220.4181 |         -21.2333 |
[32m[20221208 14:04:23 @agent_ppo2.py:179][0m |          -0.0313 |         217.7082 |         -22.0038 |
[32m[20221208 14:04:23 @agent_ppo2.py:179][0m |          -0.0354 |         217.1282 |         -22.9274 |
[32m[20221208 14:04:23 @agent_ppo2.py:179][0m |          -0.0383 |         216.9592 |         -23.8307 |
[32m[20221208 14:04:23 @agent_ppo2.py:179][0m |          -0.0418 |         215.8322 |         -23.6021 |
[32m[20221208 14:04:23 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:04:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.02
[32m[20221208 14:04:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.47
[32m[20221208 14:04:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.95
[32m[20221208 14:04:23 @agent_ppo2.py:137][0m Total time:      19.05 min
[32m[20221208 14:04:23 @agent_ppo2.py:139][0m 1556480 total steps have happened
[32m[20221208 14:04:23 @agent_ppo2.py:115][0m #------------------------ Iteration 760 --------------------------#
[32m[20221208 14:04:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:04:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |           0.0509 |         255.8356 |         -20.5549 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |           0.0630 |         249.0057 |         -15.7725 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |           0.0243 |         246.9210 |         -15.4479 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |          -0.0058 |         244.4303 |         -21.1371 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |          -0.0205 |         243.8649 |         -22.3039 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |          -0.0285 |         241.4175 |         -22.9911 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |          -0.0353 |         240.7699 |         -24.1609 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |          -0.0384 |         239.6413 |         -25.0590 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |          -0.0410 |         239.1202 |         -25.8926 |
[32m[20221208 14:04:24 @agent_ppo2.py:179][0m |          -0.0444 |         239.4394 |         -26.6404 |
[32m[20221208 14:04:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:04:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 959.56
[32m[20221208 14:04:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.08
[32m[20221208 14:04:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.62
[32m[20221208 14:04:25 @agent_ppo2.py:137][0m Total time:      19.07 min
[32m[20221208 14:04:25 @agent_ppo2.py:139][0m 1558528 total steps have happened
[32m[20221208 14:04:25 @agent_ppo2.py:115][0m #------------------------ Iteration 761 --------------------------#
[32m[20221208 14:04:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:04:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:25 @agent_ppo2.py:179][0m |           0.0809 |         244.5751 |         -17.8759 |
[32m[20221208 14:04:25 @agent_ppo2.py:179][0m |           0.0387 |         240.3021 |         -14.2032 |
[32m[20221208 14:04:25 @agent_ppo2.py:179][0m |           0.0015 |         236.8632 |         -17.8962 |
[32m[20221208 14:04:25 @agent_ppo2.py:179][0m |          -0.0167 |         234.5482 |         -20.3390 |
[32m[20221208 14:04:26 @agent_ppo2.py:179][0m |          -0.0243 |         233.4541 |         -22.0645 |
[32m[20221208 14:04:26 @agent_ppo2.py:179][0m |          -0.0317 |         232.4361 |         -22.9447 |
[32m[20221208 14:04:26 @agent_ppo2.py:179][0m |          -0.0376 |         230.9476 |         -23.1254 |
[32m[20221208 14:04:26 @agent_ppo2.py:179][0m |          -0.0349 |         230.3959 |         -24.4771 |
[32m[20221208 14:04:26 @agent_ppo2.py:179][0m |          -0.0376 |         230.3507 |         -24.6581 |
[32m[20221208 14:04:26 @agent_ppo2.py:179][0m |          -0.0426 |         228.1203 |         -25.3794 |
[32m[20221208 14:04:26 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:04:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 929.03
[32m[20221208 14:04:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.98
[32m[20221208 14:04:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 645.04
[32m[20221208 14:04:26 @agent_ppo2.py:137][0m Total time:      19.10 min
[32m[20221208 14:04:26 @agent_ppo2.py:139][0m 1560576 total steps have happened
[32m[20221208 14:04:26 @agent_ppo2.py:115][0m #------------------------ Iteration 762 --------------------------#
[32m[20221208 14:04:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |           0.0454 |         255.4888 |         -19.3358 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |           0.0210 |         248.2984 |         -17.9845 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0013 |         246.0330 |         -18.8956 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0200 |         246.7359 |         -21.9033 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0260 |         243.9730 |         -21.7671 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0318 |         243.1468 |         -23.1580 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0380 |         241.3934 |         -23.8538 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0417 |         241.0769 |         -25.3345 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0451 |         240.7432 |         -25.3078 |
[32m[20221208 14:04:27 @agent_ppo2.py:179][0m |          -0.0467 |         240.4254 |         -26.0809 |
[32m[20221208 14:04:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.75
[32m[20221208 14:04:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.39
[32m[20221208 14:04:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 775.74
[32m[20221208 14:04:28 @agent_ppo2.py:137][0m Total time:      19.12 min
[32m[20221208 14:04:28 @agent_ppo2.py:139][0m 1562624 total steps have happened
[32m[20221208 14:04:28 @agent_ppo2.py:115][0m #------------------------ Iteration 763 --------------------------#
[32m[20221208 14:04:28 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:28 @agent_ppo2.py:179][0m |           0.1130 |         250.5251 |         -19.7253 |
[32m[20221208 14:04:28 @agent_ppo2.py:179][0m |           0.0759 |         244.8889 |         -11.3779 |
[32m[20221208 14:04:28 @agent_ppo2.py:179][0m |           0.0177 |         243.9802 |         -15.8965 |
[32m[20221208 14:04:28 @agent_ppo2.py:179][0m |          -0.0088 |         241.0701 |         -20.0843 |
[32m[20221208 14:04:28 @agent_ppo2.py:179][0m |          -0.0133 |         239.5847 |         -21.4985 |
[32m[20221208 14:04:29 @agent_ppo2.py:179][0m |          -0.0235 |         238.9563 |         -23.1566 |
[32m[20221208 14:04:29 @agent_ppo2.py:179][0m |          -0.0287 |         237.2223 |         -23.0547 |
[32m[20221208 14:04:29 @agent_ppo2.py:179][0m |          -0.0359 |         236.8909 |         -23.4520 |
[32m[20221208 14:04:29 @agent_ppo2.py:179][0m |          -0.0410 |         236.0548 |         -24.3245 |
[32m[20221208 14:04:29 @agent_ppo2.py:179][0m |          -0.0439 |         235.6284 |         -25.1508 |
[32m[20221208 14:04:29 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 885.90
[32m[20221208 14:04:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.15
[32m[20221208 14:04:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.54
[32m[20221208 14:04:29 @agent_ppo2.py:137][0m Total time:      19.15 min
[32m[20221208 14:04:29 @agent_ppo2.py:139][0m 1564672 total steps have happened
[32m[20221208 14:04:29 @agent_ppo2.py:115][0m #------------------------ Iteration 764 --------------------------#
[32m[20221208 14:04:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |           0.0153 |         246.4553 |         -23.1826 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |           0.0200 |         240.0942 |         -19.8468 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |           0.0097 |         237.8544 |         -18.9471 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |          -0.0165 |         234.7797 |         -22.1320 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |          -0.0233 |         233.0655 |         -23.4812 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |          -0.0272 |         233.5333 |         -24.0306 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |          -0.0322 |         231.2620 |         -25.4567 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |          -0.0345 |         230.6752 |         -24.7923 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |          -0.0338 |         229.4721 |         -25.2562 |
[32m[20221208 14:04:30 @agent_ppo2.py:179][0m |          -0.0403 |         229.0334 |         -26.5757 |
[32m[20221208 14:04:30 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 974.45
[32m[20221208 14:04:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.40
[32m[20221208 14:04:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.31
[32m[20221208 14:04:31 @agent_ppo2.py:137][0m Total time:      19.17 min
[32m[20221208 14:04:31 @agent_ppo2.py:139][0m 1566720 total steps have happened
[32m[20221208 14:04:31 @agent_ppo2.py:115][0m #------------------------ Iteration 765 --------------------------#
[32m[20221208 14:04:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:31 @agent_ppo2.py:179][0m |           0.0554 |         251.7946 |         -18.9904 |
[32m[20221208 14:04:31 @agent_ppo2.py:179][0m |           0.0384 |         238.0861 |         -15.3850 |
[32m[20221208 14:04:31 @agent_ppo2.py:179][0m |           0.0126 |         231.5012 |         -16.8447 |
[32m[20221208 14:04:31 @agent_ppo2.py:179][0m |          -0.0117 |         227.0240 |         -19.1507 |
[32m[20221208 14:04:31 @agent_ppo2.py:179][0m |          -0.0289 |         225.1902 |         -21.2008 |
[32m[20221208 14:04:31 @agent_ppo2.py:179][0m |          -0.0356 |         224.4248 |         -22.0464 |
[32m[20221208 14:04:32 @agent_ppo2.py:179][0m |          -0.0393 |         225.2010 |         -22.6926 |
[32m[20221208 14:04:32 @agent_ppo2.py:179][0m |          -0.0409 |         222.4186 |         -23.5403 |
[32m[20221208 14:04:32 @agent_ppo2.py:179][0m |          -0.0458 |         222.6933 |         -23.6342 |
[32m[20221208 14:04:32 @agent_ppo2.py:179][0m |          -0.0437 |         221.9614 |         -23.8570 |
[32m[20221208 14:04:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 880.42
[32m[20221208 14:04:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 907.65
[32m[20221208 14:04:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.44
[32m[20221208 14:04:32 @agent_ppo2.py:137][0m Total time:      19.20 min
[32m[20221208 14:04:32 @agent_ppo2.py:139][0m 1568768 total steps have happened
[32m[20221208 14:04:32 @agent_ppo2.py:115][0m #------------------------ Iteration 766 --------------------------#
[32m[20221208 14:04:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |           0.0441 |         242.2146 |         -21.6737 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |           0.0239 |         232.0168 |         -18.1470 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0030 |         227.5474 |         -20.5793 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0179 |         225.6693 |         -22.3086 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0274 |         224.2858 |         -23.4735 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0347 |         223.7320 |         -24.3319 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0367 |         222.3453 |         -24.9506 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0397 |         220.4020 |         -25.4930 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0425 |         218.5044 |         -25.8604 |
[32m[20221208 14:04:33 @agent_ppo2.py:179][0m |          -0.0425 |         217.8666 |         -26.8183 |
[32m[20221208 14:04:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.27
[32m[20221208 14:04:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.87
[32m[20221208 14:04:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 964.73
[32m[20221208 14:04:34 @agent_ppo2.py:137][0m Total time:      19.22 min
[32m[20221208 14:04:34 @agent_ppo2.py:139][0m 1570816 total steps have happened
[32m[20221208 14:04:34 @agent_ppo2.py:115][0m #------------------------ Iteration 767 --------------------------#
[32m[20221208 14:04:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:34 @agent_ppo2.py:179][0m |           0.4593 |         207.2745 |         -19.3814 |
[32m[20221208 14:04:34 @agent_ppo2.py:179][0m |           0.0651 |         198.8312 |         -11.2287 |
[32m[20221208 14:04:34 @agent_ppo2.py:179][0m |           0.0043 |         195.3499 |         -16.0004 |
[32m[20221208 14:04:34 @agent_ppo2.py:179][0m |          -0.0214 |         191.7671 |         -17.5631 |
[32m[20221208 14:04:34 @agent_ppo2.py:179][0m |          -0.0295 |         189.9630 |         -19.3502 |
[32m[20221208 14:04:34 @agent_ppo2.py:179][0m |          -0.0425 |         188.3459 |         -20.1644 |
[32m[20221208 14:04:34 @agent_ppo2.py:179][0m |          -0.0477 |         187.2188 |         -21.7886 |
[32m[20221208 14:04:35 @agent_ppo2.py:179][0m |          -0.0516 |         187.9576 |         -22.3665 |
[32m[20221208 14:04:35 @agent_ppo2.py:179][0m |          -0.0557 |         185.4929 |         -23.1703 |
[32m[20221208 14:04:35 @agent_ppo2.py:179][0m |          -0.0593 |         184.5745 |         -24.6458 |
[32m[20221208 14:04:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 763.22
[32m[20221208 14:04:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.21
[32m[20221208 14:04:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.48
[32m[20221208 14:04:35 @agent_ppo2.py:137][0m Total time:      19.25 min
[32m[20221208 14:04:35 @agent_ppo2.py:139][0m 1572864 total steps have happened
[32m[20221208 14:04:35 @agent_ppo2.py:115][0m #------------------------ Iteration 768 --------------------------#
[32m[20221208 14:04:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |           0.0451 |         245.0991 |         -25.7644 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |           0.0319 |         236.8223 |         -22.2210 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |           0.0057 |         233.0579 |         -26.8028 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |           0.0111 |         231.3381 |         -21.6300 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |          -0.0114 |         230.2473 |         -27.5776 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |          -0.0244 |         230.0433 |         -29.0032 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |          -0.0268 |         229.1840 |         -29.9141 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |          -0.0318 |         230.5522 |         -31.2540 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |          -0.0343 |         228.4822 |         -31.3479 |
[32m[20221208 14:04:36 @agent_ppo2.py:179][0m |          -0.0391 |         228.3215 |         -32.5256 |
[32m[20221208 14:04:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 927.48
[32m[20221208 14:04:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.86
[32m[20221208 14:04:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.22
[32m[20221208 14:04:37 @agent_ppo2.py:137][0m Total time:      19.27 min
[32m[20221208 14:04:37 @agent_ppo2.py:139][0m 1574912 total steps have happened
[32m[20221208 14:04:37 @agent_ppo2.py:115][0m #------------------------ Iteration 769 --------------------------#
[32m[20221208 14:04:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |           0.0250 |         240.4294 |         -27.3064 |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |           0.0355 |         232.8898 |         -20.8165 |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |           0.0117 |         228.1972 |         -24.0806 |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |          -0.0117 |         225.1678 |         -25.4891 |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |          -0.0231 |         223.6918 |         -27.7115 |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |          -0.0319 |         221.7588 |         -29.3424 |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |          -0.0376 |         220.2265 |         -30.7837 |
[32m[20221208 14:04:37 @agent_ppo2.py:179][0m |          -0.0383 |         220.3744 |         -31.8317 |
[32m[20221208 14:04:38 @agent_ppo2.py:179][0m |          -0.0415 |         218.4513 |         -32.3210 |
[32m[20221208 14:04:38 @agent_ppo2.py:179][0m |          -0.0416 |         218.3824 |         -32.9896 |
[32m[20221208 14:04:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:04:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.82
[32m[20221208 14:04:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 948.71
[32m[20221208 14:04:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.69
[32m[20221208 14:04:38 @agent_ppo2.py:137][0m Total time:      19.29 min
[32m[20221208 14:04:38 @agent_ppo2.py:139][0m 1576960 total steps have happened
[32m[20221208 14:04:38 @agent_ppo2.py:115][0m #------------------------ Iteration 770 --------------------------#
[32m[20221208 14:04:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |           0.1273 |         248.9315 |         -21.0196 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |           0.0602 |         243.2350 |         -13.3775 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |           0.0136 |         240.4728 |         -19.2155 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |          -0.0015 |         237.6460 |         -22.4340 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |          -0.0141 |         236.3226 |         -23.4918 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |          -0.0183 |         235.3784 |         -24.0050 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |          -0.0310 |         235.1184 |         -25.4529 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |          -0.0323 |         234.6847 |         -26.4048 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |          -0.0368 |         233.0971 |         -27.7204 |
[32m[20221208 14:04:39 @agent_ppo2.py:179][0m |          -0.0409 |         233.1327 |         -28.9320 |
[32m[20221208 14:04:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 911.03
[32m[20221208 14:04:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.03
[32m[20221208 14:04:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.51
[32m[20221208 14:04:40 @agent_ppo2.py:137][0m Total time:      19.32 min
[32m[20221208 14:04:40 @agent_ppo2.py:139][0m 1579008 total steps have happened
[32m[20221208 14:04:40 @agent_ppo2.py:115][0m #------------------------ Iteration 771 --------------------------#
[32m[20221208 14:04:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |           0.0778 |         240.3784 |         -22.7785 |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |           0.0688 |         233.3872 |         -16.7202 |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |           0.0365 |         226.6820 |         -17.6519 |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |           0.0088 |         223.5394 |         -22.3363 |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |          -0.0079 |         222.1184 |         -23.5712 |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |          -0.0258 |         219.9749 |         -24.5798 |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |          -0.0303 |         219.0088 |         -25.7953 |
[32m[20221208 14:04:40 @agent_ppo2.py:179][0m |          -0.0358 |         218.2512 |         -25.8263 |
[32m[20221208 14:04:41 @agent_ppo2.py:179][0m |          -0.0426 |         216.3090 |         -27.3843 |
[32m[20221208 14:04:41 @agent_ppo2.py:179][0m |          -0.0411 |         215.6754 |         -27.6589 |
[32m[20221208 14:04:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 866.89
[32m[20221208 14:04:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.89
[32m[20221208 14:04:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 906.12
[32m[20221208 14:04:41 @agent_ppo2.py:137][0m Total time:      19.34 min
[32m[20221208 14:04:41 @agent_ppo2.py:139][0m 1581056 total steps have happened
[32m[20221208 14:04:41 @agent_ppo2.py:115][0m #------------------------ Iteration 772 --------------------------#
[32m[20221208 14:04:41 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |           0.0518 |         220.0751 |         -24.6854 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |           0.0262 |         199.7111 |         -20.2044 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0143 |         194.0374 |         -24.5103 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0286 |         191.0025 |         -25.2074 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0350 |         186.3662 |         -26.8682 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0397 |         184.4640 |         -27.2682 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0459 |         182.7397 |         -29.2652 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0457 |         183.0379 |         -30.0354 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0475 |         179.6040 |         -30.2833 |
[32m[20221208 14:04:42 @agent_ppo2.py:179][0m |          -0.0479 |         177.9513 |         -31.2499 |
[32m[20221208 14:04:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 884.87
[32m[20221208 14:04:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.26
[32m[20221208 14:04:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 855.42
[32m[20221208 14:04:42 @agent_ppo2.py:137][0m Total time:      19.37 min
[32m[20221208 14:04:42 @agent_ppo2.py:139][0m 1583104 total steps have happened
[32m[20221208 14:04:42 @agent_ppo2.py:115][0m #------------------------ Iteration 773 --------------------------#
[32m[20221208 14:04:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |           0.0477 |         244.3235 |         -28.1040 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |           0.0336 |         235.0555 |         -24.2053 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |           0.0231 |         232.8876 |         -23.7678 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |           0.0195 |         229.4225 |         -24.1975 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |          -0.0054 |         229.3148 |         -29.0504 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |          -0.0201 |         227.2883 |         -30.8386 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |          -0.0319 |         225.7143 |         -32.2972 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |          -0.0356 |         223.6500 |         -33.3130 |
[32m[20221208 14:04:43 @agent_ppo2.py:179][0m |          -0.0337 |         222.5595 |         -32.9629 |
[32m[20221208 14:04:44 @agent_ppo2.py:179][0m |          -0.0422 |         223.4187 |         -33.7395 |
[32m[20221208 14:04:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.74
[32m[20221208 14:04:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.43
[32m[20221208 14:04:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.47
[32m[20221208 14:04:44 @agent_ppo2.py:137][0m Total time:      19.39 min
[32m[20221208 14:04:44 @agent_ppo2.py:139][0m 1585152 total steps have happened
[32m[20221208 14:04:44 @agent_ppo2.py:115][0m #------------------------ Iteration 774 --------------------------#
[32m[20221208 14:04:44 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:44 @agent_ppo2.py:179][0m |           0.0351 |         235.9247 |         -24.3836 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |           0.1101 |         230.4572 |         -15.6622 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |           0.0739 |         228.2621 |          -7.7135 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |           0.0141 |         225.8743 |         -16.9637 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |          -0.0055 |         223.7065 |         -21.1442 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |          -0.0213 |         223.1291 |         -23.2013 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |          -0.0282 |         222.1935 |         -25.8073 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |          -0.0318 |         221.1230 |         -26.2083 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |          -0.0385 |         220.1212 |         -27.4711 |
[32m[20221208 14:04:45 @agent_ppo2.py:179][0m |          -0.0397 |         218.4925 |         -28.2268 |
[32m[20221208 14:04:45 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:04:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 927.64
[32m[20221208 14:04:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.33
[32m[20221208 14:04:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 889.26
[32m[20221208 14:04:45 @agent_ppo2.py:137][0m Total time:      19.42 min
[32m[20221208 14:04:45 @agent_ppo2.py:139][0m 1587200 total steps have happened
[32m[20221208 14:04:45 @agent_ppo2.py:115][0m #------------------------ Iteration 775 --------------------------#
[32m[20221208 14:04:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |           0.0482 |         248.6225 |         -21.5268 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |           0.0550 |         240.0496 |         -15.4568 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |           0.0329 |         236.6019 |         -16.4209 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |          -0.0002 |         234.3262 |         -19.5428 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |          -0.0194 |         232.5548 |         -21.9906 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |          -0.0277 |         231.9352 |         -22.2611 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |          -0.0325 |         231.3468 |         -22.7585 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |          -0.0402 |         231.0293 |         -24.1003 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |          -0.0412 |         231.4924 |         -24.5542 |
[32m[20221208 14:04:46 @agent_ppo2.py:179][0m |          -0.0450 |         230.9795 |         -25.0894 |
[32m[20221208 14:04:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 856.16
[32m[20221208 14:04:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 906.47
[32m[20221208 14:04:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 921.28
[32m[20221208 14:04:47 @agent_ppo2.py:137][0m Total time:      19.44 min
[32m[20221208 14:04:47 @agent_ppo2.py:139][0m 1589248 total steps have happened
[32m[20221208 14:04:47 @agent_ppo2.py:115][0m #------------------------ Iteration 776 --------------------------#
[32m[20221208 14:04:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:47 @agent_ppo2.py:179][0m |           0.0430 |         243.6332 |         -24.8587 |
[32m[20221208 14:04:47 @agent_ppo2.py:179][0m |           0.0494 |         235.2354 |         -17.2900 |
[32m[20221208 14:04:47 @agent_ppo2.py:179][0m |           0.0300 |         229.9535 |         -19.1256 |
[32m[20221208 14:04:48 @agent_ppo2.py:179][0m |          -0.0083 |         227.4808 |         -20.8044 |
[32m[20221208 14:04:48 @agent_ppo2.py:179][0m |          -0.0282 |         226.0447 |         -23.1265 |
[32m[20221208 14:04:48 @agent_ppo2.py:179][0m |          -0.0310 |         221.4992 |         -23.4664 |
[32m[20221208 14:04:48 @agent_ppo2.py:179][0m |          -0.0370 |         215.8240 |         -24.8675 |
[32m[20221208 14:04:48 @agent_ppo2.py:179][0m |          -0.0425 |         212.0268 |         -25.8583 |
[32m[20221208 14:04:48 @agent_ppo2.py:179][0m |          -0.0439 |         209.9843 |         -26.7257 |
[32m[20221208 14:04:48 @agent_ppo2.py:179][0m |          -0.0472 |         208.3333 |         -27.5464 |
[32m[20221208 14:04:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 898.71
[32m[20221208 14:04:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.92
[32m[20221208 14:04:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.27
[32m[20221208 14:04:48 @agent_ppo2.py:137][0m Total time:      19.47 min
[32m[20221208 14:04:48 @agent_ppo2.py:139][0m 1591296 total steps have happened
[32m[20221208 14:04:48 @agent_ppo2.py:115][0m #------------------------ Iteration 777 --------------------------#
[32m[20221208 14:04:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |           0.0830 |         193.0741 |         -14.8229 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |           0.0116 |         173.4571 |         -12.9273 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0187 |         165.6620 |         -15.5678 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0498 |         162.5172 |         -17.1963 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0587 |         162.0284 |         -18.5273 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0603 |         158.0614 |         -18.9542 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0620 |         155.5695 |         -19.3590 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0717 |         154.4003 |         -20.8913 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0779 |         153.8919 |         -21.1281 |
[32m[20221208 14:04:49 @agent_ppo2.py:179][0m |          -0.0791 |         151.8968 |         -22.5295 |
[32m[20221208 14:04:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 599.32
[32m[20221208 14:04:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.07
[32m[20221208 14:04:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 925.39
[32m[20221208 14:04:50 @agent_ppo2.py:137][0m Total time:      19.49 min
[32m[20221208 14:04:50 @agent_ppo2.py:139][0m 1593344 total steps have happened
[32m[20221208 14:04:50 @agent_ppo2.py:115][0m #------------------------ Iteration 778 --------------------------#
[32m[20221208 14:04:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:50 @agent_ppo2.py:179][0m |           0.2301 |         246.3726 |         -24.6001 |
[32m[20221208 14:04:50 @agent_ppo2.py:179][0m |           0.0769 |         241.1080 |          -9.4311 |
[32m[20221208 14:04:50 @agent_ppo2.py:179][0m |           0.0403 |         238.4696 |         -16.3717 |
[32m[20221208 14:04:50 @agent_ppo2.py:179][0m |           0.0156 |         237.1054 |         -23.3094 |
[32m[20221208 14:04:51 @agent_ppo2.py:179][0m |           0.0009 |         236.7477 |         -28.6303 |
[32m[20221208 14:04:51 @agent_ppo2.py:179][0m |          -0.0099 |         237.0881 |         -30.1882 |
[32m[20221208 14:04:51 @agent_ppo2.py:179][0m |          -0.0153 |         235.4214 |         -33.3043 |
[32m[20221208 14:04:51 @agent_ppo2.py:179][0m |          -0.0224 |         236.3401 |         -34.3044 |
[32m[20221208 14:04:51 @agent_ppo2.py:179][0m |          -0.0212 |         234.5242 |         -34.5786 |
[32m[20221208 14:04:51 @agent_ppo2.py:179][0m |          -0.0297 |         234.2818 |         -36.4458 |
[32m[20221208 14:04:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.68
[32m[20221208 14:04:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.43
[32m[20221208 14:04:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 944.72
[32m[20221208 14:04:51 @agent_ppo2.py:137][0m Total time:      19.51 min
[32m[20221208 14:04:51 @agent_ppo2.py:139][0m 1595392 total steps have happened
[32m[20221208 14:04:51 @agent_ppo2.py:115][0m #------------------------ Iteration 779 --------------------------#
[32m[20221208 14:04:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |           0.0352 |         243.9328 |         -23.6606 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |           0.0439 |         238.5971 |         -18.1961 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |           0.0166 |         237.4391 |         -18.9236 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |          -0.0052 |         236.9368 |         -24.8831 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |          -0.0190 |         236.0981 |         -27.5213 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |          -0.0255 |         235.7288 |         -28.2975 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |          -0.0242 |         235.1492 |         -28.5898 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |          -0.0295 |         235.8714 |         -29.7733 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |          -0.0336 |         234.2814 |         -30.5161 |
[32m[20221208 14:04:52 @agent_ppo2.py:179][0m |          -0.0365 |         233.8713 |         -31.1185 |
[32m[20221208 14:04:52 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:04:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 977.69
[32m[20221208 14:04:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.72
[32m[20221208 14:04:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.48
[32m[20221208 14:04:53 @agent_ppo2.py:137][0m Total time:      19.54 min
[32m[20221208 14:04:53 @agent_ppo2.py:139][0m 1597440 total steps have happened
[32m[20221208 14:04:53 @agent_ppo2.py:115][0m #------------------------ Iteration 780 --------------------------#
[32m[20221208 14:04:53 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:53 @agent_ppo2.py:179][0m |           0.0653 |         245.5369 |         -23.9972 |
[32m[20221208 14:04:53 @agent_ppo2.py:179][0m |           0.0761 |         239.1369 |         -13.2829 |
[32m[20221208 14:04:53 @agent_ppo2.py:179][0m |           0.0231 |         234.8801 |         -18.1783 |
[32m[20221208 14:04:53 @agent_ppo2.py:179][0m |           0.0065 |         232.4394 |         -21.1800 |
[32m[20221208 14:04:53 @agent_ppo2.py:179][0m |          -0.0050 |         232.2907 |         -23.6847 |
[32m[20221208 14:04:54 @agent_ppo2.py:179][0m |          -0.0223 |         228.7559 |         -26.0186 |
[32m[20221208 14:04:54 @agent_ppo2.py:179][0m |          -0.0277 |         227.3645 |         -26.8549 |
[32m[20221208 14:04:54 @agent_ppo2.py:179][0m |          -0.0325 |         226.4474 |         -28.1757 |
[32m[20221208 14:04:54 @agent_ppo2.py:179][0m |          -0.0293 |         225.7226 |         -28.4185 |
[32m[20221208 14:04:54 @agent_ppo2.py:179][0m |          -0.0331 |         224.3907 |         -28.9850 |
[32m[20221208 14:04:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:04:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.74
[32m[20221208 14:04:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.33
[32m[20221208 14:04:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.20
[32m[20221208 14:04:54 @agent_ppo2.py:137][0m Total time:      19.56 min
[32m[20221208 14:04:54 @agent_ppo2.py:139][0m 1599488 total steps have happened
[32m[20221208 14:04:54 @agent_ppo2.py:115][0m #------------------------ Iteration 781 --------------------------#
[32m[20221208 14:04:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:04:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |           0.0589 |         252.0112 |         -21.4006 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |           0.0347 |         241.5310 |         -18.5290 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0033 |         234.4105 |         -20.8638 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0207 |         232.0747 |         -22.8150 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0358 |         230.1563 |         -25.1785 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0364 |         229.2539 |         -25.1247 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0439 |         227.5623 |         -25.9676 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0467 |         226.1215 |         -26.6121 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0454 |         226.4135 |         -27.5945 |
[32m[20221208 14:04:55 @agent_ppo2.py:179][0m |          -0.0486 |         224.9211 |         -27.9692 |
[32m[20221208 14:04:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 888.51
[32m[20221208 14:04:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.33
[32m[20221208 14:04:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.60
[32m[20221208 14:04:56 @agent_ppo2.py:137][0m Total time:      19.59 min
[32m[20221208 14:04:56 @agent_ppo2.py:139][0m 1601536 total steps have happened
[32m[20221208 14:04:56 @agent_ppo2.py:115][0m #------------------------ Iteration 782 --------------------------#
[32m[20221208 14:04:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:56 @agent_ppo2.py:179][0m |           0.0494 |         238.0830 |         -24.3011 |
[32m[20221208 14:04:56 @agent_ppo2.py:179][0m |           0.0225 |         229.1966 |         -20.3210 |
[32m[20221208 14:04:56 @agent_ppo2.py:179][0m |          -0.0143 |         225.7677 |         -26.1400 |
[32m[20221208 14:04:56 @agent_ppo2.py:179][0m |          -0.0238 |         224.4550 |         -26.2901 |
[32m[20221208 14:04:56 @agent_ppo2.py:179][0m |          -0.0277 |         222.8544 |         -28.3080 |
[32m[20221208 14:04:56 @agent_ppo2.py:179][0m |          -0.0260 |         221.2957 |         -26.9988 |
[32m[20221208 14:04:57 @agent_ppo2.py:179][0m |          -0.0382 |         220.3141 |         -29.0477 |
[32m[20221208 14:04:57 @agent_ppo2.py:179][0m |          -0.0391 |         219.7878 |         -29.7723 |
[32m[20221208 14:04:57 @agent_ppo2.py:179][0m |          -0.0354 |         217.2309 |         -30.1629 |
[32m[20221208 14:04:57 @agent_ppo2.py:179][0m |          -0.0446 |         217.0116 |         -31.2033 |
[32m[20221208 14:04:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:04:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.17
[32m[20221208 14:04:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.44
[32m[20221208 14:04:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.18
[32m[20221208 14:04:57 @agent_ppo2.py:137][0m Total time:      19.61 min
[32m[20221208 14:04:57 @agent_ppo2.py:139][0m 1603584 total steps have happened
[32m[20221208 14:04:57 @agent_ppo2.py:115][0m #------------------------ Iteration 783 --------------------------#
[32m[20221208 14:04:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:04:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |           0.0590 |         251.0933 |         -24.4557 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |           0.0505 |         240.9588 |         -18.2461 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |           0.0079 |         240.4596 |         -21.9920 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |          -0.0178 |         239.0765 |         -26.6170 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |          -0.0244 |         238.9509 |         -28.2838 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |          -0.0322 |         238.3726 |         -28.8936 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |          -0.0372 |         238.2729 |         -29.8975 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |          -0.0393 |         235.9372 |         -30.4220 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |          -0.0383 |         236.0026 |         -30.4891 |
[32m[20221208 14:04:58 @agent_ppo2.py:179][0m |          -0.0339 |         236.7130 |         -31.2999 |
[32m[20221208 14:04:58 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:04:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 957.52
[32m[20221208 14:04:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.67
[32m[20221208 14:04:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.46
[32m[20221208 14:04:59 @agent_ppo2.py:137][0m Total time:      19.64 min
[32m[20221208 14:04:59 @agent_ppo2.py:139][0m 1605632 total steps have happened
[32m[20221208 14:04:59 @agent_ppo2.py:115][0m #------------------------ Iteration 784 --------------------------#
[32m[20221208 14:04:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:04:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:04:59 @agent_ppo2.py:179][0m |           0.0408 |         221.3060 |         -23.7881 |
[32m[20221208 14:04:59 @agent_ppo2.py:179][0m |           0.0226 |         199.1942 |         -22.2183 |
[32m[20221208 14:04:59 @agent_ppo2.py:179][0m |          -0.0068 |         188.5664 |         -26.9474 |
[32m[20221208 14:04:59 @agent_ppo2.py:179][0m |          -0.0073 |         182.1231 |         -26.5086 |
[32m[20221208 14:04:59 @agent_ppo2.py:179][0m |          -0.0291 |         176.4849 |         -29.6696 |
[32m[20221208 14:04:59 @agent_ppo2.py:179][0m |          -0.0347 |         173.4030 |         -30.5570 |
[32m[20221208 14:05:00 @agent_ppo2.py:179][0m |          -0.0396 |         169.9607 |         -30.9450 |
[32m[20221208 14:05:00 @agent_ppo2.py:179][0m |          -0.0407 |         167.3010 |         -32.1114 |
[32m[20221208 14:05:00 @agent_ppo2.py:179][0m |          -0.0448 |         165.7344 |         -33.4966 |
[32m[20221208 14:05:00 @agent_ppo2.py:179][0m |          -0.0450 |         163.6466 |         -33.7161 |
[32m[20221208 14:05:00 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 911.47
[32m[20221208 14:05:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.59
[32m[20221208 14:05:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 988.63
[32m[20221208 14:05:00 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Get the best episode reward: 988.63
[32m[20221208 14:05:00 @agent_ppo2.py:107][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 988.63
[32m[20221208 14:05:00 @agent_ppo2.py:137][0m Total time:      19.66 min
[32m[20221208 14:05:00 @agent_ppo2.py:139][0m 1607680 total steps have happened
[32m[20221208 14:05:00 @agent_ppo2.py:115][0m #------------------------ Iteration 785 --------------------------#
[32m[20221208 14:05:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |           0.0316 |         245.0873 |         -29.3825 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |           0.0423 |         236.4579 |         -20.2087 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0047 |         232.6066 |         -26.3676 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0186 |         231.7593 |         -29.5880 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0203 |         231.3856 |         -30.2672 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0297 |         231.8629 |         -31.1352 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0356 |         231.1578 |         -32.8121 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0406 |         231.2093 |         -33.7074 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0414 |         231.0741 |         -35.1145 |
[32m[20221208 14:05:01 @agent_ppo2.py:179][0m |          -0.0422 |         229.9132 |         -35.6647 |
[32m[20221208 14:05:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.33
[32m[20221208 14:05:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.62
[32m[20221208 14:05:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 866.62
[32m[20221208 14:05:02 @agent_ppo2.py:137][0m Total time:      19.69 min
[32m[20221208 14:05:02 @agent_ppo2.py:139][0m 1609728 total steps have happened
[32m[20221208 14:05:02 @agent_ppo2.py:115][0m #------------------------ Iteration 786 --------------------------#
[32m[20221208 14:05:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:02 @agent_ppo2.py:179][0m |           0.1131 |         232.0563 |         -25.7396 |
[32m[20221208 14:05:02 @agent_ppo2.py:179][0m |           0.1476 |         222.8567 |          -6.8467 |
[32m[20221208 14:05:02 @agent_ppo2.py:179][0m |           0.0618 |         220.9637 |         -12.7274 |
[32m[20221208 14:05:02 @agent_ppo2.py:179][0m |           0.0182 |         216.5925 |         -21.9530 |
[32m[20221208 14:05:02 @agent_ppo2.py:179][0m |          -0.0005 |         211.7389 |         -24.6572 |
[32m[20221208 14:05:02 @agent_ppo2.py:179][0m |          -0.0115 |         206.7189 |         -26.6843 |
[32m[20221208 14:05:02 @agent_ppo2.py:179][0m |          -0.0188 |         204.5962 |         -27.7972 |
[32m[20221208 14:05:03 @agent_ppo2.py:179][0m |          -0.0316 |         203.1043 |         -30.1339 |
[32m[20221208 14:05:03 @agent_ppo2.py:179][0m |          -0.0299 |         200.2466 |         -30.7920 |
[32m[20221208 14:05:03 @agent_ppo2.py:179][0m |          -0.0360 |         199.3709 |         -31.2144 |
[32m[20221208 14:05:03 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.57
[32m[20221208 14:05:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.18
[32m[20221208 14:05:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.22
[32m[20221208 14:05:03 @agent_ppo2.py:137][0m Total time:      19.71 min
[32m[20221208 14:05:03 @agent_ppo2.py:139][0m 1611776 total steps have happened
[32m[20221208 14:05:03 @agent_ppo2.py:115][0m #------------------------ Iteration 787 --------------------------#
[32m[20221208 14:05:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |           0.0600 |         249.5839 |         -26.9026 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |           0.0201 |         242.3270 |         -26.8009 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0005 |         239.8296 |         -28.0487 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0138 |         237.7042 |         -27.1595 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0273 |         236.4801 |         -29.6236 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0340 |         235.7553 |         -30.0417 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0397 |         234.8452 |         -31.4716 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0415 |         233.6925 |         -31.4349 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0426 |         233.6547 |         -33.7350 |
[32m[20221208 14:05:04 @agent_ppo2.py:179][0m |          -0.0406 |         233.3873 |         -33.9556 |
[32m[20221208 14:05:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.87
[32m[20221208 14:05:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.71
[32m[20221208 14:05:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.66
[32m[20221208 14:05:05 @agent_ppo2.py:137][0m Total time:      19.74 min
[32m[20221208 14:05:05 @agent_ppo2.py:139][0m 1613824 total steps have happened
[32m[20221208 14:05:05 @agent_ppo2.py:115][0m #------------------------ Iteration 788 --------------------------#
[32m[20221208 14:05:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |           0.0695 |         250.3629 |         -27.8243 |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |           0.0701 |         243.5401 |         -20.6891 |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |           0.0576 |         241.0003 |         -17.0559 |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |           0.0084 |         239.9406 |         -23.7442 |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |          -0.0100 |         239.1159 |         -27.6011 |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |          -0.0259 |         239.0870 |         -29.2059 |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |          -0.0348 |         237.9768 |         -30.7582 |
[32m[20221208 14:05:05 @agent_ppo2.py:179][0m |          -0.0392 |         236.8372 |         -31.7117 |
[32m[20221208 14:05:06 @agent_ppo2.py:179][0m |          -0.0420 |         237.1587 |         -32.5310 |
[32m[20221208 14:05:06 @agent_ppo2.py:179][0m |          -0.0448 |         235.7907 |         -33.0918 |
[32m[20221208 14:05:06 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:05:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 892.20
[32m[20221208 14:05:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 997.78
[32m[20221208 14:05:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.13
[32m[20221208 14:05:06 @agent_ppo2.py:137][0m Total time:      19.76 min
[32m[20221208 14:05:06 @agent_ppo2.py:139][0m 1615872 total steps have happened
[32m[20221208 14:05:06 @agent_ppo2.py:115][0m #------------------------ Iteration 789 --------------------------#
[32m[20221208 14:05:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |           0.1727 |         240.3493 |         -24.5579 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |           0.0636 |         225.4847 |         -16.3010 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |           0.0132 |         218.9919 |         -22.7761 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |          -0.0161 |         211.5854 |         -25.3720 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |          -0.0267 |         206.0387 |         -26.5184 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |          -0.0298 |         201.4841 |         -27.1237 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |          -0.0342 |         201.4765 |         -28.2889 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |          -0.0389 |         197.7409 |         -29.2470 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |          -0.0412 |         195.4922 |         -30.4213 |
[32m[20221208 14:05:07 @agent_ppo2.py:179][0m |          -0.0442 |         192.9814 |         -30.7558 |
[32m[20221208 14:05:07 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.94
[32m[20221208 14:05:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.69
[32m[20221208 14:05:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 964.75
[32m[20221208 14:05:07 @agent_ppo2.py:137][0m Total time:      19.79 min
[32m[20221208 14:05:07 @agent_ppo2.py:139][0m 1617920 total steps have happened
[32m[20221208 14:05:07 @agent_ppo2.py:115][0m #------------------------ Iteration 790 --------------------------#
[32m[20221208 14:05:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |           0.0592 |         246.6689 |         -24.7820 |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |           0.0816 |         237.5616 |         -13.4806 |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |           0.0481 |         233.9891 |         -17.1927 |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |           0.0145 |         231.3765 |         -23.0361 |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |          -0.0057 |         230.3550 |         -27.7795 |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |          -0.0158 |         231.1410 |         -28.8456 |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |          -0.0268 |         228.8908 |         -29.4526 |
[32m[20221208 14:05:08 @agent_ppo2.py:179][0m |          -0.0305 |         227.9905 |         -30.1059 |
[32m[20221208 14:05:09 @agent_ppo2.py:179][0m |          -0.0372 |         227.1353 |         -31.3824 |
[32m[20221208 14:05:09 @agent_ppo2.py:179][0m |          -0.0403 |         227.0533 |         -31.1291 |
[32m[20221208 14:05:09 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:05:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 941.84
[32m[20221208 14:05:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.01
[32m[20221208 14:05:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 900.10
[32m[20221208 14:05:09 @agent_ppo2.py:137][0m Total time:      19.81 min
[32m[20221208 14:05:09 @agent_ppo2.py:139][0m 1619968 total steps have happened
[32m[20221208 14:05:09 @agent_ppo2.py:115][0m #------------------------ Iteration 791 --------------------------#
[32m[20221208 14:05:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |           0.0732 |         240.2139 |         -24.8705 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |           0.0216 |         235.9266 |         -20.8748 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0079 |         233.5448 |         -24.3138 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0236 |         232.0707 |         -25.6257 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0357 |         230.6121 |         -27.5495 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0435 |         228.3103 |         -27.8077 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0410 |         226.7441 |         -27.9478 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0492 |         226.0875 |         -29.1451 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0519 |         226.1596 |         -29.4391 |
[32m[20221208 14:05:10 @agent_ppo2.py:179][0m |          -0.0525 |         224.6309 |         -30.7036 |
[32m[20221208 14:05:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 835.23
[32m[20221208 14:05:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 931.21
[32m[20221208 14:05:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 857.92
[32m[20221208 14:05:10 @agent_ppo2.py:137][0m Total time:      19.83 min
[32m[20221208 14:05:10 @agent_ppo2.py:139][0m 1622016 total steps have happened
[32m[20221208 14:05:10 @agent_ppo2.py:115][0m #------------------------ Iteration 792 --------------------------#
[32m[20221208 14:05:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |           0.0574 |         235.1999 |         -20.9934 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |           0.0344 |         230.9771 |         -18.2081 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |           0.0006 |         229.5858 |         -23.4215 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |          -0.0098 |         228.2225 |         -23.6376 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |          -0.0290 |         227.0250 |         -24.4893 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |          -0.0352 |         226.5766 |         -24.6841 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |          -0.0365 |         226.0688 |         -26.0645 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |          -0.0365 |         225.5847 |         -26.7588 |
[32m[20221208 14:05:11 @agent_ppo2.py:179][0m |          -0.0372 |         225.4168 |         -27.6265 |
[32m[20221208 14:05:12 @agent_ppo2.py:179][0m |          -0.0429 |         224.8532 |         -28.0250 |
[32m[20221208 14:05:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 881.37
[32m[20221208 14:05:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 930.57
[32m[20221208 14:05:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.22
[32m[20221208 14:05:12 @agent_ppo2.py:137][0m Total time:      19.86 min
[32m[20221208 14:05:12 @agent_ppo2.py:139][0m 1624064 total steps have happened
[32m[20221208 14:05:12 @agent_ppo2.py:115][0m #------------------------ Iteration 793 --------------------------#
[32m[20221208 14:05:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:12 @agent_ppo2.py:179][0m |           0.0890 |         235.8048 |         -23.5562 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |           0.0471 |         229.6382 |         -18.3782 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |           0.0038 |         226.5130 |         -24.0235 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |          -0.0243 |         224.8248 |         -27.2647 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |          -0.0342 |         224.3641 |         -27.7815 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |          -0.0388 |         223.3415 |         -28.3456 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |          -0.0403 |         222.9603 |         -29.1678 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |          -0.0415 |         221.2401 |         -29.6019 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |          -0.0455 |         220.8628 |         -30.7428 |
[32m[20221208 14:05:13 @agent_ppo2.py:179][0m |          -0.0497 |         221.6726 |         -31.5613 |
[32m[20221208 14:05:13 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 864.79
[32m[20221208 14:05:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.86
[32m[20221208 14:05:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 980.15
[32m[20221208 14:05:13 @agent_ppo2.py:137][0m Total time:      19.88 min
[32m[20221208 14:05:13 @agent_ppo2.py:139][0m 1626112 total steps have happened
[32m[20221208 14:05:13 @agent_ppo2.py:115][0m #------------------------ Iteration 794 --------------------------#
[32m[20221208 14:05:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:05:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |           0.1114 |         236.9637 |         -21.3906 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |           0.1262 |         231.9266 |         -11.1379 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |           0.0382 |         229.2244 |         -17.2074 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |           0.0036 |         226.7311 |         -21.0694 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |          -0.0133 |         224.2521 |         -23.2700 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |          -0.0211 |         223.0689 |         -25.2408 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |          -0.0318 |         220.9423 |         -26.7544 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |          -0.0359 |         221.5835 |         -27.4927 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |          -0.0432 |         219.6766 |         -28.8873 |
[32m[20221208 14:05:14 @agent_ppo2.py:179][0m |          -0.0461 |         219.9211 |         -29.0986 |
[32m[20221208 14:05:14 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 835.71
[32m[20221208 14:05:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 921.60
[32m[20221208 14:05:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 906.32
[32m[20221208 14:05:15 @agent_ppo2.py:137][0m Total time:      19.91 min
[32m[20221208 14:05:15 @agent_ppo2.py:139][0m 1628160 total steps have happened
[32m[20221208 14:05:15 @agent_ppo2.py:115][0m #------------------------ Iteration 795 --------------------------#
[32m[20221208 14:05:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:15 @agent_ppo2.py:179][0m |           0.0610 |         224.3919 |         -25.4129 |
[32m[20221208 14:05:15 @agent_ppo2.py:179][0m |           0.0630 |         215.0531 |         -20.0990 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |           0.0175 |         211.4916 |         -23.2476 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |          -0.0108 |         208.7907 |         -25.3377 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |          -0.0273 |         207.2553 |         -28.6179 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |          -0.0323 |         203.1026 |         -29.3249 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |          -0.0335 |         201.2936 |         -29.1864 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |          -0.0439 |         199.6900 |         -31.0033 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |          -0.0474 |         197.1631 |         -31.9098 |
[32m[20221208 14:05:16 @agent_ppo2.py:179][0m |          -0.0487 |         196.2684 |         -32.4304 |
[32m[20221208 14:05:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 872.29
[32m[20221208 14:05:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 914.42
[32m[20221208 14:05:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.36
[32m[20221208 14:05:16 @agent_ppo2.py:137][0m Total time:      19.93 min
[32m[20221208 14:05:16 @agent_ppo2.py:139][0m 1630208 total steps have happened
[32m[20221208 14:05:16 @agent_ppo2.py:115][0m #------------------------ Iteration 796 --------------------------#
[32m[20221208 14:05:17 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |           0.0788 |         217.3797 |         -23.8590 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |           0.0271 |         202.5679 |         -23.3803 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |           0.0119 |         195.3881 |         -24.0123 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |          -0.0197 |         189.3077 |         -27.6604 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |          -0.0267 |         185.9094 |         -29.0439 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |          -0.0370 |         181.7329 |         -30.2434 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |          -0.0407 |         177.9903 |         -30.8669 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |          -0.0414 |         175.6446 |         -32.0908 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |          -0.0466 |         172.5620 |         -33.0925 |
[32m[20221208 14:05:17 @agent_ppo2.py:179][0m |          -0.0492 |         170.5298 |         -34.5072 |
[32m[20221208 14:05:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 884.11
[32m[20221208 14:05:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.66
[32m[20221208 14:05:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.08
[32m[20221208 14:05:18 @agent_ppo2.py:137][0m Total time:      19.96 min
[32m[20221208 14:05:18 @agent_ppo2.py:139][0m 1632256 total steps have happened
[32m[20221208 14:05:18 @agent_ppo2.py:115][0m #------------------------ Iteration 797 --------------------------#
[32m[20221208 14:05:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:18 @agent_ppo2.py:179][0m |           0.0602 |         238.2643 |         -28.6619 |
[32m[20221208 14:05:18 @agent_ppo2.py:179][0m |           0.1405 |         231.5936 |         -14.3833 |
[32m[20221208 14:05:18 @agent_ppo2.py:179][0m |           0.0194 |         225.9111 |         -23.1433 |
[32m[20221208 14:05:19 @agent_ppo2.py:179][0m |          -0.0064 |         224.4477 |         -27.3218 |
[32m[20221208 14:05:19 @agent_ppo2.py:179][0m |          -0.0163 |         222.5397 |         -28.7343 |
[32m[20221208 14:05:19 @agent_ppo2.py:179][0m |          -0.0297 |         221.0655 |         -31.8550 |
[32m[20221208 14:05:19 @agent_ppo2.py:179][0m |          -0.0366 |         219.3055 |         -32.7531 |
[32m[20221208 14:05:19 @agent_ppo2.py:179][0m |          -0.0423 |         219.3491 |         -34.3930 |
[32m[20221208 14:05:19 @agent_ppo2.py:179][0m |          -0.0440 |         216.6097 |         -34.8306 |
[32m[20221208 14:05:19 @agent_ppo2.py:179][0m |          -0.0455 |         216.4692 |         -34.8271 |
[32m[20221208 14:05:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 910.40
[32m[20221208 14:05:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.90
[32m[20221208 14:05:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 577.00
[32m[20221208 14:05:19 @agent_ppo2.py:137][0m Total time:      19.98 min
[32m[20221208 14:05:19 @agent_ppo2.py:139][0m 1634304 total steps have happened
[32m[20221208 14:05:19 @agent_ppo2.py:115][0m #------------------------ Iteration 798 --------------------------#
[32m[20221208 14:05:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |           0.0566 |         235.3742 |         -28.6421 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |           0.0596 |         230.8985 |         -21.7696 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |           0.0215 |         226.3335 |         -22.3981 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |          -0.0113 |         224.1955 |         -31.7533 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |          -0.0253 |         223.3493 |         -32.7336 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |          -0.0312 |         222.9591 |         -33.6687 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |          -0.0341 |         221.3752 |         -35.5815 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |          -0.0406 |         220.6250 |         -36.5566 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |          -0.0435 |         220.5411 |         -37.2533 |
[32m[20221208 14:05:20 @agent_ppo2.py:179][0m |          -0.0446 |         219.2925 |         -38.4755 |
[32m[20221208 14:05:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:05:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 915.84
[32m[20221208 14:05:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 937.10
[32m[20221208 14:05:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 914.20
[32m[20221208 14:05:21 @agent_ppo2.py:137][0m Total time:      20.01 min
[32m[20221208 14:05:21 @agent_ppo2.py:139][0m 1636352 total steps have happened
[32m[20221208 14:05:21 @agent_ppo2.py:115][0m #------------------------ Iteration 799 --------------------------#
[32m[20221208 14:05:21 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:21 @agent_ppo2.py:179][0m |           0.0572 |         236.0680 |         -29.6629 |
[32m[20221208 14:05:21 @agent_ppo2.py:179][0m |           0.0462 |         232.1392 |         -21.8337 |
[32m[20221208 14:05:21 @agent_ppo2.py:179][0m |           0.0406 |         228.8789 |         -23.8796 |
[32m[20221208 14:05:21 @agent_ppo2.py:179][0m |           0.0108 |         226.9713 |         -25.7691 |
[32m[20221208 14:05:22 @agent_ppo2.py:179][0m |          -0.0141 |         227.7031 |         -32.7401 |
[32m[20221208 14:05:22 @agent_ppo2.py:179][0m |          -0.0260 |         225.3431 |         -33.7497 |
[32m[20221208 14:05:22 @agent_ppo2.py:179][0m |          -0.0246 |         224.1078 |         -34.2302 |
[32m[20221208 14:05:22 @agent_ppo2.py:179][0m |          -0.0330 |         223.1776 |         -35.0048 |
[32m[20221208 14:05:22 @agent_ppo2.py:179][0m |          -0.0374 |         223.5954 |         -36.4090 |
[32m[20221208 14:05:22 @agent_ppo2.py:179][0m |          -0.0384 |         222.9161 |         -36.8534 |
[32m[20221208 14:05:22 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.98
[32m[20221208 14:05:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.67
[32m[20221208 14:05:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 970.14
[32m[20221208 14:05:22 @agent_ppo2.py:137][0m Total time:      20.03 min
[32m[20221208 14:05:22 @agent_ppo2.py:139][0m 1638400 total steps have happened
[32m[20221208 14:05:22 @agent_ppo2.py:115][0m #------------------------ Iteration 800 --------------------------#
[32m[20221208 14:05:23 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |           0.0382 |         213.6498 |         -29.4290 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |           0.0653 |         189.3406 |         -23.2572 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |           0.0150 |         177.1197 |         -26.6000 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |          -0.0080 |         169.1547 |         -30.5221 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |          -0.0206 |         164.7890 |         -32.7253 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |          -0.0311 |         160.3859 |         -33.5063 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |          -0.0364 |         158.3646 |         -35.2171 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |          -0.0349 |         156.7795 |         -36.0103 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |          -0.0384 |         156.2641 |         -36.6070 |
[32m[20221208 14:05:23 @agent_ppo2.py:179][0m |          -0.0394 |         153.6359 |         -36.5659 |
[32m[20221208 14:05:23 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.36
[32m[20221208 14:05:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.88
[32m[20221208 14:05:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.72
[32m[20221208 14:05:24 @agent_ppo2.py:137][0m Total time:      20.06 min
[32m[20221208 14:05:24 @agent_ppo2.py:139][0m 1640448 total steps have happened
[32m[20221208 14:05:24 @agent_ppo2.py:115][0m #------------------------ Iteration 801 --------------------------#
[32m[20221208 14:05:24 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:05:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:24 @agent_ppo2.py:179][0m |           0.0519 |         182.2845 |         -29.9577 |
[32m[20221208 14:05:24 @agent_ppo2.py:179][0m |           0.0166 |         140.6592 |         -27.3487 |
[32m[20221208 14:05:24 @agent_ppo2.py:179][0m |          -0.0228 |         129.2618 |         -32.4125 |
[32m[20221208 14:05:24 @agent_ppo2.py:179][0m |          -0.0377 |         122.8547 |         -33.4538 |
[32m[20221208 14:05:24 @agent_ppo2.py:179][0m |          -0.0454 |         118.2844 |         -35.1061 |
[32m[20221208 14:05:25 @agent_ppo2.py:179][0m |          -0.0485 |         113.7678 |         -35.9561 |
[32m[20221208 14:05:25 @agent_ppo2.py:179][0m |          -0.0496 |         111.0424 |         -37.4442 |
[32m[20221208 14:05:25 @agent_ppo2.py:179][0m |          -0.0460 |         108.3775 |         -38.1515 |
[32m[20221208 14:05:25 @agent_ppo2.py:179][0m |          -0.0534 |         106.4056 |         -38.0334 |
[32m[20221208 14:05:25 @agent_ppo2.py:179][0m |          -0.0583 |         104.1692 |         -39.3297 |
[32m[20221208 14:05:25 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:05:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 882.46
[32m[20221208 14:05:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.79
[32m[20221208 14:05:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.04
[32m[20221208 14:05:25 @agent_ppo2.py:137][0m Total time:      20.08 min
[32m[20221208 14:05:25 @agent_ppo2.py:139][0m 1642496 total steps have happened
[32m[20221208 14:05:25 @agent_ppo2.py:115][0m #------------------------ Iteration 802 --------------------------#
[32m[20221208 14:05:26 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |           0.0671 |         299.4210 |         -25.0635 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |           0.0190 |         239.1853 |         -19.8024 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0463 |         230.1175 |         -24.1110 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0617 |         222.3071 |         -26.6910 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0714 |         219.5227 |         -27.5782 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0783 |         216.1459 |         -28.9666 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0803 |         213.1718 |         -30.7247 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0816 |         210.8082 |         -31.0793 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0845 |         209.0641 |         -30.9659 |
[32m[20221208 14:05:26 @agent_ppo2.py:179][0m |          -0.0887 |         206.9926 |         -32.5007 |
[32m[20221208 14:05:26 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:05:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 601.39
[32m[20221208 14:05:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 905.21
[32m[20221208 14:05:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 977.32
[32m[20221208 14:05:27 @agent_ppo2.py:137][0m Total time:      20.11 min
[32m[20221208 14:05:27 @agent_ppo2.py:139][0m 1644544 total steps have happened
[32m[20221208 14:05:27 @agent_ppo2.py:115][0m #------------------------ Iteration 803 --------------------------#
[32m[20221208 14:05:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:27 @agent_ppo2.py:179][0m |           0.0465 |         222.1259 |         -34.9322 |
[32m[20221208 14:05:27 @agent_ppo2.py:179][0m |           0.0427 |         196.9872 |         -26.1634 |
[32m[20221208 14:05:27 @agent_ppo2.py:179][0m |          -0.0204 |         185.5297 |         -31.6283 |
[32m[20221208 14:05:27 @agent_ppo2.py:179][0m |          -0.0435 |         178.7132 |         -34.4506 |
[32m[20221208 14:05:27 @agent_ppo2.py:179][0m |          -0.0563 |         172.1980 |         -35.6042 |
[32m[20221208 14:05:27 @agent_ppo2.py:179][0m |          -0.0617 |         169.5643 |         -36.7967 |
[32m[20221208 14:05:28 @agent_ppo2.py:179][0m |          -0.0677 |         165.8684 |         -38.0983 |
[32m[20221208 14:05:28 @agent_ppo2.py:179][0m |          -0.0710 |         163.0820 |         -39.5249 |
[32m[20221208 14:05:28 @agent_ppo2.py:179][0m |          -0.0738 |         162.0060 |         -40.4394 |
[32m[20221208 14:05:28 @agent_ppo2.py:179][0m |          -0.0764 |         158.8367 |         -40.7412 |
[32m[20221208 14:05:28 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 735.85
[32m[20221208 14:05:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.68
[32m[20221208 14:05:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.18
[32m[20221208 14:05:28 @agent_ppo2.py:137][0m Total time:      20.13 min
[32m[20221208 14:05:28 @agent_ppo2.py:139][0m 1646592 total steps have happened
[32m[20221208 14:05:28 @agent_ppo2.py:115][0m #------------------------ Iteration 804 --------------------------#
[32m[20221208 14:05:29 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |           0.0848 |         270.5363 |         -43.2466 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |           0.0545 |         250.1657 |         -38.1930 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |           0.0246 |         246.6686 |         -39.2155 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |          -0.0147 |         241.9277 |         -42.7968 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |          -0.0294 |         238.5861 |         -45.3131 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |          -0.0429 |         236.9848 |         -47.3459 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |          -0.0500 |         234.8965 |         -49.1210 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |          -0.0539 |         235.4441 |         -49.2577 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |          -0.0600 |         232.9963 |         -51.2080 |
[32m[20221208 14:05:29 @agent_ppo2.py:179][0m |          -0.0623 |         230.7666 |         -53.0359 |
[32m[20221208 14:05:29 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 809.80
[32m[20221208 14:05:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 913.61
[32m[20221208 14:05:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 610.79
[32m[20221208 14:05:30 @agent_ppo2.py:137][0m Total time:      20.15 min
[32m[20221208 14:05:30 @agent_ppo2.py:139][0m 1648640 total steps have happened
[32m[20221208 14:05:30 @agent_ppo2.py:115][0m #------------------------ Iteration 805 --------------------------#
[32m[20221208 14:05:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:30 @agent_ppo2.py:179][0m |           0.0938 |         252.7641 |         -35.6981 |
[32m[20221208 14:05:30 @agent_ppo2.py:179][0m |           0.0780 |         239.5805 |         -22.1571 |
[32m[20221208 14:05:30 @agent_ppo2.py:179][0m |           0.0328 |         232.6453 |         -32.2259 |
[32m[20221208 14:05:30 @agent_ppo2.py:179][0m |           0.0087 |         230.0180 |         -41.2487 |
[32m[20221208 14:05:30 @agent_ppo2.py:179][0m |          -0.0089 |         228.1229 |         -43.4416 |
[32m[20221208 14:05:30 @agent_ppo2.py:179][0m |          -0.0178 |         225.7737 |         -45.4585 |
[32m[20221208 14:05:31 @agent_ppo2.py:179][0m |          -0.0221 |         224.1603 |         -47.0139 |
[32m[20221208 14:05:31 @agent_ppo2.py:179][0m |          -0.0323 |         222.6609 |         -49.2166 |
[32m[20221208 14:05:31 @agent_ppo2.py:179][0m |          -0.0400 |         221.4884 |         -52.2372 |
[32m[20221208 14:05:31 @agent_ppo2.py:179][0m |          -0.0465 |         220.4825 |         -54.2364 |
[32m[20221208 14:05:31 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 877.67
[32m[20221208 14:05:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 894.77
[32m[20221208 14:05:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.60
[32m[20221208 14:05:31 @agent_ppo2.py:137][0m Total time:      20.18 min
[32m[20221208 14:05:31 @agent_ppo2.py:139][0m 1650688 total steps have happened
[32m[20221208 14:05:31 @agent_ppo2.py:115][0m #------------------------ Iteration 806 --------------------------#
[32m[20221208 14:05:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |           0.0388 |         240.1706 |         -40.5778 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |           0.0199 |         227.4058 |         -33.6013 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |           0.0047 |         223.0206 |         -42.5495 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |          -0.0095 |         218.1595 |         -43.6317 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |          -0.0122 |         216.9832 |         -43.2993 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |          -0.0273 |         215.3636 |         -46.0296 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |          -0.0359 |         213.6744 |         -48.2538 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |          -0.0420 |         212.2045 |         -49.9685 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |          -0.0424 |         211.1052 |         -50.6414 |
[32m[20221208 14:05:32 @agent_ppo2.py:179][0m |          -0.0453 |         209.7330 |         -51.7148 |
[32m[20221208 14:05:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.95
[32m[20221208 14:05:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.08
[32m[20221208 14:05:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 911.76
[32m[20221208 14:05:33 @agent_ppo2.py:137][0m Total time:      20.20 min
[32m[20221208 14:05:33 @agent_ppo2.py:139][0m 1652736 total steps have happened
[32m[20221208 14:05:33 @agent_ppo2.py:115][0m #------------------------ Iteration 807 --------------------------#
[32m[20221208 14:05:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:33 @agent_ppo2.py:179][0m |           0.1587 |         214.0877 |         -42.5410 |
[32m[20221208 14:05:33 @agent_ppo2.py:179][0m |           0.0206 |         204.3038 |         -33.5415 |
[32m[20221208 14:05:33 @agent_ppo2.py:179][0m |          -0.0212 |         200.3024 |         -34.6824 |
[32m[20221208 14:05:33 @agent_ppo2.py:179][0m |          -0.0455 |         198.8871 |         -37.1058 |
[32m[20221208 14:05:33 @agent_ppo2.py:179][0m |          -0.0545 |         197.0531 |         -38.9704 |
[32m[20221208 14:05:33 @agent_ppo2.py:179][0m |          -0.0608 |         195.9917 |         -40.5409 |
[32m[20221208 14:05:33 @agent_ppo2.py:179][0m |          -0.0621 |         194.9713 |         -42.1148 |
[32m[20221208 14:05:34 @agent_ppo2.py:179][0m |          -0.0697 |         193.6833 |         -43.9081 |
[32m[20221208 14:05:34 @agent_ppo2.py:179][0m |          -0.0740 |         193.3962 |         -44.4285 |
[32m[20221208 14:05:34 @agent_ppo2.py:179][0m |          -0.0746 |         194.7626 |         -45.7355 |
[32m[20221208 14:05:34 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 637.07
[32m[20221208 14:05:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 913.22
[32m[20221208 14:05:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.96
[32m[20221208 14:05:34 @agent_ppo2.py:137][0m Total time:      20.23 min
[32m[20221208 14:05:34 @agent_ppo2.py:139][0m 1654784 total steps have happened
[32m[20221208 14:05:34 @agent_ppo2.py:115][0m #------------------------ Iteration 808 --------------------------#
[32m[20221208 14:05:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |           0.1072 |         237.7331 |         -46.8282 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |           0.0877 |         220.0090 |         -28.2632 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |           0.0377 |         211.8291 |         -35.6588 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |           0.0047 |         206.8112 |         -48.6789 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |          -0.0087 |         201.6776 |         -51.3668 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |          -0.0237 |         197.2709 |         -54.9756 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |          -0.0305 |         193.9795 |         -56.0017 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |          -0.0362 |         192.0741 |         -57.7769 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |          -0.0389 |         191.0021 |         -60.1051 |
[32m[20221208 14:05:35 @agent_ppo2.py:179][0m |          -0.0431 |         188.8391 |         -60.8613 |
[32m[20221208 14:05:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.88
[32m[20221208 14:05:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 916.05
[32m[20221208 14:05:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 966.67
[32m[20221208 14:05:36 @agent_ppo2.py:137][0m Total time:      20.25 min
[32m[20221208 14:05:36 @agent_ppo2.py:139][0m 1656832 total steps have happened
[32m[20221208 14:05:36 @agent_ppo2.py:115][0m #------------------------ Iteration 809 --------------------------#
[32m[20221208 14:05:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:36 @agent_ppo2.py:179][0m |           0.0856 |         221.2667 |         -40.9231 |
[32m[20221208 14:05:36 @agent_ppo2.py:179][0m |           0.0123 |         205.7240 |         -29.5122 |
[32m[20221208 14:05:36 @agent_ppo2.py:179][0m |          -0.0323 |         197.3207 |         -35.1686 |
[32m[20221208 14:05:36 @agent_ppo2.py:179][0m |          -0.0468 |         192.1239 |         -37.5304 |
[32m[20221208 14:05:36 @agent_ppo2.py:179][0m |          -0.0559 |         185.6406 |         -39.9963 |
[32m[20221208 14:05:36 @agent_ppo2.py:179][0m |          -0.0596 |         183.7842 |         -41.5769 |
[32m[20221208 14:05:36 @agent_ppo2.py:179][0m |          -0.0668 |         181.2496 |         -42.2184 |
[32m[20221208 14:05:37 @agent_ppo2.py:179][0m |          -0.0641 |         179.8717 |         -44.2244 |
[32m[20221208 14:05:37 @agent_ppo2.py:179][0m |          -0.0704 |         178.1674 |         -44.8731 |
[32m[20221208 14:05:37 @agent_ppo2.py:179][0m |          -0.0729 |         177.9146 |         -45.4489 |
[32m[20221208 14:05:37 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:05:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 654.02
[32m[20221208 14:05:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.67
[32m[20221208 14:05:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.00
[32m[20221208 14:05:37 @agent_ppo2.py:137][0m Total time:      20.28 min
[32m[20221208 14:05:37 @agent_ppo2.py:139][0m 1658880 total steps have happened
[32m[20221208 14:05:37 @agent_ppo2.py:115][0m #------------------------ Iteration 810 --------------------------#
[32m[20221208 14:05:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |           0.0372 |         185.9342 |         -44.3036 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |           0.0086 |         165.9534 |         -40.5221 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0249 |         160.9237 |         -44.6171 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0291 |         157.2458 |         -46.9242 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0538 |         153.1105 |         -51.6648 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0655 |         151.5503 |         -54.4983 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0727 |         148.4398 |         -56.8194 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0722 |         145.9984 |         -57.9762 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0746 |         144.6722 |         -59.6191 |
[32m[20221208 14:05:38 @agent_ppo2.py:179][0m |          -0.0693 |         142.6733 |         -59.4183 |
[32m[20221208 14:05:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 640.37
[32m[20221208 14:05:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 928.24
[32m[20221208 14:05:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 877.21
[32m[20221208 14:05:39 @agent_ppo2.py:137][0m Total time:      20.30 min
[32m[20221208 14:05:39 @agent_ppo2.py:139][0m 1660928 total steps have happened
[32m[20221208 14:05:39 @agent_ppo2.py:115][0m #------------------------ Iteration 811 --------------------------#
[32m[20221208 14:05:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |           0.0789 |         257.5384 |         -61.9543 |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |           0.1239 |         235.0961 |         -17.2195 |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |           0.0706 |         224.1152 |         -28.8410 |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |           0.0573 |         216.8526 |         -40.8923 |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |           0.0219 |         212.2962 |         -52.7832 |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |          -0.0053 |         206.5991 |         -60.9684 |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |          -0.0190 |         204.3548 |         -65.4835 |
[32m[20221208 14:05:39 @agent_ppo2.py:179][0m |          -0.0237 |         199.9603 |         -69.5524 |
[32m[20221208 14:05:40 @agent_ppo2.py:179][0m |          -0.0312 |         196.7943 |         -70.5675 |
[32m[20221208 14:05:40 @agent_ppo2.py:179][0m |          -0.0358 |         194.6507 |         -72.6154 |
[32m[20221208 14:05:40 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.61
[32m[20221208 14:05:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.05
[32m[20221208 14:05:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.32
[32m[20221208 14:05:40 @agent_ppo2.py:137][0m Total time:      20.33 min
[32m[20221208 14:05:40 @agent_ppo2.py:139][0m 1662976 total steps have happened
[32m[20221208 14:05:40 @agent_ppo2.py:115][0m #------------------------ Iteration 812 --------------------------#
[32m[20221208 14:05:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |           0.0476 |         261.9187 |         -52.9889 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |           0.0333 |         243.8890 |         -44.9206 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0013 |         237.5631 |         -52.8723 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0159 |         234.7006 |         -56.8924 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0160 |         230.7845 |         -57.4596 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0293 |         229.5284 |         -61.8703 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0422 |         226.8185 |         -66.6921 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0457 |         226.3644 |         -68.0101 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0483 |         223.7094 |         -68.5905 |
[32m[20221208 14:05:41 @agent_ppo2.py:179][0m |          -0.0523 |         222.0717 |         -70.5474 |
[32m[20221208 14:05:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:05:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.64
[32m[20221208 14:05:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.43
[32m[20221208 14:05:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.25
[32m[20221208 14:05:41 @agent_ppo2.py:137][0m Total time:      20.35 min
[32m[20221208 14:05:41 @agent_ppo2.py:139][0m 1665024 total steps have happened
[32m[20221208 14:05:41 @agent_ppo2.py:115][0m #------------------------ Iteration 813 --------------------------#
[32m[20221208 14:05:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |           0.0510 |         258.6617 |         -64.8135 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |           0.0854 |         247.2085 |         -41.3541 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |           0.0185 |         242.8551 |         -55.1522 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |          -0.0064 |         240.6091 |         -61.1640 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |          -0.0197 |         236.6301 |         -65.0200 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |          -0.0284 |         236.7034 |         -68.0450 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |          -0.0329 |         234.1009 |         -70.0381 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |          -0.0388 |         232.0051 |         -70.9709 |
[32m[20221208 14:05:42 @agent_ppo2.py:179][0m |          -0.0402 |         230.5588 |         -71.7230 |
[32m[20221208 14:05:43 @agent_ppo2.py:179][0m |          -0.0427 |         229.4015 |         -72.8483 |
[32m[20221208 14:05:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:05:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 956.98
[32m[20221208 14:05:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 997.23
[32m[20221208 14:05:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.94
[32m[20221208 14:05:43 @agent_ppo2.py:137][0m Total time:      20.38 min
[32m[20221208 14:05:43 @agent_ppo2.py:139][0m 1667072 total steps have happened
[32m[20221208 14:05:43 @agent_ppo2.py:115][0m #------------------------ Iteration 814 --------------------------#
[32m[20221208 14:05:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:43 @agent_ppo2.py:179][0m |           0.0584 |         248.2210 |         -62.8197 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |           0.0556 |         231.0883 |         -56.3119 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |           0.0070 |         223.8534 |         -59.7208 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |          -0.0194 |         219.9015 |         -61.0504 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |          -0.0353 |         216.8549 |         -64.6115 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |          -0.0450 |         214.4674 |         -67.1406 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |          -0.0387 |         212.1643 |         -66.9260 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |          -0.0454 |         210.6914 |         -68.3501 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |          -0.0543 |         208.9911 |         -71.6351 |
[32m[20221208 14:05:44 @agent_ppo2.py:179][0m |          -0.0586 |         206.5945 |         -73.7897 |
[32m[20221208 14:05:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 798.18
[32m[20221208 14:05:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.59
[32m[20221208 14:05:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 922.64
[32m[20221208 14:05:44 @agent_ppo2.py:137][0m Total time:      20.40 min
[32m[20221208 14:05:44 @agent_ppo2.py:139][0m 1669120 total steps have happened
[32m[20221208 14:05:44 @agent_ppo2.py:115][0m #------------------------ Iteration 815 --------------------------#
[32m[20221208 14:05:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |           0.0684 |         231.3533 |         -61.1551 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |           0.0398 |         209.6453 |         -48.0965 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |           0.0021 |         197.3783 |         -59.3899 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |          -0.0093 |         187.7738 |         -59.9033 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |          -0.0252 |         179.8472 |         -64.2764 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |          -0.0326 |         174.1204 |         -67.5661 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |          -0.0332 |         172.0944 |         -68.0245 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |          -0.0364 |         169.5118 |         -70.1289 |
[32m[20221208 14:05:45 @agent_ppo2.py:179][0m |          -0.0419 |         166.5489 |         -71.8316 |
[32m[20221208 14:05:46 @agent_ppo2.py:179][0m |          -0.0459 |         164.9007 |         -73.5761 |
[32m[20221208 14:05:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.53
[32m[20221208 14:05:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.92
[32m[20221208 14:05:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.45
[32m[20221208 14:05:46 @agent_ppo2.py:137][0m Total time:      20.43 min
[32m[20221208 14:05:46 @agent_ppo2.py:139][0m 1671168 total steps have happened
[32m[20221208 14:05:46 @agent_ppo2.py:115][0m #------------------------ Iteration 816 --------------------------#
[32m[20221208 14:05:46 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:05:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:46 @agent_ppo2.py:179][0m |           0.0818 |         269.9606 |         -59.8502 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |           0.0979 |         248.4754 |         -30.6563 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |           0.0442 |         240.3522 |         -40.6636 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |           0.0150 |         235.3726 |         -54.9302 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |          -0.0134 |         232.8877 |         -63.8137 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |          -0.0231 |         230.3017 |         -67.4203 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |          -0.0293 |         228.3824 |         -68.8658 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |          -0.0352 |         227.1493 |         -72.0333 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |          -0.0416 |         225.5170 |         -73.2881 |
[32m[20221208 14:05:47 @agent_ppo2.py:179][0m |          -0.0470 |         225.1620 |         -76.0535 |
[32m[20221208 14:05:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 902.90
[32m[20221208 14:05:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.60
[32m[20221208 14:05:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 864.53
[32m[20221208 14:05:47 @agent_ppo2.py:137][0m Total time:      20.45 min
[32m[20221208 14:05:47 @agent_ppo2.py:139][0m 1673216 total steps have happened
[32m[20221208 14:05:47 @agent_ppo2.py:115][0m #------------------------ Iteration 817 --------------------------#
[32m[20221208 14:05:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |           0.0757 |         216.5757 |         -52.8369 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |           0.0488 |         205.1213 |         -43.4242 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |           0.0123 |         200.6913 |         -52.6696 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |          -0.0138 |         198.1329 |         -58.2102 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |          -0.0261 |         196.3497 |         -59.3635 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |          -0.0329 |         193.5255 |         -62.8236 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |          -0.0392 |         192.7971 |         -65.0697 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |          -0.0417 |         192.5722 |         -67.6627 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |          -0.0431 |         190.1912 |         -68.9505 |
[32m[20221208 14:05:48 @agent_ppo2.py:179][0m |          -0.0484 |         189.2181 |         -69.8122 |
[32m[20221208 14:05:48 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:05:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.98
[32m[20221208 14:05:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.12
[32m[20221208 14:05:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 944.03
[32m[20221208 14:05:49 @agent_ppo2.py:137][0m Total time:      20.47 min
[32m[20221208 14:05:49 @agent_ppo2.py:139][0m 1675264 total steps have happened
[32m[20221208 14:05:49 @agent_ppo2.py:115][0m #------------------------ Iteration 818 --------------------------#
[32m[20221208 14:05:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:49 @agent_ppo2.py:179][0m |           0.0854 |         226.5558 |         -45.8842 |
[32m[20221208 14:05:49 @agent_ppo2.py:179][0m |           0.0319 |         204.6397 |         -35.6615 |
[32m[20221208 14:05:49 @agent_ppo2.py:179][0m |          -0.0244 |         197.0686 |         -45.3667 |
[32m[20221208 14:05:50 @agent_ppo2.py:179][0m |          -0.0436 |         193.4796 |         -48.1027 |
[32m[20221208 14:05:50 @agent_ppo2.py:179][0m |          -0.0568 |         191.6896 |         -50.7539 |
[32m[20221208 14:05:50 @agent_ppo2.py:179][0m |          -0.0633 |         189.7625 |         -51.3413 |
[32m[20221208 14:05:50 @agent_ppo2.py:179][0m |          -0.0633 |         188.9011 |         -54.5696 |
[32m[20221208 14:05:50 @agent_ppo2.py:179][0m |          -0.0683 |         186.2618 |         -56.2853 |
[32m[20221208 14:05:50 @agent_ppo2.py:179][0m |          -0.0753 |         185.9503 |         -58.4034 |
[32m[20221208 14:05:50 @agent_ppo2.py:179][0m |          -0.0743 |         184.9351 |         -58.9996 |
[32m[20221208 14:05:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:05:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 635.13
[32m[20221208 14:05:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 944.31
[32m[20221208 14:05:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.72
[32m[20221208 14:05:50 @agent_ppo2.py:137][0m Total time:      20.50 min
[32m[20221208 14:05:50 @agent_ppo2.py:139][0m 1677312 total steps have happened
[32m[20221208 14:05:50 @agent_ppo2.py:115][0m #------------------------ Iteration 819 --------------------------#
[32m[20221208 14:05:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |           0.0739 |         228.1609 |         -54.9216 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |           0.0449 |         201.3863 |         -40.3968 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0136 |         194.6182 |         -47.6370 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0392 |         191.2524 |         -53.5362 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0497 |         189.0627 |         -58.4055 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0620 |         189.0989 |         -61.3452 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0647 |         187.3249 |         -62.4204 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0672 |         186.3750 |         -63.9078 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0662 |         184.6950 |         -66.0394 |
[32m[20221208 14:05:51 @agent_ppo2.py:179][0m |          -0.0708 |         183.9136 |         -65.4321 |
[32m[20221208 14:05:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 670.22
[32m[20221208 14:05:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 944.83
[32m[20221208 14:05:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.84
[32m[20221208 14:05:52 @agent_ppo2.py:137][0m Total time:      20.52 min
[32m[20221208 14:05:52 @agent_ppo2.py:139][0m 1679360 total steps have happened
[32m[20221208 14:05:52 @agent_ppo2.py:115][0m #------------------------ Iteration 820 --------------------------#
[32m[20221208 14:05:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:52 @agent_ppo2.py:179][0m |           0.1595 |         249.8891 |         -68.2529 |
[32m[20221208 14:05:52 @agent_ppo2.py:179][0m |           0.1439 |         243.4675 |         -20.0301 |
[32m[20221208 14:05:52 @agent_ppo2.py:179][0m |           0.0979 |         240.1904 |         -20.4605 |
[32m[20221208 14:05:53 @agent_ppo2.py:179][0m |           0.0559 |         238.3260 |         -37.8109 |
[32m[20221208 14:05:53 @agent_ppo2.py:179][0m |           0.0220 |         236.1037 |         -56.4974 |
[32m[20221208 14:05:53 @agent_ppo2.py:179][0m |           0.0024 |         236.2208 |         -69.7230 |
[32m[20221208 14:05:53 @agent_ppo2.py:179][0m |          -0.0126 |         234.4269 |         -76.7967 |
[32m[20221208 14:05:53 @agent_ppo2.py:179][0m |          -0.0226 |         235.3599 |         -80.6886 |
[32m[20221208 14:05:53 @agent_ppo2.py:179][0m |          -0.0293 |         233.3434 |         -83.9225 |
[32m[20221208 14:05:53 @agent_ppo2.py:179][0m |          -0.0320 |         233.0434 |         -85.5847 |
[32m[20221208 14:05:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:05:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 915.52
[32m[20221208 14:05:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.80
[32m[20221208 14:05:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 898.77
[32m[20221208 14:05:53 @agent_ppo2.py:137][0m Total time:      20.55 min
[32m[20221208 14:05:53 @agent_ppo2.py:139][0m 1681408 total steps have happened
[32m[20221208 14:05:53 @agent_ppo2.py:115][0m #------------------------ Iteration 821 --------------------------#
[32m[20221208 14:05:54 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:05:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |           0.0829 |         245.1625 |         -67.6012 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |           0.0589 |         240.5884 |         -52.7869 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |           0.0132 |         238.6496 |         -66.2594 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |          -0.0109 |         235.9320 |         -76.0028 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |          -0.0234 |         235.0145 |         -81.3344 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |          -0.0298 |         232.7649 |         -82.9236 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |          -0.0320 |         230.4099 |         -83.3811 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |          -0.0359 |         231.4150 |         -85.4437 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |          -0.0369 |         228.8112 |         -88.0641 |
[32m[20221208 14:05:54 @agent_ppo2.py:179][0m |          -0.0409 |         229.7300 |         -91.1887 |
[32m[20221208 14:05:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.28
[32m[20221208 14:05:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 954.90
[32m[20221208 14:05:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 874.96
[32m[20221208 14:05:55 @agent_ppo2.py:137][0m Total time:      20.57 min
[32m[20221208 14:05:55 @agent_ppo2.py:139][0m 1683456 total steps have happened
[32m[20221208 14:05:55 @agent_ppo2.py:115][0m #------------------------ Iteration 822 --------------------------#
[32m[20221208 14:05:55 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:55 @agent_ppo2.py:179][0m |           0.0603 |         215.2825 |         -69.4062 |
[32m[20221208 14:05:55 @agent_ppo2.py:179][0m |           0.1425 |         188.9283 |         -55.6449 |
[32m[20221208 14:05:55 @agent_ppo2.py:179][0m |           0.0108 |         176.1431 |         -63.4872 |
[32m[20221208 14:05:55 @agent_ppo2.py:179][0m |          -0.0094 |         167.3908 |         -73.9737 |
[32m[20221208 14:05:56 @agent_ppo2.py:179][0m |          -0.0154 |         161.4330 |         -73.3540 |
[32m[20221208 14:05:56 @agent_ppo2.py:179][0m |          -0.0267 |         157.0044 |         -79.3442 |
[32m[20221208 14:05:56 @agent_ppo2.py:179][0m |          -0.0335 |         152.8032 |         -82.8969 |
[32m[20221208 14:05:56 @agent_ppo2.py:179][0m |          -0.0393 |         149.6026 |         -84.3570 |
[32m[20221208 14:05:56 @agent_ppo2.py:179][0m |          -0.0411 |         146.0309 |         -86.3861 |
[32m[20221208 14:05:56 @agent_ppo2.py:179][0m |          -0.0431 |         143.6572 |         -88.0918 |
[32m[20221208 14:05:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 892.76
[32m[20221208 14:05:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.86
[32m[20221208 14:05:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.05
[32m[20221208 14:05:56 @agent_ppo2.py:137][0m Total time:      20.60 min
[32m[20221208 14:05:56 @agent_ppo2.py:139][0m 1685504 total steps have happened
[32m[20221208 14:05:56 @agent_ppo2.py:115][0m #------------------------ Iteration 823 --------------------------#
[32m[20221208 14:05:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |           0.0744 |         209.1929 |         -62.3404 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |           0.0383 |         198.9538 |         -42.9442 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |           0.0114 |         195.8004 |         -44.6692 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |          -0.0306 |         193.6224 |         -53.0732 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |          -0.0432 |         190.4703 |         -55.6969 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |          -0.0551 |         187.9872 |         -58.6842 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |          -0.0606 |         186.1608 |         -61.9135 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |          -0.0641 |         185.0563 |         -64.0213 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |          -0.0713 |         183.1477 |         -66.4309 |
[32m[20221208 14:05:57 @agent_ppo2.py:179][0m |          -0.0710 |         182.0819 |         -67.1858 |
[32m[20221208 14:05:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:05:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 614.56
[32m[20221208 14:05:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.61
[32m[20221208 14:05:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 640.16
[32m[20221208 14:05:58 @agent_ppo2.py:137][0m Total time:      20.62 min
[32m[20221208 14:05:58 @agent_ppo2.py:139][0m 1687552 total steps have happened
[32m[20221208 14:05:58 @agent_ppo2.py:115][0m #------------------------ Iteration 824 --------------------------#
[32m[20221208 14:05:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:05:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:05:58 @agent_ppo2.py:179][0m |           0.0347 |         233.7711 |         -83.0385 |
[32m[20221208 14:05:58 @agent_ppo2.py:179][0m |           0.0298 |         221.5925 |         -69.3161 |
[32m[20221208 14:05:58 @agent_ppo2.py:179][0m |           0.0152 |         214.7925 |         -72.2162 |
[32m[20221208 14:05:58 @agent_ppo2.py:179][0m |          -0.0186 |         211.8464 |         -80.9126 |
[32m[20221208 14:05:58 @agent_ppo2.py:179][0m |          -0.0297 |         208.9435 |         -86.2439 |
[32m[20221208 14:05:59 @agent_ppo2.py:179][0m |          -0.0423 |         207.0252 |         -87.7998 |
[32m[20221208 14:05:59 @agent_ppo2.py:179][0m |          -0.0396 |         205.0408 |         -90.0091 |
[32m[20221208 14:05:59 @agent_ppo2.py:179][0m |          -0.0460 |         203.5467 |         -91.5922 |
[32m[20221208 14:05:59 @agent_ppo2.py:179][0m |          -0.0495 |         202.0284 |         -93.0597 |
[32m[20221208 14:05:59 @agent_ppo2.py:179][0m |          -0.0485 |         200.7551 |         -95.4489 |
[32m[20221208 14:05:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:05:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 905.19
[32m[20221208 14:05:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 921.35
[32m[20221208 14:05:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.70
[32m[20221208 14:05:59 @agent_ppo2.py:137][0m Total time:      20.65 min
[32m[20221208 14:05:59 @agent_ppo2.py:139][0m 1689600 total steps have happened
[32m[20221208 14:05:59 @agent_ppo2.py:115][0m #------------------------ Iteration 825 --------------------------#
[32m[20221208 14:06:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |           0.0728 |         228.1212 |         -83.3539 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |           0.0546 |         208.4246 |         -61.6912 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |           0.0179 |         198.8590 |         -70.8028 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |          -0.0105 |         195.5837 |         -83.1270 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |          -0.0169 |         191.4402 |         -83.0429 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |          -0.0322 |         189.2322 |         -87.3209 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |          -0.0403 |         186.7730 |         -92.0204 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |          -0.0440 |         184.5649 |         -91.9605 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |          -0.0497 |         182.5750 |         -94.4650 |
[32m[20221208 14:06:00 @agent_ppo2.py:179][0m |          -0.0494 |         181.0772 |         -95.7646 |
[32m[20221208 14:06:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 899.75
[32m[20221208 14:06:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.92
[32m[20221208 14:06:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.41
[32m[20221208 14:06:01 @agent_ppo2.py:137][0m Total time:      20.67 min
[32m[20221208 14:06:01 @agent_ppo2.py:139][0m 1691648 total steps have happened
[32m[20221208 14:06:01 @agent_ppo2.py:115][0m #------------------------ Iteration 826 --------------------------#
[32m[20221208 14:06:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:01 @agent_ppo2.py:179][0m |           0.1181 |         210.5918 |         -85.0268 |
[32m[20221208 14:06:01 @agent_ppo2.py:179][0m |          -0.0096 |         192.3697 |         -66.5489 |
[32m[20221208 14:06:01 @agent_ppo2.py:179][0m |          -0.0419 |         185.7626 |         -75.2090 |
[32m[20221208 14:06:01 @agent_ppo2.py:179][0m |          -0.0499 |         181.2242 |         -79.1115 |
[32m[20221208 14:06:01 @agent_ppo2.py:179][0m |          -0.0534 |         176.7202 |         -80.2834 |
[32m[20221208 14:06:02 @agent_ppo2.py:179][0m |          -0.0605 |         174.1856 |         -85.8070 |
[32m[20221208 14:06:02 @agent_ppo2.py:179][0m |          -0.0692 |         170.6793 |         -87.1942 |
[32m[20221208 14:06:02 @agent_ppo2.py:179][0m |          -0.0747 |         167.8752 |         -88.6064 |
[32m[20221208 14:06:02 @agent_ppo2.py:179][0m |          -0.0762 |         166.4702 |         -90.2088 |
[32m[20221208 14:06:02 @agent_ppo2.py:179][0m |          -0.0754 |         163.0154 |         -91.9521 |
[32m[20221208 14:06:02 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 672.03
[32m[20221208 14:06:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.85
[32m[20221208 14:06:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.62
[32m[20221208 14:06:02 @agent_ppo2.py:137][0m Total time:      20.70 min
[32m[20221208 14:06:02 @agent_ppo2.py:139][0m 1693696 total steps have happened
[32m[20221208 14:06:02 @agent_ppo2.py:115][0m #------------------------ Iteration 827 --------------------------#
[32m[20221208 14:06:03 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:06:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |           0.0570 |         265.4277 |         -85.8103 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |           0.0243 |         257.0528 |         -84.5971 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |           0.0207 |         253.8895 |         -93.7381 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |           0.0006 |         250.9510 |         -92.4974 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |          -0.0221 |         248.1797 |        -102.0552 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |          -0.0265 |         246.0978 |        -104.5590 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |          -0.0299 |         244.5004 |        -104.7711 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |          -0.0385 |         241.8926 |        -107.7472 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |          -0.0394 |         241.1966 |        -108.7939 |
[32m[20221208 14:06:03 @agent_ppo2.py:179][0m |          -0.0438 |         239.6043 |        -110.7101 |
[32m[20221208 14:06:03 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 926.16
[32m[20221208 14:06:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.29
[32m[20221208 14:06:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.30
[32m[20221208 14:06:04 @agent_ppo2.py:137][0m Total time:      20.72 min
[32m[20221208 14:06:04 @agent_ppo2.py:139][0m 1695744 total steps have happened
[32m[20221208 14:06:04 @agent_ppo2.py:115][0m #------------------------ Iteration 828 --------------------------#
[32m[20221208 14:06:04 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:04 @agent_ppo2.py:179][0m |           0.0586 |         260.6662 |         -89.2438 |
[32m[20221208 14:06:04 @agent_ppo2.py:179][0m |           0.0462 |         252.1530 |         -67.0015 |
[32m[20221208 14:06:04 @agent_ppo2.py:179][0m |           0.0189 |         248.6306 |         -88.8901 |
[32m[20221208 14:06:04 @agent_ppo2.py:179][0m |           0.0008 |         246.5198 |         -94.6133 |
[32m[20221208 14:06:04 @agent_ppo2.py:179][0m |          -0.0106 |         245.7890 |        -101.2283 |
[32m[20221208 14:06:04 @agent_ppo2.py:179][0m |          -0.0260 |         244.7079 |        -107.3962 |
[32m[20221208 14:06:05 @agent_ppo2.py:179][0m |          -0.0380 |         243.1362 |        -111.5688 |
[32m[20221208 14:06:05 @agent_ppo2.py:179][0m |          -0.0432 |         242.0210 |        -115.2346 |
[32m[20221208 14:06:05 @agent_ppo2.py:179][0m |          -0.0431 |         241.4394 |        -115.6168 |
[32m[20221208 14:06:05 @agent_ppo2.py:179][0m |          -0.0427 |         241.5658 |        -117.5418 |
[32m[20221208 14:06:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:06:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.54
[32m[20221208 14:06:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.11
[32m[20221208 14:06:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.42
[32m[20221208 14:06:05 @agent_ppo2.py:137][0m Total time:      20.75 min
[32m[20221208 14:06:05 @agent_ppo2.py:139][0m 1697792 total steps have happened
[32m[20221208 14:06:05 @agent_ppo2.py:115][0m #------------------------ Iteration 829 --------------------------#
[32m[20221208 14:06:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |           0.0483 |         251.0815 |         -95.7051 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |           0.0540 |         250.3895 |         -79.5779 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |           0.0952 |         246.9267 |         -60.2838 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |           0.0422 |         245.5047 |         -69.9531 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |           0.0064 |         245.6791 |         -86.1653 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |          -0.0093 |         243.1598 |         -93.7055 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |          -0.0174 |         242.4774 |         -96.7899 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |          -0.0218 |         240.7487 |         -99.1159 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |          -0.0303 |         240.2463 |        -104.2737 |
[32m[20221208 14:06:06 @agent_ppo2.py:179][0m |          -0.0292 |         241.2204 |        -105.7043 |
[32m[20221208 14:06:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 922.04
[32m[20221208 14:06:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.44
[32m[20221208 14:06:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 933.60
[32m[20221208 14:06:07 @agent_ppo2.py:137][0m Total time:      20.77 min
[32m[20221208 14:06:07 @agent_ppo2.py:139][0m 1699840 total steps have happened
[32m[20221208 14:06:07 @agent_ppo2.py:115][0m #------------------------ Iteration 830 --------------------------#
[32m[20221208 14:06:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:07 @agent_ppo2.py:179][0m |           0.0763 |         256.0992 |         -95.7189 |
[32m[20221208 14:06:07 @agent_ppo2.py:179][0m |           0.0892 |         245.7383 |         -61.5991 |
[32m[20221208 14:06:07 @agent_ppo2.py:179][0m |           0.0191 |         241.5408 |         -76.8353 |
[32m[20221208 14:06:07 @agent_ppo2.py:179][0m |          -0.0139 |         237.7572 |         -88.4676 |
[32m[20221208 14:06:07 @agent_ppo2.py:179][0m |          -0.0328 |         234.5346 |         -93.2958 |
[32m[20221208 14:06:07 @agent_ppo2.py:179][0m |          -0.0431 |         234.3834 |         -96.9813 |
[32m[20221208 14:06:08 @agent_ppo2.py:179][0m |          -0.0477 |         232.3434 |         -99.3028 |
[32m[20221208 14:06:08 @agent_ppo2.py:179][0m |          -0.0518 |         232.8816 |        -101.5168 |
[32m[20221208 14:06:08 @agent_ppo2.py:179][0m |          -0.0540 |         230.3562 |        -102.8776 |
[32m[20221208 14:06:08 @agent_ppo2.py:179][0m |          -0.0581 |         230.5422 |        -104.2641 |
[32m[20221208 14:06:08 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:06:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 755.30
[32m[20221208 14:06:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.34
[32m[20221208 14:06:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.44
[32m[20221208 14:06:08 @agent_ppo2.py:137][0m Total time:      20.80 min
[32m[20221208 14:06:08 @agent_ppo2.py:139][0m 1701888 total steps have happened
[32m[20221208 14:06:08 @agent_ppo2.py:115][0m #------------------------ Iteration 831 --------------------------#
[32m[20221208 14:06:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |           0.0636 |         231.4920 |         -88.4582 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |           0.0869 |         213.9314 |         -63.6413 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |           0.0267 |         204.8285 |         -72.5683 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |          -0.0034 |         197.8162 |         -82.8384 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |          -0.0272 |         191.6921 |         -90.7534 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |          -0.0363 |         188.2333 |         -95.9568 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |          -0.0396 |         185.5979 |         -97.3825 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |          -0.0472 |         182.2853 |        -100.5758 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |          -0.0514 |         179.3418 |        -103.3590 |
[32m[20221208 14:06:09 @agent_ppo2.py:179][0m |          -0.0546 |         177.5604 |        -103.9142 |
[32m[20221208 14:06:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:06:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 834.18
[32m[20221208 14:06:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 878.82
[32m[20221208 14:06:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.28
[32m[20221208 14:06:10 @agent_ppo2.py:137][0m Total time:      20.82 min
[32m[20221208 14:06:10 @agent_ppo2.py:139][0m 1703936 total steps have happened
[32m[20221208 14:06:10 @agent_ppo2.py:115][0m #------------------------ Iteration 832 --------------------------#
[32m[20221208 14:06:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:10 @agent_ppo2.py:179][0m |           0.0702 |         251.7167 |         -84.2375 |
[32m[20221208 14:06:10 @agent_ppo2.py:179][0m |           0.0217 |         233.3587 |         -78.2672 |
[32m[20221208 14:06:10 @agent_ppo2.py:179][0m |          -0.0089 |         225.4390 |         -89.8931 |
[32m[20221208 14:06:10 @agent_ppo2.py:179][0m |          -0.0247 |         220.2096 |         -91.2060 |
[32m[20221208 14:06:10 @agent_ppo2.py:179][0m |          -0.0338 |         215.1516 |         -97.0736 |
[32m[20221208 14:06:11 @agent_ppo2.py:179][0m |          -0.0413 |         211.3731 |         -97.3542 |
[32m[20221208 14:06:11 @agent_ppo2.py:179][0m |          -0.0484 |         208.9010 |        -101.0118 |
[32m[20221208 14:06:11 @agent_ppo2.py:179][0m |          -0.0505 |         205.2147 |        -102.8412 |
[32m[20221208 14:06:11 @agent_ppo2.py:179][0m |          -0.0529 |         202.9880 |        -103.5655 |
[32m[20221208 14:06:11 @agent_ppo2.py:179][0m |          -0.0501 |         200.1870 |        -105.1448 |
[32m[20221208 14:06:11 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:06:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 881.59
[32m[20221208 14:06:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 907.01
[32m[20221208 14:06:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.36
[32m[20221208 14:06:11 @agent_ppo2.py:137][0m Total time:      20.85 min
[32m[20221208 14:06:11 @agent_ppo2.py:139][0m 1705984 total steps have happened
[32m[20221208 14:06:11 @agent_ppo2.py:115][0m #------------------------ Iteration 833 --------------------------#
[32m[20221208 14:06:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |           0.0579 |         255.1131 |         -76.3938 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |           0.0425 |         239.7373 |         -65.3798 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |           0.0018 |         233.0263 |         -77.7846 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |          -0.0155 |         231.0421 |         -85.2142 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |          -0.0200 |         228.0640 |         -83.7614 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |          -0.0374 |         226.8691 |         -91.6536 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |          -0.0409 |         226.5389 |         -91.0345 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |          -0.0432 |         225.2602 |         -94.1241 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |          -0.0451 |         223.5122 |         -92.9383 |
[32m[20221208 14:06:12 @agent_ppo2.py:179][0m |          -0.0449 |         223.0342 |         -94.8846 |
[32m[20221208 14:06:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 863.51
[32m[20221208 14:06:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.11
[32m[20221208 14:06:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 928.75
[32m[20221208 14:06:13 @agent_ppo2.py:137][0m Total time:      20.87 min
[32m[20221208 14:06:13 @agent_ppo2.py:139][0m 1708032 total steps have happened
[32m[20221208 14:06:13 @agent_ppo2.py:115][0m #------------------------ Iteration 834 --------------------------#
[32m[20221208 14:06:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:13 @agent_ppo2.py:179][0m |           0.1239 |         241.2227 |         -72.7978 |
[32m[20221208 14:06:13 @agent_ppo2.py:179][0m |           0.1234 |         234.5933 |         -21.9257 |
[32m[20221208 14:06:13 @agent_ppo2.py:179][0m |           0.1064 |         230.3106 |         -19.2815 |
[32m[20221208 14:06:13 @agent_ppo2.py:179][0m |           0.0752 |         228.2663 |         -33.4297 |
[32m[20221208 14:06:13 @agent_ppo2.py:179][0m |           0.0414 |         226.7901 |         -57.9258 |
[32m[20221208 14:06:13 @agent_ppo2.py:179][0m |           0.0162 |         225.6801 |         -75.1140 |
[32m[20221208 14:06:14 @agent_ppo2.py:179][0m |          -0.0022 |         224.2976 |         -84.9274 |
[32m[20221208 14:06:14 @agent_ppo2.py:179][0m |          -0.0198 |         224.1730 |         -93.7303 |
[32m[20221208 14:06:14 @agent_ppo2.py:179][0m |          -0.0119 |         222.2871 |         -92.7409 |
[32m[20221208 14:06:14 @agent_ppo2.py:179][0m |          -0.0152 |         221.4664 |         -92.5925 |
[32m[20221208 14:06:14 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:06:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 922.93
[32m[20221208 14:06:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.96
[32m[20221208 14:06:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 912.08
[32m[20221208 14:06:14 @agent_ppo2.py:137][0m Total time:      20.90 min
[32m[20221208 14:06:14 @agent_ppo2.py:139][0m 1710080 total steps have happened
[32m[20221208 14:06:14 @agent_ppo2.py:115][0m #------------------------ Iteration 835 --------------------------#
[32m[20221208 14:06:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |           0.0567 |         240.3083 |         -86.7055 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |           0.0469 |         224.3127 |         -72.4439 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |           0.0076 |         212.7781 |         -80.7455 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |          -0.0128 |         206.5239 |         -90.0885 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |          -0.0336 |         202.7790 |         -96.7140 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |          -0.0413 |         199.9648 |         -99.7344 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |          -0.0468 |         196.2762 |        -102.1278 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |          -0.0493 |         193.4243 |        -101.9967 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |          -0.0522 |         192.3367 |        -105.1959 |
[32m[20221208 14:06:15 @agent_ppo2.py:179][0m |          -0.0541 |         189.8297 |        -107.8484 |
[32m[20221208 14:06:15 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:06:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 893.69
[32m[20221208 14:06:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 929.71
[32m[20221208 14:06:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.08
[32m[20221208 14:06:16 @agent_ppo2.py:137][0m Total time:      20.92 min
[32m[20221208 14:06:16 @agent_ppo2.py:139][0m 1712128 total steps have happened
[32m[20221208 14:06:16 @agent_ppo2.py:115][0m #------------------------ Iteration 836 --------------------------#
[32m[20221208 14:06:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:16 @agent_ppo2.py:179][0m |           0.1051 |         235.8324 |         -73.7840 |
[32m[20221208 14:06:16 @agent_ppo2.py:179][0m |           0.0951 |         218.6280 |         -30.7954 |
[32m[20221208 14:06:16 @agent_ppo2.py:179][0m |           0.0461 |         208.5193 |         -59.6268 |
[32m[20221208 14:06:16 @agent_ppo2.py:179][0m |           0.0170 |         204.0256 |         -73.6162 |
[32m[20221208 14:06:16 @agent_ppo2.py:179][0m |          -0.0036 |         200.1129 |         -81.3450 |
[32m[20221208 14:06:16 @agent_ppo2.py:179][0m |          -0.0173 |         196.2521 |         -87.7041 |
[32m[20221208 14:06:16 @agent_ppo2.py:179][0m |          -0.0216 |         193.2036 |         -89.5369 |
[32m[20221208 14:06:17 @agent_ppo2.py:179][0m |          -0.0303 |         191.4146 |         -93.0512 |
[32m[20221208 14:06:17 @agent_ppo2.py:179][0m |          -0.0366 |         189.4746 |         -95.1669 |
[32m[20221208 14:06:17 @agent_ppo2.py:179][0m |          -0.0300 |         187.2552 |         -94.5520 |
[32m[20221208 14:06:17 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 904.27
[32m[20221208 14:06:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.00
[32m[20221208 14:06:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.15
[32m[20221208 14:06:17 @agent_ppo2.py:137][0m Total time:      20.95 min
[32m[20221208 14:06:17 @agent_ppo2.py:139][0m 1714176 total steps have happened
[32m[20221208 14:06:17 @agent_ppo2.py:115][0m #------------------------ Iteration 837 --------------------------#
[32m[20221208 14:06:17 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |           0.1012 |         253.2062 |         -69.9154 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |           0.0318 |         240.9530 |         -66.5448 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0021 |         235.3050 |         -73.5060 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0234 |         231.9426 |         -79.0195 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0311 |         229.3925 |         -80.8725 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0409 |         227.0084 |         -83.8852 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0356 |         227.5575 |         -83.4423 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0346 |         224.3697 |         -83.9090 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0484 |         223.3223 |         -86.7502 |
[32m[20221208 14:06:18 @agent_ppo2.py:179][0m |          -0.0510 |         222.2819 |         -87.6450 |
[32m[20221208 14:06:18 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:06:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 865.01
[32m[20221208 14:06:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.45
[32m[20221208 14:06:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 877.02
[32m[20221208 14:06:19 @agent_ppo2.py:137][0m Total time:      20.97 min
[32m[20221208 14:06:19 @agent_ppo2.py:139][0m 1716224 total steps have happened
[32m[20221208 14:06:19 @agent_ppo2.py:115][0m #------------------------ Iteration 838 --------------------------#
[32m[20221208 14:06:19 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:06:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:19 @agent_ppo2.py:179][0m |           0.0498 |         254.6664 |         -81.1004 |
[32m[20221208 14:06:19 @agent_ppo2.py:179][0m |           0.0219 |         242.1228 |         -78.3054 |
[32m[20221208 14:06:19 @agent_ppo2.py:179][0m |           0.0017 |         238.7511 |         -79.5052 |
[32m[20221208 14:06:19 @agent_ppo2.py:179][0m |          -0.0197 |         238.4001 |         -85.8745 |
[32m[20221208 14:06:19 @agent_ppo2.py:179][0m |          -0.0298 |         235.9911 |         -91.1148 |
[32m[20221208 14:06:19 @agent_ppo2.py:179][0m |          -0.0346 |         234.8886 |         -93.8021 |
[32m[20221208 14:06:19 @agent_ppo2.py:179][0m |          -0.0393 |         234.9814 |         -95.9968 |
[32m[20221208 14:06:20 @agent_ppo2.py:179][0m |          -0.0448 |         234.5123 |         -98.5853 |
[32m[20221208 14:06:20 @agent_ppo2.py:179][0m |          -0.0451 |         233.4299 |         -98.9856 |
[32m[20221208 14:06:20 @agent_ppo2.py:179][0m |          -0.0453 |         233.7437 |        -100.7841 |
[32m[20221208 14:06:20 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:06:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.26
[32m[20221208 14:06:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.94
[32m[20221208 14:06:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 974.38
[32m[20221208 14:06:20 @agent_ppo2.py:137][0m Total time:      20.99 min
[32m[20221208 14:06:20 @agent_ppo2.py:139][0m 1718272 total steps have happened
[32m[20221208 14:06:20 @agent_ppo2.py:115][0m #------------------------ Iteration 839 --------------------------#
[32m[20221208 14:06:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |           0.0773 |         234.3903 |         -78.2391 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |           0.0826 |         221.7829 |         -40.5865 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |           0.0370 |         213.9279 |         -59.3522 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |           0.0072 |         208.7661 |         -74.3585 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |          -0.0134 |         205.6070 |         -83.0793 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |          -0.0248 |         204.4039 |         -86.8270 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |          -0.0326 |         202.1352 |         -90.0827 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |          -0.0345 |         200.7609 |         -91.6584 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |          -0.0355 |         198.5963 |         -92.9004 |
[32m[20221208 14:06:21 @agent_ppo2.py:179][0m |          -0.0392 |         195.5242 |         -92.9484 |
[32m[20221208 14:06:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 922.50
[32m[20221208 14:06:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.09
[32m[20221208 14:06:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 925.11
[32m[20221208 14:06:21 @agent_ppo2.py:137][0m Total time:      21.02 min
[32m[20221208 14:06:21 @agent_ppo2.py:139][0m 1720320 total steps have happened
[32m[20221208 14:06:21 @agent_ppo2.py:115][0m #------------------------ Iteration 840 --------------------------#
[32m[20221208 14:06:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:06:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |           0.1019 |         223.1890 |         -63.1191 |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |           0.1036 |         205.6047 |         -29.5858 |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |           0.0673 |         198.6086 |         -42.3104 |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |           0.0233 |         193.3754 |         -66.5607 |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |          -0.0015 |         189.8381 |         -77.0546 |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |          -0.0140 |         186.8942 |         -81.3702 |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |          -0.0197 |         184.3047 |         -86.5377 |
[32m[20221208 14:06:22 @agent_ppo2.py:179][0m |          -0.0281 |         181.6852 |         -89.0431 |
[32m[20221208 14:06:23 @agent_ppo2.py:179][0m |          -0.0346 |         179.3161 |         -91.9794 |
[32m[20221208 14:06:23 @agent_ppo2.py:179][0m |          -0.0352 |         177.1920 |         -93.2793 |
[32m[20221208 14:06:23 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:06:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.54
[32m[20221208 14:06:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.55
[32m[20221208 14:06:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.83
[32m[20221208 14:06:23 @agent_ppo2.py:137][0m Total time:      21.04 min
[32m[20221208 14:06:23 @agent_ppo2.py:139][0m 1722368 total steps have happened
[32m[20221208 14:06:23 @agent_ppo2.py:115][0m #------------------------ Iteration 841 --------------------------#
[32m[20221208 14:06:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:06:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |           0.0450 |         256.0028 |         -85.3218 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |           0.0179 |         230.2913 |         -73.4473 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0078 |         221.1602 |         -79.2917 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0276 |         213.1163 |         -85.9000 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0387 |         209.2515 |         -89.3958 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0399 |         207.3476 |         -91.6858 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0337 |         206.8924 |         -92.0347 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0397 |         205.0694 |         -90.7264 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0456 |         204.1066 |         -93.0646 |
[32m[20221208 14:06:24 @agent_ppo2.py:179][0m |          -0.0489 |         202.0383 |         -96.2820 |
[32m[20221208 14:06:24 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:06:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 906.37
[32m[20221208 14:06:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.59
[32m[20221208 14:06:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 905.89
[32m[20221208 14:06:25 @agent_ppo2.py:137][0m Total time:      21.07 min
[32m[20221208 14:06:25 @agent_ppo2.py:139][0m 1724416 total steps have happened
[32m[20221208 14:06:25 @agent_ppo2.py:115][0m #------------------------ Iteration 842 --------------------------#
[32m[20221208 14:06:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:06:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:25 @agent_ppo2.py:179][0m |           0.0638 |         245.0255 |         -86.9764 |
[32m[20221208 14:06:25 @agent_ppo2.py:179][0m |           0.0789 |         239.6883 |         -54.1553 |
[32m[20221208 14:06:25 @agent_ppo2.py:179][0m |           0.0252 |         237.2056 |         -67.6233 |
[32m[20221208 14:06:25 @agent_ppo2.py:179][0m |           0.0002 |         236.1459 |         -76.3422 |
[32m[20221208 14:06:25 @agent_ppo2.py:179][0m |          -0.0150 |         235.4382 |         -81.7485 |
[32m[20221208 14:06:25 @agent_ppo2.py:179][0m |          -0.0265 |         234.3425 |         -87.3243 |
[32m[20221208 14:06:26 @agent_ppo2.py:179][0m |          -0.0311 |         235.6378 |         -89.6893 |
[32m[20221208 14:06:26 @agent_ppo2.py:179][0m |          -0.0387 |         235.0543 |         -91.7624 |
[32m[20221208 14:06:26 @agent_ppo2.py:179][0m |          -0.0419 |         232.7989 |         -94.0189 |
[32m[20221208 14:06:26 @agent_ppo2.py:179][0m |          -0.0419 |         233.4767 |         -95.4674 |
[32m[20221208 14:06:26 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:06:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 908.40
[32m[20221208 14:06:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 923.98
[32m[20221208 14:06:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.06
[32m[20221208 14:06:26 @agent_ppo2.py:137][0m Total time:      21.10 min
[32m[20221208 14:06:26 @agent_ppo2.py:139][0m 1726464 total steps have happened
[32m[20221208 14:06:26 @agent_ppo2.py:115][0m #------------------------ Iteration 843 --------------------------#
[32m[20221208 14:06:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |           0.0473 |         224.5440 |         -78.4275 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |           0.0318 |         199.7222 |         -65.9788 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0101 |         191.1533 |         -78.5382 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0240 |         186.7190 |         -85.4682 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0369 |         183.3496 |         -88.7470 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0421 |         181.2356 |         -92.3737 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0359 |         179.7677 |         -91.5261 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0420 |         176.9572 |         -93.0352 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0490 |         174.9537 |         -96.5932 |
[32m[20221208 14:06:27 @agent_ppo2.py:179][0m |          -0.0510 |         174.7373 |         -98.2778 |
[32m[20221208 14:06:27 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:06:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.95
[32m[20221208 14:06:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.06
[32m[20221208 14:06:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 940.67
[32m[20221208 14:06:28 @agent_ppo2.py:137][0m Total time:      21.12 min
[32m[20221208 14:06:28 @agent_ppo2.py:139][0m 1728512 total steps have happened
[32m[20221208 14:06:28 @agent_ppo2.py:115][0m #------------------------ Iteration 844 --------------------------#
[32m[20221208 14:06:28 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:06:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:28 @agent_ppo2.py:179][0m |           0.0564 |         262.5019 |         -79.3425 |
[32m[20221208 14:06:28 @agent_ppo2.py:179][0m |           0.0681 |         251.4793 |         -46.0913 |
[32m[20221208 14:06:28 @agent_ppo2.py:179][0m |           0.0348 |         247.6849 |         -52.5235 |
[32m[20221208 14:06:28 @agent_ppo2.py:179][0m |          -0.0044 |         244.9036 |         -72.3081 |
[32m[20221208 14:06:29 @agent_ppo2.py:179][0m |          -0.0173 |         242.8873 |         -79.9243 |
[32m[20221208 14:06:29 @agent_ppo2.py:179][0m |          -0.0301 |         240.5529 |         -82.3550 |
[32m[20221208 14:06:29 @agent_ppo2.py:179][0m |          -0.0303 |         240.7243 |         -83.3693 |
[32m[20221208 14:06:29 @agent_ppo2.py:179][0m |          -0.0399 |         238.2391 |         -86.2806 |
[32m[20221208 14:06:29 @agent_ppo2.py:179][0m |          -0.0464 |         237.5052 |         -86.9551 |
[32m[20221208 14:06:29 @agent_ppo2.py:179][0m |          -0.0474 |         236.7938 |         -88.4734 |
[32m[20221208 14:06:29 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:06:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 889.99
[32m[20221208 14:06:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.87
[32m[20221208 14:06:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.97
[32m[20221208 14:06:29 @agent_ppo2.py:137][0m Total time:      21.15 min
[32m[20221208 14:06:29 @agent_ppo2.py:139][0m 1730560 total steps have happened
[32m[20221208 14:06:29 @agent_ppo2.py:115][0m #------------------------ Iteration 845 --------------------------#
[32m[20221208 14:06:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:06:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |           0.1024 |         236.8699 |         -70.1726 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |           0.0761 |         220.5266 |         -47.1787 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |           0.0472 |         213.5253 |         -48.8492 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |           0.0137 |         206.5881 |         -64.7066 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |          -0.0079 |         199.8041 |         -74.6715 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |          -0.0207 |         196.1304 |         -80.3019 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |          -0.0273 |         193.6380 |         -82.9895 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |          -0.0301 |         190.9138 |         -85.5030 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |          -0.0312 |         189.7447 |         -85.7226 |
[32m[20221208 14:06:30 @agent_ppo2.py:179][0m |          -0.0354 |         187.9332 |         -88.8870 |
[32m[20221208 14:06:30 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:06:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.90
[32m[20221208 14:06:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 968.57
[32m[20221208 14:06:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 916.93
[32m[20221208 14:06:31 @agent_ppo2.py:137][0m Total time:      21.17 min
[32m[20221208 14:06:31 @agent_ppo2.py:139][0m 1732608 total steps have happened
[32m[20221208 14:06:31 @agent_ppo2.py:115][0m #------------------------ Iteration 846 --------------------------#
[32m[20221208 14:06:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:31 @agent_ppo2.py:179][0m |           0.0857 |         248.6618 |         -70.7676 |
[32m[20221208 14:06:31 @agent_ppo2.py:179][0m |           0.0572 |         237.4497 |         -57.4898 |
[32m[20221208 14:06:31 @agent_ppo2.py:179][0m |           0.0123 |         231.4106 |         -67.6712 |
[32m[20221208 14:06:32 @agent_ppo2.py:179][0m |          -0.0046 |         228.0259 |         -73.3292 |
[32m[20221208 14:06:32 @agent_ppo2.py:179][0m |          -0.0242 |         225.7667 |         -79.8415 |
[32m[20221208 14:06:32 @agent_ppo2.py:179][0m |          -0.0268 |         224.7466 |         -81.9991 |
[32m[20221208 14:06:32 @agent_ppo2.py:179][0m |          -0.0302 |         223.8677 |         -79.7799 |
[32m[20221208 14:06:32 @agent_ppo2.py:179][0m |          -0.0373 |         221.9225 |         -83.6897 |
[32m[20221208 14:06:32 @agent_ppo2.py:179][0m |          -0.0383 |         220.5224 |         -84.2603 |
[32m[20221208 14:06:32 @agent_ppo2.py:179][0m |          -0.0445 |         219.7834 |         -87.0117 |
[32m[20221208 14:06:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.40
[32m[20221208 14:06:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.77
[32m[20221208 14:06:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.99
[32m[20221208 14:06:32 @agent_ppo2.py:137][0m Total time:      21.20 min
[32m[20221208 14:06:32 @agent_ppo2.py:139][0m 1734656 total steps have happened
[32m[20221208 14:06:32 @agent_ppo2.py:115][0m #------------------------ Iteration 847 --------------------------#
[32m[20221208 14:06:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |           0.0367 |         245.4791 |         -77.3144 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |           0.0654 |         236.7362 |         -61.8179 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |           0.0229 |         233.3853 |         -61.2336 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |          -0.0045 |         231.6215 |         -69.2828 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |          -0.0220 |         230.2185 |         -74.7400 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |          -0.0338 |         229.8059 |         -76.9057 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |          -0.0426 |         228.2789 |         -82.6354 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |          -0.0448 |         227.8774 |         -85.8500 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |          -0.0469 |         227.2710 |         -86.9755 |
[32m[20221208 14:06:33 @agent_ppo2.py:179][0m |          -0.0480 |         228.1554 |         -89.3432 |
[32m[20221208 14:06:33 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:06:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.35
[32m[20221208 14:06:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.26
[32m[20221208 14:06:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.82
[32m[20221208 14:06:34 @agent_ppo2.py:137][0m Total time:      21.22 min
[32m[20221208 14:06:34 @agent_ppo2.py:139][0m 1736704 total steps have happened
[32m[20221208 14:06:34 @agent_ppo2.py:115][0m #------------------------ Iteration 848 --------------------------#
[32m[20221208 14:06:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:34 @agent_ppo2.py:179][0m |           0.0456 |         240.0386 |         -73.4493 |
[32m[20221208 14:06:34 @agent_ppo2.py:179][0m |           0.0215 |         232.5329 |         -73.4023 |
[32m[20221208 14:06:34 @agent_ppo2.py:179][0m |          -0.0045 |         228.5245 |         -74.8608 |
[32m[20221208 14:06:34 @agent_ppo2.py:179][0m |          -0.0237 |         227.1792 |         -81.6107 |
[32m[20221208 14:06:35 @agent_ppo2.py:179][0m |          -0.0232 |         227.1217 |         -80.8706 |
[32m[20221208 14:06:35 @agent_ppo2.py:179][0m |          -0.0258 |         226.2028 |         -81.8003 |
[32m[20221208 14:06:35 @agent_ppo2.py:179][0m |          -0.0379 |         225.3701 |         -85.4019 |
[32m[20221208 14:06:35 @agent_ppo2.py:179][0m |          -0.0414 |         226.3452 |         -87.5172 |
[32m[20221208 14:06:35 @agent_ppo2.py:179][0m |          -0.0426 |         224.9173 |         -87.8169 |
[32m[20221208 14:06:35 @agent_ppo2.py:179][0m |          -0.0489 |         224.3978 |         -90.6650 |
[32m[20221208 14:06:35 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 920.29
[32m[20221208 14:06:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.29
[32m[20221208 14:06:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 922.46
[32m[20221208 14:06:35 @agent_ppo2.py:137][0m Total time:      21.25 min
[32m[20221208 14:06:35 @agent_ppo2.py:139][0m 1738752 total steps have happened
[32m[20221208 14:06:35 @agent_ppo2.py:115][0m #------------------------ Iteration 849 --------------------------#
[32m[20221208 14:06:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |           0.0512 |         242.9071 |         -71.4846 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |           0.0570 |         237.4204 |         -55.2391 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |           0.0074 |         232.4511 |         -65.9486 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |          -0.0120 |         225.5913 |         -73.7310 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |          -0.0273 |         223.3902 |         -79.7387 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |          -0.0336 |         222.3803 |         -83.5192 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |          -0.0346 |         218.9149 |         -82.0191 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |          -0.0377 |         216.8717 |         -83.3534 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |          -0.0433 |         215.9743 |         -86.8271 |
[32m[20221208 14:06:36 @agent_ppo2.py:179][0m |          -0.0426 |         214.1393 |         -87.5137 |
[32m[20221208 14:06:36 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 960.71
[32m[20221208 14:06:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.61
[32m[20221208 14:06:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 970.47
[32m[20221208 14:06:37 @agent_ppo2.py:137][0m Total time:      21.27 min
[32m[20221208 14:06:37 @agent_ppo2.py:139][0m 1740800 total steps have happened
[32m[20221208 14:06:37 @agent_ppo2.py:115][0m #------------------------ Iteration 850 --------------------------#
[32m[20221208 14:06:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:37 @agent_ppo2.py:179][0m |           0.0442 |         238.4330 |         -75.9924 |
[32m[20221208 14:06:37 @agent_ppo2.py:179][0m |           0.0227 |         235.2179 |         -64.0055 |
[32m[20221208 14:06:37 @agent_ppo2.py:179][0m |           0.0132 |         232.7094 |         -65.6007 |
[32m[20221208 14:06:37 @agent_ppo2.py:179][0m |          -0.0129 |         232.5507 |         -74.0276 |
[32m[20221208 14:06:38 @agent_ppo2.py:179][0m |          -0.0203 |         230.6078 |         -79.0330 |
[32m[20221208 14:06:38 @agent_ppo2.py:179][0m |          -0.0281 |         229.9474 |         -79.9634 |
[32m[20221208 14:06:38 @agent_ppo2.py:179][0m |          -0.0360 |         229.6513 |         -81.0779 |
[32m[20221208 14:06:38 @agent_ppo2.py:179][0m |          -0.0395 |         229.7674 |         -83.0622 |
[32m[20221208 14:06:38 @agent_ppo2.py:179][0m |          -0.0395 |         229.1863 |         -83.8932 |
[32m[20221208 14:06:38 @agent_ppo2.py:179][0m |          -0.0400 |         228.4464 |         -84.9315 |
[32m[20221208 14:06:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.79
[32m[20221208 14:06:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.71
[32m[20221208 14:06:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.17
[32m[20221208 14:06:38 @agent_ppo2.py:137][0m Total time:      21.30 min
[32m[20221208 14:06:38 @agent_ppo2.py:139][0m 1742848 total steps have happened
[32m[20221208 14:06:38 @agent_ppo2.py:115][0m #------------------------ Iteration 851 --------------------------#
[32m[20221208 14:06:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |           0.0771 |         232.7682 |         -66.3654 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |           0.0383 |         209.4221 |         -56.9732 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |           0.0095 |         194.6542 |         -70.2763 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |          -0.0113 |         186.9943 |         -75.8486 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |          -0.0224 |         181.6296 |         -81.9757 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |          -0.0264 |         175.3676 |         -84.1902 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |          -0.0321 |         169.5700 |         -85.8629 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |          -0.0381 |         168.0492 |         -88.1449 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |          -0.0407 |         164.3239 |         -90.6994 |
[32m[20221208 14:06:39 @agent_ppo2.py:179][0m |          -0.0431 |         162.3577 |         -92.2942 |
[32m[20221208 14:06:39 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 930.17
[32m[20221208 14:06:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.22
[32m[20221208 14:06:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 911.83
[32m[20221208 14:06:40 @agent_ppo2.py:137][0m Total time:      21.32 min
[32m[20221208 14:06:40 @agent_ppo2.py:139][0m 1744896 total steps have happened
[32m[20221208 14:06:40 @agent_ppo2.py:115][0m #------------------------ Iteration 852 --------------------------#
[32m[20221208 14:06:40 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:40 @agent_ppo2.py:179][0m |           0.0527 |         247.3206 |         -70.1625 |
[32m[20221208 14:06:40 @agent_ppo2.py:179][0m |           0.0278 |         238.7251 |         -60.6536 |
[32m[20221208 14:06:40 @agent_ppo2.py:179][0m |           0.0127 |         233.5584 |         -62.6995 |
[32m[20221208 14:06:40 @agent_ppo2.py:179][0m |          -0.0141 |         231.9192 |         -66.8413 |
[32m[20221208 14:06:40 @agent_ppo2.py:179][0m |          -0.0279 |         230.1931 |         -72.9925 |
[32m[20221208 14:06:41 @agent_ppo2.py:179][0m |          -0.0302 |         231.0381 |         -74.8452 |
[32m[20221208 14:06:41 @agent_ppo2.py:179][0m |          -0.0369 |         229.6782 |         -76.0835 |
[32m[20221208 14:06:41 @agent_ppo2.py:179][0m |          -0.0413 |         229.4023 |         -78.8507 |
[32m[20221208 14:06:41 @agent_ppo2.py:179][0m |          -0.0449 |         225.9528 |         -80.6949 |
[32m[20221208 14:06:41 @agent_ppo2.py:179][0m |          -0.0470 |         226.2306 |         -81.8030 |
[32m[20221208 14:06:41 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:06:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.22
[32m[20221208 14:06:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.36
[32m[20221208 14:06:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 899.71
[32m[20221208 14:06:41 @agent_ppo2.py:137][0m Total time:      21.35 min
[32m[20221208 14:06:41 @agent_ppo2.py:139][0m 1746944 total steps have happened
[32m[20221208 14:06:41 @agent_ppo2.py:115][0m #------------------------ Iteration 853 --------------------------#
[32m[20221208 14:06:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |           0.0482 |         245.6783 |         -76.5967 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |           0.0750 |         239.4951 |         -38.5232 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |           0.0361 |         235.9884 |         -52.0345 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |           0.0311 |         233.6554 |         -54.9723 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |          -0.0009 |         232.6125 |         -70.5599 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |          -0.0155 |         230.0681 |         -77.6492 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |          -0.0253 |         230.4325 |         -82.2671 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |          -0.0322 |         228.1317 |         -85.5884 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |          -0.0353 |         228.3121 |         -89.2349 |
[32m[20221208 14:06:42 @agent_ppo2.py:179][0m |          -0.0363 |         226.4437 |         -91.3419 |
[32m[20221208 14:06:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 932.58
[32m[20221208 14:06:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.45
[32m[20221208 14:06:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 808.85
[32m[20221208 14:06:43 @agent_ppo2.py:137][0m Total time:      21.37 min
[32m[20221208 14:06:43 @agent_ppo2.py:139][0m 1748992 total steps have happened
[32m[20221208 14:06:43 @agent_ppo2.py:115][0m #------------------------ Iteration 854 --------------------------#
[32m[20221208 14:06:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:43 @agent_ppo2.py:179][0m |           0.1147 |         211.5980 |         -57.3555 |
[32m[20221208 14:06:43 @agent_ppo2.py:179][0m |           0.0175 |         191.7835 |         -48.9540 |
[32m[20221208 14:06:43 @agent_ppo2.py:179][0m |          -0.0226 |         182.0070 |         -54.3448 |
[32m[20221208 14:06:43 @agent_ppo2.py:179][0m |          -0.0460 |         178.3054 |         -61.3911 |
[32m[20221208 14:06:43 @agent_ppo2.py:179][0m |          -0.0571 |         174.7192 |         -64.4793 |
[32m[20221208 14:06:43 @agent_ppo2.py:179][0m |          -0.0626 |         171.5087 |         -65.8126 |
[32m[20221208 14:06:44 @agent_ppo2.py:179][0m |          -0.0691 |         169.8382 |         -69.8533 |
[32m[20221208 14:06:44 @agent_ppo2.py:179][0m |          -0.0693 |         168.5824 |         -70.6142 |
[32m[20221208 14:06:44 @agent_ppo2.py:179][0m |          -0.0744 |         168.6628 |         -73.0731 |
[32m[20221208 14:06:44 @agent_ppo2.py:179][0m |          -0.0777 |         166.1028 |         -74.4287 |
[32m[20221208 14:06:44 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:06:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 670.57
[32m[20221208 14:06:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 992.28
[32m[20221208 14:06:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 665.03
[32m[20221208 14:06:44 @agent_ppo2.py:137][0m Total time:      21.40 min
[32m[20221208 14:06:44 @agent_ppo2.py:139][0m 1751040 total steps have happened
[32m[20221208 14:06:44 @agent_ppo2.py:115][0m #------------------------ Iteration 855 --------------------------#
[32m[20221208 14:06:45 @agent_ppo2.py:121][0m Sampling time: 0.41 s by 1 slaves
[32m[20221208 14:06:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |           0.0439 |         169.9270 |         -69.6115 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |           0.0136 |         140.8727 |         -53.9547 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0363 |         131.2577 |         -61.4137 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0577 |         126.9994 |         -65.3678 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0662 |         122.8298 |         -68.5767 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0726 |         117.5861 |         -70.7706 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0795 |         115.0953 |         -74.0074 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0836 |         112.6701 |         -75.9032 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0867 |         110.8470 |         -79.1920 |
[32m[20221208 14:06:45 @agent_ppo2.py:179][0m |          -0.0863 |         108.4449 |         -79.8480 |
[32m[20221208 14:06:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 576.12
[32m[20221208 14:06:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 869.05
[32m[20221208 14:06:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 958.01
[32m[20221208 14:06:46 @agent_ppo2.py:137][0m Total time:      21.42 min
[32m[20221208 14:06:46 @agent_ppo2.py:139][0m 1753088 total steps have happened
[32m[20221208 14:06:46 @agent_ppo2.py:115][0m #------------------------ Iteration 856 --------------------------#
[32m[20221208 14:06:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:46 @agent_ppo2.py:179][0m |           0.0699 |         271.6567 |        -104.3811 |
[32m[20221208 14:06:46 @agent_ppo2.py:179][0m |           0.1403 |         261.1485 |         -58.2057 |
[32m[20221208 14:06:46 @agent_ppo2.py:179][0m |           0.0829 |         255.6145 |         -52.4746 |
[32m[20221208 14:06:46 @agent_ppo2.py:179][0m |           0.0171 |         251.7784 |         -76.4223 |
[32m[20221208 14:06:46 @agent_ppo2.py:179][0m |          -0.0033 |         249.2240 |         -88.6572 |
[32m[20221208 14:06:46 @agent_ppo2.py:179][0m |          -0.0180 |         247.9764 |         -97.0572 |
[32m[20221208 14:06:46 @agent_ppo2.py:179][0m |          -0.0265 |         245.9007 |        -103.1003 |
[32m[20221208 14:06:47 @agent_ppo2.py:179][0m |          -0.0312 |         245.7038 |        -104.9864 |
[32m[20221208 14:06:47 @agent_ppo2.py:179][0m |          -0.0363 |         243.6263 |        -110.6495 |
[32m[20221208 14:06:47 @agent_ppo2.py:179][0m |          -0.0391 |         241.7025 |        -113.7665 |
[32m[20221208 14:06:47 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 918.99
[32m[20221208 14:06:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.18
[32m[20221208 14:06:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.89
[32m[20221208 14:06:47 @agent_ppo2.py:137][0m Total time:      21.44 min
[32m[20221208 14:06:47 @agent_ppo2.py:139][0m 1755136 total steps have happened
[32m[20221208 14:06:47 @agent_ppo2.py:115][0m #------------------------ Iteration 857 --------------------------#
[32m[20221208 14:06:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |           0.1127 |         270.8865 |         -78.5785 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |           0.1105 |         256.9196 |         -57.7244 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |           0.0206 |         253.0659 |         -75.4188 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |          -0.0126 |         248.6936 |         -88.8231 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |          -0.0294 |         246.0077 |         -92.1761 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |          -0.0369 |         244.8790 |         -94.0099 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |          -0.0437 |         243.6166 |         -95.6203 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |          -0.0502 |         242.8160 |         -97.2939 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |          -0.0531 |         241.4454 |        -100.4685 |
[32m[20221208 14:06:48 @agent_ppo2.py:179][0m |          -0.0550 |         241.3288 |         -99.2054 |
[32m[20221208 14:06:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 886.36
[32m[20221208 14:06:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.49
[32m[20221208 14:06:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 680.23
[32m[20221208 14:06:49 @agent_ppo2.py:137][0m Total time:      21.47 min
[32m[20221208 14:06:49 @agent_ppo2.py:139][0m 1757184 total steps have happened
[32m[20221208 14:06:49 @agent_ppo2.py:115][0m #------------------------ Iteration 858 --------------------------#
[32m[20221208 14:06:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |           0.0439 |         248.1177 |         -82.7633 |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |           0.0357 |         236.9421 |         -70.9820 |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |          -0.0039 |         232.3823 |         -78.5578 |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |          -0.0257 |         231.6728 |         -87.2613 |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |          -0.0367 |         227.9597 |         -89.9855 |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |          -0.0418 |         227.2829 |         -93.6244 |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |          -0.0463 |         225.3727 |         -95.5306 |
[32m[20221208 14:06:49 @agent_ppo2.py:179][0m |          -0.0461 |         223.6652 |         -96.9405 |
[32m[20221208 14:06:50 @agent_ppo2.py:179][0m |          -0.0488 |         222.6764 |         -98.4035 |
[32m[20221208 14:06:50 @agent_ppo2.py:179][0m |          -0.0500 |         221.9484 |        -100.8124 |
[32m[20221208 14:06:50 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.67
[32m[20221208 14:06:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.69
[32m[20221208 14:06:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.79
[32m[20221208 14:06:50 @agent_ppo2.py:137][0m Total time:      21.49 min
[32m[20221208 14:06:50 @agent_ppo2.py:139][0m 1759232 total steps have happened
[32m[20221208 14:06:50 @agent_ppo2.py:115][0m #------------------------ Iteration 859 --------------------------#
[32m[20221208 14:06:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |           0.0828 |         254.1115 |         -87.7586 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |           0.0454 |         249.5591 |         -73.5633 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |           0.0223 |         245.3606 |         -73.5901 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |          -0.0100 |         243.1503 |         -88.5992 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |          -0.0237 |         241.6255 |         -96.1564 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |          -0.0319 |         240.7631 |         -96.7342 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |          -0.0371 |         239.5174 |        -100.8889 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |          -0.0366 |         239.1120 |        -101.0101 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |          -0.0334 |         238.2123 |        -100.7789 |
[32m[20221208 14:06:51 @agent_ppo2.py:179][0m |          -0.0386 |         238.3143 |        -102.6970 |
[32m[20221208 14:06:51 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 930.65
[32m[20221208 14:06:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.17
[32m[20221208 14:06:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.05
[32m[20221208 14:06:51 @agent_ppo2.py:137][0m Total time:      21.52 min
[32m[20221208 14:06:51 @agent_ppo2.py:139][0m 1761280 total steps have happened
[32m[20221208 14:06:51 @agent_ppo2.py:115][0m #------------------------ Iteration 860 --------------------------#
[32m[20221208 14:06:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |           0.0373 |         247.5423 |         -87.7378 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |           0.0391 |         241.5804 |         -75.4121 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |          -0.0040 |         239.1032 |         -83.0558 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |          -0.0218 |         237.6420 |         -89.8786 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |          -0.0300 |         236.0863 |         -91.9530 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |          -0.0384 |         235.3790 |         -95.4333 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |          -0.0435 |         236.2033 |         -96.0348 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |          -0.0456 |         234.2346 |         -96.4299 |
[32m[20221208 14:06:52 @agent_ppo2.py:179][0m |          -0.0484 |         233.5296 |         -97.5539 |
[32m[20221208 14:06:53 @agent_ppo2.py:179][0m |          -0.0482 |         235.1017 |        -100.9366 |
[32m[20221208 14:06:53 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:06:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 902.78
[32m[20221208 14:06:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.07
[32m[20221208 14:06:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 922.70
[32m[20221208 14:06:53 @agent_ppo2.py:137][0m Total time:      21.54 min
[32m[20221208 14:06:53 @agent_ppo2.py:139][0m 1763328 total steps have happened
[32m[20221208 14:06:53 @agent_ppo2.py:115][0m #------------------------ Iteration 861 --------------------------#
[32m[20221208 14:06:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:06:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:53 @agent_ppo2.py:179][0m |           0.0457 |         251.1376 |         -76.7270 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |           0.0345 |         246.6154 |         -70.7026 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |           0.0017 |         244.1320 |         -78.4028 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |          -0.0114 |         242.9186 |         -84.2379 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |          -0.0181 |         242.1772 |         -86.0426 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |          -0.0302 |         241.5708 |         -90.1301 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |          -0.0315 |         244.2315 |         -90.5001 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |          -0.0316 |         242.3470 |         -90.9911 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |          -0.0293 |         241.6017 |         -92.9245 |
[32m[20221208 14:06:54 @agent_ppo2.py:179][0m |          -0.0401 |         240.1631 |         -98.2036 |
[32m[20221208 14:06:54 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.24
[32m[20221208 14:06:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.50
[32m[20221208 14:06:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.75
[32m[20221208 14:06:54 @agent_ppo2.py:137][0m Total time:      21.57 min
[32m[20221208 14:06:54 @agent_ppo2.py:139][0m 1765376 total steps have happened
[32m[20221208 14:06:54 @agent_ppo2.py:115][0m #------------------------ Iteration 862 --------------------------#
[32m[20221208 14:06:55 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |           0.0543 |         238.6744 |         -84.9381 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |           0.0335 |         233.5454 |         -77.2080 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |           0.0003 |         232.4127 |         -93.9259 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |          -0.0011 |         229.1669 |         -96.8236 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |          -0.0174 |         227.2137 |         -99.1655 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |          -0.0275 |         225.7589 |        -104.0551 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |          -0.0348 |         225.0942 |        -107.0937 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |          -0.0404 |         224.6352 |        -110.6421 |
[32m[20221208 14:06:55 @agent_ppo2.py:179][0m |          -0.0368 |         225.0913 |        -110.3572 |
[32m[20221208 14:06:56 @agent_ppo2.py:179][0m |          -0.0391 |         223.0015 |        -114.0604 |
[32m[20221208 14:06:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:06:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 912.30
[32m[20221208 14:06:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.40
[32m[20221208 14:06:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.91
[32m[20221208 14:06:56 @agent_ppo2.py:137][0m Total time:      21.59 min
[32m[20221208 14:06:56 @agent_ppo2.py:139][0m 1767424 total steps have happened
[32m[20221208 14:06:56 @agent_ppo2.py:115][0m #------------------------ Iteration 863 --------------------------#
[32m[20221208 14:06:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:56 @agent_ppo2.py:179][0m |           0.0565 |         246.9665 |         -77.7493 |
[32m[20221208 14:06:56 @agent_ppo2.py:179][0m |           0.0544 |         242.5736 |         -68.9873 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |           0.0164 |         238.6448 |         -77.3193 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |          -0.0045 |         237.8549 |         -82.8357 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |          -0.0198 |         237.1736 |         -83.8552 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |          -0.0275 |         239.0680 |         -88.5573 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |          -0.0286 |         236.3178 |         -87.7296 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |          -0.0352 |         236.9771 |         -89.4787 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |          -0.0398 |         236.5603 |         -91.6760 |
[32m[20221208 14:06:57 @agent_ppo2.py:179][0m |          -0.0408 |         235.6902 |         -93.2757 |
[32m[20221208 14:06:57 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:06:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 898.43
[32m[20221208 14:06:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 912.99
[32m[20221208 14:06:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.49
[32m[20221208 14:06:57 @agent_ppo2.py:137][0m Total time:      21.62 min
[32m[20221208 14:06:57 @agent_ppo2.py:139][0m 1769472 total steps have happened
[32m[20221208 14:06:57 @agent_ppo2.py:115][0m #------------------------ Iteration 864 --------------------------#
[32m[20221208 14:06:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.1223 |         238.2282 |         -56.0701 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.1298 |         236.2209 |         -26.8081 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.0912 |         233.1906 |         -23.0559 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.0649 |         232.0312 |         -37.1921 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.0670 |         232.1680 |         -44.7083 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.0302 |         230.4330 |         -52.7281 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.0088 |         228.8765 |         -62.0267 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |           0.0001 |         228.8302 |         -65.9120 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |          -0.0109 |         227.3812 |         -71.0419 |
[32m[20221208 14:06:58 @agent_ppo2.py:179][0m |          -0.0170 |         227.8943 |         -75.2470 |
[32m[20221208 14:06:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:06:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.42
[32m[20221208 14:06:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.03
[32m[20221208 14:06:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 910.33
[32m[20221208 14:06:59 @agent_ppo2.py:137][0m Total time:      21.64 min
[32m[20221208 14:06:59 @agent_ppo2.py:139][0m 1771520 total steps have happened
[32m[20221208 14:06:59 @agent_ppo2.py:115][0m #------------------------ Iteration 865 --------------------------#
[32m[20221208 14:06:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:06:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:06:59 @agent_ppo2.py:179][0m |           0.0653 |         238.1526 |         -66.6217 |
[32m[20221208 14:06:59 @agent_ppo2.py:179][0m |           0.0475 |         233.9545 |         -46.2347 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |           0.0202 |         231.1853 |         -60.3231 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |           0.0013 |         228.3573 |         -75.4430 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |          -0.0171 |         227.0495 |         -77.1595 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |          -0.0229 |         225.5195 |         -80.0079 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |          -0.0290 |         223.9995 |         -81.2237 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |          -0.0314 |         223.7364 |         -84.4274 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |          -0.0345 |         222.1237 |         -84.4315 |
[32m[20221208 14:07:00 @agent_ppo2.py:179][0m |          -0.0394 |         221.2482 |         -87.6681 |
[32m[20221208 14:07:00 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 956.39
[32m[20221208 14:07:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.61
[32m[20221208 14:07:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 906.79
[32m[20221208 14:07:00 @agent_ppo2.py:137][0m Total time:      21.67 min
[32m[20221208 14:07:00 @agent_ppo2.py:139][0m 1773568 total steps have happened
[32m[20221208 14:07:00 @agent_ppo2.py:115][0m #------------------------ Iteration 866 --------------------------#
[32m[20221208 14:07:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |           0.0608 |         242.1904 |         -63.4956 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |           0.0898 |         236.3187 |         -32.8625 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |           0.0545 |         233.9572 |         -38.0982 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |           0.0372 |         231.9585 |         -46.6333 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |           0.0059 |         229.8858 |         -59.9272 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |          -0.0071 |         230.8678 |         -66.5694 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |          -0.0134 |         228.0393 |         -68.8629 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |          -0.0276 |         227.4094 |         -72.0782 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |          -0.0315 |         226.4498 |         -75.1891 |
[32m[20221208 14:07:01 @agent_ppo2.py:179][0m |          -0.0364 |         226.6725 |         -77.6632 |
[32m[20221208 14:07:01 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.78
[32m[20221208 14:07:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.95
[32m[20221208 14:07:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.84
[32m[20221208 14:07:02 @agent_ppo2.py:137][0m Total time:      21.69 min
[32m[20221208 14:07:02 @agent_ppo2.py:139][0m 1775616 total steps have happened
[32m[20221208 14:07:02 @agent_ppo2.py:115][0m #------------------------ Iteration 867 --------------------------#
[32m[20221208 14:07:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:02 @agent_ppo2.py:179][0m |           0.0409 |         237.9280 |         -68.2257 |
[32m[20221208 14:07:02 @agent_ppo2.py:179][0m |           0.0954 |         230.7360 |         -44.8173 |
[32m[20221208 14:07:02 @agent_ppo2.py:179][0m |           0.0400 |         228.9457 |         -47.3290 |
[32m[20221208 14:07:03 @agent_ppo2.py:179][0m |           0.0195 |         225.7927 |         -60.9769 |
[32m[20221208 14:07:03 @agent_ppo2.py:179][0m |          -0.0048 |         222.9641 |         -61.8221 |
[32m[20221208 14:07:03 @agent_ppo2.py:179][0m |          -0.0241 |         221.3841 |         -69.3159 |
[32m[20221208 14:07:03 @agent_ppo2.py:179][0m |          -0.0294 |         220.6808 |         -70.4769 |
[32m[20221208 14:07:03 @agent_ppo2.py:179][0m |          -0.0336 |         218.9822 |         -72.5158 |
[32m[20221208 14:07:03 @agent_ppo2.py:179][0m |          -0.0385 |         218.6669 |         -74.4094 |
[32m[20221208 14:07:03 @agent_ppo2.py:179][0m |          -0.0414 |         218.4978 |         -76.5198 |
[32m[20221208 14:07:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 879.41
[32m[20221208 14:07:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.02
[32m[20221208 14:07:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.84
[32m[20221208 14:07:03 @agent_ppo2.py:137][0m Total time:      21.72 min
[32m[20221208 14:07:03 @agent_ppo2.py:139][0m 1777664 total steps have happened
[32m[20221208 14:07:03 @agent_ppo2.py:115][0m #------------------------ Iteration 868 --------------------------#
[32m[20221208 14:07:04 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |           0.0491 |         236.8608 |         -66.8746 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |           0.0342 |         233.4092 |         -60.5000 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0062 |         230.5267 |         -64.4609 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0215 |         228.8196 |         -69.9508 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0241 |         227.9500 |         -70.5974 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0305 |         227.9290 |         -72.0512 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0373 |         226.8072 |         -72.0851 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0461 |         227.1983 |         -75.5679 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0442 |         226.5785 |         -75.9940 |
[32m[20221208 14:07:04 @agent_ppo2.py:179][0m |          -0.0502 |         226.3005 |         -78.3406 |
[32m[20221208 14:07:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 881.27
[32m[20221208 14:07:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.91
[32m[20221208 14:07:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 373.79
[32m[20221208 14:07:05 @agent_ppo2.py:137][0m Total time:      21.74 min
[32m[20221208 14:07:05 @agent_ppo2.py:139][0m 1779712 total steps have happened
[32m[20221208 14:07:05 @agent_ppo2.py:115][0m #------------------------ Iteration 869 --------------------------#
[32m[20221208 14:07:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:05 @agent_ppo2.py:179][0m |           0.0751 |         235.6990 |         -66.9114 |
[32m[20221208 14:07:05 @agent_ppo2.py:179][0m |           0.1292 |         232.4354 |         -53.4367 |
[32m[20221208 14:07:05 @agent_ppo2.py:179][0m |           0.0033 |         229.6485 |         -65.9146 |
[32m[20221208 14:07:05 @agent_ppo2.py:179][0m |          -0.0092 |         228.4268 |         -69.8786 |
[32m[20221208 14:07:06 @agent_ppo2.py:179][0m |          -0.0229 |         227.5321 |         -72.6212 |
[32m[20221208 14:07:06 @agent_ppo2.py:179][0m |          -0.0272 |         226.5098 |         -73.5374 |
[32m[20221208 14:07:06 @agent_ppo2.py:179][0m |          -0.0348 |         226.1057 |         -75.1102 |
[32m[20221208 14:07:06 @agent_ppo2.py:179][0m |          -0.0301 |         225.7915 |         -77.2808 |
[32m[20221208 14:07:06 @agent_ppo2.py:179][0m |          -0.0342 |         224.3983 |         -78.9445 |
[32m[20221208 14:07:06 @agent_ppo2.py:179][0m |          -0.0283 |         223.7309 |         -76.2335 |
[32m[20221208 14:07:06 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.99
[32m[20221208 14:07:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.60
[32m[20221208 14:07:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 896.68
[32m[20221208 14:07:06 @agent_ppo2.py:137][0m Total time:      21.76 min
[32m[20221208 14:07:06 @agent_ppo2.py:139][0m 1781760 total steps have happened
[32m[20221208 14:07:06 @agent_ppo2.py:115][0m #------------------------ Iteration 870 --------------------------#
[32m[20221208 14:07:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |           0.0421 |         236.0392 |         -63.1972 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |           0.0468 |         232.3104 |         -52.5271 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |           0.0121 |         231.5155 |         -57.5647 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |          -0.0159 |         231.1307 |         -64.9349 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |          -0.0260 |         228.9857 |         -67.5255 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |          -0.0266 |         228.3921 |         -68.4407 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |          -0.0326 |         228.3408 |         -69.4774 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |          -0.0354 |         227.7798 |         -70.9137 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |          -0.0378 |         228.1128 |         -72.3594 |
[32m[20221208 14:07:07 @agent_ppo2.py:179][0m |          -0.0390 |         227.5878 |         -73.6140 |
[32m[20221208 14:07:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.76
[32m[20221208 14:07:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.49
[32m[20221208 14:07:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 959.78
[32m[20221208 14:07:08 @agent_ppo2.py:137][0m Total time:      21.79 min
[32m[20221208 14:07:08 @agent_ppo2.py:139][0m 1783808 total steps have happened
[32m[20221208 14:07:08 @agent_ppo2.py:115][0m #------------------------ Iteration 871 --------------------------#
[32m[20221208 14:07:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:08 @agent_ppo2.py:179][0m |           0.0913 |         247.6292 |         -62.5176 |
[32m[20221208 14:07:08 @agent_ppo2.py:179][0m |           0.0307 |         236.4705 |         -53.4665 |
[32m[20221208 14:07:08 @agent_ppo2.py:179][0m |          -0.0011 |         229.3077 |         -60.0982 |
[32m[20221208 14:07:08 @agent_ppo2.py:179][0m |          -0.0126 |         228.1758 |         -63.2115 |
[32m[20221208 14:07:09 @agent_ppo2.py:179][0m |          -0.0262 |         225.3034 |         -65.9019 |
[32m[20221208 14:07:09 @agent_ppo2.py:179][0m |          -0.0282 |         223.4533 |         -67.5160 |
[32m[20221208 14:07:09 @agent_ppo2.py:179][0m |          -0.0301 |         223.3385 |         -68.2741 |
[32m[20221208 14:07:09 @agent_ppo2.py:179][0m |          -0.0357 |         221.5097 |         -69.2761 |
[32m[20221208 14:07:09 @agent_ppo2.py:179][0m |          -0.0382 |         221.6866 |         -72.0610 |
[32m[20221208 14:07:09 @agent_ppo2.py:179][0m |          -0.0378 |         219.8400 |         -70.5934 |
[32m[20221208 14:07:09 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:07:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.48
[32m[20221208 14:07:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.52
[32m[20221208 14:07:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 894.38
[32m[20221208 14:07:09 @agent_ppo2.py:137][0m Total time:      21.81 min
[32m[20221208 14:07:09 @agent_ppo2.py:139][0m 1785856 total steps have happened
[32m[20221208 14:07:09 @agent_ppo2.py:115][0m #------------------------ Iteration 872 --------------------------#
[32m[20221208 14:07:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |           0.0702 |         241.0723 |         -60.3026 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |           0.0556 |         230.8538 |         -48.6035 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |           0.0402 |         225.6172 |         -46.5206 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |           0.0004 |         222.8682 |         -55.8660 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |          -0.0154 |         221.1781 |         -62.9659 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |          -0.0255 |         219.1753 |         -65.5183 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |          -0.0274 |         217.0434 |         -67.3830 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |          -0.0355 |         214.9187 |         -69.5713 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |          -0.0353 |         212.9539 |         -71.5740 |
[32m[20221208 14:07:10 @agent_ppo2.py:179][0m |          -0.0405 |         211.3129 |         -73.6840 |
[32m[20221208 14:07:10 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:07:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.71
[32m[20221208 14:07:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.22
[32m[20221208 14:07:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 901.28
[32m[20221208 14:07:11 @agent_ppo2.py:137][0m Total time:      21.84 min
[32m[20221208 14:07:11 @agent_ppo2.py:139][0m 1787904 total steps have happened
[32m[20221208 14:07:11 @agent_ppo2.py:115][0m #------------------------ Iteration 873 --------------------------#
[32m[20221208 14:07:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:11 @agent_ppo2.py:179][0m |           0.0496 |         238.2481 |         -62.5364 |
[32m[20221208 14:07:11 @agent_ppo2.py:179][0m |           0.0251 |         228.3036 |         -56.0459 |
[32m[20221208 14:07:11 @agent_ppo2.py:179][0m |          -0.0049 |         223.6113 |         -60.1968 |
[32m[20221208 14:07:11 @agent_ppo2.py:179][0m |          -0.0222 |         221.7850 |         -64.9842 |
[32m[20221208 14:07:12 @agent_ppo2.py:179][0m |          -0.0276 |         221.7921 |         -68.1644 |
[32m[20221208 14:07:12 @agent_ppo2.py:179][0m |          -0.0212 |         221.4291 |         -66.3025 |
[32m[20221208 14:07:12 @agent_ppo2.py:179][0m |          -0.0329 |         219.0224 |         -71.4381 |
[32m[20221208 14:07:12 @agent_ppo2.py:179][0m |          -0.0344 |         216.4995 |         -70.8829 |
[32m[20221208 14:07:12 @agent_ppo2.py:179][0m |          -0.0427 |         216.1904 |         -74.1126 |
[32m[20221208 14:07:12 @agent_ppo2.py:179][0m |          -0.0443 |         214.4801 |         -76.4480 |
[32m[20221208 14:07:12 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 932.67
[32m[20221208 14:07:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.65
[32m[20221208 14:07:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 899.06
[32m[20221208 14:07:12 @agent_ppo2.py:137][0m Total time:      21.86 min
[32m[20221208 14:07:12 @agent_ppo2.py:139][0m 1789952 total steps have happened
[32m[20221208 14:07:12 @agent_ppo2.py:115][0m #------------------------ Iteration 874 --------------------------#
[32m[20221208 14:07:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |           0.0748 |         247.7788 |         -58.8220 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |           0.0559 |         239.7291 |         -46.6327 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |           0.0190 |         234.6514 |         -59.7923 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |           0.0124 |         232.6447 |         -67.1342 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |          -0.0063 |         230.3276 |         -71.0983 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |          -0.0189 |         227.4664 |         -77.7408 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |          -0.0261 |         226.6305 |         -78.8582 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |          -0.0221 |         227.9313 |         -77.8132 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |          -0.0334 |         226.1841 |         -81.1919 |
[32m[20221208 14:07:13 @agent_ppo2.py:179][0m |          -0.0330 |         225.3192 |         -83.3626 |
[32m[20221208 14:07:13 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:07:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 951.86
[32m[20221208 14:07:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.78
[32m[20221208 14:07:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 915.34
[32m[20221208 14:07:14 @agent_ppo2.py:137][0m Total time:      21.89 min
[32m[20221208 14:07:14 @agent_ppo2.py:139][0m 1792000 total steps have happened
[32m[20221208 14:07:14 @agent_ppo2.py:115][0m #------------------------ Iteration 875 --------------------------#
[32m[20221208 14:07:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:14 @agent_ppo2.py:179][0m |           0.0641 |         242.2201 |         -68.1537 |
[32m[20221208 14:07:14 @agent_ppo2.py:179][0m |           0.0356 |         236.1015 |         -61.4104 |
[32m[20221208 14:07:14 @agent_ppo2.py:179][0m |          -0.0005 |         234.5037 |         -74.0056 |
[32m[20221208 14:07:14 @agent_ppo2.py:179][0m |          -0.0179 |         230.7787 |         -76.8089 |
[32m[20221208 14:07:15 @agent_ppo2.py:179][0m |          -0.0254 |         229.8334 |         -79.1273 |
[32m[20221208 14:07:15 @agent_ppo2.py:179][0m |          -0.0351 |         227.2842 |         -80.5280 |
[32m[20221208 14:07:15 @agent_ppo2.py:179][0m |          -0.0406 |         227.0234 |         -82.5039 |
[32m[20221208 14:07:15 @agent_ppo2.py:179][0m |          -0.0421 |         224.9703 |         -84.5145 |
[32m[20221208 14:07:15 @agent_ppo2.py:179][0m |          -0.0468 |         223.6726 |         -88.5999 |
[32m[20221208 14:07:15 @agent_ppo2.py:179][0m |          -0.0490 |         222.4533 |         -88.6131 |
[32m[20221208 14:07:15 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.35
[32m[20221208 14:07:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 968.03
[32m[20221208 14:07:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.23
[32m[20221208 14:07:15 @agent_ppo2.py:137][0m Total time:      21.91 min
[32m[20221208 14:07:15 @agent_ppo2.py:139][0m 1794048 total steps have happened
[32m[20221208 14:07:15 @agent_ppo2.py:115][0m #------------------------ Iteration 876 --------------------------#
[32m[20221208 14:07:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |           0.1125 |         249.6612 |         -57.5532 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |           0.1164 |         243.0020 |         -22.1466 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |           0.0422 |         236.6995 |         -50.2454 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |           0.0155 |         234.1882 |         -63.9228 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |          -0.0027 |         233.2709 |         -69.3157 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |          -0.0189 |         230.9544 |         -74.7369 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |          -0.0264 |         229.3726 |         -77.2980 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |          -0.0328 |         228.6881 |         -81.1505 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |          -0.0376 |         228.0398 |         -82.1482 |
[32m[20221208 14:07:16 @agent_ppo2.py:179][0m |          -0.0380 |         227.1243 |         -84.6689 |
[32m[20221208 14:07:16 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:07:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.82
[32m[20221208 14:07:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 978.71
[32m[20221208 14:07:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.92
[32m[20221208 14:07:17 @agent_ppo2.py:137][0m Total time:      21.94 min
[32m[20221208 14:07:17 @agent_ppo2.py:139][0m 1796096 total steps have happened
[32m[20221208 14:07:17 @agent_ppo2.py:115][0m #------------------------ Iteration 877 --------------------------#
[32m[20221208 14:07:17 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:17 @agent_ppo2.py:179][0m |           0.0589 |         243.0036 |         -67.4209 |
[32m[20221208 14:07:17 @agent_ppo2.py:179][0m |           0.0383 |         235.3288 |         -56.9454 |
[32m[20221208 14:07:17 @agent_ppo2.py:179][0m |           0.0121 |         230.1352 |         -66.9687 |
[32m[20221208 14:07:17 @agent_ppo2.py:179][0m |          -0.0043 |         228.4594 |         -71.8575 |
[32m[20221208 14:07:17 @agent_ppo2.py:179][0m |          -0.0278 |         225.6716 |         -74.4980 |
[32m[20221208 14:07:18 @agent_ppo2.py:179][0m |          -0.0300 |         223.4874 |         -76.2436 |
[32m[20221208 14:07:18 @agent_ppo2.py:179][0m |          -0.0394 |         222.2229 |         -79.6740 |
[32m[20221208 14:07:18 @agent_ppo2.py:179][0m |          -0.0474 |         220.0369 |         -82.1682 |
[32m[20221208 14:07:18 @agent_ppo2.py:179][0m |          -0.0495 |         219.7722 |         -83.9933 |
[32m[20221208 14:07:18 @agent_ppo2.py:179][0m |          -0.0498 |         217.6323 |         -85.3099 |
[32m[20221208 14:07:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 883.67
[32m[20221208 14:07:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 920.99
[32m[20221208 14:07:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 913.46
[32m[20221208 14:07:18 @agent_ppo2.py:137][0m Total time:      21.96 min
[32m[20221208 14:07:18 @agent_ppo2.py:139][0m 1798144 total steps have happened
[32m[20221208 14:07:18 @agent_ppo2.py:115][0m #------------------------ Iteration 878 --------------------------#
[32m[20221208 14:07:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |           0.0494 |         248.9768 |         -66.1682 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |           0.1380 |         240.8809 |         -32.6190 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |           0.0710 |         238.5455 |         -34.5627 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |           0.0294 |         238.7965 |         -54.6328 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |           0.0030 |         235.3429 |         -67.3190 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |          -0.0141 |         235.1656 |         -73.0269 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |          -0.0295 |         234.8730 |         -76.7194 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |          -0.0325 |         232.9456 |         -79.9024 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |          -0.0382 |         233.1906 |         -81.5940 |
[32m[20221208 14:07:19 @agent_ppo2.py:179][0m |          -0.0437 |         233.1753 |         -83.1492 |
[32m[20221208 14:07:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.87
[32m[20221208 14:07:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.53
[32m[20221208 14:07:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.21
[32m[20221208 14:07:20 @agent_ppo2.py:137][0m Total time:      21.99 min
[32m[20221208 14:07:20 @agent_ppo2.py:139][0m 1800192 total steps have happened
[32m[20221208 14:07:20 @agent_ppo2.py:115][0m #------------------------ Iteration 879 --------------------------#
[32m[20221208 14:07:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:20 @agent_ppo2.py:179][0m |           0.0896 |         250.4822 |         -61.7046 |
[32m[20221208 14:07:20 @agent_ppo2.py:179][0m |           0.1132 |         238.6736 |         -33.1047 |
[32m[20221208 14:07:20 @agent_ppo2.py:179][0m |           0.0451 |         232.5086 |         -51.6476 |
[32m[20221208 14:07:20 @agent_ppo2.py:179][0m |           0.0122 |         228.6508 |         -67.1850 |
[32m[20221208 14:07:20 @agent_ppo2.py:179][0m |          -0.0079 |         224.9861 |         -74.4757 |
[32m[20221208 14:07:20 @agent_ppo2.py:179][0m |          -0.0182 |         223.2546 |         -78.4803 |
[32m[20221208 14:07:21 @agent_ppo2.py:179][0m |          -0.0270 |         220.6841 |         -80.9370 |
[32m[20221208 14:07:21 @agent_ppo2.py:179][0m |          -0.0338 |         219.4037 |         -83.6003 |
[32m[20221208 14:07:21 @agent_ppo2.py:179][0m |          -0.0384 |         218.6627 |         -86.1577 |
[32m[20221208 14:07:21 @agent_ppo2.py:179][0m |          -0.0417 |         217.4426 |         -87.9229 |
[32m[20221208 14:07:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.22
[32m[20221208 14:07:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.83
[32m[20221208 14:07:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 947.74
[32m[20221208 14:07:21 @agent_ppo2.py:137][0m Total time:      22.01 min
[32m[20221208 14:07:21 @agent_ppo2.py:139][0m 1802240 total steps have happened
[32m[20221208 14:07:21 @agent_ppo2.py:115][0m #------------------------ Iteration 880 --------------------------#
[32m[20221208 14:07:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |           0.0600 |         244.6018 |         -73.1633 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |           0.1313 |         239.5600 |         -50.6460 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |           0.0707 |         237.0878 |         -44.6829 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |           0.0315 |         233.3455 |         -56.0277 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |           0.0012 |         232.4869 |         -62.9045 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |          -0.0171 |         231.2335 |         -71.4036 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |          -0.0278 |         230.5109 |         -76.8261 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |          -0.0338 |         229.9994 |         -79.3700 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |          -0.0379 |         229.4920 |         -81.3534 |
[32m[20221208 14:07:22 @agent_ppo2.py:179][0m |          -0.0401 |         230.2255 |         -83.7970 |
[32m[20221208 14:07:22 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 896.25
[32m[20221208 14:07:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.03
[32m[20221208 14:07:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.90
[32m[20221208 14:07:23 @agent_ppo2.py:137][0m Total time:      22.04 min
[32m[20221208 14:07:23 @agent_ppo2.py:139][0m 1804288 total steps have happened
[32m[20221208 14:07:23 @agent_ppo2.py:115][0m #------------------------ Iteration 881 --------------------------#
[32m[20221208 14:07:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:07:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:23 @agent_ppo2.py:179][0m |           0.1134 |         210.0720 |         -46.1185 |
[32m[20221208 14:07:23 @agent_ppo2.py:179][0m |           0.0125 |         202.3340 |         -45.7335 |
[32m[20221208 14:07:23 @agent_ppo2.py:179][0m |          -0.0208 |         198.0381 |         -50.9847 |
[32m[20221208 14:07:23 @agent_ppo2.py:179][0m |          -0.0334 |         197.6527 |         -52.0893 |
[32m[20221208 14:07:23 @agent_ppo2.py:179][0m |          -0.0496 |         195.2918 |         -56.1801 |
[32m[20221208 14:07:23 @agent_ppo2.py:179][0m |          -0.0594 |         193.6455 |         -57.8979 |
[32m[20221208 14:07:24 @agent_ppo2.py:179][0m |          -0.0651 |         193.3703 |         -59.8619 |
[32m[20221208 14:07:24 @agent_ppo2.py:179][0m |          -0.0680 |         192.3301 |         -62.6740 |
[32m[20221208 14:07:24 @agent_ppo2.py:179][0m |          -0.0699 |         192.4975 |         -65.2456 |
[32m[20221208 14:07:24 @agent_ppo2.py:179][0m |          -0.0720 |         191.3747 |         -65.4645 |
[32m[20221208 14:07:24 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 661.69
[32m[20221208 14:07:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.09
[32m[20221208 14:07:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.99
[32m[20221208 14:07:24 @agent_ppo2.py:137][0m Total time:      22.06 min
[32m[20221208 14:07:24 @agent_ppo2.py:139][0m 1806336 total steps have happened
[32m[20221208 14:07:24 @agent_ppo2.py:115][0m #------------------------ Iteration 882 --------------------------#
[32m[20221208 14:07:25 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |           0.0484 |         252.9494 |         -79.8371 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |           0.0664 |         242.9756 |         -60.3651 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |           0.0516 |         240.5295 |         -59.5050 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |           0.0051 |         238.1814 |         -76.4400 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |          -0.0141 |         234.9704 |         -82.1743 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |          -0.0265 |         233.8101 |         -84.2875 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |          -0.0316 |         233.2564 |         -86.5747 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |          -0.0361 |         232.4966 |         -87.8407 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |          -0.0404 |         231.9012 |         -89.5064 |
[32m[20221208 14:07:25 @agent_ppo2.py:179][0m |          -0.0353 |         231.2026 |         -88.0338 |
[32m[20221208 14:07:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.32
[32m[20221208 14:07:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.62
[32m[20221208 14:07:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 621.85
[32m[20221208 14:07:26 @agent_ppo2.py:137][0m Total time:      22.09 min
[32m[20221208 14:07:26 @agent_ppo2.py:139][0m 1808384 total steps have happened
[32m[20221208 14:07:26 @agent_ppo2.py:115][0m #------------------------ Iteration 883 --------------------------#
[32m[20221208 14:07:26 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:26 @agent_ppo2.py:179][0m |           0.0626 |         227.8385 |         -71.8563 |
[32m[20221208 14:07:26 @agent_ppo2.py:179][0m |           0.0763 |         208.0544 |         -57.7878 |
[32m[20221208 14:07:26 @agent_ppo2.py:179][0m |           0.0468 |         202.9815 |         -61.2162 |
[32m[20221208 14:07:26 @agent_ppo2.py:179][0m |           0.0035 |         199.3673 |         -73.3257 |
[32m[20221208 14:07:26 @agent_ppo2.py:179][0m |           0.0006 |         197.6494 |         -69.0704 |
[32m[20221208 14:07:26 @agent_ppo2.py:179][0m |          -0.0202 |         196.5768 |         -79.0259 |
[32m[20221208 14:07:26 @agent_ppo2.py:179][0m |          -0.0299 |         195.1705 |         -83.2919 |
[32m[20221208 14:07:27 @agent_ppo2.py:179][0m |          -0.0336 |         194.9554 |         -86.4168 |
[32m[20221208 14:07:27 @agent_ppo2.py:179][0m |          -0.0372 |         193.1439 |         -88.1167 |
[32m[20221208 14:07:27 @agent_ppo2.py:179][0m |          -0.0377 |         192.6226 |         -91.0638 |
[32m[20221208 14:07:27 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 968.70
[32m[20221208 14:07:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.55
[32m[20221208 14:07:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.14
[32m[20221208 14:07:27 @agent_ppo2.py:137][0m Total time:      22.11 min
[32m[20221208 14:07:27 @agent_ppo2.py:139][0m 1810432 total steps have happened
[32m[20221208 14:07:27 @agent_ppo2.py:115][0m #------------------------ Iteration 884 --------------------------#
[32m[20221208 14:07:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |           0.0635 |         243.7795 |         -81.3486 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |           0.0226 |         224.7213 |         -77.3939 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0097 |         214.5092 |         -82.5433 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0250 |         208.3219 |         -85.2203 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0264 |         203.3764 |         -86.4112 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0378 |         200.2010 |         -89.0909 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0410 |         195.6242 |         -89.2910 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0476 |         192.4815 |         -89.4212 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0504 |         188.9487 |         -90.0370 |
[32m[20221208 14:07:28 @agent_ppo2.py:179][0m |          -0.0548 |         186.9915 |         -92.3592 |
[32m[20221208 14:07:28 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 793.94
[32m[20221208 14:07:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.34
[32m[20221208 14:07:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.07
[32m[20221208 14:07:29 @agent_ppo2.py:137][0m Total time:      22.14 min
[32m[20221208 14:07:29 @agent_ppo2.py:139][0m 1812480 total steps have happened
[32m[20221208 14:07:29 @agent_ppo2.py:115][0m #------------------------ Iteration 885 --------------------------#
[32m[20221208 14:07:29 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |           0.0582 |         246.4915 |         -88.1005 |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |           0.0121 |         224.4002 |         -66.8625 |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |          -0.0194 |         214.7681 |         -71.4516 |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |          -0.0405 |         209.6117 |         -75.9729 |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |          -0.0551 |         205.9433 |         -80.2718 |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |          -0.0623 |         203.4494 |         -83.2554 |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |          -0.0657 |         201.2988 |         -85.7062 |
[32m[20221208 14:07:29 @agent_ppo2.py:179][0m |          -0.0680 |         197.8492 |         -85.9774 |
[32m[20221208 14:07:30 @agent_ppo2.py:179][0m |          -0.0682 |         196.7989 |         -87.9130 |
[32m[20221208 14:07:30 @agent_ppo2.py:179][0m |          -0.0707 |         194.1417 |         -89.5778 |
[32m[20221208 14:07:30 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:07:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 693.37
[32m[20221208 14:07:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 920.12
[32m[20221208 14:07:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 916.35
[32m[20221208 14:07:30 @agent_ppo2.py:137][0m Total time:      22.16 min
[32m[20221208 14:07:30 @agent_ppo2.py:139][0m 1814528 total steps have happened
[32m[20221208 14:07:30 @agent_ppo2.py:115][0m #------------------------ Iteration 886 --------------------------#
[32m[20221208 14:07:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |           0.0641 |         250.4052 |         -86.3023 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |           0.0099 |         218.1539 |         -87.9091 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0169 |         206.5301 |         -92.7858 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0276 |         201.4275 |         -92.1555 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0407 |         195.7791 |         -96.5741 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0509 |         191.7312 |         -96.5985 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0574 |         188.4829 |         -98.4101 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0618 |         186.2286 |        -100.4996 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0672 |         183.4007 |        -102.0503 |
[32m[20221208 14:07:31 @agent_ppo2.py:179][0m |          -0.0705 |         180.8317 |        -103.5499 |
[32m[20221208 14:07:31 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 749.61
[32m[20221208 14:07:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.91
[32m[20221208 14:07:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 897.63
[32m[20221208 14:07:31 @agent_ppo2.py:137][0m Total time:      22.19 min
[32m[20221208 14:07:31 @agent_ppo2.py:139][0m 1816576 total steps have happened
[32m[20221208 14:07:31 @agent_ppo2.py:115][0m #------------------------ Iteration 887 --------------------------#
[32m[20221208 14:07:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |           0.0385 |         244.6198 |         -92.1688 |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |           0.0160 |         234.6236 |         -83.6778 |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |          -0.0172 |         229.6315 |         -94.6690 |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |          -0.0189 |         229.1202 |         -97.2259 |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |          -0.0253 |         226.1539 |         -97.7410 |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |          -0.0328 |         225.7102 |         -98.9325 |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |          -0.0331 |         223.8307 |         -99.1018 |
[32m[20221208 14:07:32 @agent_ppo2.py:179][0m |          -0.0423 |         223.0735 |        -102.5186 |
[32m[20221208 14:07:33 @agent_ppo2.py:179][0m |          -0.0464 |         223.3361 |        -105.0103 |
[32m[20221208 14:07:33 @agent_ppo2.py:179][0m |          -0.0507 |         221.3116 |        -107.1692 |
[32m[20221208 14:07:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 912.72
[32m[20221208 14:07:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 953.27
[32m[20221208 14:07:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 912.45
[32m[20221208 14:07:33 @agent_ppo2.py:137][0m Total time:      22.21 min
[32m[20221208 14:07:33 @agent_ppo2.py:139][0m 1818624 total steps have happened
[32m[20221208 14:07:33 @agent_ppo2.py:115][0m #------------------------ Iteration 888 --------------------------#
[32m[20221208 14:07:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:33 @agent_ppo2.py:179][0m |           0.0681 |         229.5405 |         -91.7839 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |           0.0637 |         207.0374 |         -65.6552 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |           0.0187 |         199.5106 |         -82.6367 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |          -0.0080 |         195.0811 |         -93.2749 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |          -0.0248 |         191.8206 |        -100.7641 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |          -0.0333 |         187.4465 |        -103.9387 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |          -0.0360 |         183.2712 |        -105.0935 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |          -0.0388 |         181.0884 |        -107.6781 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |          -0.0405 |         177.4028 |        -108.1028 |
[32m[20221208 14:07:34 @agent_ppo2.py:179][0m |          -0.0360 |         175.3893 |        -107.9870 |
[32m[20221208 14:07:34 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.97
[32m[20221208 14:07:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.21
[32m[20221208 14:07:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.56
[32m[20221208 14:07:34 @agent_ppo2.py:137][0m Total time:      22.23 min
[32m[20221208 14:07:34 @agent_ppo2.py:139][0m 1820672 total steps have happened
[32m[20221208 14:07:34 @agent_ppo2.py:115][0m #------------------------ Iteration 889 --------------------------#
[32m[20221208 14:07:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |           0.0743 |         250.0263 |         -83.7364 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |           0.1267 |         242.7642 |         -31.4474 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |           0.0737 |         238.4138 |         -42.3513 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |           0.0219 |         235.3448 |         -68.9985 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |          -0.0161 |         232.9584 |         -84.4255 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |          -0.0204 |         229.5363 |         -88.8983 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |          -0.0362 |         228.7247 |         -95.0407 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |          -0.0450 |         227.4362 |         -98.8008 |
[32m[20221208 14:07:35 @agent_ppo2.py:179][0m |          -0.0445 |         226.2644 |        -100.6161 |
[32m[20221208 14:07:36 @agent_ppo2.py:179][0m |          -0.0469 |         226.9541 |        -102.4432 |
[32m[20221208 14:07:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 857.23
[32m[20221208 14:07:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.32
[32m[20221208 14:07:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 681.88
[32m[20221208 14:07:36 @agent_ppo2.py:137][0m Total time:      22.26 min
[32m[20221208 14:07:36 @agent_ppo2.py:139][0m 1822720 total steps have happened
[32m[20221208 14:07:36 @agent_ppo2.py:115][0m #------------------------ Iteration 890 --------------------------#
[32m[20221208 14:07:36 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:07:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:36 @agent_ppo2.py:179][0m |           0.1117 |         267.9574 |         -94.5178 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |           0.0674 |         249.9407 |         -73.8134 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |           0.0319 |         244.6546 |         -96.1400 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |           0.0027 |         242.6702 |         -99.0146 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |          -0.0208 |         241.4410 |        -111.0911 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |          -0.0293 |         239.8653 |        -115.4922 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |          -0.0366 |         239.3792 |        -119.4186 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |          -0.0409 |         238.0084 |        -119.2115 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |          -0.0466 |         236.3277 |        -123.9827 |
[32m[20221208 14:07:37 @agent_ppo2.py:179][0m |          -0.0487 |         235.6363 |        -126.0518 |
[32m[20221208 14:07:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 898.09
[32m[20221208 14:07:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.55
[32m[20221208 14:07:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 872.93
[32m[20221208 14:07:37 @agent_ppo2.py:137][0m Total time:      22.28 min
[32m[20221208 14:07:37 @agent_ppo2.py:139][0m 1824768 total steps have happened
[32m[20221208 14:07:37 @agent_ppo2.py:115][0m #------------------------ Iteration 891 --------------------------#
[32m[20221208 14:07:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |           0.0417 |         240.8945 |        -103.2278 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |           0.0375 |         228.8993 |         -95.9189 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |           0.0077 |         223.8816 |         -98.6447 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |          -0.0193 |         219.8371 |        -113.5826 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |          -0.0274 |         217.8010 |        -120.7198 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |          -0.0351 |         214.7157 |        -123.2724 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |          -0.0372 |         214.0297 |        -124.2291 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |          -0.0392 |         211.0741 |        -128.1675 |
[32m[20221208 14:07:38 @agent_ppo2.py:179][0m |          -0.0418 |         208.6836 |        -127.9327 |
[32m[20221208 14:07:39 @agent_ppo2.py:179][0m |          -0.0457 |         207.8345 |        -131.0088 |
[32m[20221208 14:07:39 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:07:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.58
[32m[20221208 14:07:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.97
[32m[20221208 14:07:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 417.15
[32m[20221208 14:07:39 @agent_ppo2.py:137][0m Total time:      22.31 min
[32m[20221208 14:07:39 @agent_ppo2.py:139][0m 1826816 total steps have happened
[32m[20221208 14:07:39 @agent_ppo2.py:115][0m #------------------------ Iteration 892 --------------------------#
[32m[20221208 14:07:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:39 @agent_ppo2.py:179][0m |           0.0591 |         240.8895 |        -109.6176 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |           0.0494 |         231.7120 |         -92.6449 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |           0.0448 |         225.6346 |         -83.3762 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |           0.0001 |         222.9809 |        -104.4273 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |          -0.0207 |         220.3038 |        -108.0735 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |          -0.0380 |         217.9523 |        -114.0974 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |          -0.0402 |         216.5952 |        -116.4740 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |          -0.0491 |         215.6435 |        -118.1583 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |          -0.0552 |         214.3902 |        -122.6501 |
[32m[20221208 14:07:40 @agent_ppo2.py:179][0m |          -0.0589 |         214.4709 |        -124.3655 |
[32m[20221208 14:07:40 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 835.42
[32m[20221208 14:07:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 850.45
[32m[20221208 14:07:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.04
[32m[20221208 14:07:40 @agent_ppo2.py:137][0m Total time:      22.33 min
[32m[20221208 14:07:40 @agent_ppo2.py:139][0m 1828864 total steps have happened
[32m[20221208 14:07:40 @agent_ppo2.py:115][0m #------------------------ Iteration 893 --------------------------#
[32m[20221208 14:07:41 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |           0.0703 |         250.1535 |        -114.2284 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |           0.1091 |         240.1642 |         -72.1841 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |           0.0496 |         236.2375 |         -83.6690 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |           0.0098 |         234.2644 |        -101.0305 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |          -0.0056 |         232.4557 |        -105.4663 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |          -0.0248 |         229.5869 |        -114.0384 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |          -0.0336 |         228.7487 |        -119.8032 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |          -0.0372 |         227.6622 |        -122.0230 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |          -0.0411 |         226.7807 |        -123.1413 |
[32m[20221208 14:07:41 @agent_ppo2.py:179][0m |          -0.0461 |         225.8835 |        -127.5218 |
[32m[20221208 14:07:41 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.97
[32m[20221208 14:07:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 975.18
[32m[20221208 14:07:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 948.11
[32m[20221208 14:07:42 @agent_ppo2.py:137][0m Total time:      22.36 min
[32m[20221208 14:07:42 @agent_ppo2.py:139][0m 1830912 total steps have happened
[32m[20221208 14:07:42 @agent_ppo2.py:115][0m #------------------------ Iteration 894 --------------------------#
[32m[20221208 14:07:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:42 @agent_ppo2.py:179][0m |           0.1661 |         162.0915 |         -75.2892 |
[32m[20221208 14:07:42 @agent_ppo2.py:179][0m |           0.0416 |         146.6331 |         -33.2323 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0053 |         139.8794 |         -41.4361 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0262 |         134.9606 |         -47.0169 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0320 |         131.7523 |         -49.3753 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0426 |         130.4433 |         -51.7424 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0509 |         126.8767 |         -54.8788 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0539 |         124.5214 |         -57.1890 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0576 |         121.8303 |         -59.4478 |
[32m[20221208 14:07:43 @agent_ppo2.py:179][0m |          -0.0606 |         119.4847 |         -60.5857 |
[32m[20221208 14:07:43 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 363.01
[32m[20221208 14:07:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 805.95
[32m[20221208 14:07:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 890.39
[32m[20221208 14:07:43 @agent_ppo2.py:137][0m Total time:      22.38 min
[32m[20221208 14:07:43 @agent_ppo2.py:139][0m 1832960 total steps have happened
[32m[20221208 14:07:43 @agent_ppo2.py:115][0m #------------------------ Iteration 895 --------------------------#
[32m[20221208 14:07:44 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |           0.0652 |         248.7219 |        -131.7130 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |           0.1152 |         222.5473 |         -90.0919 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |           0.0477 |         213.2053 |         -87.7646 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |          -0.0054 |         205.9754 |        -113.6665 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |          -0.0216 |         201.6015 |        -122.4689 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |          -0.0328 |         199.7899 |        -127.5947 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |          -0.0416 |         198.2277 |        -133.2595 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |          -0.0476 |         194.6414 |        -134.9730 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |          -0.0519 |         192.3433 |        -140.4398 |
[32m[20221208 14:07:44 @agent_ppo2.py:179][0m |          -0.0578 |         190.0778 |        -142.8895 |
[32m[20221208 14:07:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 843.17
[32m[20221208 14:07:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 940.38
[32m[20221208 14:07:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.93
[32m[20221208 14:07:45 @agent_ppo2.py:137][0m Total time:      22.41 min
[32m[20221208 14:07:45 @agent_ppo2.py:139][0m 1835008 total steps have happened
[32m[20221208 14:07:45 @agent_ppo2.py:115][0m #------------------------ Iteration 896 --------------------------#
[32m[20221208 14:07:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:45 @agent_ppo2.py:179][0m |           0.0931 |         250.3811 |        -115.5996 |
[32m[20221208 14:07:45 @agent_ppo2.py:179][0m |           0.0558 |         238.0558 |         -92.9239 |
[32m[20221208 14:07:45 @agent_ppo2.py:179][0m |          -0.0033 |         234.1983 |        -115.8181 |
[32m[20221208 14:07:46 @agent_ppo2.py:179][0m |          -0.0235 |         232.9327 |        -126.0634 |
[32m[20221208 14:07:46 @agent_ppo2.py:179][0m |          -0.0320 |         231.8382 |        -128.4387 |
[32m[20221208 14:07:46 @agent_ppo2.py:179][0m |          -0.0408 |         229.0040 |        -129.8430 |
[32m[20221208 14:07:46 @agent_ppo2.py:179][0m |          -0.0417 |         228.8712 |        -132.7445 |
[32m[20221208 14:07:46 @agent_ppo2.py:179][0m |          -0.0466 |         226.4632 |        -133.5595 |
[32m[20221208 14:07:46 @agent_ppo2.py:179][0m |          -0.0505 |         226.2793 |        -137.7932 |
[32m[20221208 14:07:46 @agent_ppo2.py:179][0m |          -0.0538 |         224.7035 |        -139.1462 |
[32m[20221208 14:07:46 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 889.06
[32m[20221208 14:07:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.95
[32m[20221208 14:07:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 882.76
[32m[20221208 14:07:46 @agent_ppo2.py:137][0m Total time:      22.43 min
[32m[20221208 14:07:46 @agent_ppo2.py:139][0m 1837056 total steps have happened
[32m[20221208 14:07:46 @agent_ppo2.py:115][0m #------------------------ Iteration 897 --------------------------#
[32m[20221208 14:07:47 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |           0.0461 |         212.5377 |        -105.5492 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |           0.0056 |         203.6394 |         -96.7070 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0295 |         200.4268 |        -105.1479 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0455 |         199.0464 |        -112.2864 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0544 |         198.2399 |        -114.4311 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0614 |         196.8095 |        -116.5135 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0668 |         195.2687 |        -118.5402 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0620 |         194.5652 |        -118.8308 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0691 |         194.3731 |        -123.9139 |
[32m[20221208 14:07:47 @agent_ppo2.py:179][0m |          -0.0704 |         193.3405 |        -124.6850 |
[32m[20221208 14:07:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 698.12
[32m[20221208 14:07:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.64
[32m[20221208 14:07:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 907.46
[32m[20221208 14:07:48 @agent_ppo2.py:137][0m Total time:      22.46 min
[32m[20221208 14:07:48 @agent_ppo2.py:139][0m 1839104 total steps have happened
[32m[20221208 14:07:48 @agent_ppo2.py:115][0m #------------------------ Iteration 898 --------------------------#
[32m[20221208 14:07:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:48 @agent_ppo2.py:179][0m |           0.0509 |         226.0611 |        -128.4488 |
[32m[20221208 14:07:48 @agent_ppo2.py:179][0m |           0.0290 |         211.5117 |        -126.8256 |
[32m[20221208 14:07:48 @agent_ppo2.py:179][0m |           0.0207 |         204.7625 |        -129.7944 |
[32m[20221208 14:07:48 @agent_ppo2.py:179][0m |          -0.0101 |         201.2810 |        -141.2140 |
[32m[20221208 14:07:49 @agent_ppo2.py:179][0m |          -0.0182 |         200.1778 |        -151.5773 |
[32m[20221208 14:07:49 @agent_ppo2.py:179][0m |          -0.0291 |         197.3629 |        -156.9516 |
[32m[20221208 14:07:49 @agent_ppo2.py:179][0m |          -0.0273 |         196.3366 |        -156.0881 |
[32m[20221208 14:07:49 @agent_ppo2.py:179][0m |          -0.0360 |         196.0745 |        -161.3881 |
[32m[20221208 14:07:49 @agent_ppo2.py:179][0m |          -0.0362 |         194.3538 |        -162.6649 |
[32m[20221208 14:07:49 @agent_ppo2.py:179][0m |          -0.0414 |         192.7382 |        -168.7053 |
[32m[20221208 14:07:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.64
[32m[20221208 14:07:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.82
[32m[20221208 14:07:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 911.65
[32m[20221208 14:07:49 @agent_ppo2.py:137][0m Total time:      22.48 min
[32m[20221208 14:07:49 @agent_ppo2.py:139][0m 1841152 total steps have happened
[32m[20221208 14:07:49 @agent_ppo2.py:115][0m #------------------------ Iteration 899 --------------------------#
[32m[20221208 14:07:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |           0.0729 |         246.9904 |        -132.8595 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |           0.0718 |         237.4151 |         -90.5103 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |           0.0326 |         231.2684 |        -105.1020 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |           0.0165 |         227.6174 |        -118.5907 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |          -0.0030 |         227.1723 |        -132.0319 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |          -0.0186 |         225.0556 |        -142.4976 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |          -0.0282 |         222.7867 |        -151.7589 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |          -0.0303 |         220.5233 |        -156.6509 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |          -0.0354 |         218.4763 |        -160.4843 |
[32m[20221208 14:07:50 @agent_ppo2.py:179][0m |          -0.0414 |         218.5078 |        -164.7709 |
[32m[20221208 14:07:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.99
[32m[20221208 14:07:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.02
[32m[20221208 14:07:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 899.60
[32m[20221208 14:07:51 @agent_ppo2.py:137][0m Total time:      22.51 min
[32m[20221208 14:07:51 @agent_ppo2.py:139][0m 1843200 total steps have happened
[32m[20221208 14:07:51 @agent_ppo2.py:115][0m #------------------------ Iteration 900 --------------------------#
[32m[20221208 14:07:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:51 @agent_ppo2.py:179][0m |           0.0722 |         243.2524 |        -115.4429 |
[32m[20221208 14:07:51 @agent_ppo2.py:179][0m |           0.1220 |         209.0852 |         -70.2564 |
[32m[20221208 14:07:51 @agent_ppo2.py:179][0m |           0.0623 |         190.6056 |         -80.6763 |
[32m[20221208 14:07:51 @agent_ppo2.py:179][0m |           0.0308 |         180.5232 |        -106.4838 |
[32m[20221208 14:07:51 @agent_ppo2.py:179][0m |           0.0028 |         172.6313 |        -127.1724 |
[32m[20221208 14:07:52 @agent_ppo2.py:179][0m |          -0.0141 |         170.4007 |        -137.8396 |
[32m[20221208 14:07:52 @agent_ppo2.py:179][0m |          -0.0221 |         167.1378 |        -145.4438 |
[32m[20221208 14:07:52 @agent_ppo2.py:179][0m |          -0.0305 |         166.8026 |        -148.4214 |
[32m[20221208 14:07:52 @agent_ppo2.py:179][0m |          -0.0288 |         164.6459 |        -151.3808 |
[32m[20221208 14:07:52 @agent_ppo2.py:179][0m |          -0.0394 |         163.2145 |        -157.6833 |
[32m[20221208 14:07:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:07:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 886.77
[32m[20221208 14:07:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 926.09
[32m[20221208 14:07:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 933.30
[32m[20221208 14:07:52 @agent_ppo2.py:137][0m Total time:      22.53 min
[32m[20221208 14:07:52 @agent_ppo2.py:139][0m 1845248 total steps have happened
[32m[20221208 14:07:52 @agent_ppo2.py:115][0m #------------------------ Iteration 901 --------------------------#
[32m[20221208 14:07:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:07:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |           0.0745 |         266.0574 |        -129.8388 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |           0.0133 |         245.2069 |        -133.0144 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0112 |         240.8082 |        -142.5349 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0295 |         236.9747 |        -145.5755 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0414 |         234.7826 |        -151.2311 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0459 |         233.4708 |        -152.5435 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0479 |         231.8902 |        -154.8209 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0502 |         230.8404 |        -155.0393 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0556 |         230.2708 |        -158.0185 |
[32m[20221208 14:07:53 @agent_ppo2.py:179][0m |          -0.0558 |         230.3171 |        -159.5161 |
[32m[20221208 14:07:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:07:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 931.76
[32m[20221208 14:07:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.44
[32m[20221208 14:07:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.05
[32m[20221208 14:07:54 @agent_ppo2.py:137][0m Total time:      22.56 min
[32m[20221208 14:07:54 @agent_ppo2.py:139][0m 1847296 total steps have happened
[32m[20221208 14:07:54 @agent_ppo2.py:115][0m #------------------------ Iteration 902 --------------------------#
[32m[20221208 14:07:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:54 @agent_ppo2.py:179][0m |           0.1534 |         234.0138 |        -135.2849 |
[32m[20221208 14:07:54 @agent_ppo2.py:179][0m |           0.0991 |         220.1492 |         -83.9729 |
[32m[20221208 14:07:54 @agent_ppo2.py:179][0m |           0.0242 |         213.0384 |        -112.9186 |
[32m[20221208 14:07:54 @agent_ppo2.py:179][0m |          -0.0097 |         210.0012 |        -129.2748 |
[32m[20221208 14:07:54 @agent_ppo2.py:179][0m |          -0.0206 |         205.8287 |        -138.7952 |
[32m[20221208 14:07:55 @agent_ppo2.py:179][0m |          -0.0300 |         204.8843 |        -141.5803 |
[32m[20221208 14:07:55 @agent_ppo2.py:179][0m |          -0.0356 |         201.1208 |        -144.9035 |
[32m[20221208 14:07:55 @agent_ppo2.py:179][0m |          -0.0393 |         200.7511 |        -147.7546 |
[32m[20221208 14:07:55 @agent_ppo2.py:179][0m |          -0.0417 |         197.6988 |        -152.5708 |
[32m[20221208 14:07:55 @agent_ppo2.py:179][0m |          -0.0494 |         197.3593 |        -158.2983 |
[32m[20221208 14:07:55 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:07:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 834.14
[32m[20221208 14:07:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.97
[32m[20221208 14:07:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 644.92
[32m[20221208 14:07:55 @agent_ppo2.py:137][0m Total time:      22.58 min
[32m[20221208 14:07:55 @agent_ppo2.py:139][0m 1849344 total steps have happened
[32m[20221208 14:07:55 @agent_ppo2.py:115][0m #------------------------ Iteration 903 --------------------------#
[32m[20221208 14:07:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:07:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |           0.0649 |         249.4335 |        -150.4454 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |           0.0719 |         241.5742 |        -128.1446 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |           0.0254 |         239.0745 |        -123.5967 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |          -0.0124 |         234.7343 |        -147.9488 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |          -0.0255 |         232.5351 |        -158.8099 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |          -0.0354 |         230.8050 |        -162.9136 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |          -0.0437 |         230.5714 |        -169.3809 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |          -0.0491 |         229.7745 |        -173.7114 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |          -0.0513 |         228.9183 |        -177.4229 |
[32m[20221208 14:07:56 @agent_ppo2.py:179][0m |          -0.0522 |         227.3286 |        -177.8321 |
[32m[20221208 14:07:56 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:07:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 848.86
[32m[20221208 14:07:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 925.26
[32m[20221208 14:07:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.13
[32m[20221208 14:07:57 @agent_ppo2.py:137][0m Total time:      22.61 min
[32m[20221208 14:07:57 @agent_ppo2.py:139][0m 1851392 total steps have happened
[32m[20221208 14:07:57 @agent_ppo2.py:115][0m #------------------------ Iteration 904 --------------------------#
[32m[20221208 14:07:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:57 @agent_ppo2.py:179][0m |           0.0889 |         231.9204 |        -140.9978 |
[32m[20221208 14:07:57 @agent_ppo2.py:179][0m |           0.0441 |         209.6859 |        -126.8093 |
[32m[20221208 14:07:57 @agent_ppo2.py:179][0m |           0.0081 |         198.8053 |        -135.4757 |
[32m[20221208 14:07:57 @agent_ppo2.py:179][0m |          -0.0129 |         189.8829 |        -147.3233 |
[32m[20221208 14:07:58 @agent_ppo2.py:179][0m |          -0.0255 |         182.3052 |        -157.3626 |
[32m[20221208 14:07:58 @agent_ppo2.py:179][0m |          -0.0312 |         177.9428 |        -158.0961 |
[32m[20221208 14:07:58 @agent_ppo2.py:179][0m |          -0.0370 |         174.9119 |        -164.1584 |
[32m[20221208 14:07:58 @agent_ppo2.py:179][0m |          -0.0400 |         173.6081 |        -167.3920 |
[32m[20221208 14:07:58 @agent_ppo2.py:179][0m |          -0.0417 |         172.1688 |        -169.4525 |
[32m[20221208 14:07:58 @agent_ppo2.py:179][0m |          -0.0461 |         170.3957 |        -173.8305 |
[32m[20221208 14:07:58 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:07:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 929.31
[32m[20221208 14:07:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.84
[32m[20221208 14:07:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 793.14
[32m[20221208 14:07:58 @agent_ppo2.py:137][0m Total time:      22.63 min
[32m[20221208 14:07:58 @agent_ppo2.py:139][0m 1853440 total steps have happened
[32m[20221208 14:07:58 @agent_ppo2.py:115][0m #------------------------ Iteration 905 --------------------------#
[32m[20221208 14:07:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:07:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |           0.0797 |         225.6201 |        -143.6051 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |           0.0331 |         203.5404 |        -123.3798 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0096 |         197.5363 |        -142.7569 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0288 |         193.7191 |        -148.3979 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0371 |         190.9085 |        -152.4216 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0481 |         188.8000 |        -159.7829 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0512 |         186.2671 |        -159.6622 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0566 |         186.0135 |        -164.0647 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0580 |         182.4075 |        -167.7953 |
[32m[20221208 14:07:59 @agent_ppo2.py:179][0m |          -0.0635 |         180.8180 |        -168.7916 |
[32m[20221208 14:07:59 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.32
[32m[20221208 14:08:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.15
[32m[20221208 14:08:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.73
[32m[20221208 14:08:00 @agent_ppo2.py:137][0m Total time:      22.66 min
[32m[20221208 14:08:00 @agent_ppo2.py:139][0m 1855488 total steps have happened
[32m[20221208 14:08:00 @agent_ppo2.py:115][0m #------------------------ Iteration 906 --------------------------#
[32m[20221208 14:08:00 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:00 @agent_ppo2.py:179][0m |           0.0569 |         228.5373 |        -178.6138 |
[32m[20221208 14:08:00 @agent_ppo2.py:179][0m |           0.0495 |         203.9843 |        -147.9240 |
[32m[20221208 14:08:00 @agent_ppo2.py:179][0m |           0.0027 |         191.3777 |        -159.5655 |
[32m[20221208 14:08:00 @agent_ppo2.py:179][0m |          -0.0246 |         180.4079 |        -178.5315 |
[32m[20221208 14:08:00 @agent_ppo2.py:179][0m |          -0.0334 |         173.7500 |        -185.9720 |
[32m[20221208 14:08:01 @agent_ppo2.py:179][0m |          -0.0428 |         168.4386 |        -193.8773 |
[32m[20221208 14:08:01 @agent_ppo2.py:179][0m |          -0.0498 |         164.4718 |        -197.3587 |
[32m[20221208 14:08:01 @agent_ppo2.py:179][0m |          -0.0548 |         160.6680 |        -204.8565 |
[32m[20221208 14:08:01 @agent_ppo2.py:179][0m |          -0.0574 |         157.4104 |        -207.9851 |
[32m[20221208 14:08:01 @agent_ppo2.py:179][0m |          -0.0584 |         154.2660 |        -214.0926 |
[32m[20221208 14:08:01 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:08:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 678.19
[32m[20221208 14:08:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 916.70
[32m[20221208 14:08:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 891.01
[32m[20221208 14:08:01 @agent_ppo2.py:137][0m Total time:      22.68 min
[32m[20221208 14:08:01 @agent_ppo2.py:139][0m 1857536 total steps have happened
[32m[20221208 14:08:01 @agent_ppo2.py:115][0m #------------------------ Iteration 907 --------------------------#
[32m[20221208 14:08:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |           0.1227 |         253.2037 |        -167.4803 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |           0.0638 |         238.3491 |        -135.0274 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |           0.0370 |         233.0461 |        -149.7540 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |           0.0092 |         230.6045 |        -168.4621 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |          -0.0176 |         229.6461 |        -187.7183 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |          -0.0274 |         230.3495 |        -194.0408 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |          -0.0357 |         229.1682 |        -197.1207 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |          -0.0402 |         227.1847 |        -201.1977 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |          -0.0419 |         226.9365 |        -203.8513 |
[32m[20221208 14:08:02 @agent_ppo2.py:179][0m |          -0.0467 |         226.4021 |        -204.6819 |
[32m[20221208 14:08:02 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:08:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 920.78
[32m[20221208 14:08:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 978.98
[32m[20221208 14:08:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 657.98
[32m[20221208 14:08:03 @agent_ppo2.py:137][0m Total time:      22.70 min
[32m[20221208 14:08:03 @agent_ppo2.py:139][0m 1859584 total steps have happened
[32m[20221208 14:08:03 @agent_ppo2.py:115][0m #------------------------ Iteration 908 --------------------------#
[32m[20221208 14:08:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:03 @agent_ppo2.py:179][0m |           0.1503 |         247.0757 |        -164.8945 |
[32m[20221208 14:08:03 @agent_ppo2.py:179][0m |           0.0662 |         229.1138 |        -123.8449 |
[32m[20221208 14:08:03 @agent_ppo2.py:179][0m |           0.0209 |         221.5399 |        -150.3224 |
[32m[20221208 14:08:03 @agent_ppo2.py:179][0m |          -0.0143 |         216.3703 |        -172.6289 |
[32m[20221208 14:08:03 @agent_ppo2.py:179][0m |          -0.0300 |         212.6558 |        -180.8280 |
[32m[20221208 14:08:03 @agent_ppo2.py:179][0m |          -0.0408 |         210.2094 |        -187.3526 |
[32m[20221208 14:08:04 @agent_ppo2.py:179][0m |          -0.0421 |         208.1721 |        -191.0527 |
[32m[20221208 14:08:04 @agent_ppo2.py:179][0m |          -0.0529 |         205.8149 |        -197.5045 |
[32m[20221208 14:08:04 @agent_ppo2.py:179][0m |          -0.0554 |         203.4659 |        -201.1805 |
[32m[20221208 14:08:04 @agent_ppo2.py:179][0m |          -0.0590 |         201.7413 |        -201.4994 |
[32m[20221208 14:08:04 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 793.91
[32m[20221208 14:08:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 965.05
[32m[20221208 14:08:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 682.46
[32m[20221208 14:08:04 @agent_ppo2.py:137][0m Total time:      22.73 min
[32m[20221208 14:08:04 @agent_ppo2.py:139][0m 1861632 total steps have happened
[32m[20221208 14:08:04 @agent_ppo2.py:115][0m #------------------------ Iteration 909 --------------------------#
[32m[20221208 14:08:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |           0.0735 |         244.2709 |        -172.7579 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |           0.0955 |         237.1854 |        -118.9872 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |           0.0473 |         233.6876 |        -137.2265 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |           0.0026 |         231.1356 |        -166.4569 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |          -0.0160 |         228.8842 |        -182.6509 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |          -0.0230 |         229.1972 |        -183.6666 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |          -0.0237 |         228.5323 |        -185.6073 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |          -0.0273 |         227.3515 |        -186.5731 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |          -0.0365 |         226.1966 |        -192.1834 |
[32m[20221208 14:08:05 @agent_ppo2.py:179][0m |          -0.0407 |         226.9070 |        -200.6956 |
[32m[20221208 14:08:05 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:08:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.14
[32m[20221208 14:08:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.64
[32m[20221208 14:08:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 973.58
[32m[20221208 14:08:06 @agent_ppo2.py:137][0m Total time:      22.75 min
[32m[20221208 14:08:06 @agent_ppo2.py:139][0m 1863680 total steps have happened
[32m[20221208 14:08:06 @agent_ppo2.py:115][0m #------------------------ Iteration 910 --------------------------#
[32m[20221208 14:08:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:06 @agent_ppo2.py:179][0m |           0.0401 |         244.2976 |        -179.2356 |
[32m[20221208 14:08:06 @agent_ppo2.py:179][0m |           0.0351 |         235.7680 |        -153.4371 |
[32m[20221208 14:08:06 @agent_ppo2.py:179][0m |           0.0126 |         229.5209 |        -165.2054 |
[32m[20221208 14:08:06 @agent_ppo2.py:179][0m |          -0.0166 |         226.1608 |        -182.1661 |
[32m[20221208 14:08:06 @agent_ppo2.py:179][0m |          -0.0266 |         224.5908 |        -191.2419 |
[32m[20221208 14:08:06 @agent_ppo2.py:179][0m |          -0.0343 |         221.1226 |        -193.8003 |
[32m[20221208 14:08:07 @agent_ppo2.py:179][0m |          -0.0379 |         220.1727 |        -199.7479 |
[32m[20221208 14:08:07 @agent_ppo2.py:179][0m |          -0.0410 |         219.2642 |        -201.7212 |
[32m[20221208 14:08:07 @agent_ppo2.py:179][0m |          -0.0435 |         217.7575 |        -203.5201 |
[32m[20221208 14:08:07 @agent_ppo2.py:179][0m |          -0.0458 |         217.2604 |        -206.6001 |
[32m[20221208 14:08:07 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 912.60
[32m[20221208 14:08:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.74
[32m[20221208 14:08:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 926.70
[32m[20221208 14:08:07 @agent_ppo2.py:137][0m Total time:      22.78 min
[32m[20221208 14:08:07 @agent_ppo2.py:139][0m 1865728 total steps have happened
[32m[20221208 14:08:07 @agent_ppo2.py:115][0m #------------------------ Iteration 911 --------------------------#
[32m[20221208 14:08:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |           0.0687 |         246.0242 |        -175.9517 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |           0.0443 |         231.2895 |        -148.0808 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |           0.0035 |         224.4129 |        -176.4325 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |          -0.0114 |         220.5281 |        -185.3384 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |          -0.0260 |         217.9812 |        -194.5459 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |          -0.0368 |         216.1792 |        -206.3637 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |          -0.0413 |         215.1130 |        -209.5684 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |          -0.0450 |         213.3857 |        -211.5774 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |          -0.0481 |         212.9150 |        -219.1825 |
[32m[20221208 14:08:08 @agent_ppo2.py:179][0m |          -0.0510 |         211.5898 |        -221.6341 |
[32m[20221208 14:08:08 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.05
[32m[20221208 14:08:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 980.89
[32m[20221208 14:08:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.36
[32m[20221208 14:08:09 @agent_ppo2.py:137][0m Total time:      22.80 min
[32m[20221208 14:08:09 @agent_ppo2.py:139][0m 1867776 total steps have happened
[32m[20221208 14:08:09 @agent_ppo2.py:115][0m #------------------------ Iteration 912 --------------------------#
[32m[20221208 14:08:09 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:09 @agent_ppo2.py:179][0m |           0.0436 |         224.7455 |        -189.8332 |
[32m[20221208 14:08:09 @agent_ppo2.py:179][0m |           0.0437 |         203.2084 |        -164.0908 |
[32m[20221208 14:08:09 @agent_ppo2.py:179][0m |           0.0279 |         188.5796 |        -170.4966 |
[32m[20221208 14:08:09 @agent_ppo2.py:179][0m |          -0.0080 |         182.8708 |        -191.3045 |
[32m[20221208 14:08:09 @agent_ppo2.py:179][0m |          -0.0247 |         178.5146 |        -205.7454 |
[32m[20221208 14:08:09 @agent_ppo2.py:179][0m |          -0.0309 |         174.2050 |        -214.2782 |
[32m[20221208 14:08:09 @agent_ppo2.py:179][0m |          -0.0378 |         171.6892 |        -219.7498 |
[32m[20221208 14:08:10 @agent_ppo2.py:179][0m |          -0.0393 |         169.4886 |        -221.8524 |
[32m[20221208 14:08:10 @agent_ppo2.py:179][0m |          -0.0433 |         168.3948 |        -226.1813 |
[32m[20221208 14:08:10 @agent_ppo2.py:179][0m |          -0.0467 |         166.3167 |        -229.0376 |
[32m[20221208 14:08:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.02
[32m[20221208 14:08:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.02
[32m[20221208 14:08:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.79
[32m[20221208 14:08:10 @agent_ppo2.py:137][0m Total time:      22.83 min
[32m[20221208 14:08:10 @agent_ppo2.py:139][0m 1869824 total steps have happened
[32m[20221208 14:08:10 @agent_ppo2.py:115][0m #------------------------ Iteration 913 --------------------------#
[32m[20221208 14:08:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |           0.0367 |         201.8658 |        -149.8918 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |           0.0122 |         184.0244 |        -127.9797 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0327 |         178.4221 |        -142.6163 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0491 |         173.4440 |        -151.1277 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0636 |         170.6674 |        -152.3363 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0708 |         167.9506 |        -162.2183 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0702 |         166.4307 |        -166.1624 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0772 |         163.2274 |        -171.9046 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0819 |         162.1801 |        -176.6982 |
[32m[20221208 14:08:11 @agent_ppo2.py:179][0m |          -0.0842 |         160.0147 |        -176.6255 |
[32m[20221208 14:08:11 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 635.05
[32m[20221208 14:08:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.57
[32m[20221208 14:08:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.54
[32m[20221208 14:08:12 @agent_ppo2.py:137][0m Total time:      22.85 min
[32m[20221208 14:08:12 @agent_ppo2.py:139][0m 1871872 total steps have happened
[32m[20221208 14:08:12 @agent_ppo2.py:115][0m #------------------------ Iteration 914 --------------------------#
[32m[20221208 14:08:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |           0.0311 |         225.3434 |        -218.1575 |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |           0.0179 |         205.2403 |        -207.3447 |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |          -0.0074 |         195.1015 |        -218.5944 |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |          -0.0196 |         187.4396 |        -224.3397 |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |          -0.0354 |         182.1471 |        -233.8976 |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |          -0.0428 |         179.1680 |        -238.7486 |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |          -0.0441 |         177.9193 |        -241.5377 |
[32m[20221208 14:08:12 @agent_ppo2.py:179][0m |          -0.0413 |         175.7687 |        -243.2973 |
[32m[20221208 14:08:13 @agent_ppo2.py:179][0m |          -0.0444 |         175.1283 |        -243.4281 |
[32m[20221208 14:08:13 @agent_ppo2.py:179][0m |          -0.0446 |         172.7454 |        -242.2482 |
[32m[20221208 14:08:13 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.34
[32m[20221208 14:08:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.19
[32m[20221208 14:08:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 885.45
[32m[20221208 14:08:13 @agent_ppo2.py:137][0m Total time:      22.88 min
[32m[20221208 14:08:13 @agent_ppo2.py:139][0m 1873920 total steps have happened
[32m[20221208 14:08:13 @agent_ppo2.py:115][0m #------------------------ Iteration 915 --------------------------#
[32m[20221208 14:08:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:08:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |           0.0830 |         260.7392 |        -200.1573 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |           0.0435 |         238.6356 |        -177.9616 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |           0.0076 |         230.6600 |        -202.5797 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |          -0.0104 |         225.9925 |        -198.2279 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |          -0.0282 |         223.2824 |        -207.2915 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |          -0.0433 |         221.7691 |        -215.7370 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |          -0.0502 |         219.2798 |        -220.1898 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |          -0.0543 |         217.7155 |        -225.0861 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |          -0.0566 |         215.8847 |        -230.8751 |
[32m[20221208 14:08:14 @agent_ppo2.py:179][0m |          -0.0570 |         214.6887 |        -229.7315 |
[32m[20221208 14:08:14 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 809.56
[32m[20221208 14:08:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.78
[32m[20221208 14:08:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 619.68
[32m[20221208 14:08:14 @agent_ppo2.py:137][0m Total time:      22.90 min
[32m[20221208 14:08:14 @agent_ppo2.py:139][0m 1875968 total steps have happened
[32m[20221208 14:08:14 @agent_ppo2.py:115][0m #------------------------ Iteration 916 --------------------------#
[32m[20221208 14:08:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |           0.0931 |         235.2488 |        -196.6927 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |           0.0564 |         218.5900 |        -148.3204 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |           0.0046 |         211.7501 |        -197.7717 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |          -0.0146 |         206.8287 |        -210.9894 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |          -0.0205 |         203.8788 |        -216.5754 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |          -0.0356 |         201.9296 |        -225.4208 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |          -0.0440 |         200.1382 |        -233.2585 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |          -0.0468 |         198.9526 |        -234.6734 |
[32m[20221208 14:08:15 @agent_ppo2.py:179][0m |          -0.0502 |         197.0751 |        -240.5539 |
[32m[20221208 14:08:16 @agent_ppo2.py:179][0m |          -0.0524 |         198.1218 |        -243.8216 |
[32m[20221208 14:08:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:16 @agent_ppo2.py:132][0m Average TRAINING episode reward: 868.58
[32m[20221208 14:08:16 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.19
[32m[20221208 14:08:16 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.61
[32m[20221208 14:08:16 @agent_ppo2.py:137][0m Total time:      22.93 min
[32m[20221208 14:08:16 @agent_ppo2.py:139][0m 1878016 total steps have happened
[32m[20221208 14:08:16 @agent_ppo2.py:115][0m #------------------------ Iteration 917 --------------------------#
[32m[20221208 14:08:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:16 @agent_ppo2.py:179][0m |           0.0627 |         265.5477 |        -219.0055 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |           0.0605 |         250.2990 |        -160.4760 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |           0.0242 |         245.7093 |        -191.2208 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |          -0.0169 |         241.1686 |        -205.3999 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |          -0.0315 |         239.3019 |        -217.8416 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |          -0.0391 |         237.7884 |        -229.3498 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |          -0.0469 |         235.1784 |        -231.6037 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |          -0.0507 |         233.7521 |        -237.2829 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |          -0.0563 |         232.4015 |        -242.6903 |
[32m[20221208 14:08:17 @agent_ppo2.py:179][0m |          -0.0583 |         231.3141 |        -244.2414 |
[32m[20221208 14:08:17 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:08:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 867.53
[32m[20221208 14:08:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.64
[32m[20221208 14:08:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 883.56
[32m[20221208 14:08:17 @agent_ppo2.py:137][0m Total time:      22.95 min
[32m[20221208 14:08:17 @agent_ppo2.py:139][0m 1880064 total steps have happened
[32m[20221208 14:08:17 @agent_ppo2.py:115][0m #------------------------ Iteration 918 --------------------------#
[32m[20221208 14:08:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |           0.0654 |         244.1917 |        -179.5102 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |           0.0732 |         232.6585 |        -127.6838 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |           0.0153 |         227.3720 |        -178.7500 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |          -0.0025 |         224.7086 |        -191.8215 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |          -0.0117 |         224.1549 |        -196.5660 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |          -0.0326 |         222.0416 |        -214.2524 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |          -0.0387 |         220.6120 |        -221.2858 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |          -0.0395 |         219.7646 |        -224.7103 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |          -0.0482 |         219.2173 |        -232.6407 |
[32m[20221208 14:08:18 @agent_ppo2.py:179][0m |          -0.0453 |         219.4550 |        -230.4614 |
[32m[20221208 14:08:18 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:08:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 897.59
[32m[20221208 14:08:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 948.69
[32m[20221208 14:08:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.27
[32m[20221208 14:08:19 @agent_ppo2.py:137][0m Total time:      22.98 min
[32m[20221208 14:08:19 @agent_ppo2.py:139][0m 1882112 total steps have happened
[32m[20221208 14:08:19 @agent_ppo2.py:115][0m #------------------------ Iteration 919 --------------------------#
[32m[20221208 14:08:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:19 @agent_ppo2.py:179][0m |           0.1193 |         239.1888 |        -177.7281 |
[32m[20221208 14:08:19 @agent_ppo2.py:179][0m |           0.0770 |         225.0186 |        -133.1888 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |           0.0307 |         220.9443 |        -176.3190 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |          -0.0024 |         216.3141 |        -205.9403 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |          -0.0156 |         215.0517 |        -210.6017 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |          -0.0262 |         211.6887 |        -221.3693 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |          -0.0336 |         210.1065 |        -228.6145 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |          -0.0369 |         208.9083 |        -236.5491 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |          -0.0407 |         207.9825 |        -239.8753 |
[32m[20221208 14:08:20 @agent_ppo2.py:179][0m |          -0.0438 |         205.9162 |        -247.5607 |
[32m[20221208 14:08:20 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.10
[32m[20221208 14:08:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.78
[32m[20221208 14:08:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 905.76
[32m[20221208 14:08:20 @agent_ppo2.py:137][0m Total time:      23.00 min
[32m[20221208 14:08:20 @agent_ppo2.py:139][0m 1884160 total steps have happened
[32m[20221208 14:08:20 @agent_ppo2.py:115][0m #------------------------ Iteration 920 --------------------------#
[32m[20221208 14:08:21 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |           0.0709 |         243.8665 |        -196.2587 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |           0.0640 |         232.2550 |        -137.1615 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |           0.0117 |         225.3394 |        -199.3181 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |          -0.0035 |         220.6238 |        -217.9787 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |          -0.0229 |         216.5835 |        -230.1445 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |          -0.0315 |         213.6570 |        -240.8980 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |          -0.0320 |         212.9642 |        -246.8523 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |          -0.0294 |         212.1697 |        -240.1898 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |          -0.0382 |         209.8332 |        -251.7075 |
[32m[20221208 14:08:21 @agent_ppo2.py:179][0m |          -0.0478 |         208.1953 |        -260.6134 |
[32m[20221208 14:08:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 904.66
[32m[20221208 14:08:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 933.18
[32m[20221208 14:08:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 906.63
[32m[20221208 14:08:22 @agent_ppo2.py:137][0m Total time:      23.02 min
[32m[20221208 14:08:22 @agent_ppo2.py:139][0m 1886208 total steps have happened
[32m[20221208 14:08:22 @agent_ppo2.py:115][0m #------------------------ Iteration 921 --------------------------#
[32m[20221208 14:08:22 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:08:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:22 @agent_ppo2.py:179][0m |           0.1345 |         241.4665 |        -199.2039 |
[32m[20221208 14:08:22 @agent_ppo2.py:179][0m |           0.0702 |         218.9458 |        -155.1983 |
[32m[20221208 14:08:22 @agent_ppo2.py:179][0m |           0.0305 |         211.7868 |        -179.0872 |
[32m[20221208 14:08:23 @agent_ppo2.py:179][0m |          -0.0048 |         210.3387 |        -214.6630 |
[32m[20221208 14:08:23 @agent_ppo2.py:179][0m |          -0.0193 |         208.9151 |        -225.8615 |
[32m[20221208 14:08:23 @agent_ppo2.py:179][0m |          -0.0284 |         204.8114 |        -235.1160 |
[32m[20221208 14:08:23 @agent_ppo2.py:179][0m |          -0.0377 |         203.4855 |        -243.2917 |
[32m[20221208 14:08:23 @agent_ppo2.py:179][0m |          -0.0395 |         202.5495 |        -247.4336 |
[32m[20221208 14:08:23 @agent_ppo2.py:179][0m |          -0.0447 |         201.0725 |        -253.0077 |
[32m[20221208 14:08:23 @agent_ppo2.py:179][0m |          -0.0505 |         200.0258 |        -257.1132 |
[32m[20221208 14:08:23 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 893.67
[32m[20221208 14:08:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 909.95
[32m[20221208 14:08:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.47
[32m[20221208 14:08:23 @agent_ppo2.py:137][0m Total time:      23.05 min
[32m[20221208 14:08:23 @agent_ppo2.py:139][0m 1888256 total steps have happened
[32m[20221208 14:08:23 @agent_ppo2.py:115][0m #------------------------ Iteration 922 --------------------------#
[32m[20221208 14:08:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |           0.0847 |         246.7188 |        -201.3440 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |           0.0751 |         233.2382 |        -143.2089 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |           0.0195 |         228.6677 |        -188.0387 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |          -0.0128 |         225.6668 |        -220.9555 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |          -0.0265 |         223.8267 |        -235.1655 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |          -0.0350 |         222.9382 |        -237.9290 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |          -0.0406 |         222.3482 |        -240.2862 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |          -0.0444 |         220.9165 |        -245.0475 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |          -0.0501 |         220.2713 |        -251.8672 |
[32m[20221208 14:08:24 @agent_ppo2.py:179][0m |          -0.0521 |         220.2833 |        -256.2605 |
[32m[20221208 14:08:24 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:08:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 874.56
[32m[20221208 14:08:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 928.01
[32m[20221208 14:08:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 927.57
[32m[20221208 14:08:25 @agent_ppo2.py:137][0m Total time:      23.07 min
[32m[20221208 14:08:25 @agent_ppo2.py:139][0m 1890304 total steps have happened
[32m[20221208 14:08:25 @agent_ppo2.py:115][0m #------------------------ Iteration 923 --------------------------#
[32m[20221208 14:08:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:08:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:25 @agent_ppo2.py:179][0m |           0.0380 |         244.1132 |        -210.3691 |
[32m[20221208 14:08:25 @agent_ppo2.py:179][0m |           0.0445 |         236.5542 |        -187.5769 |
[32m[20221208 14:08:25 @agent_ppo2.py:179][0m |           0.0144 |         232.8518 |        -197.2178 |
[32m[20221208 14:08:26 @agent_ppo2.py:179][0m |           0.0014 |         230.2377 |        -210.2114 |
[32m[20221208 14:08:26 @agent_ppo2.py:179][0m |          -0.0146 |         227.6105 |        -220.3765 |
[32m[20221208 14:08:26 @agent_ppo2.py:179][0m |          -0.0286 |         226.6223 |        -225.6715 |
[32m[20221208 14:08:26 @agent_ppo2.py:179][0m |          -0.0370 |         225.8691 |        -238.4457 |
[32m[20221208 14:08:26 @agent_ppo2.py:179][0m |          -0.0403 |         225.5370 |        -239.8024 |
[32m[20221208 14:08:26 @agent_ppo2.py:179][0m |          -0.0472 |         224.1442 |        -246.4479 |
[32m[20221208 14:08:26 @agent_ppo2.py:179][0m |          -0.0498 |         223.7136 |        -251.7002 |
[32m[20221208 14:08:26 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:08:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 919.94
[32m[20221208 14:08:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.22
[32m[20221208 14:08:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 910.40
[32m[20221208 14:08:26 @agent_ppo2.py:137][0m Total time:      23.10 min
[32m[20221208 14:08:26 @agent_ppo2.py:139][0m 1892352 total steps have happened
[32m[20221208 14:08:26 @agent_ppo2.py:115][0m #------------------------ Iteration 924 --------------------------#
[32m[20221208 14:08:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:08:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |           0.0874 |         244.0402 |        -211.3173 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |           0.1233 |         222.7649 |        -130.1926 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |           0.0527 |         213.4768 |        -158.8901 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |          -0.0004 |         208.0729 |        -187.0211 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |          -0.0251 |         205.9206 |        -197.7655 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |          -0.0357 |         201.8024 |        -201.4420 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |          -0.0433 |         199.6988 |        -209.6993 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |          -0.0510 |         197.0626 |        -210.3699 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |          -0.0556 |         195.2396 |        -218.9382 |
[32m[20221208 14:08:27 @agent_ppo2.py:179][0m |          -0.0590 |         193.8859 |        -221.6004 |
[32m[20221208 14:08:27 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:08:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 741.51
[32m[20221208 14:08:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.37
[32m[20221208 14:08:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.32
[32m[20221208 14:08:28 @agent_ppo2.py:137][0m Total time:      23.13 min
[32m[20221208 14:08:28 @agent_ppo2.py:139][0m 1894400 total steps have happened
[32m[20221208 14:08:28 @agent_ppo2.py:115][0m #------------------------ Iteration 925 --------------------------#
[32m[20221208 14:08:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:08:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:28 @agent_ppo2.py:179][0m |           0.0939 |         238.7716 |        -202.9705 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |           0.0740 |         224.6112 |        -154.4017 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |           0.0254 |         222.0605 |        -182.8806 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |          -0.0012 |         221.7171 |        -206.0591 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |          -0.0111 |         219.6798 |        -216.2319 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |          -0.0108 |         220.1647 |        -218.2342 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |          -0.0258 |         218.2076 |        -222.8005 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |          -0.0322 |         217.3993 |        -231.6933 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |          -0.0384 |         216.9768 |        -234.9352 |
[32m[20221208 14:08:29 @agent_ppo2.py:179][0m |          -0.0409 |         217.5062 |        -239.1980 |
[32m[20221208 14:08:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:08:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 961.89
[32m[20221208 14:08:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.14
[32m[20221208 14:08:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.52
[32m[20221208 14:08:29 @agent_ppo2.py:137][0m Total time:      23.15 min
[32m[20221208 14:08:29 @agent_ppo2.py:139][0m 1896448 total steps have happened
[32m[20221208 14:08:29 @agent_ppo2.py:115][0m #------------------------ Iteration 926 --------------------------#
[32m[20221208 14:08:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |           0.0503 |         234.4527 |        -184.3424 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |           0.0583 |         229.5950 |        -156.5346 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |           0.0277 |         228.2583 |        -162.4405 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |           0.0011 |         225.4576 |        -196.2007 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |          -0.0096 |         224.4577 |        -199.9801 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |          -0.0256 |         223.9288 |        -217.5126 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |          -0.0269 |         222.9624 |        -219.3029 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |          -0.0359 |         221.9158 |        -229.5280 |
[32m[20221208 14:08:30 @agent_ppo2.py:179][0m |          -0.0408 |         221.6091 |        -230.7021 |
[32m[20221208 14:08:31 @agent_ppo2.py:179][0m |          -0.0425 |         221.1125 |        -234.9802 |
[32m[20221208 14:08:31 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:08:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.40
[32m[20221208 14:08:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.93
[32m[20221208 14:08:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 904.98
[32m[20221208 14:08:31 @agent_ppo2.py:137][0m Total time:      23.18 min
[32m[20221208 14:08:31 @agent_ppo2.py:139][0m 1898496 total steps have happened
[32m[20221208 14:08:31 @agent_ppo2.py:115][0m #------------------------ Iteration 927 --------------------------#
[32m[20221208 14:08:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:31 @agent_ppo2.py:179][0m |           0.0506 |         230.1275 |        -203.1595 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |           0.0541 |         212.4362 |        -179.2354 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |           0.0596 |         204.9671 |        -141.2433 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |           0.0201 |         199.6797 |        -179.0975 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |          -0.0031 |         195.7966 |        -191.9079 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |          -0.0201 |         192.7311 |        -208.5838 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |          -0.0247 |         191.4340 |        -213.2856 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |          -0.0328 |         189.4773 |        -221.4206 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |          -0.0384 |         187.0926 |        -226.8904 |
[32m[20221208 14:08:32 @agent_ppo2.py:179][0m |          -0.0425 |         186.1014 |        -233.7153 |
[32m[20221208 14:08:32 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.17
[32m[20221208 14:08:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.79
[32m[20221208 14:08:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.90
[32m[20221208 14:08:32 @agent_ppo2.py:137][0m Total time:      23.20 min
[32m[20221208 14:08:32 @agent_ppo2.py:139][0m 1900544 total steps have happened
[32m[20221208 14:08:32 @agent_ppo2.py:115][0m #------------------------ Iteration 928 --------------------------#
[32m[20221208 14:08:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |           0.0913 |         228.6624 |        -191.2324 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |           0.1260 |         218.6894 |         -95.9425 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |           0.0770 |         214.1390 |        -100.5869 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |           0.0431 |         210.8623 |        -136.2934 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |           0.0127 |         209.5768 |        -173.5856 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |          -0.0074 |         208.3768 |        -189.8905 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |          -0.0189 |         207.2711 |        -203.3996 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |          -0.0263 |         205.5141 |        -209.8386 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |          -0.0307 |         206.0845 |        -218.4436 |
[32m[20221208 14:08:33 @agent_ppo2.py:179][0m |          -0.0357 |         205.0031 |        -223.4089 |
[32m[20221208 14:08:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 955.33
[32m[20221208 14:08:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 960.11
[32m[20221208 14:08:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.19
[32m[20221208 14:08:34 @agent_ppo2.py:137][0m Total time:      23.23 min
[32m[20221208 14:08:34 @agent_ppo2.py:139][0m 1902592 total steps have happened
[32m[20221208 14:08:34 @agent_ppo2.py:115][0m #------------------------ Iteration 929 --------------------------#
[32m[20221208 14:08:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:34 @agent_ppo2.py:179][0m |           0.0833 |         246.3140 |        -167.4861 |
[32m[20221208 14:08:34 @agent_ppo2.py:179][0m |           0.0693 |         233.7985 |        -148.8273 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |           0.0350 |         228.9196 |        -148.7519 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |          -0.0048 |         224.7291 |        -173.4914 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |          -0.0172 |         220.9703 |        -183.2367 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |          -0.0224 |         219.5413 |        -184.2145 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |          -0.0317 |         218.2370 |        -189.0934 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |          -0.0382 |         217.1910 |        -192.9623 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |          -0.0426 |         216.1830 |        -195.9763 |
[32m[20221208 14:08:35 @agent_ppo2.py:179][0m |          -0.0454 |         216.1946 |        -198.8469 |
[32m[20221208 14:08:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:08:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.39
[32m[20221208 14:08:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.01
[32m[20221208 14:08:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 691.48
[32m[20221208 14:08:35 @agent_ppo2.py:137][0m Total time:      23.25 min
[32m[20221208 14:08:35 @agent_ppo2.py:139][0m 1904640 total steps have happened
[32m[20221208 14:08:35 @agent_ppo2.py:115][0m #------------------------ Iteration 930 --------------------------#
[32m[20221208 14:08:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |           0.0788 |         242.7373 |        -177.9949 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |           0.0731 |         230.5643 |        -150.5857 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |           0.0101 |         224.4940 |        -177.5003 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |          -0.0086 |         216.8541 |        -189.9934 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |          -0.0229 |         212.3401 |        -200.0923 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |          -0.0330 |         210.0373 |        -206.9157 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |          -0.0383 |         206.9669 |        -213.0875 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |          -0.0418 |         205.4755 |        -218.5160 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |          -0.0388 |         203.1308 |        -216.8198 |
[32m[20221208 14:08:36 @agent_ppo2.py:179][0m |          -0.0442 |         203.0700 |        -222.5140 |
[32m[20221208 14:08:36 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.68
[32m[20221208 14:08:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.84
[32m[20221208 14:08:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 671.21
[32m[20221208 14:08:37 @agent_ppo2.py:137][0m Total time:      23.27 min
[32m[20221208 14:08:37 @agent_ppo2.py:139][0m 1906688 total steps have happened
[32m[20221208 14:08:37 @agent_ppo2.py:115][0m #------------------------ Iteration 931 --------------------------#
[32m[20221208 14:08:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:37 @agent_ppo2.py:179][0m |           0.0421 |         245.8873 |        -188.9050 |
[32m[20221208 14:08:37 @agent_ppo2.py:179][0m |           0.0147 |         220.0390 |        -180.3364 |
[32m[20221208 14:08:37 @agent_ppo2.py:179][0m |          -0.0086 |         210.9618 |        -191.8078 |
[32m[20221208 14:08:38 @agent_ppo2.py:179][0m |          -0.0246 |         206.4691 |        -199.5862 |
[32m[20221208 14:08:38 @agent_ppo2.py:179][0m |          -0.0360 |         204.2743 |        -208.5175 |
[32m[20221208 14:08:38 @agent_ppo2.py:179][0m |          -0.0422 |         200.8371 |        -209.6238 |
[32m[20221208 14:08:38 @agent_ppo2.py:179][0m |          -0.0428 |         198.0379 |        -210.7924 |
[32m[20221208 14:08:38 @agent_ppo2.py:179][0m |          -0.0499 |         197.4240 |        -220.1311 |
[32m[20221208 14:08:38 @agent_ppo2.py:179][0m |          -0.0544 |         194.9721 |        -225.6928 |
[32m[20221208 14:08:38 @agent_ppo2.py:179][0m |          -0.0527 |         194.7479 |        -227.2278 |
[32m[20221208 14:08:38 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.77
[32m[20221208 14:08:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.94
[32m[20221208 14:08:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 944.00
[32m[20221208 14:08:38 @agent_ppo2.py:137][0m Total time:      23.30 min
[32m[20221208 14:08:38 @agent_ppo2.py:139][0m 1908736 total steps have happened
[32m[20221208 14:08:38 @agent_ppo2.py:115][0m #------------------------ Iteration 932 --------------------------#
[32m[20221208 14:08:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |           0.0838 |         235.2534 |        -181.0846 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |           0.0701 |         219.7798 |        -138.6230 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |           0.0346 |         208.9668 |        -156.0673 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |          -0.0042 |         199.4756 |        -201.2871 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |          -0.0183 |         192.9748 |        -214.7884 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |          -0.0269 |         188.5439 |        -222.1078 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |          -0.0338 |         185.2914 |        -229.5151 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |          -0.0407 |         180.2934 |        -233.9725 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |          -0.0462 |         177.6655 |        -238.0134 |
[32m[20221208 14:08:39 @agent_ppo2.py:179][0m |          -0.0485 |         174.1748 |        -246.4136 |
[32m[20221208 14:08:39 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:08:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.09
[32m[20221208 14:08:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.61
[32m[20221208 14:08:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 870.20
[32m[20221208 14:08:40 @agent_ppo2.py:137][0m Total time:      23.32 min
[32m[20221208 14:08:40 @agent_ppo2.py:139][0m 1910784 total steps have happened
[32m[20221208 14:08:40 @agent_ppo2.py:115][0m #------------------------ Iteration 933 --------------------------#
[32m[20221208 14:08:40 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:08:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:40 @agent_ppo2.py:179][0m |           0.0671 |         260.8953 |        -211.1718 |
[32m[20221208 14:08:40 @agent_ppo2.py:179][0m |           0.0844 |         243.4953 |        -154.8349 |
[32m[20221208 14:08:40 @agent_ppo2.py:179][0m |           0.0573 |         237.2357 |        -149.7672 |
[32m[20221208 14:08:41 @agent_ppo2.py:179][0m |           0.0151 |         232.7617 |        -185.8927 |
[32m[20221208 14:08:41 @agent_ppo2.py:179][0m |          -0.0116 |         229.5982 |        -208.7327 |
[32m[20221208 14:08:41 @agent_ppo2.py:179][0m |          -0.0242 |         227.1861 |        -220.2499 |
[32m[20221208 14:08:41 @agent_ppo2.py:179][0m |          -0.0285 |         226.7149 |        -225.4063 |
[32m[20221208 14:08:41 @agent_ppo2.py:179][0m |          -0.0344 |         224.0198 |        -234.4490 |
[32m[20221208 14:08:41 @agent_ppo2.py:179][0m |          -0.0398 |         223.8810 |        -229.8123 |
[32m[20221208 14:08:41 @agent_ppo2.py:179][0m |          -0.0459 |         224.8195 |        -239.9437 |
[32m[20221208 14:08:41 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.88
[32m[20221208 14:08:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 988.61
[32m[20221208 14:08:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.68
[32m[20221208 14:08:41 @agent_ppo2.py:137][0m Total time:      23.35 min
[32m[20221208 14:08:41 @agent_ppo2.py:139][0m 1912832 total steps have happened
[32m[20221208 14:08:41 @agent_ppo2.py:115][0m #------------------------ Iteration 934 --------------------------#
[32m[20221208 14:08:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |           0.1084 |         250.9758 |        -195.7334 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |           0.1152 |         228.2189 |        -140.3157 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |           0.0409 |         220.5660 |        -157.0268 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |           0.0069 |         214.9621 |        -195.7788 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |          -0.0120 |         211.5734 |        -207.4807 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |          -0.0277 |         209.5150 |        -219.5860 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |          -0.0359 |         207.1463 |        -227.1402 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |          -0.0458 |         207.2975 |        -236.8610 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |          -0.0519 |         206.2570 |        -241.9560 |
[32m[20221208 14:08:42 @agent_ppo2.py:179][0m |          -0.0555 |         204.0431 |        -245.1196 |
[32m[20221208 14:08:42 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 868.07
[32m[20221208 14:08:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 870.05
[32m[20221208 14:08:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 912.81
[32m[20221208 14:08:43 @agent_ppo2.py:137][0m Total time:      23.37 min
[32m[20221208 14:08:43 @agent_ppo2.py:139][0m 1914880 total steps have happened
[32m[20221208 14:08:43 @agent_ppo2.py:115][0m #------------------------ Iteration 935 --------------------------#
[32m[20221208 14:08:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:43 @agent_ppo2.py:179][0m |           0.0717 |         248.4165 |        -209.0514 |
[32m[20221208 14:08:43 @agent_ppo2.py:179][0m |           0.0643 |         236.2187 |        -160.6095 |
[32m[20221208 14:08:43 @agent_ppo2.py:179][0m |           0.0114 |         232.0915 |        -198.3782 |
[32m[20221208 14:08:43 @agent_ppo2.py:179][0m |          -0.0197 |         230.6119 |        -220.5221 |
[32m[20221208 14:08:44 @agent_ppo2.py:179][0m |          -0.0316 |         230.2668 |        -232.6196 |
[32m[20221208 14:08:44 @agent_ppo2.py:179][0m |          -0.0359 |         230.5077 |        -233.4172 |
[32m[20221208 14:08:44 @agent_ppo2.py:179][0m |          -0.0411 |         227.8321 |        -243.8533 |
[32m[20221208 14:08:44 @agent_ppo2.py:179][0m |          -0.0464 |         228.1609 |        -247.7462 |
[32m[20221208 14:08:44 @agent_ppo2.py:179][0m |          -0.0472 |         227.5066 |        -250.6550 |
[32m[20221208 14:08:44 @agent_ppo2.py:179][0m |          -0.0539 |         226.4321 |        -255.6894 |
[32m[20221208 14:08:44 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:44 @agent_ppo2.py:132][0m Average TRAINING episode reward: 873.36
[32m[20221208 14:08:44 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.93
[32m[20221208 14:08:44 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 887.86
[32m[20221208 14:08:44 @agent_ppo2.py:137][0m Total time:      23.40 min
[32m[20221208 14:08:44 @agent_ppo2.py:139][0m 1916928 total steps have happened
[32m[20221208 14:08:44 @agent_ppo2.py:115][0m #------------------------ Iteration 936 --------------------------#
[32m[20221208 14:08:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |           0.0746 |         250.4251 |        -212.0549 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |           0.0595 |         239.7717 |        -170.1873 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |           0.0065 |         234.7245 |        -217.6325 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |          -0.0149 |         232.3162 |        -228.0936 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |          -0.0282 |         232.6643 |        -237.4447 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |          -0.0343 |         230.1488 |        -243.4578 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |          -0.0439 |         229.3362 |        -251.4978 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |          -0.0457 |         226.6837 |        -258.0296 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |          -0.0489 |         226.1158 |        -257.7423 |
[32m[20221208 14:08:45 @agent_ppo2.py:179][0m |          -0.0496 |         226.5357 |        -262.4713 |
[32m[20221208 14:08:45 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 875.40
[32m[20221208 14:08:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.35
[32m[20221208 14:08:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.86
[32m[20221208 14:08:46 @agent_ppo2.py:137][0m Total time:      23.42 min
[32m[20221208 14:08:46 @agent_ppo2.py:139][0m 1918976 total steps have happened
[32m[20221208 14:08:46 @agent_ppo2.py:115][0m #------------------------ Iteration 937 --------------------------#
[32m[20221208 14:08:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:46 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:46 @agent_ppo2.py:179][0m |           0.1071 |         235.3471 |        -194.0827 |
[32m[20221208 14:08:46 @agent_ppo2.py:179][0m |           0.0650 |         230.4390 |        -150.2743 |
[32m[20221208 14:08:46 @agent_ppo2.py:179][0m |           0.0592 |         229.1290 |        -151.4639 |
[32m[20221208 14:08:46 @agent_ppo2.py:179][0m |           0.0156 |         226.5184 |        -183.6340 |
[32m[20221208 14:08:46 @agent_ppo2.py:179][0m |          -0.0034 |         225.8617 |        -202.5970 |
[32m[20221208 14:08:46 @agent_ppo2.py:179][0m |          -0.0165 |         225.7394 |        -212.6414 |
[32m[20221208 14:08:47 @agent_ppo2.py:179][0m |          -0.0276 |         222.9610 |        -224.5137 |
[32m[20221208 14:08:47 @agent_ppo2.py:179][0m |          -0.0376 |         221.3486 |        -231.0484 |
[32m[20221208 14:08:47 @agent_ppo2.py:179][0m |          -0.0422 |         221.0550 |        -236.8442 |
[32m[20221208 14:08:47 @agent_ppo2.py:179][0m |          -0.0450 |         221.4887 |        -240.8761 |
[32m[20221208 14:08:47 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:47 @agent_ppo2.py:132][0m Average TRAINING episode reward: 858.14
[32m[20221208 14:08:47 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 917.11
[32m[20221208 14:08:47 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 930.14
[32m[20221208 14:08:47 @agent_ppo2.py:137][0m Total time:      23.45 min
[32m[20221208 14:08:47 @agent_ppo2.py:139][0m 1921024 total steps have happened
[32m[20221208 14:08:47 @agent_ppo2.py:115][0m #------------------------ Iteration 938 --------------------------#
[32m[20221208 14:08:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |           0.0562 |         229.0991 |        -196.7652 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |           0.0576 |         205.2986 |        -158.6786 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |           0.0239 |         191.8628 |        -189.5324 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |           0.0010 |         183.3171 |        -199.9413 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |          -0.0206 |         182.6390 |        -221.7743 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |          -0.0273 |         179.0346 |        -229.5297 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |          -0.0356 |         176.9833 |        -238.6483 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |          -0.0390 |         175.0813 |        -242.2275 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |          -0.0451 |         173.6115 |        -248.1528 |
[32m[20221208 14:08:48 @agent_ppo2.py:179][0m |          -0.0474 |         172.5326 |        -250.8448 |
[32m[20221208 14:08:48 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:08:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.31
[32m[20221208 14:08:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.20
[32m[20221208 14:08:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 906.02
[32m[20221208 14:08:49 @agent_ppo2.py:137][0m Total time:      23.47 min
[32m[20221208 14:08:49 @agent_ppo2.py:139][0m 1923072 total steps have happened
[32m[20221208 14:08:49 @agent_ppo2.py:115][0m #------------------------ Iteration 939 --------------------------#
[32m[20221208 14:08:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:49 @agent_ppo2.py:179][0m |           0.0691 |         249.4543 |        -205.4324 |
[32m[20221208 14:08:49 @agent_ppo2.py:179][0m |           0.0645 |         241.2259 |        -146.4310 |
[32m[20221208 14:08:49 @agent_ppo2.py:179][0m |           0.0250 |         237.8114 |        -173.8541 |
[32m[20221208 14:08:49 @agent_ppo2.py:179][0m |          -0.0011 |         236.0202 |        -196.2521 |
[32m[20221208 14:08:49 @agent_ppo2.py:179][0m |          -0.0176 |         234.7785 |        -208.3056 |
[32m[20221208 14:08:49 @agent_ppo2.py:179][0m |          -0.0302 |         232.7964 |        -214.5176 |
[32m[20221208 14:08:49 @agent_ppo2.py:179][0m |          -0.0309 |         233.3598 |        -220.8465 |
[32m[20221208 14:08:50 @agent_ppo2.py:179][0m |          -0.0397 |         231.2637 |        -230.2873 |
[32m[20221208 14:08:50 @agent_ppo2.py:179][0m |          -0.0438 |         231.3723 |        -233.4154 |
[32m[20221208 14:08:50 @agent_ppo2.py:179][0m |          -0.0469 |         231.1612 |        -238.8592 |
[32m[20221208 14:08:50 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 899.36
[32m[20221208 14:08:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 958.34
[32m[20221208 14:08:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 903.93
[32m[20221208 14:08:50 @agent_ppo2.py:137][0m Total time:      23.49 min
[32m[20221208 14:08:50 @agent_ppo2.py:139][0m 1925120 total steps have happened
[32m[20221208 14:08:50 @agent_ppo2.py:115][0m #------------------------ Iteration 940 --------------------------#
[32m[20221208 14:08:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |           0.0718 |         240.9338 |        -193.9162 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |           0.0776 |         235.5317 |        -153.0689 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |           0.0343 |         232.7314 |        -173.1331 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |           0.0051 |         229.6353 |        -208.7888 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |          -0.0132 |         227.9583 |        -219.5268 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |          -0.0204 |         228.0982 |        -224.1701 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |          -0.0280 |         225.6024 |        -228.5491 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |          -0.0365 |         223.9808 |        -236.4314 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |          -0.0397 |         223.2269 |        -238.1289 |
[32m[20221208 14:08:51 @agent_ppo2.py:179][0m |          -0.0358 |         222.8652 |        -237.8320 |
[32m[20221208 14:08:51 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:08:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 887.49
[32m[20221208 14:08:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 904.45
[32m[20221208 14:08:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 843.14
[32m[20221208 14:08:52 @agent_ppo2.py:137][0m Total time:      23.52 min
[32m[20221208 14:08:52 @agent_ppo2.py:139][0m 1927168 total steps have happened
[32m[20221208 14:08:52 @agent_ppo2.py:115][0m #------------------------ Iteration 941 --------------------------#
[32m[20221208 14:08:52 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:08:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:52 @agent_ppo2.py:179][0m |           0.0534 |         192.1262 |        -176.3912 |
[32m[20221208 14:08:52 @agent_ppo2.py:179][0m |           0.0067 |         182.4535 |        -152.8033 |
[32m[20221208 14:08:52 @agent_ppo2.py:179][0m |          -0.0049 |         179.4937 |        -150.2423 |
[32m[20221208 14:08:52 @agent_ppo2.py:179][0m |          -0.0380 |         178.1149 |        -162.4129 |
[32m[20221208 14:08:52 @agent_ppo2.py:179][0m |          -0.0569 |         175.9749 |        -175.6893 |
[32m[20221208 14:08:52 @agent_ppo2.py:179][0m |          -0.0576 |         174.9656 |        -179.7508 |
[32m[20221208 14:08:52 @agent_ppo2.py:179][0m |          -0.0664 |         173.0095 |        -189.3169 |
[32m[20221208 14:08:53 @agent_ppo2.py:179][0m |          -0.0693 |         172.8504 |        -190.9342 |
[32m[20221208 14:08:53 @agent_ppo2.py:179][0m |          -0.0732 |         170.8153 |        -196.5195 |
[32m[20221208 14:08:53 @agent_ppo2.py:179][0m |          -0.0770 |         170.6303 |        -200.6351 |
[32m[20221208 14:08:53 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 644.49
[32m[20221208 14:08:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 929.70
[32m[20221208 14:08:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 640.27
[32m[20221208 14:08:53 @agent_ppo2.py:137][0m Total time:      23.54 min
[32m[20221208 14:08:53 @agent_ppo2.py:139][0m 1929216 total steps have happened
[32m[20221208 14:08:53 @agent_ppo2.py:115][0m #------------------------ Iteration 942 --------------------------#
[32m[20221208 14:08:53 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |           0.0721 |         227.9847 |        -188.2910 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |           0.0570 |         215.7122 |        -155.0991 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |           0.0484 |         212.7387 |        -176.9543 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |           0.0063 |         208.3296 |        -193.4858 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |          -0.0156 |         206.2999 |        -213.8508 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |          -0.0164 |         205.3375 |        -210.2257 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |          -0.0321 |         204.4621 |        -223.0935 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |          -0.0379 |         204.0250 |        -226.0783 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |          -0.0408 |         202.7886 |        -232.8240 |
[32m[20221208 14:08:54 @agent_ppo2.py:179][0m |          -0.0408 |         202.5548 |        -229.6835 |
[32m[20221208 14:08:54 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.20
[32m[20221208 14:08:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.66
[32m[20221208 14:08:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.14
[32m[20221208 14:08:55 @agent_ppo2.py:137][0m Total time:      23.57 min
[32m[20221208 14:08:55 @agent_ppo2.py:139][0m 1931264 total steps have happened
[32m[20221208 14:08:55 @agent_ppo2.py:115][0m #------------------------ Iteration 943 --------------------------#
[32m[20221208 14:08:55 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |           0.0417 |         242.1454 |        -207.2417 |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |           0.0669 |         234.5660 |        -170.0350 |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |           0.0094 |         232.5125 |        -188.4223 |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |          -0.0112 |         230.4136 |        -206.2153 |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |          -0.0275 |         231.5114 |        -221.8292 |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |          -0.0338 |         228.9636 |        -220.9481 |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |          -0.0400 |         228.1535 |        -224.6133 |
[32m[20221208 14:08:55 @agent_ppo2.py:179][0m |          -0.0452 |         227.4576 |        -229.4855 |
[32m[20221208 14:08:56 @agent_ppo2.py:179][0m |          -0.0454 |         228.3198 |        -231.6361 |
[32m[20221208 14:08:56 @agent_ppo2.py:179][0m |          -0.0487 |         227.7459 |        -233.2817 |
[32m[20221208 14:08:56 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:08:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 905.75
[32m[20221208 14:08:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.73
[32m[20221208 14:08:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 891.37
[32m[20221208 14:08:56 @agent_ppo2.py:137][0m Total time:      23.59 min
[32m[20221208 14:08:56 @agent_ppo2.py:139][0m 1933312 total steps have happened
[32m[20221208 14:08:56 @agent_ppo2.py:115][0m #------------------------ Iteration 944 --------------------------#
[32m[20221208 14:08:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |           0.0640 |         246.2519 |        -193.9459 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |           0.1356 |         236.4242 |        -126.9238 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |           0.0397 |         228.9281 |        -148.9401 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |          -0.0039 |         224.8224 |        -184.7214 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |          -0.0168 |         222.5746 |        -193.9920 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |          -0.0290 |         220.5859 |        -203.8942 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |          -0.0328 |         218.6335 |        -205.2091 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |          -0.0313 |         217.8995 |        -204.8128 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |          -0.0387 |         216.0270 |        -211.9168 |
[32m[20221208 14:08:57 @agent_ppo2.py:179][0m |          -0.0451 |         215.9701 |        -217.9462 |
[32m[20221208 14:08:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:08:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.28
[32m[20221208 14:08:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.16
[32m[20221208 14:08:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 769.36
[32m[20221208 14:08:57 @agent_ppo2.py:137][0m Total time:      23.62 min
[32m[20221208 14:08:57 @agent_ppo2.py:139][0m 1935360 total steps have happened
[32m[20221208 14:08:57 @agent_ppo2.py:115][0m #------------------------ Iteration 945 --------------------------#
[32m[20221208 14:08:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |           0.0635 |         245.8913 |        -176.5767 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |           0.0510 |         238.4002 |        -135.5189 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |           0.0185 |         233.5063 |        -171.6546 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |          -0.0104 |         234.0681 |        -186.6771 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |          -0.0143 |         234.1243 |        -194.1113 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |          -0.0261 |         230.4187 |        -199.8383 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |          -0.0357 |         230.5951 |        -209.1733 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |          -0.0386 |         230.3146 |        -211.0048 |
[32m[20221208 14:08:58 @agent_ppo2.py:179][0m |          -0.0432 |         228.0861 |        -215.9578 |
[32m[20221208 14:08:59 @agent_ppo2.py:179][0m |          -0.0462 |         227.8629 |        -221.6814 |
[32m[20221208 14:08:59 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:08:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.39
[32m[20221208 14:08:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.87
[32m[20221208 14:08:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 639.79
[32m[20221208 14:08:59 @agent_ppo2.py:137][0m Total time:      23.64 min
[32m[20221208 14:08:59 @agent_ppo2.py:139][0m 1937408 total steps have happened
[32m[20221208 14:08:59 @agent_ppo2.py:115][0m #------------------------ Iteration 946 --------------------------#
[32m[20221208 14:08:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:08:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:08:59 @agent_ppo2.py:179][0m |           0.0531 |         243.0263 |        -176.0581 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |           0.1313 |         233.3635 |        -134.5139 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |           0.0181 |         229.6077 |        -159.3269 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |          -0.0057 |         226.8185 |        -183.3147 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |          -0.0183 |         225.1447 |        -196.2389 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |          -0.0267 |         223.5710 |        -202.6387 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |          -0.0387 |         222.5009 |        -211.0846 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |          -0.0403 |         222.1758 |        -216.6547 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |          -0.0421 |         220.9258 |        -219.8830 |
[32m[20221208 14:09:00 @agent_ppo2.py:179][0m |          -0.0394 |         220.3521 |        -220.2163 |
[32m[20221208 14:09:00 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 897.73
[32m[20221208 14:09:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 937.66
[32m[20221208 14:09:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 686.44
[32m[20221208 14:09:00 @agent_ppo2.py:137][0m Total time:      23.67 min
[32m[20221208 14:09:00 @agent_ppo2.py:139][0m 1939456 total steps have happened
[32m[20221208 14:09:00 @agent_ppo2.py:115][0m #------------------------ Iteration 947 --------------------------#
[32m[20221208 14:09:01 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |           0.0883 |         244.1419 |        -166.1874 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |           0.0893 |         232.4196 |        -101.2331 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |           0.0192 |         225.4860 |        -159.5660 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |          -0.0077 |         223.4621 |        -184.5746 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |          -0.0247 |         222.2635 |        -190.9057 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |          -0.0326 |         220.6027 |        -193.9184 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |          -0.0405 |         221.0888 |        -197.5312 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |          -0.0328 |         219.6890 |        -197.5316 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |          -0.0444 |         219.6283 |        -205.5213 |
[32m[20221208 14:09:01 @agent_ppo2.py:179][0m |          -0.0500 |         219.6816 |        -210.3045 |
[32m[20221208 14:09:01 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 853.69
[32m[20221208 14:09:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.44
[32m[20221208 14:09:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 691.82
[32m[20221208 14:09:02 @agent_ppo2.py:137][0m Total time:      23.69 min
[32m[20221208 14:09:02 @agent_ppo2.py:139][0m 1941504 total steps have happened
[32m[20221208 14:09:02 @agent_ppo2.py:115][0m #------------------------ Iteration 948 --------------------------#
[32m[20221208 14:09:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:02 @agent_ppo2.py:179][0m |           0.0833 |         246.0212 |        -189.2099 |
[32m[20221208 14:09:02 @agent_ppo2.py:179][0m |           0.0349 |         239.0756 |        -148.7466 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |           0.0081 |         233.3622 |        -180.6970 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |          -0.0118 |         231.0610 |        -193.6774 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |          -0.0190 |         230.5820 |        -203.0701 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |          -0.0198 |         229.4799 |        -200.0632 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |          -0.0312 |         227.8725 |        -208.5246 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |          -0.0298 |         228.0479 |        -211.8043 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |          -0.0409 |         225.7136 |        -218.6630 |
[32m[20221208 14:09:03 @agent_ppo2.py:179][0m |          -0.0451 |         225.0033 |        -226.1139 |
[32m[20221208 14:09:03 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 941.71
[32m[20221208 14:09:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 957.79
[32m[20221208 14:09:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 747.91
[32m[20221208 14:09:03 @agent_ppo2.py:137][0m Total time:      23.72 min
[32m[20221208 14:09:03 @agent_ppo2.py:139][0m 1943552 total steps have happened
[32m[20221208 14:09:03 @agent_ppo2.py:115][0m #------------------------ Iteration 949 --------------------------#
[32m[20221208 14:09:04 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |           0.0745 |         242.2892 |        -186.1194 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |           0.0353 |         234.1949 |        -167.8477 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |           0.0053 |         230.2134 |        -177.3158 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |          -0.0147 |         227.4801 |        -193.3270 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |          -0.0275 |         226.2050 |        -202.1923 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |          -0.0332 |         224.4205 |        -204.7874 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |          -0.0396 |         223.6690 |        -213.5682 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |          -0.0433 |         222.9833 |        -213.6535 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |          -0.0468 |         222.9716 |        -219.1138 |
[32m[20221208 14:09:04 @agent_ppo2.py:179][0m |          -0.0483 |         222.6869 |        -223.2518 |
[32m[20221208 14:09:04 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.66
[32m[20221208 14:09:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.67
[32m[20221208 14:09:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.31
[32m[20221208 14:09:05 @agent_ppo2.py:137][0m Total time:      23.74 min
[32m[20221208 14:09:05 @agent_ppo2.py:139][0m 1945600 total steps have happened
[32m[20221208 14:09:05 @agent_ppo2.py:115][0m #------------------------ Iteration 950 --------------------------#
[32m[20221208 14:09:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:05 @agent_ppo2.py:179][0m |           0.0881 |         223.7889 |        -174.3520 |
[32m[20221208 14:09:05 @agent_ppo2.py:179][0m |           0.0701 |         210.3212 |        -107.5432 |
[32m[20221208 14:09:05 @agent_ppo2.py:179][0m |           0.0261 |         202.6028 |        -141.0381 |
[32m[20221208 14:09:06 @agent_ppo2.py:179][0m |           0.0009 |         197.7782 |        -165.9626 |
[32m[20221208 14:09:06 @agent_ppo2.py:179][0m |          -0.0138 |         194.4428 |        -180.9438 |
[32m[20221208 14:09:06 @agent_ppo2.py:179][0m |          -0.0245 |         191.0573 |        -187.8507 |
[32m[20221208 14:09:06 @agent_ppo2.py:179][0m |          -0.0254 |         188.3790 |        -193.6184 |
[32m[20221208 14:09:06 @agent_ppo2.py:179][0m |          -0.0336 |         186.1877 |        -199.9013 |
[32m[20221208 14:09:06 @agent_ppo2.py:179][0m |          -0.0401 |         182.5378 |        -204.2845 |
[32m[20221208 14:09:06 @agent_ppo2.py:179][0m |          -0.0424 |         180.2539 |        -205.9523 |
[32m[20221208 14:09:06 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 844.43
[32m[20221208 14:09:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 917.23
[32m[20221208 14:09:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.57
[32m[20221208 14:09:06 @agent_ppo2.py:137][0m Total time:      23.77 min
[32m[20221208 14:09:06 @agent_ppo2.py:139][0m 1947648 total steps have happened
[32m[20221208 14:09:06 @agent_ppo2.py:115][0m #------------------------ Iteration 951 --------------------------#
[32m[20221208 14:09:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |           0.0805 |         247.3935 |        -183.1427 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |           0.0226 |         240.7760 |        -195.1688 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0085 |         234.4145 |        -213.0469 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0187 |         233.8487 |        -216.8707 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0228 |         231.8605 |        -220.7943 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0383 |         230.4249 |        -229.5431 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0419 |         229.4002 |        -237.3914 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0439 |         228.3266 |        -236.6909 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0471 |         228.2520 |        -241.8178 |
[32m[20221208 14:09:07 @agent_ppo2.py:179][0m |          -0.0493 |         227.1722 |        -246.2539 |
[32m[20221208 14:09:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:09:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 871.08
[32m[20221208 14:09:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 878.09
[32m[20221208 14:09:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 917.59
[32m[20221208 14:09:08 @agent_ppo2.py:137][0m Total time:      23.79 min
[32m[20221208 14:09:08 @agent_ppo2.py:139][0m 1949696 total steps have happened
[32m[20221208 14:09:08 @agent_ppo2.py:115][0m #------------------------ Iteration 952 --------------------------#
[32m[20221208 14:09:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:08 @agent_ppo2.py:179][0m |           0.0753 |         217.1145 |        -209.6014 |
[32m[20221208 14:09:08 @agent_ppo2.py:179][0m |           0.0773 |         197.9940 |        -142.3073 |
[32m[20221208 14:09:08 @agent_ppo2.py:179][0m |           0.0439 |         187.7864 |        -142.7019 |
[32m[20221208 14:09:08 @agent_ppo2.py:179][0m |           0.0057 |         178.5548 |        -187.1546 |
[32m[20221208 14:09:09 @agent_ppo2.py:179][0m |          -0.0115 |         171.2961 |        -197.2699 |
[32m[20221208 14:09:09 @agent_ppo2.py:179][0m |          -0.0216 |         166.9799 |        -207.8643 |
[32m[20221208 14:09:09 @agent_ppo2.py:179][0m |          -0.0280 |         162.6202 |        -214.4293 |
[32m[20221208 14:09:09 @agent_ppo2.py:179][0m |          -0.0293 |         158.3746 |        -218.0236 |
[32m[20221208 14:09:09 @agent_ppo2.py:179][0m |          -0.0332 |         154.0962 |        -221.6642 |
[32m[20221208 14:09:09 @agent_ppo2.py:179][0m |          -0.0367 |         151.5983 |        -228.4515 |
[32m[20221208 14:09:09 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.70
[32m[20221208 14:09:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.08
[32m[20221208 14:09:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 670.40
[32m[20221208 14:09:09 @agent_ppo2.py:137][0m Total time:      23.81 min
[32m[20221208 14:09:09 @agent_ppo2.py:139][0m 1951744 total steps have happened
[32m[20221208 14:09:09 @agent_ppo2.py:115][0m #------------------------ Iteration 953 --------------------------#
[32m[20221208 14:09:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:09:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |           0.0810 |         252.3923 |        -184.3781 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |           0.0438 |         246.3081 |        -170.3729 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |           0.0146 |         241.5550 |        -191.3601 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |          -0.0135 |         237.7739 |        -204.0957 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |          -0.0263 |         235.0474 |        -216.8648 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |          -0.0353 |         232.3638 |        -220.8612 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |          -0.0400 |         230.6326 |        -226.2861 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |          -0.0381 |         228.3841 |        -232.3101 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |          -0.0403 |         227.7954 |        -232.3964 |
[32m[20221208 14:09:10 @agent_ppo2.py:179][0m |          -0.0459 |         225.5094 |        -241.7206 |
[32m[20221208 14:09:10 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 900.52
[32m[20221208 14:09:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 928.10
[32m[20221208 14:09:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 675.08
[32m[20221208 14:09:11 @agent_ppo2.py:137][0m Total time:      23.84 min
[32m[20221208 14:09:11 @agent_ppo2.py:139][0m 1953792 total steps have happened
[32m[20221208 14:09:11 @agent_ppo2.py:115][0m #------------------------ Iteration 954 --------------------------#
[32m[20221208 14:09:11 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:11 @agent_ppo2.py:179][0m |           0.0730 |         242.8861 |        -201.1017 |
[32m[20221208 14:09:11 @agent_ppo2.py:179][0m |           0.1055 |         229.7627 |        -109.3370 |
[32m[20221208 14:09:11 @agent_ppo2.py:179][0m |           0.0607 |         226.8246 |        -125.4897 |
[32m[20221208 14:09:11 @agent_ppo2.py:179][0m |           0.0183 |         225.2570 |        -171.0590 |
[32m[20221208 14:09:11 @agent_ppo2.py:179][0m |          -0.0035 |         225.4253 |        -198.1514 |
[32m[20221208 14:09:12 @agent_ppo2.py:179][0m |          -0.0189 |         222.8486 |        -209.6260 |
[32m[20221208 14:09:12 @agent_ppo2.py:179][0m |          -0.0264 |         221.9776 |        -219.0028 |
[32m[20221208 14:09:12 @agent_ppo2.py:179][0m |          -0.0360 |         222.8957 |        -227.0748 |
[32m[20221208 14:09:12 @agent_ppo2.py:179][0m |          -0.0398 |         220.7100 |        -234.1380 |
[32m[20221208 14:09:12 @agent_ppo2.py:179][0m |          -0.0421 |         221.0284 |        -237.4515 |
[32m[20221208 14:09:12 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 897.32
[32m[20221208 14:09:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.11
[32m[20221208 14:09:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 645.49
[32m[20221208 14:09:12 @agent_ppo2.py:137][0m Total time:      23.86 min
[32m[20221208 14:09:12 @agent_ppo2.py:139][0m 1955840 total steps have happened
[32m[20221208 14:09:12 @agent_ppo2.py:115][0m #------------------------ Iteration 955 --------------------------#
[32m[20221208 14:09:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |           0.1142 |         245.0639 |        -182.0475 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |           0.0802 |         240.6620 |        -117.0316 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |           0.0263 |         236.9109 |        -157.8923 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |           0.0099 |         236.0785 |        -182.9206 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |          -0.0085 |         235.4401 |        -195.7810 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |          -0.0233 |         232.9965 |        -209.5286 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |          -0.0291 |         232.2268 |        -214.1441 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |          -0.0369 |         232.2762 |        -221.0835 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |          -0.0431 |         230.9435 |        -226.6999 |
[32m[20221208 14:09:13 @agent_ppo2.py:179][0m |          -0.0444 |         230.3829 |        -229.8841 |
[32m[20221208 14:09:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:09:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 964.80
[32m[20221208 14:09:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.23
[32m[20221208 14:09:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 895.15
[32m[20221208 14:09:14 @agent_ppo2.py:137][0m Total time:      23.89 min
[32m[20221208 14:09:14 @agent_ppo2.py:139][0m 1957888 total steps have happened
[32m[20221208 14:09:14 @agent_ppo2.py:115][0m #------------------------ Iteration 956 --------------------------#
[32m[20221208 14:09:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:14 @agent_ppo2.py:179][0m |           0.0521 |         245.8369 |        -189.2387 |
[32m[20221208 14:09:14 @agent_ppo2.py:179][0m |           0.0281 |         239.1487 |        -178.2095 |
[32m[20221208 14:09:14 @agent_ppo2.py:179][0m |           0.0099 |         236.1219 |        -180.4081 |
[32m[20221208 14:09:14 @agent_ppo2.py:179][0m |           0.0003 |         235.0639 |        -181.5295 |
[32m[20221208 14:09:14 @agent_ppo2.py:179][0m |          -0.0129 |         234.4256 |        -188.1761 |
[32m[20221208 14:09:14 @agent_ppo2.py:179][0m |          -0.0270 |         233.1625 |        -198.3753 |
[32m[20221208 14:09:15 @agent_ppo2.py:179][0m |          -0.0372 |         232.7719 |        -207.0570 |
[32m[20221208 14:09:15 @agent_ppo2.py:179][0m |          -0.0382 |         232.0447 |        -212.0795 |
[32m[20221208 14:09:15 @agent_ppo2.py:179][0m |          -0.0375 |         231.3226 |        -213.9433 |
[32m[20221208 14:09:15 @agent_ppo2.py:179][0m |          -0.0399 |         231.2301 |        -217.8731 |
[32m[20221208 14:09:15 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.81
[32m[20221208 14:09:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.15
[32m[20221208 14:09:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.80
[32m[20221208 14:09:15 @agent_ppo2.py:137][0m Total time:      23.91 min
[32m[20221208 14:09:15 @agent_ppo2.py:139][0m 1959936 total steps have happened
[32m[20221208 14:09:15 @agent_ppo2.py:115][0m #------------------------ Iteration 957 --------------------------#
[32m[20221208 14:09:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |           0.0698 |         239.6819 |        -180.3796 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |           0.0546 |         234.9912 |        -135.4841 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |           0.0101 |         231.1384 |        -179.6863 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |          -0.0089 |         228.6811 |        -196.8125 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |          -0.0207 |         226.8095 |        -206.0534 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |          -0.0293 |         225.2863 |        -217.4602 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |          -0.0342 |         222.1658 |        -220.8482 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |          -0.0307 |         221.4862 |        -220.7005 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |          -0.0324 |         218.9245 |        -217.1953 |
[32m[20221208 14:09:16 @agent_ppo2.py:179][0m |          -0.0393 |         217.8792 |        -226.4425 |
[32m[20221208 14:09:16 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 954.90
[32m[20221208 14:09:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.17
[32m[20221208 14:09:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.94
[32m[20221208 14:09:17 @agent_ppo2.py:137][0m Total time:      23.94 min
[32m[20221208 14:09:17 @agent_ppo2.py:139][0m 1961984 total steps have happened
[32m[20221208 14:09:17 @agent_ppo2.py:115][0m #------------------------ Iteration 958 --------------------------#
[32m[20221208 14:09:17 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:17 @agent_ppo2.py:179][0m |           0.3030 |         203.4655 |        -156.0093 |
[32m[20221208 14:09:17 @agent_ppo2.py:179][0m |          -0.0099 |         190.9924 |        -144.8122 |
[32m[20221208 14:09:17 @agent_ppo2.py:179][0m |          -0.0361 |         186.5697 |        -160.6189 |
[32m[20221208 14:09:17 @agent_ppo2.py:179][0m |          -0.0456 |         183.4144 |        -165.9355 |
[32m[20221208 14:09:17 @agent_ppo2.py:179][0m |          -0.0548 |         180.9125 |        -171.0735 |
[32m[20221208 14:09:17 @agent_ppo2.py:179][0m |          -0.0660 |         178.9820 |        -179.3846 |
[32m[20221208 14:09:17 @agent_ppo2.py:179][0m |          -0.0718 |         179.2083 |        -182.9573 |
[32m[20221208 14:09:18 @agent_ppo2.py:179][0m |          -0.0748 |         177.2037 |        -190.5541 |
[32m[20221208 14:09:18 @agent_ppo2.py:179][0m |          -0.0702 |         175.5246 |        -190.9835 |
[32m[20221208 14:09:18 @agent_ppo2.py:179][0m |          -0.0729 |         174.5938 |        -194.2958 |
[32m[20221208 14:09:18 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 669.07
[32m[20221208 14:09:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.81
[32m[20221208 14:09:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 644.00
[32m[20221208 14:09:18 @agent_ppo2.py:137][0m Total time:      23.96 min
[32m[20221208 14:09:18 @agent_ppo2.py:139][0m 1964032 total steps have happened
[32m[20221208 14:09:18 @agent_ppo2.py:115][0m #------------------------ Iteration 959 --------------------------#
[32m[20221208 14:09:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |           0.0636 |         243.9976 |        -182.1192 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |           0.0175 |         230.2315 |        -190.0601 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |           0.0053 |         222.3785 |        -219.7103 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |          -0.0030 |         217.5057 |        -203.0892 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |          -0.0227 |         212.2565 |        -228.3484 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |          -0.0325 |         210.7698 |        -240.9353 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |          -0.0371 |         208.0018 |        -249.9463 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |          -0.0394 |         205.0234 |        -253.3031 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |          -0.0432 |         202.6329 |        -256.6419 |
[32m[20221208 14:09:19 @agent_ppo2.py:179][0m |          -0.0445 |         201.1036 |        -261.9238 |
[32m[20221208 14:09:19 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 947.82
[32m[20221208 14:09:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.30
[32m[20221208 14:09:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 649.28
[32m[20221208 14:09:20 @agent_ppo2.py:137][0m Total time:      23.99 min
[32m[20221208 14:09:20 @agent_ppo2.py:139][0m 1966080 total steps have happened
[32m[20221208 14:09:20 @agent_ppo2.py:115][0m #------------------------ Iteration 960 --------------------------#
[32m[20221208 14:09:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |           0.0879 |         251.5348 |        -182.5471 |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |           0.0642 |         238.3137 |        -149.0238 |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |           0.0219 |         233.6510 |        -193.9888 |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |           0.0025 |         231.4416 |        -207.1471 |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |           0.0148 |         229.5853 |        -208.5572 |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |          -0.0023 |         228.8162 |        -214.3302 |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |          -0.0260 |         227.9898 |        -237.5689 |
[32m[20221208 14:09:20 @agent_ppo2.py:179][0m |          -0.0359 |         226.7429 |        -243.0227 |
[32m[20221208 14:09:21 @agent_ppo2.py:179][0m |          -0.0406 |         226.9266 |        -251.1623 |
[32m[20221208 14:09:21 @agent_ppo2.py:179][0m |          -0.0426 |         227.2552 |        -255.2467 |
[32m[20221208 14:09:21 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 885.36
[32m[20221208 14:09:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 919.80
[32m[20221208 14:09:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.71
[32m[20221208 14:09:21 @agent_ppo2.py:137][0m Total time:      24.01 min
[32m[20221208 14:09:21 @agent_ppo2.py:139][0m 1968128 total steps have happened
[32m[20221208 14:09:21 @agent_ppo2.py:115][0m #------------------------ Iteration 961 --------------------------#
[32m[20221208 14:09:21 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:09:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |           0.0587 |         228.8086 |        -205.2941 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |           0.0347 |         210.2053 |        -174.0054 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |           0.0087 |         198.5624 |        -205.6212 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |          -0.0119 |         192.0350 |        -224.8167 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |          -0.0225 |         189.3762 |        -238.5535 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |          -0.0202 |         188.1961 |        -238.2009 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |          -0.0236 |         186.3554 |        -236.2123 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |          -0.0294 |         185.0614 |        -244.3386 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |          -0.0358 |         182.9425 |        -251.3241 |
[32m[20221208 14:09:22 @agent_ppo2.py:179][0m |          -0.0340 |         181.5544 |        -251.6761 |
[32m[20221208 14:09:22 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 984.02
[32m[20221208 14:09:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.73
[32m[20221208 14:09:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.22
[32m[20221208 14:09:22 @agent_ppo2.py:137][0m Total time:      24.04 min
[32m[20221208 14:09:22 @agent_ppo2.py:139][0m 1970176 total steps have happened
[32m[20221208 14:09:22 @agent_ppo2.py:115][0m #------------------------ Iteration 962 --------------------------#
[32m[20221208 14:09:23 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |           0.0489 |         250.7809 |        -214.2385 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |           0.0725 |         243.5052 |        -167.0307 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |           0.0495 |         241.0455 |        -150.6799 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |          -0.0004 |         238.9799 |        -185.6485 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |          -0.0175 |         237.1165 |        -203.9187 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |          -0.0302 |         236.5520 |        -218.7892 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |          -0.0375 |         236.2974 |        -227.7093 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |          -0.0431 |         234.9169 |        -234.1453 |
[32m[20221208 14:09:23 @agent_ppo2.py:179][0m |          -0.0431 |         233.4428 |        -234.4660 |
[32m[20221208 14:09:24 @agent_ppo2.py:179][0m |          -0.0474 |         232.2597 |        -241.0922 |
[32m[20221208 14:09:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:09:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 903.83
[32m[20221208 14:09:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.53
[32m[20221208 14:09:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 916.07
[32m[20221208 14:09:24 @agent_ppo2.py:137][0m Total time:      24.06 min
[32m[20221208 14:09:24 @agent_ppo2.py:139][0m 1972224 total steps have happened
[32m[20221208 14:09:24 @agent_ppo2.py:115][0m #------------------------ Iteration 963 --------------------------#
[32m[20221208 14:09:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:24 @agent_ppo2.py:179][0m |           0.0928 |         202.5479 |        -192.8468 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |           0.0976 |         193.6802 |        -120.1761 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |           0.0097 |         190.5696 |        -140.3951 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |          -0.0222 |         188.6229 |        -150.3165 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |          -0.0389 |         186.4788 |        -157.7584 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |          -0.0471 |         186.4856 |        -164.4953 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |          -0.0556 |         184.6963 |        -171.6474 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |          -0.0505 |         182.8883 |        -175.8421 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |          -0.0610 |         182.0539 |        -180.5722 |
[32m[20221208 14:09:25 @agent_ppo2.py:179][0m |          -0.0608 |         181.7527 |        -185.0960 |
[32m[20221208 14:09:25 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 659.73
[32m[20221208 14:09:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 940.38
[32m[20221208 14:09:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 979.28
[32m[20221208 14:09:25 @agent_ppo2.py:137][0m Total time:      24.08 min
[32m[20221208 14:09:25 @agent_ppo2.py:139][0m 1974272 total steps have happened
[32m[20221208 14:09:25 @agent_ppo2.py:115][0m #------------------------ Iteration 964 --------------------------#
[32m[20221208 14:09:26 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |           0.0618 |         245.9728 |        -177.8449 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |           0.0550 |         238.0648 |        -133.2989 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |           0.0187 |         234.8337 |        -181.6713 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |          -0.0042 |         234.0450 |        -200.5504 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |          -0.0082 |         232.7363 |        -208.7127 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |          -0.0234 |         232.3240 |        -220.3120 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |          -0.0324 |         230.8818 |        -228.5928 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |          -0.0386 |         231.6684 |        -235.6270 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |          -0.0374 |         231.3151 |        -235.8388 |
[32m[20221208 14:09:26 @agent_ppo2.py:179][0m |          -0.0382 |         232.9054 |        -238.9500 |
[32m[20221208 14:09:26 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:27 @agent_ppo2.py:132][0m Average TRAINING episode reward: 954.42
[32m[20221208 14:09:27 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.94
[32m[20221208 14:09:27 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 642.39
[32m[20221208 14:09:27 @agent_ppo2.py:137][0m Total time:      24.11 min
[32m[20221208 14:09:27 @agent_ppo2.py:139][0m 1976320 total steps have happened
[32m[20221208 14:09:27 @agent_ppo2.py:115][0m #------------------------ Iteration 965 --------------------------#
[32m[20221208 14:09:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:27 @agent_ppo2.py:179][0m |           0.1031 |         242.3880 |        -153.4587 |
[32m[20221208 14:09:27 @agent_ppo2.py:179][0m |           0.0944 |         235.6734 |         -94.5359 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |           0.0481 |         233.8697 |        -147.9334 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |           0.0190 |         233.0835 |        -174.7232 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |          -0.0076 |         231.6276 |        -199.3440 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |          -0.0208 |         230.2068 |        -213.0744 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |          -0.0259 |         230.5982 |        -217.1697 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |          -0.0242 |         228.9303 |        -219.7870 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |          -0.0362 |         228.9754 |        -228.1370 |
[32m[20221208 14:09:28 @agent_ppo2.py:179][0m |          -0.0396 |         228.9101 |        -234.1831 |
[32m[20221208 14:09:28 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.06
[32m[20221208 14:09:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.57
[32m[20221208 14:09:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 382.84
[32m[20221208 14:09:28 @agent_ppo2.py:137][0m Total time:      24.13 min
[32m[20221208 14:09:28 @agent_ppo2.py:139][0m 1978368 total steps have happened
[32m[20221208 14:09:28 @agent_ppo2.py:115][0m #------------------------ Iteration 966 --------------------------#
[32m[20221208 14:09:29 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |           0.0906 |         198.8376 |        -151.9169 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |           0.0664 |         186.1395 |        -100.7642 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |           0.0105 |         182.2141 |        -130.6516 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |          -0.0253 |         180.3798 |        -143.0138 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |          -0.0453 |         179.3378 |        -156.2807 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |          -0.0569 |         178.7818 |        -161.3214 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |          -0.0639 |         175.7858 |        -167.2872 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |          -0.0674 |         173.6969 |        -171.1891 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |          -0.0644 |         172.8253 |        -169.9526 |
[32m[20221208 14:09:29 @agent_ppo2.py:179][0m |          -0.0674 |         171.3131 |        -177.9167 |
[32m[20221208 14:09:29 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:09:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 659.83
[32m[20221208 14:09:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.45
[32m[20221208 14:09:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 881.06
[32m[20221208 14:09:30 @agent_ppo2.py:137][0m Total time:      24.16 min
[32m[20221208 14:09:30 @agent_ppo2.py:139][0m 1980416 total steps have happened
[32m[20221208 14:09:30 @agent_ppo2.py:115][0m #------------------------ Iteration 967 --------------------------#
[32m[20221208 14:09:30 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:30 @agent_ppo2.py:179][0m |           0.0660 |         189.4779 |        -181.8071 |
[32m[20221208 14:09:30 @agent_ppo2.py:179][0m |           0.0142 |         183.7694 |        -153.5012 |
[32m[20221208 14:09:30 @agent_ppo2.py:179][0m |          -0.0233 |         181.2842 |        -169.1964 |
[32m[20221208 14:09:31 @agent_ppo2.py:179][0m |          -0.0412 |         179.9057 |        -184.9294 |
[32m[20221208 14:09:31 @agent_ppo2.py:179][0m |          -0.0506 |         178.2484 |        -192.1211 |
[32m[20221208 14:09:31 @agent_ppo2.py:179][0m |          -0.0634 |         177.9389 |        -201.9531 |
[32m[20221208 14:09:31 @agent_ppo2.py:179][0m |          -0.0626 |         177.1258 |        -204.8995 |
[32m[20221208 14:09:31 @agent_ppo2.py:179][0m |          -0.0662 |         175.9653 |        -208.6380 |
[32m[20221208 14:09:31 @agent_ppo2.py:179][0m |          -0.0712 |         176.6147 |        -217.2014 |
[32m[20221208 14:09:31 @agent_ppo2.py:179][0m |          -0.0737 |         175.6851 |        -218.5568 |
[32m[20221208 14:09:31 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 642.52
[32m[20221208 14:09:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 927.16
[32m[20221208 14:09:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 348.30
[32m[20221208 14:09:31 @agent_ppo2.py:137][0m Total time:      24.18 min
[32m[20221208 14:09:31 @agent_ppo2.py:139][0m 1982464 total steps have happened
[32m[20221208 14:09:31 @agent_ppo2.py:115][0m #------------------------ Iteration 968 --------------------------#
[32m[20221208 14:09:32 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |           0.0654 |         242.4264 |        -232.2356 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |           0.1048 |         231.4197 |        -120.8601 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |           0.0644 |         225.0177 |        -128.5800 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |           0.0298 |         219.1252 |        -187.2846 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |          -0.0014 |         216.5575 |        -227.0879 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |          -0.0101 |         213.7356 |        -235.2905 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |          -0.0125 |         210.9091 |        -238.3548 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |          -0.0269 |         210.1433 |        -252.7031 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |          -0.0323 |         207.4926 |        -259.6650 |
[32m[20221208 14:09:32 @agent_ppo2.py:179][0m |          -0.0377 |         205.8102 |        -267.2728 |
[32m[20221208 14:09:32 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.85
[32m[20221208 14:09:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 956.01
[32m[20221208 14:09:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 657.63
[32m[20221208 14:09:33 @agent_ppo2.py:137][0m Total time:      24.21 min
[32m[20221208 14:09:33 @agent_ppo2.py:139][0m 1984512 total steps have happened
[32m[20221208 14:09:33 @agent_ppo2.py:115][0m #------------------------ Iteration 969 --------------------------#
[32m[20221208 14:09:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:33 @agent_ppo2.py:179][0m |           0.0978 |         256.3699 |        -246.0529 |
[32m[20221208 14:09:33 @agent_ppo2.py:179][0m |           0.1936 |         242.9810 |        -177.1305 |
[32m[20221208 14:09:33 @agent_ppo2.py:179][0m |           0.0421 |         238.5066 |        -199.1653 |
[32m[20221208 14:09:33 @agent_ppo2.py:179][0m |           0.0025 |         235.1151 |        -238.5364 |
[32m[20221208 14:09:34 @agent_ppo2.py:179][0m |          -0.0173 |         234.3537 |        -255.5830 |
[32m[20221208 14:09:34 @agent_ppo2.py:179][0m |          -0.0300 |         232.7393 |        -269.9705 |
[32m[20221208 14:09:34 @agent_ppo2.py:179][0m |          -0.0366 |         231.3044 |        -273.7989 |
[32m[20221208 14:09:34 @agent_ppo2.py:179][0m |          -0.0295 |         230.0365 |        -276.3864 |
[32m[20221208 14:09:34 @agent_ppo2.py:179][0m |          -0.0290 |         229.8000 |        -275.0907 |
[32m[20221208 14:09:34 @agent_ppo2.py:179][0m |          -0.0417 |         228.5994 |        -286.2343 |
[32m[20221208 14:09:34 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.07
[32m[20221208 14:09:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.66
[32m[20221208 14:09:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 631.43
[32m[20221208 14:09:34 @agent_ppo2.py:137][0m Total time:      24.23 min
[32m[20221208 14:09:34 @agent_ppo2.py:139][0m 1986560 total steps have happened
[32m[20221208 14:09:34 @agent_ppo2.py:115][0m #------------------------ Iteration 970 --------------------------#
[32m[20221208 14:09:35 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |           0.0663 |         246.5131 |        -221.8575 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |           0.0597 |         235.8224 |        -186.4764 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |           0.0214 |         229.2949 |        -210.9268 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |          -0.0099 |         224.8985 |        -229.8836 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |          -0.0162 |         221.8625 |        -237.6425 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |          -0.0313 |         220.3714 |        -247.7713 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |          -0.0366 |         219.9383 |        -256.1926 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |          -0.0439 |         217.6219 |        -259.7474 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |          -0.0481 |         217.1284 |        -265.2925 |
[32m[20221208 14:09:35 @agent_ppo2.py:179][0m |          -0.0491 |         216.2074 |        -272.6719 |
[32m[20221208 14:09:35 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 882.51
[32m[20221208 14:09:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 937.85
[32m[20221208 14:09:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 678.97
[32m[20221208 14:09:36 @agent_ppo2.py:137][0m Total time:      24.26 min
[32m[20221208 14:09:36 @agent_ppo2.py:139][0m 1988608 total steps have happened
[32m[20221208 14:09:36 @agent_ppo2.py:115][0m #------------------------ Iteration 971 --------------------------#
[32m[20221208 14:09:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:36 @agent_ppo2.py:179][0m |           0.0510 |         185.3417 |        -174.2046 |
[32m[20221208 14:09:36 @agent_ppo2.py:179][0m |           0.0180 |         176.4984 |        -159.7527 |
[32m[20221208 14:09:36 @agent_ppo2.py:179][0m |          -0.0234 |         173.2400 |        -176.0123 |
[32m[20221208 14:09:36 @agent_ppo2.py:179][0m |          -0.0446 |         171.4939 |        -192.9900 |
[32m[20221208 14:09:36 @agent_ppo2.py:179][0m |          -0.0521 |         171.0793 |        -197.7474 |
[32m[20221208 14:09:37 @agent_ppo2.py:179][0m |          -0.0597 |         169.1262 |        -203.8040 |
[32m[20221208 14:09:37 @agent_ppo2.py:179][0m |          -0.0647 |         168.1287 |        -209.0537 |
[32m[20221208 14:09:37 @agent_ppo2.py:179][0m |          -0.0681 |         168.4197 |        -219.1024 |
[32m[20221208 14:09:37 @agent_ppo2.py:179][0m |          -0.0736 |         167.3638 |        -223.6343 |
[32m[20221208 14:09:37 @agent_ppo2.py:179][0m |          -0.0740 |         166.1548 |        -224.9389 |
[32m[20221208 14:09:37 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 633.93
[32m[20221208 14:09:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.38
[32m[20221208 14:09:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.31
[32m[20221208 14:09:37 @agent_ppo2.py:137][0m Total time:      24.28 min
[32m[20221208 14:09:37 @agent_ppo2.py:139][0m 1990656 total steps have happened
[32m[20221208 14:09:37 @agent_ppo2.py:115][0m #------------------------ Iteration 972 --------------------------#
[32m[20221208 14:09:38 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |           0.0903 |         203.4714 |        -231.2129 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |           0.0286 |         187.6593 |        -169.7742 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0106 |         180.7002 |        -182.3957 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0370 |         177.4430 |        -192.8851 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0511 |         175.5850 |        -205.0109 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0594 |         173.8099 |        -214.4965 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0625 |         171.9844 |        -221.3190 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0658 |         170.7847 |        -224.8559 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0712 |         170.2564 |        -231.6769 |
[32m[20221208 14:09:38 @agent_ppo2.py:179][0m |          -0.0716 |         169.5601 |        -235.1877 |
[32m[20221208 14:09:38 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 662.68
[32m[20221208 14:09:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.49
[32m[20221208 14:09:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 970.02
[32m[20221208 14:09:39 @agent_ppo2.py:137][0m Total time:      24.30 min
[32m[20221208 14:09:39 @agent_ppo2.py:139][0m 1992704 total steps have happened
[32m[20221208 14:09:39 @agent_ppo2.py:115][0m #------------------------ Iteration 973 --------------------------#
[32m[20221208 14:09:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:39 @agent_ppo2.py:179][0m |           0.1152 |         193.1987 |        -228.7542 |
[32m[20221208 14:09:39 @agent_ppo2.py:179][0m |           0.1019 |         181.0947 |        -122.0771 |
[32m[20221208 14:09:39 @agent_ppo2.py:179][0m |           0.0252 |         176.5282 |        -158.6018 |
[32m[20221208 14:09:39 @agent_ppo2.py:179][0m |          -0.0147 |         174.7215 |        -195.9761 |
[32m[20221208 14:09:39 @agent_ppo2.py:179][0m |          -0.0284 |         174.1968 |        -211.3176 |
[32m[20221208 14:09:39 @agent_ppo2.py:179][0m |          -0.0403 |         173.3777 |        -217.5909 |
[32m[20221208 14:09:40 @agent_ppo2.py:179][0m |          -0.0546 |         171.9022 |        -229.9408 |
[32m[20221208 14:09:40 @agent_ppo2.py:179][0m |          -0.0596 |         170.6387 |        -237.4636 |
[32m[20221208 14:09:40 @agent_ppo2.py:179][0m |          -0.0586 |         171.1347 |        -242.0221 |
[32m[20221208 14:09:40 @agent_ppo2.py:179][0m |          -0.0650 |         169.6391 |        -246.4335 |
[32m[20221208 14:09:40 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:09:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 664.96
[32m[20221208 14:09:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.40
[32m[20221208 14:09:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 674.59
[32m[20221208 14:09:40 @agent_ppo2.py:137][0m Total time:      24.33 min
[32m[20221208 14:09:40 @agent_ppo2.py:139][0m 1994752 total steps have happened
[32m[20221208 14:09:40 @agent_ppo2.py:115][0m #------------------------ Iteration 974 --------------------------#
[32m[20221208 14:09:41 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |           0.0857 |         255.0111 |        -292.1949 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |           0.0732 |         245.8093 |        -213.0479 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |           0.0404 |         241.7068 |        -245.6184 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |          -0.0031 |         240.9655 |        -291.9356 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |          -0.0161 |         239.6060 |        -304.4518 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |          -0.0170 |         238.0263 |        -300.2998 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |          -0.0292 |         237.5896 |        -316.0427 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |          -0.0303 |         237.1051 |        -320.4511 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |          -0.0338 |         235.5467 |        -323.3582 |
[32m[20221208 14:09:41 @agent_ppo2.py:179][0m |          -0.0380 |         236.4380 |        -327.6644 |
[32m[20221208 14:09:41 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:09:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.60
[32m[20221208 14:09:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.53
[32m[20221208 14:09:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 974.51
[32m[20221208 14:09:42 @agent_ppo2.py:137][0m Total time:      24.35 min
[32m[20221208 14:09:42 @agent_ppo2.py:139][0m 1996800 total steps have happened
[32m[20221208 14:09:42 @agent_ppo2.py:115][0m #------------------------ Iteration 975 --------------------------#
[32m[20221208 14:09:42 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:42 @agent_ppo2.py:179][0m |           0.1347 |         186.3681 |        -202.9649 |
[32m[20221208 14:09:42 @agent_ppo2.py:179][0m |           0.0456 |         180.1264 |        -160.6179 |
[32m[20221208 14:09:42 @agent_ppo2.py:179][0m |           0.0014 |         180.1759 |        -197.8081 |
[32m[20221208 14:09:42 @agent_ppo2.py:179][0m |          -0.0220 |         177.2285 |        -220.3381 |
[32m[20221208 14:09:42 @agent_ppo2.py:179][0m |          -0.0348 |         175.4028 |        -226.6256 |
[32m[20221208 14:09:42 @agent_ppo2.py:179][0m |          -0.0464 |         174.1905 |        -235.7009 |
[32m[20221208 14:09:42 @agent_ppo2.py:179][0m |          -0.0531 |         174.1935 |        -242.0579 |
[32m[20221208 14:09:43 @agent_ppo2.py:179][0m |          -0.0579 |         172.7073 |        -248.3347 |
[32m[20221208 14:09:43 @agent_ppo2.py:179][0m |          -0.0610 |         172.0729 |        -254.5258 |
[32m[20221208 14:09:43 @agent_ppo2.py:179][0m |          -0.0623 |         172.5202 |        -259.8042 |
[32m[20221208 14:09:43 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 672.47
[32m[20221208 14:09:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.90
[32m[20221208 14:09:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 683.76
[32m[20221208 14:09:43 @agent_ppo2.py:137][0m Total time:      24.38 min
[32m[20221208 14:09:43 @agent_ppo2.py:139][0m 1998848 total steps have happened
[32m[20221208 14:09:43 @agent_ppo2.py:115][0m #------------------------ Iteration 976 --------------------------#
[32m[20221208 14:09:43 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |           0.0802 |         251.5093 |        -302.4078 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |           0.0636 |         234.2266 |        -241.7465 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |           0.0178 |         224.6548 |        -277.8633 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |          -0.0067 |         218.9234 |        -316.8989 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |          -0.0177 |         215.7545 |        -342.1719 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |          -0.0263 |         214.3167 |        -348.9545 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |          -0.0311 |         212.9857 |        -363.2826 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |          -0.0342 |         209.4969 |        -368.7896 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |          -0.0382 |         207.6729 |        -374.3202 |
[32m[20221208 14:09:44 @agent_ppo2.py:179][0m |          -0.0405 |         207.5389 |        -383.4810 |
[32m[20221208 14:09:44 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:09:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.11
[32m[20221208 14:09:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 948.45
[32m[20221208 14:09:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 923.37
[32m[20221208 14:09:45 @agent_ppo2.py:137][0m Total time:      24.40 min
[32m[20221208 14:09:45 @agent_ppo2.py:139][0m 2000896 total steps have happened
[32m[20221208 14:09:45 @agent_ppo2.py:115][0m #------------------------ Iteration 977 --------------------------#
[32m[20221208 14:09:45 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:45 @agent_ppo2.py:179][0m |           0.0564 |         251.8030 |        -304.5798 |
[32m[20221208 14:09:45 @agent_ppo2.py:179][0m |           0.0481 |         238.5634 |        -255.0831 |
[32m[20221208 14:09:45 @agent_ppo2.py:179][0m |           0.0091 |         229.6839 |        -285.6015 |
[32m[20221208 14:09:45 @agent_ppo2.py:179][0m |          -0.0062 |         223.4356 |        -308.6795 |
[32m[20221208 14:09:45 @agent_ppo2.py:179][0m |          -0.0248 |         218.2648 |        -326.1536 |
[32m[20221208 14:09:45 @agent_ppo2.py:179][0m |          -0.0309 |         214.8822 |        -332.0197 |
[32m[20221208 14:09:45 @agent_ppo2.py:179][0m |          -0.0378 |         211.6277 |        -344.6882 |
[32m[20221208 14:09:46 @agent_ppo2.py:179][0m |          -0.0410 |         210.9873 |        -347.1370 |
[32m[20221208 14:09:46 @agent_ppo2.py:179][0m |          -0.0445 |         208.9091 |        -354.6042 |
[32m[20221208 14:09:46 @agent_ppo2.py:179][0m |          -0.0445 |         207.4387 |        -362.6265 |
[32m[20221208 14:09:46 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:09:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 956.23
[32m[20221208 14:09:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.05
[32m[20221208 14:09:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 643.08
[32m[20221208 14:09:46 @agent_ppo2.py:137][0m Total time:      24.43 min
[32m[20221208 14:09:46 @agent_ppo2.py:139][0m 2002944 total steps have happened
[32m[20221208 14:09:46 @agent_ppo2.py:115][0m #------------------------ Iteration 978 --------------------------#
[32m[20221208 14:09:46 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |           0.0566 |         256.7218 |        -282.5076 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |           0.0693 |         241.5094 |        -211.8367 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |           0.0278 |         235.3653 |        -247.6669 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |           0.0035 |         231.3010 |        -273.0666 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |          -0.0196 |         227.9788 |        -296.5197 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |          -0.0228 |         227.2528 |        -297.2504 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |          -0.0408 |         224.2262 |        -312.5726 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |          -0.0422 |         222.4903 |        -316.9517 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |          -0.0464 |         221.4273 |        -321.6206 |
[32m[20221208 14:09:47 @agent_ppo2.py:179][0m |          -0.0480 |         221.4450 |        -323.6828 |
[32m[20221208 14:09:47 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:09:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 906.68
[32m[20221208 14:09:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 937.20
[32m[20221208 14:09:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 625.02
[32m[20221208 14:09:48 @agent_ppo2.py:137][0m Total time:      24.45 min
[32m[20221208 14:09:48 @agent_ppo2.py:139][0m 2004992 total steps have happened
[32m[20221208 14:09:48 @agent_ppo2.py:115][0m #------------------------ Iteration 979 --------------------------#
[32m[20221208 14:09:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |           0.0514 |         237.7299 |        -290.3497 |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |           0.0426 |         230.1641 |        -240.5008 |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |           0.0110 |         227.1158 |        -297.0265 |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |           0.0168 |         224.5906 |        -271.6854 |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |          -0.0054 |         222.9396 |        -302.6803 |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |          -0.0250 |         220.5065 |        -340.9094 |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |          -0.0263 |         218.9977 |        -349.8907 |
[32m[20221208 14:09:48 @agent_ppo2.py:179][0m |          -0.0315 |         217.7188 |        -352.4842 |
[32m[20221208 14:09:49 @agent_ppo2.py:179][0m |          -0.0308 |         216.6379 |        -358.3342 |
[32m[20221208 14:09:49 @agent_ppo2.py:179][0m |          -0.0372 |         216.6832 |        -357.3095 |
[32m[20221208 14:09:49 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 963.65
[32m[20221208 14:09:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.67
[32m[20221208 14:09:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.17
[32m[20221208 14:09:49 @agent_ppo2.py:137][0m Total time:      24.48 min
[32m[20221208 14:09:49 @agent_ppo2.py:139][0m 2007040 total steps have happened
[32m[20221208 14:09:49 @agent_ppo2.py:115][0m #------------------------ Iteration 980 --------------------------#
[32m[20221208 14:09:49 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |           0.1273 |         248.4461 |        -281.7522 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |           0.0982 |         242.1175 |        -163.1592 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |           0.0419 |         237.4062 |        -208.9149 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |           0.0123 |         235.9456 |        -267.3687 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |          -0.0013 |         234.6337 |        -286.8422 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |          -0.0123 |         233.7217 |        -304.7660 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |          -0.0236 |         231.9656 |        -316.7691 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |          -0.0293 |         231.4969 |        -325.3378 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |          -0.0343 |         231.7067 |        -337.8781 |
[32m[20221208 14:09:50 @agent_ppo2.py:179][0m |          -0.0337 |         229.5149 |        -338.5649 |
[32m[20221208 14:09:50 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 964.18
[32m[20221208 14:09:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.01
[32m[20221208 14:09:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 875.08
[32m[20221208 14:09:50 @agent_ppo2.py:137][0m Total time:      24.50 min
[32m[20221208 14:09:50 @agent_ppo2.py:139][0m 2009088 total steps have happened
[32m[20221208 14:09:50 @agent_ppo2.py:115][0m #------------------------ Iteration 981 --------------------------#
[32m[20221208 14:09:51 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:09:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |           0.0573 |         223.5230 |        -299.1550 |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |           0.1174 |         198.4529 |        -196.3926 |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |           0.0292 |         190.1253 |        -259.7397 |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |           0.0036 |         182.4527 |        -292.9303 |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |          -0.0185 |         177.1362 |        -318.2996 |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |          -0.0317 |         173.8627 |        -335.4840 |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |          -0.0372 |         171.6899 |        -343.6426 |
[32m[20221208 14:09:51 @agent_ppo2.py:179][0m |          -0.0436 |         168.3033 |        -351.3196 |
[32m[20221208 14:09:52 @agent_ppo2.py:179][0m |          -0.0451 |         165.8476 |        -352.0383 |
[32m[20221208 14:09:52 @agent_ppo2.py:179][0m |          -0.0491 |         164.7265 |        -359.4495 |
[32m[20221208 14:09:52 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 883.66
[32m[20221208 14:09:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 917.18
[32m[20221208 14:09:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 962.07
[32m[20221208 14:09:52 @agent_ppo2.py:137][0m Total time:      24.53 min
[32m[20221208 14:09:52 @agent_ppo2.py:139][0m 2011136 total steps have happened
[32m[20221208 14:09:52 @agent_ppo2.py:115][0m #------------------------ Iteration 982 --------------------------#
[32m[20221208 14:09:52 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:52 @agent_ppo2.py:179][0m |           0.0650 |         257.9520 |        -303.2277 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |           0.1192 |         233.1345 |        -217.1128 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |           0.0219 |         220.5321 |        -241.7979 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |          -0.0079 |         212.7014 |        -268.2859 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |          -0.0225 |         207.2573 |        -286.5253 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |          -0.0323 |         204.0701 |        -295.1053 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |          -0.0373 |         200.8536 |        -307.8411 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |          -0.0452 |         198.1598 |        -316.7778 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |          -0.0482 |         195.9848 |        -324.5667 |
[32m[20221208 14:09:53 @agent_ppo2.py:179][0m |          -0.0449 |         193.8598 |        -327.6914 |
[32m[20221208 14:09:53 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 821.73
[32m[20221208 14:09:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.49
[32m[20221208 14:09:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 899.38
[32m[20221208 14:09:53 @agent_ppo2.py:137][0m Total time:      24.55 min
[32m[20221208 14:09:53 @agent_ppo2.py:139][0m 2013184 total steps have happened
[32m[20221208 14:09:53 @agent_ppo2.py:115][0m #------------------------ Iteration 983 --------------------------#
[32m[20221208 14:09:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |           0.0617 |         246.5118 |        -300.6192 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |           0.0723 |         233.2088 |        -232.8374 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |           0.0193 |         223.8822 |        -264.9202 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |          -0.0079 |         219.2466 |        -293.2936 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |          -0.0254 |         213.7941 |        -308.7803 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |          -0.0354 |         209.9540 |        -317.2541 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |          -0.0417 |         208.3535 |        -325.6938 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |          -0.0454 |         207.1212 |        -333.0907 |
[32m[20221208 14:09:54 @agent_ppo2.py:179][0m |          -0.0500 |         205.5168 |        -342.1459 |
[32m[20221208 14:09:55 @agent_ppo2.py:179][0m |          -0.0524 |         204.3299 |        -347.7883 |
[32m[20221208 14:09:55 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.10
[32m[20221208 14:09:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.07
[32m[20221208 14:09:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 972.92
[32m[20221208 14:09:55 @agent_ppo2.py:137][0m Total time:      24.58 min
[32m[20221208 14:09:55 @agent_ppo2.py:139][0m 2015232 total steps have happened
[32m[20221208 14:09:55 @agent_ppo2.py:115][0m #------------------------ Iteration 984 --------------------------#
[32m[20221208 14:09:55 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:55 @agent_ppo2.py:179][0m |           0.1699 |         263.7273 |        -262.4561 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |           0.1412 |         248.3982 |        -144.3865 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |           0.0846 |         244.3751 |        -153.3259 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |           0.0460 |         242.5564 |        -210.6945 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |           0.0163 |         239.7758 |        -255.6791 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |           0.0017 |         241.0193 |        -284.6003 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |          -0.0140 |         238.2091 |        -300.9914 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |          -0.0237 |         237.5036 |        -318.5089 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |          -0.0285 |         236.9084 |        -318.6442 |
[32m[20221208 14:09:56 @agent_ppo2.py:179][0m |          -0.0360 |         235.9001 |        -330.6878 |
[32m[20221208 14:09:56 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 914.17
[32m[20221208 14:09:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.62
[32m[20221208 14:09:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.24
[32m[20221208 14:09:56 @agent_ppo2.py:137][0m Total time:      24.60 min
[32m[20221208 14:09:56 @agent_ppo2.py:139][0m 2017280 total steps have happened
[32m[20221208 14:09:56 @agent_ppo2.py:115][0m #------------------------ Iteration 985 --------------------------#
[32m[20221208 14:09:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |           0.0514 |         248.0503 |        -290.4840 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |           0.0270 |         243.9599 |        -271.3013 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |           0.0114 |         241.0287 |        -288.3870 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |          -0.0174 |         240.1952 |        -309.3354 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |          -0.0263 |         239.3306 |        -316.4385 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |          -0.0285 |         238.0337 |        -315.5795 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |          -0.0379 |         237.2644 |        -324.8210 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |          -0.0369 |         237.0150 |        -329.1812 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |          -0.0421 |         237.7196 |        -336.8861 |
[32m[20221208 14:09:57 @agent_ppo2.py:179][0m |          -0.0464 |         235.5251 |        -339.6997 |
[32m[20221208 14:09:57 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:09:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.81
[32m[20221208 14:09:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 972.94
[32m[20221208 14:09:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.71
[32m[20221208 14:09:58 @agent_ppo2.py:137][0m Total time:      24.63 min
[32m[20221208 14:09:58 @agent_ppo2.py:139][0m 2019328 total steps have happened
[32m[20221208 14:09:58 @agent_ppo2.py:115][0m #------------------------ Iteration 986 --------------------------#
[32m[20221208 14:09:58 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:09:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:09:58 @agent_ppo2.py:179][0m |           0.0878 |         242.8473 |        -248.6011 |
[32m[20221208 14:09:58 @agent_ppo2.py:179][0m |           0.0607 |         232.7820 |        -197.5069 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |           0.0202 |         228.7687 |        -257.9950 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |           0.0076 |         223.7925 |        -282.7035 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |          -0.0126 |         221.9462 |        -300.7080 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |          -0.0258 |         220.6367 |        -309.5181 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |          -0.0320 |         219.6909 |        -315.8683 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |          -0.0381 |         219.6845 |        -317.0294 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |          -0.0422 |         219.8800 |        -327.8830 |
[32m[20221208 14:09:59 @agent_ppo2.py:179][0m |          -0.0419 |         217.3460 |        -329.3423 |
[32m[20221208 14:09:59 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:09:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.78
[32m[20221208 14:09:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.53
[32m[20221208 14:09:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 966.74
[32m[20221208 14:09:59 @agent_ppo2.py:137][0m Total time:      24.65 min
[32m[20221208 14:09:59 @agent_ppo2.py:139][0m 2021376 total steps have happened
[32m[20221208 14:09:59 @agent_ppo2.py:115][0m #------------------------ Iteration 987 --------------------------#
[32m[20221208 14:10:00 @agent_ppo2.py:121][0m Sampling time: 0.51 s by 1 slaves
[32m[20221208 14:10:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |           0.1482 |         249.5796 |        -261.4957 |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |           0.1767 |         238.5696 |        -162.0735 |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |           0.0785 |         233.7050 |        -175.7673 |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |           0.0221 |         228.9680 |        -233.8255 |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |          -0.0011 |         226.3221 |        -277.6218 |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |          -0.0208 |         223.5553 |        -300.7153 |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |          -0.0308 |         222.6634 |        -311.0513 |
[32m[20221208 14:10:00 @agent_ppo2.py:179][0m |          -0.0370 |         221.9323 |        -318.4550 |
[32m[20221208 14:10:01 @agent_ppo2.py:179][0m |          -0.0410 |         221.0284 |        -326.5672 |
[32m[20221208 14:10:01 @agent_ppo2.py:179][0m |          -0.0438 |         219.7349 |        -331.7090 |
[32m[20221208 14:10:01 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:10:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 897.43
[32m[20221208 14:10:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.33
[32m[20221208 14:10:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 646.00
[32m[20221208 14:10:01 @agent_ppo2.py:137][0m Total time:      24.68 min
[32m[20221208 14:10:01 @agent_ppo2.py:139][0m 2023424 total steps have happened
[32m[20221208 14:10:01 @agent_ppo2.py:115][0m #------------------------ Iteration 988 --------------------------#
[32m[20221208 14:10:01 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:10:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |           0.0699 |         245.2916 |        -242.9206 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |           0.0696 |         240.7223 |        -165.2791 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |           0.0350 |         238.3518 |        -232.8041 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |           0.0076 |         236.7674 |        -256.7054 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |          -0.0130 |         236.0521 |        -280.0136 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |          -0.0252 |         235.9951 |        -294.6092 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |          -0.0300 |         236.5617 |        -297.1570 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |          -0.0350 |         234.3453 |        -301.0691 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |          -0.0363 |         233.7258 |        -305.6268 |
[32m[20221208 14:10:02 @agent_ppo2.py:179][0m |          -0.0392 |         234.1102 |        -312.2217 |
[32m[20221208 14:10:02 @agent_ppo2.py:124][0m Policy update time: 0.83 s
[32m[20221208 14:10:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 962.92
[32m[20221208 14:10:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.56
[32m[20221208 14:10:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.15
[32m[20221208 14:10:03 @agent_ppo2.py:137][0m Total time:      24.71 min
[32m[20221208 14:10:03 @agent_ppo2.py:139][0m 2025472 total steps have happened
[32m[20221208 14:10:03 @agent_ppo2.py:115][0m #------------------------ Iteration 989 --------------------------#
[32m[20221208 14:10:03 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:10:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:03 @agent_ppo2.py:179][0m |           0.0269 |         190.4064 |        -183.6867 |
[32m[20221208 14:10:03 @agent_ppo2.py:179][0m |           0.0078 |         181.8458 |        -158.6353 |
[32m[20221208 14:10:03 @agent_ppo2.py:179][0m |          -0.0260 |         179.3034 |        -182.0779 |
[32m[20221208 14:10:03 @agent_ppo2.py:179][0m |          -0.0408 |         178.5687 |        -197.8222 |
[32m[20221208 14:10:04 @agent_ppo2.py:179][0m |          -0.0571 |         176.4262 |        -206.3409 |
[32m[20221208 14:10:04 @agent_ppo2.py:179][0m |          -0.0625 |         175.2874 |        -212.0456 |
[32m[20221208 14:10:04 @agent_ppo2.py:179][0m |          -0.0645 |         174.4997 |        -220.1377 |
[32m[20221208 14:10:04 @agent_ppo2.py:179][0m |          -0.0687 |         173.9114 |        -222.9994 |
[32m[20221208 14:10:04 @agent_ppo2.py:179][0m |          -0.0688 |         173.3218 |        -226.8636 |
[32m[20221208 14:10:04 @agent_ppo2.py:179][0m |          -0.0741 |         172.9275 |        -231.3878 |
[32m[20221208 14:10:04 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:10:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 666.20
[32m[20221208 14:10:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.21
[32m[20221208 14:10:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.33
[32m[20221208 14:10:04 @agent_ppo2.py:137][0m Total time:      24.73 min
[32m[20221208 14:10:04 @agent_ppo2.py:139][0m 2027520 total steps have happened
[32m[20221208 14:10:04 @agent_ppo2.py:115][0m #------------------------ Iteration 990 --------------------------#
[32m[20221208 14:10:05 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:10:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |           0.1353 |         244.9649 |        -260.9799 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |           0.0862 |         238.4258 |        -180.1287 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |           0.0727 |         235.8201 |        -202.3081 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |           0.0224 |         233.8312 |        -246.7125 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |          -0.0050 |         230.1950 |        -277.9152 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |          -0.0143 |         228.9275 |        -294.1969 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |          -0.0229 |         227.0696 |        -293.5150 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |          -0.0328 |         225.8895 |        -312.8260 |
[32m[20221208 14:10:05 @agent_ppo2.py:179][0m |          -0.0393 |         224.6407 |        -321.4321 |
[32m[20221208 14:10:06 @agent_ppo2.py:179][0m |          -0.0423 |         224.7203 |        -329.3636 |
[32m[20221208 14:10:06 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 14:10:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 848.65
[32m[20221208 14:10:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 858.59
[32m[20221208 14:10:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 642.96
[32m[20221208 14:10:06 @agent_ppo2.py:137][0m Total time:      24.76 min
[32m[20221208 14:10:06 @agent_ppo2.py:139][0m 2029568 total steps have happened
[32m[20221208 14:10:06 @agent_ppo2.py:115][0m #------------------------ Iteration 991 --------------------------#
[32m[20221208 14:10:06 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |           0.0594 |         247.9310 |        -259.4948 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |           0.0569 |         239.0491 |        -206.8841 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |           0.0208 |         236.5851 |        -251.5231 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |           0.0058 |         233.8933 |        -265.4156 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |          -0.0027 |         232.8721 |        -265.2834 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |          -0.0256 |         234.3066 |        -298.6608 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |          -0.0339 |         231.6017 |        -304.0719 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |          -0.0404 |         232.2433 |        -314.0452 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |          -0.0400 |         232.1156 |        -320.8355 |
[32m[20221208 14:10:07 @agent_ppo2.py:179][0m |          -0.0371 |         231.2677 |        -321.5130 |
[32m[20221208 14:10:07 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 14:10:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 922.05
[32m[20221208 14:10:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 944.87
[32m[20221208 14:10:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 907.63
[32m[20221208 14:10:08 @agent_ppo2.py:137][0m Total time:      24.79 min
[32m[20221208 14:10:08 @agent_ppo2.py:139][0m 2031616 total steps have happened
[32m[20221208 14:10:08 @agent_ppo2.py:115][0m #------------------------ Iteration 992 --------------------------#
[32m[20221208 14:10:08 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:08 @agent_ppo2.py:179][0m |           0.0866 |         243.8678 |        -250.0894 |
[32m[20221208 14:10:08 @agent_ppo2.py:179][0m |           0.0649 |         237.6161 |        -227.5982 |
[32m[20221208 14:10:08 @agent_ppo2.py:179][0m |           0.0342 |         235.8618 |        -230.3765 |
[32m[20221208 14:10:08 @agent_ppo2.py:179][0m |          -0.0053 |         233.6990 |        -259.1426 |
[32m[20221208 14:10:08 @agent_ppo2.py:179][0m |          -0.0114 |         232.6268 |        -268.6159 |
[32m[20221208 14:10:08 @agent_ppo2.py:179][0m |          -0.0218 |         232.2886 |        -277.2580 |
[32m[20221208 14:10:09 @agent_ppo2.py:179][0m |          -0.0329 |         231.7136 |        -283.3885 |
[32m[20221208 14:10:09 @agent_ppo2.py:179][0m |          -0.0413 |         231.3484 |        -291.7683 |
[32m[20221208 14:10:09 @agent_ppo2.py:179][0m |          -0.0445 |         230.8139 |        -301.1146 |
[32m[20221208 14:10:09 @agent_ppo2.py:179][0m |          -0.0410 |         230.6522 |        -297.9987 |
[32m[20221208 14:10:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:10:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 886.22
[32m[20221208 14:10:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.76
[32m[20221208 14:10:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 787.21
[32m[20221208 14:10:09 @agent_ppo2.py:137][0m Total time:      24.81 min
[32m[20221208 14:10:09 @agent_ppo2.py:139][0m 2033664 total steps have happened
[32m[20221208 14:10:09 @agent_ppo2.py:115][0m #------------------------ Iteration 993 --------------------------#
[32m[20221208 14:10:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |           0.0572 |         222.5336 |        -200.1269 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |           0.0125 |         209.1348 |        -171.7151 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0278 |         204.6673 |        -197.3392 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0507 |         203.1471 |        -217.4389 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0485 |         203.2201 |        -216.7798 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0551 |         201.9134 |        -226.1813 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0630 |         201.1000 |        -234.4888 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0685 |         202.5424 |        -242.5840 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0672 |         200.7266 |        -241.7905 |
[32m[20221208 14:10:10 @agent_ppo2.py:179][0m |          -0.0668 |         200.8502 |        -245.3724 |
[32m[20221208 14:10:10 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:10:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 660.81
[32m[20221208 14:10:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.52
[32m[20221208 14:10:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.24
[32m[20221208 14:10:11 @agent_ppo2.py:137][0m Total time:      24.84 min
[32m[20221208 14:10:11 @agent_ppo2.py:139][0m 2035712 total steps have happened
[32m[20221208 14:10:11 @agent_ppo2.py:115][0m #------------------------ Iteration 994 --------------------------#
[32m[20221208 14:10:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:10:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:11 @agent_ppo2.py:179][0m |           0.0892 |         243.7248 |        -283.1105 |
[32m[20221208 14:10:11 @agent_ppo2.py:179][0m |           0.0742 |         235.0956 |        -172.6217 |
[32m[20221208 14:10:11 @agent_ppo2.py:179][0m |           0.0371 |         232.2772 |        -233.1176 |
[32m[20221208 14:10:11 @agent_ppo2.py:179][0m |           0.0156 |         227.3606 |        -259.8305 |
[32m[20221208 14:10:11 @agent_ppo2.py:179][0m |          -0.0086 |         225.5034 |        -279.5130 |
[32m[20221208 14:10:11 @agent_ppo2.py:179][0m |          -0.0189 |         222.9781 |        -295.6441 |
[32m[20221208 14:10:12 @agent_ppo2.py:179][0m |          -0.0275 |         223.4261 |        -304.0374 |
[32m[20221208 14:10:12 @agent_ppo2.py:179][0m |          -0.0278 |         222.2584 |        -308.1225 |
[32m[20221208 14:10:12 @agent_ppo2.py:179][0m |          -0.0322 |         220.1083 |        -312.3174 |
[32m[20221208 14:10:12 @agent_ppo2.py:179][0m |          -0.0359 |         220.0730 |        -315.8873 |
[32m[20221208 14:10:12 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:10:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.55
[32m[20221208 14:10:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.32
[32m[20221208 14:10:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 648.85
[32m[20221208 14:10:12 @agent_ppo2.py:137][0m Total time:      24.86 min
[32m[20221208 14:10:12 @agent_ppo2.py:139][0m 2037760 total steps have happened
[32m[20221208 14:10:12 @agent_ppo2.py:115][0m #------------------------ Iteration 995 --------------------------#
[32m[20221208 14:10:13 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:10:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |           0.0868 |         219.4202 |        -265.0443 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |           0.0292 |         202.0668 |        -229.7293 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0025 |         196.0196 |        -260.1433 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0129 |         192.9603 |        -281.3024 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0278 |         191.3640 |        -292.5568 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0358 |         189.0754 |        -302.8092 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0334 |         186.4527 |        -299.9194 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0423 |         183.1588 |        -313.4520 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0326 |         182.1029 |        -313.4293 |
[32m[20221208 14:10:13 @agent_ppo2.py:179][0m |          -0.0437 |         178.9868 |        -322.0093 |
[32m[20221208 14:10:13 @agent_ppo2.py:124][0m Policy update time: 0.81 s
[32m[20221208 14:10:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.86
[32m[20221208 14:10:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.34
[32m[20221208 14:10:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.07
[32m[20221208 14:10:14 @agent_ppo2.py:137][0m Total time:      24.89 min
[32m[20221208 14:10:14 @agent_ppo2.py:139][0m 2039808 total steps have happened
[32m[20221208 14:10:14 @agent_ppo2.py:115][0m #------------------------ Iteration 996 --------------------------#
[32m[20221208 14:10:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:14 @agent_ppo2.py:179][0m |           0.1097 |         247.5370 |        -201.8943 |
[32m[20221208 14:10:14 @agent_ppo2.py:179][0m |           0.1031 |         238.3747 |         -94.9163 |
[32m[20221208 14:10:14 @agent_ppo2.py:179][0m |           0.0391 |         232.5906 |        -184.8236 |
[32m[20221208 14:10:15 @agent_ppo2.py:179][0m |           0.0010 |         228.4339 |        -249.6833 |
[32m[20221208 14:10:15 @agent_ppo2.py:179][0m |          -0.0168 |         225.4881 |        -268.3622 |
[32m[20221208 14:10:15 @agent_ppo2.py:179][0m |          -0.0209 |         224.8522 |        -272.3360 |
[32m[20221208 14:10:15 @agent_ppo2.py:179][0m |          -0.0303 |         222.3998 |        -286.7281 |
[32m[20221208 14:10:15 @agent_ppo2.py:179][0m |          -0.0339 |         222.1846 |        -291.1896 |
[32m[20221208 14:10:15 @agent_ppo2.py:179][0m |          -0.0401 |         220.4926 |        -299.0457 |
[32m[20221208 14:10:15 @agent_ppo2.py:179][0m |          -0.0391 |         222.5038 |        -301.5428 |
[32m[20221208 14:10:15 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:10:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.02
[32m[20221208 14:10:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.59
[32m[20221208 14:10:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.42
[32m[20221208 14:10:15 @agent_ppo2.py:137][0m Total time:      24.92 min
[32m[20221208 14:10:15 @agent_ppo2.py:139][0m 2041856 total steps have happened
[32m[20221208 14:10:15 @agent_ppo2.py:115][0m #------------------------ Iteration 997 --------------------------#
[32m[20221208 14:10:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |           0.1665 |         228.5994 |        -243.4589 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |           0.1548 |         210.5486 |        -127.7895 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |           0.0425 |         205.1057 |        -164.0212 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |           0.0003 |         200.2047 |        -177.0881 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |          -0.0100 |         197.3795 |        -187.6446 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |          -0.0233 |         195.3397 |        -196.1503 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |          -0.0316 |         195.3001 |        -198.7587 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |          -0.0349 |         191.9599 |        -207.5573 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |          -0.0418 |         190.0792 |        -209.8380 |
[32m[20221208 14:10:16 @agent_ppo2.py:179][0m |          -0.0458 |         189.0477 |        -213.8097 |
[32m[20221208 14:10:16 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:10:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 696.14
[32m[20221208 14:10:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.85
[32m[20221208 14:10:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 899.52
[32m[20221208 14:10:17 @agent_ppo2.py:137][0m Total time:      24.94 min
[32m[20221208 14:10:17 @agent_ppo2.py:139][0m 2043904 total steps have happened
[32m[20221208 14:10:17 @agent_ppo2.py:115][0m #------------------------ Iteration 998 --------------------------#
[32m[20221208 14:10:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:10:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:17 @agent_ppo2.py:179][0m |           0.0725 |         247.8470 |        -223.8667 |
[32m[20221208 14:10:17 @agent_ppo2.py:179][0m |           0.0541 |         238.4584 |        -190.3974 |
[32m[20221208 14:10:17 @agent_ppo2.py:179][0m |           0.0349 |         235.0640 |        -214.7593 |
[32m[20221208 14:10:18 @agent_ppo2.py:179][0m |          -0.0026 |         230.8219 |        -262.3971 |
[32m[20221208 14:10:18 @agent_ppo2.py:179][0m |          -0.0204 |         228.0261 |        -285.0650 |
[32m[20221208 14:10:18 @agent_ppo2.py:179][0m |          -0.0267 |         226.6939 |        -287.4463 |
[32m[20221208 14:10:18 @agent_ppo2.py:179][0m |          -0.0147 |         224.7817 |        -272.6414 |
[32m[20221208 14:10:18 @agent_ppo2.py:179][0m |          -0.0252 |         224.2141 |        -287.9491 |
[32m[20221208 14:10:18 @agent_ppo2.py:179][0m |          -0.0336 |         220.9637 |        -296.4105 |
[32m[20221208 14:10:18 @agent_ppo2.py:179][0m |          -0.0384 |         219.7927 |        -302.1364 |
[32m[20221208 14:10:18 @agent_ppo2.py:124][0m Policy update time: 0.77 s
[32m[20221208 14:10:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.55
[32m[20221208 14:10:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.92
[32m[20221208 14:10:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 934.63
[32m[20221208 14:10:18 @agent_ppo2.py:137][0m Total time:      24.97 min
[32m[20221208 14:10:18 @agent_ppo2.py:139][0m 2045952 total steps have happened
[32m[20221208 14:10:18 @agent_ppo2.py:115][0m #------------------------ Iteration 999 --------------------------#
[32m[20221208 14:10:19 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:10:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:19 @agent_ppo2.py:179][0m |           0.1194 |         200.1617 |        -186.6913 |
[32m[20221208 14:10:19 @agent_ppo2.py:179][0m |           0.0561 |         177.9442 |        -136.6249 |
[32m[20221208 14:10:19 @agent_ppo2.py:179][0m |           0.0058 |         170.9676 |        -171.2566 |
[32m[20221208 14:10:19 @agent_ppo2.py:179][0m |          -0.0290 |         168.5528 |        -197.0638 |
[32m[20221208 14:10:19 @agent_ppo2.py:179][0m |          -0.0435 |         167.1717 |        -207.7464 |
[32m[20221208 14:10:19 @agent_ppo2.py:179][0m |          -0.0528 |         165.4852 |        -217.7441 |
[32m[20221208 14:10:20 @agent_ppo2.py:179][0m |          -0.0599 |         165.5723 |        -225.5197 |
[32m[20221208 14:10:20 @agent_ppo2.py:179][0m |          -0.0627 |         163.8375 |        -232.4588 |
[32m[20221208 14:10:20 @agent_ppo2.py:179][0m |          -0.0667 |         162.9935 |        -236.9374 |
[32m[20221208 14:10:20 @agent_ppo2.py:179][0m |          -0.0612 |         162.1148 |        -233.2658 |
[32m[20221208 14:10:20 @agent_ppo2.py:124][0m Policy update time: 0.87 s
[32m[20221208 14:10:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 690.75
[32m[20221208 14:10:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.08
[32m[20221208 14:10:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 940.41
[32m[20221208 14:10:20 @agent_ppo2.py:97][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 988.63
[32m[20221208 14:10:20 @agent_ppo2.py:137][0m Total time:      25.00 min
[32m[20221208 14:10:20 @agent_ppo2.py:139][0m 2048000 total steps have happened
[32m[20221208 14:10:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1000 --------------------------#
[32m[20221208 14:10:21 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |           0.0581 |         263.9671 |        -231.5137 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |           0.0871 |         253.3826 |        -195.0880 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |           0.0346 |         249.8045 |        -226.0092 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |           0.0387 |         248.1580 |        -193.0360 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |           0.0037 |         247.5670 |        -228.3428 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |          -0.0176 |         247.0210 |        -248.8789 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |          -0.0301 |         244.7846 |        -262.2430 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |          -0.0324 |         243.9201 |        -261.0321 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |          -0.0377 |         243.1931 |        -267.1081 |
[32m[20221208 14:10:21 @agent_ppo2.py:179][0m |          -0.0450 |         242.7413 |        -273.8307 |
[32m[20221208 14:10:21 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:10:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 899.20
[32m[20221208 14:10:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 919.62
[32m[20221208 14:10:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.01
[32m[20221208 14:10:22 @agent_ppo2.py:137][0m Total time:      25.02 min
[32m[20221208 14:10:22 @agent_ppo2.py:139][0m 2050048 total steps have happened
[32m[20221208 14:10:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1001 --------------------------#
[32m[20221208 14:10:22 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:10:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:22 @agent_ppo2.py:179][0m |           0.0911 |         255.0903 |        -260.5877 |
[32m[20221208 14:10:22 @agent_ppo2.py:179][0m |           0.1082 |         249.0477 |        -157.0686 |
[32m[20221208 14:10:22 @agent_ppo2.py:179][0m |           0.0526 |         246.6023 |        -174.3705 |
[32m[20221208 14:10:23 @agent_ppo2.py:179][0m |           0.0228 |         245.2759 |        -224.2683 |
[32m[20221208 14:10:23 @agent_ppo2.py:179][0m |          -0.0029 |         241.9592 |        -263.2975 |
[32m[20221208 14:10:23 @agent_ppo2.py:179][0m |          -0.0147 |         242.0558 |        -278.9940 |
[32m[20221208 14:10:23 @agent_ppo2.py:179][0m |          -0.0198 |         239.0440 |        -278.7044 |
[32m[20221208 14:10:23 @agent_ppo2.py:179][0m |          -0.0265 |         238.1031 |        -289.1721 |
[32m[20221208 14:10:23 @agent_ppo2.py:179][0m |          -0.0305 |         237.3263 |        -294.1312 |
[32m[20221208 14:10:23 @agent_ppo2.py:179][0m |          -0.0383 |         236.4880 |        -301.4119 |
[32m[20221208 14:10:23 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:10:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.17
[32m[20221208 14:10:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 950.53
[32m[20221208 14:10:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 910.16
[32m[20221208 14:10:23 @agent_ppo2.py:137][0m Total time:      25.05 min
[32m[20221208 14:10:23 @agent_ppo2.py:139][0m 2052096 total steps have happened
[32m[20221208 14:10:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1002 --------------------------#
[32m[20221208 14:10:24 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |           0.0755 |         244.1313 |        -233.5570 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |           0.0649 |         237.7709 |        -159.8357 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |           0.0407 |         235.7305 |        -191.4859 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |           0.0503 |         234.2432 |        -195.5317 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |           0.0018 |         233.2542 |        -241.2297 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |          -0.0170 |         233.5730 |        -262.2217 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |          -0.0236 |         233.0542 |        -269.9233 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |          -0.0283 |         233.3893 |        -278.9979 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |          -0.0310 |         232.9047 |        -278.2631 |
[32m[20221208 14:10:24 @agent_ppo2.py:179][0m |          -0.0353 |         231.8737 |        -287.6258 |
[32m[20221208 14:10:24 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:10:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 989.26
[32m[20221208 14:10:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.43
[32m[20221208 14:10:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.82
[32m[20221208 14:10:25 @agent_ppo2.py:137][0m Total time:      25.07 min
[32m[20221208 14:10:25 @agent_ppo2.py:139][0m 2054144 total steps have happened
[32m[20221208 14:10:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1003 --------------------------#
[32m[20221208 14:10:25 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:25 @agent_ppo2.py:179][0m |           0.0573 |         249.1673 |        -247.9074 |
[32m[20221208 14:10:25 @agent_ppo2.py:179][0m |           0.0487 |         245.6035 |        -190.8525 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |           0.0065 |         244.7578 |        -234.9815 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |          -0.0125 |         243.0284 |        -258.0943 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |          -0.0224 |         243.1041 |        -272.8620 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |          -0.0300 |         242.0191 |        -280.0356 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |          -0.0353 |         241.3665 |        -283.4966 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |          -0.0389 |         241.2078 |        -294.3532 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |          -0.0410 |         241.3353 |        -296.6907 |
[32m[20221208 14:10:26 @agent_ppo2.py:179][0m |          -0.0421 |         240.3911 |        -301.1014 |
[32m[20221208 14:10:26 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:10:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 961.95
[32m[20221208 14:10:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.06
[32m[20221208 14:10:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 940.39
[32m[20221208 14:10:26 @agent_ppo2.py:137][0m Total time:      25.10 min
[32m[20221208 14:10:26 @agent_ppo2.py:139][0m 2056192 total steps have happened
[32m[20221208 14:10:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1004 --------------------------#
[32m[20221208 14:10:27 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |           0.0972 |         244.6804 |        -217.6858 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |           0.0634 |         234.4135 |        -156.0212 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |           0.0353 |         224.7959 |        -202.7832 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |           0.0040 |         217.7769 |        -249.6412 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |          -0.0062 |         213.9729 |        -267.7439 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |          -0.0182 |         213.1630 |        -277.9915 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |          -0.0271 |         210.7023 |        -281.5049 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |          -0.0270 |         209.4399 |        -282.4556 |
[32m[20221208 14:10:27 @agent_ppo2.py:179][0m |          -0.0298 |         211.1926 |        -293.8033 |
[32m[20221208 14:10:28 @agent_ppo2.py:179][0m |          -0.0347 |         207.9711 |        -303.1387 |
[32m[20221208 14:10:28 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:10:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 940.33
[32m[20221208 14:10:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.93
[32m[20221208 14:10:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.35
[32m[20221208 14:10:28 @agent_ppo2.py:137][0m Total time:      25.13 min
[32m[20221208 14:10:28 @agent_ppo2.py:139][0m 2058240 total steps have happened
[32m[20221208 14:10:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1005 --------------------------#
[32m[20221208 14:10:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |           0.0773 |         252.4925 |        -201.4089 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |           0.1374 |         246.5319 |        -113.7979 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |           0.0719 |         244.2623 |        -118.6387 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |           0.0370 |         242.9567 |        -171.1308 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |           0.0031 |         242.6082 |        -216.3718 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |          -0.0134 |         242.0963 |        -231.8648 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |          -0.0212 |         241.7755 |        -241.7057 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |          -0.0260 |         240.8041 |        -247.4080 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |          -0.0336 |         239.1576 |        -252.1128 |
[32m[20221208 14:10:29 @agent_ppo2.py:179][0m |          -0.0395 |         239.1829 |        -262.4460 |
[32m[20221208 14:10:29 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 14:10:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.04
[32m[20221208 14:10:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 927.89
[32m[20221208 14:10:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.02
[32m[20221208 14:10:30 @agent_ppo2.py:137][0m Total time:      25.15 min
[32m[20221208 14:10:30 @agent_ppo2.py:139][0m 2060288 total steps have happened
[32m[20221208 14:10:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1006 --------------------------#
[32m[20221208 14:10:30 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:10:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:30 @agent_ppo2.py:179][0m |           0.0556 |         249.3844 |        -228.8705 |
[32m[20221208 14:10:30 @agent_ppo2.py:179][0m |           0.0657 |         239.7449 |        -198.6241 |
[32m[20221208 14:10:30 @agent_ppo2.py:179][0m |           0.0108 |         232.0354 |        -225.4057 |
[32m[20221208 14:10:30 @agent_ppo2.py:179][0m |          -0.0092 |         226.9268 |        -238.5194 |
[32m[20221208 14:10:30 @agent_ppo2.py:179][0m |          -0.0228 |         225.5100 |        -248.7750 |
[32m[20221208 14:10:31 @agent_ppo2.py:179][0m |          -0.0307 |         221.1774 |        -256.6195 |
[32m[20221208 14:10:31 @agent_ppo2.py:179][0m |          -0.0361 |         218.9240 |        -262.0251 |
[32m[20221208 14:10:31 @agent_ppo2.py:179][0m |          -0.0399 |         217.2146 |        -266.8831 |
[32m[20221208 14:10:31 @agent_ppo2.py:179][0m |          -0.0431 |         215.5896 |        -271.1719 |
[32m[20221208 14:10:31 @agent_ppo2.py:179][0m |          -0.0443 |         213.7993 |        -272.4893 |
[32m[20221208 14:10:31 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:10:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 922.17
[32m[20221208 14:10:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.04
[32m[20221208 14:10:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 969.75
[32m[20221208 14:10:31 @agent_ppo2.py:137][0m Total time:      25.18 min
[32m[20221208 14:10:31 @agent_ppo2.py:139][0m 2062336 total steps have happened
[32m[20221208 14:10:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1007 --------------------------#
[32m[20221208 14:10:32 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |           0.0961 |         240.7066 |        -203.1101 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |           0.0321 |         209.7667 |        -185.8559 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |           0.0013 |         198.9916 |        -201.8055 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |          -0.0181 |         191.3893 |        -221.9960 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |          -0.0267 |         187.1034 |        -232.7248 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |          -0.0327 |         185.2066 |        -243.6335 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |          -0.0366 |         178.1805 |        -248.9143 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |          -0.0390 |         174.9626 |        -253.2461 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |          -0.0420 |         172.1118 |        -260.6453 |
[32m[20221208 14:10:32 @agent_ppo2.py:179][0m |          -0.0447 |         170.2390 |        -264.5664 |
[32m[20221208 14:10:32 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:10:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.17
[32m[20221208 14:10:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.51
[32m[20221208 14:10:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.74
[32m[20221208 14:10:33 @agent_ppo2.py:137][0m Total time:      25.21 min
[32m[20221208 14:10:33 @agent_ppo2.py:139][0m 2064384 total steps have happened
[32m[20221208 14:10:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1008 --------------------------#
[32m[20221208 14:10:33 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:33 @agent_ppo2.py:179][0m |           0.1249 |         249.6554 |        -202.1958 |
[32m[20221208 14:10:33 @agent_ppo2.py:179][0m |           0.1244 |         237.6488 |        -135.3782 |
[32m[20221208 14:10:33 @agent_ppo2.py:179][0m |           0.0466 |         232.8862 |        -165.3540 |
[32m[20221208 14:10:33 @agent_ppo2.py:179][0m |           0.0231 |         231.0250 |        -206.0297 |
[32m[20221208 14:10:34 @agent_ppo2.py:179][0m |           0.0109 |         229.0420 |        -220.6618 |
[32m[20221208 14:10:34 @agent_ppo2.py:179][0m |          -0.0097 |         228.4336 |        -239.5576 |
[32m[20221208 14:10:34 @agent_ppo2.py:179][0m |          -0.0188 |         225.8387 |        -254.3492 |
[32m[20221208 14:10:34 @agent_ppo2.py:179][0m |          -0.0250 |         224.9022 |        -260.7991 |
[32m[20221208 14:10:34 @agent_ppo2.py:179][0m |          -0.0336 |         225.0280 |        -269.9624 |
[32m[20221208 14:10:34 @agent_ppo2.py:179][0m |          -0.0381 |         222.9172 |        -271.4296 |
[32m[20221208 14:10:34 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:10:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 949.84
[32m[20221208 14:10:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.71
[32m[20221208 14:10:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.89
[32m[20221208 14:10:34 @agent_ppo2.py:137][0m Total time:      25.23 min
[32m[20221208 14:10:34 @agent_ppo2.py:139][0m 2066432 total steps have happened
[32m[20221208 14:10:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1009 --------------------------#
[32m[20221208 14:10:35 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:35 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |           0.1545 |         257.2884 |        -206.5403 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |           0.1036 |         247.0821 |        -105.3101 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |           0.0791 |         240.3170 |        -109.5615 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |           0.0333 |         238.1024 |        -181.3008 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |           0.0034 |         235.0931 |        -226.3959 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |          -0.0170 |         232.0511 |        -246.5481 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |          -0.0185 |         231.8373 |        -249.3241 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |          -0.0151 |         231.3531 |        -246.2796 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |          -0.0304 |         230.6448 |        -265.6513 |
[32m[20221208 14:10:35 @agent_ppo2.py:179][0m |          -0.0373 |         229.8069 |        -268.3631 |
[32m[20221208 14:10:35 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:10:36 @agent_ppo2.py:132][0m Average TRAINING episode reward: 953.49
[32m[20221208 14:10:36 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 999.04
[32m[20221208 14:10:36 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.64
[32m[20221208 14:10:36 @agent_ppo2.py:137][0m Total time:      25.26 min
[32m[20221208 14:10:36 @agent_ppo2.py:139][0m 2068480 total steps have happened
[32m[20221208 14:10:36 @agent_ppo2.py:115][0m #------------------------ Iteration 1010 --------------------------#
[32m[20221208 14:10:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:36 @agent_ppo2.py:179][0m |           0.0859 |         232.9716 |        -172.3747 |
[32m[20221208 14:10:36 @agent_ppo2.py:179][0m |           0.0043 |         222.3068 |        -152.5599 |
[32m[20221208 14:10:36 @agent_ppo2.py:179][0m |          -0.0185 |         218.6616 |        -168.7111 |
[32m[20221208 14:10:37 @agent_ppo2.py:179][0m |          -0.0361 |         216.7439 |        -175.4831 |
[32m[20221208 14:10:37 @agent_ppo2.py:179][0m |          -0.0454 |         215.4693 |        -183.4053 |
[32m[20221208 14:10:37 @agent_ppo2.py:179][0m |          -0.0544 |         214.0482 |        -191.3289 |
[32m[20221208 14:10:37 @agent_ppo2.py:179][0m |          -0.0600 |         212.7715 |        -195.8780 |
[32m[20221208 14:10:37 @agent_ppo2.py:179][0m |          -0.0613 |         213.6504 |        -201.5907 |
[32m[20221208 14:10:37 @agent_ppo2.py:179][0m |          -0.0655 |         212.2351 |        -204.9408 |
[32m[20221208 14:10:37 @agent_ppo2.py:179][0m |          -0.0685 |         210.6497 |        -209.0206 |
[32m[20221208 14:10:37 @agent_ppo2.py:124][0m Policy update time: 0.64 s
[32m[20221208 14:10:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 735.06
[32m[20221208 14:10:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 928.81
[32m[20221208 14:10:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 903.26
[32m[20221208 14:10:37 @agent_ppo2.py:137][0m Total time:      25.28 min
[32m[20221208 14:10:37 @agent_ppo2.py:139][0m 2070528 total steps have happened
[32m[20221208 14:10:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1011 --------------------------#
[32m[20221208 14:10:38 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:38 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |           0.0903 |         252.6980 |        -218.3926 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |           0.0494 |         239.5458 |        -176.3737 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |           0.0113 |         233.1123 |        -214.2490 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |          -0.0069 |         229.6189 |        -238.3731 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |          -0.0212 |         224.8749 |        -252.6783 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |          -0.0283 |         221.6513 |        -256.3030 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |          -0.0347 |         218.9321 |        -257.7933 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |          -0.0412 |         218.7705 |        -269.2194 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |          -0.0347 |         216.8245 |        -269.2604 |
[32m[20221208 14:10:38 @agent_ppo2.py:179][0m |          -0.0377 |         214.1750 |        -268.2513 |
[32m[20221208 14:10:38 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:10:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 894.99
[32m[20221208 14:10:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 919.94
[32m[20221208 14:10:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 697.73
[32m[20221208 14:10:39 @agent_ppo2.py:137][0m Total time:      25.31 min
[32m[20221208 14:10:39 @agent_ppo2.py:139][0m 2072576 total steps have happened
[32m[20221208 14:10:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1012 --------------------------#
[32m[20221208 14:10:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:39 @agent_ppo2.py:179][0m |           0.0866 |         250.1840 |        -252.4914 |
[32m[20221208 14:10:39 @agent_ppo2.py:179][0m |           0.0667 |         244.8412 |        -161.7849 |
[32m[20221208 14:10:39 @agent_ppo2.py:179][0m |           0.0166 |         244.3969 |        -223.9708 |
[32m[20221208 14:10:40 @agent_ppo2.py:179][0m |          -0.0004 |         243.9095 |        -245.3254 |
[32m[20221208 14:10:40 @agent_ppo2.py:179][0m |          -0.0179 |         242.5392 |        -255.8611 |
[32m[20221208 14:10:40 @agent_ppo2.py:179][0m |          -0.0290 |         243.4176 |        -271.9759 |
[32m[20221208 14:10:40 @agent_ppo2.py:179][0m |          -0.0364 |         242.0594 |        -280.6276 |
[32m[20221208 14:10:40 @agent_ppo2.py:179][0m |          -0.0356 |         241.8253 |        -282.3385 |
[32m[20221208 14:10:40 @agent_ppo2.py:179][0m |          -0.0389 |         242.5672 |        -287.8860 |
[32m[20221208 14:10:40 @agent_ppo2.py:179][0m |          -0.0446 |         241.3411 |        -295.6587 |
[32m[20221208 14:10:40 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:10:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 928.73
[32m[20221208 14:10:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 951.77
[32m[20221208 14:10:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 963.01
[32m[20221208 14:10:40 @agent_ppo2.py:137][0m Total time:      25.33 min
[32m[20221208 14:10:40 @agent_ppo2.py:139][0m 2074624 total steps have happened
[32m[20221208 14:10:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1013 --------------------------#
[32m[20221208 14:10:41 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |           0.0837 |         251.0490 |        -234.4409 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |           0.0501 |         246.6646 |        -176.0806 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |           0.0171 |         245.9184 |        -226.1307 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |          -0.0038 |         242.2654 |        -245.2361 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |          -0.0201 |         240.1239 |        -266.0815 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |          -0.0281 |         238.5881 |        -273.2360 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |          -0.0285 |         237.3409 |        -271.6703 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |          -0.0364 |         237.3202 |        -282.6058 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |          -0.0416 |         237.2079 |        -290.2425 |
[32m[20221208 14:10:41 @agent_ppo2.py:179][0m |          -0.0444 |         236.0709 |        -297.7117 |
[32m[20221208 14:10:41 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:10:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 920.32
[32m[20221208 14:10:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 986.01
[32m[20221208 14:10:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.03
[32m[20221208 14:10:42 @agent_ppo2.py:137][0m Total time:      25.36 min
[32m[20221208 14:10:42 @agent_ppo2.py:139][0m 2076672 total steps have happened
[32m[20221208 14:10:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1014 --------------------------#
[32m[20221208 14:10:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:42 @agent_ppo2.py:179][0m |           0.0856 |         246.5384 |        -225.7058 |
[32m[20221208 14:10:42 @agent_ppo2.py:179][0m |           0.1545 |         234.9029 |        -137.9276 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |           0.0623 |         230.0363 |        -140.6991 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |           0.0308 |         222.9026 |        -184.1316 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |           0.0129 |         219.9344 |        -218.7262 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |           0.0033 |         216.5225 |        -237.5471 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |          -0.0140 |         214.8801 |        -257.1035 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |          -0.0209 |         212.6587 |        -263.3184 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |          -0.0285 |         210.6632 |        -275.4306 |
[32m[20221208 14:10:43 @agent_ppo2.py:179][0m |          -0.0336 |         209.6486 |        -278.9237 |
[32m[20221208 14:10:43 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:10:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 945.95
[32m[20221208 14:10:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.79
[32m[20221208 14:10:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 914.40
[32m[20221208 14:10:43 @agent_ppo2.py:137][0m Total time:      25.38 min
[32m[20221208 14:10:43 @agent_ppo2.py:139][0m 2078720 total steps have happened
[32m[20221208 14:10:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1015 --------------------------#
[32m[20221208 14:10:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |           0.0398 |         246.6573 |        -239.8023 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |           0.0358 |         241.3739 |        -189.0969 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |           0.0250 |         239.6760 |        -194.5919 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |          -0.0008 |         239.1213 |        -229.7671 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |          -0.0106 |         238.4207 |        -236.1634 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |          -0.0219 |         237.8777 |        -250.7264 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |          -0.0283 |         237.8222 |        -259.7600 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |          -0.0345 |         237.4873 |        -269.0710 |
[32m[20221208 14:10:44 @agent_ppo2.py:179][0m |          -0.0373 |         236.7275 |        -273.9385 |
[32m[20221208 14:10:45 @agent_ppo2.py:179][0m |          -0.0384 |         236.8673 |        -279.0159 |
[32m[20221208 14:10:45 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:10:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 952.56
[32m[20221208 14:10:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.24
[32m[20221208 14:10:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.93
[32m[20221208 14:10:45 @agent_ppo2.py:137][0m Total time:      25.41 min
[32m[20221208 14:10:45 @agent_ppo2.py:139][0m 2080768 total steps have happened
[32m[20221208 14:10:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1016 --------------------------#
[32m[20221208 14:10:45 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:45 @agent_ppo2.py:179][0m |           0.0537 |         243.8900 |        -237.7760 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |           0.0813 |         241.9248 |        -178.4402 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |           0.0356 |         239.9821 |        -180.8636 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |           0.0045 |         238.8506 |        -220.8212 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |          -0.0087 |         238.4075 |        -238.9688 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |          -0.0208 |         237.0256 |        -258.3576 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |          -0.0305 |         235.6508 |        -268.3837 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |          -0.0361 |         235.1455 |        -275.3043 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |          -0.0277 |         234.9091 |        -271.9463 |
[32m[20221208 14:10:46 @agent_ppo2.py:179][0m |          -0.0348 |         235.3608 |        -280.9990 |
[32m[20221208 14:10:46 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:10:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 942.83
[32m[20221208 14:10:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 959.91
[32m[20221208 14:10:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 872.32
[32m[20221208 14:10:46 @agent_ppo2.py:137][0m Total time:      25.43 min
[32m[20221208 14:10:46 @agent_ppo2.py:139][0m 2082816 total steps have happened
[32m[20221208 14:10:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1017 --------------------------#
[32m[20221208 14:10:47 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |           0.0486 |         247.0247 |        -236.1376 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |           0.0521 |         238.8453 |        -174.7266 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |           0.0044 |         234.7200 |        -214.7647 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |          -0.0121 |         232.8988 |        -235.7014 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |          -0.0227 |         230.8139 |        -243.1960 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |          -0.0323 |         227.5311 |        -255.1913 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |          -0.0334 |         226.8410 |        -250.1959 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |          -0.0432 |         225.4223 |        -259.3032 |
[32m[20221208 14:10:47 @agent_ppo2.py:179][0m |          -0.0473 |         223.5011 |        -266.9406 |
[32m[20221208 14:10:48 @agent_ppo2.py:179][0m |          -0.0485 |         222.6459 |        -271.6921 |
[32m[20221208 14:10:48 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:10:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 865.65
[32m[20221208 14:10:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 917.90
[32m[20221208 14:10:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 946.68
[32m[20221208 14:10:48 @agent_ppo2.py:137][0m Total time:      25.46 min
[32m[20221208 14:10:48 @agent_ppo2.py:139][0m 2084864 total steps have happened
[32m[20221208 14:10:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1018 --------------------------#
[32m[20221208 14:10:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:48 @agent_ppo2.py:179][0m |           0.0560 |         249.4263 |        -226.7544 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |           0.0449 |         239.9928 |        -196.1890 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0019 |         235.3219 |        -237.3467 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0156 |         232.7798 |        -247.8228 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0160 |         230.6480 |        -245.2484 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0247 |         228.7074 |        -260.2417 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0311 |         227.8227 |        -261.9342 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0345 |         226.7609 |        -264.6784 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0386 |         225.5070 |        -270.2774 |
[32m[20221208 14:10:49 @agent_ppo2.py:179][0m |          -0.0446 |         224.2932 |        -276.4847 |
[32m[20221208 14:10:49 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:10:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.40
[32m[20221208 14:10:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.29
[32m[20221208 14:10:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 961.39
[32m[20221208 14:10:49 @agent_ppo2.py:137][0m Total time:      25.48 min
[32m[20221208 14:10:49 @agent_ppo2.py:139][0m 2086912 total steps have happened
[32m[20221208 14:10:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1019 --------------------------#
[32m[20221208 14:10:50 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |           0.1014 |         246.6727 |        -200.2045 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |           0.0904 |         240.5464 |        -109.9931 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |           0.0423 |         238.4628 |        -161.3782 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |           0.0057 |         238.9345 |        -206.4482 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |          -0.0088 |         236.6215 |        -223.0793 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |          -0.0201 |         236.2976 |        -229.2869 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |          -0.0211 |         237.5735 |        -234.5749 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |          -0.0296 |         235.6117 |        -242.4953 |
[32m[20221208 14:10:50 @agent_ppo2.py:179][0m |          -0.0233 |         235.6708 |        -244.6288 |
[32m[20221208 14:10:51 @agent_ppo2.py:179][0m |          -0.0378 |         234.5860 |        -251.3508 |
[32m[20221208 14:10:51 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:10:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 916.75
[32m[20221208 14:10:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 939.70
[32m[20221208 14:10:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 931.24
[32m[20221208 14:10:51 @agent_ppo2.py:137][0m Total time:      25.51 min
[32m[20221208 14:10:51 @agent_ppo2.py:139][0m 2088960 total steps have happened
[32m[20221208 14:10:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1020 --------------------------#
[32m[20221208 14:10:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:51 @agent_ppo2.py:179][0m |           0.0763 |         244.2697 |        -206.6847 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |           0.1876 |         241.8685 |         -97.8464 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |           0.0696 |         240.4533 |        -101.5958 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |           0.0295 |         239.3191 |        -166.0812 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |           0.0026 |         239.1152 |        -208.1022 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |          -0.0074 |         238.3345 |        -222.2740 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |          -0.0055 |         237.5678 |        -220.6391 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |          -0.0220 |         236.7279 |        -235.6277 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |          -0.0308 |         238.8459 |        -248.5072 |
[32m[20221208 14:10:52 @agent_ppo2.py:179][0m |          -0.0359 |         237.4362 |        -254.0089 |
[32m[20221208 14:10:52 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:10:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 959.57
[32m[20221208 14:10:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.39
[32m[20221208 14:10:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 639.81
[32m[20221208 14:10:52 @agent_ppo2.py:137][0m Total time:      25.53 min
[32m[20221208 14:10:52 @agent_ppo2.py:139][0m 2091008 total steps have happened
[32m[20221208 14:10:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1021 --------------------------#
[32m[20221208 14:10:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:10:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |           0.0635 |         240.4006 |        -204.6099 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |           0.0733 |         237.4468 |        -141.0394 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |           0.0366 |         235.0997 |        -162.5237 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |           0.0040 |         235.2125 |        -192.9083 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |          -0.0050 |         234.2125 |        -201.7006 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |          -0.0236 |         233.6032 |        -219.1393 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |          -0.0327 |         232.4235 |        -226.2763 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |          -0.0368 |         231.9976 |        -232.5260 |
[32m[20221208 14:10:53 @agent_ppo2.py:179][0m |          -0.0380 |         232.6850 |        -233.8505 |
[32m[20221208 14:10:54 @agent_ppo2.py:179][0m |          -0.0353 |         232.2202 |        -239.2544 |
[32m[20221208 14:10:54 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:10:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 923.87
[32m[20221208 14:10:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.35
[32m[20221208 14:10:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.50
[32m[20221208 14:10:54 @agent_ppo2.py:137][0m Total time:      25.56 min
[32m[20221208 14:10:54 @agent_ppo2.py:139][0m 2093056 total steps have happened
[32m[20221208 14:10:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1022 --------------------------#
[32m[20221208 14:10:54 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:54 @agent_ppo2.py:179][0m |           0.1010 |         243.9349 |        -179.4399 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |           0.0909 |         241.5367 |         -95.5716 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |           0.0587 |         239.5136 |        -128.9222 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |           0.0329 |         238.2018 |        -165.7916 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |           0.0157 |         238.3773 |        -198.2372 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |          -0.0016 |         237.0978 |        -210.1871 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |          -0.0045 |         235.0430 |        -210.9532 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |          -0.0173 |         235.6074 |        -218.9382 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |          -0.0287 |         235.2280 |        -231.0965 |
[32m[20221208 14:10:55 @agent_ppo2.py:179][0m |          -0.0328 |         233.0501 |        -235.5039 |
[32m[20221208 14:10:55 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:10:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 946.26
[32m[20221208 14:10:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 970.02
[32m[20221208 14:10:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.09
[32m[20221208 14:10:55 @agent_ppo2.py:137][0m Total time:      25.59 min
[32m[20221208 14:10:55 @agent_ppo2.py:139][0m 2095104 total steps have happened
[32m[20221208 14:10:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1023 --------------------------#
[32m[20221208 14:10:56 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |           0.0637 |         248.1110 |        -193.7775 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |           0.0588 |         234.0009 |        -145.9469 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |           0.0153 |         229.0668 |        -181.8942 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |          -0.0096 |         224.9844 |        -197.1899 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |          -0.0202 |         222.7850 |        -210.0927 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |          -0.0300 |         220.5303 |        -216.7064 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |          -0.0354 |         220.5094 |        -219.9396 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |          -0.0423 |         218.9821 |        -225.1489 |
[32m[20221208 14:10:56 @agent_ppo2.py:179][0m |          -0.0382 |         219.0286 |        -223.6541 |
[32m[20221208 14:10:57 @agent_ppo2.py:179][0m |          -0.0413 |         215.9831 |        -229.5190 |
[32m[20221208 14:10:57 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:10:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.20
[32m[20221208 14:10:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.33
[32m[20221208 14:10:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 968.31
[32m[20221208 14:10:57 @agent_ppo2.py:137][0m Total time:      25.61 min
[32m[20221208 14:10:57 @agent_ppo2.py:139][0m 2097152 total steps have happened
[32m[20221208 14:10:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1024 --------------------------#
[32m[20221208 14:10:57 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:10:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:57 @agent_ppo2.py:179][0m |           0.0515 |         247.2257 |        -199.4973 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |           0.0456 |         237.8849 |        -167.5781 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |           0.0076 |         233.4495 |        -197.3273 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |          -0.0125 |         231.2133 |        -211.4787 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |          -0.0212 |         229.1316 |        -219.5203 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |          -0.0322 |         228.5239 |        -227.6352 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |          -0.0370 |         226.7364 |        -230.4802 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |          -0.0422 |         225.5636 |        -236.1378 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |          -0.0439 |         224.5055 |        -242.0462 |
[32m[20221208 14:10:58 @agent_ppo2.py:179][0m |          -0.0461 |         224.0794 |        -246.5076 |
[32m[20221208 14:10:58 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:10:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 944.35
[32m[20221208 14:10:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.66
[32m[20221208 14:10:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 954.34
[32m[20221208 14:10:58 @agent_ppo2.py:137][0m Total time:      25.63 min
[32m[20221208 14:10:58 @agent_ppo2.py:139][0m 2099200 total steps have happened
[32m[20221208 14:10:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1025 --------------------------#
[32m[20221208 14:10:59 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:10:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |           0.0559 |         243.1587 |        -186.4755 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |           0.0888 |         238.9344 |        -148.6678 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |           0.0301 |         237.4748 |        -182.0492 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |           0.0148 |         237.8869 |        -190.5782 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |          -0.0128 |         237.7892 |        -205.6082 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |          -0.0201 |         235.7425 |        -217.1638 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |          -0.0269 |         235.5095 |        -223.1714 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |          -0.0338 |         235.8953 |        -232.5815 |
[32m[20221208 14:10:59 @agent_ppo2.py:179][0m |          -0.0410 |         235.1750 |        -239.6568 |
[32m[20221208 14:11:00 @agent_ppo2.py:179][0m |          -0.0437 |         234.5800 |        -243.6916 |
[32m[20221208 14:11:00 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:11:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 909.23
[32m[20221208 14:11:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 931.05
[32m[20221208 14:11:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 983.00
[32m[20221208 14:11:00 @agent_ppo2.py:137][0m Total time:      25.66 min
[32m[20221208 14:11:00 @agent_ppo2.py:139][0m 2101248 total steps have happened
[32m[20221208 14:11:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1026 --------------------------#
[32m[20221208 14:11:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |           0.0964 |         236.2146 |        -195.7595 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |           0.0691 |         210.4819 |        -131.3682 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |           0.0191 |         194.2495 |        -177.8561 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |          -0.0045 |         185.3946 |        -203.3042 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |          -0.0156 |         177.8333 |        -212.7597 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |          -0.0255 |         173.8240 |        -221.1405 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |          -0.0293 |         169.6745 |        -227.9618 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |          -0.0373 |         165.6877 |        -235.4205 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |          -0.0391 |         162.5887 |        -239.4870 |
[32m[20221208 14:11:01 @agent_ppo2.py:179][0m |          -0.0428 |         161.1671 |        -244.5364 |
[32m[20221208 14:11:01 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:11:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 940.22
[32m[20221208 14:11:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.40
[32m[20221208 14:11:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 618.29
[32m[20221208 14:11:02 @agent_ppo2.py:137][0m Total time:      25.69 min
[32m[20221208 14:11:02 @agent_ppo2.py:139][0m 2103296 total steps have happened
[32m[20221208 14:11:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1027 --------------------------#
[32m[20221208 14:11:02 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:02 @agent_ppo2.py:179][0m |           0.0633 |         293.8289 |        -214.4275 |
[32m[20221208 14:11:02 @agent_ppo2.py:179][0m |           0.0621 |         256.4800 |        -176.3685 |
[32m[20221208 14:11:02 @agent_ppo2.py:179][0m |           0.0080 |         246.1350 |        -187.3843 |
[32m[20221208 14:11:02 @agent_ppo2.py:179][0m |          -0.0235 |         240.1980 |        -211.8401 |
[32m[20221208 14:11:02 @agent_ppo2.py:179][0m |          -0.0350 |         235.9866 |        -223.0912 |
[32m[20221208 14:11:02 @agent_ppo2.py:179][0m |          -0.0371 |         231.4575 |        -224.8971 |
[32m[20221208 14:11:02 @agent_ppo2.py:179][0m |          -0.0481 |         229.5159 |        -234.7058 |
[32m[20221208 14:11:03 @agent_ppo2.py:179][0m |          -0.0509 |         226.0032 |        -235.0283 |
[32m[20221208 14:11:03 @agent_ppo2.py:179][0m |          -0.0554 |         224.0123 |        -238.0169 |
[32m[20221208 14:11:03 @agent_ppo2.py:179][0m |          -0.0608 |         222.3651 |        -243.6613 |
[32m[20221208 14:11:03 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:11:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 933.57
[32m[20221208 14:11:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.60
[32m[20221208 14:11:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 937.18
[32m[20221208 14:11:03 @agent_ppo2.py:137][0m Total time:      25.71 min
[32m[20221208 14:11:03 @agent_ppo2.py:139][0m 2105344 total steps have happened
[32m[20221208 14:11:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1028 --------------------------#
[32m[20221208 14:11:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |           0.0588 |         256.8152 |        -226.8731 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |           0.0519 |         250.7823 |        -194.3698 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |           0.0070 |         246.5980 |        -200.6844 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |          -0.0195 |         244.7084 |        -227.4933 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |          -0.0344 |         244.8579 |        -235.4746 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |          -0.0394 |         243.8245 |        -240.8676 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |          -0.0440 |         243.0207 |        -251.8657 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |          -0.0488 |         242.3802 |        -257.7620 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |          -0.0500 |         241.8446 |        -260.1811 |
[32m[20221208 14:11:04 @agent_ppo2.py:179][0m |          -0.0499 |         242.5074 |        -263.6935 |
[32m[20221208 14:11:04 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:11:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 911.05
[32m[20221208 14:11:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.32
[32m[20221208 14:11:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 906.96
[32m[20221208 14:11:05 @agent_ppo2.py:137][0m Total time:      25.74 min
[32m[20221208 14:11:05 @agent_ppo2.py:139][0m 2107392 total steps have happened
[32m[20221208 14:11:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1029 --------------------------#
[32m[20221208 14:11:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:05 @agent_ppo2.py:179][0m |           0.1028 |         258.6780 |        -226.6777 |
[32m[20221208 14:11:05 @agent_ppo2.py:179][0m |           0.0470 |         252.4494 |        -182.4167 |
[32m[20221208 14:11:05 @agent_ppo2.py:179][0m |           0.0121 |         245.7957 |        -208.9296 |
[32m[20221208 14:11:05 @agent_ppo2.py:179][0m |          -0.0149 |         242.6159 |        -233.4848 |
[32m[20221208 14:11:05 @agent_ppo2.py:179][0m |          -0.0296 |         240.6777 |        -247.6757 |
[32m[20221208 14:11:05 @agent_ppo2.py:179][0m |          -0.0381 |         238.9372 |        -260.9722 |
[32m[20221208 14:11:06 @agent_ppo2.py:179][0m |          -0.0381 |         239.2012 |        -260.8096 |
[32m[20221208 14:11:06 @agent_ppo2.py:179][0m |          -0.0432 |         237.3433 |        -269.1801 |
[32m[20221208 14:11:06 @agent_ppo2.py:179][0m |          -0.0438 |         236.7111 |        -274.2144 |
[32m[20221208 14:11:06 @agent_ppo2.py:179][0m |          -0.0472 |         235.8533 |        -275.7628 |
[32m[20221208 14:11:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:11:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 924.55
[32m[20221208 14:11:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 942.65
[32m[20221208 14:11:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 943.28
[32m[20221208 14:11:06 @agent_ppo2.py:137][0m Total time:      25.76 min
[32m[20221208 14:11:06 @agent_ppo2.py:139][0m 2109440 total steps have happened
[32m[20221208 14:11:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1030 --------------------------#
[32m[20221208 14:11:07 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |           0.0569 |         241.0577 |        -204.6080 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |           0.0707 |         225.6096 |        -139.5826 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |           0.0294 |         221.5309 |        -192.2437 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |           0.0015 |         219.7940 |        -220.0726 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |          -0.0120 |         218.8951 |        -231.5826 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |          -0.0209 |         218.2915 |        -242.8047 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |          -0.0254 |         217.1309 |        -245.7459 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |          -0.0347 |         216.8615 |        -253.7167 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |          -0.0381 |         215.3286 |        -256.4395 |
[32m[20221208 14:11:07 @agent_ppo2.py:179][0m |          -0.0420 |         214.7321 |        -258.7353 |
[32m[20221208 14:11:07 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:11:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.81
[32m[20221208 14:11:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 963.91
[32m[20221208 14:11:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 905.91
[32m[20221208 14:11:08 @agent_ppo2.py:137][0m Total time:      25.79 min
[32m[20221208 14:11:08 @agent_ppo2.py:139][0m 2111488 total steps have happened
[32m[20221208 14:11:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1031 --------------------------#
[32m[20221208 14:11:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:08 @agent_ppo2.py:179][0m |           0.0630 |         229.9467 |        -178.3969 |
[32m[20221208 14:11:08 @agent_ppo2.py:179][0m |           0.0012 |         221.6118 |        -159.6705 |
[32m[20221208 14:11:08 @agent_ppo2.py:179][0m |          -0.0265 |         219.3664 |        -176.3935 |
[32m[20221208 14:11:08 @agent_ppo2.py:179][0m |          -0.0348 |         216.7013 |        -172.8866 |
[32m[20221208 14:11:08 @agent_ppo2.py:179][0m |          -0.0580 |         213.5281 |        -192.5640 |
[32m[20221208 14:11:09 @agent_ppo2.py:179][0m |          -0.0667 |         212.2222 |        -200.9919 |
[32m[20221208 14:11:09 @agent_ppo2.py:179][0m |          -0.0701 |         211.6101 |        -202.3175 |
[32m[20221208 14:11:09 @agent_ppo2.py:179][0m |          -0.0746 |         210.5386 |        -209.3105 |
[32m[20221208 14:11:09 @agent_ppo2.py:179][0m |          -0.0783 |         210.7128 |        -213.0650 |
[32m[20221208 14:11:09 @agent_ppo2.py:179][0m |          -0.0801 |         212.0970 |        -215.2265 |
[32m[20221208 14:11:09 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:11:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 604.45
[32m[20221208 14:11:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.26
[32m[20221208 14:11:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.00
[32m[20221208 14:11:09 @agent_ppo2.py:137][0m Total time:      25.81 min
[32m[20221208 14:11:09 @agent_ppo2.py:139][0m 2113536 total steps have happened
[32m[20221208 14:11:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1032 --------------------------#
[32m[20221208 14:11:10 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |           0.0806 |         219.4494 |        -179.2098 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |           0.0172 |         206.9691 |        -153.4746 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0192 |         204.9468 |        -181.1538 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0447 |         203.2053 |        -201.7147 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0588 |         201.2892 |        -209.3813 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0682 |         202.0512 |        -217.3804 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0717 |         200.7457 |        -223.0002 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0769 |         199.2987 |        -229.0499 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0817 |         199.2592 |        -237.4600 |
[32m[20221208 14:11:10 @agent_ppo2.py:179][0m |          -0.0842 |         197.9100 |        -241.1959 |
[32m[20221208 14:11:10 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:11:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 637.18
[32m[20221208 14:11:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 949.21
[32m[20221208 14:11:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 674.71
[32m[20221208 14:11:11 @agent_ppo2.py:137][0m Total time:      25.84 min
[32m[20221208 14:11:11 @agent_ppo2.py:139][0m 2115584 total steps have happened
[32m[20221208 14:11:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1033 --------------------------#
[32m[20221208 14:11:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:11:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:11 @agent_ppo2.py:179][0m |           0.0617 |         245.1809 |        -254.1030 |
[32m[20221208 14:11:11 @agent_ppo2.py:179][0m |           0.0889 |         239.8184 |        -184.6087 |
[32m[20221208 14:11:11 @agent_ppo2.py:179][0m |           0.0256 |         238.4599 |        -231.8826 |
[32m[20221208 14:11:11 @agent_ppo2.py:179][0m |          -0.0103 |         236.1879 |        -269.7863 |
[32m[20221208 14:11:12 @agent_ppo2.py:179][0m |          -0.0182 |         235.1519 |        -275.3573 |
[32m[20221208 14:11:12 @agent_ppo2.py:179][0m |          -0.0282 |         234.7960 |        -281.9364 |
[32m[20221208 14:11:12 @agent_ppo2.py:179][0m |          -0.0329 |         233.9697 |        -292.2845 |
[32m[20221208 14:11:12 @agent_ppo2.py:179][0m |          -0.0190 |         233.8339 |        -279.1561 |
[32m[20221208 14:11:12 @agent_ppo2.py:179][0m |          -0.0366 |         232.9299 |        -294.9776 |
[32m[20221208 14:11:12 @agent_ppo2.py:179][0m |          -0.0416 |         232.1483 |        -297.6661 |
[32m[20221208 14:11:12 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:11:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 901.21
[32m[20221208 14:11:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 910.90
[32m[20221208 14:11:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 919.93
[32m[20221208 14:11:12 @agent_ppo2.py:137][0m Total time:      25.87 min
[32m[20221208 14:11:12 @agent_ppo2.py:139][0m 2117632 total steps have happened
[32m[20221208 14:11:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1034 --------------------------#
[32m[20221208 14:11:13 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |           0.0604 |         246.3185 |        -252.6486 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |           0.0652 |         238.7004 |        -172.2152 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |           0.0294 |         236.3800 |        -205.5429 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |           0.0178 |         235.4227 |        -222.2920 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |           0.0102 |         234.3984 |        -216.7024 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |          -0.0135 |         234.6926 |        -243.2940 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |          -0.0280 |         232.8108 |        -260.1061 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |          -0.0348 |         232.1544 |        -269.3689 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |          -0.0404 |         232.9079 |        -282.8887 |
[32m[20221208 14:11:13 @agent_ppo2.py:179][0m |          -0.0413 |         231.5416 |        -283.0244 |
[32m[20221208 14:11:13 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:11:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 891.95
[32m[20221208 14:11:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 923.73
[32m[20221208 14:11:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 953.35
[32m[20221208 14:11:14 @agent_ppo2.py:137][0m Total time:      25.89 min
[32m[20221208 14:11:14 @agent_ppo2.py:139][0m 2119680 total steps have happened
[32m[20221208 14:11:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1035 --------------------------#
[32m[20221208 14:11:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:14 @agent_ppo2.py:179][0m |           0.0538 |         232.5571 |        -261.1355 |
[32m[20221208 14:11:14 @agent_ppo2.py:179][0m |           0.0172 |         215.2970 |        -228.1976 |
[32m[20221208 14:11:14 @agent_ppo2.py:179][0m |           0.0032 |         208.7033 |        -244.3813 |
[32m[20221208 14:11:15 @agent_ppo2.py:179][0m |          -0.0175 |         202.3695 |        -261.0614 |
[32m[20221208 14:11:15 @agent_ppo2.py:179][0m |          -0.0194 |         198.9368 |        -261.0190 |
[32m[20221208 14:11:15 @agent_ppo2.py:179][0m |          -0.0303 |         197.8445 |        -274.9669 |
[32m[20221208 14:11:15 @agent_ppo2.py:179][0m |          -0.0338 |         195.1494 |        -283.6352 |
[32m[20221208 14:11:15 @agent_ppo2.py:179][0m |          -0.0431 |         192.7680 |        -297.8157 |
[32m[20221208 14:11:15 @agent_ppo2.py:179][0m |          -0.0449 |         191.3499 |        -307.0860 |
[32m[20221208 14:11:15 @agent_ppo2.py:179][0m |          -0.0473 |         190.3764 |        -315.8536 |
[32m[20221208 14:11:15 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:11:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 903.94
[32m[20221208 14:11:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.05
[32m[20221208 14:11:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 944.04
[32m[20221208 14:11:15 @agent_ppo2.py:137][0m Total time:      25.92 min
[32m[20221208 14:11:15 @agent_ppo2.py:139][0m 2121728 total steps have happened
[32m[20221208 14:11:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1036 --------------------------#
[32m[20221208 14:11:16 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |           0.0808 |         252.5744 |        -270.0782 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |           0.0821 |         243.1642 |        -168.8080 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |           0.0271 |         240.1863 |        -232.9498 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |           0.0003 |         237.9957 |        -270.6057 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |          -0.0044 |         237.5667 |        -281.2615 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |          -0.0242 |         236.1101 |        -299.0862 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |          -0.0307 |         235.6213 |        -306.4340 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |          -0.0339 |         234.9047 |        -311.9052 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |          -0.0363 |         234.7406 |        -311.9438 |
[32m[20221208 14:11:16 @agent_ppo2.py:179][0m |          -0.0422 |         233.2310 |        -324.4740 |
[32m[20221208 14:11:16 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:11:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 941.23
[32m[20221208 14:11:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.03
[32m[20221208 14:11:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.07
[32m[20221208 14:11:17 @agent_ppo2.py:137][0m Total time:      25.94 min
[32m[20221208 14:11:17 @agent_ppo2.py:139][0m 2123776 total steps have happened
[32m[20221208 14:11:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1037 --------------------------#
[32m[20221208 14:11:17 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:17 @agent_ppo2.py:179][0m |           0.1149 |         244.3677 |        -221.3902 |
[32m[20221208 14:11:17 @agent_ppo2.py:179][0m |           0.0606 |         236.8449 |        -171.9347 |
[32m[20221208 14:11:17 @agent_ppo2.py:179][0m |           0.0223 |         231.5300 |        -216.1473 |
[32m[20221208 14:11:18 @agent_ppo2.py:179][0m |           0.0051 |         227.7562 |        -254.5716 |
[32m[20221208 14:11:18 @agent_ppo2.py:179][0m |          -0.0092 |         224.7171 |        -265.8634 |
[32m[20221208 14:11:18 @agent_ppo2.py:179][0m |          -0.0194 |         223.3874 |        -284.8603 |
[32m[20221208 14:11:18 @agent_ppo2.py:179][0m |          -0.0287 |         219.7765 |        -292.6950 |
[32m[20221208 14:11:18 @agent_ppo2.py:179][0m |          -0.0364 |         217.9634 |        -301.0751 |
[32m[20221208 14:11:18 @agent_ppo2.py:179][0m |          -0.0407 |         217.0296 |        -305.4774 |
[32m[20221208 14:11:18 @agent_ppo2.py:179][0m |          -0.0407 |         215.2392 |        -307.8961 |
[32m[20221208 14:11:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:11:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.69
[32m[20221208 14:11:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.31
[32m[20221208 14:11:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 951.11
[32m[20221208 14:11:18 @agent_ppo2.py:137][0m Total time:      25.97 min
[32m[20221208 14:11:18 @agent_ppo2.py:139][0m 2125824 total steps have happened
[32m[20221208 14:11:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1038 --------------------------#
[32m[20221208 14:11:19 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |           0.0703 |         257.3576 |        -242.8884 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |           0.0908 |         251.6150 |        -167.7104 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |           0.0580 |         249.3873 |        -166.4128 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |           0.0193 |         246.3494 |        -219.0836 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |          -0.0052 |         245.2593 |        -253.6848 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |          -0.0153 |         244.9817 |        -268.1030 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |          -0.0235 |         243.2022 |        -274.2910 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |          -0.0306 |         242.6264 |        -288.7088 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |          -0.0345 |         242.5962 |        -289.8215 |
[32m[20221208 14:11:19 @agent_ppo2.py:179][0m |          -0.0364 |         241.9464 |        -301.3090 |
[32m[20221208 14:11:19 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:11:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 974.52
[32m[20221208 14:11:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.53
[32m[20221208 14:11:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.94
[32m[20221208 14:11:20 @agent_ppo2.py:137][0m Total time:      25.99 min
[32m[20221208 14:11:20 @agent_ppo2.py:139][0m 2127872 total steps have happened
[32m[20221208 14:11:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1039 --------------------------#
[32m[20221208 14:11:20 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:20 @agent_ppo2.py:179][0m |           0.0660 |         247.1424 |        -237.8650 |
[32m[20221208 14:11:20 @agent_ppo2.py:179][0m |           0.0926 |         241.3825 |        -123.6815 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |           0.0519 |         238.5226 |        -155.4516 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |           0.0047 |         236.4432 |        -234.8935 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |          -0.0107 |         235.5891 |        -262.9738 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |          -0.0226 |         234.2072 |        -288.0138 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |          -0.0327 |         233.6526 |        -299.7955 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |          -0.0320 |         233.7059 |        -300.9536 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |          -0.0347 |         231.5629 |        -311.5569 |
[32m[20221208 14:11:21 @agent_ppo2.py:179][0m |          -0.0397 |         231.6979 |        -317.8151 |
[32m[20221208 14:11:21 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:11:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.64
[32m[20221208 14:11:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.81
[32m[20221208 14:11:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 942.11
[32m[20221208 14:11:21 @agent_ppo2.py:137][0m Total time:      26.02 min
[32m[20221208 14:11:21 @agent_ppo2.py:139][0m 2129920 total steps have happened
[32m[20221208 14:11:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1040 --------------------------#
[32m[20221208 14:11:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |           0.0620 |         250.5176 |        -243.5970 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |           0.0451 |         248.0561 |        -188.6436 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |           0.0037 |         246.5426 |        -262.7375 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |           0.0085 |         244.3352 |        -244.7455 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |          -0.0044 |         243.0275 |        -265.9504 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |          -0.0270 |         242.3274 |        -283.8728 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |          -0.0352 |         241.9610 |        -293.3142 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |          -0.0386 |         241.6120 |        -295.9922 |
[32m[20221208 14:11:22 @agent_ppo2.py:179][0m |          -0.0391 |         240.9828 |        -302.8336 |
[32m[20221208 14:11:23 @agent_ppo2.py:179][0m |          -0.0422 |         240.0037 |        -309.1198 |
[32m[20221208 14:11:23 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:11:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.49
[32m[20221208 14:11:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.39
[32m[20221208 14:11:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.93
[32m[20221208 14:11:23 @agent_ppo2.py:137][0m Total time:      26.04 min
[32m[20221208 14:11:23 @agent_ppo2.py:139][0m 2131968 total steps have happened
[32m[20221208 14:11:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1041 --------------------------#
[32m[20221208 14:11:23 @agent_ppo2.py:121][0m Sampling time: 0.52 s by 1 slaves
[32m[20221208 14:11:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |           0.1311 |         246.2088 |        -239.5655 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |           0.0448 |         238.2793 |        -193.6453 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |           0.0170 |         235.9408 |        -223.8057 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |          -0.0015 |         234.2142 |        -247.3675 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |          -0.0160 |         233.9544 |        -259.5851 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |          -0.0254 |         232.4223 |        -272.4378 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |          -0.0275 |         232.5361 |        -271.8483 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |          -0.0308 |         231.2845 |        -277.2090 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |          -0.0366 |         232.7524 |        -290.3246 |
[32m[20221208 14:11:24 @agent_ppo2.py:179][0m |          -0.0409 |         231.2955 |        -293.1651 |
[32m[20221208 14:11:24 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:11:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 934.27
[32m[20221208 14:11:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 955.24
[32m[20221208 14:11:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.15
[32m[20221208 14:11:25 @agent_ppo2.py:137][0m Total time:      26.07 min
[32m[20221208 14:11:25 @agent_ppo2.py:139][0m 2134016 total steps have happened
[32m[20221208 14:11:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1042 --------------------------#
[32m[20221208 14:11:25 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:25 @agent_ppo2.py:179][0m |           0.0559 |         250.6554 |        -258.7470 |
[32m[20221208 14:11:25 @agent_ppo2.py:179][0m |           0.0535 |         243.6446 |        -186.5138 |
[32m[20221208 14:11:25 @agent_ppo2.py:179][0m |           0.0208 |         237.2502 |        -227.4026 |
[32m[20221208 14:11:25 @agent_ppo2.py:179][0m |          -0.0013 |         234.8969 |        -245.3322 |
[32m[20221208 14:11:25 @agent_ppo2.py:179][0m |          -0.0171 |         233.2324 |        -266.4731 |
[32m[20221208 14:11:25 @agent_ppo2.py:179][0m |          -0.0224 |         230.5086 |        -277.1164 |
[32m[20221208 14:11:26 @agent_ppo2.py:179][0m |          -0.0309 |         230.1720 |        -288.6952 |
[32m[20221208 14:11:26 @agent_ppo2.py:179][0m |          -0.0345 |         228.9302 |        -296.3980 |
[32m[20221208 14:11:26 @agent_ppo2.py:179][0m |          -0.0327 |         227.7537 |        -299.1820 |
[32m[20221208 14:11:26 @agent_ppo2.py:179][0m |          -0.0341 |         228.4810 |        -303.9200 |
[32m[20221208 14:11:26 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:11:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 957.30
[32m[20221208 14:11:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 973.73
[32m[20221208 14:11:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.42
[32m[20221208 14:11:26 @agent_ppo2.py:137][0m Total time:      26.10 min
[32m[20221208 14:11:26 @agent_ppo2.py:139][0m 2136064 total steps have happened
[32m[20221208 14:11:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1043 --------------------------#
[32m[20221208 14:11:27 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |           0.0952 |         246.6294 |        -224.8827 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |           0.0828 |         245.3390 |        -139.6883 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |           0.0742 |         243.2193 |        -146.0402 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |           0.0371 |         241.3026 |        -167.0104 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |           0.0055 |         240.9796 |        -231.6513 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |           0.0010 |         239.7472 |        -247.0342 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |          -0.0062 |         239.2301 |        -253.5314 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |          -0.0226 |         238.7490 |        -277.6600 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |          -0.0302 |         238.6799 |        -284.1345 |
[32m[20221208 14:11:27 @agent_ppo2.py:179][0m |          -0.0323 |         238.4944 |        -289.6040 |
[32m[20221208 14:11:27 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:11:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 966.01
[32m[20221208 14:11:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.28
[32m[20221208 14:11:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 986.86
[32m[20221208 14:11:28 @agent_ppo2.py:137][0m Total time:      26.12 min
[32m[20221208 14:11:28 @agent_ppo2.py:139][0m 2138112 total steps have happened
[32m[20221208 14:11:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1044 --------------------------#
[32m[20221208 14:11:28 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:28 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:28 @agent_ppo2.py:179][0m |           0.1241 |         248.8366 |        -245.0750 |
[32m[20221208 14:11:28 @agent_ppo2.py:179][0m |           0.0680 |         243.4650 |        -181.1045 |
[32m[20221208 14:11:28 @agent_ppo2.py:179][0m |           0.0212 |         241.1279 |        -230.9305 |
[32m[20221208 14:11:28 @agent_ppo2.py:179][0m |           0.0084 |         239.5924 |        -253.9921 |
[32m[20221208 14:11:28 @agent_ppo2.py:179][0m |          -0.0106 |         238.7155 |        -263.6481 |
[32m[20221208 14:11:28 @agent_ppo2.py:179][0m |          -0.0236 |         238.4868 |        -276.6625 |
[32m[20221208 14:11:29 @agent_ppo2.py:179][0m |          -0.0282 |         236.6357 |        -288.5744 |
[32m[20221208 14:11:29 @agent_ppo2.py:179][0m |          -0.0302 |         236.6961 |        -290.7339 |
[32m[20221208 14:11:29 @agent_ppo2.py:179][0m |          -0.0363 |         234.4849 |        -296.8771 |
[32m[20221208 14:11:29 @agent_ppo2.py:179][0m |          -0.0392 |         233.3848 |        -302.7452 |
[32m[20221208 14:11:29 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:11:29 @agent_ppo2.py:132][0m Average TRAINING episode reward: 940.68
[32m[20221208 14:11:29 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.83
[32m[20221208 14:11:29 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 944.17
[32m[20221208 14:11:29 @agent_ppo2.py:137][0m Total time:      26.15 min
[32m[20221208 14:11:29 @agent_ppo2.py:139][0m 2140160 total steps have happened
[32m[20221208 14:11:29 @agent_ppo2.py:115][0m #------------------------ Iteration 1045 --------------------------#
[32m[20221208 14:11:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |           0.0948 |         217.9050 |        -194.5880 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |           0.0298 |         211.2882 |        -150.2132 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0121 |         208.9319 |        -172.3713 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0361 |         208.2112 |        -184.3583 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0525 |         207.5345 |        -195.0934 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0576 |         205.7660 |        -202.6972 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0660 |         205.9922 |        -209.0240 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0650 |         205.0967 |        -209.6101 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0675 |         204.8694 |        -210.0733 |
[32m[20221208 14:11:30 @agent_ppo2.py:179][0m |          -0.0736 |         204.3417 |        -221.0228 |
[32m[20221208 14:11:30 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:11:31 @agent_ppo2.py:132][0m Average TRAINING episode reward: 668.04
[32m[20221208 14:11:31 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 985.33
[32m[20221208 14:11:31 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 936.50
[32m[20221208 14:11:31 @agent_ppo2.py:137][0m Total time:      26.17 min
[32m[20221208 14:11:31 @agent_ppo2.py:139][0m 2142208 total steps have happened
[32m[20221208 14:11:31 @agent_ppo2.py:115][0m #------------------------ Iteration 1046 --------------------------#
[32m[20221208 14:11:31 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:31 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:31 @agent_ppo2.py:179][0m |           0.1230 |         249.3855 |        -262.6146 |
[32m[20221208 14:11:31 @agent_ppo2.py:179][0m |           0.0687 |         242.9871 |        -157.6585 |
[32m[20221208 14:11:31 @agent_ppo2.py:179][0m |           0.0198 |         239.8894 |        -220.1132 |
[32m[20221208 14:11:31 @agent_ppo2.py:179][0m |           0.0007 |         231.8187 |        -258.8923 |
[32m[20221208 14:11:32 @agent_ppo2.py:179][0m |          -0.0097 |         228.5840 |        -272.2063 |
[32m[20221208 14:11:32 @agent_ppo2.py:179][0m |          -0.0225 |         224.7924 |        -285.2330 |
[32m[20221208 14:11:32 @agent_ppo2.py:179][0m |          -0.0277 |         224.3581 |        -293.8819 |
[32m[20221208 14:11:32 @agent_ppo2.py:179][0m |          -0.0335 |         222.3890 |        -300.8228 |
[32m[20221208 14:11:32 @agent_ppo2.py:179][0m |          -0.0372 |         221.6810 |        -305.0850 |
[32m[20221208 14:11:32 @agent_ppo2.py:179][0m |          -0.0367 |         219.1116 |        -309.4930 |
[32m[20221208 14:11:32 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:11:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 961.51
[32m[20221208 14:11:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.99
[32m[20221208 14:11:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 910.96
[32m[20221208 14:11:32 @agent_ppo2.py:137][0m Total time:      26.20 min
[32m[20221208 14:11:32 @agent_ppo2.py:139][0m 2144256 total steps have happened
[32m[20221208 14:11:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1047 --------------------------#
[32m[20221208 14:11:33 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:33 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |           0.0652 |         233.6926 |        -256.9652 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |           0.0326 |         222.9456 |        -232.6638 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0021 |         217.3599 |        -256.1572 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0204 |         213.3351 |        -279.4025 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0254 |         206.5850 |        -282.1135 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0358 |         200.9307 |        -292.8862 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0400 |         196.9960 |        -295.1214 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0416 |         191.7496 |        -300.7966 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0432 |         190.2867 |        -306.7348 |
[32m[20221208 14:11:33 @agent_ppo2.py:179][0m |          -0.0457 |         187.4544 |        -309.1946 |
[32m[20221208 14:11:33 @agent_ppo2.py:124][0m Policy update time: 0.65 s
[32m[20221208 14:11:34 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.38
[32m[20221208 14:11:34 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.82
[32m[20221208 14:11:34 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.78
[32m[20221208 14:11:34 @agent_ppo2.py:137][0m Total time:      26.22 min
[32m[20221208 14:11:34 @agent_ppo2.py:139][0m 2146304 total steps have happened
[32m[20221208 14:11:34 @agent_ppo2.py:115][0m #------------------------ Iteration 1048 --------------------------#
[32m[20221208 14:11:34 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:34 @agent_ppo2.py:179][0m |           0.1045 |         253.2166 |        -270.0187 |
[32m[20221208 14:11:34 @agent_ppo2.py:179][0m |           0.0317 |         237.7730 |        -259.4474 |
[32m[20221208 14:11:34 @agent_ppo2.py:179][0m |           0.0047 |         232.0708 |        -281.9559 |
[32m[20221208 14:11:34 @agent_ppo2.py:179][0m |          -0.0069 |         230.9686 |        -293.0029 |
[32m[20221208 14:11:34 @agent_ppo2.py:179][0m |          -0.0083 |         228.5377 |        -290.5789 |
[32m[20221208 14:11:35 @agent_ppo2.py:179][0m |          -0.0147 |         225.9035 |        -302.5480 |
[32m[20221208 14:11:35 @agent_ppo2.py:179][0m |          -0.0329 |         224.4258 |        -322.4715 |
[32m[20221208 14:11:35 @agent_ppo2.py:179][0m |          -0.0423 |         223.2285 |        -332.0431 |
[32m[20221208 14:11:35 @agent_ppo2.py:179][0m |          -0.0428 |         221.3384 |        -337.6184 |
[32m[20221208 14:11:35 @agent_ppo2.py:179][0m |          -0.0471 |         220.5695 |        -342.6213 |
[32m[20221208 14:11:35 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:11:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.43
[32m[20221208 14:11:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 943.18
[32m[20221208 14:11:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.94
[32m[20221208 14:11:35 @agent_ppo2.py:137][0m Total time:      26.25 min
[32m[20221208 14:11:35 @agent_ppo2.py:139][0m 2148352 total steps have happened
[32m[20221208 14:11:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1049 --------------------------#
[32m[20221208 14:11:36 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |           0.0690 |         249.7356 |        -256.7654 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |           0.0683 |         234.2883 |        -182.7314 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |           0.0208 |         229.9349 |        -224.0912 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |          -0.0016 |         227.1641 |        -270.1702 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |          -0.0087 |         224.6058 |        -272.3997 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |          -0.0197 |         222.9997 |        -285.9159 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |          -0.0313 |         219.7693 |        -306.4190 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |          -0.0360 |         218.0731 |        -309.9670 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |          -0.0391 |         217.3382 |        -318.1341 |
[32m[20221208 14:11:36 @agent_ppo2.py:179][0m |          -0.0423 |         217.6938 |        -327.1192 |
[32m[20221208 14:11:36 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:11:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 956.14
[32m[20221208 14:11:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.03
[32m[20221208 14:11:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 935.42
[32m[20221208 14:11:37 @agent_ppo2.py:137][0m Total time:      26.27 min
[32m[20221208 14:11:37 @agent_ppo2.py:139][0m 2150400 total steps have happened
[32m[20221208 14:11:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1050 --------------------------#
[32m[20221208 14:11:37 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:37 @agent_ppo2.py:179][0m |           0.0572 |         267.1439 |        -266.1154 |
[32m[20221208 14:11:37 @agent_ppo2.py:179][0m |           0.0485 |         252.9747 |        -217.0891 |
[32m[20221208 14:11:37 @agent_ppo2.py:179][0m |           0.0064 |         247.5131 |        -278.9589 |
[32m[20221208 14:11:37 @agent_ppo2.py:179][0m |          -0.0068 |         245.2928 |        -296.6893 |
[32m[20221208 14:11:37 @agent_ppo2.py:179][0m |           0.0175 |         243.3866 |        -277.6486 |
[32m[20221208 14:11:38 @agent_ppo2.py:179][0m |          -0.0201 |         243.1201 |        -302.0614 |
[32m[20221208 14:11:38 @agent_ppo2.py:179][0m |          -0.0227 |         242.6828 |        -313.6322 |
[32m[20221208 14:11:38 @agent_ppo2.py:179][0m |          -0.0379 |         241.7148 |        -326.3007 |
[32m[20221208 14:11:38 @agent_ppo2.py:179][0m |          -0.0448 |         241.6683 |        -337.7820 |
[32m[20221208 14:11:38 @agent_ppo2.py:179][0m |          -0.0470 |         240.4730 |        -342.4148 |
[32m[20221208 14:11:38 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:11:38 @agent_ppo2.py:132][0m Average TRAINING episode reward: 919.05
[32m[20221208 14:11:38 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 976.55
[32m[20221208 14:11:38 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 941.86
[32m[20221208 14:11:38 @agent_ppo2.py:137][0m Total time:      26.30 min
[32m[20221208 14:11:38 @agent_ppo2.py:139][0m 2152448 total steps have happened
[32m[20221208 14:11:38 @agent_ppo2.py:115][0m #------------------------ Iteration 1051 --------------------------#
[32m[20221208 14:11:39 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |           0.0461 |         245.9753 |        -296.4551 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |           0.0569 |         246.4676 |        -231.7246 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |           0.0141 |         241.9641 |        -262.1349 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |          -0.0049 |         241.9071 |        -305.7551 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |          -0.0144 |         240.1970 |        -315.0233 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |          -0.0198 |         238.8031 |        -324.7935 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |          -0.0211 |         236.8383 |        -322.6589 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |          -0.0321 |         236.3976 |        -338.2634 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |          -0.0350 |         234.3323 |        -344.9270 |
[32m[20221208 14:11:39 @agent_ppo2.py:179][0m |          -0.0380 |         235.2147 |        -351.2939 |
[32m[20221208 14:11:39 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:11:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.77
[32m[20221208 14:11:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.38
[32m[20221208 14:11:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.71
[32m[20221208 14:11:40 @agent_ppo2.py:137][0m Total time:      26.32 min
[32m[20221208 14:11:40 @agent_ppo2.py:139][0m 2154496 total steps have happened
[32m[20221208 14:11:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1052 --------------------------#
[32m[20221208 14:11:40 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:11:40 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:40 @agent_ppo2.py:179][0m |           0.0496 |         247.4372 |        -279.6586 |
[32m[20221208 14:11:40 @agent_ppo2.py:179][0m |           0.0682 |         242.8094 |        -196.0645 |
[32m[20221208 14:11:40 @agent_ppo2.py:179][0m |           0.0385 |         240.8971 |        -239.9946 |
[32m[20221208 14:11:41 @agent_ppo2.py:179][0m |           0.0108 |         240.5702 |        -278.1590 |
[32m[20221208 14:11:41 @agent_ppo2.py:179][0m |           0.0008 |         238.1927 |        -284.7941 |
[32m[20221208 14:11:41 @agent_ppo2.py:179][0m |          -0.0163 |         237.9868 |        -308.1508 |
[32m[20221208 14:11:41 @agent_ppo2.py:179][0m |          -0.0279 |         236.8646 |        -322.1248 |
[32m[20221208 14:11:41 @agent_ppo2.py:179][0m |          -0.0325 |         236.5095 |        -337.7989 |
[32m[20221208 14:11:41 @agent_ppo2.py:179][0m |          -0.0344 |         237.1384 |        -343.2451 |
[32m[20221208 14:11:41 @agent_ppo2.py:179][0m |          -0.0406 |         235.5312 |        -348.9742 |
[32m[20221208 14:11:41 @agent_ppo2.py:124][0m Policy update time: 0.85 s
[32m[20221208 14:11:41 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.64
[32m[20221208 14:11:41 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.72
[32m[20221208 14:11:41 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 910.71
[32m[20221208 14:11:41 @agent_ppo2.py:137][0m Total time:      26.35 min
[32m[20221208 14:11:41 @agent_ppo2.py:139][0m 2156544 total steps have happened
[32m[20221208 14:11:41 @agent_ppo2.py:115][0m #------------------------ Iteration 1053 --------------------------#
[32m[20221208 14:11:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |           0.1060 |         246.8995 |        -248.1899 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |           0.0887 |         243.0537 |        -172.6941 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |           0.0547 |         241.1327 |        -202.1337 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |           0.0237 |         240.2306 |        -237.4814 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |          -0.0020 |         239.0121 |        -281.4732 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |          -0.0086 |         239.7998 |        -291.0725 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |          -0.0221 |         237.7794 |        -303.6697 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |          -0.0324 |         237.5987 |        -318.3472 |
[32m[20221208 14:11:42 @agent_ppo2.py:179][0m |          -0.0324 |         236.7530 |        -323.3389 |
[32m[20221208 14:11:43 @agent_ppo2.py:179][0m |          -0.0378 |         236.8845 |        -329.1468 |
[32m[20221208 14:11:43 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:11:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 917.07
[32m[20221208 14:11:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.00
[32m[20221208 14:11:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 932.18
[32m[20221208 14:11:43 @agent_ppo2.py:137][0m Total time:      26.38 min
[32m[20221208 14:11:43 @agent_ppo2.py:139][0m 2158592 total steps have happened
[32m[20221208 14:11:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1054 --------------------------#
[32m[20221208 14:11:43 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:11:43 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |           0.0772 |         240.9113 |        -246.2596 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |           0.0915 |         229.6074 |        -154.8402 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |           0.0346 |         221.1986 |        -218.9438 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |           0.0056 |         216.0463 |        -258.7710 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |          -0.0085 |         213.9210 |        -277.7184 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |          -0.0196 |         211.7675 |        -286.6553 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |          -0.0284 |         209.6673 |        -298.3515 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |          -0.0363 |         209.0783 |        -303.6168 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |          -0.0416 |         207.4530 |        -309.3149 |
[32m[20221208 14:11:44 @agent_ppo2.py:179][0m |          -0.0423 |         206.7928 |        -312.5248 |
[32m[20221208 14:11:44 @agent_ppo2.py:124][0m Policy update time: 0.83 s
[32m[20221208 14:11:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 889.42
[32m[20221208 14:11:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 922.05
[32m[20221208 14:11:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 931.05
[32m[20221208 14:11:45 @agent_ppo2.py:137][0m Total time:      26.40 min
[32m[20221208 14:11:45 @agent_ppo2.py:139][0m 2160640 total steps have happened
[32m[20221208 14:11:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1055 --------------------------#
[32m[20221208 14:11:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:11:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:45 @agent_ppo2.py:179][0m |           0.0683 |         256.3380 |        -241.8019 |
[32m[20221208 14:11:45 @agent_ppo2.py:179][0m |           0.0596 |         246.1924 |        -218.4985 |
[32m[20221208 14:11:45 @agent_ppo2.py:179][0m |           0.0221 |         244.2766 |        -237.4416 |
[32m[20221208 14:11:45 @agent_ppo2.py:179][0m |          -0.0035 |         241.6392 |        -263.1673 |
[32m[20221208 14:11:45 @agent_ppo2.py:179][0m |          -0.0192 |         241.4474 |        -284.1609 |
[32m[20221208 14:11:46 @agent_ppo2.py:179][0m |          -0.0243 |         239.6177 |        -287.9927 |
[32m[20221208 14:11:46 @agent_ppo2.py:179][0m |          -0.0342 |         238.8488 |        -296.7574 |
[32m[20221208 14:11:46 @agent_ppo2.py:179][0m |          -0.0354 |         237.9638 |        -302.5076 |
[32m[20221208 14:11:46 @agent_ppo2.py:179][0m |          -0.0412 |         237.3841 |        -304.0816 |
[32m[20221208 14:11:46 @agent_ppo2.py:179][0m |          -0.0426 |         236.7710 |        -311.0998 |
[32m[20221208 14:11:46 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:11:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 930.34
[32m[20221208 14:11:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.20
[32m[20221208 14:11:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 956.44
[32m[20221208 14:11:46 @agent_ppo2.py:137][0m Total time:      26.43 min
[32m[20221208 14:11:46 @agent_ppo2.py:139][0m 2162688 total steps have happened
[32m[20221208 14:11:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1056 --------------------------#
[32m[20221208 14:11:47 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:11:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |           0.0883 |         252.2208 |        -255.7429 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |           0.0515 |         240.7139 |        -208.9970 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |           0.0112 |         231.9911 |        -246.2372 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |           0.0119 |         225.2229 |        -246.4305 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |          -0.0090 |         221.3973 |        -262.8668 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |          -0.0201 |         217.9544 |        -274.7517 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |          -0.0261 |         215.8700 |        -289.9988 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |          -0.0324 |         213.4054 |        -297.9705 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |          -0.0373 |         211.0780 |        -307.8678 |
[32m[20221208 14:11:47 @agent_ppo2.py:179][0m |          -0.0391 |         210.7682 |        -311.9365 |
[32m[20221208 14:11:47 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:11:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 950.37
[32m[20221208 14:11:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.45
[32m[20221208 14:11:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.51
[32m[20221208 14:11:48 @agent_ppo2.py:137][0m Total time:      26.46 min
[32m[20221208 14:11:48 @agent_ppo2.py:139][0m 2164736 total steps have happened
[32m[20221208 14:11:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1057 --------------------------#
[32m[20221208 14:11:48 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:48 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:48 @agent_ppo2.py:179][0m |           0.0611 |         247.4671 |        -261.5711 |
[32m[20221208 14:11:48 @agent_ppo2.py:179][0m |           0.0706 |         243.0927 |        -182.8362 |
[32m[20221208 14:11:48 @agent_ppo2.py:179][0m |           0.0305 |         240.7668 |        -230.8676 |
[32m[20221208 14:11:49 @agent_ppo2.py:179][0m |          -0.0020 |         240.6607 |        -273.2030 |
[32m[20221208 14:11:49 @agent_ppo2.py:179][0m |          -0.0150 |         238.9214 |        -284.3038 |
[32m[20221208 14:11:49 @agent_ppo2.py:179][0m |          -0.0259 |         238.6688 |        -290.1950 |
[32m[20221208 14:11:49 @agent_ppo2.py:179][0m |          -0.0221 |         238.6739 |        -285.7991 |
[32m[20221208 14:11:49 @agent_ppo2.py:179][0m |          -0.0297 |         237.6222 |        -298.9950 |
[32m[20221208 14:11:49 @agent_ppo2.py:179][0m |          -0.0303 |         236.3690 |        -303.5104 |
[32m[20221208 14:11:49 @agent_ppo2.py:179][0m |          -0.0367 |         236.3717 |        -309.9529 |
[32m[20221208 14:11:49 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:11:49 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.62
[32m[20221208 14:11:49 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 977.59
[32m[20221208 14:11:49 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.74
[32m[20221208 14:11:49 @agent_ppo2.py:137][0m Total time:      26.48 min
[32m[20221208 14:11:49 @agent_ppo2.py:139][0m 2166784 total steps have happened
[32m[20221208 14:11:49 @agent_ppo2.py:115][0m #------------------------ Iteration 1058 --------------------------#
[32m[20221208 14:11:50 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |           0.0923 |         238.8981 |        -250.2678 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |           0.0617 |         231.3567 |        -221.7045 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |           0.0564 |         229.2123 |        -230.0294 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |           0.0210 |         227.0708 |        -239.2373 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |          -0.0090 |         224.3281 |        -279.3225 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |          -0.0223 |         223.8204 |        -292.9832 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |          -0.0289 |         221.9224 |        -298.0542 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |          -0.0347 |         220.3930 |        -302.0923 |
[32m[20221208 14:11:50 @agent_ppo2.py:179][0m |          -0.0377 |         219.3770 |        -308.4013 |
[32m[20221208 14:11:51 @agent_ppo2.py:179][0m |          -0.0435 |         219.2657 |        -314.6424 |
[32m[20221208 14:11:51 @agent_ppo2.py:124][0m Policy update time: 0.77 s
[32m[20221208 14:11:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 847.10
[32m[20221208 14:11:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 860.16
[32m[20221208 14:11:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 924.95
[32m[20221208 14:11:51 @agent_ppo2.py:137][0m Total time:      26.51 min
[32m[20221208 14:11:51 @agent_ppo2.py:139][0m 2168832 total steps have happened
[32m[20221208 14:11:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1059 --------------------------#
[32m[20221208 14:11:51 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:51 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:51 @agent_ppo2.py:179][0m |           0.0779 |         235.3252 |        -264.1804 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |           0.0527 |         230.6964 |        -208.8178 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |           0.0461 |         226.9954 |        -230.0951 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |           0.0256 |         223.5659 |        -233.0837 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |          -0.0105 |         221.0971 |        -273.8946 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |          -0.0227 |         218.6300 |        -284.4144 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |          -0.0291 |         219.4663 |        -292.6705 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |          -0.0346 |         218.0476 |        -300.2079 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |          -0.0364 |         216.4152 |        -306.4761 |
[32m[20221208 14:11:52 @agent_ppo2.py:179][0m |          -0.0393 |         216.3725 |        -305.3949 |
[32m[20221208 14:11:52 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:11:52 @agent_ppo2.py:132][0m Average TRAINING episode reward: 943.64
[32m[20221208 14:11:52 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.99
[32m[20221208 14:11:52 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 957.36
[32m[20221208 14:11:52 @agent_ppo2.py:137][0m Total time:      26.53 min
[32m[20221208 14:11:52 @agent_ppo2.py:139][0m 2170880 total steps have happened
[32m[20221208 14:11:52 @agent_ppo2.py:115][0m #------------------------ Iteration 1060 --------------------------#
[32m[20221208 14:11:53 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:11:53 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |           0.0441 |         230.1850 |        -262.8098 |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |           0.0436 |         215.8914 |        -225.4852 |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |          -0.0045 |         210.0290 |        -256.3714 |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |          -0.0185 |         208.5495 |        -261.7196 |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |          -0.0262 |         206.5684 |        -269.9192 |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |          -0.0369 |         204.0204 |        -276.5736 |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |          -0.0435 |         202.6188 |        -282.6580 |
[32m[20221208 14:11:53 @agent_ppo2.py:179][0m |          -0.0453 |         201.2258 |        -292.4429 |
[32m[20221208 14:11:54 @agent_ppo2.py:179][0m |          -0.0504 |         199.6545 |        -293.2773 |
[32m[20221208 14:11:54 @agent_ppo2.py:179][0m |          -0.0520 |         199.2641 |        -298.4018 |
[32m[20221208 14:11:54 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:11:54 @agent_ppo2.py:132][0m Average TRAINING episode reward: 847.58
[32m[20221208 14:11:54 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 887.17
[32m[20221208 14:11:54 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 911.96
[32m[20221208 14:11:54 @agent_ppo2.py:137][0m Total time:      26.56 min
[32m[20221208 14:11:54 @agent_ppo2.py:139][0m 2172928 total steps have happened
[32m[20221208 14:11:54 @agent_ppo2.py:115][0m #------------------------ Iteration 1061 --------------------------#
[32m[20221208 14:11:54 @agent_ppo2.py:121][0m Sampling time: 0.47 s by 1 slaves
[32m[20221208 14:11:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |           0.0729 |         209.9684 |        -201.7037 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |           0.0371 |         199.7251 |        -155.7916 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0219 |         197.1498 |        -187.0104 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0422 |         196.0512 |        -195.7397 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0562 |         196.1066 |        -210.0504 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0634 |         194.5255 |        -215.3497 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0685 |         194.4817 |        -221.6218 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0743 |         193.7761 |        -227.3253 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0762 |         192.9974 |        -231.8531 |
[32m[20221208 14:11:55 @agent_ppo2.py:179][0m |          -0.0757 |         192.5146 |        -237.3715 |
[32m[20221208 14:11:55 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:11:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 624.98
[32m[20221208 14:11:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 928.67
[32m[20221208 14:11:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 617.18
[32m[20221208 14:11:56 @agent_ppo2.py:137][0m Total time:      26.59 min
[32m[20221208 14:11:56 @agent_ppo2.py:139][0m 2174976 total steps have happened
[32m[20221208 14:11:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1062 --------------------------#
[32m[20221208 14:11:56 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:56 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:56 @agent_ppo2.py:179][0m |           0.0862 |         236.7959 |        -279.2745 |
[32m[20221208 14:11:56 @agent_ppo2.py:179][0m |           0.0983 |         232.9873 |        -166.7831 |
[32m[20221208 14:11:56 @agent_ppo2.py:179][0m |           0.0820 |         230.9852 |        -167.9087 |
[32m[20221208 14:11:56 @agent_ppo2.py:179][0m |           0.0306 |         229.9315 |        -235.6182 |
[32m[20221208 14:11:56 @agent_ppo2.py:179][0m |           0.0004 |         228.7499 |        -287.1642 |
[32m[20221208 14:11:56 @agent_ppo2.py:179][0m |          -0.0109 |         228.7225 |        -298.3057 |
[32m[20221208 14:11:56 @agent_ppo2.py:179][0m |          -0.0152 |         228.3045 |        -308.6150 |
[32m[20221208 14:11:57 @agent_ppo2.py:179][0m |          -0.0244 |         228.8370 |        -319.9434 |
[32m[20221208 14:11:57 @agent_ppo2.py:179][0m |          -0.0343 |         226.4421 |        -332.7552 |
[32m[20221208 14:11:57 @agent_ppo2.py:179][0m |          -0.0357 |         226.0062 |        -339.3023 |
[32m[20221208 14:11:57 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:11:57 @agent_ppo2.py:132][0m Average TRAINING episode reward: 965.47
[32m[20221208 14:11:57 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.30
[32m[20221208 14:11:57 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 882.08
[32m[20221208 14:11:57 @agent_ppo2.py:137][0m Total time:      26.61 min
[32m[20221208 14:11:57 @agent_ppo2.py:139][0m 2177024 total steps have happened
[32m[20221208 14:11:57 @agent_ppo2.py:115][0m #------------------------ Iteration 1063 --------------------------#
[32m[20221208 14:11:58 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:11:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |           0.0807 |         233.4594 |        -247.0635 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |           0.0915 |         229.5077 |        -150.4894 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |           0.0418 |         227.0579 |        -201.1190 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |           0.0139 |         225.8722 |        -246.6560 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |          -0.0011 |         224.8816 |        -263.0006 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |          -0.0163 |         223.9534 |        -277.2297 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |          -0.0268 |         222.7604 |        -285.9892 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |          -0.0326 |         222.2434 |        -294.4508 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |          -0.0362 |         222.4527 |        -296.9783 |
[32m[20221208 14:11:58 @agent_ppo2.py:179][0m |          -0.0379 |         221.2771 |        -306.5955 |
[32m[20221208 14:11:58 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:11:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.05
[32m[20221208 14:11:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.95
[32m[20221208 14:11:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 628.71
[32m[20221208 14:11:59 @agent_ppo2.py:137][0m Total time:      26.64 min
[32m[20221208 14:11:59 @agent_ppo2.py:139][0m 2179072 total steps have happened
[32m[20221208 14:11:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1064 --------------------------#
[32m[20221208 14:11:59 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:11:59 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:11:59 @agent_ppo2.py:179][0m |           0.0410 |         232.6078 |        -281.3401 |
[32m[20221208 14:11:59 @agent_ppo2.py:179][0m |           0.0510 |         226.3416 |        -234.8079 |
[32m[20221208 14:11:59 @agent_ppo2.py:179][0m |           0.0210 |         223.6232 |        -248.7549 |
[32m[20221208 14:11:59 @agent_ppo2.py:179][0m |           0.0506 |         221.6949 |        -218.7396 |
[32m[20221208 14:11:59 @agent_ppo2.py:179][0m |           0.0001 |         220.3446 |        -249.5993 |
[32m[20221208 14:11:59 @agent_ppo2.py:179][0m |          -0.0186 |         219.6639 |        -273.4072 |
[32m[20221208 14:12:00 @agent_ppo2.py:179][0m |          -0.0277 |         218.9646 |        -285.4176 |
[32m[20221208 14:12:00 @agent_ppo2.py:179][0m |          -0.0331 |         217.8869 |        -294.8650 |
[32m[20221208 14:12:00 @agent_ppo2.py:179][0m |          -0.0388 |         216.7507 |        -302.4274 |
[32m[20221208 14:12:00 @agent_ppo2.py:179][0m |          -0.0364 |         217.1492 |        -305.4624 |
[32m[20221208 14:12:00 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:12:00 @agent_ppo2.py:132][0m Average TRAINING episode reward: 937.81
[32m[20221208 14:12:00 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 979.66
[32m[20221208 14:12:00 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 976.41
[32m[20221208 14:12:00 @agent_ppo2.py:137][0m Total time:      26.66 min
[32m[20221208 14:12:00 @agent_ppo2.py:139][0m 2181120 total steps have happened
[32m[20221208 14:12:00 @agent_ppo2.py:115][0m #------------------------ Iteration 1065 --------------------------#
[32m[20221208 14:12:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |           0.0579 |         234.3321 |        -252.3075 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |           0.0758 |         224.2452 |        -209.7505 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |           0.0372 |         220.6451 |        -223.9229 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |           0.0040 |         219.3147 |        -266.3627 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |          -0.0089 |         217.0851 |        -278.4860 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |          -0.0229 |         215.6045 |        -291.8592 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |          -0.0312 |         214.4145 |        -294.6261 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |          -0.0360 |         215.0601 |        -304.3881 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |          -0.0410 |         211.3254 |        -312.7058 |
[32m[20221208 14:12:01 @agent_ppo2.py:179][0m |          -0.0453 |         211.4201 |        -320.5247 |
[32m[20221208 14:12:01 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:12:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 956.74
[32m[20221208 14:12:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 968.96
[32m[20221208 14:12:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 965.79
[32m[20221208 14:12:02 @agent_ppo2.py:137][0m Total time:      26.69 min
[32m[20221208 14:12:02 @agent_ppo2.py:139][0m 2183168 total steps have happened
[32m[20221208 14:12:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1066 --------------------------#
[32m[20221208 14:12:02 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:02 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:02 @agent_ppo2.py:179][0m |           0.0585 |         241.5394 |        -244.0323 |
[32m[20221208 14:12:02 @agent_ppo2.py:179][0m |           0.0535 |         232.0427 |        -184.7570 |
[32m[20221208 14:12:02 @agent_ppo2.py:179][0m |           0.0068 |         229.6901 |        -239.6045 |
[32m[20221208 14:12:02 @agent_ppo2.py:179][0m |          -0.0101 |         227.8956 |        -259.0895 |
[32m[20221208 14:12:02 @agent_ppo2.py:179][0m |          -0.0213 |         225.3515 |        -265.9502 |
[32m[20221208 14:12:03 @agent_ppo2.py:179][0m |          -0.0259 |         224.8948 |        -272.4318 |
[32m[20221208 14:12:03 @agent_ppo2.py:179][0m |          -0.0338 |         223.4885 |        -280.1702 |
[32m[20221208 14:12:03 @agent_ppo2.py:179][0m |          -0.0401 |         222.7419 |        -290.1840 |
[32m[20221208 14:12:03 @agent_ppo2.py:179][0m |          -0.0410 |         222.2070 |        -297.0402 |
[32m[20221208 14:12:03 @agent_ppo2.py:179][0m |          -0.0443 |         221.0967 |        -300.3089 |
[32m[20221208 14:12:03 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:12:03 @agent_ppo2.py:132][0m Average TRAINING episode reward: 907.85
[32m[20221208 14:12:03 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.47
[32m[20221208 14:12:03 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 967.71
[32m[20221208 14:12:03 @agent_ppo2.py:137][0m Total time:      26.71 min
[32m[20221208 14:12:03 @agent_ppo2.py:139][0m 2185216 total steps have happened
[32m[20221208 14:12:03 @agent_ppo2.py:115][0m #------------------------ Iteration 1067 --------------------------#
[32m[20221208 14:12:04 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |           0.0999 |         236.5223 |        -253.0724 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |           0.0656 |         230.8933 |        -200.5690 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |           0.0125 |         227.2389 |        -255.8297 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |           0.0005 |         225.4886 |        -263.6652 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |          -0.0199 |         223.6757 |        -281.0773 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |          -0.0301 |         221.3887 |        -295.9532 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |          -0.0280 |         218.6560 |        -298.1106 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |          -0.0331 |         216.4386 |        -301.5685 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |          -0.0398 |         214.4320 |        -311.9274 |
[32m[20221208 14:12:04 @agent_ppo2.py:179][0m |          -0.0447 |         213.6731 |        -316.5975 |
[32m[20221208 14:12:04 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:12:05 @agent_ppo2.py:132][0m Average TRAINING episode reward: 902.87
[32m[20221208 14:12:05 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.80
[32m[20221208 14:12:05 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 632.74
[32m[20221208 14:12:05 @agent_ppo2.py:137][0m Total time:      26.74 min
[32m[20221208 14:12:05 @agent_ppo2.py:139][0m 2187264 total steps have happened
[32m[20221208 14:12:05 @agent_ppo2.py:115][0m #------------------------ Iteration 1068 --------------------------#
[32m[20221208 14:12:05 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:05 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:05 @agent_ppo2.py:179][0m |           0.0680 |         240.1488 |        -275.9590 |
[32m[20221208 14:12:05 @agent_ppo2.py:179][0m |           0.0464 |         222.9066 |        -244.7666 |
[32m[20221208 14:12:05 @agent_ppo2.py:179][0m |           0.0014 |         215.3113 |        -271.4673 |
[32m[20221208 14:12:05 @agent_ppo2.py:179][0m |          -0.0203 |         211.1783 |        -298.1380 |
[32m[20221208 14:12:05 @agent_ppo2.py:179][0m |          -0.0287 |         209.2574 |        -308.6380 |
[32m[20221208 14:12:06 @agent_ppo2.py:179][0m |          -0.0380 |         205.4242 |        -325.6498 |
[32m[20221208 14:12:06 @agent_ppo2.py:179][0m |          -0.0428 |         203.1271 |        -330.2441 |
[32m[20221208 14:12:06 @agent_ppo2.py:179][0m |          -0.0464 |         202.1447 |        -341.0947 |
[32m[20221208 14:12:06 @agent_ppo2.py:179][0m |          -0.0475 |         200.7873 |        -349.4787 |
[32m[20221208 14:12:06 @agent_ppo2.py:179][0m |          -0.0485 |         198.0888 |        -349.6380 |
[32m[20221208 14:12:06 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:12:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 932.93
[32m[20221208 14:12:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 945.90
[32m[20221208 14:12:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 945.50
[32m[20221208 14:12:06 @agent_ppo2.py:137][0m Total time:      26.76 min
[32m[20221208 14:12:06 @agent_ppo2.py:139][0m 2189312 total steps have happened
[32m[20221208 14:12:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1069 --------------------------#
[32m[20221208 14:12:07 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:07 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |           0.0701 |         212.6087 |        -235.2462 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |           0.0596 |         200.2365 |        -155.8285 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |           0.0196 |         197.5385 |        -190.0658 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |          -0.0328 |         196.3524 |        -216.2294 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |          -0.0541 |         194.2133 |        -231.2131 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |          -0.0644 |         193.6051 |        -240.8373 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |          -0.0703 |         192.9026 |        -244.1387 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |          -0.0727 |         192.7590 |        -254.5906 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |          -0.0771 |         192.0472 |        -255.8378 |
[32m[20221208 14:12:07 @agent_ppo2.py:179][0m |          -0.0792 |         192.3833 |        -261.7587 |
[32m[20221208 14:12:07 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:12:08 @agent_ppo2.py:132][0m Average TRAINING episode reward: 643.47
[32m[20221208 14:12:08 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.68
[32m[20221208 14:12:08 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 928.44
[32m[20221208 14:12:08 @agent_ppo2.py:137][0m Total time:      26.79 min
[32m[20221208 14:12:08 @agent_ppo2.py:139][0m 2191360 total steps have happened
[32m[20221208 14:12:08 @agent_ppo2.py:115][0m #------------------------ Iteration 1070 --------------------------#
[32m[20221208 14:12:08 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:08 @agent_ppo2.py:179][0m |           0.0521 |         237.8160 |        -302.7267 |
[32m[20221208 14:12:08 @agent_ppo2.py:179][0m |           0.0519 |         230.1177 |        -244.3593 |
[32m[20221208 14:12:08 @agent_ppo2.py:179][0m |           0.0131 |         226.5818 |        -286.7440 |
[32m[20221208 14:12:08 @agent_ppo2.py:179][0m |          -0.0131 |         224.8263 |        -317.3818 |
[32m[20221208 14:12:08 @agent_ppo2.py:179][0m |          -0.0253 |         222.7994 |        -326.6791 |
[32m[20221208 14:12:09 @agent_ppo2.py:179][0m |          -0.0341 |         221.7269 |        -336.9052 |
[32m[20221208 14:12:09 @agent_ppo2.py:179][0m |          -0.0367 |         220.3644 |        -346.1373 |
[32m[20221208 14:12:09 @agent_ppo2.py:179][0m |          -0.0415 |         220.0403 |        -350.2915 |
[32m[20221208 14:12:09 @agent_ppo2.py:179][0m |          -0.0312 |         218.4015 |        -338.5965 |
[32m[20221208 14:12:09 @agent_ppo2.py:179][0m |          -0.0382 |         218.4834 |        -354.4563 |
[32m[20221208 14:12:09 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:12:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 906.04
[32m[20221208 14:12:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.81
[32m[20221208 14:12:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 735.87
[32m[20221208 14:12:09 @agent_ppo2.py:137][0m Total time:      26.81 min
[32m[20221208 14:12:09 @agent_ppo2.py:139][0m 2193408 total steps have happened
[32m[20221208 14:12:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1071 --------------------------#
[32m[20221208 14:12:10 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:10 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |           0.0517 |         238.6284 |        -311.0655 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |           0.0451 |         230.2513 |        -262.8104 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |           0.0229 |         228.3476 |        -280.8749 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |          -0.0086 |         226.1290 |        -317.1840 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |          -0.0233 |         224.1217 |        -332.0053 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |          -0.0199 |         221.9166 |        -330.8433 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |          -0.0305 |         221.3992 |        -344.8667 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |          -0.0329 |         219.4462 |        -347.4065 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |          -0.0394 |         219.5175 |        -352.5363 |
[32m[20221208 14:12:10 @agent_ppo2.py:179][0m |          -0.0419 |         218.1957 |        -360.9459 |
[32m[20221208 14:12:10 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:12:11 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.96
[32m[20221208 14:12:11 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.09
[32m[20221208 14:12:11 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 737.26
[32m[20221208 14:12:11 @agent_ppo2.py:137][0m Total time:      26.84 min
[32m[20221208 14:12:11 @agent_ppo2.py:139][0m 2195456 total steps have happened
[32m[20221208 14:12:11 @agent_ppo2.py:115][0m #------------------------ Iteration 1072 --------------------------#
[32m[20221208 14:12:11 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:11 @agent_ppo2.py:179][0m |           0.1104 |         242.7998 |        -258.2761 |
[32m[20221208 14:12:11 @agent_ppo2.py:179][0m |           0.0730 |         235.6526 |        -185.9511 |
[32m[20221208 14:12:11 @agent_ppo2.py:179][0m |           0.0505 |         231.2819 |        -245.9412 |
[32m[20221208 14:12:11 @agent_ppo2.py:179][0m |           0.0204 |         230.4780 |        -250.1398 |
[32m[20221208 14:12:11 @agent_ppo2.py:179][0m |           0.0002 |         227.4593 |        -289.1965 |
[32m[20221208 14:12:12 @agent_ppo2.py:179][0m |          -0.0125 |         226.2903 |        -307.7321 |
[32m[20221208 14:12:12 @agent_ppo2.py:179][0m |          -0.0231 |         224.6927 |        -325.7923 |
[32m[20221208 14:12:12 @agent_ppo2.py:179][0m |          -0.0285 |         224.7309 |        -332.1616 |
[32m[20221208 14:12:12 @agent_ppo2.py:179][0m |          -0.0316 |         224.3973 |        -344.0496 |
[32m[20221208 14:12:12 @agent_ppo2.py:179][0m |          -0.0364 |         223.8925 |        -352.4480 |
[32m[20221208 14:12:12 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:12:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 936.75
[32m[20221208 14:12:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.83
[32m[20221208 14:12:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 649.86
[32m[20221208 14:12:12 @agent_ppo2.py:137][0m Total time:      26.86 min
[32m[20221208 14:12:12 @agent_ppo2.py:139][0m 2197504 total steps have happened
[32m[20221208 14:12:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1073 --------------------------#
[32m[20221208 14:12:13 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:13 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |           0.0994 |         237.5054 |        -272.0279 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |           0.0441 |         230.5824 |        -244.2398 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |           0.0086 |         227.4670 |        -283.8413 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |          -0.0074 |         222.9908 |        -298.0388 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |          -0.0189 |         218.5820 |        -312.2130 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |          -0.0270 |         217.1172 |        -322.8994 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |          -0.0304 |         212.2099 |        -323.3509 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |          -0.0332 |         212.2049 |        -329.7158 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |          -0.0390 |         209.1944 |        -339.3635 |
[32m[20221208 14:12:13 @agent_ppo2.py:179][0m |          -0.0410 |         207.7995 |        -345.2431 |
[32m[20221208 14:12:13 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:12:14 @agent_ppo2.py:132][0m Average TRAINING episode reward: 960.60
[32m[20221208 14:12:14 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 987.34
[32m[20221208 14:12:14 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 596.88
[32m[20221208 14:12:14 @agent_ppo2.py:137][0m Total time:      26.89 min
[32m[20221208 14:12:14 @agent_ppo2.py:139][0m 2199552 total steps have happened
[32m[20221208 14:12:14 @agent_ppo2.py:115][0m #------------------------ Iteration 1074 --------------------------#
[32m[20221208 14:12:14 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:14 @agent_ppo2.py:179][0m |           0.0763 |         241.3758 |        -266.2052 |
[32m[20221208 14:12:14 @agent_ppo2.py:179][0m |           0.0560 |         234.0916 |        -220.8541 |
[32m[20221208 14:12:14 @agent_ppo2.py:179][0m |           0.0110 |         230.4800 |        -259.9606 |
[32m[20221208 14:12:14 @agent_ppo2.py:179][0m |          -0.0002 |         229.5797 |        -274.1413 |
[32m[20221208 14:12:15 @agent_ppo2.py:179][0m |          -0.0153 |         229.8627 |        -290.7111 |
[32m[20221208 14:12:15 @agent_ppo2.py:179][0m |          -0.0230 |         227.1905 |        -299.4555 |
[32m[20221208 14:12:15 @agent_ppo2.py:179][0m |          -0.0307 |         226.7543 |        -310.3667 |
[32m[20221208 14:12:15 @agent_ppo2.py:179][0m |          -0.0365 |         226.3815 |        -317.3393 |
[32m[20221208 14:12:15 @agent_ppo2.py:179][0m |          -0.0399 |         224.9338 |        -324.5052 |
[32m[20221208 14:12:15 @agent_ppo2.py:179][0m |          -0.0394 |         226.2748 |        -324.9760 |
[32m[20221208 14:12:15 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 14:12:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 961.00
[32m[20221208 14:12:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.07
[32m[20221208 14:12:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 155.90
[32m[20221208 14:12:15 @agent_ppo2.py:137][0m Total time:      26.92 min
[32m[20221208 14:12:15 @agent_ppo2.py:139][0m 2201600 total steps have happened
[32m[20221208 14:12:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1075 --------------------------#
[32m[20221208 14:12:16 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:16 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |           0.0786 |         226.2828 |        -259.3078 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |           0.1295 |         208.7162 |        -158.8163 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |           0.0632 |         201.5932 |        -168.8843 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |           0.0284 |         197.9907 |        -227.4670 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |           0.0003 |         194.5363 |        -266.8819 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |          -0.0166 |         185.9765 |        -288.5622 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |          -0.0259 |         180.7663 |        -301.9460 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |          -0.0339 |         177.0105 |        -310.1434 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |          -0.0359 |         174.3351 |        -312.9373 |
[32m[20221208 14:12:16 @agent_ppo2.py:179][0m |          -0.0391 |         170.7451 |        -318.8488 |
[32m[20221208 14:12:16 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:12:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 921.79
[32m[20221208 14:12:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 971.79
[32m[20221208 14:12:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 663.69
[32m[20221208 14:12:17 @agent_ppo2.py:137][0m Total time:      26.94 min
[32m[20221208 14:12:17 @agent_ppo2.py:139][0m 2203648 total steps have happened
[32m[20221208 14:12:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1076 --------------------------#
[32m[20221208 14:12:17 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:12:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:17 @agent_ppo2.py:179][0m |           0.0749 |         240.5606 |        -275.6421 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |           0.0437 |         226.5082 |        -242.7488 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |           0.0107 |         218.2750 |        -267.3508 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |          -0.0117 |         214.4357 |        -289.9134 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |          -0.0232 |         210.7914 |        -302.8538 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |          -0.0318 |         206.9294 |        -313.3282 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |          -0.0329 |         205.1777 |        -319.9746 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |          -0.0373 |         201.4189 |        -330.3665 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |          -0.0379 |         199.2138 |        -335.5522 |
[32m[20221208 14:12:18 @agent_ppo2.py:179][0m |          -0.0310 |         197.1576 |        -332.7142 |
[32m[20221208 14:12:18 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:12:19 @agent_ppo2.py:132][0m Average TRAINING episode reward: 956.83
[32m[20221208 14:12:19 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 983.38
[32m[20221208 14:12:19 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 939.59
[32m[20221208 14:12:19 @agent_ppo2.py:137][0m Total time:      26.97 min
[32m[20221208 14:12:19 @agent_ppo2.py:139][0m 2205696 total steps have happened
[32m[20221208 14:12:19 @agent_ppo2.py:115][0m #------------------------ Iteration 1077 --------------------------#
[32m[20221208 14:12:19 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:12:19 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:19 @agent_ppo2.py:179][0m |           0.0598 |         254.2968 |        -312.0569 |
[32m[20221208 14:12:19 @agent_ppo2.py:179][0m |           0.0281 |         240.7873 |        -291.7177 |
[32m[20221208 14:12:19 @agent_ppo2.py:179][0m |           0.0070 |         233.5214 |        -299.2698 |
[32m[20221208 14:12:19 @agent_ppo2.py:179][0m |          -0.0125 |         229.1781 |        -318.7945 |
[32m[20221208 14:12:19 @agent_ppo2.py:179][0m |          -0.0288 |         228.0571 |        -340.9088 |
[32m[20221208 14:12:19 @agent_ppo2.py:179][0m |          -0.0381 |         224.7626 |        -349.2809 |
[32m[20221208 14:12:20 @agent_ppo2.py:179][0m |          -0.0443 |         222.8307 |        -355.1331 |
[32m[20221208 14:12:20 @agent_ppo2.py:179][0m |          -0.0484 |         222.2791 |        -358.9888 |
[32m[20221208 14:12:20 @agent_ppo2.py:179][0m |          -0.0539 |         220.7498 |        -369.1369 |
[32m[20221208 14:12:20 @agent_ppo2.py:179][0m |          -0.0559 |         220.2637 |        -377.3873 |
[32m[20221208 14:12:20 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 14:12:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 896.92
[32m[20221208 14:12:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 941.92
[32m[20221208 14:12:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.59
[32m[20221208 14:12:20 @agent_ppo2.py:137][0m Total time:      27.00 min
[32m[20221208 14:12:20 @agent_ppo2.py:139][0m 2207744 total steps have happened
[32m[20221208 14:12:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1078 --------------------------#
[32m[20221208 14:12:21 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:12:21 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |           0.0843 |         250.1880 |        -296.3620 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |           0.0705 |         240.8515 |        -238.8712 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |           0.0238 |         234.8726 |        -274.1967 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |           0.0115 |         231.1747 |        -290.7042 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |          -0.0159 |         229.6373 |        -316.1867 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |          -0.0270 |         226.8256 |        -326.7188 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |          -0.0392 |         224.4626 |        -342.0717 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |          -0.0440 |         222.6484 |        -351.6893 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |          -0.0427 |         221.6258 |        -351.9973 |
[32m[20221208 14:12:21 @agent_ppo2.py:179][0m |          -0.0440 |         219.5471 |        -361.4784 |
[32m[20221208 14:12:21 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:12:22 @agent_ppo2.py:132][0m Average TRAINING episode reward: 927.96
[32m[20221208 14:12:22 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 940.58
[32m[20221208 14:12:22 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 949.47
[32m[20221208 14:12:22 @agent_ppo2.py:137][0m Total time:      27.02 min
[32m[20221208 14:12:22 @agent_ppo2.py:139][0m 2209792 total steps have happened
[32m[20221208 14:12:22 @agent_ppo2.py:115][0m #------------------------ Iteration 1079 --------------------------#
[32m[20221208 14:12:22 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:22 @agent_ppo2.py:179][0m |           0.0711 |         250.3053 |        -278.4001 |
[32m[20221208 14:12:22 @agent_ppo2.py:179][0m |           0.0566 |         239.5814 |        -248.9709 |
[32m[20221208 14:12:22 @agent_ppo2.py:179][0m |           0.0239 |         233.9400 |        -265.2211 |
[32m[20221208 14:12:23 @agent_ppo2.py:179][0m |          -0.0119 |         229.7440 |        -297.6838 |
[32m[20221208 14:12:23 @agent_ppo2.py:179][0m |          -0.0226 |         228.4785 |        -311.8801 |
[32m[20221208 14:12:23 @agent_ppo2.py:179][0m |          -0.0331 |         225.1997 |        -324.0998 |
[32m[20221208 14:12:23 @agent_ppo2.py:179][0m |          -0.0391 |         224.1888 |        -330.5852 |
[32m[20221208 14:12:23 @agent_ppo2.py:179][0m |          -0.0423 |         221.1525 |        -338.8479 |
[32m[20221208 14:12:23 @agent_ppo2.py:179][0m |          -0.0448 |         220.2946 |        -342.4137 |
[32m[20221208 14:12:23 @agent_ppo2.py:179][0m |          -0.0468 |         219.9703 |        -347.6706 |
[32m[20221208 14:12:23 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:12:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 925.31
[32m[20221208 14:12:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.06
[32m[20221208 14:12:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 966.22
[32m[20221208 14:12:23 @agent_ppo2.py:137][0m Total time:      27.05 min
[32m[20221208 14:12:23 @agent_ppo2.py:139][0m 2211840 total steps have happened
[32m[20221208 14:12:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1080 --------------------------#
[32m[20221208 14:12:24 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:12:24 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |           0.1433 |         247.7690 |        -285.5387 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |           0.0968 |         233.1706 |        -147.0857 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |           0.0525 |         225.1937 |        -215.1174 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |           0.0149 |         221.3689 |        -279.2297 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |           0.0032 |         218.2655 |        -290.4963 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |          -0.0133 |         214.2636 |        -317.8419 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |          -0.0291 |         214.2496 |        -339.5029 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |          -0.0372 |         211.3975 |        -359.5039 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |          -0.0398 |         210.3586 |        -368.3042 |
[32m[20221208 14:12:24 @agent_ppo2.py:179][0m |          -0.0456 |         209.9837 |        -378.0760 |
[32m[20221208 14:12:24 @agent_ppo2.py:124][0m Policy update time: 0.66 s
[32m[20221208 14:12:25 @agent_ppo2.py:132][0m Average TRAINING episode reward: 905.34
[32m[20221208 14:12:25 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.25
[32m[20221208 14:12:25 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 950.97
[32m[20221208 14:12:25 @agent_ppo2.py:137][0m Total time:      27.07 min
[32m[20221208 14:12:25 @agent_ppo2.py:139][0m 2213888 total steps have happened
[32m[20221208 14:12:25 @agent_ppo2.py:115][0m #------------------------ Iteration 1081 --------------------------#
[32m[20221208 14:12:25 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:12:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:25 @agent_ppo2.py:179][0m |           0.0925 |         258.0217 |        -323.7618 |
[32m[20221208 14:12:25 @agent_ppo2.py:179][0m |           0.0820 |         242.2379 |        -250.0492 |
[32m[20221208 14:12:25 @agent_ppo2.py:179][0m |           0.0421 |         236.7867 |        -243.3516 |
[32m[20221208 14:12:26 @agent_ppo2.py:179][0m |           0.0080 |         232.7497 |        -294.5204 |
[32m[20221208 14:12:26 @agent_ppo2.py:179][0m |          -0.0104 |         230.3452 |        -332.1779 |
[32m[20221208 14:12:26 @agent_ppo2.py:179][0m |          -0.0255 |         227.0998 |        -342.5411 |
[32m[20221208 14:12:26 @agent_ppo2.py:179][0m |          -0.0337 |         225.2843 |        -358.5571 |
[32m[20221208 14:12:26 @agent_ppo2.py:179][0m |          -0.0375 |         223.8675 |        -365.7373 |
[32m[20221208 14:12:26 @agent_ppo2.py:179][0m |          -0.0445 |         223.0448 |        -375.9441 |
[32m[20221208 14:12:26 @agent_ppo2.py:179][0m |          -0.0487 |         220.8501 |        -383.0478 |
[32m[20221208 14:12:26 @agent_ppo2.py:124][0m Policy update time: 0.77 s
[32m[20221208 14:12:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 912.96
[32m[20221208 14:12:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 967.32
[32m[20221208 14:12:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 840.85
[32m[20221208 14:12:26 @agent_ppo2.py:137][0m Total time:      27.10 min
[32m[20221208 14:12:26 @agent_ppo2.py:139][0m 2215936 total steps have happened
[32m[20221208 14:12:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1082 --------------------------#
[32m[20221208 14:12:27 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:12:27 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:27 @agent_ppo2.py:179][0m |           0.1100 |         229.0567 |        -306.2495 |
[32m[20221208 14:12:27 @agent_ppo2.py:179][0m |           0.0886 |         198.8033 |        -225.6071 |
[32m[20221208 14:12:27 @agent_ppo2.py:179][0m |           0.0315 |         185.5633 |        -252.2411 |
[32m[20221208 14:12:27 @agent_ppo2.py:179][0m |          -0.0035 |         176.4720 |        -316.8703 |
[32m[20221208 14:12:27 @agent_ppo2.py:179][0m |          -0.0191 |         170.7612 |        -333.0467 |
[32m[20221208 14:12:27 @agent_ppo2.py:179][0m |          -0.0299 |         168.3361 |        -345.3460 |
[32m[20221208 14:12:28 @agent_ppo2.py:179][0m |          -0.0360 |         165.4417 |        -356.0645 |
[32m[20221208 14:12:28 @agent_ppo2.py:179][0m |          -0.0409 |         163.2573 |        -360.8859 |
[32m[20221208 14:12:28 @agent_ppo2.py:179][0m |          -0.0471 |         161.9370 |        -373.2404 |
[32m[20221208 14:12:28 @agent_ppo2.py:179][0m |          -0.0507 |         160.9593 |        -381.1000 |
[32m[20221208 14:12:28 @agent_ppo2.py:124][0m Policy update time: 0.86 s
[32m[20221208 14:12:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 882.62
[32m[20221208 14:12:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 934.66
[32m[20221208 14:12:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 925.02
[32m[20221208 14:12:28 @agent_ppo2.py:137][0m Total time:      27.13 min
[32m[20221208 14:12:28 @agent_ppo2.py:139][0m 2217984 total steps have happened
[32m[20221208 14:12:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1083 --------------------------#
[32m[20221208 14:12:29 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:12:29 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |           0.0957 |         294.1861 |        -329.8001 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |           0.0265 |         265.6440 |        -272.7299 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0105 |         259.2870 |        -316.4175 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0331 |         256.6689 |        -345.3692 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0399 |         252.6952 |        -352.1989 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0508 |         251.1135 |        -362.0488 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0542 |         250.0197 |        -369.2921 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0576 |         246.8042 |        -375.9061 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0598 |         244.6418 |        -376.7652 |
[32m[20221208 14:12:29 @agent_ppo2.py:179][0m |          -0.0597 |         243.4302 |        -382.9450 |
[32m[20221208 14:12:29 @agent_ppo2.py:124][0m Policy update time: 0.87 s
[32m[20221208 14:12:30 @agent_ppo2.py:132][0m Average TRAINING episode reward: 895.25
[32m[20221208 14:12:30 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 968.01
[32m[20221208 14:12:30 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 921.80
[32m[20221208 14:12:30 @agent_ppo2.py:137][0m Total time:      27.16 min
[32m[20221208 14:12:30 @agent_ppo2.py:139][0m 2220032 total steps have happened
[32m[20221208 14:12:30 @agent_ppo2.py:115][0m #------------------------ Iteration 1084 --------------------------#
[32m[20221208 14:12:30 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:30 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:30 @agent_ppo2.py:179][0m |           0.1095 |         215.2906 |        -264.8259 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |           0.0274 |         203.1595 |        -190.8968 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |           0.0032 |         198.9086 |        -207.0937 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |          -0.0345 |         196.3981 |        -239.1769 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |          -0.0534 |         194.9684 |        -252.6241 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |          -0.0617 |         193.7888 |        -265.6477 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |          -0.0690 |         193.1904 |        -277.6534 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |          -0.0706 |         192.1284 |        -279.2492 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |          -0.0754 |         191.0723 |        -285.8203 |
[32m[20221208 14:12:31 @agent_ppo2.py:179][0m |          -0.0806 |         190.7182 |        -296.7521 |
[32m[20221208 14:12:31 @agent_ppo2.py:124][0m Policy update time: 0.80 s
[32m[20221208 14:12:32 @agent_ppo2.py:132][0m Average TRAINING episode reward: 614.01
[32m[20221208 14:12:32 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 952.09
[32m[20221208 14:12:32 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 890.24
[32m[20221208 14:12:32 @agent_ppo2.py:137][0m Total time:      27.19 min
[32m[20221208 14:12:32 @agent_ppo2.py:139][0m 2222080 total steps have happened
[32m[20221208 14:12:32 @agent_ppo2.py:115][0m #------------------------ Iteration 1085 --------------------------#
[32m[20221208 14:12:32 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:12:32 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:32 @agent_ppo2.py:179][0m |           0.0652 |         211.9266 |        -277.2112 |
[32m[20221208 14:12:32 @agent_ppo2.py:179][0m |           0.0375 |         197.8348 |        -211.7643 |
[32m[20221208 14:12:32 @agent_ppo2.py:179][0m |          -0.0119 |         194.3356 |        -246.2596 |
[32m[20221208 14:12:32 @agent_ppo2.py:179][0m |          -0.0434 |         192.2200 |        -279.8807 |
[32m[20221208 14:12:32 @agent_ppo2.py:179][0m |          -0.0534 |         191.4403 |        -293.7783 |
[32m[20221208 14:12:33 @agent_ppo2.py:179][0m |          -0.0612 |         189.6759 |        -306.5013 |
[32m[20221208 14:12:33 @agent_ppo2.py:179][0m |          -0.0671 |         188.3006 |        -312.0489 |
[32m[20221208 14:12:33 @agent_ppo2.py:179][0m |          -0.0730 |         188.2587 |        -324.2372 |
[32m[20221208 14:12:33 @agent_ppo2.py:179][0m |          -0.0739 |         187.3982 |        -328.6831 |
[32m[20221208 14:12:33 @agent_ppo2.py:179][0m |          -0.0789 |         187.2481 |        -336.7861 |
[32m[20221208 14:12:33 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 14:12:33 @agent_ppo2.py:132][0m Average TRAINING episode reward: 559.12
[32m[20221208 14:12:33 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 829.98
[32m[20221208 14:12:33 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 929.21
[32m[20221208 14:12:33 @agent_ppo2.py:137][0m Total time:      27.22 min
[32m[20221208 14:12:33 @agent_ppo2.py:139][0m 2224128 total steps have happened
[32m[20221208 14:12:33 @agent_ppo2.py:115][0m #------------------------ Iteration 1086 --------------------------#
[32m[20221208 14:12:34 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:12:34 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |           0.0858 |         238.0529 |        -353.7145 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |           0.0678 |         212.0216 |        -299.6282 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |           0.0142 |         202.9854 |        -357.7061 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |          -0.0074 |         199.0291 |        -387.7951 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |          -0.0254 |         193.8323 |        -409.0106 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |          -0.0325 |         191.2360 |        -419.4112 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |          -0.0381 |         189.0772 |        -426.4695 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |          -0.0426 |         186.9870 |        -436.5234 |
[32m[20221208 14:12:34 @agent_ppo2.py:179][0m |          -0.0418 |         185.1327 |        -439.5295 |
[32m[20221208 14:12:35 @agent_ppo2.py:179][0m |          -0.0446 |         182.8895 |        -438.3819 |
[32m[20221208 14:12:35 @agent_ppo2.py:124][0m Policy update time: 0.82 s
[32m[20221208 14:12:35 @agent_ppo2.py:132][0m Average TRAINING episode reward: 940.78
[32m[20221208 14:12:35 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 969.99
[32m[20221208 14:12:35 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 897.84
[32m[20221208 14:12:35 @agent_ppo2.py:137][0m Total time:      27.24 min
[32m[20221208 14:12:35 @agent_ppo2.py:139][0m 2226176 total steps have happened
[32m[20221208 14:12:35 @agent_ppo2.py:115][0m #------------------------ Iteration 1087 --------------------------#
[32m[20221208 14:12:35 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:12:36 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |           0.0667 |         202.5837 |        -269.0210 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |           0.0335 |         176.7835 |        -233.0993 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0029 |         168.0258 |        -252.3235 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0433 |         160.2550 |        -287.9468 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0601 |         155.4710 |        -307.1327 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0684 |         152.7184 |        -318.4069 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0752 |         149.9089 |        -330.3693 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0799 |         148.6701 |        -337.7551 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0823 |         146.9242 |        -338.8895 |
[32m[20221208 14:12:36 @agent_ppo2.py:179][0m |          -0.0875 |         144.4001 |        -350.6102 |
[32m[20221208 14:12:36 @agent_ppo2.py:124][0m Policy update time: 0.86 s
[32m[20221208 14:12:37 @agent_ppo2.py:132][0m Average TRAINING episode reward: 560.97
[32m[20221208 14:12:37 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 853.58
[32m[20221208 14:12:37 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 361.86
[32m[20221208 14:12:37 @agent_ppo2.py:137][0m Total time:      27.27 min
[32m[20221208 14:12:37 @agent_ppo2.py:139][0m 2228224 total steps have happened
[32m[20221208 14:12:37 @agent_ppo2.py:115][0m #------------------------ Iteration 1088 --------------------------#
[32m[20221208 14:12:37 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:12:37 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:37 @agent_ppo2.py:179][0m |           0.0855 |         265.3690 |        -397.3887 |
[32m[20221208 14:12:37 @agent_ppo2.py:179][0m |           0.0457 |         242.1439 |        -379.7472 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |           0.0069 |         235.5101 |        -387.3811 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |          -0.0192 |         232.4696 |        -409.7641 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |          -0.0394 |         231.0840 |        -418.0451 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |          -0.0408 |         229.8589 |        -424.6039 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |          -0.0478 |         227.0073 |        -418.7302 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |          -0.0568 |         227.0706 |        -426.6316 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |          -0.0599 |         227.5174 |        -432.0521 |
[32m[20221208 14:12:38 @agent_ppo2.py:179][0m |          -0.0645 |         225.5785 |        -434.9488 |
[32m[20221208 14:12:38 @agent_ppo2.py:124][0m Policy update time: 0.87 s
[32m[20221208 14:12:39 @agent_ppo2.py:132][0m Average TRAINING episode reward: 939.78
[32m[20221208 14:12:39 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.76
[32m[20221208 14:12:39 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 876.62
[32m[20221208 14:12:39 @agent_ppo2.py:137][0m Total time:      27.30 min
[32m[20221208 14:12:39 @agent_ppo2.py:139][0m 2230272 total steps have happened
[32m[20221208 14:12:39 @agent_ppo2.py:115][0m #------------------------ Iteration 1089 --------------------------#
[32m[20221208 14:12:39 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:12:39 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:39 @agent_ppo2.py:179][0m |           0.0582 |         192.8903 |        -356.4261 |
[32m[20221208 14:12:39 @agent_ppo2.py:179][0m |           0.0111 |         173.6302 |        -291.5418 |
[32m[20221208 14:12:39 @agent_ppo2.py:179][0m |          -0.0354 |         167.8267 |        -315.0401 |
[32m[20221208 14:12:39 @agent_ppo2.py:179][0m |          -0.0395 |         164.4685 |        -337.9886 |
[32m[20221208 14:12:39 @agent_ppo2.py:179][0m |          -0.0542 |         160.7890 |        -346.3931 |
[32m[20221208 14:12:39 @agent_ppo2.py:179][0m |          -0.0685 |         156.3409 |        -368.1707 |
[32m[20221208 14:12:39 @agent_ppo2.py:179][0m |          -0.0735 |         152.7898 |        -382.3287 |
[32m[20221208 14:12:40 @agent_ppo2.py:179][0m |          -0.0773 |         151.8663 |        -386.3868 |
[32m[20221208 14:12:40 @agent_ppo2.py:179][0m |          -0.0822 |         149.1544 |        -395.7824 |
[32m[20221208 14:12:40 @agent_ppo2.py:179][0m |          -0.0824 |         148.1645 |        -407.0421 |
[32m[20221208 14:12:40 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:12:40 @agent_ppo2.py:132][0m Average TRAINING episode reward: 574.45
[32m[20221208 14:12:40 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 886.60
[32m[20221208 14:12:40 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 591.94
[32m[20221208 14:12:40 @agent_ppo2.py:137][0m Total time:      27.33 min
[32m[20221208 14:12:40 @agent_ppo2.py:139][0m 2232320 total steps have happened
[32m[20221208 14:12:40 @agent_ppo2.py:115][0m #------------------------ Iteration 1090 --------------------------#
[32m[20221208 14:12:41 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:12:41 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |           0.0893 |         212.3095 |        -428.3074 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |           0.0553 |         183.7550 |        -361.5005 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |           0.0031 |         173.0957 |        -436.8899 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |          -0.0214 |         166.6836 |        -459.6095 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |          -0.0338 |         162.5100 |        -484.4909 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |          -0.0456 |         159.6561 |        -501.3316 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |          -0.0513 |         156.0983 |        -515.2900 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |          -0.0529 |         154.0177 |        -514.6398 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |          -0.0575 |         153.1616 |        -525.4058 |
[32m[20221208 14:12:41 @agent_ppo2.py:179][0m |          -0.0592 |         150.2592 |        -541.7780 |
[32m[20221208 14:12:41 @agent_ppo2.py:124][0m Policy update time: 0.76 s
[32m[20221208 14:12:42 @agent_ppo2.py:132][0m Average TRAINING episode reward: 895.60
[32m[20221208 14:12:42 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 981.49
[32m[20221208 14:12:42 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 858.98
[32m[20221208 14:12:42 @agent_ppo2.py:137][0m Total time:      27.36 min
[32m[20221208 14:12:42 @agent_ppo2.py:139][0m 2234368 total steps have happened
[32m[20221208 14:12:42 @agent_ppo2.py:115][0m #------------------------ Iteration 1091 --------------------------#
[32m[20221208 14:12:42 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:42 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:42 @agent_ppo2.py:179][0m |           0.0900 |         238.2731 |        -450.1729 |
[32m[20221208 14:12:42 @agent_ppo2.py:179][0m |           0.0675 |         218.7593 |        -364.5773 |
[32m[20221208 14:12:42 @agent_ppo2.py:179][0m |           0.0205 |         211.4229 |        -426.9930 |
[32m[20221208 14:12:42 @agent_ppo2.py:179][0m |          -0.0116 |         207.3567 |        -488.0081 |
[32m[20221208 14:12:43 @agent_ppo2.py:179][0m |          -0.0232 |         203.7381 |        -495.9063 |
[32m[20221208 14:12:43 @agent_ppo2.py:179][0m |          -0.0380 |         201.3499 |        -522.8641 |
[32m[20221208 14:12:43 @agent_ppo2.py:179][0m |          -0.0477 |         199.7878 |        -532.6336 |
[32m[20221208 14:12:43 @agent_ppo2.py:179][0m |          -0.0538 |         197.7524 |        -539.7243 |
[32m[20221208 14:12:43 @agent_ppo2.py:179][0m |          -0.0504 |         197.3878 |        -545.0018 |
[32m[20221208 14:12:43 @agent_ppo2.py:179][0m |          -0.0568 |         195.6436 |        -548.3485 |
[32m[20221208 14:12:43 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:12:43 @agent_ppo2.py:132][0m Average TRAINING episode reward: 834.80
[32m[20221208 14:12:43 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 966.80
[32m[20221208 14:12:43 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 854.89
[32m[20221208 14:12:43 @agent_ppo2.py:137][0m Total time:      27.38 min
[32m[20221208 14:12:43 @agent_ppo2.py:139][0m 2236416 total steps have happened
[32m[20221208 14:12:43 @agent_ppo2.py:115][0m #------------------------ Iteration 1092 --------------------------#
[32m[20221208 14:12:44 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:44 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |           0.0830 |         185.4090 |        -397.7757 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |           0.0253 |         170.8424 |        -284.2903 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |          -0.0011 |         166.0615 |        -321.7775 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |          -0.0318 |         162.3139 |        -347.6075 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |          -0.0459 |         159.9177 |        -370.7008 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |          -0.0615 |         158.4890 |        -390.0440 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |          -0.0689 |         156.3865 |        -399.5587 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |          -0.0734 |         155.5331 |        -415.6116 |
[32m[20221208 14:12:44 @agent_ppo2.py:179][0m |          -0.0784 |         155.1423 |        -427.2008 |
[32m[20221208 14:12:45 @agent_ppo2.py:179][0m |          -0.0794 |         154.0949 |        -434.7177 |
[32m[20221208 14:12:45 @agent_ppo2.py:124][0m Policy update time: 0.79 s
[32m[20221208 14:12:45 @agent_ppo2.py:132][0m Average TRAINING episode reward: 547.34
[32m[20221208 14:12:45 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 928.35
[32m[20221208 14:12:45 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 620.08
[32m[20221208 14:12:45 @agent_ppo2.py:137][0m Total time:      27.41 min
[32m[20221208 14:12:45 @agent_ppo2.py:139][0m 2238464 total steps have happened
[32m[20221208 14:12:45 @agent_ppo2.py:115][0m #------------------------ Iteration 1093 --------------------------#
[32m[20221208 14:12:45 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:12:45 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:45 @agent_ppo2.py:179][0m |           0.0757 |         219.9367 |        -460.0358 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |           0.0854 |         204.3080 |        -337.7233 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |           0.0271 |         194.8627 |        -421.8298 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |          -0.0094 |         188.9748 |        -475.6562 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |          -0.0261 |         185.8239 |        -503.0030 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |          -0.0330 |         182.8621 |        -521.2868 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |          -0.0411 |         178.7253 |        -527.7068 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |          -0.0488 |         177.7514 |        -557.3675 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |          -0.0346 |         174.6366 |        -536.1593 |
[32m[20221208 14:12:46 @agent_ppo2.py:179][0m |          -0.0449 |         172.8379 |        -549.3338 |
[32m[20221208 14:12:46 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:12:46 @agent_ppo2.py:132][0m Average TRAINING episode reward: 845.73
[32m[20221208 14:12:46 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 920.24
[32m[20221208 14:12:46 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.44
[32m[20221208 14:12:46 @agent_ppo2.py:137][0m Total time:      27.43 min
[32m[20221208 14:12:46 @agent_ppo2.py:139][0m 2240512 total steps have happened
[32m[20221208 14:12:46 @agent_ppo2.py:115][0m #------------------------ Iteration 1094 --------------------------#
[32m[20221208 14:12:47 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:12:47 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |           0.0914 |         222.5673 |        -487.8203 |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |           0.0735 |         207.0279 |        -336.3640 |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |           0.0312 |         201.8606 |        -448.9680 |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |           0.0072 |         197.3211 |        -494.1818 |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |          -0.0131 |         193.6906 |        -525.5814 |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |          -0.0220 |         192.0444 |        -537.7171 |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |          -0.0328 |         189.7159 |        -545.9454 |
[32m[20221208 14:12:47 @agent_ppo2.py:179][0m |          -0.0429 |         187.8503 |        -572.7569 |
[32m[20221208 14:12:48 @agent_ppo2.py:179][0m |          -0.0490 |         187.0251 |        -582.2767 |
[32m[20221208 14:12:48 @agent_ppo2.py:179][0m |          -0.0477 |         185.3743 |        -597.0256 |
[32m[20221208 14:12:48 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:12:48 @agent_ppo2.py:132][0m Average TRAINING episode reward: 895.91
[32m[20221208 14:12:48 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 927.47
[32m[20221208 14:12:48 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 880.03
[32m[20221208 14:12:48 @agent_ppo2.py:137][0m Total time:      27.46 min
[32m[20221208 14:12:48 @agent_ppo2.py:139][0m 2242560 total steps have happened
[32m[20221208 14:12:48 @agent_ppo2.py:115][0m #------------------------ Iteration 1095 --------------------------#
[32m[20221208 14:12:49 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:12:49 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |           0.0632 |         229.3785 |        -517.8802 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |           0.0685 |         221.6560 |        -332.7114 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |           0.0324 |         215.2349 |        -397.9558 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |          -0.0003 |         211.0573 |        -475.1027 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |          -0.0192 |         207.3037 |        -522.1479 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |          -0.0291 |         203.6158 |        -539.0622 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |          -0.0384 |         201.8056 |        -557.8520 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |          -0.0410 |         201.6393 |        -576.3580 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |          -0.0444 |         198.9933 |        -576.7273 |
[32m[20221208 14:12:49 @agent_ppo2.py:179][0m |          -0.0497 |         198.1606 |        -587.4211 |
[32m[20221208 14:12:49 @agent_ppo2.py:124][0m Policy update time: 0.86 s
[32m[20221208 14:12:50 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.44
[32m[20221208 14:12:50 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.00
[32m[20221208 14:12:50 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 817.83
[32m[20221208 14:12:50 @agent_ppo2.py:137][0m Total time:      27.49 min
[32m[20221208 14:12:50 @agent_ppo2.py:139][0m 2244608 total steps have happened
[32m[20221208 14:12:50 @agent_ppo2.py:115][0m #------------------------ Iteration 1096 --------------------------#
[32m[20221208 14:12:50 @agent_ppo2.py:121][0m Sampling time: 0.46 s by 1 slaves
[32m[20221208 14:12:50 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:50 @agent_ppo2.py:179][0m |           0.1040 |         186.9613 |        -436.1452 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |           0.0503 |         175.3111 |        -353.4826 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0106 |         172.9333 |        -408.7993 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0374 |         170.0964 |        -445.7700 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0488 |         168.7360 |        -464.8773 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0565 |         168.0504 |        -480.2885 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0694 |         167.0979 |        -504.3904 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0752 |         165.7027 |        -519.3799 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0796 |         165.0592 |        -527.5859 |
[32m[20221208 14:12:51 @agent_ppo2.py:179][0m |          -0.0816 |         164.3210 |        -544.7526 |
[32m[20221208 14:12:51 @agent_ppo2.py:124][0m Policy update time: 0.77 s
[32m[20221208 14:12:51 @agent_ppo2.py:132][0m Average TRAINING episode reward: 683.81
[32m[20221208 14:12:51 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 962.93
[32m[20221208 14:12:51 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 909.53
[32m[20221208 14:12:51 @agent_ppo2.py:137][0m Total time:      27.52 min
[32m[20221208 14:12:51 @agent_ppo2.py:139][0m 2246656 total steps have happened
[32m[20221208 14:12:51 @agent_ppo2.py:115][0m #------------------------ Iteration 1097 --------------------------#
[32m[20221208 14:12:52 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:52 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:52 @agent_ppo2.py:179][0m |           0.0764 |         177.4575 |        -448.1504 |
[32m[20221208 14:12:52 @agent_ppo2.py:179][0m |           0.0304 |         160.1376 |        -381.2115 |
[32m[20221208 14:12:52 @agent_ppo2.py:179][0m |           0.0042 |         155.2212 |        -425.2638 |
[32m[20221208 14:12:52 @agent_ppo2.py:179][0m |          -0.0336 |         150.6550 |        -480.4518 |
[32m[20221208 14:12:52 @agent_ppo2.py:179][0m |          -0.0457 |         148.1370 |        -506.3148 |
[32m[20221208 14:12:52 @agent_ppo2.py:179][0m |          -0.0568 |         145.1079 |        -528.0700 |
[32m[20221208 14:12:52 @agent_ppo2.py:179][0m |          -0.0598 |         143.6531 |        -543.4938 |
[32m[20221208 14:12:53 @agent_ppo2.py:179][0m |          -0.0664 |         141.8597 |        -549.7641 |
[32m[20221208 14:12:53 @agent_ppo2.py:179][0m |          -0.0683 |         141.1014 |        -554.4386 |
[32m[20221208 14:12:53 @agent_ppo2.py:179][0m |          -0.0725 |         140.4493 |        -570.7121 |
[32m[20221208 14:12:53 @agent_ppo2.py:124][0m Policy update time: 0.75 s
[32m[20221208 14:12:53 @agent_ppo2.py:132][0m Average TRAINING episode reward: 668.10
[32m[20221208 14:12:53 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 892.25
[32m[20221208 14:12:53 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 847.38
[32m[20221208 14:12:53 @agent_ppo2.py:137][0m Total time:      27.55 min
[32m[20221208 14:12:53 @agent_ppo2.py:139][0m 2248704 total steps have happened
[32m[20221208 14:12:53 @agent_ppo2.py:115][0m #------------------------ Iteration 1098 --------------------------#
[32m[20221208 14:12:54 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:54 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |           0.0808 |         229.9371 |        -591.2704 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |           0.1042 |         217.5763 |        -397.5012 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |           0.0504 |         212.0705 |        -416.5835 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |           0.0075 |         206.9017 |        -541.7492 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |          -0.0070 |         204.4932 |        -579.8734 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |          -0.0193 |         203.0282 |        -612.0114 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |          -0.0312 |         200.6550 |        -647.0952 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |          -0.0344 |         199.0362 |        -663.7419 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |          -0.0431 |         196.8986 |        -685.9039 |
[32m[20221208 14:12:54 @agent_ppo2.py:179][0m |          -0.0468 |         197.0016 |        -701.7703 |
[32m[20221208 14:12:54 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:12:55 @agent_ppo2.py:132][0m Average TRAINING episode reward: 870.06
[32m[20221208 14:12:55 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 905.93
[32m[20221208 14:12:55 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 649.80
[32m[20221208 14:12:55 @agent_ppo2.py:137][0m Total time:      27.57 min
[32m[20221208 14:12:55 @agent_ppo2.py:139][0m 2250752 total steps have happened
[32m[20221208 14:12:55 @agent_ppo2.py:115][0m #------------------------ Iteration 1099 --------------------------#
[32m[20221208 14:12:55 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:12:55 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:55 @agent_ppo2.py:179][0m |           0.0787 |         232.4126 |        -555.2681 |
[32m[20221208 14:12:55 @agent_ppo2.py:179][0m |           0.0891 |         214.1432 |        -434.6839 |
[32m[20221208 14:12:55 @agent_ppo2.py:179][0m |           0.0253 |         209.6437 |        -499.9927 |
[32m[20221208 14:12:55 @agent_ppo2.py:179][0m |           0.0022 |         206.9298 |        -513.4770 |
[32m[20221208 14:12:55 @agent_ppo2.py:179][0m |          -0.0225 |         204.9256 |        -574.6513 |
[32m[20221208 14:12:56 @agent_ppo2.py:179][0m |          -0.0335 |         202.6680 |        -596.0085 |
[32m[20221208 14:12:56 @agent_ppo2.py:179][0m |          -0.0424 |         202.4178 |        -610.6185 |
[32m[20221208 14:12:56 @agent_ppo2.py:179][0m |          -0.0458 |         200.0531 |        -623.2959 |
[32m[20221208 14:12:56 @agent_ppo2.py:179][0m |          -0.0522 |         199.4705 |        -628.0245 |
[32m[20221208 14:12:56 @agent_ppo2.py:179][0m |          -0.0539 |         198.8229 |        -640.7581 |
[32m[20221208 14:12:56 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:12:56 @agent_ppo2.py:132][0m Average TRAINING episode reward: 847.39
[32m[20221208 14:12:56 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.11
[32m[20221208 14:12:56 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 655.97
[32m[20221208 14:12:56 @agent_ppo2.py:137][0m Total time:      27.60 min
[32m[20221208 14:12:56 @agent_ppo2.py:139][0m 2252800 total steps have happened
[32m[20221208 14:12:56 @agent_ppo2.py:115][0m #------------------------ Iteration 1100 --------------------------#
[32m[20221208 14:12:57 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:12:57 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |           0.0681 |         236.0185 |        -567.0433 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |           0.0586 |         225.3858 |        -444.7681 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |           0.0290 |         221.4765 |        -500.2726 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |           0.0023 |         218.6678 |        -588.0339 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |          -0.0141 |         216.7718 |        -626.9971 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |          -0.0273 |         215.0153 |        -668.5168 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |          -0.0357 |         214.4505 |        -689.1978 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |          -0.0397 |         213.8117 |        -705.5897 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |          -0.0438 |         213.2346 |        -727.9202 |
[32m[20221208 14:12:57 @agent_ppo2.py:179][0m |          -0.0449 |         211.9147 |        -731.7890 |
[32m[20221208 14:12:57 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:12:58 @agent_ppo2.py:132][0m Average TRAINING episode reward: 938.74
[32m[20221208 14:12:58 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 946.85
[32m[20221208 14:12:58 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 918.31
[32m[20221208 14:12:58 @agent_ppo2.py:137][0m Total time:      27.62 min
[32m[20221208 14:12:58 @agent_ppo2.py:139][0m 2254848 total steps have happened
[32m[20221208 14:12:58 @agent_ppo2.py:115][0m #------------------------ Iteration 1101 --------------------------#
[32m[20221208 14:12:58 @agent_ppo2.py:121][0m Sampling time: 0.45 s by 1 slaves
[32m[20221208 14:12:58 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:12:58 @agent_ppo2.py:179][0m |           0.1210 |         237.3134 |        -474.3099 |
[32m[20221208 14:12:58 @agent_ppo2.py:179][0m |           0.1083 |         210.1356 |        -248.0341 |
[32m[20221208 14:12:58 @agent_ppo2.py:179][0m |           0.0633 |         199.0796 |        -381.5867 |
[32m[20221208 14:12:58 @agent_ppo2.py:179][0m |           0.0227 |         191.4740 |        -480.5467 |
[32m[20221208 14:12:59 @agent_ppo2.py:179][0m |          -0.0084 |         186.2671 |        -555.6233 |
[32m[20221208 14:12:59 @agent_ppo2.py:179][0m |          -0.0179 |         182.9481 |        -590.0636 |
[32m[20221208 14:12:59 @agent_ppo2.py:179][0m |          -0.0279 |         180.3640 |        -606.6087 |
[32m[20221208 14:12:59 @agent_ppo2.py:179][0m |          -0.0362 |         176.7925 |        -632.8066 |
[32m[20221208 14:12:59 @agent_ppo2.py:179][0m |          -0.0377 |         174.2596 |        -635.1976 |
[32m[20221208 14:12:59 @agent_ppo2.py:179][0m |          -0.0451 |         171.6861 |        -648.3715 |
[32m[20221208 14:12:59 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:12:59 @agent_ppo2.py:132][0m Average TRAINING episode reward: 850.94
[32m[20221208 14:12:59 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.09
[32m[20221208 14:12:59 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 893.75
[32m[20221208 14:12:59 @agent_ppo2.py:137][0m Total time:      27.65 min
[32m[20221208 14:12:59 @agent_ppo2.py:139][0m 2256896 total steps have happened
[32m[20221208 14:12:59 @agent_ppo2.py:115][0m #------------------------ Iteration 1102 --------------------------#
[32m[20221208 14:13:00 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:00 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |           0.0468 |         232.2554 |        -618.3086 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |           0.0423 |         222.6348 |        -496.9723 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0100 |         217.1326 |        -595.9364 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0278 |         213.9725 |        -636.5439 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0381 |         213.4987 |        -664.8371 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0454 |         210.7565 |        -681.7410 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0504 |         208.4073 |        -701.1525 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0527 |         207.4978 |        -708.7089 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0554 |         205.5301 |        -722.8252 |
[32m[20221208 14:13:00 @agent_ppo2.py:179][0m |          -0.0576 |         204.7499 |        -729.8604 |
[32m[20221208 14:13:00 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:13:01 @agent_ppo2.py:132][0m Average TRAINING episode reward: 869.70
[32m[20221208 14:13:01 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 982.72
[32m[20221208 14:13:01 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 748.42
[32m[20221208 14:13:01 @agent_ppo2.py:137][0m Total time:      27.67 min
[32m[20221208 14:13:01 @agent_ppo2.py:139][0m 2258944 total steps have happened
[32m[20221208 14:13:01 @agent_ppo2.py:115][0m #------------------------ Iteration 1103 --------------------------#
[32m[20221208 14:13:01 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:01 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:01 @agent_ppo2.py:179][0m |           0.0610 |         242.0586 |        -602.8969 |
[32m[20221208 14:13:01 @agent_ppo2.py:179][0m |           0.0937 |         229.4785 |        -468.4708 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |           0.0435 |         222.1602 |        -520.5177 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |           0.0037 |         218.5887 |        -574.5568 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |          -0.0197 |         215.2886 |        -628.7585 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |          -0.0275 |         213.8777 |        -647.5260 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |          -0.0274 |         211.6619 |        -655.9967 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |          -0.0285 |         210.3233 |        -660.7005 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |          -0.0285 |         209.6351 |        -658.7651 |
[32m[20221208 14:13:02 @agent_ppo2.py:179][0m |          -0.0227 |         208.4251 |        -634.8362 |
[32m[20221208 14:13:02 @agent_ppo2.py:124][0m Policy update time: 0.74 s
[32m[20221208 14:13:02 @agent_ppo2.py:132][0m Average TRAINING episode reward: 829.02
[32m[20221208 14:13:02 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.75
[32m[20221208 14:13:02 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 955.19
[32m[20221208 14:13:02 @agent_ppo2.py:137][0m Total time:      27.70 min
[32m[20221208 14:13:02 @agent_ppo2.py:139][0m 2260992 total steps have happened
[32m[20221208 14:13:02 @agent_ppo2.py:115][0m #------------------------ Iteration 1104 --------------------------#
[32m[20221208 14:13:03 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:13:03 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |           0.0899 |         200.4286 |        -456.3393 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |           0.0438 |         179.9083 |        -399.5090 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0236 |         172.2020 |        -479.4428 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0470 |         166.3705 |        -504.7567 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0624 |         162.5284 |        -528.8775 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0651 |         158.0091 |        -550.9965 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0692 |         154.8349 |        -576.6625 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0714 |         152.3102 |        -580.4960 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0787 |         149.5400 |        -599.2721 |
[32m[20221208 14:13:03 @agent_ppo2.py:179][0m |          -0.0838 |         146.3970 |        -614.9835 |
[32m[20221208 14:13:03 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:13:04 @agent_ppo2.py:132][0m Average TRAINING episode reward: 623.01
[32m[20221208 14:13:04 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 932.49
[32m[20221208 14:13:04 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 786.35
[32m[20221208 14:13:04 @agent_ppo2.py:137][0m Total time:      27.73 min
[32m[20221208 14:13:04 @agent_ppo2.py:139][0m 2263040 total steps have happened
[32m[20221208 14:13:04 @agent_ppo2.py:115][0m #------------------------ Iteration 1105 --------------------------#
[32m[20221208 14:13:04 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:04 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:04 @agent_ppo2.py:179][0m |           0.0838 |         251.7403 |        -654.1435 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |           0.0877 |         238.0063 |        -526.4303 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |           0.0252 |         232.0676 |        -588.5544 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |           0.0006 |         229.3636 |        -659.4179 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |          -0.0212 |         227.2495 |        -711.8361 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |          -0.0251 |         224.7191 |        -737.7006 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |          -0.0349 |         223.3600 |        -749.6667 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |          -0.0465 |         221.4996 |        -796.0006 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |          -0.0506 |         219.9344 |        -815.4326 |
[32m[20221208 14:13:05 @agent_ppo2.py:179][0m |          -0.0523 |         219.3416 |        -834.0111 |
[32m[20221208 14:13:05 @agent_ppo2.py:124][0m Policy update time: 0.78 s
[32m[20221208 14:13:06 @agent_ppo2.py:132][0m Average TRAINING episode reward: 913.42
[32m[20221208 14:13:06 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 947.55
[32m[20221208 14:13:06 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 767.79
[32m[20221208 14:13:06 @agent_ppo2.py:137][0m Total time:      27.75 min
[32m[20221208 14:13:06 @agent_ppo2.py:139][0m 2265088 total steps have happened
[32m[20221208 14:13:06 @agent_ppo2.py:115][0m #------------------------ Iteration 1106 --------------------------#
[32m[20221208 14:13:06 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:13:06 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:06 @agent_ppo2.py:179][0m |           0.1368 |         204.2118 |        -525.5047 |
[32m[20221208 14:13:06 @agent_ppo2.py:179][0m |           0.0408 |         188.8863 |        -351.2997 |
[32m[20221208 14:13:06 @agent_ppo2.py:179][0m |          -0.0090 |         184.9977 |        -461.8662 |
[32m[20221208 14:13:06 @agent_ppo2.py:179][0m |          -0.0368 |         182.4748 |        -509.6461 |
[32m[20221208 14:13:06 @agent_ppo2.py:179][0m |          -0.0540 |         180.5396 |        -536.7631 |
[32m[20221208 14:13:06 @agent_ppo2.py:179][0m |          -0.0624 |         179.8063 |        -554.5745 |
[32m[20221208 14:13:06 @agent_ppo2.py:179][0m |          -0.0693 |         178.6044 |        -575.3710 |
[32m[20221208 14:13:07 @agent_ppo2.py:179][0m |          -0.0764 |         178.0528 |        -599.8467 |
[32m[20221208 14:13:07 @agent_ppo2.py:179][0m |          -0.0766 |         177.2933 |        -609.9712 |
[32m[20221208 14:13:07 @agent_ppo2.py:179][0m |          -0.0788 |         177.0960 |        -625.7361 |
[32m[20221208 14:13:07 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:13:07 @agent_ppo2.py:132][0m Average TRAINING episode reward: 601.27
[32m[20221208 14:13:07 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 924.10
[32m[20221208 14:13:07 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 795.99
[32m[20221208 14:13:07 @agent_ppo2.py:137][0m Total time:      27.78 min
[32m[20221208 14:13:07 @agent_ppo2.py:139][0m 2267136 total steps have happened
[32m[20221208 14:13:07 @agent_ppo2.py:115][0m #------------------------ Iteration 1107 --------------------------#
[32m[20221208 14:13:08 @agent_ppo2.py:121][0m Sampling time: 0.48 s by 1 slaves
[32m[20221208 14:13:08 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |           0.0851 |         194.4811 |        -543.0857 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |           0.0328 |         181.7376 |        -455.3318 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0066 |         177.8580 |        -481.3082 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0394 |         175.2084 |        -531.9171 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0508 |         174.5552 |        -570.6675 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0536 |         172.9105 |        -571.3171 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0655 |         170.6299 |        -604.3041 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0718 |         170.3562 |        -612.7248 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0792 |         167.9148 |        -634.6199 |
[32m[20221208 14:13:08 @agent_ppo2.py:179][0m |          -0.0826 |         167.4978 |        -646.6188 |
[32m[20221208 14:13:08 @agent_ppo2.py:124][0m Policy update time: 0.81 s
[32m[20221208 14:13:09 @agent_ppo2.py:132][0m Average TRAINING episode reward: 587.20
[32m[20221208 14:13:09 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 895.37
[32m[20221208 14:13:09 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 952.11
[32m[20221208 14:13:09 @agent_ppo2.py:137][0m Total time:      27.81 min
[32m[20221208 14:13:09 @agent_ppo2.py:139][0m 2269184 total steps have happened
[32m[20221208 14:13:09 @agent_ppo2.py:115][0m #------------------------ Iteration 1108 --------------------------#
[32m[20221208 14:13:09 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:09 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:09 @agent_ppo2.py:179][0m |           0.1230 |         233.0284 |        -692.3622 |
[32m[20221208 14:13:09 @agent_ppo2.py:179][0m |           0.0854 |         216.5783 |        -543.1689 |
[32m[20221208 14:13:09 @agent_ppo2.py:179][0m |           0.0396 |         210.1206 |        -598.8595 |
[32m[20221208 14:13:10 @agent_ppo2.py:179][0m |           0.0058 |         206.3435 |        -689.9262 |
[32m[20221208 14:13:10 @agent_ppo2.py:179][0m |          -0.0149 |         203.7321 |        -747.5484 |
[32m[20221208 14:13:10 @agent_ppo2.py:179][0m |          -0.0285 |         200.4767 |        -778.9287 |
[32m[20221208 14:13:10 @agent_ppo2.py:179][0m |          -0.0379 |         197.8719 |        -797.8379 |
[32m[20221208 14:13:10 @agent_ppo2.py:179][0m |          -0.0393 |         197.7518 |        -806.2738 |
[32m[20221208 14:13:10 @agent_ppo2.py:179][0m |          -0.0424 |         194.7740 |        -820.7018 |
[32m[20221208 14:13:10 @agent_ppo2.py:179][0m |          -0.0485 |         193.2012 |        -836.1461 |
[32m[20221208 14:13:10 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:13:10 @agent_ppo2.py:132][0m Average TRAINING episode reward: 870.52
[32m[20221208 14:13:10 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 998.52
[32m[20221208 14:13:10 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 836.32
[32m[20221208 14:13:10 @agent_ppo2.py:137][0m Total time:      27.83 min
[32m[20221208 14:13:10 @agent_ppo2.py:139][0m 2271232 total steps have happened
[32m[20221208 14:13:10 @agent_ppo2.py:115][0m #------------------------ Iteration 1109 --------------------------#
[32m[20221208 14:13:11 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:13:11 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |           0.0955 |         235.4487 |        -726.1666 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |           0.0686 |         223.5761 |        -646.1791 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |           0.0138 |         217.8966 |        -740.3612 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |          -0.0055 |         212.2757 |        -788.7846 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |          -0.0245 |         208.9351 |        -836.6464 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |          -0.0354 |         206.6558 |        -849.4463 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |          -0.0435 |         205.3327 |        -879.9579 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |          -0.0449 |         202.3670 |        -889.1047 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |          -0.0483 |         200.3596 |        -887.8156 |
[32m[20221208 14:13:11 @agent_ppo2.py:179][0m |          -0.0573 |         199.7996 |        -921.3874 |
[32m[20221208 14:13:11 @agent_ppo2.py:124][0m Policy update time: 0.70 s
[32m[20221208 14:13:12 @agent_ppo2.py:132][0m Average TRAINING episode reward: 809.21
[32m[20221208 14:13:12 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 850.12
[32m[20221208 14:13:12 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 892.21
[32m[20221208 14:13:12 @agent_ppo2.py:137][0m Total time:      27.86 min
[32m[20221208 14:13:12 @agent_ppo2.py:139][0m 2273280 total steps have happened
[32m[20221208 14:13:12 @agent_ppo2.py:115][0m #------------------------ Iteration 1110 --------------------------#
[32m[20221208 14:13:12 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:13:12 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:12 @agent_ppo2.py:179][0m |           0.0985 |         216.9153 |        -713.9355 |
[32m[20221208 14:13:12 @agent_ppo2.py:179][0m |           0.0668 |         191.4652 |        -451.8176 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |           0.0143 |         178.3489 |        -544.5813 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |          -0.0229 |         170.5570 |        -617.6085 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |          -0.0339 |         165.6224 |        -643.1387 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |          -0.0421 |         162.8150 |        -684.0673 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |          -0.0511 |         157.4576 |        -716.8877 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |          -0.0609 |         154.0064 |        -723.6420 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |          -0.0665 |         150.1426 |        -742.4601 |
[32m[20221208 14:13:13 @agent_ppo2.py:179][0m |          -0.0695 |         147.6265 |        -760.9290 |
[32m[20221208 14:13:13 @agent_ppo2.py:124][0m Policy update time: 0.69 s
[32m[20221208 14:13:13 @agent_ppo2.py:132][0m Average TRAINING episode reward: 543.01
[32m[20221208 14:13:13 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 834.69
[32m[20221208 14:13:13 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 886.35
[32m[20221208 14:13:13 @agent_ppo2.py:137][0m Total time:      27.88 min
[32m[20221208 14:13:13 @agent_ppo2.py:139][0m 2275328 total steps have happened
[32m[20221208 14:13:13 @agent_ppo2.py:115][0m #------------------------ Iteration 1111 --------------------------#
[32m[20221208 14:13:14 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:14 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |           0.0385 |         236.1238 |        -855.5099 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |           0.0237 |         223.1644 |        -777.6871 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |           0.0271 |         214.1825 |        -727.7901 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |          -0.0153 |         210.1845 |        -801.6554 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |          -0.0252 |         206.8259 |        -840.8546 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |          -0.0372 |         204.6862 |        -868.1678 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |          -0.0429 |         203.7435 |        -879.4193 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |          -0.0493 |         200.6779 |        -905.3243 |
[32m[20221208 14:13:14 @agent_ppo2.py:179][0m |          -0.0523 |         199.4889 |        -918.5531 |
[32m[20221208 14:13:15 @agent_ppo2.py:179][0m |          -0.0526 |         198.4255 |        -943.5448 |
[32m[20221208 14:13:15 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:13:15 @agent_ppo2.py:132][0m Average TRAINING episode reward: 935.05
[32m[20221208 14:13:15 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 964.47
[32m[20221208 14:13:15 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 938.23
[32m[20221208 14:13:15 @agent_ppo2.py:137][0m Total time:      27.91 min
[32m[20221208 14:13:15 @agent_ppo2.py:139][0m 2277376 total steps have happened
[32m[20221208 14:13:15 @agent_ppo2.py:115][0m #------------------------ Iteration 1112 --------------------------#
[32m[20221208 14:13:15 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:13:15 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:15 @agent_ppo2.py:179][0m |           0.0845 |         217.3064 |        -811.6548 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |           0.1053 |         208.4826 |        -522.8389 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |           0.0710 |         205.7814 |        -526.9010 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |           0.0315 |         202.6588 |        -672.7581 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |           0.0133 |         201.6133 |        -709.8242 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |          -0.0115 |         199.9974 |        -828.1041 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |          -0.0251 |         198.9034 |        -867.4900 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |          -0.0315 |         199.0936 |        -898.8092 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |          -0.0384 |         198.9822 |        -928.2786 |
[32m[20221208 14:13:16 @agent_ppo2.py:179][0m |          -0.0463 |         196.3887 |        -955.9421 |
[32m[20221208 14:13:16 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:13:17 @agent_ppo2.py:132][0m Average TRAINING episode reward: 905.22
[32m[20221208 14:13:17 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 984.40
[32m[20221208 14:13:17 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 895.31
[32m[20221208 14:13:17 @agent_ppo2.py:137][0m Total time:      27.94 min
[32m[20221208 14:13:17 @agent_ppo2.py:139][0m 2279424 total steps have happened
[32m[20221208 14:13:17 @agent_ppo2.py:115][0m #------------------------ Iteration 1113 --------------------------#
[32m[20221208 14:13:17 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:13:17 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |           0.0730 |         213.1490 |        -837.3693 |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |           0.0746 |         195.7424 |        -585.4350 |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |           0.0314 |         189.8921 |        -665.2834 |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |           0.0076 |         185.4341 |        -773.2700 |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |          -0.0032 |         182.6728 |        -798.3684 |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |          -0.0254 |         179.8383 |        -847.0914 |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |          -0.0350 |         177.9423 |        -896.2950 |
[32m[20221208 14:13:17 @agent_ppo2.py:179][0m |          -0.0419 |         175.3039 |        -908.1699 |
[32m[20221208 14:13:18 @agent_ppo2.py:179][0m |          -0.0467 |         173.8156 |        -936.7836 |
[32m[20221208 14:13:18 @agent_ppo2.py:179][0m |          -0.0492 |         171.4999 |        -948.0340 |
[32m[20221208 14:13:18 @agent_ppo2.py:124][0m Policy update time: 0.67 s
[32m[20221208 14:13:18 @agent_ppo2.py:132][0m Average TRAINING episode reward: 837.74
[32m[20221208 14:13:18 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 862.47
[32m[20221208 14:13:18 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 873.97
[32m[20221208 14:13:18 @agent_ppo2.py:137][0m Total time:      27.96 min
[32m[20221208 14:13:18 @agent_ppo2.py:139][0m 2281472 total steps have happened
[32m[20221208 14:13:18 @agent_ppo2.py:115][0m #------------------------ Iteration 1114 --------------------------#
[32m[20221208 14:13:18 @agent_ppo2.py:121][0m Sampling time: 0.42 s by 1 slaves
[32m[20221208 14:13:18 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |           0.0645 |         229.4474 |        -798.4994 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |           0.0644 |         219.3316 |        -604.8753 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |           0.0375 |         214.5933 |        -660.3388 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |           0.0185 |         211.8875 |        -698.2238 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |          -0.0057 |         209.1429 |        -786.5774 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |          -0.0207 |         208.7407 |        -844.8316 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |          -0.0294 |         206.4108 |        -861.9730 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |          -0.0204 |         207.4708 |        -862.3088 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |          -0.0300 |         205.0209 |        -877.3632 |
[32m[20221208 14:13:19 @agent_ppo2.py:179][0m |          -0.0439 |         205.5225 |        -924.1808 |
[32m[20221208 14:13:19 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:13:20 @agent_ppo2.py:132][0m Average TRAINING episode reward: 849.06
[32m[20221208 14:13:20 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 935.48
[32m[20221208 14:13:20 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 861.30
[32m[20221208 14:13:20 @agent_ppo2.py:137][0m Total time:      27.99 min
[32m[20221208 14:13:20 @agent_ppo2.py:139][0m 2283520 total steps have happened
[32m[20221208 14:13:20 @agent_ppo2.py:115][0m #------------------------ Iteration 1115 --------------------------#
[32m[20221208 14:13:20 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:20 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:20 @agent_ppo2.py:179][0m |           0.0585 |         228.6117 |        -921.0585 |
[32m[20221208 14:13:20 @agent_ppo2.py:179][0m |           0.0267 |         210.2522 |        -820.0529 |
[32m[20221208 14:13:20 @agent_ppo2.py:179][0m |           0.0078 |         201.2449 |        -871.7383 |
[32m[20221208 14:13:20 @agent_ppo2.py:179][0m |          -0.0236 |         195.3281 |        -924.2780 |
[32m[20221208 14:13:20 @agent_ppo2.py:179][0m |          -0.0351 |         191.5751 |        -955.3324 |
[32m[20221208 14:13:20 @agent_ppo2.py:179][0m |          -0.0425 |         188.6483 |        -987.4465 |
[32m[20221208 14:13:20 @agent_ppo2.py:179][0m |          -0.0465 |         185.8329 |       -1005.0882 |
[32m[20221208 14:13:21 @agent_ppo2.py:179][0m |          -0.0496 |         184.7950 |       -1015.6110 |
[32m[20221208 14:13:21 @agent_ppo2.py:179][0m |          -0.0554 |         180.9765 |       -1036.9062 |
[32m[20221208 14:13:21 @agent_ppo2.py:179][0m |          -0.0600 |         179.8451 |       -1047.9181 |
[32m[20221208 14:13:21 @agent_ppo2.py:124][0m Policy update time: 0.68 s
[32m[20221208 14:13:21 @agent_ppo2.py:132][0m Average TRAINING episode reward: 850.56
[32m[20221208 14:13:21 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 936.09
[32m[20221208 14:13:21 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 824.94
[32m[20221208 14:13:21 @agent_ppo2.py:137][0m Total time:      28.01 min
[32m[20221208 14:13:21 @agent_ppo2.py:139][0m 2285568 total steps have happened
[32m[20221208 14:13:21 @agent_ppo2.py:115][0m #------------------------ Iteration 1116 --------------------------#
[32m[20221208 14:13:22 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:22 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |           0.0852 |         217.8464 |        -821.0854 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |           0.1406 |         198.3883 |        -689.2481 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |           0.0255 |         188.8997 |        -782.0264 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |          -0.0025 |         181.5262 |        -831.9768 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |          -0.0138 |         176.4560 |        -895.4927 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |          -0.0272 |         172.2666 |        -917.4314 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |          -0.0352 |         169.1332 |        -961.3176 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |          -0.0449 |         165.9010 |        -979.9732 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |          -0.0501 |         163.9472 |       -1008.3392 |
[32m[20221208 14:13:22 @agent_ppo2.py:179][0m |          -0.0508 |         161.3655 |       -1011.5699 |
[32m[20221208 14:13:22 @agent_ppo2.py:124][0m Policy update time: 0.86 s
[32m[20221208 14:13:23 @agent_ppo2.py:132][0m Average TRAINING episode reward: 841.03
[32m[20221208 14:13:23 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 891.52
[32m[20221208 14:13:23 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 864.42
[32m[20221208 14:13:23 @agent_ppo2.py:137][0m Total time:      28.04 min
[32m[20221208 14:13:23 @agent_ppo2.py:139][0m 2287616 total steps have happened
[32m[20221208 14:13:23 @agent_ppo2.py:115][0m #------------------------ Iteration 1117 --------------------------#
[32m[20221208 14:13:23 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:13:23 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:23 @agent_ppo2.py:179][0m |           0.0868 |         178.2991 |        -703.5933 |
[32m[20221208 14:13:23 @agent_ppo2.py:179][0m |           0.0146 |         159.5725 |        -615.4630 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0179 |         151.2193 |        -695.5789 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0457 |         147.2258 |        -736.7289 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0616 |         142.3583 |        -795.8684 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0713 |         139.7399 |        -823.4916 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0789 |         137.3903 |        -845.9240 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0837 |         135.0939 |        -874.2768 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0849 |         134.0167 |        -882.9008 |
[32m[20221208 14:13:24 @agent_ppo2.py:179][0m |          -0.0869 |         131.8580 |        -905.8219 |
[32m[20221208 14:13:24 @agent_ppo2.py:124][0m Policy update time: 0.72 s
[32m[20221208 14:13:24 @agent_ppo2.py:132][0m Average TRAINING episode reward: 515.51
[32m[20221208 14:13:24 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 784.65
[32m[20221208 14:13:24 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 769.39
[32m[20221208 14:13:24 @agent_ppo2.py:137][0m Total time:      28.07 min
[32m[20221208 14:13:24 @agent_ppo2.py:139][0m 2289664 total steps have happened
[32m[20221208 14:13:24 @agent_ppo2.py:115][0m #------------------------ Iteration 1118 --------------------------#
[32m[20221208 14:13:25 @agent_ppo2.py:121][0m Sampling time: 0.44 s by 1 slaves
[32m[20221208 14:13:25 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |           0.0702 |         235.7217 |        -944.5805 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |           0.0707 |         221.1755 |        -763.6427 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |           0.0249 |         216.5319 |        -862.4624 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |          -0.0031 |         214.2674 |        -990.8659 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |          -0.0199 |         212.2714 |       -1017.8137 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |          -0.0249 |         210.5125 |       -1017.8254 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |          -0.0403 |         209.4191 |       -1076.5026 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |          -0.0482 |         207.7792 |       -1109.3104 |
[32m[20221208 14:13:25 @agent_ppo2.py:179][0m |          -0.0519 |         206.9620 |       -1131.8270 |
[32m[20221208 14:13:26 @agent_ppo2.py:179][0m |          -0.0521 |         206.0513 |       -1133.0935 |
[32m[20221208 14:13:26 @agent_ppo2.py:124][0m Policy update time: 0.71 s
[32m[20221208 14:13:26 @agent_ppo2.py:132][0m Average TRAINING episode reward: 916.70
[32m[20221208 14:13:26 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 974.82
[32m[20221208 14:13:26 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 109.00
[32m[20221208 14:13:26 @agent_ppo2.py:137][0m Total time:      28.09 min
[32m[20221208 14:13:26 @agent_ppo2.py:139][0m 2291712 total steps have happened
[32m[20221208 14:13:26 @agent_ppo2.py:115][0m #------------------------ Iteration 1119 --------------------------#
[32m[20221208 14:13:26 @agent_ppo2.py:121][0m Sampling time: 0.43 s by 1 slaves
[32m[20221208 14:13:26 @agent_ppo2.py:155][0m |      policy_loss |       value_loss |          entropy |
[32m[20221208 14:13:26 @agent_ppo2.py:179][0m |           0.1634 |         208.0256 |        -820.2891 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |           0.1200 |         193.3530 |        -399.9669 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |           0.0655 |         187.2725 |        -545.0738 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |           0.0316 |         181.4547 |        -724.0899 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |           0.0057 |         178.4676 |        -835.6927 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |          -0.0109 |         176.4855 |        -916.2034 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |          -0.0177 |         173.2981 |        -949.1219 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |          -0.0300 |         171.4923 |        -986.9506 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |          -0.0360 |         171.6688 |       -1015.2147 |
[32m[20221208 14:13:27 @agent_ppo2.py:179][0m |          -0.0425 |         168.6483 |       -1054.6323 |
[32m[20221208 14:13:27 @agent_ppo2.py:124][0m Policy update time: 0.73 s
[32m[20221208 14:13:28 @agent_ppo2.py:132][0m Average TRAINING episode reward: 829.79
[32m[20221208 14:13:28 @agent_ppo2.py:133][0m Maximum TRAINING episode reward: 904.38
[32m[20221208 14:13:28 @agent_ppo2.py:134][0m Average EVALUATION episode reward: 738.67
[32m[20221208 14:13:28 @agent_ppo2.py:137][0m Total time:      28.12 min
[32m[20221208 14:13:28 @agent_ppo2.py:139][0m 2293760 total steps have happened
[32m[20221208 14:13:28 @agent_ppo2.py:115][0m #------------------------ Iteration 1120 --------------------------#
